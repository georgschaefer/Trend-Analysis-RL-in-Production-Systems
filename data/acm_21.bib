@inproceedings{10.1145/3427773.3427863,
author = {Spangher, Lucas and Gokul, Akash and Khattar, Manan and Palakapilly, Joseph and Agwan, Utkarsha and Tawade, Akaash and Spanos, Costas},
title = {Augmenting Reinforcement Learning with a Planning Model for Optimizing Energy Demand Response},
year = {2020},
isbn = {9781450381932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427773.3427863},
doi = {10.1145/3427773.3427863},
abstract = {While reinforcement learning (RL) on humans has shown incredible promise, it often suffers from a scarcity of data and few steps. In instances like these, a planning model of human behavior may greatly help. We present an experimental setup for the development and testing of an Soft Actor Critic (SAC) V2 RL architecture for several different neural architectures for planning models: an autoML optimized LSTM, an OLS, and a baseline model. We present the effects of including a planning model in agent learning within a simulation of the office, currently reporting a limited success with the LSTM.},
booktitle = {Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings \&amp; Cities},
pages = {39–42},
numpages = {4},
keywords = {Transactive control learning, Planning models for Reinforcement Learning, Energy demand response, Social Energy competitions, Reinforcement learning, AutoML neural architecture search},
location = {Virtual Event, Japan},
series = {RLEM'20}
}

@inproceedings{10.1145/2701126.2701191,
author = {Ho, Han Nguyen and Lee, Eunseok},
title = {Model-Based Reinforcement Learning Approach for Planning in Self-Adaptive Software System},
year = {2015},
isbn = {9781450333771},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701126.2701191},
doi = {10.1145/2701126.2701191},
abstract = {Policy-based adaptation is one of interesting topics in self-adaptive software research community. Current works in the field proposed the term of policy evolution, which concentrate to tackle the impact of environmental uncertainty on adaptation decision. These works adopted the advances of Reinforcement Learning (RL) to continuously optimize system behavior in run-time. However, there are several issues remain very primitive in current researches, especially the arbitrary exploitation--exploration trade-off and random exploration, which could lead to slow learning, hence, frail decision in exceptional situations. With model-free approach, these works could not leverage the knowledge about underlying system, which is essential and plentiful in software engineering, to enhance their learning. In this paper, we introduce the advantages of model-based RL. By utilizing engineering knowledge, system maintains a model of interaction with its environment and predicts the consequence of its action, to improve and guarantee system performance. We also discuss the engineering issues and propose a procedure to adopt model-based RL to build a self-adaptive software and bring policy evolution closer to real-world applications.},
booktitle = {Proceedings of the 9th International Conference on Ubiquitous Information Management and Communication},
articleno = {103},
numpages = {8},
keywords = {Bayesian inference, reinforcement learning, policy evolution, model-based RL},
location = {Bali, Indonesia},
series = {IMCOM '15}
}

@inproceedings{10.1145/3488560.3498515,
author = {Park, Sung-Jun and Chae, Dong-Kyu and Bae, Hong-Kyun and Park, Sumin and Kim, Sang-Wook},
title = {Reinforcement Learning over Sentiment-Augmented Knowledge Graphs towards Accurate and Explainable Recommendation},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498515},
doi = {10.1145/3488560.3498515},
abstract = {Explainable recommendation has gained great attention in recent years. A lot of work in this research line has chosen to use the knowledge graphs (KG) where relations between entities can serve as explanations. However, existing studies have not considered sentiment on relations in KG, although there can be various types of sentiment on relations worth considering (e.g., a user's satisfaction on an item). In this paper, we propose a novel recommendation framework based on KG integrated with sentiment analysis for more accurate recommendation as well as more convincing explanations. To this end, we first construct a Sentiment-Aware Knowledge Graph (namely, SAKG) by analyzing reviews and ratings on items given by users. Then, we perform item recommendation and reasoning over SAKG through our proposed Sentiment-Aware Policy Learning (namely, SAPL) based on a reinforcement learning strategy. To enhance the explainability for end-users, we further developed an interactive user interface presenting textual explanations as well as a collection of reviews related with the discovered sentiment. Experimental results on three real-world datasets verified clear improvements on both the accuracy of recommendation and the quality of explanations.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {784–793},
numpages = {10},
keywords = {explainable recommendation, knowledge graph, sentiment analysis},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3563357.3564064,
author = {Xu, Shichao and Fu, Yangyang and Wang, Yixuan and Yang, Zhuoran and O'Neill, Zheng and Wang, Zhaoran and Zhu, Qi},
title = {Accelerate Online Reinforcement Learning for Building HVAC Control with Heterogeneous Expert Guidances},
year = {2022},
isbn = {9781450398909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563357.3564064},
doi = {10.1145/3563357.3564064},
abstract = {Building heating, ventilation, and air conditioning (HVAC) systems account for nearly half of building energy consumption and 20\% of total energy consumption in the US. Their operation is also crucial for ensuring the physical and mental health of building occupants. Compared with traditional model-based HVAC control methods, the recent model-free deep reinforcement learning (DRL) based methods have shown good performance while do not require the development of detailed and costly physical models. However, these model-free DRL approaches often suffer from long training time to reach a good performance, which is a major obstacle for their practical deployment. In this work, we present a systematic approach to accelerate online reinforcement learning for HVAC control by taking full advantage of the knowledge from domain experts in various forms. Specifically, the algorithm stages include learning expert functions from existing abstract physical models and from historical data via offline reinforcement learning, integrating the expert functions with rule-based guidelines, conducting training guided by the integrated expert function and performing policy initialization from distilled expert function. Experimental results demonstrate up to 8.8X speedup over previous DRL-based methods.},
booktitle = {Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {89–98},
numpages = {10},
keywords = {reinforcement learning, deep learning, HVAC control},
location = {Boston, Massachusetts},
series = {BuildSys '22}
}

@inproceedings{10.1145/3528416.3530865,
author = {Zanatta, Luca and Barchi, Francesco and Bartolini, Andrea and Acquaviva, Andrea},
title = {Artificial versus Spiking Neural Networks for Reinforcement Learning in UAV Obstacle Avoidance},
year = {2022},
isbn = {9781450393386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528416.3530865},
doi = {10.1145/3528416.3530865},
abstract = {Spiking Neural Networks (SNN) are gaining more interest from the scientific community thanks to the promise of greater energy-efficient and greater computational power. This poses several challenges as today's SNN training for RL is based on Artificial Neural Network (ANN) training and then conversion from ANN to SNN, which does not leverage SNN event-based processing inherent capabilities. The present work compares an ANN and an SNN in an event-camera-based obstacle avoidance task, trained with Reinforcement Learning (RL) using the Deep Q-Learning (DQL) algorithm. We create an experimental setup composed of Unreal Engine 4, AirSim, and an event camera that simulates a real-world obstacle avoidance environment. Additionally, we train an SNN with a gradient-based training method enabling the use of all their expressiveness even in the training phase, showing comparable performance between the ANN and the SNN. To the best of our knowledge, we are the first that implements an entire realistic pipeline with a photo-realistic simulator (Airsim) and train an SNN without converting it from a pre-trained ANN.},
booktitle = {Proceedings of the 19th ACM International Conference on Computing Frontiers},
pages = {199–200},
numpages = {2},
keywords = {spiking neural network, DQN, reinforcement learning, simulation, drone, artificial intelligence},
location = {Turin, Italy},
series = {CF '22}
}

@inproceedings{10.1145/3383972.3384039,
author = {Liu, Yiming and Hu, Zheng},
title = {The Guiding Role of Reward Based on Phased Goal in Reinforcement Learning},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383972.3384039},
doi = {10.1145/3383972.3384039},
abstract = {Sparse and delayed rewards have greatly hindered the deep reinforcement learning, which is supposed to acquire the optimal policy by learning from trajectories. Reward shaping, which has previously been introduced to accelerate learning, is one of the most effective methods to tackle this crucial yet challenging problem. However, how to reasonably implement reward shaping needs to be explored. Currently, the method of reward shaping usually requires a large number of expert demonstrations, and the environment is poorly explored. In this paper, we proposed a method of reward shaping---Reinforcement learning framework based on phased goal, which will accelerate learning convergence speed with less expert examples and explore better especially for tasks where environment rewards are particularly sparse. The framework consists of reward based on phased goal and policy learning using PPO2. The process of acquiring designed reward is divided into stage classification and calculation of goal proximity. Experiments proved that our method can effectively alleviate the problem of sparse reward and obtain higher scores in Atari game than basic algorithm.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {535–541},
numpages = {7},
keywords = {Goal proximity, Reinforcement learning, Reward shaping, Phased goal},
location = {Shenzhen, China},
series = {ICMLC '20}
}

@inproceedings{10.1145/3580305.3599528,
author = {Han, Xiao and Zhao, Xiangyu and Zhang, Liang and Wang, Wanyu},
title = {Mitigating Action Hysteresis in Traffic Signal Control with Traffic Predictive Reinforcement Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599528},
doi = {10.1145/3580305.3599528},
abstract = {Traffic signal control plays a pivotal role in the management of urban traffic flow. With the rapid advancement of reinforcement learning, the development of signal control methods has seen a significant boost. However, a major challenge in implementing these methods is ensuring that signal lights do not change abruptly, as this can lead to traffic accidents. To mitigate this risk, a time-delay is introduced in the implementation of control actions, but usually has a negative impact on the overall efficacy of the control policy. To address this challenge, this paper presents a novel Traffic Signal Control Framework (PRLight), which leverages an On-policy Traffic Control Model (OTCM) and an Online Traffic Prediction Model (OTPM) to achieve efficient and real-time control of traffic signals. The framework collects multi-source traffic information from a local-view graph in real-time and employs a novel fast attention mechanism to extract relevant traffic features. To be specific, OTCM utilizes the predicted traffic state as input, eliminating the need for communication with other agents and maximizing computational efficiency while ensuring that the most relevant information is used for signal control. The proposed framework was evaluated on both simulated and real-world road networks and compared to various state-of-the-art methods, demonstrating its effectiveness in preventing traffic congestion and accidents.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {673–684},
numpages = {12},
keywords = {attention mechanism, graph convolutional networks, traffic signal control, reinforcement learning, traffic state prediction},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3384419.3430576,
author = {Rathore, Hemant},
title = {Adversarial Attacks on Malware Detection Models for Smartphones Using Reinforcement Learning: PhD Forum Abstract},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430576},
doi = {10.1145/3384419.3430576},
abstract = {Malware analysis and detection is a rat race between malware designer and anti-malware community. Most of the current Smartphone antivirus(s) are based on the signature, heuristic and behaviour based mechanisms which are unable to detect advanced polymorphic and metamorphic malware. Recently, researchers have developed state-of-the-art Android malware detection systems based on machine learning and deep learning. However, these models are prone to adversarial attacks which threaten the anti-malware ecosystem. Therefore in this work, we are investigating the robustness of Android malware detection models against adversarial attacks. We crafted adversarial attacks using reinforcement learning against detection models built using a variety of machine learning (classical, bagging, boosting) and deep learning algorithms. We are designing two adversarial attack strategies, namely single-policy and multi-policy attack for white-box and grey-box scenarios which are based on adversary's knowledge about the system. We designed the attack using Q-learning where a malicious application(s) is modified to generate variants which will force the detection models to misclassify them. The goal of the attack policy is to convert maximum Android applications (such that they are misclassified) with minimum modifications while maintaining the functional and behavioural integrity of applications. Preliminary results show an average fooling rate of around 40\% across twelve distinct detection models based on different classification algorithms. We are also designing defence against these adversarial attack using model retraining and distillation.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {796–797},
numpages = {2},
keywords = {machine learning, malware analysis and detection, reinforcement learning, smartphones, adversarial learning},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@inproceedings{10.5555/1867750.1867757,
author = {Lunsford, Rebecca and Heeman, Peter},
title = {Using Reinforcement Learning to Create Communication Channel Management Strategies for Diverse Users},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Spoken dialogue systems typically do not manage the communication channel, instead using fixed values for such features as the amplitude and speaking rate. Yet, the quality of a dialogue can be compromised if the user has difficulty understanding the system. In this proof-of-concept research, we explore using reinforcement learning (RL) to create policies that manage the communication channel to meet the needs of diverse users. Towards this end, we first formalize a preliminary communication channel model, in which users provide explicit feedback regarding issues with the communication channel, and the system implicitly alters its amplitude to accommodate the user's optimal volume. Second, we explore whether RL is an appropriate tool for creating communication channel management strategies, comparing two different hand-crafted policies to policies trained using both a dialogue-length and a novel annoyance cost. The learned policies performed better than hand-crafted policies, with those trained using the annoyance cost learning an equitable tradeoff between users with differing needs and also learning to balance finding a user's optimal amplitude against dialogue-length. These results suggest that RL can be used to create effective communication channel management policies for diverse users.},
booktitle = {Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies},
pages = {53–61},
numpages = {9},
location = {Los Angeles, California},
series = {SLPAT '10}
}

@inproceedings{10.1145/3163080.3163117,
author = {Sharma, Rajneesh and Kukker, Amit},
title = {Neural Reinforcement Learning Based Identifier for Typing Keys Using Forearm EMG Signals},
year = {2017},
isbn = {9781450353847},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3163080.3163117},
doi = {10.1145/3163080.3163117},
abstract = {This work proposes a neural reinforcement learning (NRL) identifier for accurate classification finger movements for typing tasks using forearm Electromyogram (EMG) signals. We first extract four key statistical features from the EMG signals (channel 1 and channel 2) corresponding to seven typing keys. Next, these features are fed to a reinforcement learning based k nearest neighbor neural classifier for identifying the keys using a "trial and error" approach. We use EMG data from typing tasks of ten subjects using two acquisition electrodes: channel 1 and channel 2. In the first part of our work, we attempt to classify typing keys using EMG data corresponding to one subject only. After sufficient learning, NRL classifier achieved an accuracy of 99.01\% and 98.29\% for channel 1 and channel 2, respectively. In second part of our work, we fed the EMG data of all the ten subjects to the NRL. The NRL is able to achieve a classification accuracy of 92.7\%. We also employ a subspace ensemble nearest neighbor approximator yielding a classification accuracy of 94.3\% with 5-cross fold validation and 97.1\% with 3-cross fold validation. Results show the effectiveness and viability of using NRL for identifying typing movements using forearm EMG signals.},
booktitle = {Proceedings of the 9th International Conference on Signal Processing Systems},
pages = {225–229},
numpages = {5},
keywords = {Electromyogram, Reinforcement Learning, Neural networks},
location = {Auckland, New Zealand},
series = {ICSPS 2017}
}

@inproceedings{10.1145/3576050.3576104,
author = {Zhang, Fan and Xing, Wanli and Li, Chenglu},
title = {Predicting Students’ Algebra I Performance Using Reinforcement Learning with Multi-Group Fairness},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576104},
doi = {10.1145/3576050.3576104},
abstract = {Numerous studies have successfully adopted learning analytics techniques such as machine learning (ML) to address educational issues. However, limited research has addressed the problem of algorithmic bias in ML. In the few attempts to develop strategies to concretely mitigate algorithmic bias in education, the focus has been on debiasing ML models with single group membership. This study aimed to propose an algorithmic strategy to mitigate bias in a multi-group context. The results showed that our proposed model could effectively reduce algorithmic bias in a multi-group setting while retaining competitive accuracy. The findings implied that there could be a paradigm shift from focusing on debiasing a single group to multiple groups in educational attempts on ML.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {657–662},
numpages = {6},
keywords = {reinforcement learning, multi-group fairness, math achievement prediction, fair AI},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.5555/3463952.3464012,
author = {El Mqirmi, Pierre and Belardinelli, Francesco and Le\'{o}n, Borja G.},
title = {An Abstraction-Based Method to Check Multi-Agent Deep Reinforcement-Learning Behaviors},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent reinforcement learning (RL) often struggles to ensure the safe behaviours of the learning agents, and therefore it is generally not adapted to safety-critical applications. To address this issue, we present a methodology that combines formal verification with (deep) RL algorithms to guarantee the satisfaction of formally-specified safety constraints both in training and testing. The approach we propose expresses the constraints to verify inProbabilistic Computation Tree Logic (PCTL) and builds an abstract representation of the system to reduce the complexity of the verification step. This abstract model allows for model checking techniques to identify a set of abstract policies that meet the safety constraints expressed in PCTL. Then, the agents' behaviours are restricted according to these safe abstract policies. We provide formal guarantees that by using this method, the actions of the agents always meet the safety constraints, and provide a procedure to generate an abstract model automatically. We empirically evaluate and show the effectiveness of our method in a multi-agent environment.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {474–482},
numpages = {9},
keywords = {safe reinforcement learning, formal methods, multi-agent reinforcement learning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3526241.3530335,
author = {Khan, Kamil and Pasricha, Sudeep and Kim, Ryan Gary},
title = {RACE: A Reinforcement Learning Framework for Improved Adaptive Control of NoC Channel Buffers},
year = {2022},
isbn = {9781450393225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526241.3530335},
doi = {10.1145/3526241.3530335},
abstract = {Network-on-chip (NoC) architectures rely on buffers to store flits to cope with contention for router resources during packet switching. Recently, reversible multi-function channel (RMC) buffers have been proposed to simultaneously reduce power and enable adaptive NoC buffering between adjacent routers. While adaptive buffering can improve NoC performance by maximizing buffer utilization, controlling the RMC buffer allocations requires a congestion-aware, scalable, and proactive policy. In this work, we present RACE, a novel reinforcement learning (RL) framework that utilizes better awareness of network congestion and a new reward metric ("falsefulls") to help guide the RL agent towards better RMC buffer control decisions. We show that RACE reduces NoC latency by up to 48.9\%, and energy consumption by up to 47.1\% against state-of-the-art NoC buffer control policies.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2022},
pages = {205–210},
numpages = {6},
keywords = {dynamic buffering, network-on-chip, machine learning},
location = {Irvine, CA, USA},
series = {GLSVLSI '22}
}

@inproceedings{10.1145/3546790.3546804,
author = {Akl, Mahmoud and Sandamirskaya, Yulia and Ergene, Deniz and Walter, Florian and Knoll, Alois},
title = {Fine-Tuning Deep Reinforcement Learning Policies with r-STDP for Domain Adaptation},
year = {2022},
isbn = {9781450397896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546790.3546804},
doi = {10.1145/3546790.3546804},
abstract = {Using deep reinforcement learning policies that are trained in simulation on real robotic platforms requires fine-tuning due to discrepancies between simulated and real environments. Multiple methods like domain randomization and system identification have been suggested to overcome this problem. However, sim-to-real transfer remains an open problem in robotics and deep reinforcement learning. In this paper, we present a spiking neural network (SNN) alternative for dealing with the sim-to-real problem. In particular, we train SNNs with backpropagation using surrogate gradients and the (Deep Q-Network) DQN algorithm to solve two classical control reinforcement learning tasks. The performance of the trained DQNs degrades when evaluated on randomized versions of the environments used during training. To compensate for the drop in performance, we apply the biologically plausible reward-modulated spike timing dependent plasticity (r-STDP) learning rule. Our results show that r-STDP can be successfully utilized to restore the network’s ability to solve the task. Furthermore, since r-STDP can be directly implemented on neuromorphic hardware, we believe it provides a promising neuromorphic solution to the sim-to-real problem.},
booktitle = {Proceedings of the International Conference on Neuromorphic Systems 2022},
articleno = {14},
numpages = {8},
keywords = {neural networks, reinforcement learning, spiking neural networks},
location = {Knoxville, TN, USA},
series = {ICONS '22}
}

@inproceedings{10.1145/3511808.3557435,
author = {Xia, Yuyang and Liu, Shuncheng and Chen, Xu and Xu, Zhi and Zheng, Kai and Su, Han},
title = {RISE: A Velocity Control Framework with Minimal Impacts Based on Reinforcement Learning},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557435},
doi = {10.1145/3511808.3557435},
abstract = {Velocity control in autonomous driving is an emerging technology that has achieved rapid progress over the last decade. However, existing velocity control models are developed in single-lane scenarios and ignore the negative impacts caused by harsh velocity changes. In this work, we propose a velocity control framework based on reinforcement learning, called RISE (contRol velocIty for autonomouS vEhicle). In multi-lane circumstances, RISE improves velocity decisions regarding the autonomous vehicle itself, while minimizing impacts on rear vehicles. To achieve multiple objectives, we propose a hybrid reward function to rate each velocity decision from four aspects: safety, efficiency, comfort, and negative impact to guide the autonomous vehicle. Among these reward factors, the negative impact is used to penalize the harsh actions of the autonomous vehicle, thus prompting it to reduce the negative impacts on its rear vehicles. To detect the latent perturbations among surrounding vehicles in multiple lanes, we propose an attention-based encoder to learn the positions and interactions from an impact graph. Extensive experiments evidence that RISE enables safe driving, and outperforms state-of-the-art methods in efficiency, comfort, and alleviating negative impacts.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {2210–2219},
numpages = {10},
keywords = {velocity control, autonomous vehicle, reinforcement learning},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3459104.3459123,
author = {Eichenlaub, Tobias and Rinderknecht, Stephan},
title = {Intelligent Set Speed Estimation for Vehicle Longitudinal Control with Deep Reinforcement Learning},
year = {2021},
isbn = {9781450389839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459104.3459123},
doi = {10.1145/3459104.3459123},
abstract = {Besides the goal of reducing driving tasks, modern longitudinal control systems also aim to improve fuel efficiency and driver comfort. Most of the vehicles use Adaptive Cruise Control (ACC) systems that track constant set speeds and set headways which makes the trajectory of the vehicle in headway mode highly dependent on the trajectory of a preceding vehicle. Hence, this might lead to increased consumptions in dense traffic situations or when the leader has a less careful driving style. In this work, a method based on Deep Reinforcement Learning (DRL) is presented that finds a control strategy by estimating an intelligent variable set speed based on the system state. Additional control objectives, such as minimizing consumption, are considered explicitly through the feedback in a reward function. A DRL framework is set up that enables the training of a neural set speed estimator for vehicle longitudinal control in a simulative environment. The Deep Deterministic Policy Gradient algorithm is used for the training of the agent. Training is carried out on a simple test track to teach the basic concepts of the control objective to the DRL agent. The learned behavior is then examined in a more complex, stochastic microscopic traffic simulation of the city center of Darmstadt and is compared to a conventional ACC algorithm. The analysis shows that the DRL controller is capable of finding fuel efficient trajectories which are less dependent on the preceding vehicle and is able to generalize to more complex traffic environments, but still shows some unexpected behavior in certain situations. The combination of DRL and conventional models to build up on the existing engineering knowledge is therefore expected to yield promising results in the future.},
booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
pages = {101–107},
numpages = {7},
location = {Seoul, Republic of Korea},
series = {ISEEIE 2021}
}

@inproceedings{10.1145/3308558.3313401,
author = {He, Suining and Shin, Kang G.},
title = {Spatio-Temporal Capsule-Based Reinforcement Learning for Mobility-on-Demand Network Coordination},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313401},
doi = {10.1145/3308558.3313401},
abstract = {As an alternative means of convenient and smart transportation, mobility-on-demand (MOD), typified by online ride-sharing and connected taxicabs, has been rapidly growing and spreading worldwide. The large volume of complex traffic and the uncertainty of market supplies/demands have made it essential for many MOD service providers to proactively dispatch vehicles towards ride-seekers. To meet this need effectively, we propose STRide, an MOD coordination-learning mechanism reinforced spatio-temporally with capsules. We formalize the adaptive coordination of vehicles into a reinforcement learning framework. STRide incorporates spatial and temporal distributions of supplies (vehicles) and demands (ride requests), customers' preferences and other external factors. A novel spatio-temporal capsule neural network is designed to predict the provider's rewards based on MOD network states, vehicles and their dispatch actions. This way, the MOD platform adapts itself to the supply-demand dynamics with the best potential rewards. We have conducted extensive data analytics and experimental evaluation with three large-scale datasets (~ 21 million rides from Uber, Yellow Taxis and Didi). STRide is shown to outperform state-of-the-arts, substantially reducing request-rejection rate and passenger waiting time, and also increasing the service provider's profits, often making 30\% improvement over state-of-the-arts.},
booktitle = {The World Wide Web Conference},
pages = {2806–2813},
numpages = {8},
keywords = {smart transportation coordination, smart city., ride-sharing platform, reinforcement learning, Mobility-on-demand, capsule network},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.5555/3545946.3598922,
author = {Wai, Khaing Phyo and Geng, Minghong and Subagdja, Budhitama and Pateria, Shubham and Tan, Ah-Hwee},
title = {Towards Explaining Sequences of Actions in Multi-Agent Deep Reinforcement Learning Models},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Although Multi-agent Deep Reinforcement Learning (MADRL) has shown promising results in solving complex real-world problems, the applicability and reliability of MADRL models are often limited by a lack of understanding of their inner workings for explaining the decisions made. To address this issue, this paper proposes a novel method for explaining MADRL by generalizing the sequences of action events performed by agents into high-level abstract strategies using a spatio-temporal neural network model. Specifically, an interval-based memory retrieval procedure is developed to generalize the encoded sequences of action events over time into short sequential patterns. In addition, two abstraction algorithms are introduced, one for abstracting action events across multiple agents and the other for further abstracting the episodes over time into short sequential patterns, which can then be translated into symbolic form for interpretation. We evaluate the proposed method using the StarCraft Multi Agent Challenge (SMAC) benchmark task, which shows that the method is able to derive high-level explanations of MADRL models at various levels of granularity.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2325–2327},
numpages = {3},
keywords = {explainable deep reinforcement learning, multi agent deep reinforcement learning, explainable artificial intelligence},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3488560.3498487,
author = {Ge, Yingqiang and Zhao, Xiaoting and Yu, Lucia and Paul, Saurabh and Hu, Diane and Hsieh, Chu-Cheng and Zhang, Yongfeng},
title = {Toward Pareto Efficient Fairness-Utility Trade-off in Recommendation through Reinforcement Learning},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498487},
doi = {10.1145/3488560.3498487},
abstract = {The issue of fairness in recommendation is becoming increasingly essential as Recommender Systems (RS) touch and influence more and more people in their daily lives. In fairness-aware recommendation, most of the existing algorithmic approaches mainly aim at solving a constrained optimization problem by imposing a constraint on the level of fairness while optimizing the main recommendation objective, e.g., click through rate (CTR). While this alleviates the impact of unfair recommendations, the expected return of an approach may significantly compromise the recommendation accuracy due to the inherent trade-off between fairness and utility. This motivates us to deal with these conflicting objectives and explore the optimal trade-off between them in recommendation. One conspicuous approach is to seek aPareto efficient/optimal solution to guarantee optimal compromises between utility and fairness. Moreover, considering the needs of real-world e-commerce platforms, it would be more desirable if we can generalize the wholePareto Frontier, so that the decision-makers can specify any preference of one objective over another based on their current business needs. Therefore, in this work, we propose a fairness-aware recommendation framework usingmulti-objective reinforcement learning (MORL), called MoFIR (pronounced "more fair ''), which is able to learn a single parametric representation for optimal recommendation policies over the space of all possible preferences. Specially, we modify traditional Deep Deterministic Policy Gradient (DDPG) by introducingconditioned network (CN) into it, which conditions the networks directly on these preferences and outputs Q-value-vectors. Experiments on several real-world recommendation datasets verify the superiority of our framework on both fairness metrics and recommendation measures when compared with all other baselines. We also extract the approximate Pareto Frontier on real-world datasets generated by MoFIR and compare to state-of-the-art fairness methods.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {316–324},
numpages = {9},
keywords = {pareto efficient fairness, recommender system, unbiased recommendation, multi-objective reinforcement learning},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.5555/3535850.3536005,
author = {Xiao, Baicen and Ramasubramanian, Bhaskar and Poovendran, Radha},
title = {Agent-Temporal Attention for Reward Redistribution in Episodic Multi-Agent Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper considers multi-agent reinforcement learning (MARL) tasks where agents receive a shared global reward at the end of an episode. The delayed nature of this reward affects the ability of the agents to assess the quality of their actions at intermediate time-steps. This paper focuses on developing methods to learn a temporal redistribution of the episodic reward to obtain a dense reward signal. Solving such MARL problems requires addressing two challenges: identifying (1) relative importance of states along the length of an episode (along time), and (2) relative importance of individual agents' states at any single time-step (among agents). In this paper, we introduce Agent-Temporal Attention for Reward Redistribution in Episodic Multi-Agent Reinforcement Learning (AREL) to address these two challenges. AREL uses attention mechanisms to characterize the influence of actions on state transitions along trajectories (temporal attention), and how each agent is affected by other agents at each time-step (agent attention). The redistributed rewards predicted by AREL are dense, and can be integrated with any given MARL algorithm. We evaluate AREL on challenging tasks from the Particle World environment and the StarCraft Multi-Agent Challenge. AREL results in higher rewards in Particle World, and improved win rates in StarCraft compared to three state-of-the-art reward redistribution methods. Our code is available at https://github.com/baicenxiao/AREL.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1391–1399},
numpages = {9},
keywords = {credit assignment, attention mechanism, maulti-agent reinforcement learning, episodic rewards},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3490100.3516475,
author = {Punzi, Miriam and Ladeveze, Nicolas and Nguyen, Huyen and Ravenet, Brian},
title = {ImCasting: Nonverbal Behaviour Reinforcement Learning of Virtual Humans through Adaptive Immersive Game},
year = {2022},
isbn = {9781450391450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490100.3516475},
doi = {10.1145/3490100.3516475},
abstract = {This research work puts a focus on the user experience of an alternative method to teach nonverbal behaviour to Embodied Conversational Agents in immersive environments. We overcome the limitations of the existing approaches by proposing an adaptive Virtual Reality game, called ImCasting, in which the player takes an active role in improving the learning models of the agents. Specifically, we based our approach on the Human-in-the-loop framework with human preferences to teach the nonverbal behaviour to the agents through the system. We introduce game mechanisms built around all the tasks of this Machine Learning framework, designing how a human should interact within this framework in real-time. The study explores how a game interaction in an immersive environment can improve the user experience in performing this interactive task, sharing the same space with the learning agents. In particular, we focus on the involvement of the players as well as the usability of the system. We conducted a preliminary evaluation to compare the design of our system with a baseline system which does not use any game mechanisms in teaching the nonverbal behaviour to virtual agents. Results suggest that our design concept and the game story are more engaging, increasing the satisfaction usability factor perceived by the participants.},
booktitle = {27th International Conference on Intelligent User Interfaces},
pages = {62–65},
numpages = {4},
location = {Helsinki, Finland},
series = {IUI '22 Companion}
}

@inproceedings{10.1145/2396761.2398676,
author = {Chali, Yllias and Hasan, Sadid A. and Imam, Kaisar},
title = {Improving the Performance of the Reinforcement Learning Model for Answering Complex Questions},
year = {2012},
isbn = {9781450311564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396761.2398676},
doi = {10.1145/2396761.2398676},
abstract = {This paper addresses the task of answering complex questions using a multi-document summarization approach within a reinforcement learning setting. Given a set of complex questions, a list of relevant documents per question, and the corresponding human-generated summaries (i.e. answers to the questions) as training data, the reinforcement learning module iteratively learns a number of feature weights in order to facilitate the automatic generation of summaries i.e. answers to unseen complex questions. Previous works on this task have utilized a fully automatic reinforcement learning framework that selects the document sentences as the potential candidate (i.e. machine-generated) summary sentences by exploiting a relatedness measure with the available human-written summaries. In this paper, we propose an extension to this model that incorporates user interaction into the reinforcement learner to guide the candidate summary sentence selection process. Experimental results reveal the effectiveness of the user interaction component in the reinforcement learning framework.},
booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
pages = {2499–2502},
numpages = {4},
keywords = {user interaction, complex question answering, multi-document summarization, reinforcement learning},
location = {Maui, Hawaii, USA},
series = {CIKM '12}
}

@inproceedings{10.1145/3357384.3357799,
author = {Zhou, Ming and Jin, Jiarui and Zhang, Weinan and Qin, Zhiwei and Jiao, Yan and Wang, Chenxi and Wu, Guobin and Yu, Yong and Ye, Jieping},
title = {Multi-Agent Reinforcement Learning for Order-Dispatching via Order-Vehicle Distribution Matching},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357799},
doi = {10.1145/3357384.3357799},
abstract = {Improving the efficiency of dispatching orders to vehicles is a research hotspot in online ride-hailing systems. Most of the existing solutions for order-dispatching are centralized controlling, which require to consider all possible matches between available orders and vehicles. For large-scale ride-sharing platforms, there are thousands of vehicles and orders to be matched at every second which is of very high computational cost. In this paper, we propose a decentralized execution order-dispatching method based on multi-agent reinforcement learning to address the large-scale order-dispatching problem. Different from the previous cooperative multi-agent reinforcement learning algorithms, in our method, all agents work independently with the guidance from an evaluation of the joint policy since there is no need for communication or explicit cooperation between agents. Furthermore, we use KL-divergence optimization at each time step to speed up the learning process and to balance the vehicles (supply) and orders (demand). Experiments on both the explanatory environment and real-world simulator show that the proposed method outperforms the baselines in terms of accumulated driver income (ADI) and Order Response Rate (ORR) in various traffic environments. Besides, with the support of the online platform of Didi Chuxing, we designed a hybrid system to deploy our model.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2645–2653},
numpages = {9},
keywords = {multi-agent reinforcement learning, deep reinforcement learning, order-dispatching, ride-hailing},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3242102.3242110,
author = {Dakdouk, Hiba and Tarazona, Erika and Alami, Reda and F\'{e}raud, Rapha\"{e}l and Papadopoulos, Georgios Z. and Maill\'{e}, Patrick},
title = {Reinforcement Learning Techniques for Optimized Channel Hopping in IEEE 802.15.4-TSCH Networks},
year = {2018},
isbn = {9781450359603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242102.3242110},
doi = {10.1145/3242102.3242110},
abstract = {The Industrial Internet of Things (IIoT) faces multiple challenges to achieve high reliability, low-latency and low power consumption. The IEEE 802.15.4 Time-Slotted Channel Hopping (TSCH) protocol aims to address these issues by using frequency hopping to improve the transmission quality when coping with low-quality channels. However, an optimized transmission system should also try to favor the use of high-quality channels, which are unknown a priori. Hence reinforcement learning algorithms could be useful.In this work, we perform an evaluation of 9 Multi-Armed Bandit (MAB) algorithms--some specific learning algorithms adapted to that case--in a IEEE 802.15.4-TSCH context, in order to select the ones that choose high-performance channels, using data collected through the FIT IoT-LAB platform. Then, we propose a combined mechanism that uses the selected algorithms integrated with TSCH. The performance evaluation suggests that our proposal can significantly improve the packet delivery ratio compared to the default TSCH operation, thereby increasing the reliability and the energy efficiency of the transmissions.},
booktitle = {Proceedings of the 21st ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {99–107},
numpages = {9},
keywords = {radio characterization, ieee 802.15.4, learning algorithms, tsch, internet of things, multi-armed bandit algorithms},
location = {Montreal, QC, Canada},
series = {MSWIM '18}
}

@inproceedings{10.1145/3447928.3456639,
author = {Cohen, Max H. and Belta, Calin},
title = {Model-Based Reinforcement Learning for Approximate Optimal Control with Temporal Logic Specifications},
year = {2021},
isbn = {9781450383394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447928.3456639},
doi = {10.1145/3447928.3456639},
abstract = {In this paper we study the problem of synthesizing optimal control policies for uncertain continuous-time nonlinear systems from syntactically co-safe linear temporal logic (scLTL) formulas. We formulate this problem as a sequence of reach-avoid optimal control sub-problems. We show that the resulting hybrid optimal control policy guarantees the satisfaction of a given scLTL formula by constructing a barrier certificate. Since solving each optimal control problem may be computationally intractable, we take a learning-based approach to approximately solve this sequence of optimal control problems online without requiring full knowledge of the system dynamics. Using Lyapunov-based tools, we develop sufficient conditions under which our approximate solution maintains correctness. Finally, we demonstrate the efficacy of the developed method with a numerical example.},
booktitle = {Proceedings of the 24th International Conference on Hybrid Systems: Computation and Control},
articleno = {12},
numpages = {11},
keywords = {safe reinforcement learning, optimal control, hybrid systems},
location = {Nashville, Tennessee},
series = {HSCC '21}
}

@inproceedings{10.5555/3545946.3598769,
author = {Sun, Haoyuan and Wu, Feng},
title = {Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Offline reinforcement learning (RL) aims to learn a policy from a fixed dataset, without further interactions with the environment. However, offline datasets are often very noisy, which consist of large quantities of sub-optimal or task-agnostic trajectories. Therefore, it is very challenging for offline RL to learn an optimal policy from such datasets. To address this, we use reward machines (RM) to encode human knowledge about the task and refine datasets for offline RL. Specifically, we define the event-ordered RM to label offline datasets with RM states. Then, we further use the labeled datasets to generate refined datasets, which is smaller but better for offline RL. By using the RM, we can decompose a long-horizon task into easier sub-tasks, inform the agent about their current stage along task completion, and guide the offline learning process. In addition, we generate counterfactual experiences by RM to guide agent to complete each sub-task. Experimental results in the D4RL benchmark confirm that our method achieves better performance in long-horizon manipulation tasks with sub-optimal datasets.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1239–1247},
numpages = {9},
keywords = {offline reinforcement learning, counterfactual reasoning, reward machines, dataset refinement},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3427773.3427873,
author = {Lee, Joon-Yong and Huang, Sen and Rahman, Aowabin and Smith, Amanda D. and Katipamula, Srinivas},
title = {Flexible Reinforcement Learning Framework for Building Control Using EnergyPlus-Modelica Energy Models},
year = {2020},
isbn = {9781450381932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427773.3427873},
doi = {10.1145/3427773.3427873},
abstract = {In recent years, reinforcement learning (RL) methods have been greatly enhanced by leveraging deep learning approaches. RL methods applied to building control have shown potential in many applications because of their ability to complement or replace conventional methods such as model-based or rule-based controls. However, RL-based building control software is likely tailored either to one target building system or to a specific RL method so that significant additional effort would be required to customize the RL-based controller for use in other building systems or with other RL approaches. Also, RL-based building controls usually depend on building energy simulations to train controllers, so emulating building dynamics (i.e., thermal dynamics and control dynamics) and capturing sub-hourly dynamic profiles are crucial to further the development of effective RL-based building control methods. To address these challenges, we present an RL-based control software employing a high-fidelity hybrid EnergyPlus-Modelica building energy model that emulates building dynamics at 1 minute resolution. This software consists of decoupled components (environment, building emulator, control agent, and RL algorithm), which allows for quick prototyping and benchmarking of standard RL algorithms in different systems; for example, a single component can be replaced without revising the software. To demonstrate this software framework, we conducted a benchmark study using an EnergyPlus-Modelica building energy model for a Chicago office building with an RL-based controller to dynamically control the chilled water temperature setpoint and the air handling unit supply air temperature setpoints.},
booktitle = {Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings \&amp; Cities},
pages = {34–38},
numpages = {5},
keywords = {functional mock-up interface (FMI), deep reinforcement learning, transactive control, EnergyPlus, openai gym},
location = {Virtual Event, Japan},
series = {RLEM'20}
}

@article{10.1145/3475991,
author = {Ray, Kaustabha and Banerjee, Ansuman},
title = {Horizontal Auto-Scaling for Multi-Access Edge Computing Using Safe Reinforcement Learning},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3475991},
doi = {10.1145/3475991},
abstract = {Multi-Access Edge Computing (MEC) has emerged as a promising new paradigm allowing low latency access to services deployed on edge servers to avert network latencies often encountered in accessing cloud services. A key component of the MEC environment is an auto-scaling policy which is used to decide the overall management and scaling of container instances corresponding to individual services deployed on MEC servers to cater to traffic fluctuations. In this work, we propose a Safe Reinforcement Learning (RL)-based auto-scaling policy agent that can efficiently adapt to traffic variations to ensure adherence to service specific latency requirements. We model the MEC environment using a Markov Decision Process (MDP). We demonstrate how latency requirements can be formally expressed in Linear Temporal Logic (LTL). The LTL specification acts as a guide to the policy agent to automatically learn auto-scaling decisions that maximize the probability of satisfying the LTL formula. We introduce a quantitative reward mechanism based on the LTL formula to tailor service specific latency requirements. We prove that our reward mechanism ensures convergence of standard Safe-RL approaches. We present experimental results in practical scenarios on a test-bed setup with real-world benchmark applications to show the effectiveness of our approach in comparison to other state-of-the-art methods in literature. Furthermore, we perform extensive simulated experiments to demonstrate the effectiveness of our approach in large scale scenarios.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {oct},
articleno = {109},
numpages = {33},
keywords = {Multi-access edge computing, safe reinforcement learning, auto-scaling}
}

@article{10.5555/3455716.3455883,
author = {Kallus, Nathan and Uehara, Masatoshi},
title = {Double Reinforcement Learning for Efficient Off-Policy Evaluation in Markov Decision Processes},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Off-policy evaluation (OPE) in reinforcement learning allows one to evaluate novel decision policies without needing to conduct exploration, which is often costly or otherwise infeasible. We consider for the first time the semiparametric efficiency limits of OPE in Markov decision processes (MDPs), where actions, rewards, and states are memoryless. We show existing OPE estimators may fail to be efficient in this setting. We develop a new estimator based on cross-fold estimation of q-functions and marginalized density ratios, which we term double reinforcement learning (DRL). We show that DRL is efficient when both components are estimated at fourth-root rates and is also doubly robust when only one component is consistent. We investigate these properties empirically and demonstrate the performance benefits due to harnessing memorylessness.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {167},
numpages = {63},
keywords = {double machine learning, Markov decision processes, semiparametric efficiency, off-policy evaluation}
}

@inproceedings{10.5555/3545946.3599092,
author = {Sunehag, Peter and Vezhnevets, Alexander Sasha and Du\'{e}\~{n}ez-Guzm\'{a}n, Edgar A. and Mordatch, Igor and Leibo, Joel Z.},
title = {Diversity Through Exclusion (DTE): Niche Identification for Reinforcement Learning through Value-Decomposition},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Many environments contain numerous available niches of variable value, each associated with a different local optimum in the space of behaviors (policy space). In this work we propose a generic reinforcement learning (RL) algorithm where multiple sub-policies are learnt in a manner inspired by fitness sharing in evolutionary computation and applied in reinforcement learning using Value-Decomposition-Networks in a novel manner for a single-agent's internal population. Further, we introduce an artificial chemistry inspired platform where it is easy to create tasks with multiple rewarding strategies utilizing different resources (i.e. multiple niches). We show that agents trained this way can escape poor-but-attractive local optima to instead converge to harder-to-discover higher value strategies in both the artificial chemistry environments and in simpler illustrative environments.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2827–2829},
numpages = {3},
keywords = {artificial chemistry, ecology, competitive exclusion, reinforcement learning, value-decomposition, diversity},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3545946.3598863,
author = {Kim, Woojun and Sung, Youngchul},
title = {Parameter Sharing with Network Pruning for Scalable Multi-Agent Deep Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Handling the problem of scalability is one of the essential issues for multi-agent reinforcement learning (MARL) algorithms to be applied to real-world problems typically involving massively many agents. For this, parameter sharing across multiple agents has widely been used since it reduces the training time by decreasing the number of parameters and increasing the sample efficiency. However, using the same parameters across agents limits the representational capacity of the joint policy and consequently, the performance can be degraded in multi-agent tasks that require different behaviors for different agents. In this paper, we propose a simple method that adopts structured pruning for a deep neural network to increase the representational capacity of the joint policy without introducing additional parameters. We evaluate the proposed method on several benchmark tasks, and numerical results show that the proposed method significantly outperforms other parameter-sharing methods.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1942–1950},
numpages = {9},
keywords = {neural network pruning, parameter sharing, multi-agent reinforcement learning, scalability},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3534678.3539416,
author = {Zhang, Weijia and Liu, Hao and Han, Jindong and Ge, Yong and Xiong, Hui},
title = {Multi-Agent Graph Convolutional Reinforcement Learning for Dynamic Electric Vehicle Charging Pricing},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539416},
doi = {10.1145/3534678.3539416},
abstract = {Electric Vehicles (EVs) have been emerging as a promising low-carbon transport target. While a large number of public charging stations are available, the use of these stations is often imbalanced, causing many problems to Charging Station Operators (CSOs). To this end, in this paper, we propose a Multi-Agent Graph Convolutional Reinforcement Learning (MAGC) framework to enable CSOs to achieve more effective use of these stations by providing dynamic pricing for each of the continuously arising charging requests with optimizing multiple long-term commercial goals. Specifically, we first formulate this charging station request-specific dynamic pricing problem as a mixed competitive-cooperative multi-agent reinforcement learning task, where each charging station is regarded as an agent. Moreover, by modeling the whole charging market as a dynamic heterogeneous graph, we devise a multi-view heterogeneous graph attention networks to integrate complex interplay between agents induced by their diversified relationships. Then, we propose a shared meta generator to generate individual customized dynamic pricing policies for large-scale yet diverse agents based on the extracted meta characteristics. Finally, we design a contrastive heterogeneous graph pooling representation module to learn a condensed yet effective state action representation to facilitate policy learning of large-scale agents. Extensive experiments on two real-world datasets demonstrate the effectiveness of MAGC and empirically show that the overall use of stations can be improved if all the charging stations in a charging market embrace our dynamic pricing policy.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2471–2481},
numpages = {11},
keywords = {graph neural networks, charging station dynamic pricing, graph contrastive learning, multi-agent reinforcement learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3555858.3563282,
author = {Wang, Ziqi and Liu, Jialin and Yannakakis, Georgios N.},
title = {The Fun Facets of Mario: Multifaceted Experience-Driven PCG via Reinforcement Learning},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3563282},
doi = {10.1145/3555858.3563282},
abstract = {The recently introduced EDRL framework approaches the experience-driven (ED) procedural generation of game content via a reinforcement learning (RL) perspective. EDRL has so far shown its effectiveness in generating novel platformer game levels endlessly in an online fashion. This paper extends the framework by integrating multiple facets of game creativity in the ED generation process. In particular, we employ EDRL on the creative facets of game level and gameplay design in Super Mario Bros. Inspired by Koster’s theory of fun, we formulate fun as moderate degrees of level or gameplay divergence and equip the algorithm with such reward functions. Moreover, we enable faster and more efficient game content generation through an episodic generative soft actor-critic algorithm. The resulting multifaceted EDRL is not only capable of generating fun levels efficiently, but it is also robust with respect to dissimilar playing styles and initial game level conditions.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {44},
numpages = {8},
keywords = {procedural content generation via reinforcement learning, platformer games, online level generation, Super Mario Bros, Experience-driven procedural content generation},
location = {Athens, Greece},
series = {FDG '22}
}

@inproceedings{10.1145/3549737.3549808,
author = {Vouros, George},
title = {Tutorial on Explainable Deep Reinforcement Learning: One Framework, Three Paradigms and Many Challenges.},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549737.3549808},
doi = {10.1145/3549737.3549808},
abstract = {Interpretability, explainability and transparency are key issues to introducing Artificial Intelligence closed—box methods in many critical domains: This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability and fairness, and has important consequences towards keeping the human in the loop in high levels of automation, especially in critical cases for decision making. Reinforcement learning methods, and especially their deep versions, are closed-box methods that support agents to act autonomously in the real world. This tutorial will provide a formal specification of the deep reinforcement learning explainability problems, and will present the necessary components of a general explainable reinforcement learning framework. Based on this framework will provide distinct explainability paradigms towards solving explainability problems, with examples from state-of-the-art methods and real-world cases. The tutorial will conclude identifying open questions and important challenges. The tutorial is based on the survey paper on “Explainable Deep Reinforcement Learning” State of the Art and Challenges” [1].},
booktitle = {Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
articleno = {67},
numpages = {1},
keywords = {Transparency, Deep Learning, Explainability, Interpretability, Deep Reinforcement Learning},
location = {Corfu, Greece},
series = {SETN '22}
}

@inproceedings{10.1145/3545008.3545074,
author = {Wang, Haoyu and Zheng, Kevin and Reiss, Charles and Shen, Haiying},
title = {NCC: Neighbor-Aware Congestion Control Based on Reinforcement Learning for Datacenter Networks},
year = {2023},
isbn = {9781450397339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545008.3545074},
doi = {10.1145/3545008.3545074},
abstract = {The challenges of low latency, high throughput datacenter networks create new traffic management problems that require new congestion control mechanisms. Generally, the proposals to solve this problem have focused either on refining existing window-based congestion control like in TCP or on introducing a central controller to make congestion control decisions. In this paper, we propose a third approach, where nodes share network information with their neighbors and apply this information to make local decisions that limit global congestion. In our implementation, the rate limiting decisions on one node are driven by the local agent that uses reinforcement learning to optimize a combination of overall latency, throughput and the shared information. To make this approach efficient, the local agents choose overall rate limits for each node, and then a separate process assigns the traffic of individual flows within these limits. We show that, in trace-driven real implementation, our method achieves better congestion avoidance than several end-to-end and centralized mechanisms in prior work.},
booktitle = {Proceedings of the 51st International Conference on Parallel Processing},
articleno = {62},
numpages = {10},
keywords = {Congestion control, Reinforcement learning, Datacenter network},
location = {Bordeaux, France},
series = {ICPP '22}
}

@article{10.1109/TNET.2022.3187310,
author = {Mason, Federico and Nencioni, Gianfranco and Zanella, Andrea},
title = {Using Distributed Reinforcement Learning for Resource Orchestration in a Network Slicing Scenario},
year = {2022},
issue_date = {Feb. 2023},
publisher = {IEEE Press},
volume = {31},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3187310},
doi = {10.1109/TNET.2022.3187310},
abstract = {The Network Slicing (NS) paradigm enables the partition of physical and virtual resources among multiple logical networks, possibly managed by different tenants. In such a scenario, network resources need to be dynamically allocated according to the slice requirements. In this paper, we attack the above problem by exploiting a Deep Reinforcement Learning approach. Our framework is based on a distributed architecture, where multiple agents cooperate towards a common goal. The agent training is carried out following the Advantage Actor Critic algorithm, which permits to handle continuous action spaces. By means of extensive simulations, we show that our approach yields better performance than both a static allocation of system resources and an efficient empirical strategy. At the same time, the proposed system ensures high adaptability to different scenarios without the need for additional training.},
journal = {IEEE/ACM Trans. Netw.},
month = {jul},
pages = {88–102},
numpages = {15}
}

@inproceedings{10.1145/3568562.3568631,
author = {Do Thi Thu, Hien and Phan, The Duy and Le Anh, Hao and Nguyen Duy, Lan and Nghi Hoang, Khoa and Pham, Van-Hau},
title = {A Method of Mutating Windows Malwares Using Reinforcement Learning with Functionality Preservation},
year = {2022},
isbn = {9781450397254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568562.3568631},
doi = {10.1145/3568562.3568631},
abstract = {Recently, the development in both the quantity and complication of malware has raised a need for powerful malware detection solutions. The outstanding characteristics of machine learning (ML) and deep learning (DL) techniques have been leveraged in the fight against malware. However, they are proven to be vulnerable to adversarial attacks, where intended modifications in malware can flip the detection result and then evade the detector’s eyes. This research area is being focused on and deeply interested in many publications due to its significance in the evaluation of malware detection approaches. In such works, using Generative Adversarial Networks (GANs) or Reinforcement Learning (RL) can help malware authors craft metamorphic malware against antivirus. Unfortunately, the functionality of created malware is not mentioned and verified during the mutation phase, which can result in evasive but useless malware mutants. In this paper, we focus on Windows Portable Executable malware and propose an RL-based malware mutant creation approach to fool black-box static ML/DL-based detectors. Specifically, we introduce a validator to confirm functionality preservation, which is one of our requirements for a successfully created malware. The experiment results prove the effectiveness of our solution in crafting elusive and executable Windows malware mutants.},
booktitle = {Proceedings of the 11th International Symposium on Information and Communication Technology},
pages = {142–149},
numpages = {8},
keywords = {reinforcement learning, Malware, malware mutant},
location = {Hanoi, Vietnam},
series = {SoICT '22}
}

@inproceedings{10.1145/3544793.3560403,
author = {Tang, Xiao and Liu, Sicong},
title = {Power and Interference Control for VLC-Based UDN: A Reinforcement Learning Approach},
year = {2023},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544793.3560403},
doi = {10.1145/3544793.3560403},
abstract = {Visible light communication (VLC) has been widely applied as a promising solution for modern short range communication. When it comes to the deployment of LED arrays in VLC networks, the emerging ultra-dense network (UDN) technology can be adopted to expand the VLC network’s capacity. However, the problem of inter-cell interference (ICI) mitigation and efficient power control in the VLC-based UDN is still a critical challenge. To this end, a reinforcement learning (RL) based VLC UDN architecture is devised in this paper. The deployment of the cells is optimized via spatial reuse to mitigate ICI. An RL-based algorithm is proposed to dynamically optimize the policy of power and interference control, maximizing the system utility in the complicated and dynamic environment. Simulation results demonstrate the superiority of the proposed scheme, it increase the system utility and achievable data rate while reducing the energy consumption and ICI, which outperforms the benchmark scheme.},
booktitle = {Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
pages = {432–437},
numpages = {6},
keywords = {Visible light communication, ultra-dense network, reinforcement learning, power control, inter-cell interference.},
location = {Cambridge, United Kingdom},
series = {UbiComp/ISWC '22 Adjunct}
}

@inproceedings{10.1145/3368555.3384450,
author = {Prasad, Niranjani and Engelhardt, Barbara and Doshi-Velez, Finale},
title = {Defining Admissible Rewards for High-Confidence Policy Evaluation in Batch Reinforcement Learning},
year = {2020},
isbn = {9781450370462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368555.3384450},
doi = {10.1145/3368555.3384450},
abstract = {A key impediment to reinforcement learning (RL) in real applications with limited, batch data is in defining a reward function that reflects what we implicitly know about reasonable behaviour for a task and allows for robust off-policy evaluation. In this work, we develop a method to identify an admissible set of reward functions for policies that (a) do not deviate too far in performance from prior behaviour, and (b) can be evaluated with high confidence, given only a collection of past trajectories. Together, these ensure that we avoid proposing unreasonable policies in high-risk settings. We demonstrate our approach to reward design on synthetic domains as well as in a critical care context, to guide the design of a reward function that consolidates clinical objectives to learn a policy for weaning patients from mechanical ventilation.},
booktitle = {Proceedings of the ACM Conference on Health, Inference, and Learning},
pages = {1–9},
numpages = {9},
keywords = {reward design, off-policy evaluation, reinforcement learning},
location = {Toronto, Ontario, Canada},
series = {CHIL '20}
}

@inproceedings{10.1145/3551349.3560429,
author = {Su, Jianzhong and Dai, Hong-Ning and Zhao, Lingjun and Zheng, Zibin and Luo, Xiapu},
title = {Effectively Generating Vulnerable Transaction Sequences in Smart Contracts with Reinforcement Learning-Guided Fuzzing},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560429},
doi = {10.1145/3551349.3560429},
abstract = {As computer programs run on top of blockchain, smart contracts have proliferated a myriad of decentralized applications while bringing security vulnerabilities, which may cause huge financial losses. Thus, it is crucial and urgent to detect the vulnerabilities of smart contracts. However, existing fuzzers for smart contracts are still inefficient to detect sophisticated vulnerabilities that require specific vulnerable transaction sequences to trigger. To address this challenge, we propose a novel vulnerability-guided fuzzer based on reinforcement learning, namely RLF, for generating vulnerable transaction sequences to detect such sophisticated vulnerabilities in smart contracts. In particular, we firstly model the process of fuzzing smart contracts as a Markov decision process to construct our reinforcement learning framework. We then creatively design an appropriate reward with consideration of both vulnerability and code coverage so that it can effectively guide our fuzzer to generate specific transaction sequences to reveal vulnerabilities, especially for the vulnerabilities related to multiple functions. We conduct extensive experiments to evaluate RLF’s performance. The experimental results demonstrate that our RLF outperforms state-of-the-art vulnerability-detection tools (e.g., detecting 8\%-69\% more vulnerabilities within 30 minutes).},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {36},
numpages = {12},
keywords = {Reinforcement learning, Smart contract, Fuzzing},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1109/ASONAM49781.2020.9381471,
author = {Ali, Khurshed and Wang, Chih-Yu and Yeh, Mi-Yen and Chen, Yi-Shin},
title = {Addressing Competitive Influence Maximization on Unknown Social Network with Deep Reinforcement Learning},
year = {2021},
isbn = {9781728110561},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASONAM49781.2020.9381471},
doi = {10.1109/ASONAM49781.2020.9381471},
abstract = {Recent studies have considered the reinforcement and deep reinforcement learning models to address the competitive influence maximization (CIM) problem. However, these models assume complete network topology information is available to address the CIM problem. This assumption is unrealistic as it is difficult to obtain complete social network data and requires exhaustive efforts to obtain it. In this work, we propose a deep reinforcement learning-based (DRL) model to tackle the competitive influence maximization on unknown social networks. Our proposed model has a two-fold objective: the first is to identify the time when to explore the network to collect network information. The second is to determine key influential users from the explored network, using optimal seed-selection strategy considering the competition in the social network. Moreover, we integrate the transfer learning in DRL to improve the training efficiency of DRL models. Experimental results show that our proposed DRL and transfer learning-based DRL models achieve significantly better performance than heuristic-based methods.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {196–203},
numpages = {8},
keywords = {transfer learning, deep reinforcement learning, competitive influence maximization, social network analysis, influence maximization},
location = {Virtual Event, Netherlands},
series = {ASONAM '20}
}

@article{10.1145/3579342.3579346,
author = {Chen, Zaiwei},
title = {A Unified Lyapunov Framework for Finite-Sample Analysis of Reinforcement Learning Algorithms},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/3579342.3579346},
doi = {10.1145/3579342.3579346},
abstract = {Reinforcement learning (RL) is a paradigm where an agent learns to accomplish tasks by interacting with the environment, similar to how humans learn. RL is therefore viewed as a promising approach to achieve artificial intelligence, as evidenced by the remarkable empirical successes. However, many RL algorithms are theoretically not well-understood, especially in the setting where function approximation and off-policy sampling are employed. My thesis [1] aims at developing thorough theoretical understanding to the performance of various RL algorithms through finite-sample analysis.Since most of the RL algorithms are essentially stochastic approximation (SA) algorithms for solving variants of the Bellman equation, the first part of thesis is dedicated to the analysis of general SA involving a contraction operator, and under Markovian noise. We develop a Lyapunov approach where we construct a novel Lyapunov function called the generaled Moreau envelope. The results on SA enable us to establish finite-sample bounds of various RL algorithms in the tabular setting (cf. Part II of the thesis) and when using function approximation (cf. Part III of the thesis), which in turn provide theoretical insights to several important problems in the RL community, such as the efficiency of bootstrapping, the bias-variance trade-off in off-policy learning, and the stability of off-policy control.The main body of this document provides an overview of the contributions of my thesis.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jan},
pages = {12–15},
numpages = {4}
}

@inproceedings{10.1145/3583780.3615043,
author = {Huang, Wenzhen and Li, Tong and Cao, Yuting and Lyu, Zhe and Liang, Yanping and Yu, Li and Jin, Depeng and Zhang, Junge and Li, Yong},
title = {Safe-NORA: Safe Reinforcement Learning-Based Mobile Network Resource Allocation for Diverse User Demands},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615043},
doi = {10.1145/3583780.3615043},
abstract = {As mobile communication technologies advance, mobile networks become increasingly complex, and user requirements become increasingly diverse. To satisfy the diverse demands of users while improving the overall performance of the network system, the limited wireless network resources should be efficiently and dynamically allocated to them based on the magnitude of their demands and their relative location to the base stations. We separated the problem into four constrained subproblems, which we then solved using a safe reinforcement learning method. In addition, we design a reward mechanism to encourage agent cooperation in distributed training environments. We test our methodology in a simulated scenario with thousands of users and hundreds of base stations. According to experimental findings, our method guarantees that over 95\% of user demands are satisfied while also maximizing the overall system throughput.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {885–894},
numpages = {10},
keywords = {multi-agent, mobile networks, safe reinforcement learning, resources allocation},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.5555/3545946.3598961,
author = {Formanek, Claude and Jeewa, Asad and Shock, Jonathan and Pretorius, Arnu},
title = {Off-the-Grid MARL: Datasets and Baselines for Offline Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Being able to harness the power of large, static datasets for developing autonomous multi-agent systems could unlock enormous value for real-world applications. Many important industrial systems are multi-agent in nature and are difficult to model using bespoke simulators. However, in industry, distributed system processes can often be recorded during operation, and large quantities of demonstrative data can be stored. Offline multi-agent reinforcement learning (MARL) provides a promising paradigm for building effective online controllers from static datasets. However, offline MARL is still in its infancy, and, therefore, lacks standardised benchmarks, baselines and evaluation protocols typically found in more mature subfields of RL. This deficiency makes it difficult for the community to sensibly measure progress. In this work, we aim to fill this gap by releasing off-the-grid MARL (OG-MARL): a framework for generating offline MARL datasets and algorithms.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2442–2444},
numpages = {3},
keywords = {reinforcement learning, multi-agent reinforcement learning, offline reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3365265.3365274,
author = {Benatti, Simone and Tasora, Alessandro and Fusai, Dario and Mangoni, Dario},
title = {A Modular Simulation Platform for Training Robots via Deep Reinforcement Learning and Multibody Dynamics},
year = {2019},
isbn = {9781450372886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365265.3365274},
doi = {10.1145/3365265.3365274},
abstract = {In this work we focus on the role of Multibody Simulation in creating Reinforcement Learning virtual environments for robotic manipulation, showing a versatile, efficient and open source toolchain to create directly from CAD models. Using the Chrono::Solidworks plugin we are able to create robotic environments in the 3D CAD software Solidworks® and later convert them into PyChrono models (PyChrono is an open source Python module for multibody simulation). In addition, we demonstrate how collision detection can be made more efficient by introducing a limited number of contact primitives instead of performing collision detection and evaluation on complex 3D meshes, still reaching a policy able to avoid unwanted collisions. We tested this approach on a 6DOF robot Comau Racer3: the robot, together with a 2 fingers gripper (Hand-E by Robotiq) was modelled using Solidworks®, imported as a PyChrono model and then a NN was trained in simulation to control its motor torques to reach a target position. To demonstrate the versatility of this toolchain we also repeated the same procedure to model and then train the ABB IRB 120 robotic arm.},
booktitle = {Proceedings of the 2019 3rd International Conference on Automation, Control and Robots},
pages = {7–11},
numpages = {5},
keywords = {Physical Simulation, Reinforcement Learning, Robotics, Multibody Simulation, Control, Deep Learning, Neural Networks},
location = {Prague, Czech Republic},
series = {ICACR 2019}
}

@inproceedings{10.1145/3576781.3608717,
author = {Gomez, Jorge Torres and Spicher, Nicolai and Rios, Jorge Luis Gonz\'{a}lez and Dressler, Falko},
title = {Fine-Tuned Circuit Representation of Human Vessels through Reinforcement Learning: A Novel Digital Twin Approach for Hemodynamics},
year = {2023},
isbn = {9798400700347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576781.3608717},
doi = {10.1145/3576781.3608717},
abstract = {The modeling of human vessel hemodynamics targets various benefits in health care applications. Subject-specific models of vessels allow the accurate prediction of pressure and flow, thus, enabling the study of individuals' responses to medical treatment. Instead of developing a model for a standard person, this paper features a design for the human circulatory system (HCS) accounting for the specific physiological parameters of an individual. We report using a reinforcement learning (RL) model for customary vessel parameters when training with subject-specific pressure and flow waveforms. We use an equivalent electric-circuit model for the human arteries and adjust the values of resistors, inductors, and capacitors in combination with reinforcement learning (RL). The conceived model stems as a digital twin replicating specific subjects. The reinforcement learning (RL) method predicts the vessel length and radius with an error of less than 10 \% and fits pressure waveforms with a similarity higher than 94 \%.},
booktitle = {Proceedings of the 10th ACM International Conference on Nanoscale Computing and Communication},
pages = {46–52},
numpages = {7},
keywords = {Blood Pressure, Reinforcement Learning, Human Circulatory System, Electric Circuit Simulator},
location = {Coventry, United Kingdom},
series = {NANOCOM '23}
}

@inproceedings{10.1145/3575757.3593658,
author = {Xu, Risheng and K\"{u}hl, Marvin and Von Hasseln, Hermann and Nowotka, Dirk},
title = {Reducing Overall Path Latency in Automotive Logical Execution Time Scheduling via Reinforcement Learning},
year = {2023},
isbn = {9781450399838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575757.3593658},
doi = {10.1145/3575757.3593658},
abstract = {The Logical Execution Time paradigm is a promising approach for achieving time-deterministic communication on multi-core CPUs. Task scheduling under this paradigm is a variant of the Multi-Row Facility Layout Problem, which is known to be NP-hard. In this paper, we propose using reinforcement learning to reduce the overall path latency among all scheduled runnables while adhering to other constraints, such as schedulability, load balance, and data contention control. The neural networks, also known as agents, are trained on a real-world automotive powertrain project. We compare two schedules generated by the agents to the current one and one produced by a genetic algorithm. The agent trained with the Proximal Policy Optimization algorithm demonstrated the best performance. Additionally, we investigate the generalization ability of the agents against software updates, and the results show that our agents are well-generalized.},
booktitle = {Proceedings of the 31st International Conference on Real-Time Networks and Systems},
pages = {212–223},
numpages = {12},
keywords = {Overall Path Latency, LET Scheduling, Automotive, Reinforcement Learning},
location = {Dortmund, Germany},
series = {RTNS '23}
}

@inproceedings{10.5555/1402821.1402865,
author = {Gabel, Thomas and Riedmiller, Martin},
title = {Reinforcement Learning for DEC-MDPs with Changing Action Sets and Partially Ordered Dependencies},
year = {2008},
isbn = {9780981738123},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Decentralized Markov decision processes are frequently used to model cooperative multi-agent systems. In this paper, we identify a subclass of general DEC-MDPs that features regularities in the way agents interact with one another. This class is of high relevance for many real-world applications and features provably reduced complexity (NP-complete) compared to the general problem (NEXP-complete). Since optimally solving larger-sized NP-hard problems is intractable, we keep the learning as much decentralized as possible and use multi-agent reinforcement learning to improve the agents' behavior online. Further, we suggest a restricted message passing scheme that notifies other agents about forthcoming effects on their state transitions and that allows the agents to acquire approximate joint policies of high quality.},
booktitle = {Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3},
pages = {1333–1336},
numpages = {4},
keywords = {communication, interaction, decentralized MDPs},
location = {Estoril, Portugal},
series = {AAMAS '08}
}

@inproceedings{10.1145/3551901.3556474,
author = {Uhlmann, Yannick and Essich, Michael and Bramlage, Lennart and Scheible, J\"{u}rgen and Curio, Crist\'{o}bal},
title = {Deep Reinforcement Learning for Analog Circuit Sizing with an Electrical Design Space and Sparse Rewards},
year = {2022},
isbn = {9781450394864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551901.3556474},
doi = {10.1145/3551901.3556474},
abstract = {There is still a great reliance on human expert knowledge during the analog integrated circuit sizing design phase due to its complexity and scale, with the result that there is a very low level of automation associated with it. Current research shows that reinforcement learning is a promising approach for addressing this issue. Similarly, it has been shown that the convergence of conventional optimization approaches can be improved by transforming the design space from the geometrical domain into the electrical domain. Here, this design space transformation is employed as an alternative action space for deep reinforcement learning agents. The presented approach is based entirely on reinforcement learning, whereby agents are trained in the craft of analog circuit sizing without explicit expert guidance. After training and evaluating agents on circuits of varying complexity, their behavior when confronted with a different technology, is examined, showing the applicability, feasibility as well as transferability of this approach.},
booktitle = {Proceedings of the 2022 ACM/IEEE Workshop on Machine Learning for CAD},
pages = {21–26},
numpages = {6},
keywords = {neural networks, reinforcement learning, analog circuit sizing},
location = {Virtual Event, China},
series = {MLCAD '22}
}

@inproceedings{10.1145/3555776.3577800,
author = {Kim, Minho and Lim, Jeongtaek and Ham, Kyung Sun and Kim, Taehyoung},
title = {Optimal Charging Method for Effective Li-Ion Battery Life Extension Based on Reinforcement Learning},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577800},
doi = {10.1145/3555776.3577800},
abstract = {A reinforcement learning-based optimal charging strategy is proposed for Li-ion batteries to extend the battery life and to ensure the end-user convenience. Unlike most previous studies that do not reflect real-world scenario well, in this work, end users can set the charge time flexibly according to their own situation rather than reducing the charge time as much as possible; this is possible by using soft actor-critic (SAC), which is one of the state-of-the-art reinforcement learning algorithm. In this way, the battery is more likely to extend its life without disturbing the end users. The amount of aging is calculated quantitatively based on an accurate electrochemical battery model, which is directly minimized in the optimization procedure with SAC. SAC can deal with not only the flexible charge time but also varying parameters of the battery model caused by aging once the offline learning is completed, which is not the case for the previous studies; in the previous studies, time-consuming optimization has to be implemented for each battery model with a certain set of parameter values. The validation results show that the proposed method can both extend the battery life effectively and ensure the end-user convenience.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1659–1661},
numpages = {3},
keywords = {reinforcement learning, optimal charging, li-ion battery, aging, soft actor-critic},
location = {Tallinn, Estonia},
series = {SAC '23}
}

