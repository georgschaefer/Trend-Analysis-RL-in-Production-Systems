"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Position-Aware Communication via Self-Attention for Multi-Agent Reinforcement Learning","T. -H. Shih; H. -I. Lin","Graduate Institute of Automation Technology, National Taipei University of Technology, Taipei, Taiwan; Graduate Institute of Automation Technology, National Taipei University of Technology, Taipei, Taiwan","2020 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)","23 Nov 2020","2020","","","1","2","Multi-agent reinforcement learning is important for real-world applications but is still a challenging problem. A feasible way is to share information among all agents via a communication channel. In recent years, attentional communication emerged in order to differentiate valuable information especially with a large number of agents. However, existing attentional communication, which relies on long short-term memory (LSTM) units with attention mechanism, makes parallelization difficult. Another problem is that the output of LSTM is dependent on the sequence of agents, but the relationship between agents, in general, is not sequential. In this paper, we proposed using multi-head self-attention layer, which is proposed from a well-known net “Transformer”, as a communication channel for parallelization. Attention mechanism is independent of the sequence of agents. In addition, we also incorporate position information via positional encoding. In our experiments, the proposed method achieves improvements in terms of reward compared with existing approaches.","2575-8284","978-1-7281-7399-3","10.1109/ICCE-Taiwan49838.2020.9258271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9258271","","Reinforcement learning;Encoding;Communication channels;Games;Computer architecture;Network architecture;Markov processes","learning (artificial intelligence);multi-agent systems;parallel processing;recurrent neural nets;telecommunication channels","position-aware communication;multiagent reinforcement learning;communication channel;attentional communication;long short-term memory;attention mechanism;multihead self-attention layer;LSTM;agent sequence;transformer","","","","10","IEEE","23 Nov 2020","","","IEEE","IEEE Conferences"
"Utilizing unsupervised weightless neural network as autonomous states classifier in reinforcement learning algorithm","Y. Yusof; H. M. A. H. Mansor; A. Ahmad","Industrial Automation Section, Universiti Kuala Lumpur Malaysia France Institute, Bandar Baru Bangi, Selangor, Malaysia; Industrial Automation Section, Universiti Kuala Lumpur Malaysia France Institute, Bandar Baru Bangi, Selangor, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia","2017 IEEE 13th International Colloquium on Signal Processing & its Applications (CSPA)","12 Oct 2017","2017","","","264","269","An implementation of reinforcement learning algorithm in an autonomous system requires knowledge expert to specify anticipated states, actions and rewards; and the algorithm will autonomously discover a near optimal behaviour for the system through trial-and-error interactions with its environment. The information on anticipated states are usually extracted from data streams and pre-programmed based on the knowledge expert interpretation of the data thus making the reinforcement learning algorithm rigid to only handles anticipated circumstances and the system will not be able to optimize. As an alternative, in this paper we explore the use of AUTOWiSARD, an unsupervised weightless neural network which will autonomously classify the states based on sensor information and then used by Q-learning, a reinforcement learning algorithm in order find near optimal behavior. The implementation will be demonstrated in an autonomous mobile robot simulation and the outcome will be presented and discussed.","","978-1-5090-1184-1","10.1109/CSPA.2017.8064963","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8064963","AUTOWiSARD;Q-learning;reinforcement learning;unsupervised weightless neural network","Signal processing algorithms;Robot sensing systems;Classification algorithms;Learning (artificial intelligence);Mobile robots;Neural networks","learning (artificial intelligence);mobile robots;neural nets;neurocontrollers;unsupervised learning","unsupervised weightless neural network;autonomous states classifier;reinforcement learning algorithm;autonomous system;anticipated states;data streams;AUTOWiSARD;Q-learning;near optimal behavior;mobile robots","","","","14","IEEE","12 Oct 2017","","","IEEE","IEEE Conferences"
"Robotic Arm Motion Planning with Autonomous Obstacle Avoidance Based on Deep Reinforcement Learning","S. Yang; Q. Wang","School of Automation, Southeast University, Nanjing; School of Automation, Southeast University, Nanjing","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","3692","3697","The traditional control method of the robotic arm requires the operator to control it through a preset fixed trajectory according to the specific task and combined with the working environment, which requires a high-precision environment model. However, it lacks adaptability and cannot be applied to other working scenarios. This paper proposes an end-to-end robotic arm control method based on DRL(Deep Reinforcement Learning) to overcome the above problems. The strategy control module of the robotic arm uses the PPO(Proximal Policy Optimization) algorithm, so that the robotic arm has the ability to learn independently in a complex working environment and completes the adaptive control. In this paper, the reward shaping method is adopted in the training process of the agent, which accelerates the learning of the agent and makes the algorithm converge faster. The DRL algorithm can converge in a shorter time as shown in experimental results, and it has excellent performance in the motion planning, collision avoidance and overall strategy control of the robotic arm in the simulation environment.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902722","NNSF(grant numbers:61973074,62111530149); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902722","DRL;PPO;reward shaping;robotic arm;motion planning;collision avoidance","Training;Learning systems;Robot kinematics;Reinforcement learning;Manipulators;Planning;Trajectory","adaptive control;collision avoidance;control engineering computing;deep learning (artificial intelligence);mobile robots;motion control;optimisation;path planning;reinforcement learning","adaptive control;autonomous obstacle avoidance;collision avoidance;complex working environment;deep reinforcement learning;DRL;high-precision environment model;PPO;proximal policy optimization;robotic arm control;robotic arm motion planning","","","","18","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Coordinated Voltage Regulation of Microgrid Clusters Based on Deep Reinforcement Learning Approach","X. Xue; H. Ge","School of Electrical and Automation Engineering, Nanjing Normal University, Nanjing, China; College of Automation and College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China","2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)","7 Jul 2023","2023","","","344","349","With the rapid development of microgrid cluster operation, the problem of voltage regulation in the coordinated operation of multiple microgrids faces practical challenges. Aiming at the problem of voltage regulation of multi-microgrids, this paper firstly establishes an optimization model of coordinated voltage regulation of multiple microgrids considering the coordination of source, grid, load and storage. Since the difficulty of solving the above optimization problem, it is further reformulated as a Markov game. Then, a novel collaborative voltage regulation algorithm based on multi-agent deep reinforcement learning (MADRL) is proposed. In order to improve the scalability of the algorithm, an attention mechanism is introduced into the multi-agent deep reinforcement learning algorithm. The simulation results show that the proposed algorithm can coordinate with multiple microgrids to regulate the voltage to a safe range.","2767-9861","979-8-3503-2105-0","10.1109/DDCLS58216.2023.10166110","National Natural Science Foundation of China(grant numbers:52077106); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10166110","Multi-microgrids;Voltage regulation;MADRL;Attention mechanism","Deep learning;Privacy;Scalability;Simulation;Clustering algorithms;Microgrids;Reinforcement learning","deep learning (artificial intelligence);distributed power generation;learning (artificial intelligence);Markov processes;multi-agent systems;optimisation;power engineering computing;power generation control;reinforcement learning;voltage control","coordinated operation;coordinated voltage regulation;deep reinforcement learning approach;microgrid cluster operation;microgrid clusters;multiagent deep reinforcement learning algorithm;multimicrogrids;multiple microgrids;novel collaborative voltage regulation algorithm;optimization problem","","","","20","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"Automated Calibration of Planar Cable-Driven Parallel Manipulators by Reinforcement Learning in Joint-Space","M. M. Aref; J. Mattila","Laboratory of Automation and Hydraulic Engineering, Tampere University of Technology, Tampere, Finland; Laboratory of Automation and Hydraulic Engineering, Tampere University of Technology, Tampere, Finland","2018 6th RSI International Conference on Robotics and Mechatronics (IcRoM)","4 Mar 2019","2018","","","172","177","Benefiting from modularity, cable-driven parallel robots (CDPRs) are capable of being reconfigurable by changes in their attachment points and, therefore, significant changes in their kinematic structures. Due to their wide-range motion, measuring CDPRs' fixed attachment points location can be limiting. This paper tackles the problem of identifying the manipulators' geometry based on their interoceptive sensors by reinforcement learning. We propose using Jacobian matrix elements to map rewards and actions into joint space without the appearance of local minimums and multiple solutions of forward kinematics. Feasibility of this method is demonstrated by a planar redundant CDPR. Without an expensive tracking system, the robot is capable of autocalibration based on the cable length measurements (actuator feedback) and quantization factors of any configuration space, while keeping all the cables under tension force. For instance, if the workspace is discretized with a grid resolution of 1 cm, this algorithm is capable of reducing the initial error of 2.2 m, as low as 1 cm. Further extension of this method toward higher technology readiness levels can improve the possibility of commercializing these manipulators toward plug-and-play setups.","2572-6889","978-1-7281-0127-9","10.1109/ICRoM.2018.8657644","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8657644","","Robots;Mechatronics;Parallel processing;Tracking loops","cables (mechanical);calibration;end effectors;geometry;Jacobian matrices;learning (artificial intelligence);manipulator dynamics;manipulator kinematics;mobile robots;motion control","automated calibration;planar cable-driven parallel manipulators;reinforcement learning;joint-space;cable-driven parallel robots;kinematic structures;wide-range motion;CDPRs' fixed attachment points location;interoceptive sensors;Jacobian matrix elements;forward kinematics;planar redundant CDPR;expensive tracking system;quantization factors;configuration space;manipulators geometry","","","","9","IEEE","4 Mar 2019","","","IEEE","IEEE Conferences"
"Reinforcement Stacked Learning with Semantic-Associated Attention for Visual Question Answering","X. Xiao; C. Zhang; S. Xiang; C. Pan","Tencent, Shanghai, China; School of Computer Science and Technology, Beijing Institute of Technology; Institute of Automation, Chinese Academy of Science; Institute of Automation, Chinese Academy of Science","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","4170","4174","The task of visual question answering (VQA) is to generate an answer for a question according to the content of an image being asked. In this process, the critical problems of effectively embedding the question feature and image feature as well as transforming the features to the prediction of answer are still faithfully unresolved. In this paper, depending on these problems, a semantic-associated attention method and a reinforcement stacked learning mechanism are proposed. Firstly, within the associations of high-level semantics, a visual spatial attention model (VSA) and a multi-semantic attention model (MSA) are proposed to extract the low-level image feature and high-level semantic feature, respectively. Furthermore, we develop a reinforcement stacked learning architecture, which splits the transformation process into multiple stages, to gradually approach the answers. At each stage, a new reinforcement learning (RL) method is introduced to directly criticize inappropriate answers to optimize the model. The extensive experiments on the VQA task show that our method can achieve state-of-the-art performance.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414636","VQA;Deep Learning;LSTM;Attention;Reinforcement Learning","Learning systems;Visualization;Semantics;Reinforcement learning;Signal processing;Feature extraction;Knowledge discovery","feature extraction;learning (artificial intelligence);video signal processing","visual question answering;question feature;semantic-associated attention method;reinforcement stacked learning mechanism;high-level semantics;visual spatial attention model;multisemantic attention model;low-level image feature;high-level semantic feature;transformation process;reinforcement learning method;inappropriate answers;VQA task show","","","","17","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Autonomous navigation for indoor mobile robots based on reinforcement learning","L. Ge; X. Zhou; C. Zhang","Automation School Beijing University of Posts and Telecommunications, Beijing, China; Automation School Beijing University of Posts and Telecommunications, Beijing, China; Neolix Technologies Co., Ltd, Beijing, China","2021 2nd International Conference on Artificial Intelligence and Computer Engineering (ICAICE)","24 Jun 2022","2021","","","246","250","The algorithm of building a map based on SLAM (simultaneous localization and mapping) in indoor environment and using the map for autonomous localization and navigation is mature and has more successful cases applied in practical scenarios, but the cost of building a map is still very expensive, we try to use mapless method to achieve autonomous navigation of robots, according to the human in unknown environment can clearly reach the target location because human in We try to analogize human decision making thinking from the direction of intelligent decision making to guide mobile robots to achieve autonomous navigation, and reinforcement learning is an important algorithm to achieve intelligent decision making. In this paper, We use reinforcement learning methods from the original images acquired by the vision sensors for the robot to learn the optimal decision navigation method from the initial position to the target position. The experiment showed good results","","978-1-6654-2186-7","10.1109/ICAICE54393.2021.00056","Beijing University of Posts and Telecommunications(grant numbers:2021-YC-A298); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797670","reinforcement learning;robot navigition;DQN;Intelligent decision","Training;Simultaneous localization and mapping;Navigation;Decision making;Buildings;Reinforcement learning;Vision sensors","decision making;learning (artificial intelligence);mobile robots;navigation;path planning;robot vision;SLAM (robots)","autonomous navigation;indoor mobile robots;indoor environment;autonomous localization;unknown environment;human decision;intelligent decision making;reinforcement learning methods;optimal decision navigation method","","","","10","IEEE","24 Jun 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning Applied to Agent Path Planning","F. Sun; X. Wu; G. Long","Key Laboratory of Intelligent Analysis and Decision on Complex Systems, School of Science, Chongqing University of Posts and Telecommunications, Chongqing, P.R. China; Key Laboratory of Intelligent Air-Ground Cooperative Control for Universities in Chongqing, College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, P.R. China; Shangbaotian Primary School, Panzhou, Liupanshui, P.R. China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","3639","3644","This paper provides an enhanced Q-learning algorithm for agent path planning in traditional grid maps, which solves the problem of agent path planning in grid maps. In an unknown environment, the classic Q-learning algorithm addresses the agent's path planning problem. However, there are several limits to this strategy in terms of path planning: the agent could only move nearby grids, and the step size is only one frame. Improved Q-learning algorithm, including changes the agent's direction and step size. The action direction of agents is increased from four to eight. The movement step of agents is raised from one to three frames. The new method makes the convergence faster and proxy path smoother. Finally, a set of simulation tests are presented to validate the modified Q-learning algorithm.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9901681","National Natural Science Foundation of China(grant numbers:61673080,61773082); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9901681","Agent;Path planning;Q-learning;Grid method","Q-learning;Simulation;Path planning;Convergence","cartography;collision avoidance;mobile robots;reinforcement learning","agent path planning;classic Q-learning algorithm;collision-free path;enhanced Q-learning algorithm;grid maps;improved Q-learning algorithm;modified Q-learning algorithm;proxy path;reinforcement learning","","","","21","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Uncertainty-Guided Active Reinforcement Learning with Bayesian Neural Networks","X. Wu; M. El-Shamouty; C. Nitsche; M. F. Huber","Cyber Cognitive Intelligence Department, Fraunhofer IPA; Robot and Assistive Systems Department, Fraunhofer IPA; Cyber Cognitive Intelligence Department, Fraunhofer IPA; Cyber Cognitive Intelligence Department, Fraunhofer IPA","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5751","5757","Recent advances in Reinforcement Learning (RL) have made significant contributions in past years by offering intelligent solutions to solve robotic tasks. However, most RL algorithms, especially the model-free RL, are plagued by low learning efficiency and safety problems. In this paper, we propose using the Bayesian Neural Networks (BNNs) to guide the agent exploring actively to enhance the learning efficiency in RL and investigate the potential of recognizing safety risks in working environments with uncertainty information. We compare two types of uncertainty quantification methods in both action and state spaces. To validate our method, we visualize the quantified uncertainty in robot environments with or without safety hazards. Moreover, we evaluate the learning efficiency and safety performance of the RL agents learned with BNNs on different robotic tasks.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160686","","Industries;Visualization;Uncertainty;Service robots;Neural networks;Reinforcement learning;Hazards","control engineering computing;neural nets;reinforcement learning;robots","Bayesian neural networks;BNN;intelligent solutions;low learning efficiency;model-free RL;quantified uncertainty;RL agents;RL algorithms;robot environments;robotic tasks;safety hazards;safety performance;safety problems;safety risks;uncertainty information;uncertainty quantification methods;uncertainty-guided active reinforcement learning","","","","46","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Coordination of a Multi Robot System for Pick and Place Using Reinforcement Learning","X. Lan; Y. Qiao; B. Lee","Software Research Institute Technological University of the Shannon, Athlone, Ireland; Software Research Institute Technological University of the Shannon, Athlone, Ireland; Software Research Institute Technological University of the Shannon, Athlone, Ireland","2022 2nd International Conference on Computers and Automation (CompAuto)","6 Feb 2023","2022","","","87","92","Recent advances in deep reinforcement learning are enabling the creation and use of powerful agent systems in complex areas such as multi-robot coordination. These show great promise to help solve many of the difficult challenges of rapidly growing domains such as smart manufacturing. In this paper we describe our ongoing work on the use of single agent deep reinforcement learning to optimise coordination in a multi-robot pick and place (PnP) system. We describe the implementation of the DQN agent as well as a bespoke multi robot PnP simulator, implemented as an OpenAI Gym environment. We present our initial results and outline future work.","","978-1-6654-8194-6","10.1109/CompAuto55930.2022.00024","SFI(grant numbers:SFI/16/RC/3918); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10027088","simulator;pick and place;deep reinforcement learning;multi-robot system","Deep learning;Computers;Job shop scheduling;Automation;Robot kinematics;Reinforcement learning;Feeds","control engineering computing;deep learning (artificial intelligence);manipulators;multi-agent systems;multi-robot systems;optimisation;reinforcement learning","bespoke multirobot PnP simulator;complex areas;DQN agent;multirobot coordination;multirobot pick and place system;multirobot system;OpenAI Gym environment;powerful agent systems;show great promise;single agent deep reinforcement learning;smart manufacturing","","","","22","IEEE","6 Feb 2023","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Method for Pricing Electric Vehicles With Discrete Charging Levels","D. Qiu; Y. Ye; D. Papadaskalopoulos; G. Strbac","Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.","IEEE Transactions on Industry Applications","18 Sep 2020","2020","56","5","5901","5912","The effective pricing of electric vehicle (EV) charging by aggregators constitutes a key problem toward the realization of the significant EV flexibility potential in deregulated electricity systems and has been addressed by previous work through bi-level optimization formulations. However, the solution approach adopted in previous work cannot capture the discrete nature of the EV charging/discharging levels. Although reinforcement learning (RL) can tackle this challenge, state-of-the-art RL methods require discretization of state and/or action spaces and thus exhibit limitations in terms of solution optimality and computational requirements. This article proposes a novel deep reinforcement learning (DRL) method to solve the examined EV pricing problem, combining deep deterministic policy gradient (DDPG) principles with a prioritized experience replay (PER) strategy and setting up the problem in multi-dimensional continuous state and action spaces. Case studies demonstrate that the proposed method outperforms state-of-the-art RL methods in terms of both solution optimality and computational requirements and comprehensively analyze the economic impacts of smart-charging and vehicle-to-grid (V2G) flexibility on both aggregators and EV owners.","1939-9367","","10.1109/TIA.2020.2984614","E-FLEX; Innovate UK(grant numbers:104249); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9055152","Aggregators;bi-level optimization;deep reinforcement learning;electricity pricing;electric vehicles (EVs)","Pricing;Optimization;Vehicle-to-grid;Reinforcement learning;Electric potential;Low-carbon economy","battery powered vehicles;electric vehicle charging;electricity supply industry;gradient methods;learning (artificial intelligence);neural nets;power engineering computing;power system economics;pricing;vehicle-to-grid","vehicle-to-grid flexibility;pricing electric vehicles;electric vehicle charging;deregulated electricity systems;bi-level optimization formulations;deep reinforcement learning method;policy gradient principles;EV pricing problem;DRL method;deep deterministic policy gradient principle;DDPG principle;prioritized experience replay strategy;PER strategy;V2G flexibility","","47","","40","IEEE","2 Apr 2020","","","IEEE","IEEE Journals"
"Energy Storage Arbitrage in Real-Time Markets via Reinforcement Learning","H. Wang; B. Zhang","Department of Electrical Engineering, University of Washington, Seattle, WA; Department of Electrical Engineering, University of Washington, Seattle, WA","2018 IEEE Power & Energy Society General Meeting (PESGM)","23 Dec 2018","2018","","","1","5","In this paper, we derive a temporal arbitrage policy for storage via reinforcement learning. Real-time price arbitrage is an important source of revenue for storage units, but designing good strategies have proven to be difficult because of the highly uncertain nature of the prices. Instead of current model predictive or dynamic programming approaches, we use reinforcement learning to design an optimal arbitrage policy. This policy is learned through repeated charge and discharge actions performed by the storage unit through updating a value matrix. We design a reward function that does not only reflect the instant profit of charge/discharge decisions but also incorporate the history information. Simulation results demonstrate that our designed reward function leads to significant performance improvement compared with existing algorithms.","1944-9933","978-1-5386-7703-2","10.1109/PESGM.2018.8586321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8586321","","Energy storage;Discharges (electric);Real-time systems;Optimization;Energy states;Electricity supply industry","dynamic programming;energy storage;learning (artificial intelligence);optimisation;power engineering computing;power markets;pricing","energy storage arbitrage;real-time markets;reinforcement learning;temporal arbitrage policy;real-time price arbitrage;storage unit;highly uncertain nature;current model predictive;dynamic programming approaches;optimal arbitrage policy;repeated charge;discharge actions;designed reward function","","33","","19","IEEE","23 Dec 2018","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Framework for Dynamic Resource Allocation: First Results.","D. Vengerov; N. Iakovlev","Sun MicroSystems Laboratories, Inc., Menlo Park, CA, USA; Sun MicroSystems Laboratories, Inc., Menlo Park, CA, USA","Second International Conference on Autonomic Computing (ICAC'05)","6 Sep 2005","2005","","","339","340","This paper addresses the problem of dynamic resource allocation among multiple entities sharing a common set of resources. A solution approach is presented based on combining the reinforcement learning methodology with function approximation architectures. An implementation of this approach in Solaris 10 demonstrated a robust near-optimal performance on a simple problem of transferring CPUs among resource partitions so as to match the stochastically changing workload in each partition, both for large and small CPU migration costs.","","0-7695-2276-9","10.1109/ICAC.2005.4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498087","","Learning;Resource management;Sun;Function approximation;Operating systems;Laboratories;Computer architecture;Robustness;Costs;Computer industry","distributed processing;function approximation;learning (artificial intelligence);resource allocation","reinforcement learning;dynamic resource allocation;resource sharing;function approximation architecture;Solaris 10;near-optimal performance;resource partitions;stochastic workload changing;CPU migration","","18","1","4","IEEE","6 Sep 2005","","","IEEE","IEEE Conferences"
"Exploiting Domain Symmetries in Reinforcement Learning with Continuous State and Action Spaces","A. Agostini; E. Celaya","Institut de Robòtica i Informàtica Industrial (CSIC-UPC), Barcelona, Spain; Institut de Robòtica i Informàtica Industrial (CSIC-UPC), Barcelona, Spain","2009 International Conference on Machine Learning and Applications","15 Jan 2010","2009","","","331","336","A central problem in reinforcement learning is how to deal with large state and action spaces. When the problem domain presents intrinsic symmetries, exploiting them can be key to achieve good performance. We analyze the gains that can be effectively achieved by exploiting different kinds of symmetries, and the effect of combining them, in a test case: the stand-up and stabilization of an inverted pendulum.","","978-0-7695-3926-3","10.1109/ICMLA.2009.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381530","Reinforcement learning;function approximation;domain symmetries","Function approximation;Testing;State-space methods;Machine learning;Aerospace industry;Multiagent systems;State estimation;Acceleration","learning (artificial intelligence);pendulums","domain symmetry;reinforcement learning;continuous state;action spaces;intrinsic symmetry;inverted pendulum stabilization","","6","","15","IEEE","15 Jan 2010","","","IEEE","IEEE Conferences"
"Control of an Inverted Pendulum by Reinforcement Learning Method in PLC Environment","G. Demirkıran; Ö. Erdener; Ö. Akpınar; P. Demirtaş; M. Y. Arık; E. Güler","Electrical-Electronics Engineering Department, Yaşar University, İzmir, Turkey; Atak Elektrik Mühendislik Otomasyon San ve Tic. Ltd. Şti., Manisa, Turkey; Atak Elektrik Mühendislik Otomasyon San ve Tic. Ltd. Şti., Manisa, Turkey; Atak Elektrik Mühendislik Otomasyon San ve Tic. Ltd. Şti., Manisa, Turkey; Atak Elektrik Mühendislik Otomasyon San ve Tic. Ltd. Şti., Manisa, Turkey; Atak Elektrik Mühendislik Otomasyon San ve Tic. Ltd. Şti., Manisa, Turkey","2020 Innovations in Intelligent Systems and Applications Conference (ASYU)","23 Nov 2020","2020","","","1","5","The aim of this study is to implement Q-learning algorithm to move an inverted pendulum from the downright position to upright position in a PLC environment. Instead of using classical control algorithms that need a linear model of the system to be controlled, we used model-free control algorithm, i.e. Q-learning, and relaxed the linearity assumption. We demonstrate that reinforcement learning can be successfully used in industrial machine learning applications to learn complex control policies without having a detailed model of the controlled system. An experimental set up is designed using PLC controlled mechanical parts, and the code is written in PLC. After about three hours of learning stage, the Q learning algorithm successfully moved inverted pendulum from downright position to upright position and keep it in balanced upright position.","","978-1-7281-9136-2","10.1109/ASYU50717.2020.9259890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9259890","reinforcement learning;programmable logic controller;inverted pendulum","Industries;Servomotors;Reinforcement learning;Rails;Torque;Process control;Acceleration","control engineering computing;learning (artificial intelligence);nonlinear control systems;pendulums;programmable controllers","inverted pendulum;reinforcement learning method;PLC environment;Q-learning algorithm;model-free control algorithm;linearity assumption","","1","","12","IEEE","23 Nov 2020","","","IEEE","IEEE Conferences"
"On a Novel Content Edge Caching Approach based on Multi-Agent Federated Reinforcement Learning in Internet of Vehicles","Y. Liu; B. Mao","School of Cybersecurity, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Cybersecurity, Northwestern Polytechnical University, Xi'an, Shaanxi, China","2023 32nd Wireless and Optical Communications Conference (WOCC)","7 Jun 2023","2023","","","1","5","Driven by the emerging requirements of Internet of Vehicles (IoV), future vehicles are expected to have the ability to provide not only the autonomous driving services, but also the multimedia services for working and entertainment. The edge caching service enabled by the Road Side Units (RSUs) can complement the limited environment perceiving and computing ability of future vehicles to gather, pre-process, and cache the contents of driving assistance, work, and entertainments. In this paper, we use federated learning to learn the popularity variation tendency considering user preference in different districts and their concerns for privacy-preserving. We further split the possible contents into blocks and use completely cooperative multi-agent reinforcement learning based on Deep Q network to make a more flexible and accurate caching decision considering the various emergency levels and delay requirements of different contents. Numerical results demonstrate that the proposed method outperforms traditional caching strategies.","2379-1276","979-8-3503-3715-0","10.1109/WOCC58016.2023.10139417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10139417","Edge caching;federated learning;multi-agent reinforcement learning;Deep Q Network","Wireless communication;Road side unit;Entertainment industry;Reinforcement learning;Mathematical models;Optical fiber communication;Delays","cache storage;data privacy;learning (artificial intelligence);multi-agent systems;reinforcement learning;vehicular ad hoc networks","accurate caching decision;autonomous driving services;computing ability;delay requirements;edge caching service;emergency levels;emerging requirements;entertainments;environment perceiving;federated learning;flexible caching decision;future vehicles;multiagent federated reinforcement learning;multimedia services;novel content edge caching approach;popularity variation tendency;possible contents;pre-process;Road Side Units;traditional caching strategies;use completely cooperative multiagent reinforcement learning;working entertainment","","","","17","IEEE","7 Jun 2023","","","IEEE","IEEE Conferences"
"A Multi-Agent Deep Reinforcement Learning Model of Internet-Related Markets Evolution","W. Liu; K. Han","Department of Computer Science, Beijing University of Technology (BJUT), Chaoyang Beijing, China; Department of Computer Science, Beihang University, Beijing, China","2022 2nd Asia-Pacific Conference on Communications Technology and Computer Science (ACCTCS)","18 Jul 2022","2022","","","489","493","Internet economic markets have upgraded the traditional ways of product, service, consumption, and processing transactions and prompted the reforming of traditional multisided markets. With this background, our paper presents a multi-agent deep reinforce learning method with group hybrid abstraction called GHA-DQN to build Internet marketplace model and observe the evolution process including initiation and competition periods from the areas of e-transport, e-commerce, e-catering, and other e-markets. The experiment results provide references to support networking, financing, and competition equilibrium in this competitive e-economic world.","","978-1-6654-0034-3","10.1109/ACCTCS53867.2022.00105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820871","internet-related market;platform evolution;deep reinforcement learning;group hybrid abstraction","Industries;Learning systems;Computer science;Adaptation models;Computational modeling;Government;Reinforcement learning","deep learning (artificial intelligence);electronic commerce;Internet;multi-agent systems;reinforcement learning","Internet marketplace model;e-markets;multiagent deep reinforcement learning;Internet-related markets evolution;Internet economic markets;group hybrid abstraction;GHA-DQN;e-transport;e-commerce;e-catering","","","","20","IEEE","18 Jul 2022","","","IEEE","IEEE Conferences"
"Prediction based segmentation of state space and application to a subgoal finding problem in reinforcement learning","Y. Nagata; Y. Ohigashi; H. Takahashi; S. Ishikawa; T. Omori; K. Morikawa","Graduate School of Information Science, Hokkaido University, Sapporo, Japan; Graduate School of Information Science, Hokkaido University, Sapporo, Japan; Graduate School of Information Science, Hokkaido University, Sapporo, Japan; Graduate School of Information Science, Hokkaido University, Sapporo, Japan; Graduate School of Information Science, Hokkaido University, Sapporo, Japan; Matsushita Elecrric Indusrrial Company Limited, Seika, Kyoto, Japan","SICE 2004 Annual Conference","8 Aug 2005","2004","3","","2560","2565 vol. 3","Humans solve problems by segmenting perceived continuous phenomenon and searching for action in compressed problem space. In this paper, we propose a method for segmenting continuous state space based on local prediction and a long-term prediction of continuous phenomenon. Furthermore, we investigate a subgoal finding problem in reinforcement learning as an instance of application of the segmentation result. A state having a subgoal function is found by propagating value from goal in a compressed state space. Reinforcement learning is accelerated by establishing subrewards in the state.","","4-907764-22-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1491882","","State-space methods;Learning;Humans;Acceleration;Reflection;Information science;Aerospace industry;Extraterrestrial phenomena;Robots;Navigation","learning (artificial intelligence);game theory;state estimation;state-space methods","compressed state space segmentation;subgoal finding problem;reinforcement learning","","","","7","","8 Aug 2005","","","IEEE","IEEE Conferences"
"A study on reinforcement learning mechanisms with common knowledge field for heterogeneous agent systems","T. Kawakami; M. Kinoshita; Y. Kakazu","Department of Industrial Engineering, Hokkaido Institute of Technology, Sapporo, Japan; Department of Complex Systems Engineering, Hokkaido University, Sapporo, Japan; Department of Complex Systems Engineering, Hokkaido University, Sapporo, Japan","IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028)","6 Aug 2002","1999","5","","469","474 vol.5","We propose a new approach to realize a reinforcement learning scheme for heterogeneous multiagent systems. In our approach, we treat the collective agents systems in which there are multiple autonomous mobile robots, and given tasks are achieved based on the collective behavior approach. Also, each agent organizes and refines its knowledge for executing its own behaviors by reinforcement learning mechanisms. Thus, we discuss the reinforcement learning mechanism by which the common knowledge is effectively learned in heterogeneous-agents systems. In our approach, a common knowledge field is generated, and then the leaned rule formed knowledge is embedded in that field. The proposed reinforcement learning mechanism is constructed based on learning classifier systems. An extended model of learning classifier systems is defined to apply the model to heterogeneous-agent systems containing the common knowledge field. We perform computer simulations for multiagent escaping problems to verify our proposed method.","1062-922X","0-7803-5731-0","10.1109/ICSMC.1999.815596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=815596","","Learning;Multiagent systems;Controllability;Computer simulation;Sociology;Electrical equipment industry;Collaborative work;Communication effectiveness;Intelligent robots;Uncertainty","multi-agent systems;learning (artificial intelligence);knowledge representation;mobile robots","reinforcement learning;common knowledge field;heterogeneous agent systems;multiple agent systems;autonomous mobile robots;knowledge representation;learning classifier systems","","","","10","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Price-Quantity-Pair Bidding in Electricity Markets","C. Liu; Q. Yu; J. Zhou; Y. Wang; K. Zhang","Key Laboratory of Measurement and Control of CSE, Ministry of Education, Southeast University, Nanjing, China; Key Laboratory of Measurement and Control of CSE, Ministry of Education, Southeast University, Nanjing, China; China Electric Power Research Institute, Nanjing, China; Key Laboratory of Measurement and Control of CSE, Ministry of Education, Southeast University, Nanjing, China; Key Laboratory of Measurement and Control of CSE, Ministry of Education, Southeast University, Nanjing, China","2022 4th International Conference on Power and Energy Technology (ICPET)","19 Oct 2022","2022","","","888","893","To achieve the efficient operation of the smart grid, appropriate energy trading strategy plays an important role in reducing multi-agent costs in the trading process as well as alleviating grid pressure. However, with the increase of the number of participants in smart grid, energy trading has been greatly challenged in terms of stable and effective operation. In this paper, we use the method of deep reinforcement learning to simulate generation companies (GenCos) price-quantity pair bidding in electricity markets. Through the deep reinforcement learning algorithm, agents can gradually learn the environment by treating the nodal prices of previous time interval and the total load demand of current time interval as the state variables, and the bidding price and quantity as the bidding strategy. The simulation results show that with the continuation and convergence of learning, more agents will profit from trading.","","978-1-6654-8079-6","10.1109/ICPET55165.2022.9918496","State Grid Corporation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918496","electricity market;deep reinforcement learning (DRL);deep deterministic policy gradient (DDPG);price-quantity-pair bidding","Supply and demand;Costs;Simulation;Reinforcement learning;Companies;Electricity supply industry;Smart grids","deep learning (artificial intelligence);multi-agent systems;power engineering computing;power generation economics;power markets;pricing;reinforcement learning;smart power grids","grid pressure;smart grid;generation companies price-quantity pair bidding;electricity markets;deep reinforcement learning algorithm;nodal prices;energy trading strategy;multiagent costs","","","","9","IEEE","19 Oct 2022","","","IEEE","IEEE Conferences"
"Optimal bidding strategy for GENCOs using reinforcement learning process based on the PAB model","M. M. Moghaddam; M. R. Langeroudi; B. Alizadeh","Department of Electrical Engineering Lahijan Branch, Islamic Azad University Lahijan, Lahijan, Iran; Department of Electrical Engineering Lahijan Branch, Islamic Azad University Lahijan, Lahijan, Iran; Department of Electrical Engineering Lahijan Branch, Islamic Azad University Lahijan, Lahijan, Iran","2016 IEEE International Conference on Power and Energy (PECon)","19 Jun 2017","2016","","","101","106","The electricity market with open access enables participants to gain more profit out of the bidding strategy. Every supplier tries to maximize its profit as a player in the market. The decision-making process of suppliers and their mutual performance in the market is a complicated problem, can be studied by modeling single-generator and multi-generator companies. The present paper proposes a model based on the reinforcement learning algorithm, is capable of making decisions for suppliers in the single - generator and multi-generator states on proposing a bidding strategy and simulating market outputs based on mutual actions. Hence, a comparison has carried out to examine the performances of generators in the single-generator and multi-generator states without considering constraints and by considering the effect of network constraints, which can impose considerable limitations on electricity markets. The market clearing mechanism is based on Pay As Bid (PAB) model, can be used to define the optimal bidding strategy for each supplier, find market balance and assess market performance. The proposed model has applied to the Nord Pool market and its effect has indicated.","","978-1-5090-2547-3","10.1109/PECON.2016.7951541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7951541","Bidding Strategy;Nash equilibrium;Pay As Bid (PAB);Reinforcement Learning Process;Pool Market","Nash equilibrium;Learning (artificial intelligence);Electricity supply industry;Generators;Load flow;ISO;Optimization","learning (artificial intelligence);power engineering computing;power generation economics;power markets;tendering","electricity market;optimal bidding strategy;GENCO;reinforcement learning process;PAB model;decision-making process;single-generator companies;multigenerator companies;market clearing mechanism;Pay As Bid model;Nord Pool market","","","","9","IEEE","19 Jun 2017","","","IEEE","IEEE Conferences"
"Reinforcement learning-based annotation for Deep Web data","Yuefeng Lv; Yuchen Fu","School of Computer Science & Technology, Soochow University, Suzhou, Jiangsu, China; School of Computer Science & Technology, Soochow University, Suzhou, Jiangsu, China","2009 Asia-Pacific Conference on Computational Intelligence and Industrial Applications (PACIIA)","5 Feb 2010","2009","1","","345","348","A semantic annotation method for web database query result which is under the condition of uncertain schema information is proposed in this paper by adopting the reinforcement learning method. Using domain ontology to annotate the query result, this paper takes the mapping between domain ontology and query result as a process of finding the best strategy. By training the numeric attribute value, we can find the best strategy to match and then annotate the query result. By collecting web databases from different domains, the experiments indicate that the approach proposed can annotate the web database query result properly and improve the efficiency of annotating.","","978-1-4244-4606-3","10.1109/PACIIA.2009.5406419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406419","Deep Web;semantic annotation;reinforcement learning;schema matching","Databases;Data mining;Ontologies;Application software;Learning;Computational intelligence;Computer industry;Computer science;Software engineering;Research and development","database management systems;Internet;learning (artificial intelligence)","reinforcement learning based annotation;semantic annotation method;Web database query;uncertain schema information;domain ontology;numeric attribute value","","","","10","IEEE","5 Feb 2010","","","IEEE","IEEE Conferences"
"Exploring Reinforcement Learning Method in Bidding Strategy Development for Day-Ahead Electricity Market","H. Chen; R. Bo; R. Das; D. Wunsch","Missouri University of Science and Technology, Rolla, USA; Missouri University of Science and Technology, Rolla, USA; Missouri University of Science and Technology, Rolla, USA; Missouri University of Science and Technology, Rolla, USA","2020 12th IEEE PES Asia-Pacific Power and Energy Engineering Conference (APPEEC)","13 Oct 2020","2020","","","1","5","This paper introduces the detailed process of applying reinforcement learning to solve market participant bidding strategy problem. The process includes the setup of market clearing environment, reinforcement learning structure, and Q-learning algorithm. A comprehensive study on three specially designed problems demonstrates the Q-learning method can achieve significantly higher profit than the baseline method, which employs marginal cost as the offer price. The study provides insights to the learning process and the performance of Q-learning and demonstrates the performance varies with the changing condition of the environment, and tends to degrade with more complex patterns or random disturbances in the environment.","","978-1-7281-5748-1","10.1109/APPEEC48164.2020.9220677","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9220677","bidding strategy;Q-learning;reinforcement learning;electricity market","Q-learning;Costs;Conferences;Electricity supply industry","learning (artificial intelligence);power engineering computing;power markets;pricing;tendering","reinforcement learning method;day-ahead electricity market;market participant bidding strategy problem;market clearing environment;Q-learning algorithm;pricing;bidding strategy development","","","","16","IEEE","13 Oct 2020","","","IEEE","IEEE Conferences"
"Optimizing of support plan for new graduate employment market : Reinforcement learning","K. Mori; S. Kurahashi","Graduate School of Business Sciences, University of Tsukuba, Tokyo, Japan; Graduate School of Business Sciences, University of Tsukuba, Tokyo, Japan","Proceedings of SICE Annual Conference 2010","14 Oct 2010","2010","","","1281","1282","We focused on the problems of the new graduate market in Japan, where the recruitment period starts simultaneously. Therefore the competition among students become fierce, and many students spend a lot of time and efforts for their recruitment activity, but their behaviors are not effective. In order to clarify these problems, we conducted the multi-agented simulation with reinforcement learning. After dividing students into six groups by their ability and aggressiveness, we executed two types of support plans by Actor Critic which is the one of reinforcement learning. Then it was found that the support plans which encourage the students, whose abilities are middle-level and aggressiveness are low-level, are effective to increase final finding employment rate in the recruitment market.","","978-4-907764-36-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5602591","multi-agented simulation;reinforcement learning;employment market;job matching","Employment;Companies;Recruitment;Educational institutions;Learning;Industries","employment;job specification;learning (artificial intelligence);multi-agent systems;psychometric testing;recruitment","support plan;reinforcement learning;graduate employment market;Japan;recruitment period;multiagent simulation","","","","5","","14 Oct 2010","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Strategic Bidding in Electricity Markets","Y. Ye; D. Qiu; M. Sun; D. Papadaskalopoulos; G. Strbac","Electrical and Electronic Engineering, Imperial College London; Electrical and Electronic Engineering, Imperial College London; Electrical and Electronic Engineering, Imperial College London; Electrical and Electronic Engineering, Imperial College London; Electrical and Electronic Engineering, Imperial College London","2020 IEEE Power & Energy Society General Meeting (PESGM)","16 Dec 2020","2020","","","1","1","Bi-level optimization and reinforcement learning (RL) constitute the state-of-the-art frameworks for modeling strategic bidding decisions in deregulated electricity markets. However, the former neglects the market participants' physical non-convex operating characteristics, while conventional RL methods require discretization of state and / or action spaces and thus suffer from the curse of dimensionality. This paper proposes a novel deep reinforcement learning (DRL) based methodology, combining a deep deterministic policy gradient (DDPG) method with a prioritized experience replay (PER) strategy. This approach sets up the problem in multidimensional continuous state and action spaces, enabling market participants to receive accurate feedback regarding the impact of their bidding decisions on the market clearing outcome, and devise more profitable bidding decisions by exploiting the entire action domain, also accounting for the effect of non-convex operating characteristics. Case studies demonstrate that the proposed methodology achieves a significantly higher profit than the alternative state-of-the-art methods, and exhibits a more favorable computational performance than benchmark RL methods due to the employment of the PER strategy.","1944-9933","978-1-7281-5508-1","10.1109/PESGM41954.2020.9281731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9281731","","Employment;Reinforcement learning;Benchmark testing;Electricity supply industry;Optimization","concave programming;deep learning (artificial intelligence);gradient methods;power engineering computing;power markets;profitability","deregulated electricity markets;deep reinforcement learning based methodology;electricity markets;bilevel optimization;nonconvex operating characteristic;DRL based methodology;deep deterministic policy gradient method;DDPG method;prioritized experience replay strategy;PER strategy","","","","0","IEEE","16 Dec 2020","","","IEEE","IEEE Conferences"
"Optimization of Multi-microgrid Operation Based on Blockchain Technology and Multi-agent Reinforcement Learning","Z. Sun; R. Hao; T. Lu; M. Wang","School of Electrical Engineering, Shandong University, Jinan, China; National Power Dispatching and Control Center State Grid Coporation of China, Beijing, China; School of Electrical Engineering, Shandong University, Jinan, China; School of Electrical Engineering, Shandong University, Jinan, China","2023 International Conference on Power Energy Systems and Applications (ICoPESA)","5 Jun 2023","2023","","","549","553","Blockchain technology has attracted attention in the field of electricity market due to its decentralization and fair transaction characteristics. However, the existing research lacks the quantitative analysis of the benefits brought by blockchain, and the solution efficiency is not high. In view of the above problems, the paper first proposes a local energy market (LEM) architecture based on blockchain technology, which guarantees the openness and transparency of transactions, and guarantees the privacy of participants. On this basis, a distribution market operation optimization model with multi-microgrid based on blockchain technology is proposed. Using the multi-agent reinforcement learning algorithm to solve the proposed model can effectively improve the solution efficiency. By transforming the objective function from the maximization of the overall benefit to favor of the interests of one party, it is proved that blockchain can effectively improve the overall income by guaranteeing the transaction fairness.","","979-8-3503-4560-5","10.1109/ICoPESA56898.2023.10140894","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10140894","blockchain;microgrid;distribution network;operation optimization","Adaptation models;Privacy;Statistical analysis;Reinforcement learning;Microgrids;Linear programming;Electricity supply industry","blockchains;distributed power generation;multi-agent systems;power markets;reinforcement learning","blockchain technology;distribution market operation optimization model;electricity market;fair transaction characteristics;local energy market architecture;multiagent reinforcement learning;multimicrogrid operation;solution efficiency","","","","10","IEEE","5 Jun 2023","","","IEEE","IEEE Conferences"
"Training an Under-actuated Gripper for Grasping Shallow Objects Using Reinforcement Learning","W. M. Mohammed; M. Nejman; F. Castaño; J. L. Martinez Lastra; S. Strzelczak; A. Villalonga","FAST-Lab, Faculty of Engineering and Natural Sciences, Tampere University, Tampere, Finland; Department of Automation and Metal Cutting, Warsaw University of Technology, Warsaw, Poland; Centre for Automation and Robotics, Spanish National Research Council, Technical University of Madrid, Arganda del Rey, Spain; FAST-Lab, Faculty of Engineering and Natural Sciences, Tampere University CINTECX, Universidade de Vigo, Vigo, Spain; Lab of Industrial Ecosystemics, Faculty of Production Engineering, Warsaw University of Technology, Warsaw, Poland; Centre for Automation and Robotics, Spanish National Research Council, Technical University of Madrid, Arganda del Rey, Spain","2020 IEEE Conference on Industrial Cyberphysical Systems (ICPS)","4 Dec 2020","2020","1","","493","498","Robot programming and training depends on the task that needs to be completed, the end-effector properties and functionalities and the working space. These considerations can complicate the programming process, which in return, increase the time that is needed for training the robot. Thus, several research approaches have been introduced to address training the robots intuitively. In this regard, this paper presents an approach for training an under-actuated gripper and the robot attached to it for grasping shallow objects. The research work started by detailed analysis of the fingers of human hand during the grasping process. Then, a modified design of the gripper has been produced. This modification includes adding an artificial nail among other hardware-related modifications. Then, a Q-Learning algorithm has been used for training the gripper on grasping the shallow object. With two fingers, three actions were configured, and 625 states were configured for the learning algorithm. For the validation, a coin has been used for representing the shallow object. The results showed reduction in both the grasping time and the number of movements.","","978-1-7281-6389-5","10.1109/ICPS48405.2020.9274727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274727","Reinforcement learning;Robot learning;machine learning;Grasping","Robots;Fingers;Grippers;Grasping;Reinforcement learning;Service robots;Task analysis","actuators;end effectors;grippers;learning (artificial intelligence);mobile robots;robot programming","shallow object;under-actuated gripper;reinforcement learning;robot programming;end-effector properties;grasping process;Q-learning algorithm;robot training","","1","","16","IEEE","4 Dec 2020","","","IEEE","IEEE Conferences"
"Sample efficient transfer in reinforcement learning for high variable cost environments with an inaccurate source reward model","M. F. Alam; M. Shtein; K. Barton; D. J. Hoelzle","Department of Mechanical and Aerospace Engineering, The Ohio State University, Columbus, OH, USA; Department of Materials Science and Engineering, University of Michigan, Ann Arbor, Michigan, USA; Department of Mechanical Engineering, University of Michigan, Ann Arbor, Michigan, USA; Department of Mechanical and Aerospace Engineering, The Ohio State University, Columbus, OH, USA","2022 American Control Conference (ACC)","5 Sep 2022","2022","","","2892","2898","Here we propose an algorithm that combines two classic ideas, transfer learning and temporal abstraction, to accelerate learning in high variable cost environments (HVC-envs). In an HVC-env, each sampling of the environment incurs a high cost, thus methods to accelerate learning are sought to reduce the incurred cost. Transfer learning can be useful for such environments by using prior knowledge from a source environment. As only a small number of samples can be collected from an HVC-env due to high sampling cost, learning becomes challenging when the source environment provides inaccurate rewards. To overcome this challenge we propose a simple but effective way of creating useful temporally extended actions from an inaccurate physics guided model (PGM) that acts as the source task. At first we address this issue theoretically by providing performance bounds between two semi-Markov Decision Processes (SMDPs) with different reward functions. Later we develop two benchmark HVC-envs where learning must happen using a small number of real samples (often on the order of ~ 102 or 103). Finally we show that it is possible to obtain sequential high rewards in both of these environments using ~ 103 real samples by leveraging knowledge from PGMs with inaccurate reward models.","2378-5861","978-1-6654-5196-3","10.23919/ACC53348.2022.9867896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9867896","","Costs;Scalability;Transfer learning;Reinforcement learning;Benchmark testing;Data collection;Data models","costing;decision theory;manufacturing systems;Markov processes;production engineering computing;reinforcement learning","HVC-env;reinforcement learning;high variable cost environments;inaccurate source reward model;transfer learning;semiMarkov decision processes;SMDP;physics guided model;PGM;temporal abstraction;autonomous manufacturing systems","","","","17","","5 Sep 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Temporal Logic Control with Maximum Probabilistic Satisfaction","M. Cai; S. Xiao; B. Li; Z. Li; Z. Kan","Department of Mechanical Engineering, The University of Iowa, Iowa City, IA, USA; Department of Mechanical Engineering, The University of Iowa, Iowa City, IA, USA; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","806","812","This paper presents a model-free reinforcement learning (RL) algorithm to synthesize a control policy that maximizes the satisfaction probability of complex tasks, which are expressed by linear temporal logic (LTL) specifications. Due to the consideration of environment and motion uncertainties, we model the robot motion as a probabilistic labeled Markov decision process (PL-MDP) with unknown transition probabilities and probabilistic labeling functions. The LTL task specification is converted to a limit deterministic generalized Büchi automaton (LDGBA) with several accepting sets to maintain dense rewards during learning. The novelty of applying LDGBA is to construct an embedded LDGBA (E-LDGBA) by designing a synchronous tracking-frontier function, which enables the record of non-visited accepting sets of LDGBA at each round of the repeated visiting pattern, to overcome the difficulties of directly applying conventional LDGBA. With appropriate dependent reward and discount functions, rigorous analysis shows that any method, which optimizes the expected discount return of the RL-based approach, is guaranteed to find the optimal policy to maximize the satisfaction probability of the LTL specifications. A model-free RL-based motion planning strategy is developed to generate the optimal policy in this paper. The effectiveness of the RL-based control synthesis is demonstrated via simulation and experimental results.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561903","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561903","","Robot motion;Uncertainty;Learning automata;Conferences;Reinforcement learning;Markov processes;Probabilistic logic","automata theory;formal specification;Markov processes;motion control;path planning;probability;reinforcement learning;robot dynamics;temporal logic","probabilistic labeled Markov decision process;PL-MDP;unknown transition probabilities;probabilistic labeling functions;LTL task specification;accepting sets;dense rewards;embedded LDGBA;E-LDGBA;synchronous tracking-frontier function;optimal policy;satisfaction probability;RL-based control synthesis;temporal logic control;maximum probabilistic satisfaction;model-free reinforcement learning;control policy;linear temporal logic specifications;motion uncertainties;robot motion;model-free RL-based motion planning;discount return;LDGBA;limit deterministic generalized Buchi automaton","","9","","27","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"A Game-Theoretic Reinforcement Learning Approach for Adaptive Interaction at Intersections","X. Jin; K. Li; Q. -S. Jia; H. Xia; Y. Bai; D. Ren","Dept. of Automation, BNRist, Tsinghua Univ., Beijing, China; Dept. of Automation, BNRist, Tsinghua Univ., Beijing, China; Dept. of Automation, BNRist, Tsinghua Univ., Beijing, China; Meituan-Dianping Group, Beijing, China; Meituan-Dianping Group, Beijing, China; Meituan-Dianping Group, Beijing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","4451","4456","In this paper, we propose a bi-level algorithm for motion planning at intersections based on a scheme of reasoning game theory and heuristic reinforcement learning. In the upper level, a recurrent neural network is introduced to estimate the type of opponent agent. In the lower level, Q-networks are selectively connected to implement the game with different type. Then the ego agent could update its estimation step-by-step and conclude correspond action from historical joint state. The simulation results show that the bi-level controller improves pass times and collision avoidance performance.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327245","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327245","","Logic gates;Games;Reinforcement learning;Planning;Estimation;Cognition;Vehicle dynamics","collision avoidance;driver information systems;game theory;learning (artificial intelligence);metaheuristics;mobile robots;multi-agent systems;recurrent neural nets;road traffic control","autonomous driving;collision avoidance;intersections;opponent agent type;game theoretic reinforcement learning;game theory reasoning;recurrent neural network;heuristic reinforcement learning;motion planning;bilevel algorithm;adaptive interaction;bilevel controller;step-by-step estimation;ego agent;Q-networks","","6","","23","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Collision Avoidance Among Dense Heterogeneous Agents Using Deep Reinforcement Learning","K. Zhu; B. Li; W. Zhe; T. Zhang","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; JD Logistics, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Robotics and Automation Letters","24 Nov 2022","2023","8","1","57","64","Navigating in a complex congested social environment without collision is a crucial and challenging task. Recent studies have demonstrated the considerable success of Deep Reinforcement Learning (DRL) in multi-agent collision avoidance. However, the assumption of these studies that agents are homogeneous circles deviates from reality, leading to performance deterioration in congested scenarios. The current work extends the DRL-based approaches to develop a collision avoidance method for congested scenarios wherein the heterogeneity of agents can no longer be disregarded. Considering shape heterogeneity, we use the Orientated Bounding Capsule (OBC) to model the agents and transform the interactive state space of Robot-Obstacle agent pair. For speed heterogeneity, we design a velocity-related collision risk function to shape the behavior of the robot. Experimental results demonstrate that our proposed method outperforms state-of-the-art DRL-based approaches in terms of success rate and safety. It also exhibits desired collision avoidance behavior.","2377-3766","","10.1109/LRA.2022.3222989","Scientific and Technological Innovation 2030 of China(grant numbers:2021ZD0110900); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954147","Collision avoidance;reinforcement learning;autonomous agents","Collision avoidance;Robots;Navigation;Shape;Reinforcement learning;Trajectory;Deep learning","collision avoidance;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;telecommunication congestion control","collision avoidance behavior;collision avoidance method;complex congested social environment;congested scenarios;considerable success;crucial task;Deep Reinforcement Learning;dense heterogeneous agents;homogeneous circles deviates;interactive state space;multiagent collision avoidance;Orientated Bounding Capsule;performance deterioration;Robot-Obstacle agent pair;shape heterogeneity;speed heterogeneity;state-of-the-art DRL-based approaches;velocity-related collision risk function","","2","","34","IEEE","17 Nov 2022","","","IEEE","IEEE Journals"
"Guidewire feeding method based on deep reinforcement learning for vascular intervention robot","D. Yang; J. Song; Y. Hu","School of Automation, Beijing University of Posts and Telecommunications No. 10, Xitucheng Road, Haidian District, Beijing, China; School of Automation, Beijing University of Posts and Telecommunications No. 10, Xitucheng Road, Haidian District, Beijing, China; School of Automation, Beijing University of Posts and Telecommunications No. 10, Xitucheng Road, Haidian District, Beijing, China","2022 IEEE International Conference on Mechatronics and Automation (ICMA)","22 Aug 2022","2022","","","1287","1293","Vascular interventional surgery has the advantages of small trauma, rapid postoperative recovery, and a high safety factor. It is widely used in the treatment of various cardiovascular diseases. In order to reduce the X-ray radiation exposure to doctors, the existing vascular interventional surgery robot is mainly operated by human teleoperation. Currently, how to achieve autonomously intelligent guidewire feeding has become an attractive issue for researchers. This paper proposes an autonomous wire feeding method in interventional surgery based on the deep reinforcement learning framework SAC (Soft Actor-Critic). We adopt the target detection algorithm YOLOv5s to obtain the guidewire tip, which is used for state update of the SAC agent. The guidewire feeding experiments on the vessel phantom verified that the deep reinforcement learning SAC method could well complete the guidewire feeding task in interventional surgery. Further research will utilize the combination of guidewire and guide catheter to solve more complex surgical tasks. The experiment results show that guidewire end detection accuracy can reach at least 92.9% and the guidewire feeding accuracy is less than 5mm.","2152-744X","978-1-6654-0853-0","10.1109/ICMA54519.2022.9856351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9856351","vascular interventional surgery robot;autonomously guidewire feeding;deep reinforcement learning;guidewire detection","Mechatronics;Wires;Surgery;Phantoms;Reinforcement learning;Object detection;Medical services","blood vessels;catheters;diseases;learning (artificial intelligence);medical robotics;object detection;phantoms;surgery;telerobotics","vascular intervention robot;rapid postoperative recovery;safety factor;cardiovascular diseases;X-ray radiation exposure;existing vascular interventional surgery robot;human teleoperation;autonomously intelligent guidewire feeding;attractive issue;autonomous wire feeding method;deep reinforcement learning framework SAC;soft actor-critic;target detection algorithm YOLOv5;guidewire tip;guidewire end detection accuracy;guidewire feeding accuracy;size 5.0 mm","","2","","22","IEEE","22 Aug 2022","","","IEEE","IEEE Conferences"
"Data-driven virtual reference feedback tuning and reinforcement Q-learning for model-free position control of an aerodynamic system","M. -B. Radac; R. -E. Precup; R. -C. Roman","Department of Automation and Applied Informatics, Politehnica University of Timisoara, Timisoara, Romania; School of Engineering, Edith Cowan University, Australia; Department of Automation and Applied Informatics, Politehnica University of Timisoara, Timisoara, Romania","2016 24th Mediterranean Conference on Control and Automation (MED)","8 Aug 2016","2016","","","1126","1132","This paper compares a linear Virtual Reference Feedback Tuning model-free technique applied to feedback controller tuning based on input-output data with two Reinforcement Q-learning model-free nonlinear state feedback controllers that are tuned using input-state experimental data (ED) in terms of two separate learning techniques. The tuning of the state feedback controllers is done in a model reference setting that aims at linearizing the control system (CS) in a wide operating range. The two learning techniques are validated on a position control case study for an open-loop stable aerodynamic system. The performance comparison of our tuning techniques is discussed in terms of their structural complexity, CS performance, and amount of ED needed for learning.","","978-1-4673-8345-5","10.1109/MED.2016.7535876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7535876","","Data models;Process control;Tuning;Cost function;Aerodynamics;Mathematical model;Adaptation models","aerodynamics;learning (artificial intelligence);linearisation techniques;nonlinear control systems;open loop systems;position control;state feedback","data-driven virtual reference feedback tuning;reinforcement Q-learning;model-free position control;linear virtual reference feedback tuning model-free technique;feedback controller tuning;input-output data;model-free nonlinear state feedback controller;input-state experimental data;model reference setting;control system;open-loop stable aerodynamic system;structural complexity;CS performance","","2","","30","IEEE","8 Aug 2016","","","IEEE","IEEE Conferences"
"Exploiting Transformer in Sparse Reward Reinforcement Learning for Interpretable Temporal Logic Motion Planning","H. Zhang; H. Wang; Z. Kan","Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China","IEEE Robotics and Automation Letters","4 Jul 2023","2023","8","8","4831","4838","Automaton based approaches have enabled robots to perform various complex tasks. However, most existing automaton based algorithms highly rely on the manually customized representation of states for the considered task, limiting its applicability in deep reinforcement learning algorithms. To address this issue, by incorporating Transformer into reinforcement learning, we develop a Double-Transformer-guided Temporal Logic framework (T2TL) that exploits the structural feature of Transformer twice, i.e., first encoding the LTL instruction via the Transformer module for efficient understanding of task instructions during the training and then encoding the context variable via the Transformer again for improved task performance. Particularly, the LTL instruction is specified by co-safe LTL. As a semantics-preserving rewriting operation, LTL progression is exploited to decompose the complex task into learnable sub-goals, which not only converts non-Markovian reward decision processes to Markovian ones, but also improves the sampling efficiency by simultaneous learning of multiple sub-tasks. An environment-agnostic LTL pre-training scheme is further incorporated to facilitate the learning of the Transformer module resulting in an improved representation of LTL. The simulation results demonstrate the effectiveness of the T2TL framework.","2377-3766","","10.1109/LRA.2023.3290511","National Natural Science Foundation of China(grant numbers:62173314,U2013601); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10167731","Linear temporal logic;motion planning;reinforcement learning","Task analysis;Transformers;Robots;Reinforcement learning;Planning;Learning automata;Encoding","deep learning (artificial intelligence);learning (artificial intelligence);Markov processes;path planning;reinforcement learning;rewriting systems;temporal logic","automaton based approaches;co-safe LTL;complex task;considered task;deep reinforcement learning algorithms;Double-Transformer-guided Temporal Logic framework;efficient understanding;environment-agnostic LTL pre-training scheme;existing automaton;improved task performance;interpretable Temporal Logic motion planning;LTL instruction;LTL progression;multiple sub-tasks;nonMarkovian reward decision;semantics-preserving rewriting operation;simultaneous learning;sparse reward reinforcement learning;T2TL framework;task instructions;Transformer module;Transformer twice","","","","31","IEEE","28 Jun 2023","","","IEEE","IEEE Journals"
"Multi-Task Decomposition Architecture based Deep Reinforcement Learning for Obstacle Avoidance","W. Zhang; C. He; T. Wang","Department of Automation, Southeast University, Nanjing, China; Department of Automation, Southeast University, Nanjing, China; Department of Automation, Southeast University, Nanjing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","2735","2740","Obstacle avoidance is a basic skill of mobile robots. Currently, various Deep Reinforcement Learning (DRL) based approaches have been proposed to enable the robot to navigate in complex environments. However, these existing approaches merely employ collision-related reward to guide the learning of deep models, and thus fail to capture good domain knowledge for obstacle avoidance policy. Actually, practical applications also have strict requirements on speed and energy consumption, except for safety. In addition, the learning efficiency of the above DRL-based approaches is low or even unstable. To handle the above challenges, in this paper, we propose a Multi-task Decomposition Architecture (MDA) based Deep Reinforcement Learning for robot moving policy. This method decomposes robot motion control into two related sub-tasks, including speed control as well as orientation control, with obstacle avoidance inserted into each sub-task. Each sub-task is associated with one single reward and is solved using Dueling Double Q-learning (D3QN) algorithm. Q-values from two different sub-tasks are fused through aggregator to derive final Q-values which are used for selecting actions. Experiments indicate this low dimensional representation makes learning more effective, including better security and control over speed and direction. Moreover, robots can be widely used in new environments, even dynamic ones.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327414","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327414","Multi-task Decomposition Architecture;D3QN;Obstacle Avoidance;Speed Control;Orientation Control","Robots;Collision avoidance;Training;Task analysis;Sensors;Reinforcement learning;Neural networks","collision avoidance;control engineering computing;deep learning (artificial intelligence);mobile robots;motion control;path planning;velocity control","speed control;mobile robots;collision-related reward;obstacle avoidance policy;energy consumption;DRL-based approaches;robot moving policy;robot motion control;multitask decomposition architecture;deep reinforcement learning;dueling double Q-learning algorithm;obstacle avoidance","","","","23","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Trajectory Planning for Autonomous Vehicles","Z. Wang; J. Tu; C. Chen","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","7995","8000","The trajectory planning of autonomous vehicles requires making safe sequential decisions instantaneously. The most significant challenge is the uncertainties brought by complex interactions with other road users in diverse driving scenarios. Previous rule-based solutions lack generalization and only suit for limited simple environments. Moreover, another approach generates feasible action sequences based on motion planning, which ignores the interactions between vehicle participants. This paper proposes a hierarchical framework based on reinforcement learning for autonomous driving. Instead of directly mapping the sensor information to the low-level control signal, we first choose a desired state and generate a target trajectory in a Frenet frame according to the desired state. Then a low-level controller is used to track the target trajectory. A reinforcement learning algorithm combining planning and learning is introduced to choose the desired state. This trajectory planning framework is evaluated on a lane switching case in a simulated environment. The simulation results demonstrate the effectiveness of the proposed method.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727737","Autonomous driving;reinforcement learning;Frenet frame","Uncertainty;Target tracking;Trajectory planning;Simulation;Roads;Reinforcement learning;Switches","learning (artificial intelligence);mobile robots;motion control;path planning;road vehicles","autonomous vehicles;safe sequential decisions;complex interactions;road users;diverse driving scenarios;previous rule-based solutions lack generalization;simple environments;feasible action;motion planning;vehicle participants;hierarchical framework;autonomous driving;low-level control signal;desired state;target trajectory;low-level controller;reinforcement learning algorithm;trajectory planning framework","","","","19","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Hierarchical Multi-Agent Deep Reinforcement Learning for Multi-Objective Dispatching in Smart Grid","N. Yang; X. Li; Y. Huang; M. Xiao; Z. Wang; X. Song; L. Li","Beijing Key Laboratory of Research and System Evaluation of Power Dispatching Automation Technology, China Electric Power Research Institute, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Research and System Evaluation of Power Dispatching Automation Technology, China Electric Power Research Institute, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Research and System Evaluation of Power Dispatching Automation Technology, China Electric Power Research Institute, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","4714","4719","Due to the expansion of new energy sources, the complexity and difficulty of power grid dispatching are further increased, especially simultaneously considering security, economic and environmental factors. Existing power grid dispatching methods, such as the bisection method and proportional control, are not competent for the multi-objective complex dispatching. This paper proposes a hierarchical multi-object deep deterministic policy gradient (HMO-DDPG) algorithm to dispatch the smart grid with new energy sources. In this algorithm, the lower Decision Layer uses multi-agent architecture to make decisions for every generator unit, while the upper Optimization Layer evaluates the security, economic and environmental factors of the decisions in the whole power grid. Besides, a double evaluation mechanism deployed in the two layers is proposed to make the decision more reasonable and comprehensive. Moreover, a topology analysis method is used to avoid the islanding problem during smart grid dispatching. Experiments carried out on the power grid simulator provided by China Electric Power Research Institute show HMO-DDPG can enable the power grid to operate safely for over 80% days of a year without exceeding the thermal stability limit of all branches. Moreover, the economic cost is 1.80% less, and the utilization rate of new energy is 70.48% higher than that of the traditional dispatching methods on average.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727965","Electric Power Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727965","intelligent dispatching;HMO-DDPG;smart grid;multi-agent","Costs;Uncertainty;Dispatching;Environmental factors;Stability analysis;Smart grids;Topology","distributed power generation;learning (artificial intelligence);load dispatching;multi-agent systems;power engineering computing;power generation dispatch;power grids;smart power grids","hierarchical multiagent deep reinforcement learning;multiobjective dispatching;energy sources;economic factors;environmental factors;power grid dispatching methods;bisection method;proportional control;multiobjective complex dispatching;hierarchical multiobject deep deterministic policy gradient;lower Decision Layer;multiagent architecture;upper Optimization Layer;double evaluation mechanism;topology analysis method;smart grid dispatching;power grid simulator;China Electric Power Research Institute;HMO-DDPG;economic cost;traditional dispatching methods","","","","11","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Multi-Task Reinforcement Learning With Attention-Based Mixture of Experts","G. Cheng; L. Dong; W. Cai; C. Sun","School of Automation, Southeast University, Nanjing, China; School of Cyber Science and Engineering, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China","IEEE Robotics and Automation Letters","12 May 2023","2023","8","6","3812","3819","Multi-task learning is an important problem in reinforcement learning. Training multiple tasks together brings benefits from the shared useful information across different tasks and often achieves higher performance compared to single-task learning. However, it remains unclear how parameters in the network should be reused across tasks. Instead of naively sharing parameters across all tasks, we propose an attention-based mixture of experts multi-task reinforcement learning approach to learn a compositional policy for each task. The expert networks learn task-specific skills which specialize in different parts of multi-task representation space. To assemble the expert outputs, we propose an attention module to generate connections between tasks and experts to achieve the best performance automatically. The proposed approach can effectively learn task relationships from soft attention weights and alleviate mutual interference through independent expert networks. We use the proposed approach to improve both sample efficiency and overall performance over baseline algorithms in Meta-world, a benchmark for multi-task reinforcement learning containing 50 robotic manipulation tasks, and the multi-task MUJOCO environment, which contains four considerably different tasks with diverse state transition dynamics.","2377-3766","","10.1109/LRA.2023.3271445","National Key R&D Program of China(grant numbers:2021ZD0112700); National Natural Science Foundation of China(grant numbers:61921004,62236002,62173251); Zhishan” Scholars Programs of Southeast University; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10111062","Multi-task reinforcement learning;mixture of experts;attention;Meta-World;robotic manipulation task","Task analysis;Multitasking;Reinforcement learning;Training;Robots;Neural networks;Interference","deep learning (artificial intelligence);learning (artificial intelligence);manipulators;reinforcement learning","50 robotic manipulation tasks;attention-based mixture;considerably different tasks;expert networks learn task-specific skills;experts multitask reinforcement;independent expert networks;multitask learning;multitask MUJOCO environment;multitask reinforcement learning;multitask representation space;single-task learning;task relationships;training multiple tasks","","","","32","IEEE","28 Apr 2023","","","IEEE","IEEE Journals"
"Efficient Spatiotemporal Transformer for Robotic Reinforcement Learning","Y. Yang; D. Xing; B. Xu","Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Robotics and Automation Letters","6 Jul 2022","2022","7","3","7982","7989","Intense spatiotemporal coupling states frequently appear in robotic tasks, and this coupling enriches the information encapsulated in each state. Taking advantage of historical observations can provide more information about the robot, especially for partially observable Markov decision processes. How to deal with this coupling remains a challenging issue in robotic reinforcement learning (RL), and we allege that the imbalanced processing capability of spatiotemporal details is one of the bottlenecks of the vanilla transformer model in learning robotic policies. To address this problem, we novelly propose an efficient spatiotemporal transformer structure. To our knowledge, this work is the first to improve the transformer with spatiotemporal information in RL. In each attention block, we sequentially execute attention computation twice: the first to process the temporal sequence of the input and the latter to manage the spatial state. This input reconstruction enables sufficient information extraction to promote data efficiency. We also add correlation encoding into the query and key computation of multi-head attention, providing the operability of associating states between and within time steps. We evaluate the proposed approach on several robot tasks, and it outperforms state-of-the-art transformer-based online RL.","2377-3766","","10.1109/LRA.2022.3186494","National Natural Science Foundation of China(grant numbers:62073324); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9807399","Spatiotemporal transformer;reinforcement learning","Transformers;Robots;Spatiotemporal phenomena;Task analysis;Correlation;Encoding;Reinforcement learning","decision making;deep learning (artificial intelligence);Markov processes;reinforcement learning;robot programming;robots","multihead attention;correlation encoding;attention computation;spatiotemporal transformer structure;robot tasks;information extraction;spatiotemporal information;robotic policies;vanilla transformer model;partially observable Markov decision processes;robotic reinforcement learning;RL","","","","36","IEEE","27 Jun 2022","","","IEEE","IEEE Journals"
"Optimal control of building energy systems with multiple energy sources using predictive model based control and reinforcement learning","C. Huang; X. Jia; S. Seidel; F. Paschke; J. Braunig","Division Engineering of Adaptive Systems EAS, Fraunhofer Institute of Integrated Circuits IIS, Dresden, Germany; Division Engineering of Adaptive Systems EAS, Fraunhofer Institute of Integrated Circuits IIS, Dresden, Germany; Division Engineering of Adaptive Systems EAS, Fraunhofer Institute of Integrated Circuits IIS, Dresden, Germany; Division Engineering of Adaptive Systems EAS, Fraunhofer Institute of Integrated Circuits IIS, Dresden, Germany; Division Engineering of Adaptive Systems EAS, Fraunhofer Institute of Integrated Circuits IIS, Dresden, Germany","2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA )","30 Nov 2021","2021","","","1","8","In this paper the control of building energy systems with multiple energy sources and storages are analysed. The goal is to efficiently coordinate the energy production and energy distribution from different sources in order to minimize the overall energy consumption. Model predictive control (MPC) and reinforcement learning (RL) based control approaches are proposed and exemplarily applied to an energy system of a residential building with different renewable energy sources. Because of the binary control inputs of the energy system a nonconvex integer optimisation problem arises. In order to solve the problem efficiently we apply a combined optimisation method that is integrated into the model predictive controller. Furthermore, a reinforcement learning based approach is developed and compared to the MPC controller in detail. Both methods are able to decrease energy consumption and keep thermal comfort at the same time.","","978-1-7281-2989-1","10.1109/ETFA45728.2021.9613280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9613280","Building energy system;Multiple energy sources;Reinforcement learning;Model predictive control;Integer optimisation;Machine learning","Energy consumption;Renewable energy sources;Buildings;Optimization methods;Optimal control;Reinforcement learning;Production","building management systems;energy consumption;integer programming;learning (artificial intelligence);optimal control;optimisation;predictive control;renewable energy sources","binary control inputs;energy system;model predictive controller;reinforcement learning based approach;MPC controller;energy consumption;optimal control;multiple energy sources;predictive model based control;energy production;energy distribution;model predictive control;different renewable energy sources","","1","","31","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Research and Implementation of a Real-time Task Dynamic Scheduling Model based on Reinforcement Learning","Y. Ma; Z. Bi; Z. Yin; A. Chai","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","2020 13th International Conference on Intelligent Computation Technology and Automation (ICICTA)","13 Sep 2021","2020","","","717","722","The traditional task scheduling method is insufficient in queuing delay, system load balancing and CPU utilization.It is important to build a real-time task scheduling model for the high efficiency and real-time nature of scheduling.In this paper, a real-time task dynamic scheduling model based on reinforcement learning is proposed for the problem slower of task deadline and low communication in the existing task scheduling method of intelligent production line. The model divides scheduling into two stages: task sorting and task assignment, aiming at minimizing queuing time in the task sorting stage, and using dynamic Reinforcement learning method (DRL) to get the task sequence with the shortest queue time. During the task assignment phase, more accurate scheduling decisions are made by analyzing the load of the processor. The simulation results show that the model adjusts the task queue in real time under the condition of changing the task data, which is obviously better than the earliest deadline priority algorithm in task sorting performance. The load balance of the system is better than the method of random task allocation, which improves the utilization rate of CPU and the quality of service of the system, and effectively reduces the missrate of task deadline in the task scheduling process.","","978-1-7281-8666-5","10.1109/ICICTA51737.2020.00157","LiaoNing Revitalization Talents Program(grant numbers:XLYc 1802056); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9526718","Task sorting;task assignment;load balancing;dynamic scheduling","Analytical models;Processor scheduling;Reinforcement learning;Production;Dynamic scheduling;Real-time systems;Resource management","learning (artificial intelligence);microprocessor chips;processor scheduling;real-time systems;resource allocation;sorting","real-time task dynamic scheduling model;task sorting stage;task sequence;shortest queue time;task assignment phase;task queue;task sorting performance;random task allocation;task scheduling process;queuing time minimization;dynamic reinforcement learning;CPU utilization;system load balancing;queuing delay;processor load;DRL;intelligent production line","","","","16","IEEE","13 Sep 2021","","","IEEE","IEEE Conferences"
"Seeing [u] aids vocal learning: Babbling and imitation of vowels using a 3D vocal tract model, reinforcement learning, and reservoir computing","M. Murakami; B. Kröger; P. Birkholz; J. Triesch","Frankfurt Institute for Advanced Studies (FIAS), Frankfurt, Germany; Dept. of Phoniatrics, RWTH Aachen University, Aachen, Germany; Institute of Acoustics and Speech Communication, Technische Universität Dresden, Dresden, Germany; Frankfurt Institute for Advanced Studies (FIAS), Frankfurt, Germany","2015 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)","7 Dec 2015","2015","","","208","213","We present a model of imitative vocal learning consisting of two stages. First, the infant is exposed to the ambient language and forms auditory knowledge of the speech items to be acquired. Second, the infant attempts to imitate these speech items and thereby learns to control the articulators for speech production. We model these processes using a recurrent neural network and a realistic vocal tract model. We show that vowel production can be successfully learnt by imitation. Moreover, we find that acquisition of [u] is impaired if visual information is discarded during imitation. This might give sighted infants an advantage over blind infants during vocal learning, which is in agreement with experimental evidence.","","978-1-4673-9320-1","10.1109/DEVLRN.2015.7346142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346142","","Speech;Auditory system;Reservoirs;Tongue;Training;Learning (artificial intelligence);Pediatrics","learning (artificial intelligence);paediatrics;recurrent neural nets;speech processing","3D vocal tract model;reinforcement learning;reservoir computing;vocal learning;vowel imitation;vowel babbling;imitative vocal learning;ambient language;auditory knowledge;speech items;articulator control;speech production;recurrent neural network;vowel production;visual information;blind infants;sighted infants","","11","","36","IEEE","7 Dec 2015","","","IEEE","IEEE Conferences"
"Vision-Based Landing of a Simulated Unmanned Aerial Vehicle with Fast Reinforcement Learning","M. Shaker; M. N. R. Smith; S. Yue; T. Duckett","Lincoln University, Lincoln, UK; Lincoln University, Lincoln, UK; Lincoln University, Lincoln, UK; Lincoln University, Lincoln, UK","2010 International Conference on Emerging Security Technologies","14 Oct 2010","2010","","","183","188","Landing is one of the difficult challenges for an unmanned aerial vehicle (UAV). In this paper, we propose a vision-based landing approach for an autonomous UAV using reinforcement learning (RL). The autonomous UAV learns the landing skill from scratch by interacting with the environment. The reinforcement learning algorithm explored and extended in this study is Least-Squares Policy Iteration (LSPI) to gain a fast learning process and a smooth landing trajectory. The proposed approach has been tested with a simulated quadro copter in an extended version of the USAR Sim (Unified System for Automation and Robot Simulation) environment. Results showed that LSPI learned the landing skill very quickly, requiring less than 142 trials.","","978-1-4244-7845-3","10.1109/EST.2010.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5600268","Reinforcement learning;LSPI;Visual servoing","Unmanned aerial vehicles;Robots;Approximation methods;Learning;Atmospheric modeling;Airplanes;Adaptation model","aerospace robotics;iterative methods;learning (artificial intelligence);least squares approximations;mobile robots;remotely operated vehicles;robot vision;visual servoing","vision-based landing;simulated unmanned aerial vehicle;fast reinforcement learning;least-squares policy iteration;USAR Sim environment;unified system for automation and robot simulation;quadrocopter","","10","","21","IEEE","14 Oct 2010","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Energy Management Algorithm for Energy Trading and Contingency Reserve Application in a Microgrid","C. Hau; K. K. Radhakrishnan; J. Siu; S. K. Panda","Electrical & Computer Engineering Department, National University of Singapore, Singapore; SinBerBEST Berkeley Education Alliance for Research in Singapore, Singapore; Electrical & Computer Engineering Department, National University of Singapore, Singapore; Electrical & Computer Engineering Department, National University of Singapore, Singapore","2020 IEEE PES Innovative Smart Grid Technologies Europe (ISGT-Europe)","10 Nov 2020","2020","","","1005","1009","This paper presents automation of energy trading using a value-based reinforcement learning and Deep-Q-Network based energy management algorithm for microgrid participants having energy storage systems, while maintaining required contingency reserves for the microgrid. A piecewise utility function is designed to form the trading strategies in response to the dynamic environment. By adjusting parameters of the utility function, two different behaviors of the microgrid operator, namely risk-seeking and risk-averse, are studied to analyze the impact on the energy storage’s operation decision. The simulation results show that the proposed algorithm manages the storage optimally and outperforms rule-based algorithm, providing higher monetary benefits and better flexibility during an extreme scenario having highly dynamic pricing.","","978-1-7281-7100-5","10.1109/ISGT-Europe47291.2020.9248752","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9248752","Microgrid;energy management;dynamic pricing;contingency reserve;model-free reinforcement learning","Heuristic algorithms;Simulation;Microgrids;Reinforcement learning;Stability analysis;Smart grids;Energy management","distributed power generation;energy management systems;energy storage;knowledge based systems;learning (artificial intelligence);neural nets;optimisation;power engineering computing;power markets;pricing;utility theory","value-based reinforcement learning;energy storage systems;contingency reserves;piecewise utility function;microgrid operator;rule-based algorithm;energy management algorithm;Deep-Q-Network;energy trading automation;monetary benefits;dynamic pricing","","5","","10","IEEE","10 Nov 2020","","","IEEE","IEEE Conferences"
"Speed Regulation of Overhead Catenary System Inspection Robot for High-Speed Railway through Reinforcement Learning","S. Li; C. Xu; L. Chen; Z. Liu","College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China","2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)","6 Dec 2018","2018","","","1378","1383","High-speed railway has developed rapidly in recent years. The overhead catenary system (OCS) that transmits electrical energy to the train has to be regularly inspected due to catenary-related defects might directly threaten the safe operation of high-speed railway. In this paper, we present a novel method for autonomous speed regulation of OCS inspection robot using reinforcement learning. To train the robot, we first build a simulation platform based on Unity3D for accurate reconstruction of the railway environment including the OCS, the rail track and the inspection robot. Next, we utilize the range data recorded by the LiDAR mounted on the robot to detect OCS components. Then we leverage the detection results to train the robot to learn speed regulation for shortening the inspection time through reinforcement learning. Experimental results show the validity of the proposed method, which can greatly improve inspection efficiency. Our work has immediate practical significance for high-speed railway automation and informatization.","","978-1-5386-9380-3","10.1109/SmartWorld.2018.00239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8560217","catenary inspection, LiDAR, robot, simulation, reinforcement learning","","inspection;learning (artificial intelligence);optical radar;railway communication;railway engineering;railways;service robots","reinforcement learning;catenary-related defects;autonomous speed regulation;OCS inspection robot;railway environment;inspection time;inspection efficiency;high-speed railway automation;informatization;overhead catenary system inspection robot;electrical energy;LiDAR;OCS components","","3","","16","IEEE","6 Dec 2018","","","IEEE","IEEE Conferences"
"DDoS and Flash Event Detection in Higher Bandwidth SDN-IoT using Multiagent Reinforcement Learning","D. K. Dake; J. D. Gadze; G. S. Klogo","Department of Computer Engineering, Kwame Nkrumah University of Science and Technology, Kumasi, Ghana; Department of Telecommunications Engineering, Kwame Nkrumah University of Science and Technology, Kumasi, Ghana; Department of Computer Engineering, Kwame Nkrumah University of Science and Technology, Kumasi, Ghana","2021 International Conference on Computing, Computational Modelling and Applications (ICCMA)","12 Oct 2021","2021","","","16","20","The emergence of 5G, IoT, Big Data, and related technologies have necessitated a shift to SDN architectural design and DRL algorithms for network task automation. Without prompt intelligent detection, the volumetric UDP flooding attack from zombies in an SDN-IoT network tends to consume network resources and mix with flash crowd events from legitimate hosts. This paper proposes a multiagent reinforcement learning framework in SDN-IoT to detect and mitigate DDoS attacks and route flash crowd events in the network effectively without compromising benign traffic. We simulated a 200 nodes topology with higher bandwidth and transmission rate in Mininet and implemented a multiagent deep deterministic policy gradient (MADDPG) algorithm for the framework. From the simulation results, the proposed approach outperforms Deep Deterministic Policy Gradient (DDPG) algorithm for the following network metrics: delay; jitter; packet loss; intrusion detection; and bandwidth utilization of network flows","","978-1-6654-2567-4","10.1109/ICCMA53594.2021.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9565169","MARL;SDN;IoT;MADDPG;DDPG;DDoS;Traffic Engineering;Deep Reinforcement Learning","Network topology;Simulation;Packet loss;Intrusion detection;Reinforcement learning;Bandwidth;Jitter","computer network security;learning (artificial intelligence);multi-agent systems;software defined networking;telecommunication computing;telecommunication network topology;telecommunication traffic","multiagent deep deterministic policy gradient algorithm;network metrics;intrusion detection;bandwidth utilization;higher bandwidth SDN-IoT;network task automation;prompt intelligent detection;volumetric UDP flooding attack;SDN-IoT network;network resources;legitimate hosts;multiagent reinforcement learning framework;DDoS attacks;route flash crowd events;transmission rate;flash event detection;5G mobile communication;Big Data;flash crowd events;nodes topology;Mininet;deep deterministic policy gradient algorithm;DDPG algorithm;packet loss","","2","","23","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Shared Control: A Novel Teleoperation Strategy for Robotic","Z. Chang; H. Cheng; H. Ge; F. Duan","Department of Artificial Intelligence, Nankai University, Tianjin, China; Department of Artificial Intelligence, Nankai University, Tianjin, China; Interfaculty Initiative in Information Studies, The University of Tokyo, Tokyo, Japan; NA","2022 International Conference on Advanced Robotics and Mechatronics (ICARM)","29 Nov 2022","2022","","","527","532","Teleoperation is widely used in industrial automation, military, space exploration, and surgery. However, traditional control of mechanical arms teleoperation has some defects, such as the lack of present sense for operators, the excessive dependence on the experience, the low precision of operators, and the heavy workload. These problems make it challenging to teleoperate mechanical arms to complete tasks efficiently and accurately. Current Reinforcement Learning’s autonomous decision-making can accomplish tasks, but it cannot guarantee successful completion in all cases. We propose a novel teleoperation strategy in which both operator and welltrained agent play a big role. The operator teleoperates the robotic arm with a VR controller as dominant control. The agent trained by deep deterministic policy gradient (DDPG) provides assistance control to fix the motion trajectories. Through the proposed shared control strategy, the agent can correct the operator’s operation errors in real-time and improve the accuracy and efficiency of operational tasks. To validate the proposed teleoperation strategy, we built a simulation environment using the Pybullet Module. We designed two pick-and-push tasks. The goal is to pick up the object and place it at the target location. We first trained an intelligent agent using the DDPG algorithm. Then we mix the actions with human manipulations. Ten rounds of experiments were conducted, and time comparisons were made. The final experimental results show that our proposed method reduces the time to complete the task by 13% on average. The well-trained RL agent can effectively make corrections for trajectories, proving the effectiveness of the proposed method.","","978-1-6654-8306-3","10.1109/ICARM54641.2022.9959506","National Natural Science Foundation of China; Tianjin Research Innovation Project for Postgraduate Students; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9959506","","Mechatronics;Target recognition;Surgery;Reinforcement learning;Manipulators;Real-time systems;Trajectory","learning (artificial intelligence);manipulators;medical robotics;mobile robots;multi-robot systems;surgery;telerobotics","-push tasks;assistance control;complete tasks;control strategy;Current Reinforcement Learning's autonomous decision-making;deep deterministic policy gradient;dominant control;excessive dependence;heavy workload;industrial automation;intelligent agent;mechanical arms teleoperation;novel teleoperation strategy;operation errors;operational tasks;RL agent;robotic arm;shared control;space exploration;successful completion;traditional control;VR controller;welltrained agent","","1","","22","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Approach for Improving Age of Information in Mission-Critical IoT","H. Farag; M. Gidlund; Č. Stefanović","Department of Electronic Systems, Aalborg University, Copenhagen, Denmark; Department of Information Systems and Technology, Mid Sweden University, Sundsvall, Sweden; Department of Electronic Systems, Aalborg University, Copenhagen, Denmark","2021 IEEE Global Conference on Artificial Intelligence and Internet of Things (GCAIoT)","31 Jan 2022","2021","","","14","18","The emerging mission-critical Internet of Things (IoT) play a vital role in remote healthcare, haptic interaction, and industrial automation, where timely delivery of status updates is crucial. The Age of Information (AoI) is an effective metric to capture and evaluate information freshness at the destination. A system design based solely on the optimization of the average AoI might not be adequate to capture the requirements of mission-critical applications, since averaging eliminates the effects of extreme events. In this paper, we introduce a Deep Reinforcement Learning (DRL)-based algorithm to improve AoI in mission-critical IoT applications. The objective is to minimize an AoI-based metric consisting of the weighted sum of the average AoI and the probability of exceeding an AoI threshold. We utilize the actor-critic method to train the algorithm to achieve optimized scheduling policy to solve the formulated problem. The performance of our proposed method is evaluated in a simulated setup and the results show a significant improvement in terms of the average AoI and the AoI violation probability compared to the related-work.","","978-1-6654-3841-4","10.1109/GCAIoT53516.2021.9692982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9692982","IoT;deep reinforcement learning;neural networks;age of information;mission-critical communication","Measurement;Job shop scheduling;Mission critical systems;Neural networks;Reinforcement learning;Medical services;Information age","deep learning (artificial intelligence);Internet of Things;minimisation;probability;reinforcement learning;scheduling","actor-critic method;Age-of-Information;AoI threshold;AoI violation probability;AoI-based metric consisting minimisation;average AoI;deep reinforcement learning approach;DRL-based algorithm;emerging mission-critical Internet of Things;haptic interaction;industrial automation;information freshness;metric consisting;mission-critical IoT applications;remote healthcare;status updates;system design","","1","","19","IEEE","31 Jan 2022","","","IEEE","IEEE Conferences"
"A path planning strategy for marine vehicles based on deep reinforcement learning and data-driven dynamic flow fields prediction","Q. Sang; Y. Tian; Q. Jin; J. Yu","University of Chinese Academy of Sciences, Beijing, Shenyang, China; Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences, Shenyang, China; University of Chinese Academy of Sciences, Beijing, Shenyang, China; Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences, Shenyang, China","2021 6th International Conference on Automation, Control and Robotics Engineering (CACRE)","10 Aug 2021","2021","","","466","471","This paper presents a strategy for planning a path of a marine vehicle in dynamic flow fields. This strategy composes of two modules: deep reinforcement learning based path planning and dynamic mode decomposition (DMD) based flow fields prediction. The path planning module employs the deep reinforcement learning algorithm of proximal policy optimization (PPO) to implement the time-optimal path planning of a marine vehicle in predicted spatially-temporally dynamic flow fields, where the long short-term memory (LSTM) is introduced to address the partially observable issue. The objective of the flow prediction module is to provide the path planning module with predicted dynamic flow fields. In the flow prediction module, the data-driven method of DMD is used to learn the low-dimensional model of flow dynamics and make future predictions. And a network of marine vehicles with flow sensing capability are adopted to generate data of flow fields for the on-line DMD learning and prediction, where their flow sensing locations are optimized by the deep reinforcement learning algorithm of deep-Q learning with the aim at minimizing the reconstruction error of the flow field with the sparse in-situ point flow observations by the swarm of marine vehicles. The strategy is implemented in computer simulations, where the flow data outputted by a numerical ocean model is utilized to test the strategy. The simulation results demonstrate the performance of the proposed strategy.","","978-1-6654-3576-5","10.1109/CACRE52464.2021.9501367","National Key Research and Development Program of China(grant numbers:2016YFC0301201,2016YFC0300801); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9501367","marine vehicle;path planning;deep reinforcement learning;dynamic mode decomposition;sensing optimization","Heuristic algorithms;Oceans;Reinforcement learning;Robot sensing systems;Prediction algorithms;Path planning;Sensors","learning (artificial intelligence);marine vehicles;mobile robots;optimisation;path planning","flow sensing capability;flow field;flow sensing locations;deep reinforcement learning algorithm;deep-Q learning;in-situ point flow observations;marine vehicle;flow data;path planning strategy;deep reinforcement learning based path;path planning module;time-optimal path planning;predicted spatially-temporally dynamic flow fields;flow prediction module;predicted dynamic flow fields;flow dynamics;future predictions","","1","","22","IEEE","10 Aug 2021","","","IEEE","IEEE Conferences"
"Poster: Performance Testing Driven by Reinforcement Learning","M. H. Moghadam; M. Saadatmand; M. Borg; M. Bohlin; B. Lisper","RISE Research Institutes of Sweden, Sweden; RISE Research Institutes of Sweden, Sweden; RISE Research Institutes of Sweden, Sweden; RISE Research Institutes of Sweden, Sweden; Mälardalen University, Västerås, Sweden","2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)","5 Aug 2020","2020","","","402","405","Performance testing remains a challenge, particularly for complex systems. Different application-, platform- and workload-based factors can influence the performance of software under test. Common approaches for generating platform- and workload-based test conditions are often based on system model or source code analysis, real usage modeling and use-case based design techniques. Nonetheless, creating a detailed performance model is often difficult, and also those artifacts might not be always available during the testing. On the other hand, test automation solutions such as automated test case generation can enable effort and cost reduction with the potential to improve the intended test criteria coverage. Furthermore, if the optimal way (policy) to generate test cases can be learnt by testing system, then the learnt policy can be reused in further testing situations such as testing variants, evolved versions of software, and different testing scenarios. This capability can lead to additional cost and computation time saving in the testing process. In this research, we present an autonomous performance testing framework which uses a model-free reinforcement learning augmented by fuzzy logic and self-adaptive strategies. It is able to learn the optimal policy to generate platform- and workload-based test conditions which result in meeting the intended testing objective without access to system model and source code. The use of fuzzy logic and self-adaptive strategy helps to tackle the issue of uncertainty and improve the accuracy and adaptivity of the proposed learning. Our evaluation experiments show that the proposed autonomous performance testing framework is able to generate the test conditions efficiently and in a way adaptive to varying testing situations.","2159-4848","978-1-7281-5778-8","10.1109/ICST46399.2020.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9159096","performance testing;stress testing;load testing;machine learning;reinforcement learning","Unified modeling language;Stress;Time factors;Sensitivity;Error analysis;Adaptation models","learning (artificial intelligence);program testing;source code (software)","complex systems;workload-based factors;workload-based test conditions;system model;usage modeling;use-case based design techniques;test automation solutions;automated test case generation;intended test criteria coverage;testing system;testing situations;testing variants;testing process;autonomous performance testing framework;model-free reinforcement learning;intended testing objective;source code","","1","","19","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"Development Of Deep Reinforcement Learning Multi-Agent Framework Design Using Self-Organizing Map","G. E. Setyawan; I. Cholissodin","Faculty of Computer Science, Universitas Brawijaya, Malang, Indonesia; Faculty of Computer Science, Universitas Brawijaya, Malang, Indonesia","2019 International Conference on Sustainable Information Engineering and Technology (SIET)","10 Feb 2020","2019","","","246","250","The developmental steps and paradigm changes in the use of automation technology using deep reinforcement learning (RL) are very rapid because they are also widely accompanied by the development of deep learning combination, which combines RL algorithms. One of the combinations is Q- learning algorithm with one of the deep learning algorithms family of artificial neural networks (ANN) and part of the artificial intelligence science. The combination also becomes a challenge for many researchers because so far it is very difficult to find the right combination in accordance with the case resolved although there are also those that combine with non- ANN. In addition, most RLs only use a single combination, which means that they have not found the ideal combination, whether it should be a single one of the algorithms of ANN or some of it. This study proposes a framework design using the Self-Organizing Map (SOM) algorithm that adaptively combines and plays as the actor to calculate the final Q-value value that is updated from a single or multiple Q-value values in a sustainable and dynamic manner. The result of the formed framework indicates that SOM is able to provide an adaptive combination for the algorithms that should be used in deep RL.","","978-1-7281-3880-0","10.1109/SIET48054.2019.8986121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8986121","framework design;deep reinforcement learning;q-learning;multi-agent;artificial neural network;self-organizing map","","learning (artificial intelligence);multi-agent systems;self-organising feature maps","deep reinforcement learning multiagent framework design;developmental steps;automation technology;deep learning combination;RL algorithms;Q- learning algorithm;deep learning algorithms family;artificial intelligence science;non ANN;single combination;ideal combination;self-organizing map algorithm;final Q-value value;single Q-value values;multiple Q-value values;adaptive combination;deep RL;artificial neural networks;SOM algorithm","","","","11","IEEE","10 Feb 2020","","","IEEE","IEEE Conferences"
"A Novel Adaptive Actor-Critic Reinforcement Learning Controller for Constrained Robots","L. Pantoja-Garcia; R. Garcia-Rodriguez; V. Parra-Vega","Robotics and Advanced Manufacturing Department, Research Center for Advanced Studies (CINVESTAV), Saltillo, Coah, Mexico; Aeronautical Engineering Program and Postgraduate Program in Aerospacial Engineering, Universidad Politecnica Metropolitana de Hidalgo, Tolcayuca, Mexico; Robotics and Advanced Manufacturing Department, Research Center for Advanced Studies (CINVESTAV), Saltillo, Coah, Mexico","2022 IEEE International Conference on Automation/XXV Congress of the Chilean Association of Automatic Control (ICA-ACCA)","10 Jan 2023","2022","","","1","6","Force-position control for robot manipulators has become a basic control regime for advanced applications such as interaction or cooperative tasks that imply contact to environment or external object. Thus, for many applications environmental information is evaluated continuously to generate adequate control actions to complete the task. There has been proposed techniques such as adaptive or intelligent control to cover some of these requirements, however it seems insufficient given the uncertainty involved when dealing with the environment. The emergence of deep learning has recently renewed interest in these tasks from the perspective of reinforcement learning algorithms. In this paper, an actor-critic scheme is proposed for simultaneous control of contact force and joint positions while moving along an environmental rigid surface. The critic stage evaluates information of the environment through an extended error manifold, trying to minimize the cumulative future rewards by a neural network. The actor stage, which receives environment information from the critic stage, is used to approximate inverse robot dynamics. Together with two continuous orthogonal sliding PID controllers gives rise to simultaneous tracking of position and force, with stability analysis. Numerical simulations show the performance of the proposed approach under several conditions.","","978-1-6654-9408-3","10.1109/ICA-ACCA56767.2022.10005997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10005997","Actor-Critic Learning;Neural Network;Performance Evaluator;Robot Manipulator;Sliding Mode.","Upper bound;Uncertainty;Force;Reinforcement learning;Robot sensing systems;Stability analysis;Trajectory","deep learning (artificial intelligence);force control;intelligent control;learning systems;manipulator dynamics;motion control;neurocontrollers;position control;stability;three-term control;tracking","actor stage;actor-critic scheme;adaptive control;adequate control actions;applications environmental information;approximate inverse robot dynamics;basic control regime;constrained robots;contact force;continuous orthogonal sliding PID controllers;critic stage;cumulative future rewards;deep learning;environment information;environmental rigid surface;extended error manifold;external object;force-position control;intelligent control;joint positions;neural network;novel adaptive actor-critic reinforcement learning controller;reinforcement learning algorithms;robot manipulators;stability analysis","","","","20","IEEE","10 Jan 2023","","","IEEE","IEEE Conferences"
"Multi-agent Co-evolutionary Scheduling Approach Based on Genetic Reinforcement Learning","W. Yingzi; J. Xinli; H. Pingbo; G. Kanfeng","Shenyang Ligong University, Shenyang, China; Shenyang Ligong University, Shenyang, China; Shenyang Ligong University, Shenyang, China; Shenyang Institute of Automation, Chinese Academy of Science, Shenyang, China","2009 Fifth International Conference on Natural Computation","28 Dec 2009","2009","5","","573","577","The paper presents an adaptive iterative distributed scheduling algorithm that operates dynamically to schedule the job in the dynamic job-shop. The manufacturing system is scheduled by the multi-agent system where every machine and job is associated with its own software agent. Each agent learns how to select presumably good schedules, by this way the size of the search space can be reduced. In order to get adaptive behavior, genetic algorithm is incorporated to drive parallel search and the evolution direction. Meanwhile, the reinforcement learning system is done with the phased Q-learning by defining the intermediate state pattern. The paper suggests a cooperation technique for the agents, as well. We also analyze the time and the solution and present some experimental results.","2157-9563","978-0-7695-3736-8","10.1109/ICNC.2009.475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366784","","Job shop scheduling;Optimal scheduling;Processor scheduling;Dynamic scheduling;Scheduling algorithm;Dispatching;Genetic algorithms;Machine learning;Biological cells;Machine learning algorithms","genetic algorithms;job shop scheduling;learning (artificial intelligence);multi-agent systems","multi-agent co-evolutionary scheduling;genetic reinforcement learning;distributed scheduling algorithm;dynamic job-shop;multi-agent system;genetic algorithm;Q-learning","","","","9","IEEE","28 Dec 2009","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Optimal Control of Linear Singularly Perturbed Systems","J. Zhao; C. Yang; W. Gao","School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; Engineering Research Center of Intelligent Control for Underground Space, Ministry of Education, and the School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; Department of Mechanical and Civil Engineering, Florida Institute of Technology, Melbourne, FL, USA","IEEE Transactions on Circuits and Systems II: Express Briefs","16 Mar 2022","2022","69","3","1362","1366","This brief studies the optimal control problem of linear singularly perturbed systems (SPSs) via reinforcement learning (RL). We first present an offline model-based algorithm on the basis of Kleinman algorithm to solve the underlying algebraic Riccati equation with singular perturbation parameter and its convergence is proven. This revised version of Kleinman algorithm is the key to develop the subsequent learning algorithm. Then, by means of two time-scale characteristics of SPSs, we present a novel model-free learning algorithm relating to the online measurement of state and input to design the optimal controller. Compared with the existing learning approaches, the obtained RL algorithm is free of ill-conditioned numerical issues caused by the co-existence of fast and slow modes in SPSs and thus is more robust with respect to the computation and measurement errors. Finally, we verify the effectiveness of the developed results by a simulation example.","1558-3791","","10.1109/TCSII.2021.3105652","National Natural Science Foundation of China(grant numbers:61873272,62073327); Open Project Foundation of State Key Laboratory of Synthetical Automation for Process Industries of Northeastern University, China(grant numbers:2019-KF-23-04); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200086); Postgraduate Research and Practice Innovation Program of Jiangsu Province(grant numbers:KYCX21-2249); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516710","Singularly perturbed systems;optimal control;reinforcement learning;policy iteration;model-free control","Optimal control;Mathematical model;Convergence;Eigenvalues and eigenfunctions;Symmetric matrices;Cost function;Reinforcement learning","learning (artificial intelligence);linear systems;optimal control;Riccati equations;singularly perturbed systems","singular perturbation parameter;Kleinman algorithm;subsequent learning algorithm;SPSs;novel model-free learning algorithm;optimal controller;existing learning approaches;RL algorithm;reinforcement learning;linear singularly perturbed systems;brief studies;optimal control problem;offline model-based algorithm;underlying algebraic Riccati equation","","12","","27","IEEE","18 Aug 2021","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Composite Optimal Operational Control of Industrial Systems With Multiple Unit Devices","J. Zhao; C. Yang; W. Dai; W. Gao","Engineering Research Center of Intelligent Control for Underground Space, Ministry of Education and School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; Engineering Research Center of Intelligent Control for Underground Space, Ministry of Education and School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; Engineering Research Center of Intelligent Control for Underground Space, Ministry of Education and School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; Department of Mechanical and Civil Engineering, Florida Institute of Technology, Melbourne, FL, USA","IEEE Transactions on Industrial Informatics","29 Oct 2021","2022","18","2","1091","1101","This article investigates the optimal operational control (OOC) problem for a class of industrial systems consisting of multiple unit devices with fast dynamics and an unknown operational process with slow dynamics. First, the OOC problem is formulated as a noncascade optimal control problem of two-time-scale systems with a novel performance function. Second, using singular perturbation theory, a decentralized composite control scheme is proposed by decomposing the original optimal problem into reduced-order fast and slow subsystem problems. Then, in the framework of reinforcement learning, an online controller design method for the slow subsystem is proposed by using the online measurement, and an offline controller design for the fast subsystem is proposed by using the unit device models. The obtained decentralized composite optimal controller achieves both the desired operational index tracking and disturbance rejection without requiring the dynamics of the operational process. Different from the existing cascade design methods, the proposed approach regulates the unit devices and operational process simultaneously, as well as overcomes the potential high dimensionality and ill-conditioned numerical issues. Finally, a mixed separation thickening process and a numerical example are given to illustrate the presented results.","1941-0050","","10.1109/TII.2021.3076471","National Natural Science Foundation of China(grant numbers:61873272,62073327,61973306); Open Project Foundation of State Key Laboratory of Synthetical Automation for Process Industries of Northeastern University(grant numbers:2019-KF-23-04); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200086); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9419724","Decentralized composite control;industrial systems;optimal operational control (OOC);reinforcement learning;singular perturbation theory (SPT)","Process control;Heuristic algorithms;Performance evaluation;Informatics;Indexes;Optimal control;Target tracking","control system synthesis;learning (artificial intelligence);nonlinear control systems;optimal control;optimisation;singularly perturbed systems","reinforcement learning-based composite optimal operational control;industrial systems;multiple unit devices;optimal operational control problem;unknown operational process;slow dynamics;OOC problem;noncascade optimal control problem;two-time-scale systems;singular perturbation theory;decentralized composite control scheme;original optimal problem;subsystem problems;online controller design method;slow subsystem;offline controller design;fast subsystem;unit device models;composite optimal controller;desired operational index tracking;existing cascade design methods","","11","","28","IEEE","29 Apr 2021","","","IEEE","IEEE Journals"
"Balancing Fairness and Energy Efficiency in SWIPT-Based D2D Networks: Deep Reinforcement Learning Based Approach","E. -J. Han; M. Sengly; J. -R. Lee","School of Intelligent Energy and Industry, Chung-Ang University, Seoul, South Korea; School of Intelligent Energy and Industry, Chung-Ang University, Seoul, South Korea; School of Intelligent Energy and Industry, Chung-Ang University, Seoul, South Korea","IEEE Access","22 Jun 2022","2022","10","","64495","64503","In this study, we propose a method to balance between user fairness and energy efficiency of users in the context of simultaneous wireless information and power transfer (SWIPT)-based device-to-device (D2D) networks. For this purpose, we build an optimization model which determines the subchannel allocation, transmit power level, and power splitting ratio of D2D users, with the purpose of maximizing the objective function which presents the trade-off level between the harvested energy and the average logarithmic data rate of users. To solve this problem, we employ deep reinforcement learning (DRL) which combines deep neural network (DNN) with reinforcement learning (RL). Despite the use of DRL, the dimension of the action space in our work is still very high because it should include subchannel allocation indicator, power splitting ratio, and transmit power of all D2D users. We therefore apply an interior point method to the output of the DNN in DRL to avoid the excess convergent time of DRL. Through the simulations, we compare the performance of our proposed algorithm to that of the conventional iterative algorithms; exhaustive search (ES) and gradient search (GS). Results show that the objective function value remains stable regardless of the change in the maximum transmission power. In addition, it is verified that varying the power splitting ratio has little effect on the system performance, which justifies using a constant power splitting ratio in SWIPT-based D2D networks. Furthermore, it is verified that the proposed DRL achieves near-global-optimal solution compared with conventional algorithms, with lower computational complexity.","2169-3536","","10.1109/ACCESS.2022.3182686","Ministry of Science and ICT (MSIT), South Korea, through the ITRC Support Program supervised by the Institute for Information & Communications Technology Planning & Evaluation (IITP)(grant numbers:IITP-2022-2018-0-01799); Korea Institute of Energy Technology Evaluation and Planning (KETEP) and the Ministry of Trade, Industry & Energy (MOTIE) of the Republic of Korea(grant numbers:20214000000280); National Research Foundation of Korea (NRF) Grant funded by the Korea Government (MEST)(grant numbers:NRF-2020R1A2C1010929); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9795007","Energy efficiency;packet scheduling;D2D network;joint optimization;deep reinforcement learning","Device-to-device communication;Optimization;Resource management;Energy efficiency;Reinforcement learning;Deep learning;Interference","cellular radio;channel allocation;computational complexity;concave programming;energy harvesting;iterative methods;learning (artificial intelligence);neural nets;OFDM modulation;optimisation;radio networks;resource allocation;telecommunication power management;wireless channels","user fairness;transmit power level;trade-off level;harvested energy;deep reinforcement learning;DRL;deep neural network;subchannel allocation indicator;interior point method;objective function value;maximum transmission power;constant power splitting ratio;SWIPT-based D2D networks;SWIPT-Based D2D Networks","","4","","26","CCBYNCND","13 Jun 2022","","","IEEE","IEEE Journals"
"Hierarchical Reinforcement Learning for Air-to-Air Combat","A. P. Pope; J. S. Ide; D. Mićović; H. Diaz; D. Rosenbluth; L. Ritholtz; J. C. Twedt; T. T. Walker; K. Alcedo; D. Javorsek","Applied AI Team, Lockheed Martin, Connecticut, USA; Applied AI Team, Lockheed Martin, Connecticut, USA; Applied AI Team, Lockheed Martin, Connecticut, USA; Applied AI Team, Lockheed Martin, Connecticut, USA; Applied AI Team, Lockheed Martin, Connecticut, USA; Applied AI Team, Lockheed Martin, Connecticut, USA; Applied AI Team, Lockheed Martin, Connecticut, USA; Applied AI Team, Lockheed Martin, Connecticut, USA; Applied AI Team, Lockheed Martin, Connecticut, USA; U.S. Airforce, Virginia, USA","2021 International Conference on Unmanned Aircraft Systems (ICUAS)","19 Jul 2021","2021","","","275","284","Artificial Intelligence (AI) is becoming a critical component in the defense industry, as recently demonstrated by DARPA's AlphaDogfight Trials (ADT). ADT sought to vet the feasibility of AI algorithms capable of piloting an F-16 in simulated air-to-air combat. As a participant in ADT, Lockheed Martin's (LM) approach combines a hierarchical architecture with maximum-entropy reinforcement learning (RL), integrates expert knowledge through reward shaping, and supports modularity of policies. This approach achieved a 2nd place finish in the final ADT event (among eight total competitors) and defeated a graduate of the US Air Force's (USAF) F-16 Weapons Instructor Course in match play.","2575-7296","978-1-6654-1535-4","10.1109/ICUAS51884.2021.9476700","Defense Advanced Research Projects Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9476700","hierarchical reinforcement learning;air combat;flight simulation","Defense industry;Weapons;Atmospheric modeling;Force;Reinforcement learning;Military aircraft;Artificial intelligence","aerospace simulation;defence industry;learning (artificial intelligence);military aircraft;weapons","AI algorithms;simulated air-to-air combat;Lockheed Martin's approach;hierarchical architecture;maximum-entropy reinforcement learning;final ADT event;US Air Force's F-16 Weapons Instructor Course;hierarchical reinforcement learning;artificial intelligence;critical component;defense industry;DARPA's AlphaDogfight Trials;reward shaping","","26","","53","IEEE","19 Jul 2021","","","IEEE","IEEE Conferences"
"A reinforcement-learning approach to robot navigation","Mu-Chun Su; De-Yuan Huang; Chien-Hsing Chou; Chen-Chiung Hsieh","Department of Computer Science & Information Engineering, National Central University, Taiwan; Department of Computer Science & Information Engineering, National Central University, Taiwan; Institute of Information Science, Academia Sinica, Taipei, Taiwan; Integration Technology Laboratory, Institute for Information Industry, Taiwan","IEEE International Conference on Networking, Sensing and Control, 2004","18 May 2004","2004","1","","665","669 Vol.1","This paper presents a reinforcement-learning approach to a navigation system which allows a goal-directed mobile robot to incrementally adapt to an unknown environment. Fuzzy rules which map current sensory inputs to appropriate actions are built through the reinforcement learning. Simulation results illustrate the performance of the proposed navigation system. In this paper, ACSNFIS is used as the main network architecture to implement the reinforcement-learning based navigation system.","1810-7869","0-7803-8193-9","10.1109/ICNSC.2004.1297519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1297519","","Navigation;Mobile robots;Inference algorithms;Fuzzy systems;Fuzzy neural networks;Path planning;Machine learning algorithms;Computer architecture;Service robots;Computer science","fuzzy set theory;navigation;learning (artificial intelligence);mobile robots;fuzzy neural nets","reinforcement learning;robot navigation;navigation system;goal directed mobile robot;fuzzy rules;classifier system based neurofuzzy inference system","","5","","28","IEEE","18 May 2004","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Approach for Optimal Replenishment Policy in A Vendor Managed Inventory Setting For Semiconductors","M. Tariq Afridi; S. Nieto-Isaza; H. Ehm; T. Ponsignon; A. Hamed","Technical University of Munich, Munich, GERMANY; Technical University of Munich, Munich, GERMANY; Infineon Technologies AG, GERMANY; Infineon Technologies AG, GERMANY; Infineon Technologies AG, GERMANY","2020 Winter Simulation Conference (WSC)","29 Mar 2021","2020","","","1753","1764","Vendor Managed Inventory (VMI) is a mainstream supply chain collaboration model. Measurement approaches defining minimum and maximum inventory levels for avoiding product shortages and over-stocking are rampant. No approach undertakes the responsibility aspect concerning inventory level status, especially in semiconductor industry which is confronted with short product life cycles, long process times, and volatile demand patterns. In this work, a root-cause enabling VMI performance measurement approach to assign responsibilities for poor performance is undertaken. Additionally, a solution methodology based on reinforcement learning is proposed for determining optimal replenishment policy in a VMI setting. Using a simulation model, different demand scenarios are generated based on real data from Infineon Technologies AG and compared on the basis of key performance indicators. Results obtained by the proposed method show improved performance than the current replenishment decisions of the company.","1558-4305","978-1-7281-9499-8","10.1109/WSC48552.2020.9384048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384048","","Semiconductor device modeling;Semiconductor device measurement;Supply chains;Key performance indicator;Electronics industry;Reinforcement learning;Data models","inventory management;learning (artificial intelligence);semiconductor industry;supply chain management","volatile demand patterns;root-cause enabling VMI performance measurement approach;optimal replenishment policy;VMI setting;deep reinforcement learning approach;vendor managed inventory;semiconductors;mainstream supply chain collaboration model;maximum inventory levels;product shortages;responsibility aspect;inventory level status;semiconductor industry;short product life cycles;long process times","","4","","31","IEEE","29 Mar 2021","","","IEEE","IEEE Conferences"
"Policy gradient reinforcement learning method for discrete-time linear quadratic regulation problem using estimated state value function","T. Sasaki; E. Uchibe; H. Iwane; H. Yanami; H. Anai; K. Doya","Artificial Intelligence Platform Project, Fujitsu Laboratories Ltd, Kanagawa, Japan; Neural Computation Unit, Okinawa Institute of Science and Technology Graduate University, Okinawa, Japan; Artificial Intelligence Platform Project, Fujitsu Laboratories Ltd, Kanagawa, Japan; Artificial Intelligence Platform Project, Fujitsu Laboratories Ltd, Kanagawa, Japan; Institute of Mathematics for Industry, Kyushu University, Fukuoka, Japan; Neural Computation Unit, Okinawa Institute of Science and Technology Graduate University, Okinawa, Japan","2017 56th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)","13 Nov 2017","2017","","","653","657","In this paper, we propose a policy gradient reinforcement learning method which directly estimates the gradient of the state value function (V-function) with respect to a feedback coefficient matrix using measurable data and uses it for policy improvement. The proposed method can be applicable to the case where the state-action value function (Q-function) is difficult to estimate, and can update the policy in an effective direction for reducing the accumulated cost.","","978-4-907764-57-9","10.23919/SICE.2017.8105539","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8105539","reinforcement learning;adaptive control;policy gradient method;linear quadratic regulation problem","Learning (artificial intelligence);Gradient methods;Mathematical model;Estimation;Electronic mail;Feedback control;Linear programming","adaptive control;discrete time systems;estimation theory;gradient methods;learning (artificial intelligence);matrix algebra","policy gradient reinforcement learning;discrete-time linear quadratic regulation;feedback coefficient matrix;state-action value function;Q-function;adaptive control","","3","","11","","13 Nov 2017","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Virtual Network Embedding for 6G Satellite Networks","R. Zhu; G. Li; P. Wang; J. Yuan",Zhengzhou University; Zhengzhou University; Zhengzhou University; Zhengzhou University of Light Industry,"2021 Opto-Electronics and Communications Conference (OECC)","29 Aug 2022","2021","","","1","3","We establish a satellite network and propose a deep reinforcement learning based virtual network embedding (DRVE-SN) algorithm. Simulation results show that it performs better than the state-of-art algorithm in blocking probability and average resource utilization. © 2021 The Author(s)","2166-8892","978-1-943580-92-7","10.1364/OECC.2021.JS2A.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9866437","","6G mobile communication;Satellites;Simulation;Reinforcement learning;Resource management","6G mobile communication;deep learning (artificial intelligence);satellite communication;telecommunication networks;virtualisation","6G satellite networks;DRVE-SN;deep reinforcement learning based virtual network embedding algorithm;blocking probability;average resource utilization","","1","","6","","29 Aug 2022","","","IEEE","IEEE Conferences"
"High-level behavior control of an e-pet with reinforcement learning","Chih-Wei Hsu; A. Liu","MeeGo Group, Institute for Information Industry, Tainan, Taiwan; Department of Electrical Engineering, Center of Telecommunication Research, National Chung Cheng University, Chiayi, Taiwan","2010 IEEE International Conference on Systems, Man and Cybernetics","22 Nov 2010","2010","","","29","34","One of attractive features of electronic-pets (e-pets) is interaction between the user and the e-pet. The interaction, however, is usually limited to using the predefined commands. In this paper, we present a way of involving the user in helping an e-pet learn high-level behaviors based on basic actions. The high-level behaviors are derived with planning, and the execution of the behaviors is then trained with reinforcement learning. In this research, we explain how we use a partially observable Markov decision process and the hierarchical task network planning for designing behaviors. A Q-learning method is then applied to the training of the e-pet for achieving the correct behavior. A prototype is presented to show its feasibility and effectiveness.","1062-922X","978-1-4244-6588-0","10.1109/ICSMC.2010.5642195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5642195","reinforcement learning;Q-learning;HTN planning;Markov decision process;e-pets","Variable speed drives;Databases","computer games;learning (artificial intelligence);Markov processes;user interfaces","high-level behavior control;electronic-pets;reinforcement learning;partially observable Markov decision process;hierarchical task network planning;Q-learning method;e-pet","","1","","7","IEEE","22 Nov 2010","","","IEEE","IEEE Conferences"
"RTP-Q: a reinforcement learning system with an active exploration planning structure for enhancing the convergence rate","Gang Zhao; Shoji Tatsumi; Ruoying Sun","Faculty of Engineering, Osaka University, Osaka, Japan; Faculty of Engineering, Osaka University, Osaka, Japan; College of Industry and Commerce Management, Liaoning University, Shenyang, China","IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028)","6 Aug 2002","1999","5","","475","480 vol.5","In this paper, we propose an active exploring planning method in the prioritized sweeping reinforcement learning system to make an agent explore an environment efficiently. In order to plan an active exploration behavior, considering the estimate values feature of primitive learning system in our structure, we propose an exploration planning method that fully uses the learned model, plans an active exploration action and simplifies the setting of the parameters. The proposed system utilizes the learned model efficiently not only on computation of estimates, but also for realizing the active exploration to the environment. The comparison experiments of different methods on navigation tasks demonstrate the efficiency of the proposed method.","1062-922X","0-7803-5731-0","10.1109/ICSMC.1999.815597","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=815597","","Learning;Navigation;Power system planning;Process planning;Convergence;Sun;Educational institutions;Business;Environmental management;Engineering management","learning systems;learning (artificial intelligence);path planning;navigation","reinforcement learning;active exploration planning;navigation;Dyna Q architecture;learning system;prioritized sweeping learning;path planning","","1","","10","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Derivation of integrated state equation for combined outputs-inputs vector of discrete-time linear time-invariant system and its application to reinforcement learning","T. Sasaki; E. Uchibe; H. Iwane; H. Yanami; H. Anai; K. Doya","Artificial Intelligence Platform Project, Fujitsu Laboratories Ltd, Kanagawa, Japan; Neural Computation Unit, Okinawa Institute of Science and Technology Graduate University, Okinawa, Japan; Artificial Intelligence Platform Project, Fujitsu Laboratories Ltd, Kanagawa, Japan; Artificial Intelligence Platform Project, Fujitsu Laboratories Ltd, Kanagawa, Japan; Institute of Mathematics for Industry, Kyushu University, Fukuoka, Japan; Neural Computation Unit, Okinawa Institute of Science and Technology Graduate University, Okinawa, Japan","2017 56th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)","13 Nov 2017","2017","","","648","652","For a discrete-time linear time-invariant partially observable system that satisfies the well-known algebraic condition of observability, we derive a fully observable system equation using an augmented state vector combining output and input sequences. The derived representation enables application of reinforcement learning methods for fully observable linear quadratic regulation problems to partially observable ones avoiding the problems invoked in existing methods.","","978-4-907764-57-9","10.23919/SICE.2017.8105538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8105538","reinforcement learning;adaptive control;partially observable system;output feedback control;linear quadratic regulation problem","Learning (artificial intelligence);Feedback control;Observability;Optimal control;Electronic mail;System dynamics","discrete time systems;learning (artificial intelligence);linear quadratic control;linear systems;observability;vectors","outputs-inputs vector;output sequences;observability;fully observable linear quadratic regulation problems;input sequences;augmented state vector;fully observable system equation;discrete-time linear time-invariant partially observable system;reinforcement learning;discrete-time linear time-invariant system;integrated state equation","","","","14","","13 Nov 2017","","","IEEE","IEEE Conferences"
"Digital Campus Financial Data Sharing Based on Distributed Reinforcement Learning Algorithm","Y. Hu","Jiangxi Vocational and Technical College of Industry and Trade, Nanchang, China","2022 International Conference on Artificial Intelligence and Autonomous Robot Systems (AIARS)","14 Nov 2022","2022","","","51","54","Times are changing rapidly. In recent years, the application of financial data sharing on campus is becoming more and more common. It can not only change the traditional financial management mode, but also bring many advantages such as cost reduction and efficiency increase, risk control and so on. With the strong support of national policies, many emerging technologies have gradually come into the campus. Managers find that big data, cloud computing, artificial intelligence and other technologies can promote the further reform of financial data sharing service center, bring new development opportunities, and make business processes tend to be automated and intelligent. Among many new technologies, big data technology brings the most obvious advantages. Distributed reinforcement learning with reinforcement learning decision-making ability and deep learning awareness, has realized from the input to the output of end-to-end approach to learning, it simulates animal learning process, corrected by the method of testing from the state to the action of mapping strategy, finally learned in various environmental conditions to take the best response behavior, thus improve the intelligent system should be white and robustness. In this paper, the distributed reinforcement learning algorithm is used to effectively solve the data management dilemma brought by the diversification of financial information, help financial personnel to cultivate data analysis ability, and further simplify the business process, improve work efficiency.","","978-1-6654-5457-5","10.1109/AIARS57204.2022.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9943218","Distributed Reinforcement Learning Algorithm;Digitization;Campus Financial ata sharing;Data Sharing Research","Education;Distributed databases;Reinforcement learning;Financial management;Big Data;Robustness;Personnel","artificial intelligence;Big Data;business data processing;cloud computing;cost reduction;data analysis;decision making;financial management;learning (artificial intelligence)","animal learning process;artificial intelligence;big data technology;business process;data analysis ability;data management dilemma;digital Campus financial data;distributed reinforcement learning algorithm;end-to-end approach;financial data sharing;financial information;help financial personnel;learning awareness;reinforcement learning decision-making ability;traditional financial management mode","","","","12","IEEE","14 Nov 2022","","","IEEE","IEEE Conferences"
"Multiple Suboptimal Policies Integrated Reinforcement Learning Algorithm for Path Planning","X. Hu; A. Chen; S. Zhang; Z. Yang","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Information Center of China North Industries Group, Beijing, China","2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE)","21 Feb 2022","2022","","","742","746","Reinforcement learning performs well in path planning tasks with unknown environments, where the agent relies on its own exploration capabilities to gather environment information and find the optimal path. However, in sparse reward tasks, the difficulty rises exponentially in obtaining external rewards and collecting available state-action pairs. The agent's policy may not converge by random exploration. In this paper, we propose a multiple suboptimal policies integrated reinforcement learning algorithm for path planning, which uses a reward shaping algorithm to integrate the best parts of different suboptimal policies into intrinsic reward and to assist the agent in finding the near-optimal path. On a sparse reward path planning task in the discrete action space, our method outperforms both the imitation learning method and deep Q-learning from demonstration in terms of convergence speed and policy stability.","","978-1-6654-0886-8","10.1109/ICCECE54139.2022.9712751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712751","Reinforcement learning;Path Planning;Intrinsic Rewards;Suboptimal policies;Reward shaping","Heating systems;Training;Learning systems;Q-learning;Transforms;Color;Path planning","learning (artificial intelligence);mobile robots;path planning;suboptimal control","multiple suboptimal policies integrated reinforcement learning algorithm;path planning;environment information;sparse reward tasks;external rewards;intrinsic reward;near-optimal path;sparse reward path;imitation learning method;deep Q-learning;reward shaping algorithm","","","","25","IEEE","21 Feb 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Automation of Time–Frequency Domain Reflectometry","S. S. Bang; G. -Y. Kwon","Department of Electronics Engineering, Tech University of Korea, Gyeonggi-do, South Korea; Department of Smart Safety Engineering, Dongguk University, Gyeongju, South Korea","IEEE Transactions on Industrial Electronics","","2023","PP","99","1","10","Advanced reflectometry, which can be used to estimate the location of defects in electrical cables with high accuracy, has undergone continuous development based on advanced signal processing techniques. However, in actual fields, non-experts find it difficult to use advanced reflectometry owing to the difficulty of the application process. Therefore, in this paper, a novel technique that can automate the signal-design process in reflectometry, enabling anyone to use advanced reflectometry in the actual field, is proposed. In this work, the signal-design process is automated by utilizing reinforcement learning (RL), and the application methodology is explained using time-frequency domain reflectometry. The agent of the proposed RL model is trained in a simulation-based environment. Finally, the trained agent is applied to a real-world cable to demonstrate its performance.","1557-9948","","10.1109/TIE.2023.3274867","MSIT(Ministry of Science and ICT), Korea, under the ICAN(ICT Challenge and Advanced Network of HRD)(grant numbers:IITP-2023-RS-2022-00156326,#NRF-2022R1C1C1003894); National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10124845","Automation;cable;diagnosis;reflectometry;reinforcement learning;time-frequency analysis","Reflectometry;Power cables;Testing;Time-frequency analysis;Time measurement;Reinforcement learning;Noise measurement","","","","","","","IEEE","15 May 2023","","","IEEE","IEEE Early Access Articles"
"Intelligent Dynamic Spectrum Access Using Deep Reinforcement Learning for VANETs","Y. Wang; X. Li; P. Wan; R. Shao","School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China","IEEE Sensors Journal","15 Jul 2021","2021","21","14","15554","15563","In vehicular ad hoc networks (VANETs), vehicles can communicate with other vehicles or devices through vehicle-to-X communication. However, with the rise of the Internet of Things, it is a real challenge to make the limited spectrum resource meet the increasing communication demands, i.e., the channel contention problem. To solve this problem, this paper proposes a strategy combining multi-hop forwarding through vehicles and dynamic spectrum access. Firstly, a group-based multi-hop broadcast protocol, G-hop, is proposed. G-hop classifies vehicles with similar characteristics such as moving speed and communication distance into same groups using the depth-first-search algorithm. Messages are forwarded within a group in priority and then across groups, which limits both the range and the number of relay vehicles, i.e., channel contenders. Further, we adopt deep reinforcement learning techniques to achieve dynamic spectrum access. We design a Global Optimization algorithm based on Experience Accumulation (GOEA) using deep reinforcement learning. In GOEA, a network structure combined with recurrent neural network and deep Q-network is proposed for learning the time-varying process, and then a reward method is applied to optimize the global utility. Vehicles that need to transmit messages select channels dynamically following the guidance of GOEA. The experimental results demonstrate that the G-hop protocol reduces the packet loss rate from 0.8 to about 0.1. Meanwhile, compared with Slotted Aloha and DQN, our GOEA algorithm reduces the collision probability and channel idle probability by 60%. Moreover, as the number of vehicles increases, the transmission success rate can be improved by 20%.","1558-1748","","10.1109/JSEN.2021.3056463","National Natural Science Foundation of China(grant numbers:61971147); Special Funds from Central Finance to support the development of local universities(grant numbers:400170044,400180004); Foundation of National and Local Joint Engineering Research Center of Intelligent Manufacturing Cyber-Physical Systems; Guangdong Provincial Key Laboratory of Cyber-Physical Systems(grant numbers:008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9344826","Vehicular ad hoc networks;V2X communication;broadcast;dynamic spectrum access;deep reinforcement learning","Protocols;Relays;Vehicle dynamics;Vehicular ad hoc networks;Resource management;Servers;Reinforcement learning","broadcast communication;deep learning (artificial intelligence);Internet of Things;optimisation;protocols;radio spectrum management;recurrent neural nets;resource allocation;search problems;telecommunication computing;vehicular ad hoc networks;wireless channels","GOEA algorithm;collision probability;channel idle probability;intelligent dynamic spectrum access;deep reinforcement learning;vehicular ad hoc networks;vehicle-to-X communication;Internet of Things;spectrum resource;channel contention problem;group-based multihop broadcast protocol;depth-first-search algorithm;relay vehicles;recurrent neural network;deep Q-network;G-hop protocol;global optimization algorithm based on experience accumulation;time-varying process;slotted Aloha;DQN","","10","","47","IEEE","2 Feb 2021","","","IEEE","IEEE Journals"
"Reinforcement Learning Based Variable Speed Limit Control for Mixed Traffic Flows Using Speed Transition Matrices for State Estimation","F. Vrbanić; L. Tišljarić; Ž. Majstorović; E. Ivanjko","University of Zagreb Faculty of Transport and Traffic Sciences, Zagreb, Croatia; University of Zagreb Faculty of Transport and Traffic Sciences, Zagreb, Croatia; University of Zagreb Faculty of Transport and Traffic Sciences, Zagreb, Croatia; University of Zagreb Faculty of Transport and Traffic Sciences, Zagreb, Croatia","2022 30th Mediterranean Conference on Control and Automation (MED)","1 Aug 2022","2022","","","1093","1098","The ever-increasing growth of the car industry and the demand for personal vehicles have put current traffic management systems and infrastructure to strain. The enlarged number of vehicles in traffic flows often creates congestion due to the increased demand to use the existing road capacity. This is especially evident in urban areas that consist of urban roads and urban motorways. Increasing the capacity by building additional infrastructure is not always a feasible solution. Thus, approaches derived from Intelligent Transportation Systems are frequently used to increase the level of service, especially on urban motorways. The development of Connected and Autonomous Vehicles (CAVs) creates additional challenges and opportunities for the traffic management system to cope with. In this study, the Variable Speed Limit (VSL) based on Q-Learning (QL) with CAVs as actuators and mobile sensors combined with Speed Transition Matrices (STMs) for state estimation named STM-QL-VSL is developed and analyzed. Varying traffic scenarios with different CAV penetration rates are analyzed, including the comparison of motorway configuration with one and two applicable VSL zones. The developed STM-QL-VSL algorithm managed to learn the control policy for each tested scenario and improve measured macroscopic traffic parameters such as Total Time Spent and Mean Travel Time.","2473-3504","978-1-6654-0673-4","10.1109/MED54222.2022.9837279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837279","","Industries;Actuators;Q-learning;Roads;Urban areas;Time measurement;Sensors","intelligent transportation systems;reinforcement learning;road safety;road traffic","variable speed limit control;mixed traffic flows;speed transition matrices;state estimation;car industry;personal vehicles;road capacity;urban roads;urban motorways;intelligent transportation systems;traffic management system;traffic scenarios;CAV penetration rates;motorway configuration;control policy;macroscopic traffic parameters;STM-QL-VSL algorithm;connected and autonomous vehicles;mobile sensors","","2","","26","IEEE","1 Aug 2022","","","IEEE","IEEE Conferences"
"A Control Strategy Based on Deep Reinforcement Learning Under the Combined Wind-Solar Storage System","S. Huang; P. Li; M. Yang; Y. Gao; J. Yun; C. Zhang","Key Laboratory of Power System Intelligent Dispatch and Control, Shandong University, Jinan, China; Key Laboratory of Power System Intelligent Dispatch and Control, Shandong University, Jinan, China; Key Laboratory of Power System Intelligent Dispatch and Control, Shandong University, Jinan, China; State Grid Jiangsu Elect Power Co., Wuxi Elect Power Supply Co., Wuxi, China; Key Laboratory of Power System Intelligent Dispatch and Control, Shandong University, Jinan, China; Key Laboratory of Power System Intelligent Dispatch and Control, Shandong University, Jinan, China","IEEE Transactions on Industry Applications","23 Nov 2021","2021","57","6","6547","6558","In a deregulated environment, the renewable energy producers will face the challenge of how to increase their revenues under uncertainties of power generation and time-varying electricity price. In traditional power network scheduling, prediction and optimization are two independent processes, which easily leads to information loss and modeling error. To deal with the uncertainty and realize an end-to-end controller, this article proposes an energy storage system control model (ESSCM) in the scene of the combined wind-solar storage system. The proposed ESSCM using deep reinforcement learning (DRL) algorithm is trained by interacting with the massive environment of a power grid without requiring the assumption on the uncertainties. It learns from scratch to realize the coordination operation of with wind power and photovoltaic power in a combined system, further maximize the benefits of the combined system in the electricity market. One state-of-the-art DRL algorithm, namely double deep Q-network, is used to formulate the proposed ESSCM. Numerical results illustrate that the proposed approach can effectively accommodate the uncertainty and bring high revenues to the combined system.","1939-9367","","10.1109/TIA.2021.3105497","National Key Research and Development Program of China Stem Cell and Translational Research; Research on Key Technologies and Simulation Platform of Collaborative Operation of Active Distribution Power System based on Multiple Flexibility Mining(grant numbers:2019YFE0118400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516967","Control strategy;deep reinforcement learning;energy storage system;photovoltaic (PV) power;wind power","Uncertainty;Renewable energy sources;Wind power generation;Electricity supply industry;Wind farms;Process control;Reinforcement learning","","","","9","","46","IEEE","18 Aug 2021","","","IEEE","IEEE Journals"
"Reinforcement-Learning-Based Composite Optimal Control for Looper Hydraulic Servo Systems in Hot Strip Rolling","Y. Wang; H. Shen; J. Wu; H. Yan; S. Xu","Anhui Province Key Laboratory of Special Heavy Load Robot and School of Metallurgy Engineering, Anhui University of Technology, Ma'anshan, China; School of Electrical and Information Engineering, Anhui University of Technology, Ma'anshan, China; School of Electrical and Information Engineering, Anhui University of Technology, Ma'anshan, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China","IEEE/ASME Transactions on Mechatronics","18 Oct 2023","2023","28","5","2495","2504","The control scheme of the looper angle plays an essential role in hot rolling, which is directly related to the tension maintenance of the strip. The conventional scheme uses a proportion–integration–differentiation (PID) controller to control the servo valve to drive the hydraulic actuator. For different steel types and production temperature changes, the PID control mainly relies on empirical parameter adjustment, which may bring inaccuracy or inefficiency. The objective of this article is to propose a reinforcement-learning-based looper hydraulic servo optimization control scheme to automatically tune the control gain to optimum. First, the modeling error caused by variable parameters and the influence of external disturbance are considered, and the corresponding control model of the looper system is given. Subsequently, a feedforward controller with a radial basis neural-network-based disturbance observer is used to deal with modeling errors and disturbances. A feedback controller with off-policy reinforcement learning is applied in the meantime. The proposed control scheme can realize uniformly ultimately bounded of the system with Lyapunov theory. Simulation results verify the effectiveness of the proposed method.","1941-014X","","10.1109/TMECH.2023.3248861","National Natural Science Foundation of China(grant numbers:62273006,62173001,61703004); Major Technologies Research and Development Special Program of Anhui Province(grant numbers:202003a05020001); Key research and development projects of Anhui Province(grant numbers:202104a05020015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10075062","Composite control;looper hydraulic servo systems (HSSs);nonlinear disturbance observer (NDO);reinforcement learning (RL)","Strips;Servomotors;Valves;Optimal control;Hydraulic systems;Pistons;Production","","","","","","41","IEEE","16 Mar 2023","","","IEEE","IEEE Journals"
"Edge Server Deployment for Health Monitoring With Reinforcement Learning in Internet of Medical Things","H. Yan; M. Bilal; X. Xu; S. Vimal","School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing 210044, China.; Department of Computer Engineering, Hankuk University of Foreign Studies, Yongin-si, Gyeonggi-do, South Korea.; School of Computer and Software and the Jiangsu Collaborative Innovation Center of Atmospheric Environment and Equipment Technology (CICAEET), Nanjing University of Information Science and Technology, Nanjing 210044, China, and also with the State Key Laboratory Novel Software Technology, Nanjing University, Nanjing 210023, China; Department of Artificial Intelligence and Data Science, Ramco Institute of Technology, Rajapalayam, Virudhunagar, Tamilnadu 626 117, India.","IEEE Transactions on Computational Social Systems","","2022","PP","99","1","11","The Internet of Medical Things (IoMT) has recently gained a lot of interest in the health care industry. IoMT enables real-time and omnipresent monitoring of a patient's health status, resulting in massive amounts of medical data being generated. The centralized massive data processing places enormous strain on the typical cloud computing, rendering it incapable of supporting a variety of real-time health care applications. Therefore, edge computing that moves application programs and data processing from central infrastructure to the edge nodes has attracted wide attention. However, adopting existing edge server (ES) deployment strategies for IoMT is not suitable due to the decentralized and high real-time service requirements of IoMT systems. In particular, traditional ES deployment strategies in IoMT system confront major load imbalance across ESs, latency issues, and energy consumption concerns. To address these challenges, a deployment strategy of ESs based on the state-action-reward-state-action (SARSA) learning, named ESL, is designed. Specifically, ESs are quantified by evaluating the silhouette coefficient (SC) and the sum of squared errors. Then, through fuzzy C-means (FCM) algorithm, the preliminary division of health monitoring units (HMUs) and the initial locations of ESs are obtained. Finally, SARSA learning is adopted to determine the deployment of ESs. Furthermore, extensive experiments and analyses confirm that ESL achieves the core objective of optimizing load balancing among ESs while also optimizing request-response latency and request processing energy consumption.","2329-924X","","10.1109/TCSS.2022.3161996","Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20211284); Financial and Science Technology Plan Project of Xinjiang Production and Construction Corps(grant numbers:2020DB005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9756416","Edge computing;fuzzy C-means (FCM);load balance;reinforcement learning;state-action-reward-state-action (SARSA) learning.","Monitoring;Reinforcement learning;Cloud computing;Servers;Energy consumption;Real-time systems;Load management","","","","5","","","IEEE","12 Apr 2022","","","IEEE","IEEE Early Access Articles"
"Deep Reinforcement Learning for Long Term Hydropower Production Scheduling","S. Riemer-Sørensen; G. H. Rosenlund","Mathematics and Cybernetics, SINTEF Digital, Oslo, Norway; Mathematics and Cybernetics, SINTEF Digital, Oslo, Norway","2020 International Conference on Smart Energy Systems and Technologies (SEST)","22 Sep 2020","2020","","","1","6","We explore the use of deep reinforcement learning to provide strategies for long term scheduling of hydropower production. We consider a use-case where the aim is to optimise the yearly revenue given week-by-week inflows to the reservoir and electricity prices. The challenge is to decide between immediate water release at the spot price of electricity and storing the water for later power production at an unknown price, given constraints on the system. We successfully train a soft actor-critic algorithm on a simplified scenario with historical data from the Nordic power market. The presented model is not ready to substitute traditional optimisation tools but demonstrates the complementary potential of reinforcement learning in the data-rich field of hydropower scheduling.","","978-1-7281-4701-7","10.1109/SEST48500.2020.9203208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9203208","machine learning;expert systems;power generation economics;hydroelectric power generation","Reservoirs;Learning (artificial intelligence);Training;Hydroelectric power generation;Standards;Scheduling","hydroelectric power stations;learning (artificial intelligence);optimisation;power engineering computing;power generation scheduling;power markets;pricing;social network theory;water storage","soft actor-critic algorithm;Nordic power market;deep reinforcement learning;long term hydropower production scheduling;reservoir;electricity prices;water storage;optimisation tools","","4","","22","IEEE","22 Sep 2020","","","IEEE","IEEE Conferences"
"Learning to Walk a Tripod Mobile Robot Using Nonlinear Soft Vibration Actuators With Entropy Adaptive Reinforcement Learning","J. I. Kim; M. Hong; K. Lee; D. Kim; Y. -L. Park; S. Oh","Department of Mechanical Engineering, the Institute of Advanced Machine Design (IAMD), the Institute of Engineering Research, Seoul National University, Seoul, Republic of Korea; Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, Republic of Korea; Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, Republic of Korea; Department of Mechanical Engineering, the Institute of Advanced Machine Design (IAMD), the Institute of Engineering Research, Seoul National University, Seoul, Republic of Korea; Department of Mechanical Engineering, the Institute of Advanced Machine Design (IAMD), the Institute of Engineering Research, Seoul National University, Seoul, Republic of Korea; Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, Republic of Korea","IEEE Robotics and Automation Letters","17 Feb 2020","2020","5","2","2317","2324","Soft mobile robots have shown great potential in unstructured and confined environments by taking advantage of their excellent adaptability and high dexterity. However, there are several issues to be addressed, such as actuating speeds and controllability, in soft robots. In this letter, a new vibration actuator is proposed using the nonlinear stiffness characteristic of a hyperelastic material, which creates continuous vibration of the actuator. By integrating three proposed actuators, we also present an advanced soft mobile robot with high degrees of freedom of movement. However, since the dynamic model of the soft mobile robot is generally hard to obtain(intractable), it is difficult to design a controller for the robot. In this regard, we present a method to train a controller, using a novel reinforcement learning (RL) algorithm called adaptive soft actor-critic (ASAC). ASAC gradually reduces a parameter called an entropy temperature, which regulates the entropy of the control policy. In this way, the proposed method can narrow down the search space during training, and reduce the duration of demanding data collection processes in realworld experiments. For the verification of the robustness and the controllability of our robot and the RL algorithm, experiments for zig-zagging path tracking and obstacle avoidance were conducted, and the robot successfully finished the missions with only an hour of training time.","2377-3766","","10.1109/LRA.2020.2970945","National Research Foundation(grant numbers:NRF-2016R1A5A1938472); Institute of Information & Communications Technology Planning & Evaluation(grant numbers:2019-0-01190); Korea Government (MSIT); Technology Innovation Program(grant numbers:2017-10069072); Ministry of Trade, Industry & Energy, Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978537","Modeling;control;and learning for soft robots;hydraulic/pneumatic actuators;motion and path planning","Actuators;Shafts;Vibrations;Mobile robots;Entropy;Strain","actuators;collision avoidance;control system synthesis;controllability;entropy;learning (artificial intelligence);mobile robots;robot dynamics","controllability;tripod mobile robot;nonlinear soft vibration actuators;entropy adaptive reinforcement learning;nonlinear stiffness characteristic;reinforcement learning algorithm;adaptive soft actor-critic;soft mobile robot;controller design;obstacle avoidance;zig-zagging path tracking;RL algorithm;dynamic model;ASAC","","12","","19","IEEE","3 Feb 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning for Enhanced Content Based Video Frame Retrieval System in Low Resolution Videos","T. Ullah; A. Khan; M. Waleed","Department of Computer Systems Engineering, University of Engineering and Technology, Peshawar, Peshawar, Pakistan; Department of Computer Systems Engineering, University of Engineering and Technology, Peshawar, Peshawar, Pakistan; Department of Computer Systems Engineering, University of Engineering and Technology, Peshawar, Peshawar, Pakistan","2020 12th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)","16 Oct 2020","2020","","","1","6","Over the past two decades, evolutional advancement took place in the field of computer, communication and multimedia technologies which led to the production of massive video data production and large image repositories. This research focuses on the video frame retrieval using popular feature extraction method of computer vision called Speeded Up Robust Features (SURF) and Reinforcement Learning (RL) based Point of Interest (POI) calculation and frame reduction. The proposed method uses the concept of Content-Based-Image-Retrieval (CBIR) which retrieves similar images from the collection of large image databases. In video processing, a challenging task is to remove redundant frames as most of the videos have 30 Frames per Second (FPS). This research study consists of the removal of redundant frames and retrieving of similar frames from remaining non-redundant frames based on a query image. Validation for the proposed scheme has been achieved by simulating the system for a standard dataset as well as videos captured ourselves. The results depict the efficacy of the system and provide a platform for benchmarking.","","978-1-7281-6843-2","10.1109/ECAI50035.2020.9223199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9223199","Video Frame Retrieval;Reinforcement Learning (RL);Content Based Image Retrieval (CBIR);Feature Extraction;Redundant Frame Removal;Query Image;Histogram of Oriented Gradient (HOG)","Feature extraction;Reinforcement learning;Euclidean distance;Histograms;Visualization;Task analysis;Computer vision","computer vision;content-based retrieval;feature extraction;image resolution;image retrieval;learning (artificial intelligence);video retrieval;video signal processing;visual databases","reinforcement learning;low resolution videos;evolutional advancement;multimedia technologies;massive video data production;image repositories;popular feature extraction method;computer vision;frame reduction;content-based-image-retrieval;image databases;video processing;redundant frames;similar frames;nonredundant frames;query image;robust features;enhanced content based video frame retrieval system;enhanced content","","","","26","IEEE","16 Oct 2020","","","IEEE","IEEE Conferences"
"Filling Action Selection Reinforcement Learning Algorithm for Safer Autonomous Driving in Multi-Traffic Scenes","F. Yang; X. Li; Q. Liu; C. Liu; Z. Li; Y. Liu","School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; National Key Laboratory of Special Vehicle Design and Manufacturing Integration Technology, Ministry of Science and Technology of China, Beijing, China","2023 IEEE Intelligent Vehicles Symposium (IV)","27 Jul 2023","2023","","","1","7","Learning-based algorithms are gradually emerging in the field of autonomous driving due to their powerful data processing capabilities. Researchers in the field of intelligent vehicle planning and decision-making are gradually using reinforcement learning algorithms to solve related problems. The safety research of reinforcement learning algorithms is significant and widely concerned. The main reason for the safety problem of the existing reinforcement learning algorithm is that there is still a bias in the safety judgment of the current environment, and it is impossible to make directional improvements by modifying the network and training method. In this paper, an action judgment network is designed as a standard to select the optimal action, which can assist the algorithm to judge environmental safety more deeply. Firstly, the action judgment network takes the state space and action as input, and the output is the safety state of the vehicle after the action. Secondly, this work establishes the required database to train the action judgment network through deep learning and achieves the highest accuracy of 98%. Finally, the proposed algorithm is tested in three scenarios: single-lane, intersection, and roundabout. This algorithm can judge the actions according to the reinforcement learning q value table order until the optimal and safe action is selected. The results show that the newly proposed algorithm can greatly improve the safety of the algorithm without affecting vehicle speed.","2642-7214","979-8-3503-4691-6","10.1109/IV55152.2023.10186804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10186804","Autonomous Driving;DRL;Multiple Traffic Scenarios;Safe Deep Learning","Training;Deep learning;Technological innovation;Reinforcement learning;Artificial neural networks;Filling;Stability analysis","decision making;deep learning (artificial intelligence);learning (artificial intelligence);reinforcement learning;road safety;road traffic;road vehicles;traffic engineering computing","action judgment network;deep learning;environmental safety;existing reinforcement learning algorithm;filling action selection reinforcement learning algorithm;intelligent vehicle planning;learning-based algorithms;multitraffic scenes;optimal action;powerful data processing capabilities;reinforcement learning algorithms;reinforcement learning q value table order;safe action;safer autonomous driving;safety judgment;safety problem;safety research;safety state;training method","","","","15","IEEE","27 Jul 2023","","","IEEE","IEEE Conferences"
"Combining Deep Reinforcement Learning with Rule-based Constraints for Safe Highway Driving","T. Liu; Q. Liu; H. Liu; X. Ren","School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Artificial Intelligence, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","2785","2790","Deep reinforcement learning (DRL) has been employed in solving challenging decision-making problems in autonomous driving. Safe decision-making in autonomous highway driving is among the foremost open problems due to the highly evolving driving environments and the influence of surrounding road users. In this paper, we present a powerful safe framework, which leverages the merits of both rule-based constraints and DRL for safety assurance. We model the highway scenario as a Markov Decision Process (MDP) and apply the deep Q-network (DQN) algorithm to optimize the driving performance. Moreover, a multi-head attention mechanism is introduced as a way to observe that vehicles with strong interactions make a difference in the decision-making of the ego vehicle, which can enhance the safety of the ego vehicle under complex highway driving environments. We also implement a safety module based on common traffic practices to ensure a minimum relative distance between two vehicles. This safety module will serve as feedback on the action of the DRL agent. If the action leads to risk, it will be replaced by a safer one and a negative reward will be assigned. The test and evaluation for our approach in a three-lane highway driving scenario have been done. The experiment results indicate that the proposed framework is capable of reducing the collision rate and accelerating the learning process.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055747","National Natural Science Foundation of China; Science and Technology Commission of Shanghai Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055747","Deep reinforcement learning (DRL);attention mechanism;safety module;highway scenario","Deep learning;Automation;Roads;Decision making;Reinforcement learning;Markov processes;Safety","decision making;deep learning (artificial intelligence);intelligent transportation systems;learning (artificial intelligence);Markov processes;reinforcement learning;road safety;road traffic;road vehicles;traffic engineering computing","autonomous driving;autonomous highway driving;challenging decision-making problems;complex highway driving environments;deep Q-network algorithm;deep reinforcement learning;driving performance;DRL agent;ego vehicle;foremost open problems;highly evolving driving environments;highway scenario;learning process;Markov Decision Process;multihead attention mechanism;powerful safe framework;road users;rule-based constraints;safe decision-making;safe highway driving;safety assurance;safety module;three-lane highway","","","","23","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Unified Automatic Control of Vehicular Systems With Reinforcement Learning","Z. Yan; A. R. Kreidieh; E. Vinitsky; A. M. Bayen; C. Wu","Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Civil and Environmental Engineering, University of California, Berkeley, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA, USA; Department of Civil and Environmental Engineering, Laboratory for Information and Decision Systems, Institute of Data, Systems, and Society, Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE Transactions on Automation Science and Engineering","6 Apr 2023","2023","20","2","789","804","Emerging vehicular systems with increasing proportions of automated components present opportunities for optimal control to mitigate congestion and increase efficiency. There has been a recent interest in applying deep reinforcement learning (DRL) to these nonlinear dynamical systems for the automatic design of effective control strategies. Despite conceptual advantages of DRL being model-free, studies typically nonetheless rely on training setups that are painstakingly specialized to specific vehicular systems. This is a key challenge to efficient analysis of diverse vehicular and mobility systems. To this end, this article contributes a streamlined methodology for vehicular microsimulation and discovers high performance control strategies with minimal manual design. A variable-agent, multi-task approach is presented for optimization of vehicular Partially Observed Markov Decision Processes. The methodology is experimentally validated on mixed autonomy traffic systems, where fractions of vehicles are automated; empirical improvement, typically 15-60% over a human driving baseline, is observed in all configurations of six diverse open or closed traffic systems. The study reveals numerous emergent behaviors resembling wave mitigation, traffic signaling, and ramp metering. Finally, the emergent behaviors are analyzed to produce interpretable control strategies, which are validated against the learned control strategies. Note to Practitioners—As vehicular systems such as real-world traffic systems and robotic warehouses become increasingly automated, optimizing vehicle movements sees an increasing potential to reduce congestion and increase efficiency. For many vehicular systems, simulations of varying fidelity are commonly used for analysis and optimization without the need to deploy real vehicles. This article describes a unified and practical approach for optimal control of vehicles in arbitrary simulated vehicular systems while permitting partial automation, where the behavior of fractions of vehicles at given times can be modelled but not controlled. As illustrated by the diverse traffic systems considered in this article, the presented methodology emphasizes ease of application within any simulated vehicular system while minimizing manual efforts by the practitioner. The control inputs consist of local information around each automated vehicle, while the control outputs are commands for longitudinal acceleration and lateral lane change. Experimental results are presented for relatively small simulated traffic systems, though the methodology can be adapted to larger vehicular systems with minor modifications. Experimentally optimized behaviors provide insights to the practitioner which may assist in designing simplified and interpretable control strategies. Implementation in real-world systems depends on two requirements: 1) a reliable fallback mechanism for ensuring safety of vehicles, and 2) sufficient fidelity of the simulator for simulated behaviors to transfer. These requirements are under active research for traffic systems and may be practical in some robotic settings. To facilitate robust transfer of policies from simulated to real-world systems, future extensions of this work may inject additional randomization into simulation while reducing the unmodeled stochasticity of targeted real-world systems as much as possible.","1558-3783","","10.1109/TASE.2022.3168621","Amazon Catalyst; MIT-IBM Watson AI Laboratory; Department of Transportation Dwight David Eisenhower Transportation Fellowship Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9765650","Primary topics: Mobile traffic control;automated vehicles;reinforcement learning Secondary topic keywords: Mixed autonomy;multi-agent systems","Reinforcement learning;Optimization;Markov processes;Automation;Vehicle dynamics;Tuning;Robot kinematics","control engineering computing;deep learning (artificial intelligence);learning (artificial intelligence);Markov processes;mobile robots;multi-agent systems;nonlinear dynamical systems;optimal control;reinforcement learning;road traffic;road traffic control;road vehicles;traffic engineering computing","arbitrary simulated vehicular systems;closed traffic systems;control inputs;control outputs;discovers high performance control strategies;diverse open traffic systems;diverse traffic systems;diverse vehicular systems;effective control strategies;interpretable control strategies;larger vehicular systems;learned control strategies;mixed autonomy traffic systems;mobility systems;nonlinear dynamical systems;optimal control;real-world systems;real-world traffic systems;relatively small simulated traffic systems;simulated vehicular system;specific vehicular systems;unified automatic control;vehicular microsimulation;vehicular Partially Observed Markov Decision Processes","","1","","60","IEEE","29 Apr 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning Control of a Forestry Crane Manipulator","J. Andersson; K. Bodin; D. Lindmark; M. Servin; E. Wallin","Department of Physics, Umeå University; Algoryx Simulation AB; Algoryx Simulation AB; Department of Physics, Umeå University; Department of Physics, Umeå University","2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","16 Dec 2021","2021","","","2121","2126","Forestry machines are heavy vehicles performing complex manipulation tasks in unstructured production forest environments. Together with the complex dynamics of the onboard hydraulically actuated cranes, the rough forest terrains have posed a particular challenge in forestry automation. In this study, the feasibility of applying reinforcement learning control to forestry crane manipulators is investigated in a simulated environment. Our results show that it is possible to learn successful actuator-space control policies for energy efficient log grasping by invoking a simple curriculum in a deep reinforcement learning setup. Given the pose of the selected logs, our best control policy reaches a grasping success rate of 97%. Including an energy-optimization goal in the reward function, the energy consumption is significantly reduced compared to control policies learned without incentive for energy optimization, while the increase in cycle time is marginal. The energy-optimization effects can be observed in the overall smoother motion and acceleration profiles during crane manipulation.","2153-0866","978-1-6654-1714-3","10.1109/IROS51168.2021.9636219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636219","","Cranes;Automation;Uncertainty;Forestry;Grasping;Reinforcement learning;Trajectory","control engineering computing;cranes;forestry;hydraulic actuators;manipulators;reinforcement learning","forestry crane manipulator;forestry machines;heavy vehicles;complex manipulation tasks;unstructured production forest environments;complex dynamics;cranes;rough forest terrains;forestry automation;reinforcement learning control;simulated environment;energy efficient log;deep reinforcement learning setup;control policy;grasping success rate;energy-optimization goal;energy optimization;energy-optimization effects;crane manipulation;actuator-space control policies","","8","","20","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Security-Aware Computation Offloading in Satellite Networks","S. Sthapit; S. Lakshminarayana; L. He; G. Epiphaniou; C. Maple","Warwick Manufacturing Group, University of Warwick, Coventry, U.K; School of Engineering, University of Warwick, Coventry, U.K; Department of Computer Science, University of Warwick, Coventry, U.K; Warwick Manufacturing Group, University of Warwick, Coventry, U.K; Warwick Manufacturing Group, University of Warwick, Coventry, U.K","IEEE Internet of Things Journal","6 Jul 2022","2022","9","14","12351","12363","The rise of NewSpace provides a platform for small and medium businesses to commercially launch and operate satellites in space. In contrast to traditional satellites, NewSpace provides the opportunity for delivering computing platforms in space. However, computational resources within space are usually expensive and satellites may not be able to compute all computational tasks locally. Computation offloading (CO), a popular practice in Edge/Fog computing, could prove effective in saving energy and time in this resource-limited space ecosystem. However, CO alters the threat and risk profile of the system. In this article, we analyze security issues in space systems and propose a security-aware algorithm for CO. Our method is based on the reinforcement learning technique, deep deterministic policy gradient (DDPG). We show, using Monte-Carlo simulations, that our algorithm is effective under a variety of environment and network conditions and provide novel insights into the challenge of optimized location of computation.","2327-4662","","10.1109/JIOT.2021.3135632","Engineering and Physical Sciences Research Council through UKRI under the Industry Strategic Challenge Fund (ISCF) for Robotics and AI Hubs in Extreme and Hazardous Environments(grant numbers:EP/R026092 (FAIR-SPACE Hub)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9651535","Computation offloading (CO);cyber-security;Internet of Thing (IoT);low Earth orbit (LEO) satellites;NewSpace;reinforcement learning (RL)","Satellites;Space vehicles;Earth;Security;Planetary orbits;Sensors;Encryption","Internet of Things;mobile computing;Monte Carlo methods;reinforcement learning;security of data","NewSpace;computational resources;reinforcement learning technique;security-aware computation offloading;satellite networks;CO;edge computing;fog computing;deep deterministic policy gradient;DDPG;Monte-Carlo simulations;Internet of Thing;IoT;cyber-security;low Earth orbit;LEO;RL","","6","","62","IEEE","15 Dec 2021","","","IEEE","IEEE Journals"
"Heuristics Integrated Deep Reinforcement Learning for Online 3D Bin Packing","S. Yang; S. Song; S. Chu; R. Song; J. Cheng; Y. Li; W. Zhang","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","12","Online 3D Bin Packing Problem (3D-BPP) has a wide range of industrial applications and there is an emerging research interest in learning optimal bin packing policy and deploying it for real logistics applications. From the heuristic methods to the deep reinforcement learning (DRL) methods, the previous works have proposed many solutions to solve the online 3D-BPP. However, none of them have studied what and how heuristics can be modelled into DRL to build a more effective and practical bin packing pipeline. In this work, we thoroughly investigate what heuristics can be used in online 3D-BPP and how to effectively integrate the heuristics with the DRL. First, we design 3 different heuristics based on the physical rules of the real world and the experiences of the human packers, including the Physics-Heuristics, the Packing-Heuristics and the Unpacking-Heuristics. Second, we model the 3 types of heuristics into the DRL framework and propose a novel heuristic DRL method to solve the online 3D-BPP. Extensive experimental results show that our method achieves state-of-the-art bin packing performance and the resulting real-world system is able to reliably finish the bin packing task in real logistics scenarios. Supplementary video is available at https://www.youtube.com/watch?v=x8GpmEELq18. Note to Practitioners—The rapid growth of e-commerce has significantly increased the burden of human packers in logistic warehouses, where the workers need to pick the products from a conveyor and pack them into bins (i.e. the online 3D bin packing). Thus it is of great importance to develop intelligent robotic systems to replace human labor, which is a long-standing topic in the field of control and automation science. This paper makes a substantial contribution to the related field by studying the online 3D bin packing in terms of both the theory and practice. On the one hand, the simulated experiments suggest that the presented algorithm significantly improves the space utilization of bin packing. On the other hand, the robotic system developed based on the proposed method can favourably finish the bin packing task in real logistics scenarios, demonstrating the practical use of our approach. Consequently, the approach proposed in this paper is totally applicable in logistic warehouses and is promising to drastically improve the working efficiency of the product packing in real warehouses. In the future, we will extend the presented approach to pack irregular-shaped objects and then facilitate more logistics applications.","1558-3783","","10.1109/TASE.2023.3235742","National Key Research and Development Plan of China(grant numbers:2021ZD0112002); National Natural Science Foundation of China(grant numbers:U1913204,61991411); Natural Science Foundation of Shandong Province for Distinguished Young Scholars(grant numbers:ZR2020JQ29); Project for Self-Developed Innovation Team of Jinan City(grant numbers:2021GXRC038); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10018146","Bin packing;robotics;warehouse automation;deep reinforcement learning","Logistics;Three-dimensional displays;Task analysis;Robots;Optimization;Reinforcement learning;Deep learning","","","","3","","","IEEE","16 Jan 2023","","","IEEE","IEEE Early Access Articles"
"A Novel On-Demand Charging Strategy Based on Swarm Reinforcement Learning in WRSNs","Z. Wei; M. Li; Z. Wei; L. Cheng; Z. Lyu; F. Liu","School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China","IEEE Access","13 May 2020","2020","8","","84258","84271","The charging issue in Wireless Rechargeable Sensor Networks (WRSNs) is a popular research problem. With the help of wireless energy transfer technology, electrical energy can be transfer from Wireless Charging Equipment (WCE) to the sensor nodes, providing a new paradigm to prolong the network lifetime. Existing research usually takes the periodical and deterministic charging approach, but ignore the limited energy of the WCE and the influences of non-deterministic factors such as topological changes and node failures, making them unsuitable for real networks. In this study, we aim to minimize the number of dead sensor nodes while maximizing energy utilization of WCE under the limited energy of the WCE. Furthermore, the Swarm Reinforcement Learning (SRL) method is firstly introduced to achieve the autonomous planning ability of WCE. Moreover, to solve the problem of insufficient search in existing SRL algorithm, we improve the SRL by firefly algorithm. And a novel charging algorithm, named Swarm Reinforcement Learning based on Firefly Algorithm (SRL-FA), is proposed for the on-demand charging architecture. To evaluate the performance of the proposed algorithm, SRL-FA is compared with the existing swarm reinforcement learning algorithms and classic on-demand charging algorithms in two network scenarios. The Extensive simulation shows that the proposed algorithm can achieve promising performance in energy utilization of WCE, charging success rate and other performance metrics.","2169-3536","","10.1109/ACCESS.2020.2992127","National Key Research Development Program of China(grant numbers:2018YFC0604404); Key Research and Development Project in Anhui Province, China(grant numbers:201904a07020030); Fundamental Research Funds for the Central Universities, China(grant numbers:PA2019GDPK0079); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9085330","Wireless rechargeable sensor networks;on-demand charging algorithm;swarm reinforcement learning;firefly algorithm","Reinforcement learning;Robot sensing systems;Wireless sensor networks;Path planning;Wireless communication;Planning;Energy harvesting","learning (artificial intelligence);optimisation;power engineering computing;telecommunication computing;wireless sensor networks","on-demand charging algorithms;existing swarm reinforcement learning algorithms;on-demand charging architecture;named Swarm Reinforcement Learning;novel charging algorithm;firefly algorithm;Swarm Reinforcement Learning method;energy utilization;dead sensor nodes;node failures;nondeterministic factors;deterministic charging approach;periodical charging approach;network lifetime;WCE;Wireless Charging Equipment;electrical energy;wireless energy transfer technology;Wireless Rechargeable Sensor Networks;WRSNs;On-Demand Charging Strategy","","5","","32","CCBY","4 May 2020","","","IEEE","IEEE Journals"
"Coordinated Motion Planning of Dual-arm Space Robot with Deep Reinforcement Learning","M. Tang; X. Yue; Z. Zuo; X. Huang; Y. Liu; N. Qi","Expace Technology Co., Ltd, Fourth Academy of China Aerospace Science & Industry Corp, Wuhan, China; Expace Technology Co., Ltd, Fourth Academy of China Aerospace Science & Industry Corp, Wuhan, China; Expace Technology Co., Ltd, Fourth Academy of China Aerospace Science & Industry Corp, Wuhan, China; Expace Technology Co., Ltd, Fourth Academy of China Aerospace Science & Industry Corp, Wuhan, China; School of Astronautics, Harbin Institude of Technology, Harbin, China; School of Astronautics, Harbin Institude of Technology, Harbin, China","2019 IEEE International Conference on Unmanned Systems (ICUS)","13 Feb 2020","2019","","","469","473","In this paper, we focus on coordinated motion planning of dual-arm robot. The kinematics model of the robotic arm is established by Denavit-Hartenberg (D-H) coordinate method and the mathematical model of the cooperative motion planning problem is established. The rapidly-exploring random trees (RRT) algorithm and the deep deterministic policy gradient (DDPG) algorithm are used to carry out dual-arm coordinated motion planning, respectively. The simulation results show that these algorithms can effectively complete the robot arm motion planning task, but the RRT improved algorithm cannot balance the planning efficiency and result optimization. Compared with the RRT algorithm, the DDPG algorithm trains the model through continuous trial and error to optimize its planning strategy. The trained model can be used to obtain an optimized path and it can ensure the efficiency of the planning with the optimized strategy.","","978-1-7281-3792-6","10.1109/ICUS48101.2019.8996069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8996069","dual-arm robot;motion planning;collision detection;rapidly-exploring random trees algorithm;deep deterministic policy gradient algorithm","Manipulators;Robot kinematics;Planning;Collision avoidance;Kinematics;Mathematical model","aerospace robotics;control engineering computing;gradient methods;learning (artificial intelligence);manipulator kinematics;neural nets;path planning;trees (mathematics)","dual-arm space robot;deep reinforcement learning;dual-arm robot;kinematics model;robotic arm;mathematical model;motion planning problem;random trees algorithm;deep deterministic policy gradient algorithm;dual-arm coordinated motion planning;robot arm motion planning;RRT algorithm;DDPG algorithm;Denavit-Hartenberg coordinate method","","4","","10","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Dynamic Spectrum Anti-Jamming With Reinforcement Learning Based on Value Function Approximation","X. Zhu; Y. Huang; S. Wang; Q. Wu; X. Ge; Y. Liu; Z. Gao","Key Laboratory of Dynamic Cognitive System of Electromagnetic Spectrum Space, Ministry of Industry and Information Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Key Laboratory of Dynamic Cognitive System of Electromagnetic Spectrum Space, Ministry of Industry and Information Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Key Laboratory of Dynamic Cognitive System of Electromagnetic Spectrum Space, Ministry of Industry and Information Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Key Laboratory of Dynamic Cognitive System of Electromagnetic Spectrum Space, Ministry of Industry and Information Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China","IEEE Wireless Communications Letters","9 Feb 2023","2023","12","2","386","390","This letter addresses the spectrum anti-jamming problem with multiple Internet of Things (IoT) devices for uplink transmissions, where policies for configuring frequency-domain channels have to be learned without the knowledge of the time-frequency distribution of the interference. The problem of decision-making or learning is expected to be solved by reinforcement learning (RL) approaches. However, the state-of-the-art RL-based spectrum anti-jamming methods may not be applicable in IoT systems, suffer from high computational complexity or may converge to a policy that may not be the best for each user. Therefore, we propose a novel spectrum anti-jamming scheme where configuration policies for the IoT devices are sequentially optimized with value function approximation-based multi-agent RL. Simulation results show that our proposed algorithm outperforms various baselines in terms of average normalized throughput.","2162-2345","","10.1109/LWC.2022.3228045","National Natural Science Foundation of China(grant numbers:61901216,61827801,U2001210); Natural Science Foundation of Jiangsu Province(grant numbers:BK20190400); Fund of Prospective Layout of Scientific Research for Nanjing University of Aeronautics and Astronautics (NUAA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978681","Uplink transmissions;Internet of Things;anti-jamming;Markov decision process;reinforcement learning","Jamming;Internet of Things;Wireless networks;Time-frequency analysis;Interference;Decision making;Channel estimation","computational complexity;function approximation;Internet of Things;jamming;learning (artificial intelligence);multi-agent systems;reinforcement learning;telecommunication computing","configuration policies;decision-making;dynamic spectrum anti-jamming;frequency-domain channels;IoT devices;IoT systems;multiple Internet;reinforcement learning approaches;spectrum anti-jamming problem;spectrum anti-jamming scheme;state-of-the-art RL-based spectrum anti-jamming methods;Things devices;time-frequency distribution;uplink transmissions;value function approximation-based multiagent RL","","1","","18","IEEE","9 Dec 2022","","","IEEE","IEEE Journals"
"A Train Cooperative Operation Optimization Method Considering Passenger Comfort based on Reinforcement Learning","X. Wang; Y. Wu; D. Huang; L. Zhu; Z. Lu; Y. He","Institute of System Science and Technology, School of Electrical Engineering, Southwest Jiaotong University, Chengdu, P. R. China; Institute of System Science and Technology, School of Electrical Engineering, Southwest Jiaotong University, Chengdu, P. R. China; Institute of System Science and Technology, School of Electrical Engineering, Southwest Jiaotong University, Chengdu, P. R. China; Institute of System Science and Technology, School of Electrical Engineering, Southwest Jiaotong University, Chengdu, P. R. China; The School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, P. R. China; The School of Design Arts, Southwest Jiaotong University, Chengdu, P. R. China","2022 IEEE 11th Data Driven Control and Learning Systems Conference (DDCLS)","26 Aug 2022","2022","","","368","373","This paper mainly focuses on the high-speed train cooperative operation problem, considering passenger comfort. To solve this problem, a speed curve optimization method based on Reinforcement Learning algorithm is proposed. First, according to the train dynamics system, we build the speed curve optimization object. For realizing the train cooperative operation and increasing the passenger comfort, trains distance constraint and jerk rate constraint are fully considered in formulating reward function. And then, agent of Twin Delayed Deep Deterministic Policy Gradient (TD3) is established. Based on TD3 algorithm, two groups of Q-value networks and one group of action action networks have the same structure, different initial values and update rates respectively, which improves the stability and convergence speed of the algorithm. After the training. The Agent can generate a desirable speed curve of train based on constraints of vehicle output and jerk rate under cooperative operation.","2767-9861","978-1-6654-9675-9","10.1109/DDCLS55054.2022.9858409","National Natural Science Foundation of China(grant numbers:U1934221,U21A20169); Sichuan Science and Technology Program(grant numbers:2021JDJQ0012,2020YFQ0057); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858409","Train Cooperative Operation;Reinforcement Learning;Optimization Method;Twin Delayed Deep Deterministic Policy Gradient;Comfort","Resistance;Training;Learning systems;Heuristic algorithms;Optimization methods;Reinforcement learning;Reliability engineering","ergonomics;gradient methods;optimisation;railway engineering;reinforcement learning;vehicle dynamics","high-speed train;passenger comfort;speed curve optimization method;reinforcement learning algorithm;train dynamics system;speed curve optimization object;distance constraint;jerk rate;formulating reward function;twin delayed deep deterministic policy gradient;Q-value networks;action action networks;update rates","","1","","29","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"Reassignment Algorithm of the Ride-Sourcing Market Based on Reinforcement Learning","Y. Wang; J. Wu; H. Sun; Y. Lv; G. Xu","State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China; State Key Laboratory of Rail Traffic Control and Safety, and the Key Laboratory of Transport Industry of Big Data Application Technologies for Comprehensive Transport, Beijing Jiaotong University, Beijing, China; Key Laboratory of Transport Industry of Big Data Application Technologies for Comprehensive Transport, Beijing Jiaotong University, Beijing, China; Key Laboratory of Transport Industry of Big Data Application Technologies for Comprehensive Transport, Beijing Jiaotong University, Beijing, China; Key Laboratory of Transport Industry of Big Data Application Technologies for Comprehensive Transport, Beijing Jiaotong University, Beijing, China","IEEE Transactions on Intelligent Transportation Systems","4 Oct 2023","2023","24","10","10923","10936","Reassignment strategies are of great significance to improve the dispatching efficiency of the ride-sourcing market by reassigning drivers and passengers. However, due to the focus on the feasibility of the reassignment strategy in the short period, previous studies ignore possible reassignment opportunities in the future and inevitably make short-sighted reassignment decisions. To fully exploit the effect of the reassignment strategy, this study proposes a two-stage reassignment framework, which integrates a reinforcement learning algorithm and the bilateral matching reassignment model. The Markov decision process is adopted to dynamically model the reassignment problem. In the framework, the reinforcement learning algorithm is utilized to first learn the randomness and dynamics of travel patterns from historical data and select vehicles participating in the reassignment process. Then, the bilateral matching reassignment model formulates the matching relationship after reassignment for passengers (drivers). Furthermore, for the cases where reassignment may increase individual matching distance, a personalized bilateral matching reassignment model is developed to avoid that. Experiments based on real data in Beijing found that learning passenger travel patterns and adjusting vehicle reassignment moments can greatly improve the passenger experience and reduce driving costs. The results also suggest that the efficiency of the reassignment strategy is influenced by the supply and demand conditions of the ride-sourcing system. This justifies that the framework can be applied to optimize the dispatching process, reduce carbon emissions, and build an eco-friendly travel system.","1558-0016","","10.1109/TITS.2023.3274636","National Natural Science Foundation of China(grant numbers:71890972/71890970,72288101); 111 Project(grant numbers:B20071); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10128792","Ride-sourcing;reassignment;reinforcement learning;Markov decision process","Dispatching;Vehicles;Heuristic algorithms;Vehicle dynamics;Spatiotemporal phenomena;Reinforcement learning;Optimization","","","","","","54","IEEE","17 May 2023","","","IEEE","IEEE Journals"
"Holistic Deep-Reinforcement-Learning-based Training for Autonomous Navigation in Crowded Environments","L. Kästner; M. Meusel; T. Bhuiyan; J. Lambrecht","Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany","2023 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)","2 Aug 2023","2023","","","1302","1308","In recent years, Deep Reinforcement Learning emerged as a promising approach for autonomous navigation of robots and has been utilized in various areas of navigation such as obstacle avoidance, motion planning, or decision making in crowded environments. However, most research works either focus on providing an end-to-end solution training the whole system using Deep Reinforcement Learning or focus on one specific aspect such as local motion planning. This however, comes along with a number of problems such as catastrophic forgetfulness, inefficient navigation behavior, and non-optimal synchronization between different entities of the navigation stack. In this paper, we propose a holistic Deep Reinforcement Learning training approach in which the training procedure is involving all entities of the navigation stack. This should enhance the synchronization between- and understanding of all entities of the navigation stack and as a result, improve navigational performance in crowded environments. We trained several agents with a number of different observation spaces to study the impact of different input on the navigation behavior of the agent. In profound evaluations against multiple learning-based and classic model-based navigation approaches, our proposed agent could outperform the baselines in terms of efficiency and safety attaining shorter path lengths, less roundabout paths, and less collisions especially in situations with a high number of pedestrians.","2159-6255","978-1-6654-7633-1","10.1109/AIM46323.2023.10196260","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10196260","","Training;Deep learning;Pedestrians;Navigation;Reinforcement learning;Generators;Planning","collision avoidance;control engineering computing;decision making;deep learning (artificial intelligence);mobile robots;navigation;path planning;pedestrians;reinforcement learning","autonomous navigation;classic model-based navigation approaches;crowded environments;end-to-end solution training;holistic deep reinforcement learning training approach;holistic deep-reinforcement-learning-based training;inefficient navigation behavior;local motion planning;navigation stack;navigational performance;nonoptimal synchronization;training procedure","","","","33","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Opportunistic Routing Protocol Using Depth Information for Energy-Efficient Underwater Wireless Sensor Networks","C. Wang; X. Shen; H. Wang; H. Zhang; H. Mei","School of Marine Science and Technology and the Key Laboratory of Ocean Acoustics and Sensing, Ministry of Industry and Information Technology, Northwestern Polytechnical University, Xi’an, China; School of Marine Science and Technology and the Key Laboratory of Ocean Acoustics and Sensing, Ministry of Industry and Information Technology, Northwestern Polytechnical University, Xi’an, China; School of Marine Science and Technology and the School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi’an, China; School of Marine Science and Technology and the Key Laboratory of Ocean Acoustics and Sensing, Ministry of Industry and Information Technology, Northwestern Polytechnical University, Xi’an, China; School of Marine Science and Technology and the Key Laboratory of Ocean Acoustics and Sensing, Ministry of Industry and Information Technology, Northwestern Polytechnical University, Xi’an, China","IEEE Sensors Journal","1 Aug 2023","2023","23","15","17771","17783","An efficient routing protocol is critical for the data transmission of underwater wireless sensor networks (UWSNs). Aiming to the problem of void region in UWSNs, this article proposes a reinforcement learning-based opportunistic routing protocol (DROR). By considering the limited energy and underwater environment, DROR is a receiver-based routing protocol, and combines reinforcement learning (RL) with opportunistic routing (OR) to ensure real-time performance of data transmission as well as energy efficiency. To achieve reliable transmission when encountering void regions, a void recovery mechanism is designed to enable packets to bypass void nodes and continue forwarding. Furthermore, a relative  ${Q}$ -based dynamic scheduling strategy is proposed to ensure that packets can efficiently forward along the global optimal routing path. Simulation results show that the proposed protocol performs well in terms of end-to-end delay, reliability, and energy efficiency in UWSNs.","1558-1748","","10.1109/JSEN.2023.3285751","National Natural Science Foundation of China(grant numbers:62031021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10154617","Q-learning;underwater wireless sensor networks (UWSNs);opportunistic routing (OR);void region;routing protocol","Routing;Routing protocols;Sensors;Relays;Q-learning;Wireless sensor networks;Reliability","energy conservation;reinforcement learning;routing protocols;telecommunication network routing;telecommunication power management;underwater acoustic communication;voids (solid);wireless sensor networks","combines reinforcement learning;data transmission;depth information;DROR;efficient routing protocol;encountering void regions;energy efficiency;global optimal routing path;receiver-based;reinforcement learning-based opportunistic routing protocol;relativeQ-based dynamic scheduling strategy;underwater environment;underwater wireless sensor networks;void nodes;void recovery mechanism;void region","","","","40","IEEE","16 Jun 2023","","","IEEE","IEEE Journals"
"Enhancing Navigational Safety in Crowded Environments using Semantic-Deep-Reinforcement-Learning-based Navigation","L. Kästner; J. Lil; Z. Shen; J. Lambrecht","Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany","2022 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)","24 Jan 2023","2022","","","87","93","Intelligent navigation among social crowds is an essential aspect of mobile robotics for applications such as delivery, health care, or assistance. Deep Reinforcement Learning emerged as an alternative planning method to conservative approaches and promises more efficient and flexible navigation. However, in highly dynamic environments employing different kinds of obstacle classes, safe navigation still presents a grand challenge. In this paper, we propose a semantic Deep-reinforcement-learning-based navigation approach that teaches object-specific safety rules by considering high-level obstacle information. In particular, the agent learns object-specific behavior by contemplating the specific danger zones to enhance safety around vulnerable object classes. We tested the approach against a benchmark obstacle avoidance approach and found an increase in safety. Furthermore, we demonstrate that the agent could learn to navigate more safely by keeping an individual safety distance dependent on the semantic information.","2475-8426","978-1-6654-5680-7","10.1109/SSRR56537.2022.10018699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10018699","","Training;Navigation;Semantics;Reinforcement learning;Robustness;Safety;Behavioral sciences","collision avoidance;deep learning (artificial intelligence);mobile robots;reinforcement learning","alternative planning method;benchmark obstacle avoidance approach;crowded environments;efficient navigation;essential aspect;flexible navigation;high level obstacle information;highly dynamic environments;individual safety distance;intelligent navigation;mobile robotics;navigational safety;object classes;object specific behavior;object specific safety rules;obstacle classes;safe navigation;semantic deep reinforcement learning;semantic information;social crowds;specific danger zones","","","","30","IEEE","24 Jan 2023","","","IEEE","IEEE Conferences"
"Research on ATO Control Method for Urban Rail Based on Deep Reinforcement Learning","X. Chen; X. Guo; J. Meng; R. Xu; S. Li; D. Li","Mechatronics T&R Institute, Lanzhou Jiaotong University, Lanzhou, China; Mechatronics T&R Institute, Lanzhou Jiaotong University, Lanzhou, China; Mechatronics T&R Institute, Lanzhou Jiaotong University, Lanzhou, China; Mechatronics T&R Institute, Lanzhou Jiaotong University, Lanzhou, China; Mechatronics T&R Institute, Lanzhou Jiaotong University, Lanzhou, China; Mechatronics T&R Institute, Lanzhou Jiaotong University, Lanzhou, China","IEEE Access","23 Jan 2023","2023","11","","5919","5928","Aiming at the problems of punctuality, parking accuracy and energy saving of urban rail train operation, an intelligent control method for automatic train operation (ATO) based on deep Q network (DQN) is proposed. The train dynamics model is established under the condition of satisfying the safety principle and various constraints of automatic driving of urban rail train. Considering the transformation rules and sequences of working conditions between train stations, the agent in the DQN algorithm is used as the train controller to adjust the train automatic driving strategy in real time according to the train operating state and operating environment, and optimizes the generation of the train automatic driving curve. Taking the Beijing Yizhuang Subway line as an example, the simulation test results show that the DQN urban rail train control method reduces energy consumption by 12.32% compared with the traditional train PID control method, and improves the running punctuality and parking accuracy; at the same time, the DQN train automatically driving control method can adjust the train running state in real time and dynamically, and has good adaptability and robustness to the change of train running environment parameters.","2169-3536","","10.1109/ACCESS.2023.3236413","National Natural Science Foundation of China (NSFC)(grant numbers:72061021,62063013); Science and Technology Plan Project of Gansu(grant numbers:22JR11RA146,20JR10RA251,21JR7RA284); Youth Fund Project of Lanzhou Jiaotong University(grant numbers:2021018); 2022 Gansu Province Excellent Graduate ‘‘Innovation Star’’ Project(grant numbers:2022CXZX-517); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015721","Urban rail train;DQN algorithm;multi-objective optimization;automatic driving","Energy consumption;Railway transportation;Reinforcement learning;Heuristic algorithms;Target tracking;Real-time systems;Resource management;Urban areas;Public transportation","deep learning (artificial intelligence);energy conservation;energy consumption;intelligent control;rail traffic;rail traffic control;railways;reinforcement learning;three-term control","ATO control method;automatic train operation;deep Q network;deep reinforcement learning;DQN algorithm;DQN train;DQN urban rail train control method;intelligent control method;parking accuracy;traditional train PID control method;train automatic driving curve;train automatic driving strategy;train controller;train dynamics model;train operating state;train running state;train stations;urban rail train operation","","","","23","CCBY","12 Jan 2023","","","IEEE","IEEE Journals"
"Off-Policy Integral Reinforcement Learning Method to Solve Nonlinear Continuous-Time Multiplayer Nonzero-Sum Games","R. Song; F. L. Lewis; Q. Wei","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","20 May 2017","2017","28","3","704","713","This paper establishes an off-policy integral reinforcement learning (IRL) method to solve nonlinear continuous-time (CT) nonzero-sum (NZS) games with unknown system dynamics. The IRL algorithm is presented to obtain the iterative control and off-policy learning is used to allow the dynamics to be completely unknown. Off-policy IRL is designed to do policy evaluation and policy improvement in the policy iteration algorithm. Critic and action networks are used to obtain the performance index and control for each player. The gradient descent algorithm makes the update of critic and action weights simultaneously. The convergence analysis of the weights is given. The asymptotic stability of the closed-loop system and the existence of Nash equilibrium are proved. The simulation study demonstrates the effectiveness of the developed method for nonlinear CT NZS games with unknown system dynamics.","2162-2388","","10.1109/TNNLS.2016.2582849","Directorate for Biological Sciences through the National Science Foundation(grant numbers:ECCS-1128050); National Natural Science Foundation of China(grant numbers:61304079,61433004,61374105); Fundamental Research Funds for the Central Universities(grant numbers:FRF-TP-15-056A3); Open Research Project from State Key Laboratory of Management and Control for Complex Systems(grant numbers:20150104); Office of Naval Research(grant numbers:N00014-13-1-0562); Air Force Office of Scientific Research European Office of Aerospace Research and Development(grant numbers:13-3055); U.S. Army Research Office; W911NF-11-D-0001; China National Natural Science Foundation(grant numbers:61120106011); China Education Ministry Project 111(grant numbers:B08015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7517289","Adaptive critic designs;adaptive dynamic programming (ADP);approximate dynamic programming;integral reinforcement learning (IRL);nonlinear systems;nonzero sum (NZS);off-policy","Games;System dynamics;Optimal control;Nash equilibrium;Heuristic algorithms;Convergence;Mathematical model","asymptotic stability;closed loop systems;continuous time systems;convergence of numerical methods;game theory;gradient methods;learning (artificial intelligence);nonlinear control systems;performance index","off-policy integral reinforcement learning;nonlinear continuous-time multiplayer nonzero-sum games;unknown system dynamics;IRL algorithm;iterative control;policy iteration algorithm;action networks;Critic networks;performance index;gradient descent algorithm;critic networks;action weights;critic weights;convergence analysis;asymptotic stability;closed-loop system;Nash equilibrium;nonlinear CT NZS games","","147","","47","IEEE","20 Jul 2016","","","IEEE","IEEE Journals"
"A Deep Reinforcement Learning Approach to Ride-Sharing Vehicle Dispatching in Autonomous Mobility-on-Demand Systems","G. Guo; Y. Xu","Department of Automation, Dalian Maritime University, Dalian, China; Department of Automation, Dalian Maritime University, Dalian, China","IEEE Intelligent Transportation Systems Magazine","12 Jan 2022","2022","14","1","128","140","This paper investigates a ride-sharing vehicle dispatching and routing problem in ride-sharing autonomous mobility-on-demand systems. We present a new method that can optimize both operation cost and passenger quality of service in a global and farsighted view by leveraging the historical data. It comprises two parts, one for vehicle routing decision making and the other for request-vehicle assignment. In particular, the vehicle routing decision making procedure is formulated as a Markov decision process considering idle vehicle rebalancing, with properly designed states, actions, and rewards. By sampling the future requests according to the historical probability distribution, the look-ahead decision making is realized via a deep reinforcement learning framework, which is composed of a Convolutional Neural Network and a double deep Q-learning module. Then a request-vehicle assignment scheme is presented based on the learning value attained from vehicle routing. Satisfactory performances (e.g, service rate, average waiting time and travel distance) of the method are demonstrated by experimental results under various fleet sizes and different vehicle capacities.","1941-1197","","10.1109/MITS.2019.2962159","National Natural Science Foundation of China(grant numbers:61573077,U1808205); National Key Research and Development Program of China(grant numbers:2017YFA0700300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9052756","","Quality of service;Vehicle routing;Reinforcement learning;Dispatching;Vehicle dynamics;Deep learning;Routing","control engineering computing;convolutional neural nets;decision making;decision theory;deep learning (artificial intelligence);Markov processes;mobile robots;reinforcement learning;road vehicles;statistical distributions;traffic engineering computing;vehicle routing","ride-sharing vehicle dispatching;autonomous mobility-on-demand systems;operation cost;vehicle routing decision making;Markov decision process;idle vehicle rebalancing;look-ahead decision making;double deep Q-learning;request-vehicle assignment;vehicle capacities;deep reinforcement learning;passenger quality of service;historical probability distribution;convolutional neural network;service rate;average waiting time;travel distance;fleet sizes","","16","","24","IEEE","1 Apr 2020","","","IEEE","IEEE Magazines"
"Deep Reinforcement Learning Approach for Trading Automation in the Stock Market","T. Kabbani; E. Duman","Department of Industrial Engineering, Özyeðin University, Istanbul, Turkey; Department of Industrial Engineering, Özyeðin University, Istanbul, Turkey","IEEE Access","13 Sep 2022","2022","10","","93564","93574","Deep Reinforcement Learning (DRL) algorithms can scale to previously intractable problems. The automation of profit generation in the stock market is possible using DRL, by combining the financial assets price “prediction” step and the “allocation” step of the portfolio in one unified process to produce fully autonomous systems capable of interacting with their environment to make optimal decisions through trial and error. This work represents a DRL model to generate profitable trades in the stock market, effectively overcoming the limitations of supervised learning approaches. We formulate the trading problem as a Partially Observed Markov Decision Process (POMDP) model, considering the constraints imposed by the stock market, such as liquidity and transaction costs. We then solve the formulated POMDP problem using the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm reporting a 2.68 Sharpe Ratio on unseen data set (test data). From the point of view of stock market forecasting and the intelligent decision-making mechanism, this paper demonstrates the superiority of DRL in financial markets over other types of machine learning and proves its credibility and advantages in strategic decision-making.","2169-3536","","10.1109/ACCESS.2022.3203697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9877940","Autonomous agent;deep reinforcement learning;MDP;sentiment analysis;stock market;technical indicators;twin delayed deep deterministic policy gradient","Stock markets;Mathematical models;Supervised learning;Markov processes;Approximation algorithms;Q-learning;Portfolios;Deep learning;Reinforcement learning;Sentiment analysis;Autonomous systems","decision making;decision theory;investment;learning (artificial intelligence);Markov processes;optimisation;pricing;stock markets","Deep Reinforcement;trading automation;intractable problems;profit generation;possible using DRL;financial assets price prediction step;DRL model;profitable trades;supervised learning approaches;trading problem;Partially Observed Markov Decision Process model;formulated POMDP problem;Twin Delayed Deep Deterministic Policy Gradient algorithm;stock market forecasting;financial markets;machine learning","","5","","39","CCBYNCND","5 Sep 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Adaptive Optimal Exponential Tracking Control of Linear Systems With Unknown Dynamics","C. Chen; H. Modares; K. Xie; F. L. Lewis; Y. Wan; S. Xie","School of Automation, Guangdong University of Technology, Guangdong Key Laboratory of IoT Information Technology, Guangzhou, China; Mechanical Engineering Department, Michigan State University, East Lansing, USA; School of Automation, Guangdong University of Technology, Guangzhou, China; UTA Research Institute, The University of Texas at Arlington, Fort Worth, USA; UTA Research Institute, The University of Texas at Arlington, Fort Worth, USA; School of Automation, Guangdong University of Technology, Guangzhou, China","IEEE Transactions on Automatic Control","30 Oct 2019","2019","64","11","4423","4438","Reinforcement learning (RL) has been successfully employed as a powerful tool in designing adaptive optimal controllers. Recently, off-policy learning has emerged to design optimal controllers for systems with completely unknown dynamics. However, current approaches for optimal tracking control design either result in bounded tracking error, rather than zero tracking error, or require partial knowledge of the system dynamics. Moreover, they usually require to collect a large set of data to learn the optimal solution. To obviate these limitations, this paper applies a combination of off-policy learning and experience-replay for output regulation tracking control of continuous-time linear systems with completely unknown dynamics. To this end, the off-policy integral RL-based technique is first used to obtain the optimal control feedback gain, and to explicitly identify the involved system dynamics using the same data. Second, a data-efficient-based experience replay method is developed to compute the exosystem dynamics. Finally, the output regulator equations are solved using data measured online. It is shown that the proposed control method stabilizes the closed-loop tracking error dynamics, and gives an explicit exponential convergence rate for the output tracking error. Simulation results show the effectiveness of the proposed approach.","1558-2523","","10.1109/TAC.2019.2905215","National Natural Science Foundation of China(grant numbers:61703112,61703113,61333013,61727810,61633007); U.S. National Science Foundation(grant numbers:1839804); Office of Naval Research(grant numbers:N00014-17-1-2239,N00014-18-1-2221); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667332","Adaptive optimal control;exponential tracking control;reinforcement learning (RL);output regulation","Optimal control;Regulators;System dynamics;Heuristic algorithms;Adaptive systems;Linear systems;Trajectory","adaptive control;closed loop systems;continuous time systems;control engineering computing;control system synthesis;learning (artificial intelligence);linear systems;optimal control","reinforcement learning-based adaptive optimal exponential tracking control;adaptive optimal controllers;off-policy learning;optimal tracking control design;bounded tracking error;zero tracking error;output regulation tracking control;continuous-time linear systems;off-policy integral RL-based technique;optimal control feedback gain;data-efficient-based experience replay method;exosystem dynamics;closed-loop tracking error dynamics;experience-replay","","107","","54","IEEE","14 Mar 2019","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Demand Response for Smart Facilities Energy Management","R. Lu; R. Bai; Z. Luo; J. Jiang; M. Sun; H. -T. Zhang","Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; Tsinghua-Berkeley Shenzhen Institute (TBSI), Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Department of Electronic Systems Engineering, Hanyang University, Ansan, South Korea; State Key Laboratory of Industrial Control Technology, Department of Control Science and Engineering, Zhejiang University, Hangzhou, China; Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Industrial Electronics","4 Mar 2022","2022","69","8","8554","8565","This work proposes a novel deep reinforcement learning (DRL)-based demand response algorithm for smart facilities energy management to minimize electricity costs while maintaining a satisfaction index. Specifically, to accommodate the characteristics of the decision-making problem, long short-term memory (LSTM) units are adopted to extract discriminative features from past electricity price sequences and fed into fully connected multi-layer perceptrons (MLPs) with the measured energy and time information, then a deep Q-network is developed to approximate the optimal policy. After that, an experimental setup is constructed to investigate the effectiveness of the proposed DRL-based demand response algorithm to bridge the gap between theoretical studies and practical implementations. Numerical results demonstrate that the proposed algorithm can handle energy management well for multiple smart facilities. Moreover, the proposed algorithm outperforms the model predictive control (MPC) strategy and uncontrolled solution and is close to the theoretical optimal control method.","1557-9948","","10.1109/TIE.2021.3104596","National Natural Science Foundation of China(grant numbers:62003143,U20A20159,51729501,U1713203); National Natural Science Foundation of Hubei Province(grant numbers:2019CFA005); Fundamental Research Funds for the Central Universities; HUST(grant numbers:2020kfyXJJS084,2019KFYXMBZ032,2020kfyXGYJ113); State Key Laboratory Of Alternate Electrical Power System With Renewable Energy Sources(grant numbers:LAPS21006); Key Laboratory of Industrial Internet of Things and Networked Control(grant numbers:2020FF02); Open Research Project of the State Key Laboratory of Industrial Control Technology(grant numbers:ICT2021B32); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519534","Deep reinforcement learning (DRL);demand response;energy management;experimental validation;smart facility","Energy management;Load management;Feature extraction;Approximation algorithms;Predictive models;Prediction algorithms;Power system stability","building management systems;decision making;energy management systems;learning (artificial intelligence);multilayer perceptrons;optimal control;predictive control;pricing","time information;deep Q-network;DRL-based demand response algorithm;multiple smart facilities;smart facilities energy management;deep reinforcement learning-based demand response algorithm;decision-making problem;short-term memory units;electricity price sequences;measured energy","","12","","32","IEEE","19 Aug 2021","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Framework-Based Flow Rate Rejection Control of Soft Magnetic Miniature Robots","M. Cai; Q. Wang; Z. Qi; D. Jin; X. Wu; T. Xu; L. Zhang","Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong, China; School of Mechanical Engineering and Jiangsu Key Laboratory for Design and Manufacture of Micro-Nano Biomedical Instruments, Southeast University, Nanjing, China; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong, China; School of Materials Science and Engineering, Harbin Institute of Technology, Shenzhen, China; Shenzhen Institute of Advanced Technology, Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institute of Advanced Technology, Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Chinese Academy of Sciences, Shenzhen, China; Department of Mechanical and Automation Engineering, Chow Yuk Ho Technology Centre for Innovative Medicine, and CUHK T Stone Robotics Institute, The Chinese University of Hong Kong, Hong Kong, China","IEEE Transactions on Cybernetics","","2022","PP","99","1","13","Soft magnetic miniature robots (SMMRs) have potential biomedical applications due to their flexible size and mobility to access confined environments. However, navigating the robot to a goal site with precise control performance and high repeatability in unstructured environments, especially in flow rate conditions, still remains a challenge. In this study, drawing inspiration from the control requirements of drug delivery and release to the goal lesion site in the presence of dynamic biofluids, we propose a flow rate rejection control strategy based on a deep reinforcement learning (DRL) framework to actuate an SMMR to achieve goal-reaching and hovering in fluidic tubes. To this end, an SMMR is first fabricated, which can be operated by an external magnetic field to realize its desired functionalities. Subsequently, a simulator is constructed based on neural networks to map the relationship between the applied magnetic field and robot locomotion states. With minimal prior knowledge about the environment and dynamics, a gated recurrent unit (GRU)-based DRL algorithm is formulated by considering the designed history state–action and estimated flow rates. In addition, the randomization technique is applied during training to distill the general control policy for the physical SMMR. The results of numerical simulations and experiments are illustrated to demonstrate the robustness and efficacy of the presented control framework. Finally, in-depth analyses and discussions indicate the potentiality of DRL for soft magnetic robots in biomedical applications.","2168-2275","","10.1109/TCYB.2022.3199213","Croucher Foundation(grant numbers:CAS20403); Hong Kong RGC(grant numbers:RFS2122-4S03); CUHK internal grants; Fundamental Research Funds for the Central Universities(grant numbers:2242022R10051); Natural Science Foundation of Jiangsu Province(grant numbers:BK20220834); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880935","Deep reinforcement learning (DRL);flow rates;magnetic control;soft magnetic robot;visual servoing","Robots;Soft magnetic materials;Magnetosphere;Task analysis;Navigation;Heuristic algorithms;Uncertainty","","","","4","","","IEEE","7 Sep 2022","","","IEEE","IEEE Early Access Articles"
"Reinforcement Learning-Based Fractional-Order Adaptive Fault-Tolerant Formation Control of Networked Fixed-Wing UAVs With Prescribed Performance","Z. Yu; J. Li; Y. Xu; Y. Zhang; B. Jiang; C. -Y. Su","College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Mechanical, Industrial and Aerospace Engineering, Concordia University, Montreal, Canada; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Mechanical, Industrial and Aerospace Engineering, Concordia University, Montreal, Canada","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","15","This article investigates the fault-tolerant formation control (FTFC) problem for networked fixed-wing unmanned aerial vehicles (UAVs) against faults. To constrain the distributed tracking errors of follower UAVs with respect to neighboring UAVs in the presence of faults, finite-time prescribed performance functions (PPFs) are developed to transform the distributed tracking errors into a new set of errors by incorporating user-specified transient and steady-state requirements. Then, the critic neural networks (NNs) are developed to learn the long-term performance indices, which are used to evaluate the distributed tracking performance. Based on the generated critic NNs, actor NNs are designed to learn the unknown nonlinear terms. Moreover, to compensate for the reinforcement learning errors of actor-critic NNs, nonlinear disturbance observers (DOs) with skillfully constructed auxiliary learning errors are developed to facilitate the FTFC design. Furthermore, by using the Lyapunov stability analysis, it is shown that all follower UAVs can track the leader UAV with predesigned offsets, and the distributed tracking errors are finite-time convergent. Finally, comparative simulation results are presented to show the effectiveness of the proposed control scheme.","2162-2388","","10.1109/TNNLS.2023.3281403","National Natural Science Foundation of China(grant numbers:62003162,61833013,62020106003,62233009); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20200416,BK20222012); National Key Research and Development Program of China(grant numbers:2021YFB3301300); Science and Technology on Space Intelligent Control Laboratory(grant numbers:HTKJ2022KL502015); Industry-University Research Innovation Foundation for the Chinese Ministry of Education(grant numbers:2021ZYA02005); 111 Project(grant numbers:B20007); Postgraduate Research and Practice Innovation Program of Nanjing University of Aeronautics and Astronautics (NUAA)(grant numbers:xcxjh20220325); Aeronautical Science Foundation of China(grant numbers:20200007018001); Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10149179","Disturbance observer (DO);fault-tolerant formation control (FTFC);finite-time prescribed performance;fixed-wing unmanned aerial vehicle (UAV);fractional-order (FO) control;reinforcement learning control","Artificial neural networks;Reinforcement learning;Fault tolerant systems;Fault tolerance;Transient analysis;Steady-state;Formation control","","","","","","","IEEE","13 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Hierarchical Reinforcement Learning Integrating With Human Knowledge for Practical Robot Skill Learning in Complex Multi-Stage Manipulation","X. Liu; G. Wang; Z. Liu; Y. Liu; Z. Liu; P. Huang","School of Astronautics, Research Center for Intelligent Robotics and the National Key Laboratory of Aerospace Flight Dynamics, Northwestern Polytechnical University, Xi’an, China; School of Astronautics, Research Center for Intelligent Robotics and the National Key Laboratory of Aerospace Flight Dynamics, Northwestern Polytechnical University, Xi’an, China; School of Astronautics, Research Center for Intelligent Robotics and the National Key Laboratory of Aerospace Flight Dynamics, Northwestern Polytechnical University, Xi’an, China; Beijing Institute of Tracking and Telecommunications Technology (BITTT), Beijing, China; School of Astronautics, Research Center for Intelligent Robotics and the National Key Laboratory of Aerospace Flight Dynamics, Northwestern Polytechnical University, Xi’an, China; School of Astronautics, Research Center for Intelligent Robotics and the National Key Laboratory of Aerospace Flight Dynamics, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","11","This paper proposes a novel hierarchical reinforcement learning (HRL) framework of complex manipulation tasks which integrates the human prior knowledge. The framework involves the following steps:  $ \textbf{1)} $  The manipulation process is divided into several stages based on human prior knowledge.  $ \textbf{2)} $  Transition conditions between stages are determined in the form of “if-then” rules.  $ \textbf{3)} $  The key features of each stage are selected, and the corresponding control policies are designed via human knowledge.  $ \textbf{4)} $  The policy gradient with parameter-based exploration (PGPE) method is employed to optimize the policy parameters because it does not require the policy to be derivable for the parameters. To increase the convergence speed of this framework, importance sampling and adaptive adjustment of exploration variance are employed to improve it.  $ \textbf{5)} $  On this basis, to facilitate the transfer of simulation results to practical experiments, half sim-to-real method is presented, which fully utilizes the simulation results, and the differences between simulation and experimental environments are considered. Simulation and experimental studies show that our framework can deal with the peg-hole-insertion task with a high quality in less than 1600 episodes and can safely adapt the skill into the practical scene with little iterations, which verify the efficiency of the presented method. Note to Practitioners—Intelligent robots will become the right assistants of human beings in the future, especially in various areas of complex manipulation occasions. The important premise is that the robots should have certain ability of complex manipulation skill learning. Complex manipulation tasks can be decomposed into multiple stages, and HRL is a suitable and efficient method for solving this kind of problems. This paper proposes a novel HRL framework which can better integrate the human prior knowledge. In addition, improved PGPE method is proposed to obtain the optimized policy parameters more quickly. More importantly, a novel half sim-to-real transfer method is presented to better integrate the simulation and experiment results. This provides a common paradigm, which leverages the simulation results to reduce the interaction between robot and practical environment and then utilizes the experiment results to further optimize the policy parameters.","1558-3783","","10.1109/TASE.2023.3288037","National Natural Science Foundation of China(grant numbers:62103334,62273280); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10184998","Hierarchical reinforcement learning;improved PGPE;human knowledge;robot manipulation skill learning;half sim-to-real","Reinforcement learning;Robots;Task analysis;Simulation;Adaptation models;Manipulator dynamics;Learning systems","","","","","","","IEEE","17 Jul 2023","","","IEEE","IEEE Early Access Articles"
"A Two-Level Energy Management Strategy for Multi-Microgrid Systems With Interval Prediction and Reinforcement Learning","L. Xiong; Y. Tang; S. Mao; H. Liu; K. Meng; Z. Dong; F. Qian","Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, NSW, Australia; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, NSW, Australia; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, NSW, Australia; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China","IEEE Transactions on Circuits and Systems I: Regular Papers","30 Mar 2022","2022","69","4","1788","1799","Setting retail electricity prices is one of the significant strategies for energy management of multi-microgrid (MMG) systems integrated with renewable energy. Nevertheless, the need of privacy preservation, the uncertainties of renewable energy and loads, as well as the time-varying scenarios, bring challenges for pricing problems. In this paper, a two-level pricing framework is proposed based on interval predictions and model-free reinforcement learning to address these challenges. In particular, at the higher level, the distribution system operator (DSO) is viewed as an agent, which sets retail electricity prices without detailed user information for privacy protection to maximize the total revenue from selling energy with reinforcement learning. For time-varying scenarios with intermittent photovoltaic power generation and diverse loads, a differentiable trust region layer is considered in reinforcement learning to improve the robustness of the policy updating process. While at the lower level, operators in microgrids solve three-phase unbalanced optimal power flow (OPF) problems to minimize generation cost and network power loss. Additionally, to deal with the challenges from the uncertainties of renewable power generation and user loads, interval predictions are chosen to quantify prediction errors and improve the flexibility of pricing policies. Finally, a set of experiments are conducted to validate the effectiveness of the proposed method for pricing problems in MMG systems.","1558-0806","","10.1109/TCSI.2022.3141229","National Natural Science Foundation of China (Key Program)(grant numbers:62136003); National Natural Science Foundation of China(grant numbers:22178103,61973124); International (Regional) Cooperation and Exchange Project(grant numbers:61720106008); Program of Shanghai Academic Research Leader(grant numbers:20XD1401300); Chinesisch-Deutsche Zentrum für Wissenschaftsförderung(grant numbers:M-0066); Australian Research Council Future Fellow Grant(grant numbers:FT200100369); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684239","Multi-microgrids;reinforcement learning;interval prediction;trust region layer","Microgrids;Pricing;Predictive models;Optimization;Reinforcement learning;Energy management;Privacy","cost reduction;distributed power generation;energy management systems;load flow;photovoltaic power systems;power engineering computing;power generation economics;pricing;reinforcement learning;renewable energy sources;retailing;time-varying systems","reinforcement learning;time-varying scenarios;intermittent photovoltaic power generation;diverse loads;network power loss;renewable power generation;user loads;interval prediction;pricing policies;MMG systems;two-level energy management strategy;multimicrogrid systems;retail electricity prices;renewable energy;distribution system operator;DSO;three-phase unbalanced optimal power flow;OPF;generation cost reduction;interval predictions","","20","","42","IEEE","17 Jan 2022","","","IEEE","IEEE Journals"
"Legs that can walk: embodiment-based modular reinforcement learning applied","D. Jacob; D. Polani; C. L. Nehaniv","Adaptive Systems Research Group, University of Herfordshire, Hatfield, Hertfordshire, UK; Adaptive Systems Research Group, University of Herfordshire, Hatfield, Hertfordshire, UK; Adaptive Systems Research Group, University of Herfordshire, Hatfield, Hertfordshire, UK","2005 International Symposium on Computational Intelligence in Robotics and Automation","12 Dec 2005","2005","","","365","372","Experiments to illustrate a novel methodology for reinforcement learning in embodied physical agents are described. A simulated legged robot is decomposed into structure-based modules following the authors' EMBER principles of local sensing, action and learning. The legs are individually trained to 'walk' in isolation, and re-attached to the robot; walking is then sufficiently stable that learning in situ can continue. The experiments demonstrate the benefits of the modular decomposition: state-space factorisation leads to faster learning, in this case to the extent that an otherwise intractable problem becomes learnable.","","0-7803-9355-4","10.1109/CIRA.2005.1554304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1554304","","Leg;Learning;Legged locomotion;Robot sensing systems;Jacobian matrices;Adaptive systems;Robot control;Actuators;Robotic assembly;Centralized control","legged locomotion;learning (artificial intelligence);state-space methods","reinforcement learning;embodied physical agent;legged robot;structure-based module;EMBER principle;modular decomposition;state-space factorisation","","6","","23","IEEE","12 Dec 2005","","","IEEE","IEEE Conferences"
