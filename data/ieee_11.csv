"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Reinforcement learning for spoken dialogue systems using off-policy natural gradient method","F. Jurčíček","Faculty of Mathematics and Physics, Charles University in Prague, Praha, Czech Republic","2012 IEEE Spoken Language Technology Workshop (SLT)","31 Jan 2013","2012","","","7","12","Reinforcement learning methods have been successfully used to optimise dialogue strategies in statistical dialogue systems. Typically, reinforcement techniques learn on-policy i.e., the dialogue strategy is updated online while the system is interacting with a user. An alternative to this approach is off-policy reinforcement learning, which estimates an optimal dialogue strategy offline from a fixed corpus of previously collected dialogues. This paper proposes a novel off-policy reinforcement learning method based on natural policy gradients and importance sampling. The algorithm is evaluated on a spoken dialogue system in the tourist information domain. The experiments indicate that the proposed method learns a dialogue strategy, which significantly outperforms the baseline handcrafted dialogue policy.","","978-1-4673-5126-3","10.1109/SLT.2012.6424161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424161","POMDP;dialogue management;policy gradient methods;off-policy reinforcement learning","Learning;Training;History;Gradient methods;Stochastic processes;Linear approximation","gradient methods;importance sampling;interactive systems;learning (artificial intelligence);optimisation;speech-based user interfaces;travel industry","spoken dialogue systems;off-policy natural gradient method;dialogue strategy optimisation;statistical dialogue systems;optimal dialogue strategy;off-policy reinforcement learning method;importance sampling;tourist information domain","","","","18","IEEE","31 Jan 2013","","","IEEE","IEEE Conferences"
"Reinforcement learning control for gas collector pressure of coke ovens","Q. Bin; D. Leqiang; L. Pengcheng; Z. Wangli; W. Xin","School of Electrical & Information Engineering, Hunan University of Technology, Zhuzhou, Hunan, China; School of Electrical & Information Engineering, Hunan University of Technology, Zhuzhou, Hunan, China; School of Electrical & Information Engineering, Hunan University of Technology, Zhuzhou, Hunan, China; School of Electrical & Information Engineering, Hunan University of Technology, Zhuzhou, Hunan, China; School of Electrical & Information Engineering, Hunan University of Technology, Zhuzhou, Hunan, China","The 26th Chinese Control and Decision Conference (2014 CCDC)","14 Jul 2014","2014","","","414","417","Due to nonlinear and strong coupling of coke ovens, a distributed reinforcement learning control algorithm based on a radial basis function (RBF) for gas collector pressure control was proposed, in which the discrete space was dealt with by RBF and the critic-actor method was used to update the parameters of search network and evaluation network of the controller. Moreover, the control strategies of the controller were optimized by trial and error in the strong interference environment. The simulation of gas collector pressure control of coke ovens has shown that the algorithm can effectively solve the coupling problem, and has better stability.","1948-9447","978-1-4799-3708-0","10.1109/CCDC.2014.6852183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6852183","coke oven;gas collector pressure;coupling control;reinforcement learning","Ovens;Learning (artificial intelligence);Couplings;Pressure control;Valves;Educational institutions;Interference","coke;control engineering computing;fuel processing industries;learning (artificial intelligence);nonlinear control systems;ovens;pressure control;radial basis function networks;stability","coke ovens;nonlinear coupling;strong coupling;distributed reinforcement learning control;radial basis function;RBF;gas collector pressure control;critic-actor method;stability","","","","12","IEEE","14 Jul 2014","","","IEEE","IEEE Conferences"
"Reinforcement Learning Output Feedback NN Control Using Deterministic Learning Technique","B. Xu; C. Yang; Z. Shi","School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Northwestern Polytechnical University, Xi'an, China","IEEE Transactions on Neural Networks and Learning Systems","20 May 2017","2014","25","3","635","641","In this brief, a novel adaptive-critic-based neural network (NN) controller is investigated for nonlinear pure-feedback systems. The controller design is based on the transformed predictor form, and the actor-critic NN control architecture includes two NNs, whereas the critic NN is used to approximate the strategic utility function, and the action NN is employed to minimize both the strategic utility function and the tracking error. A deterministic learning technique has been employed to guarantee that the partial persistent excitation condition of internal states is satisfied during tracking control to a periodic reference orbit. The uniformly ultimate boundedness of closed-loop signals is shown via Lyapunov stability analysis. Simulation results are presented to demonstrate the effectiveness of the proposed control.","2162-2388","","10.1109/TNNLS.2013.2292704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681972","Approximate dynamic programming;discrete-time system;output feedback control;pure-feedback system;radial basis function neural network (RBF NN)","Artificial neural networks;Approximation methods;Discrete-time systems;Learning systems;Output feedback;Nonlinear systems","adaptive control;control system synthesis;feedback;learning (artificial intelligence);Lyapunov methods;neurocontrollers;nonlinear control systems;stability","reinforcement learning output feedback NN control;deterministic learning technique;adaptive-critic-based neural network controller;nonlinear pure-feedback systems;controller design;predictor form;actor-critic NN control architecture;strategic utility function;partial persistent excitation condition;internal states;periodic reference orbit tracking control;closed-loop signal uniformly ultimate boundedness;Lyapunov stability analysis","Artificial Intelligence;Computer Simulation;Feedback;Humans;Models, Neurological;Nonlinear Dynamics;Online Systems;Reinforcement (Psychology);Time Factors","208","","26","IEEE","11 Dec 2013","","","IEEE","IEEE Journals"
"StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning","K. Shao; Y. Zhu; D. Zhao","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Emerging Topics in Computational Intelligence","21 Jan 2019","2019","3","1","73","84","Real-time strategy games have been an important field of game artificial intelligence in recent years. This paper presents a reinforcement learning and curriculum transfer learning method to control multiple units in StarCraft micro management. We define an efficient state representation, which breaks down the complexity caused by the large state space in the game environment. Then, a parameter sharing multi-agent gradient descent Sarsa(λ) algorithm is proposed to train the units. The learning policy is shared among our units to encourage cooperative behaviors. We use a neural network as a function approximator to estimate the action-value function, and propose a reward function to help units balance their move and attack. In addition, a transfer learning method is used to extend our model to more difficult scenarios, which accelerates the training process and improves the learning performance. In small-scale scenarios, our units successfully learn to combat and defeat the built-in AI with 100% win rates. In large-scale scenarios, the curriculum transfer learning method is used to progressively train a group of units, and it shows superior performance over some baseline methods in target scenarios. With reinforcement learning and curriculum transfer learning, our units are able to learn appropriate strategies in StarCraft micro management scenarios.","2471-285X","","10.1109/TETCI.2018.2823329","National Natural Science Foundation of China(grant numbers:61573353,61603382,61533017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8351991","Reinforcement learning;transfer learning;curriculum learning;neural network;game AI","Games;Learning (artificial intelligence);Learning systems;Training;Neural networks;Machine learning","computer games;function approximation;gradient methods;learning (artificial intelligence);neural nets","game artificial intelligence;reinforcement learning;curriculum transfer learning method;learning policy;learning performance;StarCraft micromanagement scenarios;realtime strategy games;multiagent gradient descent Sarsa algorithm;neural network;function approximator;action-value function;reward function","","89","","63","IEEE","27 Apr 2018","","","IEEE","IEEE Journals"
"SurRoL: An Open-source Reinforcement Learning Centered and dVRK Compatible Platform for Surgical Robot Learning","J. Xu; B. Li; B. Lu; Y. -H. Liu; Q. Dou; P. -A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Mechanical and Automation Engineering, T Stone Robotics Institute, The Chinese University of Hong Kong; Department of Mechanical and Automation Engineering, T Stone Robotics Institute, The Chinese University of Hong Kong; Department of Mechanical and Automation Engineering, T Stone Robotics Institute, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong","2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","16 Dec 2021","2021","","","1821","1828","Autonomous surgical execution relieves tedious routines and surgeon’s fatigue. Recent learning-based methods, especially reinforcement learning (RL) based methods, achieve promising performance for dexterous manipulation, which usually requires the simulation to collect data efficiently and reduce the hardware cost. The existing learning-based simulation platforms for medical robots suffer from limited scenarios and simplified physical interactions, which degrades the real-world performance of learned policies. In this work, we designed SurRoL, an RL-centered simulation platform for surgical robot learning compatible with the da Vinci Research Kit (dVRK). The designed SurRoL integrates a user-friendly RL library for algorithm development and a real-time physics engine, which is able to support more PSM/ECM scenarios and more realistic physical interactions. Ten learning-based surgical tasks are built in the platform, which are common in the real autonomous surgical execution. We evaluate SurRoL using RL algorithms in simulation, provide in-depth analysis, deploy the trained policies on the real dVRK, and show that our SurRoL achieves better transferability in the real world.","2153-0866","978-1-6654-1714-3","10.1109/IROS51168.2021.9635867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635867","","Learning systems;Medical robotics;Reinforcement learning;Real-time systems;Libraries;Hardware;Task analysis","dexterous manipulators;medical robotics;public domain software;reinforcement learning;surgery;telerobotics","RL-centered simulation platform;surgical robot learning;da Vinci Research Kit;user-friendly RL library;realistic physical interactions;learning-based surgical tasks;autonomous surgical execution;dVRK compatible platform;medical robots;learned policies;learning-based simulation platforms;open-source reinforcement learning;SurRoL","","19","","37","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Online adaptive learning of optimal control solutions using integral reinforcement learning","K. G. Vamvoudakis; D. Vrabie; F. L. Lewis","Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA; Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA; Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA","2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)","28 Jul 2011","2011","","","250","257","In this paper we introduce an online algorithm that uses integral reinforcement knowledge for learning the continuous-time optimal control solution for nonlinear systems with infinite horizon costs and partial knowledge of the system dynamics. This algorithm is a data based approach to the solution of the Hamilton-Jacobi-Bellman equation and it does not require explicit knowledge on the system's drift dynamics. The adaptive algorithm is based on policy iteration, and it is implemented on an actor/critic structure. Both actor and critic neural networks are adapted simultaneously a persistence of excitation condition is required to guarantee convergence of the critic to the actual optimal value function. Novel tuning algorithms are given for both critic and actor networks, with extra terms in the actor tuning law being required to guarantee closed-loop dynamical stability. The convergence to the optimal controller is proven, and stability of the system is also guaranteed. Simulation examples support the theoretical result.","2325-1867","978-1-4244-9888-8","10.1109/ADPRL.2011.5967359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967359","","","adaptive control;closed loop systems;continuous time systems;iterative methods;Jacobian matrices;learning (artificial intelligence);learning systems;neurocontrollers;nonlinear control systems;optimal control;stability","online adaptive learning;continuous-time optimal control solution;integral reinforcement learning;integral reinforcement knowledge;nonlinear systems;infinite horizon costs;system dynamics;Hamilton-Jacobi-Bellman equation;policy iteration;actor neural networks;critic neural networks;tuning algorithms;closed-loop dynamical stability","","13","","21","IEEE","28 Jul 2011","","","IEEE","IEEE Conferences"
"Recommendation System using Reinforcement Learning for What-If Simulation in Digital Twin","F. Pires; B. Ahmad; A. P. Moreira; P. Leitão","Research Centre in Digitalization and Intelligent Robotics (CeDRI), Instituto Politécnico de Bragança, Bragança, Portugal; University of Warwick, Warwick Manufacturing Group (WMG), Coventry, UK; INESC TEC - INESC Technology and Science, Faculty of Engineering, University of Porto, Porto, Portugal; Research Centre in Digitalization and Intelligent Robotics (CeDRI), Instituto Politécnico de Bragança, Bragança, Portugal","2021 IEEE 19th International Conference on Industrial Informatics (INDIN)","11 Oct 2021","2021","","","1","6","The research about the digital twin concept is growing worldwide, especially in the industrial sector, due to the increasing digitisation level associated to Industry 4.0. The application of the digital twin concept improves performance of a system by implementing monitoring, diagnosis, optimisation, and decision support actions. In particular, the decision-making process is very time consuming since the decision-maker is presented with hundreds of different scenarios that can be simulated and assessed in a what-if perspective. Bearing this in mind, this paper proposes to integrate a digital twin-based what-if simulation with a recommendation system to improve the decision-making cycle. The recommendation system is based on a reinforcement learning technique and takes user knowledge of the system into consideration and trust in the system recommendation. The applicability of the proposed approach is presented in an assembly line case study for recommending the best configurations for the system operation, in terms of the optimal number of AGVs (Autonomous Guided Vehicles) in various scenarios. The achieved results show its successful application and highlight the benefits of using AI-based recommendation systems for what-if simulation in digital twin systems.","","978-1-7281-4395-8","10.1109/INDIN45523.2021.9557372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9557372","Digital Twin;What-if Simulation;Recommendation Systems;Reinforcement Learning","Digital twin;Systems operation;Conferences;Decision making;Reinforcement learning;Informatics;Optimization","automatic guided vehicles;decision making;decision support systems;learning (artificial intelligence);production engineering computing;recommender systems","system operation;AI-based recommendation systems;digital twin systems;recommendation system;digital twin concept;industrial sector;decision support actions;decision-making process;digital twin;decision-making cycle;reinforcement learning technique;system recommendation;digitisation level;AGV;autonomous guided vehicles;Industry 4.0","","5","","24","IEEE","11 Oct 2021","","","IEEE","IEEE Conferences"
"Learning to Navigate for Mobile Robot with Continual Reinforcement Learning","N. Wang; D. Zhang; Y. Wang","Department of Automation, University of Science and Technology of China, Hefei, P. R. China; Department of Automation, University of Science and Technology of China, Hefei, P. R. China; Department of Automation, University of Science and Technology of China, Hefei, P. R. China","2020 39th Chinese Control Conference (CCC)","9 Sep 2020","2020","","","3701","3706","Autonomous navigation of mobile ground robots is a promising research topic due to its extensive applications. Existing works are difficult to deal with obstacle-cluttered problems and often do not generalize well because of the differences between training scenarios and practical environments. In this paper, we present an end-to-end motion planning model that has the ability to navigate the mobile robot to the given destination safely in an unknown environment without any prior knowledge. Specifically, with the application of continual reinforcement learning, our approach is able to learn subtasks in a sequential fashion by decomposing the arbitrarily complex navigating task. The experiments show that our approach is more effective and can be directly transferable to other unseen environments.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9188558","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9188558","Navigation;Mobile Robot;Reinforcement Learning;Continual Learning","Task analysis;Learning (artificial intelligence);Mobile robots;Navigation;Training;Neural networks","collision avoidance;learning (artificial intelligence);mobile robots;navigation","continual reinforcement learning;autonomous navigation;mobile ground robots;obstacle-cluttered problems;training scenarios;end-to-end motion planning model;unknown environment;arbitrarily complex navigating task","","3","","19","","9 Sep 2020","","","IEEE","IEEE Conferences"
"Explicitly Learning Policy Under Partial Observability in Multiagent Reinforcement Learning","C. Yang; G. Yang; H. Chen; J. Zhang","School of Artificial Intelligence, University of Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences","2023 International Joint Conference on Neural Networks (IJCNN)","2 Aug 2023","2023","","","1","8","We explore explicit solutions for multiagent reinforcement learning (MARL) under the constraint of partial observability. With a general framework of centralized training with decentralized execution (CTDE), existing methods implicitly alleviate partial observability by introducing global information during centralized training. However, such implicit solution cannot well address partial observability and shows low sample efficiency in many MARL problems. In this paper, we focus on the influence of partial observability on the policy of agents, and formally derive an ideal form of policy that maximizes MARL objective under partial observability. Furthermore, we develop a new method named Explicitly Learning Policy (ELP), which adopts a novel teacher-student structure and utilizes knowledge distillation to explicitly learn individual policy under partial observability for each agent. Compared to prior methods, ELP presents a more general and interpretable training process, and the procedure of ELP can be easily extended to existing methods for performance boost. Our empirical experiments on StarCraft II micromanagement benchmark show that ELP significantly outperforms prevailing state-of-the-art baselines, which demonstrates the advantage of ELP in addressing partial observability and improving sample efficiency.","2161-4407","978-1-6654-8867-9","10.1109/IJCNN54540.2023.10191476","CAS(grant numbers:JCPYJJ-22017); National Natural Science Foundation of China(grant numbers:61876181); Chinese Academy of Science(grant numbers:QYZDB-SSW-JSC006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191476","Multiagent reinforcement learning;partial observability;knowledge distillation","Training;Analytical models;Neural networks;Reinforcement learning;Benchmark testing;Observability","computer games;learning (artificial intelligence);multi-agent systems;reinforcement learning","Explicitly Learning Policy;improving sample efficiency;multiagent reinforcement learning;partial observability","","","","40","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Benchmarking Feature Extractors for Reinforcement Learning-Based Semiconductor Defect Localization","E. Dehaerne; B. Dey; S. Halder; S. De Gendt","Faculty of Science, KU Leuven, Leuven, Belgium; Interuniversity Microelectronics Centre (imec), Leuven, Belgium; Interuniversity Microelectronics Centre (imec), Leuven, Belgium; Faculty of Science, KU Leuven, Leuven, Belgium","2023 International Symposium ELMAR","22 Sep 2023","2023","","","49","53","As semiconductor patterning dimensions shrink, more advanced Scanning Electron Microscopy (SEM) image-based defect inspection techniques are needed. Recently, many Machine Learning (ML)-based approaches have been proposed for defect localization and have shown impressive results. These methods often rely on feature extraction from a full SEM image and possibly a number of regions of interest. In this study, we propose a deep Reinforcement Learning (RL)-based approach to defect localization which iteratively extracts features from increasingly smaller regions of the input image. We compare the results of 18 agents trained with different feature extractors. We discuss the advantages and disadvantages of different feature extractors as well as the RL-based framework in general for semiconductor defect localization.","2835-3781","979-8-3503-2512-6","10.1109/ELMAR59410.2023.10253916","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10253916","Feature extraction;metrology;object detection;reinforcement learning;semiconductor device manufacture","Location awareness;Deep learning;Scanning electron microscopy;Reinforcement learning;Inspection;Benchmark testing;Feature extraction","automatic optical inspection;deep learning (artificial intelligence);feature extraction;inspection;learning (artificial intelligence);reinforcement learning;scanning electron microscopy","deep Reinforcement;different feature extractors;Electron Microscopy image-based defect inspection techniques;feature extraction;increasingly smaller regions;input image;Reinforcement Learning-based semiconductor defect localization;RL-based framework;SEM image;semiconductor patterning dimensions","","","","26","IEEE","22 Sep 2023","","","IEEE","IEEE Conferences"
"Distance-Controllable Long Jump of Quadruped Robot Based on Parameter Optimization Using Deep Reinforcement Learning","Q. Liu; D. Xu; B. Yuan; Z. Mou; M. Wang","Key Laboratory of Metallurgical Equipment and Control Technology, Ministry of Education, Wuhan University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Mechanical Transmission and Manufacturing Engineering, Wuhan University of Science and Technology, Wuhan, China; Precision Manufacturing Institute, Wuhan University of Science and Technology, Wuhan, China; Precision Manufacturing Institute, Wuhan University of Science and Technology, Wuhan, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China","IEEE Access","14 Sep 2023","2023","11","","98566","98577","Quadruped robots interact with the ground with discrete foot points during locomotion, which makes them gain an advantage in obstacle crossing compared with the wheeled and tracked robots. Quadruped robots can jump from current position to one position a certain distance ahead to negotiate the obstacles between them, for example. However, current quadruped control strategies usually assume that the landing area is large enough, and thus jumping distance control of quadruped robots had not yet been studied sufficiently. This paper proposes a method for controlling the distance of quadruped robot jumps based on deep reinforcement learning (DRL). In the method, kinematic parameters in the control module are optimized to achieve the quadruped jumping tasks. Based on the understanding of the kinematics and dynamics of quadruped robot jumping, an initial jumping is realized by controlling the robot foot moving along a carefully designed parameterized trajectory. This initial trajectory is then used to train a set of jumping parameters using a deep reinforcement learning (DRL) algorithm. Through thousands of jumping trials in the Gazebo simulation environment, the optimal parameters were acquired. Our proposed method allows for accurate jumping within the 0.5 m to 0.8 m range. Additionally, the controller has been successfully implemented on a real quadruped robot.","2169-3536","","10.1109/ACCESS.2023.3313637","National Natural Science Foundation of China(grant numbers:51805381); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10246264","Quadruped robots;trajectory planning;deep reinforcement learning;jumping control","Robots;Quadrupedal robots;Legged locomotion;Robot kinematics;Trajectory;Foot;Quadrupedal robots;Trajectory planning;Reinforcement learning;Deep learning","","","","","","38","CCBYNCND","11 Sep 2023","","","IEEE","IEEE Journals"
"Spacecraft Proximity Maneuvering and Rendezvous With Collision Avoidance Based on Reinforcement Learning","Q. Qu; K. Liu; W. Wang; J. Lü","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","IEEE Transactions on Aerospace and Electronic Systems","5 Dec 2022","2022","58","6","5823","5834","The rapid development of the aerospace industry puts forward the urgent need for the evolution of autonomous spacecraft rendezvous technology, which has gained significant attention recently due to increased applications in various space missions. This article studies the relative position tracking problem of the autonomous spacecraft rendezvous under the requirement of collision avoidance. An exploration-adaptive deep deterministic policy gradient (DDPG) algorithm is proposed to train a definite control strategy for this mission. Similar to the DDPG algorithm, four neural networks are used in this method, where two of them are used to generate the deterministic policy, whereas the other two are used to score the obtained policy. Differently, adaptive noise is introduced to reduce the possibility of oscillations and divergences and to cut down the unnecessary computation by weakening the exploration of stabilization problems. In addition, in order to effectively and quickly adapt to some other similar scenarios, a metalearning-based idea is introduced by fine-tuning the prior strategy. Finally, two numerical simulations show that the trained control strategy can effectively avoid the oscillation phenomenon caused by the artificial potential function. Benefiting from this, the trained control strategy based on deep reinforcement learning technology can decrease the energy consumption by 16.44% during the close proximity phase, compared with the traditional artificial potential function method. Besides, after introducing the metalearning-based idea, a strategy available for some other perturbed scenarios can be trained in a relatively short period of time, which illustrates its adaptability.","1557-9603","","10.1109/TAES.2022.3180271","National Natural Science Foundation of China(grant numbers:92067204,61903017,62022008); National Defense Science and Technology Key Laboratory Fund Program(grant numbers:6142208200301); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793683","Aerospace control;autonomous spacecraft rendezvous (ASR);collision avoidance;deep reinforcement learning (DRL)","Space vehicles;Heuristic algorithms;Aerodynamics;Collision avoidance;Oscillators;Orbits;Mathematical models","aerospace control;collision avoidance;learning (artificial intelligence);mobile robots;neural nets;position control;space vehicles","adaptability;adaptive noise;aerospace industry;autonomous spacecraft rendezvous technology;collision avoidance;DDPG algorithm;deep reinforcement learning technology;definite control strategy;exploration-adaptive deep deterministic policy gradient algorithm;increased applications;metalearning-based idea;neural networks;oscillation phenomenon;oscillations;prior strategy;proximity phase;relative position tracking problem;similar scenarios;space missions;spacecraft proximity maneuvering;stabilization problems;traditional artificial potential function method;trained control strategy","","5","","31","IEEE","10 Jun 2022","","","IEEE","IEEE Journals"
"Effective reinforcement learning for mobile robots","W. D. Smart; L. Pack Kaelbling","Department of Computer Science, Washington University of Saint Louis, Saint Louis, MO, USA; Artificial Intelligence Laboratory, MIT, Cambridge, MA, USA","Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292)","7 Aug 2002","2002","4","","3404","3410 vol.4","Programming mobile robots can be a long, time-consuming process. Specifying the low-level mapping from sensors to actuators is prone to programmer misconceptions, and debugging such a mapping can be tedious. The idea of having a robot learn how to accomplish a task, rather than being told explicitly, is an appealing one. It seems easier and much more intuitive for the programmer to specify what the robot should be doing, and to let it learn the fine details of how to do it. In this paper, we introduce a framework for reinforcement learning on mobile robots and describe our experiments using it to learn simple tasks.","","0-7803-7272-7","10.1109/ROBOT.2002.1014237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1014237","","Mobile robots;Machine learning;Programming profession;Robot sensing systems;Robot programming;Intelligent sensors;Actuators;Debugging;Humans;Robot control","mobile robots;learning by example;learning systems;navigation","mobile robots;machine learning;reinforcement learning;learning from demonstration;navigation;obstacle avoidance","","153","4","13","IEEE","7 Aug 2002","","","IEEE","IEEE Conferences"
"Autonomous navigation of UAV by using real-time model-based reinforcement learning","N. Imanberdiyev; C. Fu; E. Kayacan; I. -M. Chen",School of Mechanical and Aerospace Engineering; School of Mechanical and Aerospace Engineering; School of Mechanical and Aerospace Engineering; School of Mechanical and Aerospace Engineering,"2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV)","2 Feb 2017","2016","","","1","6","Autonomous navigation in an unknown or uncertain environment is one of the challenging tasks for unmanned aerial vehicles (UAVs). In order to address this challenge, it is necessary to have sophisticated high level control methods that can learn and adapt themselves to changing conditions. One of the most promising frameworks for such a purpose is reinforcement learning. In this paper, a novel model-based reinforcement learning algorithm, TEXPLORE, is developed as a high level control method for autonomous navigation of UAVs. The developed approach has been extensively tested with a quadcopter UAV in ROS-Gazebo environment. The experimental results show that our method is able to learn an efficient trajectory in a few iterations and perform actions in real-time. Moreover, we show that our approach significantly outperforms Q-learning based method. To the best of our knowledge, this is the first time that TEXPLORE has been developed to achieve autonomous navigation of UAVs.","","978-1-5090-3549-6","10.1109/ICARCV.2016.7838739","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838739","","Computational modeling;Batteries;Unmanned aerial vehicles;Planning;Learning (artificial intelligence);Navigation;Real-time systems","adaptive control;autonomous aerial vehicles;helicopters;learning systems;mobile robots;navigation;trajectory control","UAV autonomous navigation;real-time model-based reinforcement learning;unknown environment;uncertain environment;unmanned aerial vehicle;learning control;adaptive control;TEXPLORE;high level control method;quadcopter UAV;ROS-Gazebo environment;trajectory learning;Q-learning based method","","75","","13","IEEE","2 Feb 2017","","","IEEE","IEEE Conferences"
"A reinforcement learning approach towards autonomous suspended load manipulation using aerial robots","I. Palunko; A. Faust; P. Cruz; L. Tapia; R. Fierro","Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, Croatia; Department of Computer Science, University of New Mexico, Albuquerque, NM, USA; Department of Electrical and Computer Engineering, University of New Mexico, Albuquerque, NM, USA; Department of Computer Science, University of New Mexico, Albuquerque, NM, USA; Department of Electrical and Computer Engineering, University of New Mexico, Albuquerque, NM, USA","2013 IEEE International Conference on Robotics and Automation","17 Oct 2013","2013","","","4896","4901","In this paper, we present a problem where a suspended load, carried by a rotorcraft aerial robot, performs trajectory tracking. We want to accomplish this by specifying the reference trajectory for the suspended load only. The aerial robot needs to discover/learn its own trajectory which ensures that the suspended load tracks the reference trajectory. As a solution, we propose a method based on least-square policy iteration (LSPI) which is a type of reinforcement learning algorithm. The proposed method is verified through simulation and experiments.","1050-4729","978-1-4673-5643-5","10.1109/ICRA.2013.6631276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6631276","Aerial robotics;aerial load transportation;motion planning and control;machine learning;quadrotor control;trajectory tracking;reinforcement learning","Legged locomotion;Target tracking;Computational modeling","autonomous aerial vehicles;helicopters;iterative methods;learning (artificial intelligence);least squares approximations;trajectory control","autonomous suspended load manipulation;suspended load;rotorcraft aerial robot;reference trajectory tracking;least-square policy iteration;reinforcement learning algorithm","","50","","19","IEEE","17 Oct 2013","","","IEEE","IEEE Conferences"
"Application of deep reinforcement learning in mobile robot path planning","J. Xin; H. Zhao; D. Liu; M. Li","Shaanxi Key Laboratory of Complex System Control and Intelligent Information Processing, Xi'an University of Technology, Xi'an, P.R.China; Shaanxi Key Laboratory of Complex System Control and Intelligent Information Processing, Xi'an University of Technology, Xi'an, P.R.China; Shaanxi Key Laboratory of Complex System Control and Intelligent Information Processing, Xi'an University of Technology, Xi'an, P.R.China; Department of the Information and Communications, Xi'an Polytechnic University, Xi'an, P.R.China","2017 Chinese Automation Congress (CAC)","1 Jan 2018","2017","","","7112","7116","In order to make the robot obtain the optimal action directly from the original visual perception without any hand-crafted features and features matching, a novel end-to-end path planning method-mobile robot path planning using deep reinforcement learning is proposed. Firstly, a deep Q-network (DQN) is designed and trained to approximate the mobile robot state-action value function. Then, the Q value corresponding to each possible mobile robot action (i.e., turn left, turn right, forward) is determined by the well trained DQN, here, the input of the DQN is the original RGB image (image pixels) captured from the environment without any hand-crafted features and features matching; Finally, the current optimal mobile robot action is selected by the action selection strategy. Mobile robot reach to the goal point while avoiding obstacles ultimately. 30 times path planning experiments are conducted in the seekavoid_arena_01 environment on DeepMind Lab platform. The experimental results show that our deep reinforcement learning based robot path planning method is an effective end-to-end mobile robot path planning method.","","978-1-5386-3524-7","10.1109/CAC.2017.8244061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8244061","End-to-end Path planning;Deep Reinforcement Learning;DQN","Mobile robots;Path planning;Machine learning;Training;Visual perception","approximation theory;control engineering computing;image colour analysis;learning (artificial intelligence);mobile robots;optimisation;path planning;robot vision","mobile robot path planning;deep Q-network;action selection strategy;optimal mobile robot action;DQN;mobile robot state-action value function approximation;end-to-end path planning method;deep reinforcement learning;RGB image;visual perception","","47","","13","IEEE","1 Jan 2018","","","IEEE","IEEE Conferences"
"Fast Reinforcement Learning for Vision-guided Mobile Robots","T. Martinez-Marin; T. Duckett","Department of Physics, Systems Engineering and Signal Theory, University of Alicante, Alicante, Spain; AASS, Department of Technology, Orebro University, Orebro, Sweden","Proceedings of the 2005 IEEE International Conference on Robotics and Automation","10 Jan 2006","2005","","","4170","4175","This paper presents a new reinforcement learning algorithm for accelerating acquisition of new skills by real mobile robots, without requiring simulation. It speeds up Q-learning by applying memory-based sweeping and enforcing the “adjoining property”, a technique that exploits the natural ordering of sensory state spaces in many robotic applications by only allowing transitions between neighbouring states. The algorithm is tested within an image-based visual servoing framework on a docking task, in which the robot has to position its gripper at a desired configuration relative to an object on a table. In experiments, we compare the performance of the new algorithm with a hand-designed linear controller and a scheme using the linear controller as a bias to further accelerate the learning. By analysis of the controllability and docking time, we show that the biased learner could improve on the performance of the linear controller, while requiring substantially lower training time than unbiased learning (less than 1 hour on the real robot).","1050-4729","0-7803-8914-X","10.1109/ROBOT.2005.1570760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1570760","","Learning;Mobile robots;Acceleration;Robot sensing systems;Orbital robotics;State-space methods;Testing;Visual servoing;Grippers;Performance analysis","","","","28","","12","IEEE","10 Jan 2006","","","IEEE","IEEE Conferences"
"Efficient reinforcement learning: model-based Acrobot control","G. Boone","Georgia Institute of Technology, College of Computing Georgia Institute of Technology, Atlanta, GA, USA","Proceedings of International Conference on Robotics and Automation","6 Aug 2002","1997","1","","229","234 vol.1","Several methods have been proposed in the reinforcement learning literature for learning optimal policies for sequential decision tasks. Q-learning is a model-free algorithm that has previously been applied to the Acrobot, a two-link arm with a single actuator at the elbow that learns to swing its free endpoint above a target height. However, applying Q-learning to a real Acrobot may be impractical due to the large number of required movements of the real robot as the controller learns. This paper explores the planning speed and data efficiency of explicitly learning models, as well as using heuristic knowledge to aid the search for solutions and reduce the amount of data required from the real robot.","","0-7803-3612-7","10.1109/ROBOT.1997.620043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=620043","","Elbow;Learning systems;Robot control;Control systems;Optimal control;Educational institutions;Actuators;Algorithm design and analysis;Ear;Real time systems","learning (artificial intelligence);nonlinear dynamical systems;search problems;manipulators;planning (artificial intelligence)","reinforcement learning;model-based Acrobot control;optimal policies;sequential decision tasks;Q-learning;two-link arm;planning speed;data efficiency;heuristic knowledge","","23","","13","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A study of reinforcement learning for the robot with many degrees of freedom - acquisition of locomotion patterns for multi-legged robot","K. Ito; F. Matsuno","Department of Computational Intelligence and Systems Science, Tokyo Institute of Technology, Yokohama, Japan; Department of Computational Intelligence and Systems Science, Tokyo Institute of Technology, Yokohama, Japan","Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292)","7 Aug 2002","2002","4","","3392","3397 vol.4","Reinforcement learning has recently been receiving much attention as a learning method for not only toy problems but also complicated systems such as robot systems. It does not need priori knowledge and has higher capability of reactive and adaptive behaviors. However, increasing of action-state space makes it difficult to accomplish the learning process. In most of the previous works, the application of the learning is restricted to simple tasks with a small action-state space. Considering this point, we present a new reinforcement learning algorithm: Q-learning with dynamic structuring of exploration space based on genetic algorithm. The algorithm is applicable to systems with high dimensional action and interior state spaces, for example, a robot with many redundant degrees of freedom. To demonstrate the effectiveness of the proposed algorithm simulations of locomotion patterns for a 12-leged robot were carried out. As the result, an effective behavior was obtained by using our proposed algorithm.","","0-7803-7272-7","10.1109/ROBOT.2002.1014235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1014235","","Space exploration;Orbital robotics;Heuristic algorithms;Legged locomotion;Learning systems;State-space methods;Genetic algorithms;Intelligent robots;Indium tin oxide;Computational intelligence","legged locomotion;robot dynamics;learning (artificial intelligence);genetic algorithms","reinforcement learning;multiple legged robot;action-state space;genetic algorithm;legged locomotion;Q-learning;dynamics;locomotion pattern","","19","","6","IEEE","7 Aug 2002","","","IEEE","IEEE Conferences"
"Freeway ramp-metering control based on Reinforcement learning","A. Fares; W. Gomaa","Computer Science and Engineering, Egypt-Japan University for Science and Technology (E-JUST); Computer Science and Engineering, Egypt-Japan University for Science and Technology (E-JUST)","11th IEEE International Conference on Control & Automation (ICCA)","7 Aug 2014","2014","","","1226","1231","Random occurrences of traffic congestion on freeways lead to system degradation over time. If no smart control measures are applied, this degradation can lead to accumulated congestion which can severely affect other parts of the traffic network. Consequently, the need for an optimal and reliable traffic control has become more critical. The aim of this research is to control the amount of vehicles entering the mainstream freeway from the ramp merging area, i.e., balance the demand and the capacity of the freeway . This keeps the freeway density below the critical density. Consequently, this leads to maximum utilization of the freeway without entering in congestion while maintaining the optimal freeway operation. The Reinforcement learning based density control agent (RLCA) is designed based on Markovion modeling with an associated Q-learning algorithm in order to address the stochastic nature of the traffic situation. Extensive analysis is conducted in order to assess the proposed definition of the (state, action) pairs, as well as the reward function. We experiment with two case studies with two different network structures and demands. The first case study, which is the benchmark network used in literature, is the network with dense demand. Whereas the other one is the network with light demand. RLCA shows a superior response with respect to a predetermined reference points especially in terms of freeway density, flow rate, and total travel time.","1948-3457","978-1-4799-2837-8","10.1109/ICCA.2014.6871097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6871097","Ramp metering;Q-learning;traffic control;intelligent control;freeway;agent-based system;intelligent transportation system;sequential decision problem","Traffic control;Vehicles;Mathematical model;Learning (artificial intelligence);Equations;Heuristic algorithms;Aerospace electronics","intelligent control;intelligent transportation systems;learning (artificial intelligence);Markov processes;multi-agent systems;road traffic control;road vehicles","total travel time;traffic flow rate;freeway density;Q-learning algorithm;Markovion modeling;RLCA;reinforcement learning based density control agent;ramp merging area;vehicles;traffic control;smart control measures;traffic congestion;freeway ramp-metering control","","19","","14","IEEE","7 Aug 2014","","","IEEE","IEEE Conferences"
"A Multi-agent Reinforcement Learning Method for Swarm Robots in Space Collaborative Exploration","Y. Huang; S. Wu; Z. Mu; X. Long; S. Chu; G. Zhao","School of Aeronautics and Astronautics, Shanghai JiaoTong University, Shanghai, China; School of Aeronautics and Astronautics, Shanghai JiaoTong University, Shanghai, China; School of Aeronautics and Astronautics, Shanghai JiaoTong University, Shanghai, China; School of Aeronautics and Astronautics, Shanghai JiaoTong University, Shanghai, China; School of Aeronautics and Astronautics, Shanghai JiaoTong University, Shanghai, China; School of Aeronautics and Astronautics, Shanghai JiaoTong University, Shanghai, China","2020 6th International Conference on Control, Automation and Robotics (ICCAR)","4 Jun 2020","2020","","","139","144","Deep-space exploration missions are known as particularly challenging with high risk and cost, as they operate in environments with high uncertainty. The fault of exploration robot can even cause the whole mission to failure. One of the solutions is to use swarm robots to operate missions collaboratively. Compared with a single capable robot, a swarm of less sophisticated robots can cooperate on multiple and complex tasks. Reinforcement learning (RL) has made a variety of progress in multi-agent system autonomous cooperative control domains. In this paper, we construct a collaborative exploration scenario, where a multi-robot system explores an unknown Mars surface. Tasks are assigned to robots by human scientists and each robot takes optimal policies autonomously. The method used to train policies is a multi-agent deep deterministic policy gradient algorithm (MADDPG) and we design an experience sample optimizer to improve this algorithm. The results show that, with the increase of robots and targets number, this method is more efficient than traditional deep RL algorithm in a multi-agent collaborative exploration environment.","2251-2446","978-1-7281-6139-6","10.1109/ICCAR49639.2020.9107997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9107997","space exploration;swarm robots;multi-agents;reinforcement learning;MADDPG","Mars;Uncertainty;Costs;Collaboration;Swarm robotics;Reinforcement learning;Space exploration","control engineering computing;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems","multiagent reinforcement learning method;swarm robots;space collaborative exploration;deep-space exploration missions;exploration robot;single capable robot;multiagent system autonomous cooperative control domains;multirobot system;multiagent deep deterministic policy gradient algorithm;deep RL algorithm;multiagent collaborative exploration environment;unknown Mars surface;human scientists;optimal policies;MADDPG;sample optimizer","","15","","14","IEEE","4 Jun 2020","","","IEEE","IEEE Conferences"
"Autonomous Decision-Making Method for Combat Mission of UAV based on Deep Reinforcement Learning","J. Xu; Q. Guo; L. Xiao; Z. Li; G. Zhang","Equipment Management and UAV Engineering College, Air Force Engineering University; Equipment Management and UAV Engineering College, Air Force Engineering University; Unmanned System Research Institute, Northwestern Polytechnical University, Xi’an, Shaanxi, China; Equipment Management and UAV Engineering College, Air Force Engineering University; Equipment Management and UAV Engineering College, Air Force Engineering University","2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","13 Feb 2020","2019","1","","538","544","Improving of the autonomous decision-making ability of UAV has become a key factor for UAV to seize the initiative of future battlefield. At present, UAV's tasks mainly depend on pre-planning, which is difficult to adapt to the complexity and dynamics of future battlefield. Aiming at this problem, in view of typical mission scenarios of UAV regional reconnaissance and air-to-air confrontation, this paper adopts Deep Learning method to develop autonomous decision-making method for UAV, constructs mission decision-making model of Deep Belief Network (DBN) and Q-Learning algorithm, and then optimizes the decision-making model based on genetic algorithm to realize ""off-line learning"" and ""online decision-making"" provide effective support. The simulation results verify that the method can effectively deal with typical tasks of UAV regional reconnaissance and air-to-air confrontation, and it is an effective attempt to carry out autonomous decision-making of UAV.","2381-0947","978-1-7281-1907-6","10.1109/IAEAC47372.2019.8998066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8998066","Deep Learning;UAV;Autonomous Control;Deep Belief Network (DBN);Q-Learning","Decision making;Reconnaissance;Task analysis;Training;Planning;Feature extraction;Machine learning","aerospace engineering;autonomous aerial vehicles;belief networks;decision making;genetic algorithms;learning (artificial intelligence);military computing","autonomous decision-making method;UAV regional reconnaissance;air-to-air confrontation;off-line learning;UAV combat mission;deep reinforcement learning;mission decision-making model;online decision-making","","14","","9","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Motion Control for Biped Robot via DDPG-based Deep Reinforcement Learning","X. Wu; S. Liu; T. Zhang; L. Yang; Y. Li; T. Wang","Institute of Electric Engineering, Yanshan University, Qinhuangdao, China; Institute of Electric Engineering, Yanshan University, Qinhuangdao, China; Institute of Electric Engineering, Yanshan University, Qinhuangdao, China; Institute of Electric Engineering, Yanshan University, Qinhuangdao, China; Institute of Electric Engineering, Yanshan University, Qinhuangdao, China; Institute of Electric Engineering, Yanshan University, Qinhuangdao, China","2018 WRC Symposium on Advanced Robotics and Automation (WRC SARA)","23 Dec 2018","2018","","","40","45","In the study of the passive biped robot, the avoidance of fall over is always an important direction of the research. In this paper, we propose Deep Deterministic Policy Gradient (DDPG) to control the biped robot walk steadily on the slope. For improve the speed of DDPG training, the DDPG used in the paper is improved by parallel actors and Prioritized Experience Replay (PER). In the simulation, we control different initial states that cause the biped robot to fall over. After the control, the biped robot can walk stably, which indicating that DDPG can effectively control the fall over of the biped robot.","","978-1-5386-7675-2","10.1109/WRC-SARA.2018.8584227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8584227","","Legged locomotion;Mathematical model;Training;Hip;Torque","control engineering computing;learning (artificial intelligence);legged locomotion;motion control","motion control;passive biped robot;DDPG training;deep deterministic policy gradient;DDPG-based deep reinforcement learning;fall over avoidance;parallel actors;prioritized experience replay","","13","","21","IEEE","23 Dec 2018","","","IEEE","IEEE Conferences"
"Cooperative collision avoidance for multi-vehicle systems using reinforcement learning","Q. Wang; C. Phillips","Queen Mary University of London, London, London, GB; Queen Mary University of London, London, London, GB","2013 18th International Conference on Methods & Models in Automation & Robotics (MMAR)","25 Nov 2013","2013","","","98","102","Collision avoidance is a fundamental problem in navigation. In this paper, we present a novel method of cooperative movement planning to examine how two vehicles can orchestrate their movements so as to avoid collisions and subsequently return to their intended paths. Movement planning in this research is solved by regarding it as a decision process. When the vehicles are at risk of a collision, the system determines appropriate steering motions for both vehicles at each time step, so that they can cooperatively change course to avoid collisions and return to their original course when the risk is averted. Reinforcement learning is applied to solve this decision-making task. States of the system are described in terms of the vehicles' position and orientation and actions are defined considering the kinematic constraints of the vehicles. In reinforcement learning, an approximate value function is iteratively developed according to certain rules to evaluate state-action combinations of the system. Appropriate motions are selected by the system after calculating the approximate value of possible target states, which also satisfy the requirement of the smoothness of paths, as well as the distances between, and velocities of, both vehicles. The method of least squares is applied in the iterative mechanism to update the approximate value function given a scoring technique for a collection of state samples featuring continuous state space and action space. This paper summarizes the concept and methodologies used to implement an online cooperative collision avoidance system. Different scenarios are tested to assess the performance of the proposed algorithm.","","978-1-4673-5508-7","10.1109/MMAR.2013.6669888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6669888","Cooperative Path Planning;Collision Avoidance;Reinforcement Learning;Adaptive Dynamic Programming","Vehicles;Collision avoidance;Learning (artificial intelligence);Planning;Least squares approximations;Robot kinematics","collision avoidance;control engineering computing;decision making;function approximation;intelligent control;iterative methods;learning (artificial intelligence);navigation;vehicles","multivehicle systems;reinforcement learning;navigation;cooperative movement planning;decision process;steering motions;decision-making task;kinematic constraints;approximate value function;iterative mechanism;value function approximation;scoring technique;continuous state space;action space;online cooperative collision avoidance system;performance assessment","","11","1","15","IEEE","25 Nov 2013","","","IEEE","IEEE Conferences"
"Motion Planning by Reinforcement Learning for an Unmanned Aerial Vehicle in Virtual Open Space with Static Obstacles","S. Kim; J. Park; J. -K. Yun; J. Seo","School of Integrated Technology, Yonsei University, Incheon, Korea; School of Integrated Technology, Yonsei University, Incheon, Korea; Electronics and Telecommunications Research Institute, Daejeon, Korea; School of Integrated Technology, Yonsei University, Incheon, Korea","2020 20th International Conference on Control, Automation and Systems (ICCAS)","1 Dec 2020","2020","","","784","787","In this study, we applied reinforcement learning based on the proximal policy optimization algorithm to perform motion planning for an unmanned aerial vehicle (UAV) in an open space with static obstacles. The application of reinforcement learning through a real UAV has several limitations such as time and cost; thus, we used the Gazebo simulator to train a virtual quadrotor UAV in a virtual environment. As the reinforcement learning progressed, the mean reward and goal rate of the model were increased. Furthermore, the test of the trained model shows that the UAV reaches the goal with an 81% goal rate using the simple reward function suggested in this work.","2642-3901","978-89-93215-20-5","10.23919/ICCAS50221.2020.9268253","Electronics and Telecommunications Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268253","motion planning;unmanned aerial vehicle (UAV);reinforcement learning;proximal policy optimization (PPO)","Reinforcement learning;Unmanned aerial vehicles;Planning;Robots;Aerospace electronics;Virtual environments;Training","aircraft control;autonomous aerial vehicles;collision avoidance;control engineering computing;learning (artificial intelligence);motion control;optimisation;path planning","motion planning;reinforcement learning;unmanned aerial vehicle;virtual open space;static obstacles;proximal policy optimization;virtual quadrotor UAV;virtual environment;Gazebo simulator","","11","","30","","1 Dec 2020","","","IEEE","IEEE Conferences"
"End-to-End Autonomous Driving Decision Based on Deep Reinforcement Learning","Z. Huang; J. Zhang; R. Tian; Y. Zhang","Faculty of Information Technology, Beijing University Of Technology, Beijing, China; Faculty of Information Technology, Beijing University Of Technology, Beijing, China; Faculty of Information Technology, Beijing University Of Technology, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China","2019 5th International Conference on Control, Automation and Robotics (ICCAR)","29 Aug 2019","2019","","","658","662","End-to-end autonomous driving decision-making is a popular research field in autonomous driving. In this paper, we propose an end-to-end decision-making model based on DDPG deep reinforcement learning. Firstly, we establish an end-to-end decision-making model to map driving state (such as tangential angle of vehicle, velocity of vehicle, distance of road) to driving action (steer, accelerate, brake) continuously. Next we train and valid our agent in different scenarios on TORCS platform. The results show that DDPG algorithms can achieve end-to-end autonomous driving decisions. Finally, we visualize the agent by analyzing which state contributes to decision.","2251-2446","978-1-7281-3326-3","10.1109/ICCAR.2019.8813431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8813431","autonomous driving;end-to-end decision;reinforcement learning;TORCS","","decision making;learning (artificial intelligence);road vehicles;traffic engineering computing","end-to-end autonomous driving decision-making;DDPG deep reinforcement learning;end-to-end autonomous driving decisions;DDPG algorithms;TORCS platform","","11","","17","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"An Improved Method Based on Deep Reinforcement Learning for Target Searching","X. L. Wei; X. L. Huang; T. Lu; G. G. Song","School of Computer Science, Communication University of China, Beijing, China; School of Computer Science, Communication University of China, Beijing, China; Simulation Research Center, China Aerospace System Simulation Technology Co., Ltd (Beijing), Beijing, China; School of Computer Science, Communication University of China, Beijing, China","2019 4th International Conference on Robotics and Automation Engineering (ICRAE)","23 Mar 2020","2019","","","130","134","Unmanned Aerial Vehicle (UAV), due to their high mobility and the ability to cover areas of different heights and locations at relatively low cost, are increasingly used for disaster monitoring and detecting. However, developing and testing UAVs in real world is an expensive task, especially in the domain of search and rescue, most of the previous systems are developed on the basis of greedy or potential-based heuristics without neural network. On the basis of the recent development of deep neural network architecture and deep reinforcement learning (DRL), in this research we improved the probability of success rate of searching target in an unstructured environment by combining image processing algorithms and reinforcement learning methods (RL). This paper aims at the deficiency of target tracking in unstructured environment, trying to propose an algorithm of stationary target positioning of UAV based on computer vision system. Firstly, a new input source is formed by acquiring depth information image of current environment and combining segmentation image. Secondly, the DQN algorithm is used to regulate the reinforcement learning model, and the specific flight response can be independently selected by the UAV through training. This paper utilizes open-source Microsoft UAV simulator AirSim as training and test environment based with Keras a machine learning framework. The main approach investigated in this research is modifying the network of Deep Q-Network, which designs the moving target tracking experiment of UAV in simulation scene. The experimental results demonstrate that this method has better tracking effect.","","978-1-7281-4740-6","10.1109/ICRAE48301.2019.9043821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9043821","search and rescue;reinforcement learning;UAV","Reinforcement learning;Image segmentation;Neural networks;Unmanned aerial vehicles;Task analysis;Meteorology;Markov processes","aerospace simulation;autonomous aerial vehicles;image segmentation;learning (artificial intelligence);neural nets;robot vision;target tracking","target searching;unmanned aerial vehicle;high mobility;disaster monitoring;expensive task;deep neural network architecture;deep reinforcement learning;image processing algorithms;reinforcement learning methods;stationary target positioning;computer vision system;depth information image;segmentation image;DQN algorithm;reinforcement learning model;open-source Microsoft UAV simulator AirSim;test environment;machine learning framework;deep Q-network;moving target tracking experiment","","11","","22","IEEE","23 Mar 2020","","","IEEE","IEEE Conferences"
"Dynamic Actor-Critic: Reinforcement Learning Based Radio Resource Scheduling for LTE-Advanced","P. K. Tathe; M. Sharma","Dept. Of Electronics and Telecommunication, D. Y. Patil College of Engineering, Akurdi, Pune, India; Dept. Of Electronics and Telecommunication, D. Y. Patil College of Engineering, Akurdi, Pune, India","2018 Fourth International Conference on Computing Communication Control and Automation (ICCUBEA)","25 Apr 2019","2018","","","1","4","This paper proposes, an Actor-Critic Reinforcement learning based radio resource scheduling policy in downlink Transmission for Long Term Evaluation Advanced (LTE-A) radio resource technology. The scheduling technique uses the neural network (NN) based actor critic architecture in order to propose proper scheduling rules at each Transmission Time Interval (TTI). The objective is to improve system capacity, system throughput and spectral efficiency. NN based Actor-critic Reinforcement learning is proposed to accomplish the resource scheduling efficiently by maintaining best QoS capabilities and user fairness. The simulation results indicate that the proposed method achieves desired throughput and increased convergence capability.","","978-1-5386-5257-2","10.1109/ICCUBEA.2018.8697808","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8697808","Actor-critic architecture;Reinforcement learning;LTE-A;Neural Network;scheduling rule;TTI","Long Term Evolution;Optimal scheduling;Neural networks;Scheduling algorithms;Reinforcement learning;Throughput;Scheduling","learning (artificial intelligence);Long Term Evolution;neural nets;quality of service;resource allocation;scheduling;telecommunication computing","dynamic actor-critic;reinforcement learning based radio resource scheduling;LTE-advanced;Actor-Critic Reinforcement learning;radio resource scheduling policy;downlink Transmission;Long Term Evaluation Advanced;scheduling technique;neural network based actor critic architecture;transmission time interval;actor-critic reinforcement learning;scheduling rules","","10","","18","IEEE","25 Apr 2019","","","IEEE","IEEE Conferences"
"Cooperative behavior acquisition in multi-mobile robots environment by reinforcement learning based on state vector estimation","E. Uchibe; M. Asada; K. Hosoda","Department of Adaptive Machine Systems Graduate School of Eng, Osaka University, Suita, Osaka, Japan; Department of Adaptive Machine Systems Graduate School of Eng, Osaka University, Suita, Osaka, Japan; Department of Adaptive Machine Systems Graduate School of Eng, Osaka University, Suita, Osaka, Japan","Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146)","6 Aug 2002","1998","2","","1558","1563 vol.2","This paper proposes a method that acquires robots' behaviors based on the estimation of the state vectors. In order to acquire the cooperative behaviors in multi-robot environments, each learning robot estimates the local predictive model between the learner and the other objects separately. Based on the local predictive models, the robots learn the desired behaviors using reinforcement learning. The proposed method is applied to a soccer playing situation, where a rolling ball and other moving robots are well modeled and the learner's behaviors are successfully acquired by the method. Computer simulations and real experiments are shown and a discussion is given.","1050-4729","0-7803-4300-X","10.1109/ROBOT.1998.677351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=677351","","Mobile robots;Learning;Robot kinematics;Predictive models;State estimation;Adaptive systems;High performance computing;Artificial intelligence;Robot sensing systems;Neurons","mobile robots;cooperative systems;learning (artificial intelligence);software agents;state estimation;predictive control","cooperative systems;multiple mobile robots;reinforcement learning;state vector estimation;predictive models;behaviour based control;multiple agents","","9","","9","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Energy Aware Resource Scheduling Algorithm for Data Center Using Reinforcement Learning","J. Yuan; X. Jiang; L. Zhong; H. Yu","Wuhan University of Technology, Wuhan, Hubei, China; Wuhan University of Technology, Wuhan, Hubei, China; Wuhan University of Technology, Wuhan, Hubei, China; Wuhan University of Technology, Wuhan, Hubei, China","2012 Fifth International Conference on Intelligent Computation Technology and Automation","13 Feb 2012","2012","","","435","438","More and more attention is paid for energy consumption aware and power control for data center with the emergency of energy crisis. The use of virtualization technology makes it possible for dynamic configuration of data center resources. The N:1 mapping visualization technology is employed to integrate many physical machines into an virtual resource pool to control resources centralized, and then reinforcement learning is applied to resource management and decision making for an uncertain task flow data center. Finally, an automatic resource control algorithm with energy consumption aware is proposed. This algorithm is implemented in the Cloud Sim platform to improve the energy consumption of the data center. The experimental results show that our algorithm can reduce about 40% of the energy consumption of the non-power-aware data center and reduce 1.7% of that of the greedy scheduling algorithm in data center.","","978-1-4673-0470-2","10.1109/ICICTA.2012.115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6150135","Data Center;N:1 Mapping Visualization;Energy Control;Reinforcement Learning;Dynamic Allocation","Virtual machining;Energy consumption;Resource management;Learning;Scheduling algorithms;Euclidean distance;Cloud computing","cloud computing;computer centres;decision making;energy consumption;learning (artificial intelligence);power aware computing;resource allocation;virtualisation","energy aware resource scheduling algorithm;reinforcement learning;energy consumption;power control;virtualization technology;dynamic configuration;N:1 mapping visualization technology;resource management;decision making;uncertain task flow data center;automatic resource control;Cloud Sim platform;greedy scheduling algorithm","","9","","12","IEEE","13 Feb 2012","","","IEEE","IEEE Conferences"
"Path tracking control and identification of tire parameters using on-line model-based reinforcement learning","T. Kim; H. J. Kim","Department of Mechanical and Aerospace Engineering, Seoul National University, Seoul, Korea; Department of Mechanical and Aerospace Engineering, Seoul National University, Seoul, Korea","2016 16th International Conference on Control, Automation and Systems (ICCAS)","26 Jan 2017","2016","","","215","219","Path tracking control for autonomous vehicle using model predictive control (MPC) algorithm maintains maneuverability by calculating a sequence of control input which minimizes a tracking error. The weakness of this method is that the performance of MPC may decrease significantly when the priori prediction model is not accurate. Therefore, it is important to keep the vehicle stable when MPC having model error. This paper uses an on-line model-based reinforcement learning (RL) to decrease the path error by learning unknown parameters and updating a prediction model. To validate, two kinds of path tracking simulation are conducted: one is the comparison the performance between on-line model-based RL and MPC with model error. The other one is about the test when the model used in MPC and the true dynamics, which actually received input, have different tire model. The model-based RL method succeeds to learn unknown tire parameters and maintains their maneuverability in both simulations.","","978-89-93215-11-3","10.1109/ICCAS.2016.7832324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832324","autonomous vehicle;model-based reinforcement learning;model predictive control","Tires;Mathematical model;Vehicles;Vehicle dynamics;Predictive models;Brushes;Wheels","learning (artificial intelligence);parameter estimation;position control;predictive control;road vehicles;tyres;vehicle dynamics","autonomous vehicle;model predictive control;control input sequence;tracking error minimization;MPC performance;priori prediction model;online model-based reinforcement learning;path error;unknown parameter learning;path tracking simulation;online model-based RL;maneuverability;path tracking control;unknown tire parameter identification","","8","","7","","26 Jan 2017","","","IEEE","IEEE Conferences"
"Motion planning algorithm for nonholonomic autonomous underwater vehicle in disturbance using reinforcement learning and teaching method","H. Kawano; T. Ura","Underwater Technology Research Center,Institute of Industrial Science, University of Tokyo, Meguro, Tokyo, Japan; Underwater Technology Research Center,Institute of Industrial Science, University of Tokyo, Meguro, Tokyo, Japan","Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292)","7 Aug 2002","2002","4","","4032","4038 vol.4","A training algorithm for motion planning of a nonholonomic autonomous underwater vehicle (AUV) in the strong water current is proposed in this paper. The proposed algorithm can be applied in the environment with obstacles placed in arbitrary configuration. In order to realize these functions, the Q-learning and teaching method are introduced and a multilayer structure is proposed. By introducing Q-learning, the motion of the nonholonomic AUV can be suitably treated. Taking advantage of the Baysian net, a motion planning algorithm in the case of an existence of obstacles, is derived automatically from the learned knowledge. The multilayer structure accelerates the learning process. Results of the demonstration by the simulation of control of R-One robot show the high performance of proposed algorithm.","","0-7803-7272-7","10.1109/ROBOT.2002.1014368","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1014368","","Underwater vehicles;Motion planning;Education;Vehicle dynamics;Service robots;Technology planning;Underwater technology;Industrial training;Inspection;Motion control","underwater vehicles;mobile robots;path planning;learning (artificial intelligence);belief networks;state-space methods","autonomous underwater vehicle;obstacle avoidance;Bayesian net;reinforcement learning;multilayer structure;training algorithm;path planning;nonholonomic AUV;Q learning process;discrete state space;mobile robot","","7","","7","IEEE","7 Aug 2002","","","IEEE","IEEE Conferences"
"A Self-Organized Fuzzy-Neuro Reinforcement Learning System for Continuous State Space for Autonomous Robots","M. Obayashi; T. Kuremoto; K. Kobayashi","Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Japan; Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Japan; Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Japan","2008 International Conference on Computational Intelligence for Modelling Control & Automation","24 Jul 2009","2008","","","551","556","This paper proposes the system that combines self-organized fuzzy-neural networks with reinforcement learning system (Q-learning, stochastic gradient ascent : SGA) to realize the autonomous robot behavior learning for continuous state space. The self-organized fuzzy neural network works as adaptive input state space classifier to adapt the change of environment, the part of reinforcement learning has the learning ability corresponding to rule for the input state space . Simultaneously, to simulate the real environment the robot has ability to estimate own-position. Finally, it is clarified that our proposed system is effective through the autonomous robot behavior learning simulation by using the khepera robot simulator.","","978-0-7695-3514-2","10.1109/CIMCA.2008.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172685","","Learning;State-space methods;Orbital robotics;Fuzzy neural networks;Robots;Stochastic processes;Fuzzy systems;Inference algorithms;Computer science;Design engineering","continuous systems;control engineering computing;fuzzy neural nets;intelligent robots;learning (artificial intelligence);neurocontrollers;self-adjusting systems;state-space methods","self-organized fuzzy-neuro reinforcement learning system;continuous state space;Q-learning;stochastic gradient ascent;autonomous robot behavior learning;adaptive input state space classifier;khepera robot simulator","","7","","8","IEEE","24 Jul 2009","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Engine Idle Speed Control","J. Xue; Q. Gao; W. Ju","Coll. of Eng., Nanjing Agric. Univ., Nanjing, China; Coll. of Eng., Nanjing Agric. Univ., Nanjing, China; Coll. of Eng., Nanjing Agric. Univ., Nanjing, China","2010 International Conference on Measuring Technology and Mechatronics Automation","6 May 2010","2010","2","","1008","1011","A control method of neural network controller with reinforcement learning is proposed to implement idle speed control of an automobile engine to reduce fluctuation of the idle speed. Firstly, the reinforcement-learning neural network is demonstrated in detail. Then, the control scheme of the reinforcement-learning controller is designed to experiment. Q learning algorithm, as one of methods of reinforcement learning, is used for learning of the neural network, which is based on evaluating the system performance and giving credit for successful actions. After the proposed controller is trained fully, the contrast experiments are implemented on an engine test bench between the proposed controller and the original controller. Experimental results show that the reinforcement learning controller has better performance in speed fluctuation and its frequency and fuel economy than that of the original controller. And, the transition of the transient idle speed controlled by the proposed controller is more smooth and stable. Meanwhile, exhaust emissions are tested during the conditions controlled by the two types of controllers respectively. And results demonstrate that the proposed controller has better fuel economy because of its lower exhaust emissions.","2157-1481","978-1-4244-5739-7","10.1109/ICMTMA.2010.249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5459777","Reinforcement Learning;Neural Network;Idle Speed;Engine;Control","Learning;Engines;Velocity control;Neural networks;Fluctuations;Testing;Fuel economy;Automobiles;System performance;Frequency","automobiles;internal combustion engines;learning (artificial intelligence);neurocontrollers;velocity control","engine idle speed control;neural network controller;automobile engine;reinforcement-learning neural network;Q learning algorithm;fuel economy","","7","","18","IEEE","6 May 2010","","","IEEE","IEEE Conferences"
"Reinforcement learning with fuzzy evaluative feedback for a biped robot","Changjiu Zhou; Qingchun Meng","Department of Electronics & Communication Engineering, Singapore polytechnics, Singapore; Department of Electronics & Communication Engineering, Singapore polytechnics, Singapore","Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065)","6 Aug 2002","2000","4","","3829","3834 vol.4","Proposes a fuzzy reinforcement learning algorithm for biped gait synthesis. It is based on a modified GARIC (generalized approximate reasoning for intelligent control) architecture that can accept fuzzy evaluative feedback rather than a numerical one. The proposed gait synthesizer forms the initial gait from intuitive balancing knowledge, and it is then trained by the fuzzy reinforcement learning algorithm that uses a fuzzy critical signal to evaluate the degree of success for the biped dynamic walking by means of the zero moment point. The performance and applicability of the proposed method are illustrated through biped simulation.","1050-4729","0-7803-5886-4","10.1109/ROBOT.2000.845328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=845328","","Learning;Feedback;Legged locomotion;Signal synthesis;Network synthesis;Neural networks;Robot sensing systems;Synthesizers;Fuzzy logic;Humans","learning (artificial intelligence);inference mechanisms;uncertainty handling;feedback;legged locomotion;hierarchical systems;path planning;fuzzy set theory;motion control","fuzzy evaluative feedback;biped robot;fuzzy reinforcement learning algorithm;biped gait synthesis;modified GARIC architecture;modified generalized approximate reasoning for intelligent control architecture;intuitive balancing knowledge;fuzzy critical signal;degree of success;biped dynamic walking;zero moment point;biped simulation","","7","","16","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Extended State Observer Based Reinforcement Learning and Disturbance Rejection for Uncertain Nonlinear Systems","M. Ran; J. Li; L. Xie","the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","2020 IEEE 16th International Conference on Control & Automation (ICCA)","30 Nov 2020","2020","","","1398","1403","This paper investigates the extended state observer (ESO) based reinforcement learning (RL) and disturbance rejection for uncertain nonlinear systems having non-simple nominal models. The ESO is first designed to estimate the system state and the total uncertainty. Based on the output of the observer, the control compensates for the total uncertainty in real time, and simultaneously, online approximates the optimal policy for the compensated system using a simulation of experience based RL technique. Rigorous theoretical analysis is given to show that the widely-used restrictive persistence of excitation (PE) condition is not required in the established framework. Simulation results are presented to illustrate the effectiveness of the proposed method.","1948-3457","978-1-7281-9093-8","10.1109/ICCA51439.2020.9264478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264478","","Uncertainty;Artificial neural networks;Optimal control;Observers;Nonlinear systems;Reinforcement learning;Stability analysis","compensation;control system synthesis;learning (artificial intelligence);nonlinear control systems;observers;uncertain systems","extended state observer;compensated system;disturbance rejection;uncertain nonlinear systems;nonsimple nominal models;system state estimation;ESO-based reinforcement learning;optimal policy;experience based RL technique;restrictive persistence of excitation condition","","6","","30","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Research on Cooperative Collision Avoidance Problem of Multiple UAV Based on Reinforcement Learning","F. Bin; F. XiaoFeng; X. Shuo","Department of Traffic Admininstration and Engineering, Hunan Police Academy, Changsha, China; Department of Traffic Admininstration and Engineering, Hunan Police Academy, Changsha, China; Department of Traffic Admininstration and Engineering, Hunan Police Academy, Changsha, China","2017 10th International Conference on Intelligent Computation Technology and Automation (ICICTA)","2 Nov 2017","2017","","","103","109","To solve the problem of multi-UAV collision avoidance, propose the solution framework of multi-UAV collision avoidance based on the reinforcement learning. Firstly, four elements of state space, action space, the environment model and return value in the multi UAV cooperative collision avoidance problem were analyzed in depth. For the environment model, modeling it from two aspects of the UAV dynamic model and multi UAV relative distance calculation model; for the reward calculation, analyzing it from three aspects of the effect of collision avoidance, pre route deviation and maneuver execution cost. Then, proposing the multi UAV cooperative collision avoidance control algorithm based on the reinforcement learning. The experiment results show that the algorithm can effectively realize multi UAV coordination collision avoidance, control decision time is about 100 milliseconds, and the algorithm has practical feasibility.","","978-1-5386-1230-9","10.1109/ICICTA.2017.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8089913","UAV;Collision Avoidance;Reinforcement Learning","Collision avoidance;Atmospheric modeling;Analytical models;Security;Aerospace electronics;Predictive models","aerospace control;autonomous aerial vehicles;collision avoidance;learning (artificial intelligence);mobile robots","cooperative collision avoidance problem;reinforcement learning;multiUAV collision avoidance;environment model;relative distance calculation model;multiUAV coordination collision avoidance;time 100.0 ms","","6","","8","IEEE","2 Nov 2017","","","IEEE","IEEE Conferences"
"A hybrid architecture for hierarchical reinforcement learning","M. Huber","Department of Computer Science and Engineering, University of Technology, Arlington, TX, USA","Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065)","6 Aug 2002","2000","4","","3290","3295 vol.4","Autonomous robot systems operating in the real world have to be able to learn new tasks and environmental conditions without the need for an outside teacher. While reinforcement learning represents a good formalism to achieve this, its long learning times and need for extensive exploration often make it impracticable for online learning on complex systems. The hybrid architecture presented in this paper addresses this issue by applying reinforcement learning on top of an automatically derived abstract discrete event dynamic system (DEDS) supervisor. This reduces the problem of policy acquisition within this approach to learning to coordinate a set of closed-loop control strategies in order to perform a given task. Besides dramatically reducing the complexity of the learning task this framework also permits the incorporation of a priori knowledge and facilitates the inclusion of learned policies as actions in order to transfer skills to new task domains. To demonstrate the applicability of this approach, the architecture is used to learn locomotion gaits on a four-legged robot platform.","1050-4729","0-7803-5886-4","10.1109/ROBOT.2000.845170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=845170","","Robot kinematics;State-space methods;Control systems;Computer architecture;Computer science;Educational robots;Automatic control;Optimal control;Learning systems;Bridges","mobile robots;learning (artificial intelligence);discrete event systems;closed loop systems;computational complexity","hybrid architecture;hierarchical reinforcement learning;autonomous robot systems;environmental condition learning;online learning;complex systems;discrete event dynamic system supervisor;DEDS supervisor;policy acquisition;closed-loop control strategies;locomotion gaits;four-legged robot platform","","6","","10","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A Neural Network based Mobile Robot Navigation Approach using Reinforcement Learning Parameter Tuning Mechanism","C. Cheng; Y. Chen","Mechanical and Electrical Engineering, Soochow University, Suzhou, China; Mechanical and Electrical Engineering, Soochow University, Suzhou, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","2600","2605","Robotic navigation mechanism is one of the most adhoc research topics on mobile robots, which requires a robot to find a suitable path and move from its current position to a destination position without colliding with any obstacles. This paper employs the intelligent method of reinforcement learning to explore a solution to address the aforementioned problem. It considers the laser beam detected distances and the relative movement angle as the input of neural network model, and the robot’s action posture is denoted as the output. This neural network model is trained by a deep Q-learning network (DQN) algorithm via positive and negative feedback rewards defined by task-specific learning goals. In this sense, the trained model helps the robot determine the appropriate action to take at each state to safely reach the destination point without any manually interference. According to the results on a simulation platform, the trained neural network model makes the robot move from random starting point to random destination successfully, which proves the effectiveness of DQN algorithm in the field of robot navigation.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728061","robotic navigation;neural network;reinforcement learning","Training;Q-learning;Navigation;Neural networks;Robot sensing systems;Laser beams;Mobile robots","control engineering computing;mobile robots;navigation;neural nets;path planning;position control;reinforcement learning","action posture;adhoc research topics;deep Q-learning network algorithm;destination point;destination position;intelligent method;mobile robot navigation approach;mobile robots;negative feedback rewards;positive feedback rewards;random destination;reinforcement learning parameter tuning mechanism;relative movement angle;robotic navigation mechanism;suitable path;task-specific learning goals;trained model;trained neural network model","","6","","16","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Multigrid Methods for Policy Evaluation and Reinforcement Learning","O. Ziv; N. Shimkin","Department of Electrical Engineering, Technion University, Haifa, Israel; Department of Electrical Engineering, Technion University, Haifa, Israel","Proceedings of the 2005 IEEE International Symposium on, Mediterrean Conference on Control and Automation Intelligent Control, 2005.","13 Mar 2006","2005","","","1391","1396","We introduce a new class of multigrid temporal-difference learning algorithms for speeding up the estimation of the value function related to a stationary policy, within the context of discounted cost Markov decision processes with linear functional approximation. The proposed scheme builds on the multi-grid framework which is used in numerical analysis to enhance the iterative solution of linear equations. We first apply the multigrid approach to policy evaluation in the known model case. We then extend this approach to the learning case, and propose a scheme in which the basic TD(lambda) learning algorithm is applied at various resolution scales. The efficacy of the proposed algorithms is demonstrated through simulation experiments","2158-9879","0-7803-8936-0","10.1109/.2005.1467218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467218","","Multigrid methods;Learning;Equations;Convergence;Iterative algorithms;Function approximation;Dynamic programming;Error correction;State-space methods;Computational complexity","differential equations;iterative methods;learning (artificial intelligence);Markov processes;optimal control","multigrid method;policy evaluation;reinforcement learning;multigrid temporal-difference learning algorithm;value function estimation;stationary policy;discounted cost Markov decision process;linear functional approximation;numerical analysis;iterative solution;linear equations","","6","","18","IEEE","13 Mar 2006","","","IEEE","IEEE Conferences"
"Human-robot collaborative manipulation through imitation and reinforcement learning","Ye Gu; A. Thobbi; W. Sheng","Department of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK, USA; Department of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK, USA; Department of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK, USA","2011 IEEE International Conference on Information and Automation","11 Jul 2011","2011","","","151","156","This paper proposes a two-phase learning framework for human-robot collaborative manipulation tasks. A table-lifting task performed jointly by a human and a humanoid robot is considered. In order to perform the task, the robot should learn to hold the table at a suitable position and then perform the lifting task cooperatively with the human. Accordingly, learning is split into two phases. The first phase enables the robot to reach out and hold one end of the table. A Programming by Demonstration (PbD) algorithm based on GMM/GMR is used to accomplish this. In the second phase the robot switches its role to an agent learning to collaborate with the human on the task. A guided reinforcement learning algorithm is developed. Using the proposed framework, the robot can successfully learn to reach and hold the table and keep the table horizontal during lifting it up with human in a reasonable amount of time.","","978-1-4577-0270-9","10.1109/ICINFA.2011.5948979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5948979","Humanoids;Human-Robot Collaboration;Cooperative Manipulation;Imitation learning;Reinforcement learning","Robot kinematics;Humans;Calibration;Wrist;Torso;Robot vision systems","groupware;humanoid robots;human-robot interaction;learning (artificial intelligence);lifting;manipulators;robot programming;task analysis","human-robot collaborative manipulation tasks;table-lifting task;humanoid robot;programming by demonstration algorithm;agent learning;guided reinforcement learning","","6","","17","IEEE","11 Jul 2011","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-based ROS-Controlled RC Car for Autonomous Path Exploration in the Unknown Environment","S. Hossain; O. Doukhi; Y. Jo; D. -J. Lee","School of Mechanical & Convergence System Engineering, Kunsan National University, Gunsan, Republic of Korea; School of Mechanical & Convergence System Engineering, Kunsan National University, Gunsan, Republic of Korea; School of Mechanical & Convergence System Engineering, Kunsan National University, Gunsan, Republic of Korea; School of Mechanical & Convergence System Engineering, Kunsan National University, Gunsan, Republic of Korea","2020 20th International Conference on Control, Automation and Systems (ICCAS)","1 Dec 2020","2020","","","1231","1236","Nowadays, Deep reinforcement learning has become the front runner to solve problems in the field of robot navigation and avoidance. This paper presents a LiDAR-equipped RC car trained in the GAZEBO environment using the deep reinforcement learning method. This paper uses reshaped LiDAR data as the data input of the neural architecture of the training network. This paper also presents a unique way to convert the LiDAR data into a 2D grid map for the input of training neural architecture. It also presents the test result from the training network in different GAZEBO environment. It also shows the development of hardware and software systems of embedded RC car. The hardware system includes-Jetson AGX Xavier, teensyduino and Hokuyo LiDAR; the software system includes-ROS and Arduino C. Finally, this paper presents the test result in the real world using the model generated from training simulation.","2642-3901","978-89-93215-20-5","10.23919/ICCAS50221.2020.9268370","Korea Institute for Advancement of Technology; National Research Foundation of Korea; National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268370","Deep-Q Network;Laser Map;ROS;Gazebo Simulation;Path Exploration","Laser radar;Training;Neural networks;Robot sensing systems;Two dimensional displays;Shape;Meters","collision avoidance;learning (artificial intelligence);mobile robots;neural net architecture;optical radar;path planning;radar computing","ROS-controlled RC car;autonomous path exploration;unknown environment;robot navigation;LiDAR-equipped RC car;LiDAR data;neural architecture;GAZEBO environment;embedded RC car;Hokuyo LiDAR;deep reinforcement learning;robot avoidance;2D grid map;Jetson AGX Xavier;teensyduino;ROS software system;Arduino C software system","","6","","20","","1 Dec 2020","","","IEEE","IEEE Conferences"
"Deep reinforcement learning based neuro-control for a two-dimensional magnetic positioning system","E. Bejar; A. Moran","Engineering Department, Pontifical Catholic University of Peru, Lima, Peru; Engineering Department, Pontifical Catholic University of Peru, Lima, Peru","2018 4th International Conference on Control, Automation and Robotics (ICCAR)","14 Jun 2018","2018","","","268","273","This paper presents a control scheme based on deep reinforcement learning for a two-dimensional positioning system with electromagnetic actuators. Two neuro-controllers are trained and used for controlling the X-Y position of an object. The neuro-controllers learning approach is based on the actor-critic architecture and the deep deterministic policy gradient (DDPG) algorithm using the Q-learning method. The performance of the control system is verified for different setpoints and working conditions.","","978-1-5386-6338-7","10.1109/ICCAR.2018.8384682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8384682","reinforcement learning;deep deterministic policy gradient (DDPG) algorithm;Q-learning;magnetic positioning system","Electromagnets;Mathematical model;Control systems;Magnetic hysteresis;Magnetomechanical effects;Machine learning;Magnetic tunneling","electromagnetic actuators;gradient methods;learning (artificial intelligence);neurocontrollers;position control","two-dimensional magnetic positioning system;electromagnetic actuators;neuro-controllers learning approach;actor-critic architecture;deep deterministic policy gradient algorithm;Q-learning method;deep reinforcement learning based neuro-control;DDPG","","5","","14","IEEE","14 Jun 2018","","","IEEE","IEEE Conferences"
"Dynamic Self-Generated Fuzzy Systems for Reinforcement Learning","Meng Joo Er; Yi Zhou","Intelligent Systems Center, Singapore; Intelligent Systems Center, Singapore","International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)","22 May 2006","2005","1","","193","198","A novel methodology for generating fuzzy reinforcement learning systems without a prior knowledge and expert effect named as dynamic self-generated fuzzy Q-learning (DSGFQL) has been proposed in this paper. Compared with authors' previous work on dynamic fuzzy Q-learning (DFQL), DSGFQL offers an automatical generation method for fuzzy reinforcement learning with capabilities of creating as well as pruning fuzzy rules. Similar as the DFQL, epsiv-completeness criterion is applied for recruiting new fuzzy rules. At the same time, global and local reward criterions are adopted for parameters modification for fuzzy rules which pass the epsiv-completeness criterion. In DSGFQL, local reward and local firing strength have been utilized for deleting unsatisfactory and unnecessary fuzzy rules. In this paper, DSGFQL has been applied for a wall-following task of a mobile robot. Experiment results and comparative studies between the novel DSGFQL and DFQL demonstrate that the proposed DSGFQL is superior to the DFQL in both overall performance and computational efficiency as the number of failures is fewer, the reward is bigger and the number of fuzzy rules is smaller. Moreover, the proposed framework can be applied of generating fuzzy inference systems (FIS) automatically for other reinforcement learning methods as well","","0-7695-2504-0","10.1109/CIMCA.2005.1631264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631264","","Fuzzy systems;Computational efficiency;Fuzzy logic;Space technology;Erbium;Intelligent systems;Recruitment;Mobile robots;Supervised learning;Feedback","fuzzy reasoning;fuzzy systems;learning (artificial intelligence);mobile robots","dynamic self-generated fuzzy Q-learning;reinforcement learning;fuzzy rule;wall following task;global reward criterion;local reward criterion;parameter modification;mobile robot;fuzzy inference system","","5","","13","IEEE","22 May 2006","","","IEEE","IEEE Conferences"
"An Intersection Signal Control Method Based on Deep Reinforcement Learning","P. Ha-li; D. Ke","College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China","2017 10th International Conference on Intelligent Computation Technology and Automation (ICICTA)","2 Nov 2017","2017","","","344","348","Urban traffic flow is dynamic and uncertain. In this paper, we combine the deep learning and the reinforcement learning, and design an intersection signal controller based on Q-learning and convolutional neural network. We redefine the state space and the reward function. The training and simulation of the controller are carried out in traffic micro-simulator SUMO. Compared with timing control, the results show that the method we have proposed is feasible and more effective.","","978-1-5386-1230-9","10.1109/ICICTA.2017.83","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8089966","Deep leaning;Reinforcement learning;Intersection signal control","Learning (artificial intelligence);Green products;Convolution;Neural networks;Training;Delays","control engineering computing;convolution;learning (artificial intelligence);neural nets;road traffic control;traffic engineering computing","traffic microsimulator SUMO;timing control;intersection signal control method;deep reinforcement learning;urban traffic flow;deep learning;Q-learning;convolutional neural network;state space;reward function","","5","","8","IEEE","2 Nov 2017","","","IEEE","IEEE Conferences"
"Towards Robotic Picking of Targets with Background Distractors using Deep Reinforcement Learning","C. Chen; H. -Y. Li; X. Zhang; X. Liu; U. -X. Tan","School of Software and Microelectronics, Peking University, China; Pillar of Engineering Product Development, Singapore University of Technology of Design, Singapore; School of Software and Microelectronics, Peking University, China; School of Software and Microelectronics, Peking University, China; Pillar of Engineering Product Development, Singapore University of Technology of Design, Singapore","2019 WRC Symposium on Advanced Robotics and Automation (WRC SARA)","16 Dec 2019","2019","","","166","171","Robotic manipulation using vision-based learning algorithms has benefits many tasks where the robot needs to interact with a desired target. However, when a desired target is placed with other objects, there remains several technical challenges. Firstly, the target might be occluded by other objects in terms of the camera field of view. Secondly, prioritizing the selection of the objects changes the polices and rewards of the learning algorithm which might deteriorate the successful rates. Thirdly, the occlusion by other objects increases the difficulty of manipulation as it requires complex synergies between pushing and grasping. Therefore, we propose a value-based deep reinforcement learning with Mask R-CNN to address the issues of robotic manipulation for multiple object. In the proposed three rewards are proposed, namely: 1) success of grasping the desired target; 2) removal of the distractors; and 3) effective pushes. This method aims to enable the manipulation to grasp a desired target among distractors. Simulation has been conducted to demonstrate the effectiveness of the proposed method. The results show that the robot is able to effectively pick the desired targets out despite the physical and optical occlusion by other objects. All tasks are completed with 100% successful rate within 30 episodes.","","978-1-7281-5552-4","10.1109/WRC-SARA.2019.8931932","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8931932","","Robots;Grasping;Training;Reinforcement learning;Task analysis;Cameras;Shape","control engineering computing;convolutional neural nets;learning (artificial intelligence);manipulators;robot vision","physical occlusion;optical occlusion;mask R-CNN;complex synergies;multiple object;value-based deep reinforcement learning;robotic manipulation","","5","","26","IEEE","16 Dec 2019","","","IEEE","IEEE Conferences"
"Controlling study of D-STATCOM based on reinforcement learning adaptive PID","X. -p. Meng; H. Wang; L. Zhao; H. Zhang","Department of Electrical Engineering, Changchun Institute of Posts and Telecommunications, Changchun, Jilin, China; Department of Electrical Engineering, Changchun Institute of Posts and Telecommunications, Changchun, Jilin, China; Gongzhuling Power Supply, Jilin Electric Power Supply Company, Changchun, Jilin, China; Department of Electrical Engineering, Changchun Institute of Posts and Telecommunications, Changchun, Jilin, China","2009 IEEE International Conference on Automation and Logistics","25 Sep 2009","2009","","","1208","1211","Distribution Static Compensator (DSTATCOM) is a shunt compensation device which is generally used to solve power quality problems in distribution systems. In distribution power system, these power quality problems mainly arise due to the pulsed loads, which causes the degradation of the entire system performance. The control strategy of DSTATCOM plays an important role to meet the objectives. A novel adaptive control strategy for the DSTATCOM is based on the Reinforcement Learning Adaptive PID (RLA-PID). With this control method, the compensator can stabilize the PCC voltage around a prescribed value. A distributes power system is developed in MATLAB environment, the validity of the method proposed is verified.","2161-816X","978-1-4244-4794-7","10.1109/ICAL.2009.5262681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5262681","DSTATCOM;Intelligent Control;Reinforcement Learning Adaptive PID;Power quality","Automatic voltage control;Learning;Programmable control;Adaptive control;Three-term control;Power quality;Pulse power systems;STATCOM;Degradation;System performance","adaptive control;learning (artificial intelligence);learning systems;power distribution control;power supply quality;static VAr compensators;three-term control","D-STATCOM control;distribution static compensator;reinforcement learning adaptive PID control;shunt compensation device;power quality;distribution power system","","5","","10","IEEE","25 Sep 2009","","","IEEE","IEEE Conferences"
"A Heuristic Reinforcement Learning for Robot Approaching Objects","B. Wang; J. w. Li; H. Liu","Robot Research Institute, Harbin Institute of Technology, Harbin, China; Robot Research Institute, Harbin Institute of Technology, Harbin, China; German AeroSpace Center(DLR), Institute of Robotics and Mechatronics, Wessling, Germany","2006 IEEE Conference on Robotics, Automation and Mechatronics","4 Dec 2006","2006","","","1","5","Autonomous approaching objects for an arm-hand robot is a very difficult problem because the possible arm-hand configurations are numerous. In this paper, we propose a modified reinforcement learning algorithm for a multifingered hand approaching target objects. The proposed approach integrates the heuristic search information with the learning system, and solves the problem of how an arm-hand robot approaches objects before grasping. In addition, this method also overcomes the problem of time consuming of traditional reinforcement learning in the initial learning phase. The algorithm is applied to an arm-hand robot to approach objects before grasping, which can enable the robot to learn approaching skill by trial-and-error and plan its path by itself. The experimental results demonstrate the effectiveness of the proposed algorithm","2158-219X","1-4244-0024-4","10.1109/RAMECH.2006.252749","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4018865","Reinforce learning;A* search;Grasping;Multifingered hand","Robot sensing systems;Orbital robotics;Grasping;Learning systems;Mechatronics;Acceleration;Aerodynamics;Sensor systems;Heuristic algorithms;Path planning","control engineering computing;heuristic programming;intelligent robots;learning (artificial intelligence);manipulators;search problems","heuristic reinforcement learning;arm-hand robot;multifingered hand;heuristic search","","5","","12","IEEE","4 Dec 2006","","","IEEE","IEEE Conferences"
"A Method of Reinforcement Learning Based Automatic Traffic Signal Control","W. Yaping; Z. Zheng","School of Mechanical Engineering, Xi''an Jiaotong University, Xi'an, China; School of Mechanical Engineering, Xi''an Jiaotong University, Xi'an, China","2011 Third International Conference on Measuring Technology and Mechatronics Automation","28 Feb 2011","2011","1","","119","122","To improve performance of traffic signal control system in urban area, a novel method is proposed in this paper. The roads, vehicles and the traffic control systems are all modeled as intelligent agents. Wireless communication network provides the possibility of the cooperation of vehicles and roads. Based on all the information from vehicles and roads, a traffic control policy can be planned online according to the updated situation on the roads. The optimum intersection signals can be learned automatically on line based on reinforcement learning. An intersection signal control system is studied as an example of the method with a Q-learning based algorithm. The simulation results show that the proposed intersection signal control can improve traffic efficiency by about 30% over the traditional method.","2157-1481","978-1-4244-9010-3","10.1109/ICMTMA.2011.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720736","Intelligent Transportation System;Traffic Control Mechanism;Reinforcement Learning","Vehicles;Traffic control;Learning;Roads;Sensors","automated highways;learning (artificial intelligence);radio networks;road traffic;road vehicles;traffic control","reinforcement learning;automatic traffic signal control;urban area;road vehicle;road traffic;intelligent agent;wireless communication network;intersection signal control;Q-learning algorithm;intelligent transportation system","","5","","9","IEEE","28 Feb 2011","","","IEEE","IEEE Conferences"
"Autonomous Robot Navigation in Dynamic Environment Using Deep Reinforcement Learning","X. Qiu; K. Wan; F. Li","China Aeronautical Radio Electronics, Research Institute, Science and Technology on Avionics Integration Laboratory, Shanghai, China; Northwestern Polytechnical University, School of Electronics and Information, Xi'an, China; Northwestern Polytechnical University, School of Electronics and Information, Xi'an, China","2019 IEEE 2nd International Conference on Automation, Electronics and Electrical Engineering (AUTEEE)","12 Mar 2020","2019","","","338","342","Compared to traditional control methods, deep reinforcement learning (DRL) has the ability to learn how to solve complex tasks in a dynamic environment simply by collecting experience. In this paper, we study the application of DRL method in robotic autonomous control with detection capability in simulated dynamic environment. More specifically, we have adopted Deep Q Network (DQN), double DQN and dueling DQN algorithms in DRL. As with fixed reward settings, these original DRL algorithms do not perform well while navigating a robot in dynamic environment. To address the problems, we designed a novel reward shaping method and conducted a series of experiment with all three improved DRL algorithms. The results show that the new reward shaping method can significantly improve the DRL performance when they are applied in robot navigation settings.","","978-1-7281-5030-7","10.1109/AUTEEE48671.2019.9033166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9033166","robot navigation;deep reinforcement learning;deep Q network;reward shaping","Navigation;Heuristic algorithms;Machine learning;Robot sensing systems;Neural networks;Collision avoidance","learning (artificial intelligence);mobile robots;path planning","autonomous robot navigation;deep reinforcement learning;traditional control methods;DRL method;robotic autonomous control;simulated dynamic environment;deep Q network;double DQN;dueling DQN algorithms;fixed reward settings;original DRL algorithms;reward shaping method;improved DRL algorithms;DRL performance;robot navigation settings","","5","","21","IEEE","12 Mar 2020","","","IEEE","IEEE Conferences"
"Biological robot arm motion through reinforcement learning","J. Izawa; T. Kondo; K. Ito","Department of Computational Intelligence and Systems Science, Institute of Technology (TIT), Tokyo, Japan; Department of Computational Intelligence and Systems Science, Institute of Technology (TIT), Tokyo, Japan; Department of Computational Intelligence and Systems Science, Institute of Technology (TIT), Tokyo, Japan","Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292)","7 Aug 2002","2002","4","","3398","3403 vol.4","The present paper discusses an optimal control method of biological robot arm which has redundancy of the mapping from the control input to the task goal. The control input space is divided into a couple of subspaces according to a priority order depending on the progress and stability of learning. In the proposed method, the search noise which is required for reinforcement learning is restricted within the first priority subspace. Then the constraint is relaxed with the progress of learning, and the search space extends to the second priority subspace in accordance with the history of learning. The method was applied to the musculoskeletal system as an example of biological control systems. Dynamic manipulation is obtained through reinforcement learning with no previous knowledge of the arm's dynamics. The effectiveness of the proposed method is shown by computational simulation.","","0-7803-7272-7","10.1109/ROBOT.2002.1014236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1014236","","Robots;Learning;Orbital robotics;Optimal control;Biological control systems;Manipulator dynamics;Stability;Subspace constraints;History;Musculoskeletal system","manipulator dynamics;learning (artificial intelligence);neural nets;motion control;optimal control","optimal control;biological robot arm;stability;reinforcement learning;search space;musculoskeletal system;dynamic manipulation;dynamics;biomimetic robot;neural network;over-actuated system","","5","","8","IEEE","7 Aug 2002","","","IEEE","IEEE Conferences"
"Reinforcement learning based flight controller capable of controlling a quadcopter with four, three and two working motors","A. R. Dooraki; D. -J. Lee","Department of Mechanical Engineering, Smart Autonomous System Lab, Kunsan National University, Korea; Department of Mechanical Engineering, Smart Autonomous System Lab, Kunsan National University, Korea","2020 20th International Conference on Control, Automation and Systems (ICCAS)","1 Dec 2020","2020","","","161","166","In this research, we show how a reinforcement learning based algorithm called Fault-Tolerant Bio-inspired Flight Controller (FT-BFC) is capable of training a single neural network based model to fly a quadcopter with two, three, and four working rotors. Our algorithm can learn a low-level flight controller that directly controls angular velocities of motors to fly a quadcopter when it has four fully functional motors, and also, despite having one or two motor failures (That is, our proposed flight controller is a fault-tolerant controller as well). In the training and running of our controller, we do not use any conventional flight controller, such as a PID or SMC controller. We test our algorithm in a simulation environment, Gazebo simulator, and illustrate our simulation results that backing up our algorithm capabilities. Finally, before concluding our paper, we discuss the implementation of our algorithm in a real quadcopter.","2642-3901","978-89-93215-20-5","10.23919/ICCAS50221.2020.9268270","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268270","Reinforcement Learning;Bio-inspired Flight Controller;Fault Tolerant Controller","Reinforcement learning;Training;Fault tolerant systems;Fault tolerance;Angular velocity;Heuristic algorithms;Trajectory","aerospace simulation;aircraft control;angular velocity control;failure (mechanical);fault tolerant control;helicopters;learning (artificial intelligence);neurocontrollers;rotors (mechanical)","quadcopter control;reinforcement learning;working rotors;neural network training;flight controller;motor failures;fault tolerant bio-inspired flight controller;angular velocities;Gazebo simulator","","4","","10","","1 Dec 2020","","","IEEE","IEEE Conferences"
"Lightweight Multi Car Dynamic Simulator for Reinforcement Learning","A. Majumdar; P. Benavidez; M. Jamshidi","Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX; Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX; Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX","2018 World Automation Congress (WAC)","9 Aug 2018","2018","","","1","6","With improvements in reinforcement learning algorithms, and the demand to implement these algorithms on real systems, the use of a simulator as an intermediate stage is essential to save time, material and financial resources. The lack of particular features in a unified simulator for applications to autonomous cars and robotics, encouraged this research, which produced a simulator capable of simulating multiple car like objects, in either one or several arenas (environments). Being a lightweight application, multiple instances of the simulator can run at the same time, only constrained by the available computational resources.","","978-1-5323-7791-4","10.23919/WAC.2018.8430473","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8430473","Multi-agent systems;Unmanned autonomous vehicles;Simulation;Reinforcement learning;Multiple instances;Multi Programming environments","Automobiles;Graphical user interfaces;Learning (artificial intelligence);Heuristic algorithms;Computational modeling;Testing;Libraries","digital simulation;learning (artificial intelligence);robots;traffic engineering computing","lightweight multicar dynamic simulator;reinforcement learning algorithms;unified simulator;autonomous cars;robotics;lightweight application;multiple car simulation","","4","","13","","9 Aug 2018","","","IEEE","IEEE Conferences"
"Model-free linear quadratic tracking control for unmanned helicopters using reinforcement learning","D. Lee; M. Choi; H. Bang","Division of AeroSpace Engineering, School of Mechanical, AeroSpace & Systems Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Division of AeroSpace Engineering, School of Mechanical, AeroSpace & Systems Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Division of AeroSpace Engineering, School of Mechanical, AeroSpace & Systems Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","The 5th International Conference on Automation, Robotics and Applications","2 Feb 2012","2011","","","19","22","This paper addresses the autonomous flight control system of an unmanned helicopter. We adopt a model-free discrete linear quadratic tracking (LQT) control architecture based on reinforcement learning algorithm by rewriting the Q-learning approach. From input and output data, the linear quadratic optimal gain is directly found without system identification procedure. Least square method is adopted in order to estimate the Q-value and the parameters related to optimal control gain. This methodology does not access to an exact model of the system and can be applied to full flight envelop maneuvering from hovering to aggressive flight with small modification. We constructed numerical simulations to evaluate the proposed algorithm with a discrete linear model of the unmanned helicopter.","","978-1-4577-0330-0","10.1109/ICARA.2011.6144849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6144849","Linear Quadratic Control;Reinforcement Learning;Unmanned Helicopter","Helicopters;Learning;Feedback control;Control systems;Aerospace control;Rotors;Modeling","aerospace computing;aircraft control;autonomous aerial vehicles;discrete systems;helicopters;learning (artificial intelligence);least squares approximations;linear quadratic control;linear systems;mobile robots;parameter estimation","unmanned helicopter;reinforcement learning;autonomous flight control system;model-free discrete linear quadratic tracking control architecture;Q-learning approach;linear quadratic optimal gain;least square method;Q-value estimation;parameter estimation;optimal control gain;full flight envelop maneuvering;numerical simulation;discrete linear model","","4","","10","IEEE","2 Feb 2012","","","IEEE","IEEE Conferences"
"A Model-free Deep Reinforcement Learning Approach for Robotic Manipulators Path Planning","W. Liu; H. Niu; M. N. Mahyuddin; G. Herrmann; J. Carrasco","Department of Electrical and Electronic Engineering, The University of Manchester, Manchester, UK; Department of Electrical and Electronic Engineering, The University of Manchester, Manchester, UK; School of Electrical and Electronic Engineering, Universiti Sains Malaysia, Pulau Pinang, Malaysia; Department of Electrical and Electronic Engineering, The University of Manchester, Manchester, UK; Department of Electrical and Electronic Engineering, The University of Manchester, Manchester, UK","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","512","517","Path planning problems have attracted much attention in robotic fields such as manipulators. In this paper, a model-free off-policy actor critic based deep reinforcement learning method is proposed to solve the classical path planning problem of a UR5 robot arm. Unlike standard path planning methods, the reward design of the proposed method contains smoothness reward, which assures smooth trajectory of the UR5 robot arm when accomplishing path planning tasks. Additionally, the proposed method does not rely on any model while the standard path planning method is model-based. The proposed method not only guarantees that the joint angle of the UR5 robotic arm lies within the allowable range each time when it reaches the random target point, but also ensures that the joint angle of the UR5 robotic arm is always within the allowable range during the entire episode of training. A standard path planning method was implemented in Robot Operating System (ROS) and the proposed method was applied in CoppeliaSim to validate the feasibility. It can be inferred from the experiment that the training with the proposed method is successful.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649802","EPSRC(grant numbers:EP/S03286X/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649802","Model-free;Deep Reinforcement Learning;Manipulators;Path Planning","Training;Simulation;Operating systems;Reinforcement learning;Manipulators;Hardware;Trajectory","learning (artificial intelligence);manipulators;mobile robots;path planning","path planning problems;robotic fields;model-free off-policy actor critic;deep reinforcement learning method;classical path planning problem;UR5 robot arm;standard path planning method;accomplishing path planning tasks;UR5 robotic arm;Robot Operating System;model-free deep reinforcement;robotic manipulators path planning","","4","","27","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Reinforcement learning method for continuous state space based on dynamic neural network","Wei Sun; Xuesong Wang; Yuhu Cheng","School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou, Jiangsu, China; School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou, Jiangsu, China; School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou, Jiangsu, China","2008 7th World Congress on Intelligent Control and Automation","8 Aug 2008","2008","","","750","754","One of the difficulties encountered in the application of reinforcement learning methods to real-world problem is the generalization of large-scale or continuous state space. In order to solve the curse of dimensionality problem caused by discretizing continuous state space, a kind of Q-learning method for continuous state space based on a dynamic Elman neural network was proposed in this paper. The inputs and the output of Elman network are the system state-action pair and the corresponding Q-value. That is, Elman network is used to estimate the Q-value of state-action pair on-line. Eligibility trace for connecting weights is introduced by borrowing ideas from the eligibility trace mechanism of state in temporal difference algorithm to improve the learning speed of neural network. Computer simulations on mountain car control illustrate the performance and applicability of the proposed Q-learning scheme.","","978-1-4244-2113-8","10.1109/WCICA.2008.4594438","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4594438","reinforcement learning;Q-learning;state space;dynamic neural network;generalization","Learning;Artificial neural networks;Nonlinear dynamical systems;Neurons;Mathematical model;Learning systems;Machine learning","continuous systems;generalisation (artificial intelligence);large-scale systems;learning (artificial intelligence);neurocontrollers;state-space methods","reinforcement learning control method;continuous state space generalization;dynamic Elman neural network;large-scale system generalization;Q-learning method;state-action pair;Q-value estimation;eligibility trace mechanism;temporal difference algorithm","","4","","16","IEEE","8 Aug 2008","","","IEEE","IEEE Conferences"
"Resource Allocation for a Wireless Coexistence Management System based on Reinforcement Learning","P. Söffker; D. Block; N. Wiebusch; U. Meier","inIT, OWL University of Applied Sciences, Lemgo, Germany; inIT, OWL University of Applied Sciences, Lemgo, Germany; inIT, OWL University of Applied Sciences, Lemgo, Germany; inIT, OWL University of Applied Sciences, Lemgo, Germany","2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA)","25 Oct 2018","2018","1","","1101","1104","In industrial environments an increasing amount of wireless devices are used, which utilize licence-free bands. As a consequence of this mutual interferences of wireless systems might decrease the state of coexistence. Therefore, a central coexistence management system is needed, which allocates conflict-free resources to wireless systems. To ensure a conflict-free resource utilization, it is useful to predict the prospective medium utilisation before resources are allocated. This paper presents a self learning concept, which is based on reinforcement learning. A simulative evaluation of reinforcement learning agents based on neural networks, called deep Q-networks and double deep Q-networks, was realised for exemplary and practically relevant coexistence scenarios. The evaluation of the double deep Q-network showed, that a prediction accuracy of at least 98 % can be reached in all investigated scenarios.","1946-0759","978-1-5386-7108-5","10.1109/ETFA.2018.8502447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502447","","Resource management;Wireless communication;Radio frequency;Wireless LAN;Optical wavelength conversion;Neural networks","learning (artificial intelligence);neural nets;radio networks;resource allocation","resource allocation;wireless coexistence management system;wireless devices;licence-free bands;mutual interferences;central coexistence management system;conflict-free resource utilization;prospective medium utilisation;self learning concept;reinforcement learning agents;double deep Q-network;practically relevant coexistence scenarios","","4","","11","IEEE","25 Oct 2018","","","IEEE","IEEE Conferences"
"Vision-based behavior for UAV reactive avoidance by using a reinforcement learning method","M. Zhaowei; N. Yifeng; S. Lincheng","National University of Defense Technology, Changsha, Hunan, CN; National University of Defense Technology, Changsha, Hunan, CN; National University of Defense Technology, Changsha, Hunan, CN","2016 12th World Congress on Intelligent Control and Automation (WCICA)","29 Sep 2016","2016","","","3301","3306","Reactive behavior is a classic control pattern in both robotic and biological systems. Inspired by this conception, this paper focuses on the mapping from the visual sensor to the actor controller by using a reinforcement learning method. This paper proposes an actor-critic algorithm based on the RBF neural network to achieve the unmanned aerial vehicle (UAV) avoidance ability. A semi-physical experiment is implemented to verify the effectiveness of the proposed algorithm.","","978-1-4673-8414-8","10.1109/WCICA.2016.7578765","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7578765","","Unmanned aerial vehicles;Learning (artificial intelligence);Mobile robots;Collision avoidance;Atmospheric modeling;Predictive models","autonomous aerial vehicles;collision avoidance;learning (artificial intelligence);radial basis function networks;robot vision","vision-based behavior;UAV reactive avoidance;reinforcement learning method;reactive behavior;visual sensor;actor controller;actor-critic algorithm;RBF neural network;unmanned aerial vehicle avoidance ability;UAV avoidance ability;unmanned aerial vehicle","","4","","","IEEE","29 Sep 2016","","","IEEE","IEEE Conferences"
"Deep reinforcement learning method based on DDPG with simulated annealing for satellite attitude control system","R. Su; F. Wu; J. Zhao","Chinese Academy of Sciences, Institute of Software, Beijing, China; Chinese Academy of Sciences, Institute of Software, Beijing, China; Chinese Academy of Sciences, Institute of Software, Beijing, China","2019 Chinese Automation Congress (CAC)","13 Feb 2020","2019","","","390","395","Satellite attitude control has been a hot issue in recent years. The rapid development of deep reinforcement learning provides a new choice for the solution of traditional control problems. A deep reinforcement learning algorithm based on DDPG for satellite attitude control system has been proposed by this paper. The method uses a simulated annealing algorithm to seek out the global optimal policy. This method can achieve robust effect in a shorter period of time compared with other algorithms through experiments.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8996860","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8996860","satellite;deep reinforcemnt learning;simulated annealing;attitude control","Satellites;Attitude control;Machine learning;Learning (artificial intelligence);Simulated annealing;Actuators;Approximation algorithms","artificial satellites;attitude control;gradient methods;learning (artificial intelligence);neurocontrollers;simulated annealing;stability","DDPG;satellite attitude control;deep reinforcement learning algorithm;simulated annealing algorithm;deep deterministic policy gradient;control system stability;neural network training","","4","","20","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"A Novel Neuromorphic Processors Realization of Spiking Deep Reinforcement Learning for Portfolio Management","S. A. Saeidi; F. Fallah; S. Barmaki; H. Farbeh","Department of Computer Engineering, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran; Department of Computer Engineering, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran; Department of Computer Engineering, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran; Department of Computer Engineering, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran","2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)","19 May 2022","2022","","","68","71","The process of constantly reallocating budgets into financial assets, aiming to increase the anticipated return of assets and minimizing the risk, is known as portfolio management. Processing speed and energy consumption of portfolio management have become crucial as the complexity of their real-world applications increasingly involves high-dimensional observation and action spaces and environment uncertainty, which their limited onboard resources cannot offset. Emerging neuromorphic chips inspired by the human brain increase processing speed by up to 500 times and reduce power consumption by several orders of magnitude. This paper proposes a spiking deep reinforcement learning (SDRL) algorithm that can predict financial markets based on unpredictable environments and achieve the defined portfolio management goal of profitability and risk reduction. This algorithm is optimized for Intel's Loihi neuromorphic processor and provides 186x and 516x energy consumption reduction compared to a high-end processor and GPU, respectively. In addition, a 1.3x and 2.0x speed-up is observed over the high-end processors and GPUs, respectively. The evaluations are performed on cryptocurrency market benchmark between 2016 and 2021.","1558-1101","978-3-9819263-6-1","10.23919/DATE54114.2022.9774598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774598","neuromorphic computing;deep reinforcement learning;portfolio management;Loihi","Energy consumption;Uncertainty;Power demand;Neuromorphics;Profitability;Graphics processing units;Reinforcement learning","brain;coprocessors;deep learning (artificial intelligence);financial data processing;investment;multiprocessing systems;neural chips;power aware computing;power consumption;stock markets","financial assets;energy consumption reduction;high-dimensional observation;action spaces;neuromorphic chips;human brain;power consumption reduction;spiking deep reinforcement learning algorithm;high-end processors;neuromorphic processors realization;portfolio management;processing speed;profitability;risk reduction;Loihi neuromorphic processor;GPU;cryptocurrency market","","4","","15","","19 May 2022","","","IEEE","IEEE Conferences"
"Application of reinforcement learning based on neural network to dynamic obstacle avoidance","Junfei Qiao; Zhanjun Hou; Xiaogang Ruan","College of Electronic Information and Control Engineering, Beijing University of Technology, Beijing, China; College of Electronic Information and Control Engineering, Beijing University of Technology, Beijing, China; College of Electronic Information and Control Engineering, Beijing University of Technology, Beijing, China","2008 International Conference on Information and Automation","26 Aug 2008","2008","","","784","788","This paper focuses on the application of reinforcement learning to obstacle avoidance in dynamic environments. Behavior-based control architecture is more robust and better in real-time performance than conventional model based architecture in the control of mobile robot. An intelligent controller is proposed by integrating reinforcement learning with the behavior-based control architecture and applied to the obstacle avoidance. Neural network is used to approximate the Q-function to store the Q-value. By using the reinforcement learning, the mobile robot can learn to select proper behavior online without knowing the exact model of the system. In experiments, dynamic and static obstacles are placed in the environments separately. Experiment results show that the mobile robot can get to the target point without colliding with any obstacle after a period of learning.","","978-1-4244-2183-1","10.1109/ICINFA.2008.4608104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4608104","","Mobile robots;Robots;Learning;Artificial neural networks;Computer architecture;Distance measurement;Neurons","collision avoidance;intelligent robots;learning (artificial intelligence);mobile robots;neural nets;neurocontrollers","reinforcement learning;neural network;dynamic obstacle avoidance;behavior-based control architecture;mobile robot;intelligent controller;Q-function;Q-value;static obstacles;dynamic obstacles","","4","","7","IEEE","26 Aug 2008","","","IEEE","IEEE Conferences"
"The Strategy Entropy of Reinforcement Learning for Mobile Robot Navigation in Complex Environments","X. Zhuang","Department of Electrical Engineering and Computer Science, HIEST University, Athens, Greece","Proceedings of the 2005 IEEE International Conference on Robotics and Automation","10 Jan 2006","2005","","","1742","1747","In this paper, the concept of entropy is introduced into reinforcement learning for mobile robot control. The definitions of the local and global strategy entropy are proposed respectively. The global strategy entropy is proved to be a quantitative problem-independent measurement for the learning progress, i.e. the convergence degree of the strategy. To improve the learning performance, a new learning algorithm with self-adaptive learning rate is proposed based on the local strategy entropy. Robot navigation in multi-obstacle environments is achieved with the proposed learning algorithm. The experimental results show that learning based on the local strategy entropy has better learning performance than learning with fixed learning rates.","1050-4729","0-7803-8914-X","10.1109/ROBOT.2005.1570365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1570365","Reinforcement learning;robot navigation;strategy entropy;self-adaptive learning rate","Entropy;Mobile robots;Navigation;Intelligent robots;Extraterrestrial measurements;Learning systems;Robot control;Convergence;Machine learning algorithms;Stochastic processes","","Reinforcement learning;robot navigation;strategy entropy;self-adaptive learning rate","","3","","10","IEEE","10 Jan 2006","","","IEEE","IEEE Conferences"
"Adaptive optimal control via reinforcement learning for omni-directional wheeled robots","A. Sheikhlar; A. Fakharian","Department of Electrical, Islamic Azad University, Qazvin, Iran; Department of Electrical, Islamic Azad University, Qazvin, Iran","2016 4th International Conference on Control, Instrumentation, and Automation (ICCIA)","2 Jun 2016","2016","","","208","213","The main problem of wheeled soccer robots is the low level controller gains regulation particularly in competition. The low level control task is tracking the desired angular velocities of the robot wheels which are generated by the high level controller. Since the robot's model and environment have many uncertainties, traditional controller gains must be adjusted before every match along the competition. In this paper, a linear quadratic tracking (LQT) scheme is designed to solve this problem. The controller can regulate the parameters on-line by policy iteration reinforcement learning algorithm. The output paths of the four-wheeled soccer robot with the adaptive LQT are compared with traditional LQT and the results show that the proposed method can provide superior performance in presence of uncertainties and nonlinearities.","","978-1-4673-8704-0","10.1109/ICCIAutom.2016.7483162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7483162","reinforcement learning;omni-directional robot;linear quadratic tracking control;optimal control;adaptive control","Mobile robots;Mathematical model;Robot kinematics;Wheels;Learning (artificial intelligence);Angular velocity","adaptive control;control system synthesis;iterative learning control;learning (artificial intelligence);linear quadratic control;mobile robots;wheels","adaptive optimal control;omnidirectional wheeled robots;wheeled soccer robots;low level controller;angular velocity;high level controller;linear quadratic tracking scheme;adaptive LQT scheme;policy iteration reinforcement learning algorithm","","3","","12","IEEE","2 Jun 2016","","","IEEE","IEEE Conferences"
"Vision Based Autonomous Navigation of Quadcopter using Reinforcement Learning","A. Walvekar; Y. Goel; A. Jain; S. Chakrabarty; A. Kumar","Indian Institute of Technology, Roorkee, India; Indian Institute of Technology, Roorkee, India; Indian Institute of Technology, Roorkee, India; Indian Institute of Technology, Roorkee, India; Indian Institute of Technology, Roorkee, India","2019 IEEE 2nd International Conference on Automation, Electronics and Electrical Engineering (AUTEEE)","12 Mar 2020","2019","","","160","165","This paper presents a vision based approach for autonomous navigation of quadcopter using deep reinforcement learning methods to learn control policies. This approach is based on model free methods and hence eliminates the need to explicitly model the agent and environment dynamics which can be a hard task due to hidden dynamics and complex structure. It has also been shown that the quadcopter learns to avoid obstacles while navigating through the environment to reach the destination point. A convolutional neural network which represents our learning policy has been trained on a variant of reinforcement learning method called deep Q-learning. The input to the network is the depth image of the front view of the quadcopter and the output control actions are commands to quadcopter on how to steer through the environment. Finally, this approach has been successfully tested in a virtual outdoor environment which shows the effectiveness of the proposed framework.","","978-1-7281-5030-7","10.1109/AUTEEE48671.2019.9033244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9033244","","Convolutional neural networks;Helicopters;Mobile robots;Collision avoidance;Aircraft navigation","aircraft navigation;collision avoidance;convolutional neural nets;helicopters;learning (artificial intelligence);mobile robots;robot vision","autonomous navigation;quadcopter;vision based approach;deep reinforcement;control policies;model free methods;hidden dynamics;complex structure;convolutional neural network;learning policy;reinforcement learning method;deep Q-learning;output control actions;virtual outdoor environment","","3","","25","IEEE","12 Mar 2020","","","IEEE","IEEE Conferences"
"An experimental approach to robotic grasping using reinforcement learning and generic grasping functions","M. A. Moussa; M. S. Kamel","Department of Systems Design Engineering, University of Waterloo, Waterloo, ONT, Canada; Department of Systems Design Engineering, University of Waterloo, Waterloo, ONT, Canada","Proceedings of IEEE International Conference on Robotics and Automation","6 Aug 2002","1996","3","","2767","2773 vol.3","In this paper we present an experimental approach to robotic grasping that is based on mapping grasping rules to a generic representation that can then be learned by experiments. Furthermore, grasping rules acquired in this format can then be used on different objects using different grippers. During experimentation, reinforcement learning is used to minimize the number of failed experiments. Results show that the system is able to learn how to grasp various objects while maintaining a small number of experiments.","1050-4729","0-7803-2988-0","10.1109/ROBOT.1996.506581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506581","","Learning;Grippers;Grasping;Humans;Robot sensing systems;Shape;Pattern analysis;Machine intelligence;System analysis and design;Design engineering","manipulators;learning (artificial intelligence)","robotic grasping;reinforcement learning;generic grasping functions;minimization","","3","","12","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Algorithm of task offloading and resource allocation based on reinforcement learning in edge computing","J. Zhao; X. Hu; X. Du","School of Electronic Information and Engineering, Lanzhou Jiaotong University, Lanzhou, China; School of Electronic Information and Engineering, Lanzhou Jiaotong University, Lanzhou, China; School of Electronic Information and Engineering, Lanzhou Jiaotong University, Lanzhou, China","2021 IEEE 5th Information Technology,Networking,Electronic and Automation Control Conference (ITNEC)","4 Nov 2021","2021","5","","1266","1269","With the development of new technologies, resource-poor mobile devices cannot withstand low-latency, high-computing applications. In order to reduce the burden of such applications on the devices, mobile edge computing is a new type of technology that will store and Computing resources are moved closer to users to improve response time and relieve backhaul pressure, so that users with resource-limited devices can offload more complex tasks to the edge server (MEC), and let the MEC handle these tasks. Consider the problem of resource optimization, this paper proposes a resource allocation algorithm based on Deep Deterministic Policy Gradient (DDPG). The algorithm has the ability to quickly find the optimal decision while maximizing long-term benefits. The simulation results show that the algorithm has high performance in finding the optimal resource allocation decision.","2693-3128","978-1-6654-1599-6","10.1109/ITNEC52019.2021.9587049","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9587049","Deep reinforcement learning;mobile edge computing;resource allocation;DDPG;Task offload","Technological innovation;Simulation;Reinforcement learning;Mobile handsets;Resource management;Time factors;Servers","learning (artificial intelligence);mobile computing;resource allocation","task offloading;reinforcement learning;resource-poor mobile devices;high-computing applications;mobile edge computing;response time;backhaul pressure;edge server;MEC;resource optimization;resource allocation algorithm;Deep Deterministic Policy Gradient;optimal resource allocation decision","","3","","8","IEEE","4 Nov 2021","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Algorithm Based Neural Network Used for Course Angle Control of Remotely Operated Vehicle","Y. -z. Gao; J. -w. Ye; P. -a. Shi; X. Song","School of Civil Engineering and Transportation, South China University of Technology, Guangzhou, China; School of Civil Engineering and Transportation, South China University of Technology, Guangzhou, China; School of Civil Engineering and Transportation, South China University of Technology, Guangzhou, China; No. 1 Division, Navy Arms Command Academy, Guangzhou, China","2009 Second International Conference on Intelligent Computation Technology and Automation","16 Oct 2009","2009","1","","31","34","The principal contribution of this paper is designed a general framework for an intelligent control system used in course angle control of remotely operated vehicle (ROV). A control scheme based on reinforcement learning (RL) agent combined with radial basis function (RBF) neural network control algorithm is applied. The effectiveness of the controller is demonstrated through simulations, and implementation issues are discussed. The control law is conceptually simple and computationally easy to implement.","","978-0-7695-3804-4","10.1109/ICICTA.2009.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5287717","reinforcement learning;radial basis function;remotely operated vehicle;course angle control","Learning;Neural networks;Remotely operated vehicles;Sliding mode control;Control systems;Mathematical model;Computer networks;Computational modeling;Military computing;Application software","learning (artificial intelligence);neurocontrollers;position control;radial basis function networks;remotely operated vehicles","reinforcement learning algorithm;neural network;course angle control;remotely operated vehicle;radial basis function","","3","","10","IEEE","16 Oct 2009","","","IEEE","IEEE Conferences"
"Distributed Reinforcement Learning of a Six-Legged Robot to Walk","Y. Zennir; P. Couturier; M. B. Temps","CLGI2P-EERIE, Parc scientifique G. Besse, Fr.; CLGI2P-EERIE, Parc scientifique G. Besse, Fr.; NA","2003 4th International Conference on Control and Automation Proceedings","21 Feb 2006","2003","","","896","900","We present the principles of the reinforcement arning we use for the training of the walk of a hexapod robot. The originality of our approach lies in the fact that the training is distributed, each leg has to achieve its own goal. A gait appears as a emerging behavior and results from the 'self-coordination' of the movements of the legs. We give the results of the simulation we have carried out and open prospects for our future work.","","0-7803-7777-X","10.1109/ICCA.2003.1595152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1595152","","Learning;Legged locomotion;Leg","","","","3","","10","IEEE","21 Feb 2006","","","IEEE","IEEE Conferences"
"Research on Driving Decision of Smart Vehicles Based on Reinforcement Learning","S. Xiao; J. Huang; L. Xiao; Y. Jiao; Z. Wang; X. Wang","Beijing Information Technology College, Beijing, China; Beijing Information Technology College, Beijing, China; School of Mechanical electronic and Automobile Engineering of Beijing, University of Civil Engineering and Architecture, Beijing, P.R. China; Beijing Information Technology College, Beijing, China; Beijing Information Technology College, Beijing, China; BEIJING POLYTECHNIC, China","2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)","19 Jul 2021","2021","4","","1466","1469","In order to ensure the smooth, reliable and safe driving process of intelligent driving cars, this paper analyzes the environmental information from the decision-making perception module of automatic driving and controls the car's own behavior to achieve the driving goal. The driving decision algorithm of intelligent vehicle based on reinforcement learning is given to ensure the safe driving of intelligent vehicle under complex constraints such as high speed, slip and roll. Through reinforcement learning, an iterative process of constantly interacting with the environment, getting rewards, updating strategies and then continuing to interact with the environment is given, and exploratory work is carried out by using reinforcement learning in TORCS simulator. Finally, the automatic driving system is built in simulink, and PreScan is used as the simulation environment for training and verification, which verifies the intelligent decision-making and control method of vehicles using reinforcement learning in the intelligent networked environment, and realizes the smooth, reliable and safe driving of intelligent vehicles.","2693-2776","978-1-7281-8535-4","10.1109/IMCEC51613.2021.9482162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9482162","Intensive learning;Smart cars;Decision algorithm;robot guiding system","Training;Intelligent vehicles;Heuristic algorithms;Roads;Decision making;Process control;Reinforcement learning","automobiles;computer simulation;decision making;intelligent transportation systems;learning (artificial intelligence);road safety;traffic engineering computing","reinforcement learning;automatic driving system;intelligent decision making;intelligent networked environment;intelligent vehicle;smart vehicles;intelligent driving cars;driving decision algorithm;environmental information;TORCS simulator;Simulink;PreScan simulation environment","","3","","10","IEEE","19 Jul 2021","","","IEEE","IEEE Conferences"
"Mapless Navigation for Autonomous Robots: A Deep Reinforcement Learning Approach","P. Zhang; C. Wei; B. Cai; Y. Ouyang","College of Mechanical and Electrical Engineering, Hohai University NO.200 Jinling Bei Road, Changzhou, China; College of Mechanical and Electrical Engineering, Hohai University NO.200 Jinling Bei Road, Changzhou, China; College of Mechanical and Electrical Engineering, Hohai University NO.200 Jinling Bei Road, Changzhou, China; College of Mechanical and Electrical Engineering, Hohai University NO.200 Jinling Bei Road, Changzhou, China","2019 Chinese Automation Congress (CAC)","13 Feb 2020","2019","","","3141","3146","Finding a collision-free path for mobile robots is a challenging task, especially in sceneries where obstacle information is partly observed. Our work presents a decentralized collision avoidance approach based on an innovative application of deep reinforcement learning. The approach takes the spare 10-dimensional range findings and the target position in mobile robot coordinate frame as input and the continuous action commands as output. Traditional method for finding collision-free paths deeply depends on extremely precise laser sensor and the map making work of the roadblocks is inevitable. Our work shows that, using an asynchronous deep reinforcement learning method, a mapless path planer can be trained from start to finish without any manual operations. The trainer is available in other virtual environment directly. We compare a traditional method with the asynchronous method and find that our asynchronous method can decrease training time at beginning.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8997292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8997292","Reinforcement Learning;Path Planning;Deep deterministic policy gradient methods;Mapless Navigation;Asynchronous Method","Training;Learning (artificial intelligence);Machine learning;Robots;Navigation;Path planning;Acceleration","collision avoidance;control engineering computing;learning (artificial intelligence);mobile robots;navigation;neural nets","mobile robot;collision-free path;laser sensor;asynchronous deep reinforcement learning method;asynchronous method;mapless navigation;autonomous robots;obstacle information;decentralized collision avoidance approach;mapless path planner","","2","","8","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Attitude Controller Design based on Deep Reinforcement Learning for Low-cost Aircraft","Z. Wang; W. Luo; Q. Gong; Y. Cui; R. Tao; Q. Wang; Q. Liang; S. Wang","Beijing Aerospace Automatic Control Institute, Beijing, China; Beijing Aerospace Automatic Control Institute, Beijing, China; Beijing Aerospace Automatic Control Institute, Beijing, China; Beijing Aerospace Automatic Control Institute, Beijing, China; Beijing Aerospace Automatic Control Institute, Beijing, China; Beijing Aerospace Automatic Control Institute, Beijing, China; Beijing Aerospace Automatic Control Institute, Beijing, China; Beijing Aerospace Automatic Control Institute, Beijing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","463","467","In this paper, for a class of low-cost aircraft whose accurate mathematical models can't be obtained, a kind of attitude controller design method based on deep reinforcement learning is proposed. Considering the state and control signals in attitude control are continuous variable, an anthropomorphic attitude controller design method based on the deep deterministic policy gradient (DDPG) theory is researched. Based on the actor-critic structure, the controller can be learned through continuous interaction and trial between the controller and the system environment, which reduces the dependence on the accurate aircraft model. The simulation results show that the DDPG based attitude controller can achieve a good control accuracy, and its end-to-end design process can effectively reduce the design complexity.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9326889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326889","Deep reinforcement learning;Intelligent control;Attitude control;DDPG","Aerodynamics;Aircraft;Reinforcement learning;Attitude control;Atmospheric modeling;Mathematical model;Force","aerospace computing;aircraft control;attitude control;control engineering computing;control system synthesis;deep learning (artificial intelligence);learning (artificial intelligence)","deep reinforcement learning;low-cost aircraft;mathematical models;control signals;attitude control;continuous variable;deep deterministic policy gradient theory;actor-critic structure;continuous interaction;aircraft model;DDPG based attitude controller;end-to-end design process;design complexity;anthropomorphic attitude controller design","","2","","10","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Robust reinforcement learning-based tracking control for wheeled mobile robot","Nguyen Tan Luy; Nguyen Duc Thanh; Nguyen Thien Thanh; Nguyen Thi Phuong Ha","National key Laboratory of Digital control and System engineering, University of Technology Ho Chi Minh, Ho Chi Minh, Vietnam; National key Laboratory of Digital control and System engineering, University of Technology Ho Chi Minh, Ho Chi Minh, Vietnam; Department of Automatic Control, University of Technology Ho Chi Minh, Ho Chi Minh, Vietnam; Department of Automatic Control, University of Technology Ho Chi Minh, Ho Chi Minh, Vietnam","2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE)","19 Apr 2010","2010","1","","171","176","This paper proposes a method to design a robust reinforcement learning-based tracking control scheme for the wheeled mobile robot. A policy iteration algorithm and a neural network are used to design an adaptive critic robust controller. A H¿ - tracking performance index optimal function is evaluated by this con troller. The stability of the closed-loop system while learning is proven by Lyapunov theory. The simulation results for wheeled mobile robot verify the effects of the proposed controller.","","978-1-4244-5586-7","10.1109/ICCAE.2010.5451973","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5451973","Adaptive critic;policy iteration;wheeled mobile robot;robust reinforcement learning","Robust control;Mobile robots;Adaptive control;Programmable control;Neural networks;Wheels;Learning;Automatic control;Control systems;Performance analysis","adaptive control;closed loop systems;control engineering computing;control system synthesis;H¿ control;iterative methods;learning (artificial intelligence);Lyapunov methods;mobile robots;neural nets;tracking","robust reinforcement learning;tracking control;wheeled mobile robot;neural network;closed-loop system;Lyapunov theory;H¿-tracking performance index","","2","","9","IEEE","19 Apr 2010","","","IEEE","IEEE Conferences"
"Exploring the use of Deep Reinforcement Learning to allocate tasks in Critical Adaptive Distributed Embedded Systems","R. Rotaeche; A. Ballesteros; J. Proenza","DMI - Universitat Illes Balears, Palma, Spain; DMI - Universitat Illes Balears, Palma, Spain; DMI - Universitat Illes Balears, Palma, Spain","2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA )","30 Nov 2021","2021","","","01","04","Critical Adaptive Distributed Embedded Systems (CADES) must carry out a set of funcionalities while fulfilling their associated real-time and dependability requirements. Moreover, they must be able to reconfigure themselves in a bounded time as the operational context changes. Finding a proper configuration can be non-trivial and time-consuming. Several studies have proposed Deep Reinforcement Learning (DRL) approaches to solve combinatorial optimization problems. In this paper, we explore the application of such approaches to CADES by solving a simple tasks allocation problem using DRL and comparing the results with three popular heuristics. The results show that DRL beats two of them and gets very close to the third, while requiring significantly less time to generate a solution.","","978-1-7281-2989-1","10.1109/ETFA45728.2021.9613409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9613409","Adaptive Distributed Embedded Systems;Deep Reinforcement Learning;Neural Combinatorial Optimization","Adaptive systems;Embedded systems;Conferences;Reinforcement learning;Real-time systems;Resource management;Task analysis","combinatorial mathematics;deep learning (artificial intelligence);distributed processing;embedded systems;optimisation;reinforcement learning","CADES;task allocation problem;DRL;critical adaptive distributed embedded systems;bounded time;operational context changes;deep reinforcement learning;combinatorial optimization problem;heuristics","","2","","12","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Analog Circuit Structure Synthesis","Z. Zhao; L. Zhang","Department of Electrical and Computer Engineering, Memorial University of Newfoundland, St. John's, Canada; Department of Electrical and Computer Engineering, Memorial University of Newfoundland, St. John's, Canada","2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)","19 May 2022","2022","","","1157","1160","This paper presents a novel deep-reinforcement-learning-based method for analog circuit structure synthesis. It behaves like a designer, who learns from trials, derives design knowledge and experience, and evolves gradually to eventually figure out a way to construct circuit structures that can meet the given design specifications. Necessary design rules are defined and applied to set up the specialized environment of reinforcement learning in order to reasonably construct circuit structures. The produced circuit structures are then verified by the simulation-in-loop sizing. In addition, hash table and symbolic analysis techniques are employed to significantly promote the evaluation efficiency. Our experimental results demonstrate the sound efficiency, strong reliability, and wide applicability of the proposed method.","1558-1101","978-3-9819263-6-1","10.23919/DATE54114.2022.9774699","Natural Sciences and Engineering Research Council of Canada; Canada Foundation for Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774699","deep reinforcement learning;hash table;analog circuit synthesis","Integrated circuits;Neural networks;MIMICs;Reinforcement learning;Analog circuits;Circuit synthesis;Reliability","analogue circuits;circuit simulation;deep learning (artificial intelligence);network synthesis","reliability;symbolic analysis techniques;evaluation efficiency;simulation-in-loop sizing;deep reinforcement learning;design rules;design specifications;design knowledge;deep-reinforcement-learning-based method;analog circuit structure synthesis","","2","","10","","19 May 2022","","","IEEE","IEEE Conferences"
"Reinforcement learning control for a robotic manipulator with unknown deadzone","Y. Li; S. Xiao; S. S. Ge","Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","Proceeding of the 11th World Congress on Intelligent Control and Automation","5 Mar 2015","2014","","","593","598","In this paper, an actor critic neural network control is developed for a robotic manipulator. Both system uncertainties and unknown deadzone are considered in the tracking control design. Stability of the closed-loop system is analyzed via the Lyapunov's direct method. The critic neural network is used to estimate the cost-to-go and the actor neural network is used to make the cost-to-go converge. Simulation studies are conducted to examine the effectiveness of the proposed actor critic neural network control.","","978-1-4799-5825-2","10.1109/WCICA.2014.7052780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052780","Reinforcement learning;robot control;neural networks;deadzone","Artificial neural networks;Manipulator dynamics;Learning (artificial intelligence);Vectors","closed loop systems;control system synthesis;learning systems;Lyapunov methods;manipulators;neurocontrollers;stability;uncertain systems","reinforcement learning control;robotic manipulator;unknown deadzone;actor critic neural network control;system uncertainties;tracking control design;stability;closed-loop system;Lyapunov direct method;cost-to-go estimation","","2","","17","IEEE","5 Mar 2015","","","IEEE","IEEE Conferences"
"A study on hierarchical modular reinforcement learning for multi-agent pursuit problem based on relative coordinate states","T. Wada; T. Okawa; T. Watanabe","Graduate School of Engineering, Osaka Electro Communication University, Neyagawa, Osaka, Japan; Graduate School of Engineering, Osaka Electro Communication University, Neyagawa, Osaka, Japan; Faculty of Engineering, Osaka Electro Communication University, Neyagawa, Osaka, Japan","2009 IEEE International Symposium on Computational Intelligence in Robotics and Automation - (CIRA)","1 Mar 2010","2009","","","302","308","In order to realize intelligent agent such as autonomous mobile robots, reinforcement learning is one of the necessary techniques in behavior control system. However, applying the reinforcement learning to actual sized problem, the ¿curse of dimensionality¿ problem in partition of sensory states should be avoided maintaining computational efficiency. In multi-agent reinforcement learning, the problem is emerged owing to the high dimensionality of each agent states. We apply the hierarchical modular reinforcement learning in order to deal with the dimensional problem and task decomposition. In this study, we focus on investigation of the learning performance of agent that represents the input states in relative coordinate system. We show effectiveness of proposed learning algorithm based on relative expressions with limited view through numerical experiments of the pursuit problem.","","978-1-4244-4808-1","10.1109/CIRA.2009.5423188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5423188","","Learning;Mobile robots;Pursuit algorithms;Intelligent agent;Sensor systems;Robot sensing systems;Robot kinematics;Intelligent robots;Intelligent sensors;Computational efficiency","learning (artificial intelligence);mobile robots;multi-agent systems","hierarchical modular reinforcement learning;multi-agent pursuit problem;relative coordinate states;intelligent agent;autonomous mobile robots;curse of dimensionality problem;multi-agent reinforcement learning","","2","","17","IEEE","1 Mar 2010","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning based Autonomous Air-to-Air Combat using Target Trajectory Prediction","J. Yoo; D. Kim; D. H. Shim","Division of Future Vehicle, Korea Advanced Institute of Science and Technology, Daejeon, Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","2172","2176","This study designed an intelligent control system for autonomous air-to-air combat and verified it in a realtime flight simulation. Previous studies of aerial combat have required significant effort to design agile control actions for different engagement conditions. In this work, optimal flight control under random engagement conditions was performed by using reinforcement learning and recurrent neural networks. A target trajectory was predicted using Sequence-to-Sequence model with LSTM, for occupying an advantageous location from an enemy aircraft in a close engagement. In addition, this study proposed an algorithm with improved performance compared to the existing algorithm. The result of the study confirmed that the maneuvers of trained agent were similar to the performance of human pilots and the future position of the enemy was tracked by own ship aircraft.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649876","Deep Reinforcement Learning;Autonomous Flight;Air-to-Air Combat;Trajectory Prediction;Flight Simulation","Target tracking;Recurrent neural networks;Atmospheric modeling;Reinforcement learning;Predictive models;Prediction algorithms;Trajectory","aerospace computing;aerospace simulation;aircraft;aircraft control;autonomous aerial vehicles;control engineering computing;deep learning (artificial intelligence);intelligent control;military aircraft;mobile robots;multi-agent systems;optimal control;recurrent neural nets;reinforcement learning;trajectory control","intelligent control system;autonomous air-to-air combat;real-time flight simulation;aerial combat;agile control actions;optimal flight control;random engagement conditions;target trajectory prediction;deep reinforcement learning;recurrent neural networks;sequence-to-sequence model;LSTM;enemy aircraft;agent maneuvers;ship aircraft","","2","","7","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Fuzzy Neural Control of Satellite Attitude by TD Based Reinforcement Learning","Xiao-ting Cui; Xiang-dong Liu","Department of Automatic Control, Beijing Institute of Technology, Beijing, China; Department of Automatic Control, Beijing Institute of Technology, Beijing, China","2006 6th World Congress on Intelligent Control and Automation","23 Oct 2006","2006","1","","3983","3986","With recent development of the space science and technology, higher requirements such as accuracy, robustness and disturbance rejection ability in satellite attitude control system have leaded to the more promising intelligent control methods. In this paper, a fuzzy neural control approach applied to the three-axis stabilized satellite is presented. In order to solve the problems of online learning and tuning of the fuzzy neural network parameters, the reinforcement learning based on temporal difference (TD) is also proposed and studied so that the training samples for the self-learning controller are not needed. Since the vibration of the solar swing cannot be ignored, a flexible mathematic model of the satellite is studied, employing Quaternion and Euler-Angles representations. The simulation results showed that the proposed control method with reinforcement learning architecture could not only improve the accuracy and robustness of the system, but also could deal with the uncertainties and external disturbance efficiently.","","1-4244-0332-4","10.1109/WCICA.2006.1713120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1713120","satellite attitude control;fuzzy neural network;reinforcement learning;temporal difference learning","Fuzzy control;Satellites;Attitude control;Learning;Space technology;Robust control;Intelligent control;Fuzzy neural networks;Mathematics;Mathematical model","artificial satellites;attitude control;fuzzy control;fuzzy neural nets;intelligent control;learning (artificial intelligence);neurocontrollers","satellite attitude control;fuzzy neural network;reinforcement learning;temporal difference learning","","2","","6","IEEE","23 Oct 2006","","","IEEE","IEEE Conferences"
"Reinforcement learning method-based stable gait synthesis for biped robot","Hu Lingyun; Sun Zengqi","State Key Laboratory of Intelligent Technology and Systems, Computer Science and Technology Department, Tsinghua University, Beijing, Beijing, China; State Key Laboratory of Intelligent Technology and Systems, Computer Science and Technology Department, Tsinghua University, Beijing, Beijing, China","ICARCV 2004 8th Control, Automation, Robotics and Vision Conference, 2004.","25 Jul 2005","2004","2","","1017","1022 Vol. 2","A stable gait generation algorithm based on T-S type fuzzy learning net is proposed in this paper. Gait generation is divided into model construction and error learning. Reference gait model and dynamic model are firstly constructed with basic gait geometric knowledge. Then reinforcement learning method is introduced into T-S type fuzzy network to learn the gain parameters for hip trajectory adjustment. Few fuzzy rules with ZMP stable knowledge are needed to formulate the nonlinear relation between the ZMP curve and hip trajectory. The problem of finding multi-variables in continuous space is also simplified to searching independent action gains simultaneously. Results of simulation on a biped robot proved the feasibility.","","0-7803-8653-1","10.1109/ICARCV.2004.1468983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1468983","","Hip;Legged locomotion;Intelligent robots;Solid modeling;Orbital robotics;Learning systems;Stability;Trajectory;Equations;Sun","learning (artificial intelligence);gait analysis;legged locomotion;fuzzy neural nets;robot dynamics","reinforcement learning method;stable gait synthesis;biped robot;stable gait generation algorithm;T-S type fuzzy learning network;model construction;error learning;dynamic model;gait geometric knowledge;ZMP stable knowledge","","2","","8","IEEE","25 Jul 2005","","","IEEE","IEEE Conferences"
"Gaussian RAM: Lightweight Image Classification via Stochastic Retina-Inspired Glimpse and Reinforcement Learning","D. Shim; H. J. Kim","Department Mechanical & Aerospace Engineering, Seoul National University, Seoul, Korea; Department Mechanical & Aerospace Engineering, Seoul National University, Seoul, Korea","2020 20th International Conference on Control, Automation and Systems (ICCAS)","1 Dec 2020","2020","","","155","160","Previous studies on image classification have mainly focused on the performance of the networks, not on real-time operation or model compression. We propose a Gaussian Deep Recurrent visual Attention Model (GDRAM) - a reinforcement learning based lightweight deep neural network for large scale image classification that outperforms the conventional CNN (Convolutional Neural Network) which uses the entire image as input. Highly inspired by the biological visual recognition process, our model mimics the stochastic location of the retina with Gaussian distribution. We evaluate the model on Large cluttered MNIST, Large CIFAR-10 and Large CIFAR-100 datasets which are resized to 128 in both width and height. The implementation of Gaussian RAM in PyTorch and its pretrained model are available at : https://github.com/dsshim0125/gaussian-ram","2642-3901","978-89-93215-20-5","10.23919/ICCAS50221.2020.9268201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268201","Deep Learning;Computer Vision;Image Classification;Reinforcement Learning","Random access memory;Uncertainty;Gaussian distribution;Biological system modeling;Visualization;Semantics;Reinforcement learning","convolutional neural nets;Gaussian distribution;image classification;image recognition;learning (artificial intelligence);object recognition","stochastic location;Gaussian distribution;CIFAR-100 datasets;Gaussian RAM;lightweight image classification;stochastic retina-inspired glimpse;reinforcement learning;real-time operation;Gaussian Deep Recurrent visual Attention Model;lightweight deep neural network;scale image classification;CNN;Convolutional Neural Network;biological visual recognition process;PyTorch;large CIFAR-100 dataset;large CIFAR-10 dataset;large cluttered MNIST dataset","","2","","18","","1 Dec 2020","","","IEEE","IEEE Conferences"
"A fuzzy reinforcement learning algorithm with a prediction mechanism","M. D. Awheda; H. M. Schwartz","Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada; Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada","22nd Mediterranean Conference on Control and Automation","20 Nov 2014","2014","","","593","598","This paper applies fuzzy reinforcement learning along with state estimation to the differential pursuit-evasion game. The proposed algorithm is a modified version of the Q(λ) Learning Fuzzy Inference System (QLFIS) algorithm proposed in [10]. The proposed algorithm combines the QLFIS algorithm with a Kalman filter estimation approach. The proposed algorithm is called the modified Q(λ)-learning fuzzy inference system (MQLFIS) algorithm. The Kalman filter is used by the pursuer to estimate the expected future position of the evader. The proposed algorithm tunes the input and the output parameters of the fuzzy logic controller (FLC) of the pursuer based on the expected future position of the evader instead of the real position of the evader. The proposed algorithm also uses the expected future position of the evader to generate the output of the FLC so that the pursuer captures the evader at the expected future position. The proposed algorithm is used to learn two different single pursuit-evasion games. Simulation results show that the performance of the proposed MQLFIS algorithm outperforms the performance of the QLFIS algorithm proposed in [10].","","978-1-4799-5901-3","10.1109/MED.2014.6961437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6961437","","Games;Inference algorithms;Prediction algorithms;Kalman filters;Fuzzy logic;Robots;Learning (artificial intelligence)","fuzzy control;fuzzy reasoning;game theory;Kalman filters;learning (artificial intelligence);state estimation","fuzzy reinforcement learning algorithm;prediction mechanism;state estimation;differential pursuit-evasion game;modified Q learning fuzzy inference system;Kalman filter estimation;MQLFIS algorithm;fuzzy logic controller;FLC","","2","","18","IEEE","20 Nov 2014","","","IEEE","IEEE Conferences"
"A Lunar Robot Obstacle Avoidance Planning Method Using Deep Reinforcement Learning for Data Fusion","R. Hu; Z. Wang","College of Aerospace Science and Engineering, National University of Defense Technology, Changsha, China; School of Aerospace Engineering, Tsinghua University, Beijing, China","2019 Chinese Automation Congress (CAC)","13 Feb 2020","2019","","","5365","5370","In future exploration and base construction on the moon, obstacle avoidance planning of lunar robots in an uncertain environment is critical for their autonomous movements and operations, with no precise location information of obstacles. In the present work, an obstacle avoidance planning method using deep reinforcement learning with a double-channel Q network is proposed, by which local surveillance video images and navigating data are merged for action value estimation. Through simulation, our method is turned out to achieve motion planning effectively from raw sensing data, and learn faster than the methods using single type of data.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8997266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8997266","lunar robot;deep reinforcement learning;obstacle avoidance planning;data fusion","Planning;Moon;Robot sensing systems;Collision avoidance;Machine learning;Navigation","collision avoidance;image fusion;learning (artificial intelligence);mobile robots;motion control;video surveillance","lunar robot obstacle avoidance planning method;deep reinforcement learning;data fusion;future exploration;base construction;uncertain environment;autonomous movements;local surveillance video images;navigating data;motion planning;raw sensing data;double-channel Q network;action value estimation","","2","","23","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Robot docking by reinforcement learning in a visual servoing framework","T. Martinez-Marin; T. Duckett","Dept. of Physics, University of Alicante, Alicante, Spain; AASS, Dept. of Technology Orebro University, Orebro, Sweden, Sweden","IEEE Conference on Robotics, Automation and Mechatronics, 2004.","13 Jun 2005","2004","1","","159","164 vol.1","This paper presents an image-based visual servoing approach for a mobile manipulation task, in which a mobile robot has to move towards an object located on a table (docking) and then picks up that object with its gripper. The robot's vision system consists of a pan-tilt camera that is used to keep track of the object and the edge of the table. A minimal number of state variables are extracted from the vision system, and a reactive controller is used to implement the docking behaviour, without requiring any geometric model of the scene. The main aim of the work was to develop a practical reinforcement learning scheme to automatically acquire a high-performance controller in a short training time (less than 1 hour) on the real robot. We compare a number of control algorithms, including a hand-designed linear controller, a novel reinforcement learning algorithm for mobile robots, and a scheme using the linear controller as a bias to accelerate reinforcement learning. By experimental analysis of the controllability and docking time, we found that the biased learning system could improve on the performance of the linear controller, while requiring substantially lower training time than unbiased learning.","","0-7803-8645-0","10.1109/RAMECH.2004.1438909","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1438909","","Learning;Visual servoing;Automatic control;Mobile robots;Robot vision systems;Grippers;Cameras;Machine vision;Solid modeling;Layout","learning (artificial intelligence);robot vision;mobile robots;grippers;linear systems;controllability;image motion analysis","reinforcement learning;image-based visual servoing;mobile robot;gripper;pan-tilt camera;hand-designed linear control;controllability;docking time","","2","","11","IEEE","13 Jun 2005","","","IEEE","IEEE Conferences"
"Multi-agent Deep Reinforcement Learning based on Maximum Entropy","Z. Wang; Y. Zhang; C. Yin; Z. Huang",Beijing Jiaotong University; Beijing Jiaotong University; Beijing Jiaotong University; Beijing University of Technology,"2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)","19 Jul 2021","2021","4","","1402","1406","Deep reinforcement learning at the same time combines the perception of deep learning and the decision-making of reinforcement learning, is currently a hot research topic in the field of artificial intelligence. Multi-agent deep reinforcement learning applies the idea and algorithm of deep reinforcement learning to the learning and control of multi-agent system, which is an important method to develop multi-agent system with swarm agent. Multi-agent deep deterministic policy gradient(MADDPG) is the most popular model-free multi-agent reinforcement learning algorithm. To solve the problem of low learning and training efficiency and slow convergence speed of MADDPG due to the deterministic single action output of policy network, this paper combines the maximum reinforcement learning soft actor -critic algorithm to make each agent’s policy network output action with a random strategy and propose a multi-agent deep reinforcement learning algorithm MASAC based on maximum entropy. The experimental results show that the training speed of MASAC is better than that of MADDPG. At the same time, the learning agent has good performance, stable performance and strong anti-interference ability.","2693-2776","978-1-7281-8535-4","10.1109/IMCEC51613.2021.9482235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9482235","deep reinforcement learning;multi-agent;deep deterministic policy gradient;soft actor-critic","Training;Deep learning;Conferences;Decision making;Reinforcement learning;Control systems;Entropy","decision making;deep learning (artificial intelligence);entropy;multi-agent systems;swarm intelligence","decision making;maximum entropy;MADDPG;artificial intelligence;model-free multiagent reinforcement learning;swarm agent;multiagent deep reinforcement learning","","2","","10","IEEE","19 Jul 2021","","","IEEE","IEEE Conferences"
"UAV/USV Cooperative Trajectory Optimization Based on Reinforcement Learning","P. Yao; Z. Gao","College of Engineering, Ocean University of China, Qingdao, China; College of Engineering, Ocean University of China, Qingdao, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","4711","4715","With the development of unmanned technology, unmanned aerial vehicle (UAV) and unmanned surface vehicle (USV) cooperative system is becoming more and more important in offshore operations. This paper consider the target search task on the sea, which requires USV to search target quickly, and UAV to track the USV in order to improve the observation accuracy and complete the task cooperatively. Based on target probability map model, a deep reinforcement learning algorithm based on proximal policy optimization (PPO) is adopted to optimize the search trajectory of USV. The PPO is also used to solve the UAV standoff tracking problem. The simulation results show that the deep reinforcement learning method can plan a reasonable search trajectory for USV, and UAV can achieve more accurate standoff tracking.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055417","Unmanned surface vehicle (USV);Unmanned aerial vehicle (UAV);Deeping reinforcement learning;Target search;Standoff tracking","Deep learning;Training;Target tracking;Simulation;Oceans;Reinforcement learning;Autonomous aerial vehicles","autonomous aerial vehicles;deep learning (artificial intelligence);mobile robots;multi-robot systems;optimisation;probability;reinforcement learning;remotely operated vehicles;unmanned surface vehicles","deep reinforcement learning algorithm;deep reinforcement learning method;proximal policy optimization;reasonable search trajectory;target probability map model;target search task;trajectory optimization;UAV standoff tracking problem;unmanned technology","","2","","13","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"A reinforcement learning system with chaotic neural networks-based adaptive hierarchical memory structure for autonomous robots","Masanao Obayashi; Kenichiro Narita; Takashi Kuremoto; Kunikazu Kobayashi","Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Japan; Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Japan; Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Japan; Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Japan","2008 International Conference on Control, Automation and Systems","2 Dec 2008","2008","","","69","74","Human learns incidents by own actions and reflects them on the subsequent action as own experiences. These experiences are memorized in his brain and recollected if necessary. This research incorporates such an intelligent information processing mechanism, and applies it to an autonomous agent that has three main functions: learning, memorization and associative recollection. In the proposed system, an actor-critic type reinforcement learning method is used for learning. Auto-associative chaotic neural network is also used like mutual associative memory system. Moreover, the memory part has an adaptive hierarchical layered structure of the memory module that consists of chaotic neural networks in consideration of the adjustment to non-MDP (Markov Decision Process) environment. Finally, the effectiveness of this proposed method is verified through the simulation applied to the maze-searching problem.","","978-89-950038-9-3","10.1109/ICCAS.2008.4694529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4694529","Reinforcement learning;chaotic neural network;hierarchical memory structure;autonomous robot","Learning;Chaos;Neural networks;Adaptive systems;Biological neural networks;Humans;Intelligent agent;Intelligent robots;Information processing;Autonomous agents","content-addressable storage;hierarchical systems;learning (artificial intelligence);mobile robots","adaptive hierarchical memory structure;autonomous robots;intelligent information processing mechanism;autonomous agent;actor-critic type reinforcement learning method;autoassociative chaotic neural network;mutual associative memory system;adaptive hierarchical layered structure;maze-searching problem","","2","","2","","2 Dec 2008","","","IEEE","IEEE Conferences"
"Traffic Signal Control with Deep Reinforcement Learning","T. Zhao; P. Wang; S. Li","Institute of Computer Science and Technology, Changchun University of Science and Technology, Changchun, China; Institute of Computer Science and Technology, Changchun University of Science and Technology, Changchun, China; Institute of Computer Science and Technology, Changchun University of Science and Technology, Changchun, China","2019 International Conference on Intelligent Computing, Automation and Systems (ICICAS)","2 Apr 2020","2019","","","763","767","To decrease the impact of Partially Observable MDP on deep reinforcement learning performance of intersection signal control, A deep reinforcement learning is proposed in this paper with utilizing the real-time GPS data as well as learning the control of the traffic lights in single intersection. We integrate deep reinforcement learning network (DRQN) with recurrent neural network (RNN) and apply deep network, experience pool and greedy strategy in deep reinforcement learning strategy. It solves the problem of overestimation of target Q value and insufficient long-term experience learning in the standard reinforcement learning of traffic signal control. The comparison of performance was made between the proposed method and standard Deep Q-Network (DQN) on the partial observation of traffic situations. The experimental results show that both DQN and DRQN methods can adjust their traffic signal timing control strategies according to the specific traffic conditions as well as calculating a lower average delay time of vehicle than that of fixed-time control. Besides, the simulation effect of DRQN learning method is better than that of DQN learning method in different probe vehicle proportion environment.","","978-1-7281-6106-8","10.1109/ICICAS48597.2019.00164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9051042","Deep Reinforcement Learning, Traffic signal control, Partially Observable MDP, Vehicle network","","control engineering computing;Global Positioning System;learning (artificial intelligence);Markov processes;recurrent neural nets;road traffic control;traffic engineering computing","traffic signal control;partially observable MDP;deep reinforcement learning performance;intersection signal control;traffic lights;deep reinforcement learning network;recurrent neural network;greedy strategy;deep reinforcement learning strategy;long-term experience learning;standard Deep Q-Network;traffic situations;fixed-time control;DRQN learning method;DQN learning method","","2","","10","IEEE","2 Apr 2020","","","IEEE","IEEE Conferences"
"Robust Reinforcement Learning Control System with H∞ tracking performance compensator","S. Uchiyama; M. Obayashi; T. Kuremoto; K. Kobayashi","Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Japan; Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Japan; Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Japan; Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Japan","2011 11th International Conference on Control, Automation and Systems","19 Dec 2011","2011","","","248","253","Robust control theory generally guarantees robustness and stability of the closed-loop system, however it requires mathematical model of the system to design the control system. Therefore, it can't often deal with nonlinear systems because of difficulty of modeling of the system. Other, reinforcement learning method can deal with the nonlinear system without mathematical model, however, it usually doesn't guarantee the stability of control. In this paper, we propose a “Robust Reinforcement Learning Control System (RRLCS)” through combining reinforcement learning to treat unknown nonlinear systems and using robust control theory to guarantee the robustness and stability of the system. As a robust control method, we adopt H∞ control which is robust to modelling error and disturbance. On the other hand, as a reinforcement learning method, we adopt an Actor-Critic method with minimal amount of computation for the continuous action and state space. Moreover, we analyze the stability of the proposed system using H∞ tracking performance and Lyapunov function. Finally, through the computer simulation for controlling the inverted pendulum system, we show the effectiveness of the proposed method comparing with an Adaptive Fuzzy Control method with H∞ tracking performance compensator (AFC) and an Auto-Structuring Fuzzy Neural Control System method (ASFNCS).","2093-7121","978-89-93215-03-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6106429","","Learning;Control systems;Stability analysis;Robustness;Robust control;Nonlinear systems;Lyapunov methods","closed loop systems;control system synthesis;H∞ control;learning (artificial intelligence);Lyapunov methods;nonlinear control systems;pendulums;robust control;state-space methods;tracking","robust reinforcement learning control system;H∞ tracking performance compensator;robust control theory;closed-loop system stability;mathematical model;control system design;nonlinear system;modelling error;state space method;Lyapunov function;inverted pendulum system;adaptive fuzzy control method;autostructuring fuzzy neural control system method;actor-critic method","","1","","7","","19 Dec 2011","","","IEEE","IEEE Conferences"
"Design of Reinforcement Learning Algorathm for Single Inverted Pendulum Swing Control","Y. Chao; L. Yongxin; W. Linglin","College of Electronic Information Engineering, Inner Mongolia University, Hohhot, China; College of Electronic Information Engineering, Inner Mongolia University, Hohhot, China; College of Computer Science, Inner Mongolia University, Hohhot, China","2018 Chinese Automation Congress (CAC)","24 Jan 2019","2018","","","1558","1562","In order to solve the time of the swinging up of the single inverted pendulum is quite long. This paper adopts an improved reinforcement learning algorithm, which is simulated in a double-layer BP neural network. The off-line BP neural network was trained by simulation data in order to acquire reinforcement learning swinging up controller, which was applied in the GLIP2003 inverted pendulum system. Experimental results show that the swinging up controller of reinforcement learning has a certain speed, and the adjustment time of the entire system is less than 8s. The steady state error of the pendulum arc is 0, and the steady state error of small car's position is 0.05m.","","978-1-7281-1312-8","10.1109/CAC.2018.8623253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8623253","single inverted pendulum;reinforcement learning;BP neural network;swinging up control","Neural networks;Reinforcement learning;Automobiles;Machine learning algorithms;Acceleration;Learning systems;Prediction algorithms","backpropagation;neural nets;neurocontrollers;nonlinear control systems;pendulums","single inverted pendulum swing control;double-layer BP neural network;off-line BP neural network;pendulum arc;reinforcement learning;GLIP2003 inverted pendulum system","","1","","14","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Reinforcement learning for a snake-like robot controlled by a central pattern generator","S. Fukunaga; Y. Nakamura; K. Aso; S. Ishii","Nara Institute of Science and Technology, Ikoma, Nara, Japan; Japan Science and Technology Agency, Japan; Future Project Division, Toyota Motor Corporation, Minato, Tokyo, Japan; Japan Science and Technology Agency, Japan","IEEE Conference on Robotics, Automation and Mechatronics, 2004.","13 Jun 2005","2004","2","","909","914 vol.2","A snake has large potential to move in various environments by drastically changing its 'gait' pattern, in spite of its simple body. We configured a control scheme for a snakelike robot, using a central pattern generator, and developed a learning algorithm to acquire a good control rule in a changing environment. Although the snake-like robot has a large degree of freedom, a computer simulation showed that a control rule that allows the robot to move to a target direction on an inclined ground or in a changing environment is obtained by our scheme.","","0-7803-8645-0","10.1109/RAMECH.2004.1438039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1438039","","Learning;Centralized control;Robot control;Recurrent neural networks;Control systems;Computer simulation;Friction;Animals;Propulsion;Oscillators","mobile robots;learning (artificial intelligence);control system analysis computing","snake-like robot;central pattern generator;reinforcement learning;gait pattern;control scheme;learning algorithm;changing environment;computer simulation;inclined ground","","1","","10","IEEE","13 Jun 2005","","","IEEE","IEEE Conferences"
"Browser/Server based Experimental Environment for Reinforcement Learning","H. Zhang; L. Duan; X. Zeng; X. Tang","College of Electronic Information and Electrical Engineering, Xiangnan University, Chenzhou, China; College of Electronic Information and Electrical Engineering, Xiangnan University, Chenzhou, China; College of Electronic Information and Electrical Engineering, Xiangnan University, Chenzhou, China; College of Electronic Information and Electrical Engineering, Xiangnan University, Chenzhou, China","2018 IEEE 3rd Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","16 Dec 2018","2018","","","571","575","This paper has proposed an implement method using browser and server framework to train the reinforcement learning based AI (Artificial Intelligence). By taking advantage of the WebSocket to transfer information during the AI learning, the browser is in charge of rendering the experimental environment by HTML5 and JavaScript, and the server is realized by c++ to train the policy gradient AI mode which feeds the output actions back to its input layer. With the proposed framework, the rendering environment and the training environment can be implemented independently to different runtime environments. Finally, the effectiveness of the B/S based AI learning model proposed in this paper is validated through the application of the game `Flappy Bird'.","2381-0947","978-1-5386-4509-3","10.1109/IAEAC.2018.8577830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8577830","B/S framework;WebSocket;artificial intelligence;reinforcement learning;policy gradient","Browsers;Servers;Games;Training;Birds","client-server systems;computer games;Java;learning (artificial intelligence);rendering (computer graphics)","experimental environment;reinforcement learning;HTML5;policy gradient AI mode;rendering environment;training environment;AI learning model;flappy bird;c++;JavaScript;browser-server based experimental environment;runtime environments;artificial intelligence","","1","","10","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"H∞ optimal control of unknown linear discrete-time systems: An off-policy reinforcement learning approach","B. Kiumarsi; H. Modares; F. L. Lewis; Z. -P. Jiang","UTA Research Institute UTARI, The University of Texas at Arlington, Ft. Worth, TX, USA; UTA Research Institute UTARI, The University of Texas at Arlington, Ft. Worth, TX, USA; UTA Research Institute UTARI, The University of Texas at Arlington, Ft. Worth, TX, USA; Department of Electrical and Computer Engineering, New York University, Brooklyn, NY, USA","2015 IEEE 7th International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM)","24 Sep 2015","2015","","","41","46","This paper proposes a model-free H∞ control design for linear discrete-time systems using reinforcement learning (RL). A novel off-policy RL algorithm is used to solve the game algebraic Riccati equation (GARE) online using the measured data along the system trajectories. The proposed RL algorithm has the following advantages compared to existing model-free RL methods for solving H∞ control problem: 1) It is data efficient and fast since a stream of experiences which is obtained from executing a fixed behavioral policy is reused to update many value functions correspond to different leaning policies sequentially. 2) The disturbance input does not need to be adjusted in a specific manner. 3) There is no bias as a result of adding a probing noise to the control input to maintain persistence of excitation conditions. A simulation example is used to verify the effectiveness of the proposed control scheme.","2326-8239","978-1-4673-7338-8","10.1109/ICCIS.2015.7274545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7274545","H∞ control;reinforcement learning;off-policy;game algebraic Riccati equation","Decision support systems;Conferences;Random access memory","control system synthesis;discrete time systems;game theory;H∞ control;learning (artificial intelligence);learning systems;linear systems;Riccati equations","H∞ optimal control;unknown linear discrete-time systems;off-policy reinforcement learning approach;model-free H∞ control design;off-policy RL algorithm;game algebraic Riccati equation;GARE;system trajectories;model-free RL methods;H∞ control problem;data efficient;fixed behavioral policy;value functions;leaning policies;disturbance input;excitation conditions","","1","","16","IEEE","24 Sep 2015","","","IEEE","IEEE Conferences"
"Optimal State Estimation Using Model-Free Reinforcement Learning","H. Ma; Y. Yang; D. Liang","Department of Mechanics and Engineering Science, State Key Lab for Turbulence and Complex Systems, College of Engineering, Peking University, Beijing, P.R. China; Department of Mechanics and Engineering Science, State Key Lab for Turbulence and Complex Systems, College of Engineering, Peking University, Beijing, P.R. China; Department of Mechanics and Engineering Science, State Key Lab for Turbulence and Complex Systems, College of Engineering, Peking University, Beijing, P.R. China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","6668","6673","This article investigates adaptive dynamic programming based optimal state estimation where the effect of inevitable noise is considered. The designed optimal state estimation process conforms to Bellman optimality principle. Based on adaptive dynamic programming, the proposed optimal state estimation method provides the design condition for the Kalman filter gain. A novel model-free reinforcement learning algorithm is proposed to synthesize the Kalman filter gain, which can handle state estimation without the knowledge of system dynamics. A simulation is demonstrated to show the effectiveness of the proposed method.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728004","State estimation;reinforcement learning;modelfree method;Kalman filter","Adaptation models;System dynamics;Simulation;Heuristic algorithms;Design methodology;Reinforcement learning;Observers","dynamic programming;Kalman filters;optimisation;reinforcement learning;state estimation","adaptive dynamic programming based optimal state estimation;designed optimal state estimation process;Bellman optimality principle;optimal state estimation method;Kalman filter gain;model-free reinforcement learning algorithm;system dynamics;design condition","","1","","20","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Coordinated Ramp Metering Control Based on Multi-Agent Reinforcement Learning","J. Tan; Q. Qiu; W. Guo","Beijing Key Lab of Urban Intelligent Traffic Control Technology North China University of Technology, Beijing, China; Beijing Key Lab of Urban Intelligent Traffic Control Technology North China University of Technology, Beijing, China; Beijing Key Lab of Urban Intelligent Traffic Control Technology North China University of Technology, Beijing, China","2020 35th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","5 Feb 2021","2020","","","492","498","In view of the high nonlinearity, fuzziness, randomness and uncertainty of expressway system, it is suitable to use the reinforcement learning method of “no model, self-learning, data-driven” to model, which can solve the problem that the traditional control method relies on prior knowledge and model parameter calibration. Based on the SARSA learning algorithm, this paper proposes a coordinated control strategy based on Multi-Agent Reinforcement Learning, which aims at maintaining the main line occupancy near the critical value and keeping the queue length of each ramp within the critical value. The online simulation platform is built by MATLAB and VISSIM, and the control model proposed in this paper is compared with ALINEA, Bottleneck and other traditional control methods. The results show that the model can not only maintain the stability of the main line traffic flow, but also balance the traffic pressure between adjacent ramps, and effectively improve the overall traffic condition of the expressway.","","978-1-7281-7684-0","10.1109/YAC51587.2020.9337711","National Key Research and Development Program of China(grant numbers:2018YFB1601104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9337711","Ramp metering;multi-agent reinforcement learning;coordinated control","Road transportation;Landline;Uncertainty;Reinforcement learning;Stability analysis;Mathematical model;Matlab","learning (artificial intelligence);multi-agent systems;road traffic;road traffic control;traffic engineering computing","VISSIM;MATLAB;ALINEA;SARSA learning algorithm;self-learning;reinforcement learning method;uncertainty;coordinated ramp metering control;traditional control method;control model;critical value;multiagent reinforcement learning;coordinated control strategy","","1","","16","IEEE","5 Feb 2021","","","IEEE","IEEE Conferences"
"Reinforcement learning based group navigation approach for multiple autonomous robotic system","O. Azouaoui; A. Cherifi; R. Bensalem; A. Farah","Centre de Dévelopment des Technologies Avancées, Algiers, Algeria; Centre de Dévelopment des Technologies Avancées, Algiers, Algeria; Centre de Dévelopment des Technologies Avancées, Algiers, Algeria; ENP Ecole Nationale Polytechnique, Algiers, Algeria","IEEE International Conference Mechatronics and Automation, 2005","8 May 2006","2005","3","","1539","1544 Vol. 3","In several complex applications, the use of multiple autonomous robotic systems (ARS) becomes necessary to achieve different tasks such as foraging and transport of heavy and large objects with less cost and more efficiency. They have to achieve a high level of flexibility, adaptability and efficiency in real environments. In this paper, a reinforcement learning (RL) based group navigation approach for multiple ARS is suggested. Indeed, the robots must have the ability to form geometric figures and navigate without collisions while maintaining the formation. Thus, each robot must learn how to take its place in the formation and avoid obstacles and other ARS from its interaction with the environment. This approach must provide ARS with capability to acquire the group navigation approach among several ARS from elementary behaviors by learning with trial and error search. Then, simulation results display the ability of the suggested approach to provide ARS with capability to navigate in a group formation in dynamic environments.","2152-744X","0-7803-9044-X","10.1109/ICMA.2005.1626784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1626784","","Learning;Navigation;Robot sensing systems;Orbital robotics;Space exploration;Humans;Intelligent robots;Application software;Costs;Displays","learning (artificial intelligence);multi-robot systems;mobile robots;telerobotics;collision avoidance","multiple autonomous robotic system;reinforcement learning;group navigation approach;elementary behaviors;collision avoidance behavior","","1","","22","IEEE","8 May 2006","","","IEEE","IEEE Conferences"
"Strategy Generation Based on Reinforcement Learning with Deep Deterministic Policy Gradient for UCAV","Y. Ma; S. Bai; Y. Zhao; C. Song; J. Yang","School of Electronics and Information, Northwestern Polytechnical University, Xian, China; School of Electronics and Information, Northwestern Polytechnical University, Xian, China; School of Electronics and Information, Northwestern Polytechnical University, Xian, China; School of Electronics and Information, Northwestern Polytechnical University, Xian, China; School of Marine Science and Technology, North-western Polytechnical University, Xian, China","2020 16th International Conference on Control, Automation, Robotics and Vision (ICARCV)","8 Jan 2021","2020","","","789","794","Unmanned Combat Aerial Vehicles (UCAVs) are essential participants in the future air-combat. Due to high dynamics and randomness of air-combat process, traditional methods are difficult to obtain the optimal maneuvering strategy. The reinforcement learning (RL) is used to solve this problem. Deep deterministic policy gradient (DDPG) is used in reinforcement learning to deal with high-dimensional and continuous action space in this paper. And a method using a temporary replay buffer is proposed to improve the efficiency of neural network training. A 3-D air-combat environment is built to verify the algorithm proposed in this paper. Result shows that the agent with strategy obtained by the RL with DDPG is able to get high advantage during the confrontation, and the training efficiency of neural network is highly improved by using temporary replay buffer.","","978-1-7281-7709-0","10.1109/ICARCV50220.2020.9305446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305446","","Neural networks;Training;Reinforcement learning;Heuristic algorithms;Games;Mathematical model;Dynamics","learning (artificial intelligence);military aircraft;neural nets;remotely operated vehicles","strategy generation;reinforcement learning;deep deterministic policy gradient;UCAV;Unmanned Combat Aerial Vehicles;essential participants;future air-combat;air-combat process;optimal maneuvering strategy;RL;DDPG;high-dimensional action space;continuous action space;temporary replay buffer;air-combat environment;high advantage","","1","","8","IEEE","8 Jan 2021","","","IEEE","IEEE Conferences"
"Mobile robot control in multi-obstacle environment based on reinforcement ant learning","Xiaodong Zhuang; Qingehun Meng; Bo Yin; Hanping Wang",Ocean University of Qingdao; Ocean University of Qingdao; Ocean University of Qingdao; Ocean University of Qingdao,"The 2002 International Conference on Control and Automation, 2002. ICCA. Final Program and Book of Abstracts.","8 Sep 2003","2002","","","220","220","","","0-7803-7412-6","10.1109/ICCA.2002.1229735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1229735","","Mobile robots;Robot control;Learning;Ant colony optimization;Oceans;Path planning;Process control;Optimal control;Navigation;Computer simulation","","","","1","","","IEEE","8 Sep 2003","","","IEEE","IEEE Conferences"
"An high-efficient online reinforcement learning algorithm for continuous-state systems","Yuanheng Zhu; Dongbin Zhao; Haibo He","The State Key Laboratory of Management and Control for Complex Systems, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Chinese Academy of Sciences, Beijing, China; Department of Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, RI, USA","Proceeding of the 11th World Congress on Intelligent Control and Automation","5 Mar 2015","2014","","","581","586","In this paper, we consider continuous-state systems and pursue a near-optimal policy through online learning. A new online reinforcement learning algorithm—MSEC (Multi-Samples in Each Cell) is proposed. The proposed algorithm combines state aggregation technique and efficient exploration principle, making high utilization of samples observed online. More concretely, we apply a grid over the continuous state space and partition it into different cells. Then, a near-upper Q iteration operator is defined to use samples in each cell and produce a near-upper Q function, whose corresponding greedy policy is efficient for exploration. MSEC is a totally model-free algorithm, which means no system dynamics is required during the implementation. It collects the system knowledge during the online learning. Based on PAC (Probability Approximately Correct) principle, MSCE can find a near-optimal policy in finite time bound online. To test the performance, an inverted pendulum is simulated and the results show the new algorithm is qualified for solving online optimal control problems.","","978-1-4799-5825-2","10.1109/WCICA.2014.7052778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052778","reinforcement learning;probability approximately correct;state aggregation","Algorithm design and analysis;Heuristic algorithms;Partitioning algorithms;Learning (artificial intelligence);Approximation algorithms;Polynomials;Upper bound","continuous time systems;control engineering computing;iterative methods;learning (artificial intelligence);nonlinear systems;optimal control;probability","online reinforcement learning;continuous-state system;near-optimal policy;MSEC;multisamples in each cell;state aggregation technique;efficient exploration principle;near-upper Q iteration operator;greedy policy;probability approximately correct principle;inverted pendulum","","1","","11","IEEE","5 Mar 2015","","","IEEE","IEEE Conferences"
"Particle Swarm Optimized Reinforcement Learning-Based Fault Tolerant Control","D. Zhang; H. Zhang; C. Li; A. Wu","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China","2018 13th World Congress on Intelligent Control and Automation (WCICA)","31 Jan 2019","2018","","","515","519","Data-driven fault-tolerant controllers have recently gained considerable attention due to their important role in modern technological systems. The reinforcement learning is an attractive tool of data-driven but it is limited to the learning speed. A particle swarm optimization is proposed to improve the learning speed of reinforcement learning by transferring the series computation to parallel operation. The numeric simulation shows it is a feasible approach.","","978-1-5386-7346-1","10.1109/WCICA.2018.8630475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630475","","Reinforcement learning;Particle swarm optimization;Performance analysis;Optimization;Fault tolerance;Fault tolerant systems;Convergence","control engineering computing;control system synthesis;fault tolerant control;learning (artificial intelligence);particle swarm optimisation","particle swarm optimization;learning speed;data-driven fault-tolerant controllers;particle swarm optimized reinforcement learning-based fault tolerant control;series computation;numeric simulation","","1","","19","IEEE","31 Jan 2019","","","IEEE","IEEE Conferences"
"Comparative Assessment of Reinforcement Learning Algorithms in the Taskof Robotic Manipulation of Deformable Linear Objects","M. Bednarek; K. Walas","Perception for Physical Interaction Laboratory, Poznan University of Technology, Poznan, Poland; Perception for Physical Interaction Laboratory, Poznan University of Technology, Poznan, Poland","2019 4th International Conference on Robotics and Automation Engineering (ICRAE)","23 Mar 2020","2019","","","173","177","Reinforcement learning systems in robotics are still limited in their number of practical applications. They are often considered as unstable and difficult to implement. Moreover, very often, they demand a significant number of trials to the convergence, which may often be treated as a critical challenge in their application. However, gathering the data from the simulation can be the solution to that problem. In our paper, we are providing a comparative assessment of reinforcement learning algorithms in the task of robotic manipulation of Deformable Linear Objects (DLOs). We provide a comparison of four methods that work on the simulated robot. The tests were performed for two tasks - one is reaching, and the other is the folding of the DLO to the predefined, sinusoidal shape. The obtained results could be treated as a guideline for other researchers on the performance of RL methods in robotic manipulation tasks.","","978-1-7281-4740-6","10.1109/ICRAE48301.2019.9043790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9043790","reinforcement learning;robotics;manipulation;deformable;elastic","Task analysis;Reinforcement learning;Shape;Service robots;Entropy;Learning systems","learning (artificial intelligence);learning systems;manipulators","comparative assessment;deformable linear objects;simulated robot;robotic manipulation tasks;reinforcement learning systems;DLO;RL methods","","1","","15","IEEE","23 Mar 2020","","","IEEE","IEEE Conferences"
