@article{LANG2021793,
title = {Modeling Production Scheduling Problems as Reinforcement Learning Environments based on Discrete-Event Simulation and OpenAI Gym},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {793-798},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.093},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321008399},
author = {Sebastian Lang and Maximilian Kuetgens and Paul Reichardt and Tobias Reggelin},
keywords = {Reinforcement Learning, Production Scheduling, Production Planning, Control, Discrete Event Modeling, Simulation, OpenAI Gym, Artificial Intelligence, Deep Learning, Neural Network},
abstract = {Reinforcement learning (RL) is an emerging research topic in production and logistics, as it offers potentials to solve complex planning and control problems in real time. In recent years, many researchers investigated RL algorithms for solving production scheduling problems. However, most of the related articles reveal only little information about the process of developing and implementing RL applications. Against this background, we present a method for modeling production scheduling problems as RL environments. More specifically, we propose the application of Discrete-Event Simulation for modeling production scheduling problems as an interoperable environments and the Gym interface of the OpenAI foundation to allow a simple integration of pre-built RL algorithms from OpenAI Baselines and Stable Baselines. We support our explanations with a simple example of a job shop scheduling problem.}
}
@article{HU2020106749,
title = {Deep reinforcement learning based AGVs real-time scheduling with mixed rule for flexible shop floor in industry 4.0},
journal = {Computers & Industrial Engineering},
volume = {149},
pages = {106749},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106749},
url = {https://www.sciencedirect.com/science/article/pii/S036083522030468X},
author = {Hao Hu and Xiaoliang Jia and Qixuan He and Shifeng Fu and Kuo Liu},
keywords = {Automated guided vehicles, Real-time scheduling, Deep reinforcement learning, Industry 4.0},
abstract = {Driven by the recent advances in industry 4.0 and industrial artificial intelligence, Automated Guided Vehicles (AGVs) has been widely used in flexible shop floor for material handling. However, great challenges aroused by the high dynamics, complexity, and uncertainty of the shop floor environment still exists on AGVs real-time scheduling. To address these challenges, an adaptive deep reinforcement learning (DRL) based AGVs real-time scheduling approach with mixed rule is proposed to the flexible shop floor to minimize the makespan and delay ratio. Firstly, the problem of AGVs real-time scheduling is formulated as a Markov Decision Process (MDP) in which state representation, action representation, reward function, and optimal mixed rule policy, are described in detail. Then a novel deep q-network (DQN) method is further developed to achieve the optimal mixed rule policy with which the suitable dispatching rules and AGVs can be selected to execute the scheduling towards various states. Finally, the case study based on a real-world flexible shop floor is illustrated and the results validate the feasibility and effectiveness of the proposed approach.}
}
@article{LI2023110658,
title = {A Reinforcement Learning-Artificial Bee Colony algorithm for Flexible Job-shop Scheduling Problem with Lot Streaming},
journal = {Applied Soft Computing},
volume = {146},
pages = {110658},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110658},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623006762},
author = {Yibing Li and Cheng Liao and Lei Wang and Yu Xiao and Yan Cao and Shunsheng Guo},
keywords = {Reinforcement Learning, Artificial Bee Colony algorithm, Flexible Job-shop Scheduling Problems, Lot Streaming},
abstract = {As a typical production model in manufacturing industry, Flexible Job-shop Scheduling Problem (FJSP) has an important impact on enhancing the productivity of enterprises. Flexible Job-shop Scheduling Problem with Lot Streaming (FJSP-LS) is an extension of FJSP that allows jobs to be split into multiple sublots so they can be processed and transported separately. Since FJSP-LS has a large solution space and it is difficult and unstable for many algorithms to find a high-quality solution, this paper proposes a hybrid algorithm combining Reinforcement Learning and Artificial Bee Colony (RL-ABC) algorithm. In RL-ABC, the utilities for solving FJSP-LS are divided into 2 stages: (1) determining the best dispatch scheme and (2) determining the best scheme of sublots. For stage 1, an algorithm with different initialization and local search strategies is proposed. For stage 2, reinforcement learning is developed by building mappings between the environment and schemes of sublots. The effectiveness and robustness of RL-ABC algorithm and its components are compared with five algorithms including three types (traditional heuristic algorithm, improved heuristic algorithm and new evolutionary algorithm) on nineteen benchmark instances and three real instances. The results show that although RL-ABC algorithm exhibits inferior performance in terms of CPU time, its effectiveness and robustness surpass all the other compared algorithms on all instances. Moreover, both components of the RL-ABC algorithm effectively reduce the Makespan. Therefore, it can be used as a new technique to solve large-scale and complex problems in scheduling domain.}
}
@article{HU20191242,
title = {Joint Manufacturing and Onsite Microgrid System Control Using Markov Decision Process and Neural Network Integrated Reinforcement Learning},
journal = {Procedia Manufacturing},
volume = {39},
pages = {1242-1249},
year = {2019},
note = {25th International Conference on Production Research Manufacturing Innovation: Cyber Physical Manufacturing August 9-14, 2019 | Chicago, Illinois (USA)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.345},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920304121},
author = {Wenqing Hu and Zeyi Sun and Yunchao Zhang and Yu Li},
keywords = {Onsite generation system, Manufacturing system, Markov Decision Process, Neural network, Reinforcement learning},
abstract = {Onsite microgrid generation systems with renewable sources are considered a promising complementary energy supply system for manufacturing plant, especially when outage occurs during which the energy supplied from the grid is not available. Compared to the widely recognized benefits in terms of the resilience improvement when it is used as a backup energy system, the operation along with the electricity grid to support the manufacturing operations in non-emergent mode has been less investigated. In this paper, we propose a joint dynamic decision-making model for the optimal control for both manufacturing system and onsite generation system. Markov Decision Process (MDP) is used to formulate the decision-making model. A neural network integrated reinforcement learning algorithm is proposed to approximately estimate the value function given policy of MDP. A case study based on a manufacturing system as well as a typical onsite microgrid generation system is conducted to validate the proposed MDP model as well as the solution strategy.}
}
@article{ARANAAREXOLALEIBA20191508,
title = {Transferring Human Manipulation Knowledge to Industrial Robots Using Reinforcement Learning},
journal = {Procedia Manufacturing},
volume = {38},
pages = {1508-1515},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.136},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920301372},
author = {N. Arana-Arexolaleiba and N. Urrestilla-Anguiozar and D. Chrysostomou and S. BÃ¸gh},
keywords = {Reinforcement Learning, Q-learning, Robot Control, Self-Learning Capabilities},
abstract = {Nowadays in the context of Industry 4.0, manufacturing companies are faced by increasing global competition and challenges, which requires them to become more flexible and able to adapt fast to rapid market changes. Advanced robot system is an enabler for achieving greater flexibility and adaptability, however, programming such systems also become increasingly more complex. Thus, new methods for programming robot systems and enabling self-learning capabilities to accommodate the natural variation exhibited in real-world tasks are needed. In this paper, we propose a Reinforcement Learning (RL) enabled robot system, which learns task trajectories from human workers. The presented work demonstrates that with minimal human effort, we can transfer manual manipulation tasks in certain domains to a robot system without the requirement for a complicated hardware system model or tedious and complex programming. Furthermore, the robot is able to build upon the learned concepts from the human expert and improve its performance over time. Initially, Q-learning is applied, which has shown very promising results. Preliminary experiments, from a use case in slaughterhouses, demonstrate the viability of the proposed approach. We conclude that the feasibility and applicability of RL for industrial robots and industrial processes, holds and unseen potential, especially for tasks where natural variation is exhibited in either the product or process.}
}
@article{KLAR2022555,
title = {A Framework for Automated Multiobjective Factory Layout Planning using Reinforcement Learning},
journal = {Procedia CIRP},
volume = {112},
pages = {555-560},
year = {2022},
note = {15th CIRP Conference on Intelligent Computation in ManufacturingEngineering, 14-16 July 2021},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.09.099},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122012689},
author = {Matthias Klar and Pascal Langlotz and Jan C. Aurich},
keywords = {Factory Planning, Layout Planning, Multiobjective Optimization, Machine Learning, Reinforcement Learning},
abstract = {Layout planning is a central element of the factory planning process. Given its complexity, layout planning is often time consuming and involves creative processes. One possible way to deal with this complexity is the training of a machine learning algorithm, which enables to generate and optimize factory layouts. Consequently, this paper outlines a reinforcement learning based concept for automated layout planning. In particular, boundary conditions and objective functions are derived from the existing planning parameters of factory layouts. The presented approach will allow a multiobjective optimization of the layout and uses material flow and energy consumption as optimization criteria.}
}
@article{ALLAGUI2023103992,
title = {Reinforcement learning for disassembly sequence planning optimization},
journal = {Computers in Industry},
volume = {151},
pages = {103992},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103992},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001422},
author = {Amal Allagui and Imen Belhadj and RÃ©gis Plateaux and Moncef Hammadi and Olivia Penas and Nizar Aifaoui},
keywords = {Disassembly sequence planning, Reinforcement learning, Q-Network, Mechanical disassembly, Selective disassembly, Full disassembly},
abstract = {Abstract-
The disassembly process is one of the most expensive phases in the product life cycle for both maintenance and the End of Life dismantling process. Industry must optimize the disassembly sequence to ensure time-cost-efficiency. This paper presents a new approach based on the Reinforcement Learning algorithm to optimize Disassembly Sequence Planning. This research work focuses on two types of dismantling: partial and full disassembly. By introducing a fitness function within the Reinforcement Learning algorithm, it is aimed at implementing optimized Disassembly Sequence Planning for five disassembly parameters or goals: (1) minimizing disassembly tool changes, (2) minimizing disassembly direction changes, (3) optimizing dismantling time including preparation and processing time, (4) prioritizing the dismantling of the smallest parts, and (5) facilitating access to wear parts. The proposed approach is applied to a demonstrative example. Finally, a comparison with other approaches from the literature is provided to demonstrate the efficiency of the new approach.}
}
@article{KHADER201894,
title = {Online control of stencil printing parameters using reinforcement learning approach},
journal = {Procedia Manufacturing},
volume = {17},
pages = {94-101},
year = {2018},
note = {28th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2018), June 11-14, 2018, Columbus, OH, USAGlobal Integration of Intelligent Manufacturing and Smart Industry for Good of Humanity},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S235197891831134X},
author = {Nourma Khader and Sang Won Yoon},
keywords = {Surface mount technology, Stencil printing process, Reinforcement learning, Q-learning},
abstract = {This research proposes a novel approach to control the stencil printing process (SPP) parameters online in surface mount technology (SMT) of printed circuit boards (PCBs). Several external variables induce variations in stencil printing quality including environment conditions, operator faults, and others. This research aims to build an optimal adaptive controller that captures these variations and consequently adjusts the controllable and significant printing parameters to enhance the solder paste volume transfer efficiency (TE) during actual production run. Q-learning which is a reinforcement learning (RL) approach is used to control the main printing parameters (printing speed and pressure, and the separation speed) online. The results show that Q-learning converges to the optimal policy for the SPP problem, and the optimal sets of actions for different states are retrieved using Q-table. Moreover, the developed controller is capable to reach the terminal state for several testing examples with taking few actions.}
}
@article{HU20201,
title = {Petri-net-based dynamic scheduling of flexible manufacturing system via deep reinforcement learning with graph convolutional network},
journal = {Journal of Manufacturing Systems},
volume = {55},
pages = {1-14},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520300145},
author = {Liang Hu and Zhenyu Liu and Weifei Hu and Yueyang Wang and Jianrong Tan and Fei Wu},
keywords = {Dynamic scheduling, Petri nets, Deep reinforcement learning, Graph convolutional networks, Digital twin},
abstract = {To benefit from the accurate simulation and high-throughput data contributed by advanced digital twin technologies in modern smart plants, the deep reinforcement learning (DRL) method is an appropriate choice to generate a self-optimizing scheduling policy. This study employs the deep Q-network (DQN), which is a successful DRL method, to solve the dynamic scheduling problem of flexible manufacturing systems (FMSs) involving shared resources, route flexibility, and stochastic arrivals of raw products. To model the system in consideration of both manufacturing efficiency and deadlock avoidance, we use a class of Petri nets combining timed-place Petri nets and a system of simple sequential processes with resources (S3PR), which is named as the timed S3PR. The dynamic scheduling problem of the timed S3PR is defined as a Markov decision process (MDP) that can be solved by the DQN. For constructing deep neural networks to approximate the DQN action-value function that maps the timed S3PR states to scheduling rewards, we innovatively employ a graph convolutional network (GCN) as the timed S3PR state approximator by proposing a novel graph convolution layer called a Petri-net convolution (PNC) layer. The PNC layer uses the input and output matrices of the timed S3PR to compute the propagation of features from places to transitions and from transitions to places, thereby reducing the number of parameters to be trained and ensuring robust convergence of the learning process. Experimental results verify that the proposed DQN with a PNC network can provide better solutions for dynamic scheduling problems in terms of manufacturing performance, computational efficiency, and adaptability compared with heuristic methods and a DQN with basic multilayer perceptrons.}
}
@article{OSSENKOPF2017329,
title = {Reinforcement Learning for Manipulators without Direct Obstacle Perception in Physically Constrained Environments},
journal = {Procedia Manufacturing},
volume = {11},
pages = {329-337},
year = {2017},
note = {27th International Conference on Flexible Automation and Intelligent Manufacturing, FAIM2017, 27-30 June 2017, Modena, Italy},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2017.07.115},
url = {https://www.sciencedirect.com/science/article/pii/S2351978917303190},
author = {Marie Ossenkopf and Philipp Ennen and Rene Vossen and Sabina Jeschke},
keywords = {Dynamic Movement Primitives, Relative Entropy Policy Search, Intelligent Manufacturing, Potential Field, Assembly Robot},
abstract = {Reinforcement Learning algorithms have the downside of potentially dangerous exploration of unknown states, which makes them largely unsuitable for the use on serial manipulators in an industrial setting. In this paper, we make use of a policy search algorithm and provide two extensions that aim to make learning more applicable on robots in industrial environments without the need of complex sensors. They build upon the use of Dynamic Movement Primitives (DMPs) as policy representation. Rather than model explicitly the skills of the robot we describe actions the robot should not try to do. First, we implement potential fields into the DMPs to keep planned movements inside the robot's workspace. Second, we monitor and evaluate the deviation in the DMPs to recognize and learn from collisions. Both extensions are evaluated in a simulation}
}
@article{LIU2022102365,
title = {A digital twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial robot grasping},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {78},
pages = {102365},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102365},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000539},
author = {Yongkui Liu and He Xu and Ding Liu and Lihui Wang},
keywords = {Deep reinforcement learning, Sim-to-real transfer, Digital twin, Robot grasping},
abstract = {Deep reinforcement learning (DRL) has proven to be an effective framework for solving various complex control problems. In manufacturing, industrial robots can be trained to learn dexterous manipulation skills from raw pixels with DRL. However, training robots in the real world is a time-consuming, high-cost and of safety concerns process. A frequently adopted approach for easing this is to train robots through simulations first and then deploy algorithms (or policies) on physical robots. How to transfer policies of robot learning from simulation to the real world is a challenging issue. Digital twin that is able to create a dynamic, up-to-date representation of a physical robotic grasping system provides an effective approach for addressing this issue. In this paper, we focus on the scenario of DRL-based assembly-oriented industrial grasping and propose a digital twin-enabled approach for achieving effective transfer of DRL algorithms to a physical robot. Two parallel training systems, i.e., the physical robotic system and corresponding digital twin system, respectively, are established, which take virtual and real images as inputs. The output of the digital twin system is used to correct the real grasping point so that accurate grasping can be achieved. Experimental results verify the effectiveness of the intelligent grasping algorithm and the digital twin-enabled sim-to-real transfer approach and mechanism.}
}
@article{AREND2022100341,
title = {MLPro 1.0 - Standardized reinforcement learning and game theory in Python},
journal = {Machine Learning with Applications},
volume = {9},
pages = {100341},
year = {2022},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2022.100341},
url = {https://www.sciencedirect.com/science/article/pii/S2666827022000482},
author = {Detlef Arend and Steve Yuwono and Mochammad Rizky Diprasetya and Andreas Schwung},
keywords = {Machine learning, Reinforcement learning, Game theory, Automation, Scientific software development, Python},
abstract = {Nowadays there are numerous powerful software packages available for most areas of machine learning (ML). These can be roughly divided into frameworks that solve detailed aspects of ML and those that pursue holistic approaches for one or two learning paradigms. For the implementation of own ML applications, several packages often have to be involved and integrated through individual coding. The latter aspect in particular makes it difficult for newcomers to get started. It also makes a comparison with other works difficult, if not impossible. Especially in the area of reinforcement learning (RL), there is a lack of frameworks that fully implement the current concepts up to multi-agents (MARL) and model-based agents (MBRL). For the related field of game theory (GT), there are hardly any packages available that aim to solve real-world applications. Here we would like to make a contribution and propose the new framework MLPro, which is designed for the holistic realization of hybrid ML applications across all learning paradigms. This is made possible by an additional base layer in which the fundamentals of ML (interaction, adaptation, training, hyperparameter optimization) are defined on an abstract level. In contrast, concrete learning paradigms are implemented in higher sub-frameworks that build on the conventions of this additional base layer. This ensures a high degree of standardization and functional recombinability. Proven concepts and algorithms of existing frameworks can still be used. The first version of MLPro includes sub-frameworks for RL and cooperative GT.}
}
@article{CHUNG2022822,
title = {Reinforcement learning-based defect mitigation for quality assurance of additive manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {822-835},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001996},
author = {Jihoon Chung and Bo Shen and Andrew Chung Chee Law and Zhenyu (James) Kong},
keywords = {Additive Manufacturing (AM), Model-free Reinforcement Learning (RL), G-Learning, Knowledge Transfer, Defect Mitigation, Fused Filament Fabrication (FFF)},
abstract = {Additive Manufacturing (AM) is a powerful technology that produces complex 3D geometries using various materials in a layer-by-layer fashion. However, quality assurance is the main challenge in AM industry due to the possible time-varying processing conditions during AM process. Notably, new defects may occur during printing, which cannot be mitigated by offline analysis tools that focus on existing defects. This challenge motivates this work to develop online learning-based methods to deal with the new defects during printing. Since AM typically fabricates a small number of customized products, this paper aims to create an online learning-based strategy to mitigate the new defects in AM process while minimizing the number of samples needed. The proposed method is based on model-free Reinforcement Learning (RL). It is called Continual G-learning since it transfers several sources of prior knowledge to reduce the needed training samples in the AM process. Offline knowledge is obtained from literature, while online knowledge is learned during printing. The proposed method develops a new algorithm for learning the optimal defect mitigation strategies proven the best performance when utilizing both knowledge sources. Numerical and real-world case studies in a fused filament fabrication (FFF) platform are performed and demonstrate the effectiveness of the proposed method.}
}
@article{WANG2022102324,
title = {Solving job scheduling problems in a resource preemption environment with multi-agent reinforcement learning},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {77},
pages = {102324},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102324},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000138},
author = {Xiaohan Wang and Lin Zhang and Tingyu Lin and Chun Zhao and Kunyu Wang and Zhen Chen},
keywords = {Job shop scheduling problem, Reinforcement learning, Smart manufacturing, Multi-agent reinforcement learning, QMIX},
abstract = {In smart manufacturing, robots gradually replace traditional machines as new processing units, which have significantly liberated laborers and reduced manufacturing expenditure. However, manufacturing resources are usually limited so that the preemption relationship exists among robots. Under this circumstance, job scheduling puts forward higher requirements on accuracy and generalization. To this end, this paper proposes a scheduling algorithm to solve job scheduling problems in a resource preemption environment with multi-agent reinforcement learning. The resource preemption environment is modeled as a decentralized partially observable Markov decision process, where each job is regarded as an intelligent agent that chooses an available robot according to its current partial observation. Based on this modeling, a multi-agent scheduling architecture is constructed to handle the high-dimension action space issue caused by multi-task simultaneous scheduling. Besides, multi-agent reinforcement learning is employed to learn both the decision-making policy of each agent and the cooperation between job agents. This paper is novel in addressing the scheduling problem in a resource preemption environment and solving the job shop scheduling problem with multi-agent reinforcement learning. The experiments of the case study indicate that our proposed method outperforms the traditional rule-based methods and the distributed-agent reinforcement learning method in total makespan, training stability, and model generalization.}
}
@article{KIM2020440,
title = {Multi-agent system and reinforcement learning approach for distributed intelligence in a flexible smart manufacturing system},
journal = {Journal of Manufacturing Systems},
volume = {57},
pages = {440-450},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301916},
author = {Yun Geon Kim and Seokgi Lee and Jiyeon Son and Heechul Bae and Byung Do Chung},
keywords = {Distributed decision making, Multi-agent system, Reinforcement learning, Smart manufacturing},
abstract = {Personalized production has emerged as a result of the increasing customer demand for more personalized products. Personalized production systems carry a greater amount of uncertainty and variability when compared with traditional manufacturing systems. In this paper, we present a smart manufacturing system using a multi-agent system and reinforcement learning, which is characterized by machines with intelligent agents to enable a system to have autonomy of decision making, sociability to interact with other systems, and intelligence to learn dynamically changing environments. In the proposed system, machines with intelligent agents evaluate the priorities of jobs and distribute them through negotiation. In addition, we propose methods for machines with intelligent agents to learn to make better decisions. The performance of the proposed system and the dispatching rule is demonstrated by comparing the results of the scheduling problem with early completion, productivity, and delay. The obtained results show that the manufacturing system with distributed artificial intelligence is competitive in a dynamic environment.}
}
@article{PARASCHOS2021401,
title = {Parametric and reinforcement learning control for degrading multi-stage systems},
journal = {Procedia Manufacturing},
volume = {55},
pages = {401-408},
year = {2021},
note = {FAIM 2021},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.10.055},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921002523},
author = {Panagiotis D. Paraschos and Georgios K. Koulinas and Dimitrios E. Koulouriotis},
keywords = {Multi-agent reinforcement learning, Parametric policies, Maintenance, Remanufacturing, Production},
abstract = {This paper addresses the joint control problem in the context of a two-stage stochastic manufacturing/remanufacturing system, which involve both manufacturing and remanufacturing processes. Its operability is affected by frequent deterioration failures. Along with the condition of the system, the manufactured items are affected as well. Thus, the system obtains lesser revenues due to the low-quality products and the downtimes of the deteriorated system. For this purpose, the state and the condition of the systems and the manufactured products must be monitored dynamically so as to devise an optimal strategy for manufacturing, maintenance, and quality control. The present paper proposes a novel two-agent reinforcement learning framework that incorporates parametric production and maintenance activities. The aim is to improve the productivity of the system and keep the system operational with minimal maintenance activities so as to maximize the overall profitability. The performance of the presented approach is evaluated through experimental scenarios.}
}
@article{SILVA2019194,
title = {Production flow control through the use of reinforcement learning},
journal = {Procedia Manufacturing},
volume = {38},
pages = {194-202},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.026},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920300275},
author = {TomÃ© Silva and AmÃ©rico Azevedo},
keywords = {Artificial Intelligence, Reinforcement Learning, Simulation, Flow control},
abstract = {This paper introduces a new research focus for the problem of flow control. Most of the research until this point in this topic comes in the form of heuristics and flow control protocols, from which we can highlight Kanban and CONWIP. These protocols have as common ground the fact that both impact flow by limiting the amount of WIP (work in process) that circulates through a production route. These limits are not static in a sense that one limit defined for a given period will not suffice for all possible conditions the future may entail. Therefore, we need strategies to find which values for the WIP caps are best (according to an optimization target), given a production system state and a customer demand level. We propose the use of a Reinforcement learning (RL) agent and introduce the problem within the framework of a reinforcement learning problem, showing that for a simulated system it is possible to reduce WIP levels up to 43% without losses in throughput (TH). As an introduction to the flow control problem comparisons between push and pull systems are made resorting to the use of discrete event simulations. We simulated a CONWIP and a push protocol and comparisons are made in terms of cycle-time, throughput and customer lead-time. The work points-out that within the field of industrial management research terms such as cycle-time, customer lead-time, and lead-time are sometimes used interchangeably, which may lead to unnecessary confusion and hindered understanding of the subject matter. Specifically, we show that cycle-time reduction does not lead directly to customer lead-time reduction in a make to order environment.}
}
@article{HUANG2023119,
title = {A novel priority dispatch rule generation method based on graph neural network and reinforcement learning for distributed job-shop scheduling},
journal = {Journal of Manufacturing Systems},
volume = {69},
pages = {119-134},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001176},
author = {Jiang-Ping Huang and Liang Gao and Xin-Yu Li and Chun-Jiang Zhang},
keywords = {Distributed job-shop, Scheduling, Reinforcement learning, Graph neural network},
abstract = {With the development of a global economy, distributed manufacturing becomes common in the industrial field. The Distributed Job-shop Scheduling Problem (DJSP), which is widespread in real-life production, is a hotspot in the academic field. The existing Priority Dispatch Rules (PDRs), which are used to assign a value to each waiting job according to some method and select the job with minimum or maximum âvalueâ for next processing, are all relatively simple but lack self-learning ability, while the metaheuristics are all complex and with fixed evolutionary trajectory and cannot change with the manufacturing environment. This paper proposes a novel PDR generation method based on Graph Neural Network (GNN) and Reinforcement Learning (RL), which can self-learn and self-evolute by interacting with the scheduling environment. To combine DJSP with GNN closely, a new solution representation based on disjunctive graph is designed. DJSP is formulated as a Markov decision process, and the problem features and inner connections among the vertices of the disjunctive graph are fully explored by the GNN. An Actor-Critic RL method is applied to automatically train the network parameters to optimize the policy, so that it can be used to schedule the best action at each step. Comprehensive experiments on 240 test instances are conducted to evaluate the performance of the proposed method, and the results indicate that the proposed method shows greater effectiveness, generalizability and stability than other 8 classical PDRs, 5 metaheuristics and 3 RL-based methods.}
}
@article{MARTINS202010810,
title = {Reinforcement Learning for Dual-Resource Constrained Scheduling*âThis work was supported by FCT through IDMEC under LAETA project UIDP/50022/2020},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {10810-10815},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2866},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320336326},
author = {Miguel S.E. Martins and Joaquim L. Viegas and Tiago Coito and Bernardo Marreiros Firme and JoÃ£o M.C. Sousa and JoÃ£o Figueiredo and Susana M. Vieira},
keywords = {Production planning, control, Job, activity scheduling, Intelligent manufacturing systems},
abstract = {This paper proposes using reinforcement learning to solve scheduling problems where two types of resources of limited availability must be allocated. The goal is to minimize the makespan of a dual-resource constrained flexible job shop scheduling problem. Efficient practical implementation is very valuable to industry, yet it is often only solved combining heuristics and expert knowledge. A framework for training a reinforcement learning agent to schedule diverse dual-resource constrained job shops is presented. Comparison with other state-of-the-art approaches is done on both simpler and more complex instances that the ones used for training. Results show the agent produces competitive solutions for small instances that can outperform the implemented heuristic if given enough time. Other extensions are needed before real-world deployment, such as deadlines and constraining resources to work shifts.}
}
@article{HUANG2024121756,
title = {An end-to-end deep reinforcement learning method based on graph neural network for distributed job-shop scheduling problem},
journal = {Expert Systems with Applications},
volume = {238},
pages = {121756},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121756},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423022583},
author = {Jiang-Ping Huang and Liang Gao and Xin-Yu Li},
keywords = {Distributed job shop, Scheduling, Distributed scheduling, Reinforcement learning},
abstract = {Distributed Job-shop Scheduling Problem (DJSP) is a hotspot in industrial and academic fields due to its valuable application in the real-life productions. For DJSP, the available methods aways complete the job selection first and then search for an appropriate factory to assign the selected job, which means job selection and job assignment are done independently. This paper proposes an end-to-end Deep Reinforcement Learning (DRL) method to make the two decisions simultaneously. To capture the problem characteristics and realize the objective optimization, the Markov Decision Process (MDP) of DJSP is formulated. Specialised action space made up of operation-factory pairs is designed to achieve the simultaneous decision-making. A stitched disjunctive graph representation of DJSP is specially designed, and a Graph Neural Network (GNN) based feature extraction architecture is proposed to dig the state embedding during problem solving. A Proximal Policy Optimization (PPO) method is applied to train an action-selection policy. To further lead the agent to assign jobs to the factory with smaller makespan, a probability enhancement mechanism is designed. The experimental results on 240 test instances have shown that the proposed method outperforms 8 classical Priority Dispatching Rules (PDRs), 3 closely-related RL methods and 5 metaheuristics in terms of effectiveness, stability and generalization.}
}
@article{FAN2023150,
title = {Spatiotemporal path tracking via deep reinforcement learning of robot for manufacturing internal logistics},
journal = {Journal of Manufacturing Systems},
volume = {69},
pages = {150-169},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001218},
author = {Fei Fan and Guanglin Xu and Na Feng and Lin Li and Wei Jiang and Lianqin Yu and Xiaoshuang Xiong},
keywords = {Wheeled mobile robot, Internal logistics, Deep reinforcement learning, Tracking control, Hybrid control, Distributed scenarios training},
abstract = {The development of engineering technology, the logistics system composed of wheeled mobile robots (WMR) and automated guided vehicles (AGV) have been widely used in industrial scenes. However, traditional manufacturing scenes with dense layout, such as weaving workshops, have higher requirements for automatic transportation of materials. How to realize the intelligent transportation and management of materials in the manufacturing workshop and balance the efficiency and safety of material processing and transportation is still a great challenge. To address this problem, we designed a logistics system based on cloth-roll handling robot (CHR) and its path tracking hybrid deep reinforcement learning (DRL) considering spatiotemporal efficiency and safety in weaving workshop. This research first focuses on the design of a dynamic observation Markov decision-making process that integrates scene features. Further, a deep reinforcement learning considering the heterogeneity of observation data is proposed to obtain the optimal action solution. Then, a distributed scenarios training is implemented to improve the interaction ability between agents and the environment in complex scenes. In addition, the balance between dynamic observation and on-site calculation is considered in the path tracking for actual weaving workshop.}
}
@article{ELGUEAAGUINACO2023102517,
title = {A review on reinforcement learning for contact-rich robotic manipulation tasks},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {81},
pages = {102517},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102517},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001995},
author = {ÃÃ±igo Elguea-Aguinaco and Antonio Serrano-MuÃ±oz and Dimitrios Chrysostomou and Ibai Inziarte-Hidalgo and Simon BÃ¸gh and Nestor Arana-Arexolaleiba},
keywords = {Reinforcement learning, Contact-rich manipulation, Industrial manipulators, Rigid object manipulation, Deformable object manipulation},
abstract = {Research and application of reinforcement learning in robotics for contact-rich manipulation tasks have exploded in recent years. Its ability to cope with unstructured environments and accomplish hard-to-engineer behaviors has led reinforcement learning agents to be increasingly applied in real-life scenarios. However, there is still a long way ahead for reinforcement learning to become a core element in industrial applications. This paper examines the landscape of reinforcement learning and reviews advances in its application in contact-rich tasks from 2017 to the present. The analysis investigates the main research for the most commonly selected tasks for testing reinforcement learning algorithms in both rigid and deformable object manipulation. Additionally, the trends around reinforcement learning associated with serial manipulators are explored as well as the various technological challenges that this machine learning control technique currently presents. Lastly, based on the state-of-the-art and the commonalities among the studies, a framework relating the main concepts of reinforcement learning in contact-rich manipulation tasks is proposed. The final goal of this review is to support the robotics community in future development of systems commanded by reinforcement learning, discuss the main challenges of this technology and suggest future research directions in the domain.}
}
@article{YANG2023243,
title = {Real-time and concurrent optimization of scheduling and reconfiguration for dynamic reconfigurable flow shop using deep reinforcement learning},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {40},
pages = {243-252},
year = {2023},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1755581722001936},
author = {Shengluo Yang and Junyi Wang and Liming Xin and Zhigang Xu},
keywords = {Reconfigurable flow shop, Deep reinforcement learning, Deep Q-network, Scheduling and reconfiguration, Real-time scheduling},
abstract = {Reconfigurable manufacturing system (RMS) is commonly adopted under mass customization. For an RMS with dynamic events, how to real-timely schedule jobs and workshop configurations is still challenging. We studied the real-time and concurrent optimization of scheduling and reconfiguration for a dynamic reconfigurable flow shop with new job arrivals using deep reinforcement learning (DRL). The objective is to minimize the total tardiness cost of all jobs by properly configuring flow lines and scheduling jobs. The intelligent scheduling and reconfiguration system was established by designing the collaboration mechanism, training and execution procedures, and DRL-based decision models. In addition, a novel variant of deep Q-network (DQN)âexpected deep Q-network (EDQN)âwas proposed by calculating the exact expected state-action value rather than using an estimation. Furthermore, the T-step evaluation and multi-step learning were employed to reduce the estimation errors and improve the data efficiency, resulting in an EDQN_T algorithm. The training experiment shows the good learning efficiency of our proposed algorithms. The comparison experiment shows the proposed algorithms outperform several popular DRL algorithms and well-known priority dispatching rules in terms of solution quality. Besides, the decision time of trained DRL algorithms is only several milliseconds, which can fulfill the real-time scheduling under dynamics.}
}
@article{FARACI2020114204,
title = {Green wireless power transfer system for a drone fleet managed by reinforcement learning in smart industry},
journal = {Applied Energy},
volume = {259},
pages = {114204},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.114204},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919318914},
author = {Giuseppe Faraci and Angelo Raciti and Santi Agatino Rizzo and Giovanni Schembra},
keywords = {Artificial intelligence, Drone, Industry 4.0, Internet of Things, Wind generator, Wireless power transfer},
abstract = {The optimal management of a fleet of drones is proposed in this paper for providing connectivity to sensors and actuators in Industrial Internet of Things (IIoT) scenarios. The persistent mission without any human intervention on the battery charge is obtained by means of an on-field wind generator supplying a charge station that adopts resonant wireless power transfer. The objective of the fleet management is to provide the best connectivity over the time considering the variability of both the bandwidth request and the wind energy availability. The optimal management is performed by a system controller adopting reinforcement learning (RL) for deciding the number of drones to take off and, consequently, the instantaneous provided bandwidth. A constant charge time of drone battery represents a key element of the system because this enables to strongly reduce the complexity of the system controller task. To this purpose, an adaptive current control for the charge station is introduced to compensate charge time variabilities due to the coupling factor changes caused by misalignments that can occur between a pad and a drone. The results have highlighted that the RL provides good performance improvement in case of green generation. An important aspect arose from this study is the ability of RL to increase the saved energy even if it is not considered as a target of the controller.}
}
@article{HU2023109631,
title = {Knowledge-enhanced reinforcement learning for multi-machine integrated production and maintenance scheduling},
journal = {Computers & Industrial Engineering},
volume = {185},
pages = {109631},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109631},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223006551},
author = {Jueming Hu and Haiyan Wang and Hsiu-Khuern Tang and Takuya Kanazawa and Chetan Gupta and Ahmed Farahat},
keywords = {Deep reinforcement learning, Knowledge enhanced, Integrated production and maintenance optimization, Multi-machine system, Stochastic degradation, Condition-dependent cost},
abstract = {Machines deteriorate as they perform production operations, leading to increased production cost rates. When the deterioration reaches a certain level, the machine may break down, which disrupts production and requires costly Corrective Maintenance (CM) to restore operation. Preventive Maintenance (PM) can improve machine health but requires production downtime and expenses. Thus, to balance the increased production cost from degradation against the maintenance cost, it is important to jointly optimize production and maintenance scheduling. This paper aims to address the joint optimization problem for a multi-machine system to achieve the optimal overall business reward under incomplete information and system production demand constraints. We propose a novel method called Knowledge Enhanced Reinforcement Learning (KERL), which adopts a centralized multi-agent actor-critic architecture. KERL enhances the performance of Reinforcement Learning (RL) for multi-machine production and maintenance scheduling by leveraging the prior knowledge of the constraint to determine the production decisions and handle the cooperation among machines in the system. The performance of KERL is evaluated in both deterministic and stochastic case studies and is compared to three baseline methods. Results show that KERL achieves a higher overall business reward than baseline methods and learns to avoid failures in the stochastic environment.}
}
@article{MARUGAN2023103487,
title = {Applications of Reinforcement Learning for maintenance of engineering systems: A review},
journal = {Advances in Engineering Software},
volume = {183},
pages = {103487},
year = {2023},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2023.103487},
url = {https://www.sciencedirect.com/science/article/pii/S0965997823000789},
author = {Alberto Pliego MarugÃ¡n},
keywords = {Machine learning, Reinforcement Learning, Maintenance management, Engineering systems, System reliability},
abstract = {Nowadays, modern engineering systems require sophisticated maintenance strategies to ensure their correct performance. Maintenance has become one of the most important tasks of the systems lifecycle. This paper presents a literature review of the application of Reinforcement Learning algorithms for the maintenance of engineering systems. Reinforcement Learning-based maintenance has been classified regarding four types of system: transportation systems, manufacturing and production systems, civil infrastructures, power and energy systems, and other systems. Based on the literature review, this paper includes an overall analysis of the current state and a discussion of main limitations, challenges, and future trends in this field. A summary table is provided to present clearly the most important references. This research work demonstrates that Reinforcement Learning algorithms have a great potential for generating maintenance policies, outperforming most conventional strategies.}
}
@article{LENG2023139249,
title = {Dual deep reinforcement learning agents-based integrated order acceptance and scheduling of mass individualized prototyping},
journal = {Journal of Cleaner Production},
volume = {427},
pages = {139249},
year = {2023},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2023.139249},
url = {https://www.sciencedirect.com/science/article/pii/S0959652623034078},
author = {Jiewu Leng and Jiwei Guo and Hu Zhang and Kailin Xu and Yan Qiao and Pai Zheng and Weiming Shen},
keywords = {Deep reinforcement learning, Virtual production scheduling, Order acceptance decision, Mass individualized prototyping, Dual deep reinforcement learning agents},
abstract = {Coordinating order acceptance decisions with production scheduling to maximize revenue is challenging for Mass Individualized Prototyping (MIP) service providers. This paper presents a dual deep reinforcement learning agents-based (DDRLA) integrated order acceptance and scheduling (IOAS) for improving revenue. Firstly, a deep reinforcement learning-based virtual production scheduling (VPS) agent together with 8 state features and 11 action rules is designed. The VPS agent quickly and virtually reschedules a dynamically-arriving accepted order to evaluate the overall impact of accepting this order, including consumed capacity and increased revenue. Then, a deep reinforcement learning-based order acceptance decision (OAD) agent is designed. Based on the information guidance resulting from an interaction with the VPS agent, the OAD agent selectively accepts orders to maximize long-term gains, as well as to improve system resilience in the presence of a high ratio of urgent orders. The experiment results show that the proposed DDRLA method has better performance, compared with other IOAS approaches.}
}
@article{YIN2022115986,
title = {Deep reinforcement learning based coastal seawater desalination via a pitching paddle wave energy converter},
journal = {Desalination},
volume = {543},
pages = {115986},
year = {2022},
issn = {0011-9164},
doi = {https://doi.org/10.1016/j.desal.2022.115986},
url = {https://www.sciencedirect.com/science/article/pii/S0011916422004416},
author = {Xiuxing Yin and Meizhen Lei},
keywords = {Wave energy converter, Pitching paddle, Hydraulic transmission, Coastal seawater desalination, Deep reinforcement learning control},
abstract = {Wave energy has the potential to become a significant contributor to the future seawater desalination thanks to its high-power density. However, the intermittent characteristics of wave energy are the main issues to be overcome in the seawater desalination process. In this paper, a spiral wave energy converter (WEC) powered desalination system is proposed by utilizing a bottom flanged pitching spiral pump based WEC to directly convert the pressurized seawater into freshwater. The dynamic model of the spiral WEC powered desalination system is established. In order to mitigate the uncertainties and fluctuations in the fresh water supply, a deep reinforcement learning (DRL) control of the WEC is also designed for smoothing freshwater production, and a prioritized experience replay mechanism is employed for improving training efficiency. The test results indicate that the freshwater flowrate can be maintained at the desired value of 1200 m3/day by using the trained DRL control and the DRL training process converges to the stable value of around â18.}
}
@article{ZHENG2023119684,
title = {Safe reinforcement learning for industrial optimal control: A case study from metallurgical industry},
journal = {Information Sciences},
volume = {649},
pages = {119684},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119684},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523012690},
author = {Jun Zheng and Runda Jia and Shaoning Liu and Dakuo He and Kang Li and Fuli Wang},
keywords = {Augmented Lagrangian, Control barrier function, Gold cyanide leaching process, Industrial optimal control, Safe reinforcement learning},
abstract = {Gold cyanide leaching is a critical step in the extraction of gold from ore. The desire for a higher leaching rate often leads to increased cyanide concentrations, which pose safety risks and raise the cost of waste treatment. To address this problem, this study introduces a novel safe reinforcement learning algorithm that satisfies joint chance constraints with a high probability for multi-constraint gold cyanide leaching processes. In particular, the proposed algorithm employs chance control barrier functions to maintain the state within the desired safe set with high probability and transforms the joint chance constraint into a cumulative cost form using a constraint relaxation method. This relaxation method guarantees the satisfaction of safety requirements within a specified time horizon. A surrogate objective function optimized by stochastic gradient ascent is derived to ensure monotonic improvement of the policy in the trust region. The augmented Lagrangian-based constrained policy optimization is utilized, converting the constrained optimization problem into an unconstrained saddle-point optimization problem and avoiding the periodic performance oscillations common in the general Lagrangian method. Case studies demonstrate that the proposed algorithm outperforms baseline algorithms in terms of policy improvement, and constraint satisfaction and operates safely in multi-constraint scenarios.}
}
@article{WANG2021107969,
title = {Dynamic job-shop scheduling in smart manufacturing using deep reinforcement learning},
journal = {Computer Networks},
volume = {190},
pages = {107969},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.107969},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621001031},
author = {Libing Wang and Xin Hu and Yin Wang and Sujie Xu and Shijun Ma and Kexin Yang and Zhijun Liu and Weidong Wang},
keywords = {Smart manufacturing, Job-shop scheduling, Deep reinforcement learning, Proximal policy optimization},
abstract = {Job-shop scheduling problem (JSP) is used to determine the processing order of the jobs and is a typical scheduling problem in smart manufacturing. Considering the dynamics and the uncertainties such as machine breakdown and job rework of the job-shop environment, it is essential to flexibly adjust the scheduling strategy according to the current state. Traditional methods can only obtain the optimal solution at the current time and need to rework if the state changes, which leads to high time complexity. To address the issue, this paper proposes a dynamic scheduling method based on deep reinforcement learning (DRL). In the proposed method, we adopt the proximal policy optimization (PPO) to find the optimal policy of the scheduling to deal with the dimension disaster of the state and action space caused by the increase of the problem scale. Compared with the traditional scheduling methods, the experimental results show that the proposed method can not only obtain comparative results but also can realize adaptive and real-time production scheduling.}
}
@article{POPPER202263,
title = {Using Multi-Agent Deep Reinforcement Learning For Flexible Job Shop Scheduling Problems},
journal = {Procedia CIRP},
volume = {112},
pages = {63-67},
year = {2022},
note = {15th CIRP Conference on Intelligent Computation in ManufacturingEngineering, 14-16 July 2021},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.09.039},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122011933},
author = {Jens Popper and Martin Ruskowski},
keywords = {Deep Learning, Reinforcement Learning, Flexible Job Shop Scheduling, Multi-Agent, Production Planning},
abstract = {The flexibilization and increase of new production concepts such as matrix manufacturing with the help of autonomous logistics robots (AGVs) pose new challenges to production scheduling. To solve these flexible job shop scheduling problems (FJSSP) for arbitrary production arrangements, a concept for a multi-agent system based on Deep Reinforcement Learning (MARL) is proposed. The focus is on speed and quality of scheduling, easy creation of new manufacturing setups and extensibility to other scheduling problems such as logistics. An algorithm to solve these problems is given and evaluated on an exemplary job shop. Future research questions and extensions are then discussed.}
}
@article{MANUELDAVILADELGADO2022101787,
title = {Robotics in construction: A critical review of the reinforcement learning and imitation learning paradigms},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101787},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101787},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002452},
author = {Juan {Manuel Davila Delgado} and Lukumon Oyedele},
keywords = {Robotics, Construction, Reinforcement learning, Imitation learning, Deep reinforcement learning},
abstract = {The reinforcement and imitation learning paradigms have the potential to revolutionise robotics. Many successful developments have been reported in literature; however, these approaches have not been explored widely in robotics for construction. The objective of this paper is to consolidate, structure, and summarise research knowledge at the intersection of robotics, reinforcement learning, and construction. A two-strand approach to literature review was employed. A bottom-up approach to analyse in detail a selected number of relevant publications, and a top-down approach in which a large number of papers were analysed to identify common relevant themes and research trends. This study found that research on robotics for construction has not increased significantly since the 1980s, in terms of number of publications. Also, robotics for construction lacks the development of dedicated systems, which limits their effectiveness. Moreover, unlike manufacturing, construction's unstructured and dynamic characteristics are a major challenge for reinforcement and imitation learning approaches. This paper provides a very useful starting point to understating research on robotics for construction by (i) identifying the strengths and limitations of the reinforcement and imitation learning approaches, and (ii) by contextualising the construction robotics problem; both of which will aid to kick-start research on the subject or boost existing research efforts.}
}
@article{HE2021103373,
title = {A deep reinforcement learning based multi-criteria decision support system for optimizing textile chemical process},
journal = {Computers in Industry},
volume = {125},
pages = {103373},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103373},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520306072},
author = {Zhenglei He and Kim-Phuc Tran and Sebastien Thomassey and Xianyi Zeng and Jie Xu and Changhai Yi},
keywords = {Deep reinforcement learning, Deep Q-Networks, Multi-criteria, Decision support, Process, Textile manufacturing},
abstract = {Textile manufacturing is a typical traditional industry involving high complexity in interconnected processes with limited capacity on the application of modern technologies. Decision-making in this domain generally takes multiple criteria into consideration, which usually arouses more complexity. To address this issue, the present paper proposes a decision support system that combines the intelligent data-based models of random forest (RF) and a human knowledge-based multi-criteria structure of analytical hierarchical process (AHP) in accordance with the objective and the subjective factors of the textile manufacturing process. More importantly, the textile chemical manufacturing process is described as the Markov decision process (MDP) paradigm, and a deep reinforcement learning scheme, the Deep Q-networks (DQN), is employed to optimize it. The effectiveness of this system has been validated in a case study of optimizing a textile ozonation process, showing that it can better master the challenging decision-making tasks in textile chemical manufacturing processes.}
}
@article{SUGISAWA2022477,
title = {Machining sequence learning via inverse reinforcement learning},
journal = {Precision Engineering},
volume = {73},
pages = {477-487},
year = {2022},
issn = {0141-6359},
doi = {https://doi.org/10.1016/j.precisioneng.2021.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S0141635921002440},
author = {Yasutomo Sugisawa and Keigo Takasugi and Naoki Asakawa},
keywords = {Machining sequence decision, Inverse reinforcement learning, Neural network, Graph representation},
abstract = {In recent years, process planning automation has been strongly promoted in conjunction with the development of information technology (IT). In manufacturing industries, machining sequencing is one of the elements of computer-aided process planning (CAPP) systems where it has a significant impact on the quality and cost of machined components. Therefore, effective and robust planning rules are essential for practical CAPP systems, and various metrics and constraints have been proposed to facilitate the creation of those rules. However, since it is challenging to address explicit factors such as interference between rules, processing difficulties, and manageability, discrepancies that require manual corrections often arise between the generated sequences and the planner's intentions. To resolve this problem, we propose a method of acquiring rules that reproduce the planner's decisions by inverse reinforcement learning (IRL). To apply the IRL process, we focus on identifying a machining sequence that characterizes the planner's decision based on past production processes and interviews with experts. This machining sequence can then be represented using a Markov decision process (MDP) when changing the workpiece shape, which enables the application of IRL. Additionally, to reflect the drawing information in the sequence decision, the workpiece shape is represented as a graph with attached tolerance and roughness values. The graphed machining sequence is then inputted to the graph, where convolutional networking and training are performed. We verified the validity of our proposed method using a small dataset.}
}
@article{CSAJI2006279,
title = {Reinforcement learning in a distributed market-based production control system},
journal = {Advanced Engineering Informatics},
volume = {20},
number = {3},
pages = {279-288},
year = {2006},
note = {Design of Complex Adaptive Systems},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2006.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034606000024},
author = {BalÃ¡zs CsanÃ¡d CsÃ¡ji and LÃ¡szlÃ³ Monostori and Botond KÃ¡dÃ¡r},
keywords = {Dynamic scheduling, Multi-agent systems, Reinforcement learning},
abstract = {The paper presents an adaptive iterative distributed scheduling algorithm that operates in a market-based production control system. The manufacturing system is agentified, thus, every machine and job is associated with its own software agent. Each agent learns how to select presumably good schedules, by this way the size of the search space can be reduced. In order to get adaptive behavior and search space reduction, a triple-level learning mechanism is proposed. The top level of learning incorporates a simulated annealing algorithm, the middle (and the most important) level contains a reinforcement learning system, while the bottom level is done by a numerical function approximator, such as an artificial neural network. The paper suggests a cooperation technique for the agents, as well. It also analyzes the time and space complexity of the solution and presents some experimental results.}
}
@article{ALVESGOULART2020106909,
title = {Autonomous pH control by reinforcement learning for electroplating industry wastewater},
journal = {Computers & Chemical Engineering},
volume = {140},
pages = {106909},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106909},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419309573},
author = {Douglas {Alves Goulart} and Renato {Dutra Pereira}},
keywords = {Machine learning, Actor-Critic, Metaheuristic optimization, Hyperparameters, Cloud computing},
abstract = {The electroplating industry, due to steps such as pickling, generates acid pH wastewater. Its treatment is important for environmental preservation and the future recovery of metals. Therefore, the main objective of this work was the development of an autonomous pH controller for electroplating industry liquid effluents, based on fully automated Reinforcement Learning (RL). In order to do that, a Continuous Stirred-Tank Reactor (CSTR) neutralization simulator, and an adapted Particle Swarm Optimization (PSO) algorithm to automate the choice of RL hyperparameters were developed. The controller was developed and validated when it stabilized the effluent's pH in a neutral range in different scenarios during the regulatory and servo operations better than a Proportional Integral Derivative (PID) controller. The development of autonomous wastewater pH control systems in coated surface treatment units is a significant advancement, as it reduces human intervention and allows the monitoring of variability associated with the electroplating industry.}
}
@article{POPPER20211523,
title = {Simultaneous Production and AGV Scheduling using Multi-Agent Deep Reinforcement Learning},
journal = {Procedia CIRP},
volume = {104},
pages = {1523-1528},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.257},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121011550},
author = {Jens Popper and Vassilios Yfantis and Martin Ruskowski},
keywords = {Flexible Job Shop Scheduling, Deep Reinforcement Learning, Multi Agent System, Flexible Manufacturing System},
abstract = {Increasing demand for customized products in the wake of the 4th Industrial Revolution is placing ever increasing demands on the flexibility of manufacturing systems. Furthermore, the increasing usage of automated guided vehicles (AGV) adds another layer of flexibility and also complexity to the overall production system. The resulting Flexible Job Shop Scheduling Problem (FJSSP), including the coordination of the AGVs, is NP-hard and therefore hard to optimize. To address this problem, a Reinforcement Learning Multi Agent (MARL) system is proposed, in which job scheduling and vehicle planning is done cooperatively. This concept is described and prototypically implemented.}
}
@article{KARDOS2021104,
title = {Dynamic scheduling in a job-shop production system with reinforcement learning},
journal = {Procedia CIRP},
volume = {97},
pages = {104-109},
year = {2021},
note = {8th CIRP Conference of Assembly Technology and Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.210},
url = {https://www.sciencedirect.com/science/article/pii/S221282712031430X},
author = {Csaba Kardos and Catherine Laflamme and Viola Gallina and Wilfried Sihn},
keywords = {Dynamic scheduling, Reinforcement learning, Simulation, Smart factory},
abstract = {Fluctuating customer demands, expected short delivery times and the need for quick order confirmation creates a fast-paced scheduling environment for modern production systems. In this turbulent scene, using the data provided by intelligent elements of cyber-physical production systems opens up new possibilities for dynamic scheduling. The paper introduces a reinforcement learning approach, in particular Q-Learning, to reduce the average lead-time of production orders in a job-shop production system. The intelligent product agents are able to choose a machine for every production step based on real-time information. A performance comparison against standard dispatching rules is given, which shows that in the presented dynamic scheduling use-cases the application of RL reduces the average lead-time.}
}
@article{CAI2024102628,
title = {Deep reinforcement learning for solving resource constrained project scheduling problems with resource disruptions},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {85},
pages = {102628},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102628},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523001035},
author = {Hongxia Cai and Yunqi Bian and Lilan Liu},
keywords = {Graph neural network, Reinforcement learning, Resource constrained project scheduling problem with resource disruptions},
abstract = {The resource-constrained project scheduling problem (RCPSP) is encountered in many fields, including manufacturing, supply chain, and construction. Nowadays, with the rapidly changing external environment and the emergence of new models such as smart manufacturing, it is more and more necessary to study RCPSP considering resource disruptions. A framework based on reinforcement learning (RL) and graph neural network (GNN) is proposed to solve RCPSP and further solve the RCPSP with resource disruptions (RCPSP-RD) on this basis. The scheduling process is formulated as sequential decision-making problems. Based on that, Markov decision process (MDP) models are developed for RL to learn scheduling policies. A GNN-based structure is proposed to extract features from problems and map them to action probability distributions by policy network. To optimize the scheduling policy, proximal policy optimization (PPO) is applied to train the model end-to-end. Computational results on benchmark instances show that the RL-GNN algorithm achieves competitive performance compared with some widely used methods.}
}
@article{KIM2022130419,
title = {Reinforcement learning approach to scheduling of precast concrete production},
journal = {Journal of Cleaner Production},
volume = {336},
pages = {130419},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.130419},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622000658},
author = {Taehoon Kim and Yong-Woo Kim and Dongmin Lee and Minju Kim},
keywords = {Precast concrete, Reinforcement learning, Deep Q-network, Production scheduling},
abstract = {The production scheduling of precast concrete (PC) is essential for successfully completing PC construction projects. The dispatching rules, widely used in practice, have the limitation that the best rule differs according to the shop conditions. In addition, mathematical programming and the metaheuristic approach, which would improve performance, entail more computational time with increasing problem size, let alone its models being revised as the problem size changes. This study proposes a PC production scheduling model based on a reinforcement learning approach, which has the advantages of a general capacity to solve various problem conditions with fast computation time and good performance in real-time. The experimental study shows that the proposed model outperformed other methods by 4â12% of the total tardiness and showed an average winning rate of 77.0%. The proposed model could contribute to the successful completion of off-site construction projects by supporting the stable progress of PC construction.}
}
@article{LIU2023106294,
title = {A deep multi-agent reinforcement learning approach to solve dynamic job shop scheduling problem},
journal = {Computers & Operations Research},
volume = {159},
pages = {106294},
year = {2023},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2023.106294},
url = {https://www.sciencedirect.com/science/article/pii/S0305054823001582},
author = {Renke Liu and Rajesh Piplani and Carlos Toro},
keywords = {Job shop scheduling, Dynamic scheduling, Deep reinforcement learning, Multi-agent reinforcement learning},
abstract = {Manufacturing industry is experiencing a revolution in the creation and utilization of data, the abundance of industrial data creates a need for data-driven techniques to implement real-time production scheduling. However, existing dynamic scheduling techniques have been mainly developed to solve problems of invariable size, and are incapable of addressing the increasing volatility and complexity of practical production scheduling problems. To facilitate near real-time decision-making on the shop floor, we propose a deep multi-agent reinforcement learning-based approach to solve the dynamic job shop scheduling problem. Double deep Q-network algorithm, attached to decentralized scheduling agents, is used to learn the relationships between production information and scheduling objectives, and to make near real-time scheduling decisions. Proposed framework utilizes centralized training and decentralized execution scheme and parameter-sharing technique to tackle the non-stationary problem in the multi-agent reinforcement learning task. Several enhancements are also developed, including the novel state and action representation that can handle size-agnostic dynamic scheduling problems, a chronological joint-action framework to alleviate the credit-assignment difficulty, and knowledge-based reward-shaping techniques to encourage cooperation. Simulation study shows that the proposed architecture significantly improves the learning effectiveness, and delivers superior performance compared to existing scheduling strategies and state-of-the-art deep reinforcement learning-based dynamic scheduling approaches.}
}
@article{STRICKER2018511,
title = {Reinforcement learning for adaptive order dispatching in the semiconductor industry},
journal = {CIRP Annals},
volume = {67},
number = {1},
pages = {511-514},
year = {2018},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2018.04.041},
url = {https://www.sciencedirect.com/science/article/pii/S0007850618300659},
author = {Nicole Stricker and Andreas Kuhnle and Roland Sturm and Simon Friess},
keywords = {Production planning, Artificial intelligence, Semiconductor industry},
abstract = {The digitalization of production systems tends to provide a huge amount of data from heterogeneous sources. This is particularly true for the semiconductor industry wherein real time process monitoring is inherently required to achieve a high yield of good parts. An application of data-driven algorithms in production planning to enhance operational excellence for complex semiconductor production systems is currently missing. This paper shows the successful implementation of a reinforcement learning-based adaptive control system for order dispatching in the semiconductor industry. Furthermore, a performance comparison of the learning-based control system with the traditionally used rule-based system shows remarkable results. Since a strict rulebook does not bind the learning-based control system, a flexible adaption to changes in the environment can be achieved through a combination of online and offline learning.}
}
@article{UEDA2000343,
title = {Reinforcement Learning Approaches to Biological Manufacturing Systems},
journal = {CIRP Annals},
volume = {49},
number = {1},
pages = {343-346},
year = {2000},
issn = {0007-8506},
doi = {https://doi.org/10.1016/S0007-8506(07)62960-6},
url = {https://www.sciencedirect.com/science/article/pii/S0007850607629606},
author = {Kanji Ueda and Itsuo Hatono and Nobutada Fujii and Jari Vaario},
keywords = {Manufacturing Systems, Machine Learning, Uncertain Environment},
abstract = {The concept of Biological Manufacturing System (BMS) aims at dealing with unpredictable changes in the external and internal environments during whole product life cycle, based on biologically-inspired ideas such as self-organization, learning and evolution. We previously developed evolution-based and self-organization models of the floor level that are able to cope with environmental changes such as system reconfiguration, machine breakdown and unforeseen production requests. This paper describes reinforcement learning approaches to the modeling of BMS, in order to deal with the various kinds of complex global objectives. The effectiveness of the proposed model is also discussed by simulation with global objectives.}
}
@article{BURGGRAF202257,
title = {Multi-agent-based deep reinforcement learning for dynamic flexible job shop scheduling},
journal = {Procedia CIRP},
volume = {112},
pages = {57-62},
year = {2022},
note = {15th CIRP Conference on Intelligent Computation in ManufacturingEngineering, 14-16 July 2021},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.09.024},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122011660},
author = {Peter BurggrÃ¤f and Johannes Wagner and Till SaÃmannshausen and Dennis Ohrndorf and Karthik Subramani},
keywords = {production planning, production control, artificial intelligence, machine learning, cyber production management},
abstract = {In disruption-prone manufacturing environments, flexible job shop scheduling becomes a dynamic problem. For achieving a high solution quality, operations research approaches can be applied. In contrast, due to the required fast response times, dispatching rules are the standard. In order to elaborate on both, we present a new deep reinforcement learning algorithm. It combines policy gradient algorithms with actor-critic architectures and interprets the production system as a multi-agent system. Our evaluation on benchmark instances shows that the algorithm generates better schedules than dispatching rules with the same response time. It also generalizes well when tested on different manufacturing environments.}
}
@article{RUIZRODRIGUEZ2022102406,
title = {Multi-agent deep reinforcement learning based Predictive Maintenance on parallel machines},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {78},
pages = {102406},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102406},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000928},
author = {Marcelo Luis {Ruiz RodrÃ­guez} and Sylvain Kubler and Andrea {de Giorgio} and Maxime Cordy and JÃ©rÃ©my Robert and Yves {Le Traon}},
keywords = {Predictive Maintenance, Scheduling, Reinforcement learning, Multi-agent systems, Industry 4.0},
abstract = {In the context of IndustryÂ 4.0, companies understand the advantages of performing Predictive Maintenance (PdM). However, when moving towards PdM, several considerations must be carefully examined. First, they need to have a sufficient number of production machines and relative fault data to generate maintenance predictions. Second, they need to adopt the right maintenance approach, which, ideally, should self-adapt to the machinery, priorities of the organization, technician skills, but also to be able to deal with uncertainty. Reinforcement learning (RL) is envisioned as a key technique in this regard due to its inherent ability to learn by interacting through trials and errors, but very few RL-based maintenance frameworks have been proposed so far in the literature, or are limited in several respects. This paper proposes a new multi-agent approach that learns a maintenance policy performed by technicians, under the uncertainty of multiple machine failures. This approach comprises RL agents that partially observe the state of each machine to coordinate the decision-making in maintenance scheduling, resulting in the dynamic assignment of maintenance tasks to technicians (with different skills) over a set of machines. Experimental evaluation shows that our RL-based maintenance policy outperforms traditional maintenance policies (incl., corrective and preventive ones) in terms of failure prevention and downtime, improving by â75% the overall performance.}
}
@article{ZHAO2023120571,
title = {A knowledge-driven cooperative scatter search algorithm with reinforcement learning for the distributed blocking flow shop scheduling problem},
journal = {Expert Systems with Applications},
volume = {230},
pages = {120571},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120571},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423010734},
author = {Fuqing Zhao and Gang Zhou and Tianpeng Xu and Ningning Zhu and  Jonrinaldi},
keywords = {Distributed blocking flow shop scheduling, Scatter search algorithm, Knowledge-driven, Reinforcement learning},
abstract = {The distributed flow shop scheduling problem has become one of the key problems related to the high efficiency impacted factor in the manufacturing industry due to its typical scenarios in real-world industrial applications. In this paper, a knowledge-driven cooperative scatter search (KCSS) is proposed to address the distributed blocking flow shop scheduling problem (DBFSP) to minimize the makespan. The scatter search (SS) is adopted as the basic optimization framework in KCSS. The neighborhood perturbation operator and the Q-learning algorithm are combined to select the appropriate perturbation operator in the search process. Firstly, considering the complexity of distributed scenarios, five search operators are used to construct a disturbance strategy pool. Secondly, the Q-learning algorithm dynamically chooses disturbance strategies to enhance exploration ability and search efficiency. Afterward, a local search method based on neighborhood reconstruction is proposed to perturb the currently found optimal solution to strengthen the ability of KCSS to develop in local areas. In addition, the path relinking mechanism is introduced into the subset combination method to guarantee the diversity of solutions in the optimization process. Finally, the performance of the KCSS algorithm is verified on the benchmark set, and the experimental results demonstrate the robustness and effectiveness of the KCSS algorithm. In addition, 518 of the best-known solutions out of 720 benchmark instances are updated.}
}
@article{NI2024102641,
title = {A New Fourier Q Operator Network Based Reinforcement Learning Method for Continuous Action Space Decision-making in Manufacturing},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {86},
pages = {102641},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102641},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523001163},
author = {Yang Ni and Yingguang Li and Changqing Liu and Yan Jin},
keywords = {Fourier Q operator network, reinforcement learning, decision-making, continuous action space, aero-engine casing, machining deformation control},
abstract = {The problems of continuous action space decision-making are widespread in industrial manufacturing. However, when dealing with these problems, existing reinforcement learning (RL) methods relies on a large number of training samples, which is always unacceptable given the limited availability or expensive nature of data, such as low-volume manufacturing. This paper proposes a new Fourier Q operator network (FQON) based RL method. The input of FQON is the expected state function and its output the Q-value function, and both functions take the action in RL as independent variables. The infinite-dimensional mapping between the function domains is established by a set of parameters that can be used with different discretization, which fixes the mapping complexity regardless of the action space resolution. By taking the advantages of the fast calculation using on Fourier kernel operator, the mapping complexity is highly reduced, and it enables that FQON can realize the decision-making in continuous action space using a small amount of training samples. Taking machining deformation control of an aero-engine casing as a case study, experimental results showed that FQON based RL method can control the deformation well with limited training samples.}
}
@article{VALET2022518,
title = {Opportunistic maintenance scheduling with deep reinforcement learning},
journal = {Journal of Manufacturing Systems},
volume = {64},
pages = {518-534},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001285},
author = {Alexander Valet and Thomas AltenmÃ¼ller and Bernd Waschneck and Marvin Carl May and Andreas Kuhnle and Gisela Lanza},
keywords = {Maintenance scheduling, Reinforcement learning, Semiconductor manufacturing, Opportunistic maintenance},
abstract = {The great complexity of advanced manufacturing processes combined with the high investment costs for manufacturing equipment makes the integration of maintenance scheduling a challenging, but similarly crucial task. Opportunistic maintenance scheduling holds the potential to increase the operational performance by considering the opportunity cost of maintenance measures. At the same time, reinforcement learning (RL) has proved to be able to handle complex scheduling tasks. Therefore, RL constitutes a promising approach to develop an integrated maintenance scheduling model to consider order dispatching and maintenance scheduling in a single decision support system. This paper models a real-world use case of a semiconductor front-end wafer fabrication by using discrete-event simulation. In the simulated scenarios, the performance of the integrated dispatching and maintenance scheduling is regulated by both complex novel heuristics adapted to opportunistic maintenance and reinforcement learning. The results show that the RL policy is able to learn a competitive joint scheduling strategy by including internal and external opportunistic opportunities. This indicates that opportunistic maintenance scheduling, with and without RL, holds the potential to improve the performance not only of semiconductor manufacturing but capital-intense machinery industry alike.}
}
@article{BAE20233277,
title = {Deep reinforcement learning for a multi-objective operation in a nuclear power plant},
journal = {Nuclear Engineering and Technology},
volume = {55},
number = {9},
pages = {3277-3290},
year = {2023},
issn = {1738-5733},
doi = {https://doi.org/10.1016/j.net.2023.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S1738573323002723},
author = {Junyong Bae and Jae Min Kim and Seung Jun Lee},
keywords = {Nuclear power plant, Automation, Deep reinforcement learning, Soft actor-critic, Hindsight experience replay},
abstract = {Nuclear power plant (NPP) operations with multiple objectives and devices are still performed manually by operators despite the potential for human error. These operations could be automated to reduce the burden on operators; however, classical approaches may not be suitable for these multi-objective tasks. An alternative approach is deep reinforcement learning (DRL), which has been successful in automating various complex tasks and has been applied in automation of certain operations in NPPs. But despite the recent progress, previous studies using DRL for NPP operations have limitations to handle complex multi-objective operations with multiple devices efficiently. This study proposes a novel DRL-based approach that addresses these limitations by employing a continuous action space and straightforward binary rewards supported by the adoption of a soft actor-critic and hindsight experience replay. The feasibility of the proposed approach was evaluated for controlling the pressure and volume of the reactor coolant while heating the coolant during NPP startup. The results show that the proposed approach can train the agent with a proper strategy for effectively achieving multiple objectives through the control of multiple devices. Moreover, hands-on testing results demonstrate that the trained agent is capable of handling untrained objectives, such as cooldown, with substantial success.}
}
@article{SUN2023108580,
title = {Blade sequencing optimization of aero-engine based on deep reinforcement learning},
journal = {Aerospace Science and Technology},
volume = {142},
pages = {108580},
year = {2023},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2023.108580},
url = {https://www.sciencedirect.com/science/article/pii/S1270963823004777},
author = {Chuanzhi Sun and Huilin Wu and Qing Lu and Yinchu Wang and Yongmeng Liu and Jiubin Tan},
keywords = {Reinforcement learning, Pointer network, Blade sorting, Unbalance},
abstract = {The unreasonable sorting of single-stage rotor blades leads to the over-tolerance of rotor unbalance, which is the main cause of excessive engine vibration. Aiming at the problems of long search time, poor repeatability, weak adaptability and difficulty in obtaining global optimum by using heuristic algorithm for blade sorting, this paper presents a deep reinforcement learning method is proposed to solve the blade ordering problem. A pointer network model including an encoder and a decoder structure is established. For the case where the blade data cannot be obtained, the unbalance of the single-stage rotor is used as the reward function, and the pointer network model is trained by the Actor Critic reinforcement learning algorithm. The experimental results show that the trained enhanced pointer network model can directly perform end-to-end reasoning on the input sequence, avoiding the iterative solution process of traditional heuristic algorithms, and has high solution efficiency. Using the enhanced pointer network blade sorting optimization model in this paper to sort a set of blade sequences, the unbalanced value of the rotor after sorting is 14.78 g.mm, which is 84.8% better than the genetic algorithm, and the search speed is increased by 95.9%. The results show that the method can quickly and accurately give the arrangement order of the leaves, and the proposed model has generalization. It can provide a reliable measurement method for rotor assembly measurement of large engine manufacturing enterprises such as China Aero-Engine Company.}
}
@article{DHARMADHIKARI2023103556,
title = {A reinforcement learning approach for process parameter optimization in additive manufacturing},
journal = {Additive Manufacturing},
volume = {71},
pages = {103556},
year = {2023},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2023.103556},
url = {https://www.sciencedirect.com/science/article/pii/S2214860423001690},
author = {Susheel Dharmadhikari and Nandana Menon and Amrita Basak},
keywords = {Reinforcement learning, Q-learning, Additive manufacturing, Directed energy deposition, Process optimization},
abstract = {Process optimization for metal additive manufacturing (AM) is crucial to ensure repeatability, control microstructure, and minimize defects. Despite efforts to address this via the traditional design of experiments and statistical process mapping, there is limited insight on an on-the-fly optimization framework that can be integrated into a metal AM system. Additionally, most of these methods, being data-intensive, cannot be supported by a metal AM alloy or system due to budget restrictions. To tackle this issue, the article introduces a Reinforcement Learning (RL) methodology transformed into an optimization problem in the realm of metal AM. An off-policy RL framework based on Q-learning is proposed to find optimal laser power (P)- scan velocity (v) combinations with the objective of maintaining steady-state melt pool depth. For this, an experimentally validated EagarâTsai formulation is used as a digital twin emulating the laser-directed energy deposition (L-DED) environment, where the laser operates as the agent across the Pâv space such that it maximizes rewards for a melt pool depth closer to the optimum. The culmination of the training process yields a Q-table where the state (P,v) with the highest Q-value corresponds to the optimized process parameters. For a desired melt pool depth of 1 mm for SS316L, the proposed algorithm predicts an optimal Pâv combination of 888.9 W - 566.7 mm/min that yields a melt pool depth within 50 Î¼m of the experimental observation. The framework, therefore, provides a model-free approach to learning without any prior.}
}
@article{ZHANG2023110698,
title = {Vehicle dynamic dispatching using curriculum-driven reinforcement learning},
journal = {Mechanical Systems and Signal Processing},
volume = {204},
pages = {110698},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2023.110698},
url = {https://www.sciencedirect.com/science/article/pii/S0888327023006064},
author = {Xiaotong Zhang and Gang Xiong and Yunfeng Ai and Kunhua Liu and Long Chen},
keywords = {Reinforcement learning, Curriculum learning, Vehicle dispatching, Network optimization},
abstract = {This study focuses on optimizing resource allocation problems in complex dynamic environments, specifically vehicle dispatching in closed bipartite queuing networks. We present a novel curriculum-driven reinforcement learning (RL) approach that seamlessly incorporates domain knowledge and environmental feedback, effectively addressing the challenges associated with sparse reward scenarios in RL applications. This approach involves a scalable reinforcement learning framework for dynamic vehicle fleet size. We design dense artificial rewards using domain knowledge and incorporate artificial actionâreward pairs into the original experience sequence forming the basic structure of the training instances. A difficulty momentum boosting strategy is proposed to produce a series of training instances with progressively increasing difficulty, ensuring that the RL agent learns decision strategies in an organized and smooth manner. Experimental results demonstrate that the proposed method significantly surpasses existing approaches in enhancing productivity and model learning efficiency for transport tasks in open-pit mines, while confirming the superiority of a flexible and automated curriculum learning process over a rigid setting. This approach has vast potential for application in dynamic resource allocation problems across industries, such as manufacturing and logistics.}
}
@article{DUHEM2023103874,
title = {Parametrization of a demand-driven operating model using reinforcement learning},
journal = {Computers in Industry},
volume = {147},
pages = {103874},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103874},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523000246},
author = {Louis Duhem and Maha Benali and Guillaume Martin},
keywords = {Production management, Parametrization, DDMRP, Reinforcement learning, Demand-Driven Operating Model},
abstract = {Nowadays, production and supply planning are more complex than ever before with low customer tolerance, complicated bill-of-materials, and high product variety. Most of the time, conventional approaches such as MRP and Lean approaches are not efficient. To overcome these issues, Ptak and Smith (2019) introduced Demand Driven Material Requirements Planning (DDMRP). This methodology relies on a Demand-Driven Operating Model (DDOM) which uses actual demand in a combination of strategic buffers to protect critical parts. For the past decade, research around DDMRP has been focused on proving and advancing the methodology in different industrial environments, while neglecting its parametrization. Indeed, the authors suggested general rules to set the DDOMâs key parameters, yet no learning approach has been developed to set them. This present paper is the first that proposes to use machine learning to parametrize a DDOM facing unknown demand, and particularly to adjust dynamically the order spike threshold and the order spike horizon. A reinforcement learning algorithm with three different reward functions is coupled to a DDMRP flowshop simulation model facing an atypical demand including spikes. Besides studying the learning ability of the algorithm, we evaluate the performance of the model which is compared to a DDOM without parameter adjustment. The analysis shows that it is possible to drive the order spike thresholds to increase the performance of the production system, regarding customer satisfaction and stock level optimization. The findings of this paper point out the possibility to drive DDOM parameters with an automatic method using reinforcement learning.}
}
@article{TANG2023102197,
title = {A constrained multi-objective deep reinforcement learning approach for temperature field optimization of zinc oxide rotary volatile kiln},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102197},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102197},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003257},
author = {Fengrun Tang and Zhenxiang Feng and Yonggang Li and Chunhua Yang and Bei Sun},
keywords = {Multi-objective, Deep reinforcement learning, Temperature field optimization, Controllability constraint, Chebyshev scalarization, Zinc oxide rotary volatile kiln},
abstract = {In the zinc oxide rotary volatile kiln (ZORVK), an optimal temperature field is essential to balance the strong conflict between zinc recovery rate and carbon emissions. However, the complex and diverse temperature distribution modes make it challenging to quickly obtain optimization results under intricate controllability constraints and multi-conflict production objectives. In this study, a novel constrained multi-objective deep reinforcement learning (CMODRL) approach for temperature field optimization of the ZORVK is proposed. First, an evaluation metric called the uncontrollable factor is designed to quantify the controllability of the temperature field. Then, a dynamic penalty method in deep reinforcement learning (DRL) is proposed to handle the controllability constraint, in which the penalty coefficient is dynamically adjusted according to the training loss. After that, the Chebyshev scalarization function is introduced as an action selection mechanism in DRL. Finally, the CMODRL is developed by integrating the dynamic penalty and Chebyshev scalarization function into the multi-objective deep reinforcement learning (MODRL) framework. As a result, for any given preference between the two production objectives, the proposed method can rapidly get the Pareto-optimal solution fulfilling the constraint. Moreover, the optimization efficiency of the MODRL-based algorithm is forty times higher than that of the multi-objective genetic algorithm, which serves better for practical optimization problems.}
}
@article{LU2023102488,
title = {Energy-efficient multi-pass cutting parameters optimisation for aviation parts in flank milling with deep reinforcement learning},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {81},
pages = {102488},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102488},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001703},
author = {Fengyi Lu and Guanghui Zhou and Chao Zhang and Yang Liu and Fengtian Chang and Zhongdong Xiao},
keywords = {Energy efficiency, Parametric optimisation, Workpiece deformation, Deep reinforcement learning, Sustainable manufacturing},
abstract = {Cutting parameters play a major role in improving the energy efficiency of the manufacturing industry. As the main processing method for aviation parts, flank milling usually adopts multi-pass constant and conservative cutting parameters to prevent workpiece deformation but degrades energy efficiency. To address the issue, this paper proposes a novel multi-pass parametric optimisation based on deep reinforcement learning (DRL), allowing parameters to vary to boost energy efficiency under the changing deformation limits in each pass. Firstly, it designs a variable workpiece deformation const.raint on the principle of stiffness decreasing along the passes, based on which it constructs an energy-efficient parametric optimisation model, giving suitable decisions that respond to the varying cutting conditions. Secondly, it transforms the model into a Markov Decision Process and Soft Actor Critic is applied as the DRL agent to cope with the dynamics in multi-pass machining. Among them, an artificial neural network-enabled surrogate model is applied to approximate the real-world machining, facilitating enough explorations of DRL. Experimental results show that, compared with the conventional method, the proposed method improves 45.71% of material removal rate and 32.27% of specific cutting energy while meeting deformation tolerance, which substantiates the benefits of the energy-efficient parametric optimisation, significantly contributing to sustainable manufacturing.}
}
@article{SCHNECKENREITHER2022108765,
title = {Average reward adjusted deep reinforcement learning for order release planning in manufacturing},
journal = {Knowledge-Based Systems},
volume = {247},
pages = {108765},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108765},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122003598},
author = {Manuel Schneckenreither and Stefan Haeussler and Juanjo PeirÃ³},
keywords = {Operations research, Production planning, Order release, Machine learning, Reinforcement learning},
abstract = {One of the key challenges in production planning, especially in discrete manufacturing, is to determine when to release which orders to the shop floor. The major aim of this planning task is to balance Work-In-Process (WIP) and utilisation levels together with timely completion of orders. The two most crucial attributes of production planning are (i) the highly nonlinear relationship between WIP, flow times and output, and (ii) the dynamically changing environment. Nonetheless, most state-of-the-art models use static lead times to address this problem. Only recently, some papers set lead times dynamically based on the flow time forecasts to react to the dynamic operational characteristics reporting promising results. This paper contributes to this line of research by presenting an order release model that uses reinforcement learning (RL) to set lead times dynamically over time. The applied RL agent is especially designed for processes with periodic feedback and highly variable context. We compare the performance of our new RL algorithm to static order release models and state-of-the-art deep Q-Learning agents by using a multi-stage, multi-product flow-shop simulation model. The results show that, especially for scenarios with high utilisation, our proposed method outperforms the other approaches.}
}
@article{LI2023424,
title = {Multi-agent evolution reinforcement learning method for machining parameters optimization based on bootstrap aggregating graph attention network simulated environment},
journal = {Journal of Manufacturing Systems},
volume = {67},
pages = {424-438},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523000390},
author = {Weiye Li and Songping He and Xinyong Mao and Bin Li and Chaochao Qiu and Jinwen Yu and Fangyu Peng and Xin Tan},
keywords = {Surface roughness, Cutting efficiency, Machining parameters optimization, Graph attention network, Multi-agent reinforcement learning, Evolutionary learning},
abstract = {Improving machining quality and production efficiency is the focus of the manufacturing industry. How to obtain efficient machining parameters under multiple constraints such as machining quality is a severe challenge for manufacturing industry. In this paper, a multi-agent evolutionary reinforcement learning method (MAERL) is proposed to optimize the machining parameters for high quality and high efficiency machining by combining the graph neural network and reinforcement learning. Firstly, a bootstrap aggregating graph attention network (Bagging-GAT) based roughness estimation method for machined surface is proposed, which combines the structural knowledge between machining parameters and vibration features. Secondly, a mathematical model of machining parameters optimization problem is established, which is formalized into Markov decision process (MDP), and a multi-agent reinforcement learning method is proposed to solve the MDP problem, and evolutionary learning is introduced to improve the stability of multi-agent training. Finally, a series of experiments were carried out on the commutator production line, and the results show that the proposed Bagging-GAT-based method can improve the prediction effect by about 25% in the case of small samples, and the MAERL-based optimization method can better deal with the coupling problem of reward function in the optimization process. Compared with the classical optimization method, the optimization effect is improved by 13% and a lot of optimization time is saved.}
}
@article{PAPADOPOULOS2024121234,
title = {Deep reinforcement learning in service of air traffic controllers to resolve tactical conflicts},
journal = {Expert Systems with Applications},
volume = {236},
pages = {121234},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121234},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423017360},
author = {George Papadopoulos and Alevizos Bastas and George A. Vouros and Ian Crook and Natalia Andrienko and Gennady Andrienko and Jose Manuel Cordero},
keywords = {Air traffic control, Conflict detection and resolution, Graph convolutional reinforcement learning, Transparency},
abstract = {Dense and complex air traffic requires higher levels of automation than those exhibited by tactical conflict detection and resolution (CD&R) tools that air traffic controllers (ATCOs) use today: AI tools can act on their own initiative, increasing the capacity of ATCOs to control higher volumes of traffic. However, given that the air traffic control (ATC) domain is safety critical, requires AI systems to which ATCOs are comfortable to relinquishing control, guaranteeing operational integrity and automation adoption. Two major factors towards this goal are quality of solutions and operational transparency. ResoLver, the system that this article presents, addresses these challenges using an enhanced graph convolutional reinforcement learning method operating in a multiagent setting where each agent â representing a flight â performs a CD&R task, jointly with other agents. We show that ResoLver can provide high-quality solutions with respect to stakeholders interests (air traffic controllers and airspace users), addressing also operational transparency issues, which have been validated by ATCOs in simulated real-world settings.}
}
@article{NASEREDDIN2019406,
title = {Hybrid Robotic Reinforcement Learning for Inspection/Correction Tasks},
journal = {Procedia Manufacturing},
volume = {39},
pages = {406-413},
year = {2019},
note = {25th International Conference on Production Research Manufacturing Innovation: Cyber Physical Manufacturing August 9-14, 2019 | Chicago, Illinois (USA)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.384},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920304558},
author = {Hoda Nasereddin and Gerald M. Knapp},
keywords = {Reinforcement Learning, Inspect, Correct, SARSA algorithm},
abstract = {The ability to rapidly program robots for complex tasks is an important precursor to wider adoption of robotics in industry. Robot programming is often time consuming and brittle to unanticipated variations in processing. Automated robot task learning is a solution to this problem. Reinforcement Learning (RL) is a commonly used approach for a robot to autonomously learn simple tasks. In RL, rewards are used to guide the robot towards learning an optimal plan or control policy. RL, however, has proven to be of limited value for problems with large-state spaces and considerable environmental variability. In this paper, we investigate formulation of the RL approach for inspect/correct types of tasks, specifically a misplaced block in a simple grid-world environment (requiring searching the gird world to identify a missing block and returning the missing block back to the target). We use a hybrid method, combining the SARSA algorithm and a model of the environment. The model of the environment is used as a reference model to reduce the state space, avoiding unnecessary exploration of the environment. A main focus of this research is the impact of task variability on RL performance.}
}
@article{LIU2024102638,
title = {An augmented reality-assisted interaction approach using deep reinforcement learning and cloud-edge orchestration for user-friendly robot teaching},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {85},
pages = {102638},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102638},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523001138},
author = {Changchun Liu and Dunbing Tang and Haihua Zhu and Qingwei Nie and Wei Chen and Zhen Zhao},
keywords = {User-friendly robot teaching, Augmented reality, Deep reinforcement learning, Cloud-edge orchestration, Robot motion planning},
abstract = {Industrial robots have emerged as pivotal components in the search for intelligent manufacturing equipment that can meet flexible and customized operational needs. Consequently, industrial robots have to frequently use motion planning schemes pre-programmed by operators. Furthermore, traditional robot teaching methods in the human-robot interaction scenario can only be applied in a fixed task environment and therefore lack generalization ability. To address these shortcomings, this research proposes an augmented reality-assisted interaction approach using deep reinforcement learning and cloud-edge orchestration for user-friendly robot teaching. Firstly, the proposed deep reinforcement learning algorithm with the position prediction function is applied for the robot motion planning, which can avoid unnecessary collision attempts during the training process. Subsequently, augmented reality glasses provide a user-friendly interaction interface, allowing both virtual and physical robots to be operated to eliminate the limitations of spatial and human factors. Apart from this, the robot target positions can be set by operators, and the visible trajectory of the calculated path can be integrated into the real scenario by virtue of AR glasses. On top of this, the cloud-edge orchestration links the communication between the industrial AR cloud platform and the edge nodes (e.g., robots and augmented reality glasses). Ultimately, comparative numerical experiments are conducted in an actual machining workshop, and the results indicate that the proposed robot teaching approach is both efficient and applicative by virtue of deep reinforcement learning and cloud-edge orchestration.}
}
@article{HAMEED202391,
title = {Graph neural networks-based scheduler for production planning problems using reinforcement learning},
journal = {Journal of Manufacturing Systems},
volume = {69},
pages = {91-102},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001097},
author = {Mohammed Sharafath Abdul Hameed and Andreas Schwung},
keywords = {Job shop scheduling, Reinforcement learning, Graph neural networks, Distributed optimization, Production planning, Tabu search, Genetic algorithm},
abstract = {Reinforcement learning (RL) is increasingly adopted in job shop scheduling problems (JSSP). But RL for JSSP is usually done using a vectorized representation of machine features as the state space. It has three major problems: (1) the relationship between the machine units and the job sequence is not fully captured, (2) exponential increase in the size of the state space with increasing machines/jobs, and (3) the generalization of the agent to unseen scenarios. This paper presents a novel framework named GraSP-RL, GRAph neural network-based Scheduler for Production planning problems using Reinforcement Learning. It represents JSSP as a graph and trains the RL agent using features extracted using a graph neural network (GNN). While the graph is itself in the non-Euclidean space, the features extracted using the GNNs provide a rich encoding of the current production state in the Euclidean space. At its core is a custom message-passing algorithm applied to the GNN. The node features encoded by the GNN are then used by the RL agent to select the next job. Further, we cast the scheduling problem as a decentralized optimization problem in which the learning agent is assigned to all the production units individually and the agent learns asynchronously from the experience collected on all the other production units. The GraSP-RL is then applied to a complex injection molding production environment with 30 jobs and 4 machines. The task is to minimize the makespan of the production plan. The schedule planned by GraSP-RL is then compared and analyzed with a priority dispatch rule algorithm like first-in-first-out (FIFO) and metaheuristics like tabu search (TS) and genetic algorithm (GA). The proposed GraSP-RL outperforms the FIFO, TS, and GA for the trained task of planning 30 jobs in JSSP. We further test the generalization capability of the trained agent on two different problem classes: Open shop system (OSS) and Reactive JSSP (RJSSP). In these modified problem classes our method produces results better than FIFO and comparable results to TS and GA, without any further training while also providing schedules instantly.}
}
@article{OBIRI202361,
title = {Optimizing the switching operation in monoclonal antibody production: Economic MPC and reinforcement learning},
journal = {Chemical Engineering Research and Design},
volume = {199},
pages = {61-73},
year = {2023},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2023.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S026387622300597X},
author = {Sandra A. Obiri and Song Bo and Bernard T. Agyeman and Sarupa Debnath and Benjamin Decardi-Nelson and Jinfeng Liu},
keywords = {EMPC, Monoclonal antibody, Rectified linear unit, Reinforcement learning, Sigmoid function, Nonlinear integer program, Artificial neural network, Control},
abstract = {Monoclonal antibodies (mAbs), crucial in medicine and biopharmaceuticals, require optimized large-scale production to meet high clinical dosage demands. Most of the processes for industrial mAb production rely on fed-batch operations, resulting in significant downtime. Transitioning to fully continuous and integrated processes holds the potential to boost product yield, enhance quality, and reduce storage costs for intermediate products. The integrated continuous mAb production process can be divided into the upstream and downstream processes. One crucial aspect that ensures the continuity of the integrated process is the switching of the capture columns, which are typically chromatography columns operated in a fed-batch manner downstream. Due to the discrete nature of the switching operation, advanced process control algorithms such as economic MPC (EMPC) are computationally difficult to implement. This is because an integer nonlinear program (INLP) needs to be solved online at each sampling time. This paper introduces two computationally-efficient approaches for EMPC implementation, namely, a sigmoid function approximation approach and a rectified linear unit (ReLU) approximation approach. It also explores the application of deep reinforcement learning (DRL). These three methods are compared to the traditional switching approach which is based on a 1 % product breakthrough rule and which involves no optimization.}
}
@article{ROMEROHDZ2020103612,
title = {Incorporating domain knowledge into reinforcement learning to expedite welding sequence optimization},
journal = {Engineering Applications of Artificial Intelligence},
volume = {91},
pages = {103612},
year = {2020},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.103612},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620300804},
author = {Jesus Romero-Hdz and Baidya Nath Saha and Seiichiro Tstutsumi and Riccardo Fincato},
keywords = {Welding sequence optimization, FEA based welding simulation, Reinforcement learning, Structural deformation, Residual stress, Artificial intelligence, Machine learning},
abstract = {Welding Sequence Optimization (WSO) is very effective to minimize the structural deformation, however selecting proper welding sequence leads to a combinatorial optimization problem. State-of-the-art algorithms could take more than one week to compute the best sequence for an assembly of eight weld beads which is unrealistic for the early stages of Product Delivery Process (PDP). In this article, we develop and implement a novel Reinforcement Q-learning algorithm for WSO where structural deformation is used to compute reward function. We utilize a thermo-mechanical Finite Element Analysis (FEA) to predict deformation. The explorationâexploitation dilemma has been tackled by domain knowledge driven Îµ-greedy algorithm into Q-RL which helps to expedite the WSO and we call this novel algorithm as DKQRL. We run welding simulation experiment using well-known SimufactÂ® software on a typical widely used mounting bracket which contains eight welding beads. DKQRL allows the reduction of structural deformation up to â¼71% and it substantially speeds up the computational time over Modified Lowest Cost Search (MLCS), Genetic Algorithm (GA), exhaustive search, and standard RL algorithm. Results of welding simulation demonstrate a reasonable agreement with real experiment in terms of structural deformation.}
}
@article{HILDEBRAND20201462,
title = {Deep Reinforcement Learning for Robot Batching Optimization and Flow Control},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1462-1468},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.203},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920320667},
author = {Max Hildebrand and Rasmus S. Andersen and Simon BÃ¸gh},
keywords = {Robot Batching, Artificial Intelligence in Smart Manufacturing, Proximal Policy Optimization, Deep Reinforcement Learning},
abstract = {Robot batching is an optimization problem found in many industrial applications. Current state-of-the-art approaches utilize a combination of heuristic based parameters and statistical analysis. This approach necessitates many tunable parameters, which again provides challenges when delivering systems to new customers. We challenge current state-of-the-art in statistical approaches by presenting a novel application of a policy gradient method for a Deep Reinforcement Learning (DRL/RL) agent. We have developed a Unity simulation framework of an existing robot-batching cell, on which a RL agent is able to successfully train and obtain a policy for performing robot batching, using a tabula rasa approach. The trained agent is capable of packaging 47.86% of 1218 total batches within the prescribed tolerances, with a positive give-away of 8.76%. The application of DRL in performing robot batching is to the authors knowledge the first of its kind.}
}
@article{LENG2020175,
title = {Deep reinforcement learning for a color-batching resequencing problem},
journal = {Journal of Manufacturing Systems},
volume = {56},
pages = {175-187},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520300911},
author = {Jinling Leng and Chun Jin and Alexander Vogl and Huiyu Liu},
keywords = {Deep reinforcement learning, Color-Batching problem, Virtual car resequencing, Production control, Automotive industry},
abstract = {In automotive paint shops, changes of colors between consecutive production orders cause costs for cleaning the painting robots. It is a significant task to re-sequence orders and group orders with identical color as a color batch to minimize the color changeover costs. In this paper, a Color-batching Resequencing Problem (CRP) with mix bank buffer systems is considered. We propose a Color-Histogram (CH) model to describe the CRP as a Markov decision process and a Deep Q-Network (DQN) algorithm to solve the CRP integrated with the virtual car resequencing technique. The CH model significantly reduces the number of possible actions of the DQN agent, so that the DQN algorithm can be applied to the CRP at a practical scale. A DQN agent is trained in a deep reinforcement learning environment to minimize the costs of color changeovers for the CRP. Two experiments with different assumptions on the order attribute distributions and cost metrics were conducted and evaluated. Experimental results show that the proposed approach outperformed conventional algorithms under both conditions. The proposed agent can run in real time on a regular personal computer with a GPU. Hence, the proposed approach can be readily applied in the production control of automotive paint shops to resolve order-resequencing problems.}
}
@article{BLAD20191308,
title = {Control of HVAC-systems with Slow Thermodynamic Using Reinforcement Learning},
journal = {Procedia Manufacturing},
volume = {38},
pages = {1308-1315},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.159},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920301608},
author = {C. Blad and S. Koch and S. Ganeswarathas and C.S. KallesÃ¸e and S. BÃ¸gh},
keywords = {Sustainable Manufacturing Engineering, Resource-Efficient Production, Artificial Intelligence in Manufacturing, Modelling, Simulation, HVAC-Systems},
abstract = {This paper proposes an adaptive controller based on Reinforcement Learning (RL), which copes with HVAC-systems consisting of slow thermodynamics. Two different RL algorithms with Q-Networks (QNs) are investigated. The HVAC-system is in this study an underfloor heating system. Underfloor heating is of great interest because it is very common in Scandinavia, but this research can be applied to a wide range of HVAC-systems, industrial processes and other control applications that are dominated by very slow dynamics. The environments consist of one, two, and four zones within a house in a simulation environment meaning that agents will be exposed to gradually more complex environments separated into test levels. The novelty of this paper is the incorporation of two different RL algorithms for industrial process control; a QN and a QN + Eligibility Trace (QN+ET). The reason for using eligibility trace is that an underfloor heating environment is dominated by slow dynamics and by using eligibility trace the agent can find correlations between the reward and actions taken in earlier iterations}
}
@article{KIM2020101863,
title = {Reinforcement learning based on movement primitives for contact tasks},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {62},
pages = {101863},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.101863},
url = {https://www.sciencedirect.com/science/article/pii/S0736584518306197},
author = {Young-Loul Kim and Kuk-Hyun Ahn and Jae-Bok Song},
keywords = {AI-based methods, Force control, Deep Learning in robotics and automation},
abstract = {Recently, robot learning through deep reinforcement learning has incorporated various robot tasks through deep neural networks, without using specific control or recognition algorithms. However, this learning method is difficult to apply to the contact tasks of a robot, due to the exertion of excessive force from the random search process of reinforcement learning. Therefore, when applying reinforcement learning to contact tasks, solving the contact problem using an existing force controller is necessary. A neural-network-based movement primitive (NNMP) that generates a continuous trajectory which can be transmitted to the force controller and learned through a deep deterministic policy gradient (DDPG) algorithm is proposed for this study. In addition, an imitation learning algorithm suitable for NNMP is proposed such that the trajectories similar to the demonstration trajectory are stably generated. The performance of the proposed algorithms was verified using a square peg-in-hole assembly task with a tolerance of 0.1â¯mm. The results confirm that the complicated assembly trajectory can be learned stably through NNMP by the proposed imitation learning algorithm, and that the assembly trajectory is improved by learning the proposed NNMP through the DDPG algorithm.}
}
@article{YANG2022101776,
title = {Real-time scheduling for distributed permutation flowshops with dynamic job arrivals using deep reinforcement learning},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101776},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101776},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002348},
author = {Shengluo Yang and Junyi Wang and Zhigang Xu},
keywords = {Distributed flowshop scheduling, Deep reinforcement learning, Real-time scheduling, Dynamic job arrivals, Intelligent scheduling, Deep Q-network},
abstract = {Distributed manufacturing plays an important role for large-scale companies to reduce production and transportation costs for globalized orders. However, how to real-timely and properly assign dynamic orders to distributed workshops is a challenging problem. To provide real-time and intelligent decision-making of scheduling for distributed flowshops, we studied the distributed permutation flowshop scheduling problem (DPFSP) with dynamic job arrivals using deep reinforcement learning (DRL). The objective is to minimize the total tardiness cost of all jobs. We provided the training and execution procedures of intelligent scheduling based on DRL for the dynamic DPFSP. In addition, we established a DRL-based scheduling model for distributed flowshops by designing suitable reward function, scheduling actions, and state features. A novel reward function is designed to directly relate to the objective. Various problem-specific dispatching rules are introduced to provide efficient actions for different production states. Furthermore, four efficient DRL algorithms, including deep Q-network (DQN), double DQN (DbDQN), dueling DQN (DlDQN), and advantage actor-critic (A2C), are adapted to train the scheduling agent. The training curves show that the agent learned to generate better solutions effectively and validate that the system design is reasonable. After training, all DRL algorithms outperform traditional meta-heuristics and well-known priority dispatching rules (PDRs) by a large margin in terms of solution quality and computation efficiency. This work shows the effectiveness of DRL for the real-time scheduling of dynamic DPFSP.}
}
@article{JORGE20181073,
title = {Reinforcement learning in real-time geometry assurance},
journal = {Procedia CIRP},
volume = {72},
pages = {1073-1078},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.168},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118303263},
author = {Emilio Jorge and Lucas Brynte and Constantin Cronrath and Oskar WigstrÃ¶m and Kristofer Bengtsson and Emil Gustavsson and Bengt Lennartson and Mats Jirstrand},
keywords = {geometry assurance, reinforcement learning, expert advice},
abstract = {To improve the assembly quality during production, expert systems are often used. These experts typically use a system model as a basis for identifying improvements. However, since a model uses approximate dynamics or imperfect parameters, the expert advice is bound to be biased. This paper presents a reinforcement learning agent that can identify and limit systematic errors of an expert systems used for geometry assurance. By observing the resulting assembly quality over time, and understanding how different decisions affect the quality, the agent learns when and how to override the biased advice from the expert software.}
}
@article{OGUNFOWORA2023244,
title = {Reinforcement and deep reinforcement learning-based solutions for machine maintenance planning, scheduling policies, and optimization},
journal = {Journal of Manufacturing Systems},
volume = {70},
pages = {244-263},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001462},
author = {Oluwaseyi Ogunfowora and Homayoun Najjaran},
keywords = {Maintenance planning, Maintenance scheduling, Preventive maintenance, Reinforcement learning, Deep reinforcement learning, Maintenance optimization},
abstract = {Systems and machines undergo various failure modes that result in machine health degradation, so maintenance actions are required to restore them back to a state where they can perform their expected functions. Since maintenance tasks are inevitable, maintenance planning is essential to ensure the smooth operations of the production system and other industries at large. Maintenance planning is a decision-making problem that aims at developing optimum maintenance policies and plans that help reduces maintenance costs, extend asset life, maximize their availability, and ultimately ensure workplace safety. Reinforcement learning is a data-driven decision-making algorithm that has been increasingly applied to develop dynamic maintenance plans while leveraging the continuous information from condition monitoring of the system and machine states. By leveraging the condition monitoring data of systems and machines with reinforcement learning, smart maintenance planners can be developed, which is a precursor to achieving a smart factory. This paper presents a literature review on the applications of reinforcement and deep reinforcement learning for maintenance planning and optimization problems. To capture the common ideas without losing touch with the uniqueness of each publication, taxonomies used to categorize the systems were developed, and reviewed publications were highlighted, classified, and summarized based on these taxonomies. Adopted methodologies, findings, and well-defined interpretations of the reviewed studies were summarized in graphical and tabular representations to maximize the utility of the work for both researchers and practitioners. This work also highlights the research gaps, key insights from the literature, and areas for future work.}
}
@article{ZHOU2020383,
title = {Deep reinforcement learning-based dynamic scheduling in smart manufacturing},
journal = {Procedia CIRP},
volume = {93},
pages = {383-388},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.163},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120307708},
author = {Longfei Zhou and Lin Zhang and Berthold K.P. Horn},
keywords = {Smart manufacturing, dynamic scheduling, deep reinforcement learning, dispatching rule},
abstract = {Scheduling problems are a classic type of optimization problems in the manufacturing domain, such as job shop scheduling, flexible job shop scheduling, and distributed job shop scheduling problems. Especially, the dynamic task scheduling problem is closer to the requirements of real manufacturing systems than the static scheduling problem. In recent years, with the deeper application of the Internet of Things, big data, and cloud platform technologies, manufacturing systems have evolved from job shops to networked, collaborative and intelligent manufacturing systems. Smart manufacturing scheduling has some characteristics compared with job shop scheduling not only because of the larger number of tasks and services, but also because of the dynamic state of services and uncertainties. In this paper, we analyze the smart manufacturing service scheduling problem and give its mathematical description. Then a deep reinforcement learning-based method is proposed to minimize the maximum completion time of all tasks. In the system framework of the proposed method, the agent, environment, as well as the interaction between them, are designed. The queue times of all candidate services are considered as the system state, and the maximum queue time at the current moment is considered as the target value. Besides, we use two networks the prediction network and the target network to learn the prediction value and the target value, respectively. Two case studies are used to show the efficiency of the considered problem and the proposed method.}
}
@article{SHIN20128736,
title = {Reinforcement learning approach to goal-regulation in a self-evolutionary manufacturing system},
journal = {Expert Systems with Applications},
volume = {39},
number = {10},
pages = {8736-8743},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.01.207},
url = {https://www.sciencedirect.com/science/article/pii/S0957417412002357},
author = {Moonsoo Shin and Kwangyeol Ryu and Mooyoung Jung},
keywords = {Self-evolutionary manufacturing system, Fractal organization, Goal-regulation, Reinforcement learning, Agent, Production planning},
abstract = {Up-to-date market dynamics has been forcing manufacturing systems to adapt quickly and continuously to the ever-changing environment. Self-evolution of manufacturing systems means a continuous process of adapting to the environment on the basis of autonomous goal-formation and goal-oriented dynamic organization. This paper proposes a goal-regulation mechanism that applies a reinforcement learning approach, which is a principal working mechanism for autonomous goal-formation. Individual goals are regulated by a neural network-based fuzzy inference system, namely, a goal-regulation network (GRN) updated by a reinforcement signal from another neural network called goal-evaluation network (GEN). The GEN approximates the compatibility of goals with current environmental situation. In this paper, a production planning problem is also examined by a simulation study in order to validate the proposed goal regulation mechanism.}
}
@article{ZIELINSKI2021107714,
title = {Flexible control of Discrete Event Systems using environment simulation and Reinforcement Learning},
journal = {Applied Soft Computing},
volume = {111},
pages = {107714},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107714},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621006359},
author = {Kallil M.C. Zielinski and Lucas V. Hendges and JoÃ£o B. Florindo and Yuri K. Lopes and Richardson Ribeiro and Marcelo Teixeira and Dalcimar Casanova},
keywords = {Discrete Event Systems, Manufacturing systems, Decision support, Process control, Process optimization, Reinforcement Learning},
abstract = {Discrete Event Systems (DESs) are classically modeled as Finite State Machines (FSMs), and controlled in a maximally permissive, controllable, and nonblocking way using Supervisory Control Theory (SCT). While SCT is powerful to orchestrate events of DESs, it fail to process events whose control is based on probabilistic assumptions. In this research, we show that some events can be approached as usual in SCT, while others can be processed using Artificial Intelligence. We present a tool to convert SCT controllers into Reinforcement Learning (RL) simulation environments, from where they become suitable for intelligent processing. Then, we propose a RL-based approach that recognizes the context under which the selected set of stochastic events occur, and treats them accordingly, aiming to find suitable decision making as complement to deterministic outcomes of the SCT. The result is an efficient combination of safe and flexible control, which tends to maximize performance for a class of DES that evolves probabilistically. Two RL algorithms are tested, StateâActionâRewardâStateâAction (SARSA) and N-step SARSA, over a flexible automotive plant control. Results suggest a performance improvement 9 times higher when using the proposed combination in comparison with non-intelligent decisions.}
}
@article{HUANG2021377,
title = {Integrated process-system modelling and control through graph neural network and reinforcement learning},
journal = {CIRP Annals},
volume = {70},
number = {1},
pages = {377-380},
year = {2021},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2021.04.056},
url = {https://www.sciencedirect.com/science/article/pii/S0007850621000809},
author = {Jing Huang and Jianjing Zhang and Qing Chang and Robert X. Gao},
keywords = {Multi-level modelling, Process control, Process-system integration},
abstract = {Modern manufacturing systems are becoming increasingly complex, dynamic, and connected, and their performance is being affected by not only their constituent processes but also their system-level interactions. This paper presents an integrated modelling method based on a graph neural network (GNN) and multi-agent reinforcement learning (MARL) collaborative control for adjusting individual machining process parameters in response to system- and process-level conditions. The structural and operational dependencies among process machines are captured with a GNN. Iteratively trained with MARL, machines learn to adaptively control local process parameters, e.g., machining speed and depth of cut, while achieving the global goal of improving production yield.}
}
@article{JUNGBLUTH2022156,
title = {Reinforcement Learning-based Scheduling of a Job-Shop Process with Distributedly Controlled Robotic Manipulators for Transport Operations},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {156-162},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.186},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322001872},
author = {Simon Jungbluth and Nigora Gafur and Jens Popper and Vassilios Yfantis and Martin Ruskowski},
keywords = {Scheduling, Machine learning, Intelligent manufacturing},
abstract = {Job-shop scheduling problems are important in the industrial context to achieve high machine utilization. Heuristics offer a possibility to solve these problems with moderate computational effort. However, they might be associated with a high development effort and generalization to other tasks is difficult. We use a reinforcement learning approach (deep Q-learning) to solve a job-shop problem in our production environment. A production process is considered where jobs are transported to allocated stations by two collaborative robots. To this end, a learning environment and a simulation environment are developed to evaluate the feasibility of an obtained schedule. The results are compared to a First In - First Out heuristic. The main objective is to consider the motion of the robots and to avoid collisions without losing unnecessary time. First, a fixed scheduling problem is analyzed to verify that a feasible solution can be obtained. Second, arbitrary instances of the scheduling problem are solved. The presented method leads to feasible schedules. An increased training and a more stable convergence process are necessary for an efficient use.}
}
@article{LIANG2021101991,
title = {Logistics-involved QoS-aware service composition in cloud manufacturing with deep reinforcement learning},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {67},
pages = {101991},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.101991},
url = {https://www.sciencedirect.com/science/article/pii/S0736584520302027},
author = {Huagang Liang and Xiaoqian Wen and Yongkui Liu and Haifeng Zhang and Lin Zhang and Lihui Wang},
keywords = {Cloud manufacturing, service composition, deep reinforcement learning, deep Q-network},
abstract = {Cloud manufacturing is a new manufacturing model that aims to provide on-demand manufacturing services to consumers over the Internet. Service composition is an essential issue as well as an important technique in cloud manufacturing (CMfg) that supports construction of larger-granularity, value-added services by combining a number of smaller-granularity services to satisfy consumersâ complex requirements. Meta-heuristics algorithms such as genetic algorithm, particle swarm optimization, and ant colony algorithm are frequently employed for addressing service composition issues in cloud manufacturing. These algorithms, however, require complex design flows and painstaking parameter tuning, and lack adaptability to dynamic environment. Deep reinforcement learning (DRL) provides an alternative approach for solving cloud manufacturing service composition (CMfg-SC) issues. DRL as model-free artificial intelligent methods enables a system to learn optimal service composition solutions through training, which can therefore circumvent the aforementioned problems with meta-heuristics algorithms. This paper is dedicated to exploring possible applications of DRL in CMfg-SC. A logistics-involved QoS-aware DRL-based CMfg-SC is proposed. A dueling Deep Q-Network (DQN) with prioritized replay named PD-DQN is designed as the DRL algorithm. Effectiveness, robustness, adaptability, and scalability of PD-DQN are investigated, and compared with that of the basic DQN and Q-learning. Experimental results indicate that PD-DQN is able to effectively address the CMfg-SC problem.}
}
@article{LIU2023102605,
title = {Integration of deep reinforcement learning and multi-agent system for dynamic scheduling of re-entrant hybrid flow shop considering worker fatigue and skill levels},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {84},
pages = {102605},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102605},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000819},
author = {Youshan Liu and Jiaxin Fan and Linlin Zhao and Weiming Shen and Chunjiang Zhang},
keywords = {Deep reinforcement learning, Multi-agent system, Self-organizing system, Dynamic scheduling, Hybrid flow shop scheduling, Human factors},
abstract = {In real-life manufacturing systems, production management is often affected by urgent demands and unexpected interruptions, such as new job insertions, machine breakdowns and operator unavailability. In this context, agent-based techniques are useful and able to respond quickly to dynamic disturbances. The ability of agents to recognize their environment and make decisions can be further enhanced by deep reinforcement learning (DRL). This paper investigates a novel dynamic re-entrant hybrid flow shop scheduling problem (DRHFSP) considering worker fatigue and skill levels to minimize the total tardiness of all production tasks. An integrated architecture of DRL and MAS (DRL-MAS) is proposed for real-time scheduling in dynamic environments. Two DRL models are proposed for different sub-decisions, where a reward-shaping technique combining long-term and short-term returns is proposed for the job sequence and machine selection sub-decisions, and an attention-based network is proposed for the worker assignment sub-decision for efficient feature extraction and decision making. Numerical experiments and case studies demonstrate the superior performance of the proposed DRL models compared with existing scheduling strategies.}
}
@article{DERAJ2023113937,
title = {Deep reinforcement learning based controller for ship navigation},
journal = {Ocean Engineering},
volume = {273},
pages = {113937},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.113937},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823003219},
author = {Rohit Deraj and R.S. Sanjeev Kumar and Md Shadab Alam and Abhilash Somayajula},
keywords = {Reinforcement learning, Autonomous vessel, Ship maneuvering, Path-following, Deep Q-network, MMG model, Deep learning},
abstract = {A majority of marine accidents that occur can be attributed to errors in human decisions. Through automation, the occurrence of such incidents can be minimized. Therefore, automation in the marine industry has been receiving increased attention in the recent years. This paper investigates the automation of the path following action of a ship. A deep Q-learning approach is proposed to solve the path-following problem of a ship. This method comes under the broader area of deep reinforcement learning (DRL) and is well suited for such tasks, as it can learn to take optimal decisions through sufficient experience. This algorithm also balances the exploration and the exploitation schemes of an agent operating in an environment. A three-degree-of-freedom (3-DOF) dynamic model is adopted to describe the shipâs motion. The Krisco container ship (KCS) is chosen for this study as it is a benchmark hull that is used in several studies and its hydrodynamic coefficients are readily available for numerical modeling. Numerical simulations for the turning circle and zig-zag maneuver tests are performed to verify the accuracy of the proposed dynamic model. A reinforcement learning (RL) agent is trained to interact with this numerical model to achieve waypoint tracking. Finally, the proposed approach is investigated not only by numerical simulations but also by model experiments using 1:75.5 scaled model.}
}
@article{HUANG2023109650,
title = {A cooperative hierarchical deep reinforcement learning based multi-agent method for distributed job shop scheduling problem with random job arrivals},
journal = {Computers & Industrial Engineering},
volume = {185},
pages = {109650},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109650},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223006745},
author = {Jiang-Ping Huang and Liang Gao and Xin-Yu Li and Chun-Jiang Zhang},
keywords = {Shop scheduling, Distributed manufacturing, Deep reinforcement learning, Multi-agent},
abstract = {Distributed manufacturing can reduce the production cost through the cooperation among factories, and it has been an important trend in the industrial field. For the enterprises with daily delivered production tasks, the random job arrivals are regular. Thus, the Distributed Job-shop Scheduling Problem (DJSP) with random job arrivals is studied, and it is a typical case from the equipment manufacturing industry. The DJSP involves two coupled decision-making processes, job assigning and job sequencing, and the distributed and uncertain production environment requires the scheduling method to be more responsive and adaptive. Thus, a Deep Reinforcement Learning (DRL) based multi-agent method is explored, and it is composed of the assigning agent and the sequencing agent. Two Markov Decision Processes (MDPs) are formulated for the two agents respectively. In the MDP for the assigning agent, fourteen factory-and-job related features are extracted as the state features, seven composite assigning rules are designed as the candidate actions, and the reward depends on the total processing time of different factories. In the MDP of the sequencing agent, five machine-and-job related features are set as the state features, six sequencing rules make up the action space, and the change of the factory makespan is the reward. Besides, to enhance the learning ability of the agents, a Deep Q-Network (DQN) framework with variable threshold probability in the training stage is designed, which can balance the exploitation and exploration in the model training. The proposed multi-agent methodâs effectiveness is proved by the independent utility test and the comparison test that are based on 1350 production instances, and its practical value in the actual production is implied by the case study from an automotive engine manufacturing company.}
}
@article{WANG2021239,
title = {A fuzzy hierarchical reinforcement learning based scheduling method for semiconductor wafer manufacturing systems},
journal = {Journal of Manufacturing Systems},
volume = {61},
pages = {239-248},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001801},
author = {Junliang Wang and Pengjie Gao and Peng Zheng and Jie Zhang and W.H. Ip},
keywords = {Scheduling, Reinforcement learning, Fuzzy, Manufacturing system},
abstract = {Scheduling semiconductor wafer manufacturing systems has been viewed as one of the most challenging optimization problems owing to the complicated constraints, and dynamic system environment. This paper proposes a fuzzy hierarchical reinforcement learning (FHRL) approach to schedule a SWFS, which controls the cycle time (CT) of each wafer lot to improve on-time delivery by adjusting the priority of each wafer lot. To cope with the layer correlation and wafer correlation of CT due to the re-entrant process constraint, a hierarchical model is presented with a recurrent reinforcement learning (RL) unit in each layer to control the corresponding sub-CT of each integrated circuit layer. In each RL unit, a fuzzy reward calculator is designed to reduce the impact of uncertainty of expected finishing time caused by the rematching of a lot to a delivery batch. The results demonstrate that the mean deviation (MD) between the actual and expected completion time of wafer lots under the scheduling of the FHRL approach is only about 30 % of the compared methods in the whole SWFS.}
}
@article{SCHEIDERER2020897,
title = {Simulation-as-a-Service for Reinforcement Learning Applications by Example of Heavy Plate Rolling Processes},
journal = {Procedia Manufacturing},
volume = {51},
pages = {897-903},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.126},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920319831},
author = {Christian Scheiderer and Timo Thun and Christian Idzik and AndrÃ©s Felipe Posada-Moreno and Alexander KrÃ¤mer and Johannes Lohmar and Gerhard Hirt and Tobias Meisen},
keywords = {Simulation-as-a-service, Reinforcement Learning, Distributed Architecture, Machine Economy},
abstract = {In the production industry, the digital transformation enables a significant optimization potential. The concept of reinforcement learning offers a suitable approach to train agents on learning control strategies, further advancing automation. While applications training directly on real-world processes are rare due to economical and safety constraints, simulations offer a way to develop and evaluate agents prior to deployment. With the rise of service-based business models, the simulation owner and the machine learning expert are likely to be different stakeholders in a joint project. Due to different requirements for both simulations and reinforcement-learning agents, the stakeholders may be reluctant or unable to grant full access to the respective software. This poses a serious impediment to the potential of the digital transformation. In this paper, a distributed architecture is proposed, which allows the remote training of reinforcement learning agents on a simulation. It is shown that this architecture allows the cooperation between two stakeholders by exposing a suitable technical interface to the simulation. The proposed architecture is implemented for a simulation of the multi-step metal forming process of heavy plate rolling. Furthermore, the implemented architecture is used to successfully train a reinforcement-learning agent on the task of designing optimal parameter schedules.}
}
@article{PAHWA202111,
title = {Dynamic matching with deep reinforcement learning for a two-sided Manufacturing-as-a-Service (MaaS) marketplace},
journal = {Manufacturing Letters},
volume = {29},
pages = {11-14},
year = {2021},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2021.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2213846321000274},
author = {Deepak Pahwa and Binil Starly},
keywords = {Cloud manufacturing, Cyber-enabled manufacturing, Resource allocation, Two-sided matching, Dynamic and stochastic knapsack problem (DSKP), Cloud based design and manufacturing (CBDM)},
abstract = {Suppliers registered within a manufacturing-as-a-service (MaaS) marketplace require near real time decision making to accept or reject orders received on the platform. Myopic decision-making such as a first come, first serve method in this dynamic and stochastic environment can lead to suboptimal revenue generation. In this paper, this sequential decision making problem is formulated as a Markov Decision Process and solved using deep reinforcement learning (DRL). Empirical simulations demonstrate that DRL has considerably better performance compared to four baselines. This early work demonstrates a learning approach for near real-time decision making for suppliers participating in a MaaS marketplace.}
}
@article{YU2021487,
title = {Optimizing task scheduling in human-robot collaboration with deep multi-agent reinforcement learning},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {487-499},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001527},
author = {Tian Yu and Jing Huang and Qing Chang},
keywords = {Human-Robot Collaboration, Real-time task scheduling, Multi-agent reinforcement learning},
abstract = {Human-Robot Collaboration (HRC) presents an opportunity to improve the efficiency of manufacturing processes. However, the existing task planning approaches for HRC are still limited in many ways, e.g., co-robot encoding must rely on expertsâ knowledge and the real-time task scheduling is applicable within small state-action spaces or simplified problem settings. In this paper, the HRC assembly working process is formatted into a novel chessboard setting, in which the selection of chess piece move is used to analogize to the decision making by both humans and robots in the HRC assembly working process. To optimize the completion time, a Markov game model is considered, which takes the task structure and the agent status as the state input and the overall completion time as the reward. Without expertsâ knowledge, this game model is capable of seeking for correlated equilibrium policy among agents with convergency in making real-time decisions facing a dynamic environment. To improve the efficiency in finding an optimal policy of the task scheduling, a deep-Q-network (DQN) based multi-agent reinforcement learning (MARL) method is applied and compared with the Nash-Q learning, dynamic programming and the DQN-based single-agent reinforcement learning method. A height-adjustable desk assembly is used as a case study to demonstrate the effectiveness of the proposed algorithm with different number of tasks and agents.}
}
@article{PRABHU19987,
title = {Fuzzy-logic-based Reinforcement Learning of Admittance Control for Automated Robotic Manufacturing},
journal = {Engineering Applications of Artificial Intelligence},
volume = {11},
number = {1},
pages = {7-23},
year = {1998},
issn = {0952-1976},
doi = {https://doi.org/10.1016/S0952-1976(97)00057-2},
url = {https://www.sciencedirect.com/science/article/pii/S0952197697000572},
author = {Sameer M. Prabhu and Devendra P. Garg},
keywords = {Admittance control, robotic manufacturing, reinforcement learning, fuzzy logic, CMAC},
abstract = {An approach to admittance control using fuzzy-logic-based reinforcement learning is proposed for the robotic automation of typical manufacturing operations. The proposed approach provides the necessary nonlinear control actions required in a typical automated robotic manufacturing task. Simultaneously, it reduces the controller development time due to the incorporation of pre-existing process knowledge in a neural-network form. The pre-existing knowledge is further refined using reinforcement learning via a CMAC (Cerebellar Model Articulation Controller) based critic network. Automated robotic deburring offers an attractive alternative to manual deburring in terms of reduced costs and improved quality of the finished parts. Hence, robotic deburring is used as an example of a typical manufacturing task to verify the performance of the proposed approach. However, the approach is general enough to be easily extended to similar manufacturing tasks. Simulation results are presented, which demonstrate the effectiveness of the proposed strategy in controlling the automated robotic deburring task.}
}
@article{MARCHESANO20222932,
title = {Dynamic scheduling of a due date constrained flow shop with Deep Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2932-2937},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.177},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322021917},
author = {Maria Grazia Marchesano and Guido Guizzi and Valentina Popolo and Giuseppe Converso},
keywords = {Smart Manufacturing Systems, Scheduling, Deep Reinforcement Learning, Flow Shop, Decentralised Manufacturing Planning, Control system, Industry 4.0},
abstract = {Manufacturers are increasingly under pressure to develop dynamic production systems and supply networks that can adjust to the climate, political, and social changes anywhere in the world at any time. Adoption of the Industry 4.0 paradigm aids in the completion of these objectives. Modern production systems necessitate a high level of manufacturing flexibility. At the same time, to keep up with the competition, manufacturers must make pledges to meet specified deadlines. In recent years, there has been a rise in interest in employing machine learning, particularly reinforcement learning, to solve production scheduling challenges of varying complexity. The general technique is to decompose the scheduling problem into a Markov Decision Process (MDP), after which an RL agent is trained using a simulation that implements the MDP. In this setting, this paper presents, in an application environment, a dispatching rule based on a deep reinforcement learning (DRL) algorithm. A DRL approach uses the DQN as the learning agent's training algorithm. The network's task is to identify the position of the job that will be executed. The objective is to present an algorithm that takes both the due date and the state of the production line into consideration to schedule jobs to meet the due dates and, at the same time, boost productivity. A flow shop configuration is considered and the performances of the proposed method are compared with the ones of dispatching rules already proposed in the scientific literature. To do so, the settings of the DRL algorithm must be specified, such as the state space, the reward function, and the hyperparameters, whereas the action is the choice of which job to be introduced in the production line. The overall objective of this research is to provide a general scheduling tool that may be used in a variety of situations, including unexpected ones.}
}
@article{LUO2020106208,
title = {Dynamic scheduling for flexible job shop with new job insertions by deep reinforcement learning},
journal = {Applied Soft Computing},
volume = {91},
pages = {106208},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106208},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620301484},
author = {Shu Luo},
keywords = {Flexible job shop scheduling, New job insertion, Dispatching rules, Deep reinforcement learning, Deep Q network},
abstract = {In modern manufacturing industry, dynamic scheduling methods are urgently needed with the sharp increase of uncertainty and complexity in production process. To this end, this paper addresses the dynamic flexible job shop scheduling problem (DFJSP) under new job insertions aiming at minimizing the total tardiness. Without lose of generality, the DFJSP can be modeled as a Markov decision process (MDP) where an intelligent agent should successively determine which operation to process next and which machine to assign it on according to the production status of current decision point, making it particularly feasible to be solved by reinforcement learning (RL) methods. In order to cope with continuous production states and learn the most suitable action (i.e. dispatching rule) at each rescheduling point, a deep Q-network (DQN) is developed to address this problem. Six composite dispatching rules are proposed to simultaneously select an operation and assign it on a feasible machine every time an operation is completed or a new job arrives. Seven generic state features are extracted to represent the production status at a rescheduling point. By taking the continuous state features as input to the DQN, the stateâaction value (Q-value) of each dispatching rule can be obtained. The proposed DQN is trained using deep Q-learning (DQL) enhanced by two improvements namely double DQN and soft target weight update. Moreover, a âsoftmaxâ action selection policy is utilized in real implementation of the trained DQN so as to promote the rules with higher Q-values while maintaining the policy entropy. Numerical experiments are conducted on a large number of instances with different production configurations. The results have confirmed both the superiority and generality of DQN compared to each composite rule, other well-known dispatching rules as well as the stand Q-learning-based agent.}
}
@article{LEE2022101710,
title = {Digital twin-driven deep reinforcement learning for adaptive task allocation in robotic construction},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101710},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101710},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001689},
author = {Dongmin Lee and SangHyun Lee and Neda Masoud and M.S. Krishnan and Victor C. Li},
keywords = {Digital Twin, Proximal Policy Optimization (PPO), Deep Reinforcement Learning (DRL), Autonomous Robot, Adaptive Task Allocation},
abstract = {In order to accomplish diverse tasks successfully in a dynamic (i.e., changing over time) construction environment, robots should be able to prioritize assigned tasks to optimize their performance in a given state. Recently, a deep reinforcement learning (DRL) approach has shown potential for addressing such adaptive task allocation. It remains unanswered, however, whether or not DRL can address adaptive task allocation problems in dynamic robotic construction environments. In this paper, we developed and tested a digital twin-driven DRL learning method to explore the potential of DRL for adaptive task allocation in robotic construction environments. Specifically, the digital twin synthesizes sensory data from physical assets and is used to simulate a variety of dynamic robotic construction site conditions within which a DRL agent can interact. As a result, the agent can learn an adaptive task allocation strategy that increases project performance. We tested this method with a case project in which a virtual robotic construction project (i.e., interlocking concrete bricks are delivered and assembled by robots) was digitally twinned for DRL training and testing. Results indicated that the DRL modelâs task allocation approach reduced construction time by 36% in three dynamic testing environments when compared to a rule-based imperative model. The proposed DRL learning method promises to be an effective tool for adaptive task allocation in dynamic robotic construction environments. Such an adaptive task allocation method can help construction robots cope with uncertainties and can ultimately improve construction project performance by efficiently prioritizing assigned tasks.}
}
@article{HUANG2020113701,
title = {Deep reinforcement learning based preventive maintenance policy for serial production lines},
journal = {Expert Systems with Applications},
volume = {160},
pages = {113701},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113701},
url = {https://www.sciencedirect.com/science/article/pii/S095741742030525X},
author = {Jing Huang and Qing Chang and Jorge Arinez},
keywords = {Preventive maintenance, Production loss, Deep reinforcement learning, Serial production line, Group maintenance, Opportunistic maintenance},
abstract = {In the manufacturing industry, the preventive maintenance (PM) is a common practice to reduce random machine failures by replacing/repairing the aged machines or parts. The decision on when and where the preventive maintenance needs to be carried out is nontrivial due to the complex and stochastic nature of a serial production line with intermediate buffers. In order to improve the cost efficiency of the serial production lines, a deep reinforcement learning based approach is proposed to obtain PM policy. A novel modeling method for the serial production line is adopted during the learning process. A reward function is proposed based on the system production loss evaluation. The algorithm based on the Double Deep Q-Network is applied to learn the PM policy. Using the simulation study, the learning algorithm is proved effective in delivering PM policy that leads to an increased throughput and reduced cost. Interestingly, the learned policy is found to frequently conduct âgroup maintenanceâ and âopportunistic maintenanceâ, although their concepts and rules are not provided during the learning process. This finding further demonstrates that the problem formulation, the proposed algorithm and the reward function setting in this paper are effective.}
}
@article{KIM2024102632,
title = {Digital twin for autonomous collaborative robot by using synthetic data and reinforcement learning},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {85},
pages = {102632},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102632},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523001072},
author = {Dongjun Kim and Minho Choi and Jumyung Um},
keywords = {Object detection, Synthetic data, Point cloud, Reinforcement learning, Digital twin},
abstract = {Training robots in real-world environments can be challenging due to time and cost constraints. To overcome these limitations, robots can be trained in virtual environments using Reinforcement Learning (RL). However, this approach faces a significant challenge in obtaining suitable data. This paper proposes a novel method for training collaborative robots in virtual environments using synthetic data and the point cloud framework. The proposed method is divided into four stages: data generation, 3D object classification, robot training, and integration. The first stage of the proposed method is data generation, where synthetic data is generated to resemble real-world scenarios. This data is then used to train robots in virtual environments. The second stage is 3D object classification, where the generated data is used to classify objects in 3D space. In the third stage, robots are trained using RL algorithms, which are based on the generated data and the 3D object classifications. Finally, the effectiveness of the proposed method is integrated in the fourth stage. This proposed method has the potential to be a significant contribution to the field of robotics and 3D computer vision. By using synthetic data and the point cloud framework, the proposed method offers an efficient and cost-effective solution for training robots in virtual environments. The ability to reduce the time and cost required for training robots in real-world environments is a major advantage of this proposed method, and has the potential to revolutionize the field of robotics and 3D computer vision.}
}
@article{MOHAMMADI2024102636,
title = {Sustainable Robotic Joints 4D Printing with Variable Stiffness Using Reinforcement Learning},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {85},
pages = {102636},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102636},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523001114},
author = {Moslem Mohammadi and Abbas Z. Kouzani and Mahdi Bodaghi and John Long and Sui Yang Khoo and Yong Xiang and Ali Zolfagharian},
keywords = {4D printing, Variable stiffness, Sustainable, Robots, Reinforcement learning},
abstract = {Nowadays, a wide range of robots are used in various fields, from car factories to assistant soft robots. In all these applications, effective control of the robot is vital to perform the tasks assigned to them. Soft robots and actuators have several advantages over traditional rigid manipulators, including lower power consumption, lighter weight, safer operation in contact with live tissues, inexpensive manufacturing costs, and quicker movements. However, controlling them is more challenging. This paper presents a three-dimensional (3D) printed structure combined with carbon fibres to provide a stimulus signal, known as four-dimensional (4D) printing. Depending on the application, the structure could provide various levels of stiffness to adapt to new conditions. A nonlinear controller based on reinforcement learning (RL) algorithms is also presented to control the stiffness of soft joints. The controller is tuned based on the mathematical model of the Simulink setup and then applied to the experimental setup. The results show that the RL controller has a high potential to adapt online to various unforeseen conditions. Additionally, this controller offers a significantly reduced lag for specific inputs, such as a sinusoidal signal, while considerably decreasing power consumption in contrast to a linear controller. This is a significant advantage of variable stiffness 4D-pritned soft joints for sustainable and circular robots manufacturing in portable medical and wearable sustainable robotic applications.}
}
@article{RADAIDEH2021110966,
title = {Physics-informed reinforcement learning optimization of nuclear assembly design},
journal = {Nuclear Engineering and Design},
volume = {372},
pages = {110966},
year = {2021},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2020.110966},
url = {https://www.sciencedirect.com/science/article/pii/S002954932030460X},
author = {Majdi I. Radaideh and Isaac Wolverton and Joshua Joseph and James J. Tusar and Uuganbayar Otgonbaatar and Nicholas Roy and Benoit Forget and Koroush Shirvan},
abstract = {Optimization of nuclear fuel assemblies if performed effectively, will lead to fuel efficiency improvement, cost reduction, and safety assurance. However, assembly optimization involves solving high-dimensional and computationally expensive combinatorial problems. As such, fuel designersâ expert judgement has commonly prevailed over the use of stochastic optimization (SO) algorithms such as genetic algorithms and simulated annealing. To improve the state-of-art, we explore a class of artificial intelligence (AI) algorithms, namely, reinforcement learning (RL) in this work. We propose a physics-informed AI optimization methodology by establishing a connection through reward shaping between RL and the tactics fuel designers follow in practice by moving fuel rods in the assembly to meet specific constraints and objectives. The methodology utilizes RL algorithms, deep Q learning and proximal policy optimization, and compares their performance to SO algorithms. The methodology is applied on two boiling water reactor assemblies of low-dimensional (â¼2Ã106 combinations) and high-dimensional (â¼1031 combinations) natures. The results demonstrate that RL is more effective than SO in solving high dimensional problems, i.e., 10â¯Ãâ¯10 assembly, through embedding expert knowledge in form of game rules and effectively exploring the search space. For a given computational resources and timeframe relevant to fuel designers, RL algorithms outperformed SO through finding more feasible patterns, 4â5 times more than SO, and through increasing search speed, as indicated by the RL outstanding computational efficiency. The results of this work clearly demonstrate RL effectiveness as another decision support tool for nuclear fuel assembly optimization.}
}
@article{DITTRICH2020389,
title = {Cooperative multi-agent system for production control using reinforcement learning},
journal = {CIRP Annals},
volume = {69},
number = {1},
pages = {389-392},
year = {2020},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2020.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0007850620300263},
author = {Marc-AndrÃ© Dittrich and Silas Fohlmeister},
keywords = {Production planning, Machine learning, Multi-agent system},
abstract = {Multi-agent systems can limit the control problem in complex production systems and solve them more efficiently. However, they often show local optimization tendencies. This paper presents a novel approach for a cooperative multi-agent system, which uses reinforcement learning and considers global key performance indicators. For this purpose, a central deep q-learning module transfers its knowledge to the cooperative order agents. The order agent's experience is stored in a replay memory for subsequent reinforcement learning. Interdependencies between the characteristics of nonlinear production systems and learning parameters are investigated and the performance is evaluated in comparison to conventional methods of production control.}
}
@article{SCHWUNG2021107382,
title = {Decentralized learning of energy optimal production policies using PLC-informed reinforcement learning},
journal = {Computers & Chemical Engineering},
volume = {152},
pages = {107382},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107382},
url = {https://www.sciencedirect.com/science/article/pii/S0098135421001605},
author = {Dorothea Schwung and Steve Yuwono and Andreas Schwung and Steven X. Ding},
keywords = {Energy optimization, Smart manufacturing, Modular production control, Distributed control, PLC-informed reinforcement learning},
abstract = {This paper presents a novel approach to distributed optimization in production systems using reinforcement learning (RL) with particular emphasis on energy efficient production. Unlike existing approaches in RL which learn optimal policies from scratch, we speed-up the learning process by considering available control code of the Programmable Logic Controller (PLC) as a baseline for further optimizations. For such PLC-informed RL, we propose Teacher-Student RL to distill the available control code of the individual modules into a neural network which is subsequently optimized using standard RL. The proposed general framework allows to incorporate PLC control code with different level of detail. We implement the approach on a laboratory scale testbed representing different production scenarios ranging from continuous to batch production. We compare the results for different control strategies which show that comparably simple control logic yields considerable improvements of the optimization compared to learning from scratch. The obtained results underline the applicability and potential of the approach in terms of improved production efficiency while considerably reducing the energy consumption of the production schedules.}
}
@article{KUHNLE2019391,
title = {Autonomous order dispatching in the semiconductor industry using reinforcement learning},
journal = {Procedia CIRP},
volume = {79},
pages = {391-396},
year = {2019},
note = {12th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 18-20 July 2018, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.02.101},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119302185},
author = {Andreas Kuhnle and Nicole RÃ¶hrig and Gisela Lanza},
keywords = {Production planning, Reinforcement learning, Semiconductor industry},
abstract = {Cyber Physical Production Systems (CPPS) provide a huge amount of data. Simultaneously, operational decisions are getting ever more complex due to smaller batch sizes, a larger product variety and complex processes in production systems. Production engineers struggle to utilize the recorded data to optimize production processes effectively because of a rising level of complexity. This paper shows the successful implementation of an autonomous order dispatching system that is based on a Reinforcement Learning (RL) algorithm. The real-world use case in the semiconductor industry is a highly suitable example of a cyber physical and digitized production system.}
}
@article{ALICASTRO2021105272,
title = {A reinforcement learning iterated local search for makespan minimization in additive manufacturing machine scheduling problems},
journal = {Computers & Operations Research},
volume = {131},
pages = {105272},
year = {2021},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2021.105272},
url = {https://www.sciencedirect.com/science/article/pii/S0305054821000642},
author = {Mirko Alicastro and Daniele Ferone and Paola Festa and Serena Fugaro and Tommaso Pastore},
keywords = {Machine scheduling, additive manufacturing, reinforcement learning, Q-learning, iterated local search},
abstract = {Additive manufacturing â also known as 3D printing â is a manufacturing process that is attracting more and more interest due to high production rates and reduced costs. This paper focuses on the scheduling problem of multiple additive manufacturing machines, recently proposed in the scientific literature. Given its intractability, instances of relevant size of additive manufacturing (AM) machine scheduling problem cannot be solved in reasonable computational times through mathematical models. For this reason, this paper proposes a Reinforcement Learning Iterated Local Search meta-heuristic, based on the implementation of a Q-Learning Variable Neighborhood Search, to provide heuristically good solutions at the cost of low computational expenses. A comprehensive computational study is conducted, comparing the proposed methodology with the results achieved by the CPLEX solver and to the performance of an Evolutionary Algorithm recently proposed for a similar problem, and adapted for the AM machine scheduling problem. Additionally, to explore the trade-off between efficiency and effectiveness more deeply, we present a further set of experiments that test the potential inclusion of a probabilistic stopping rule. The numerical results evidence that the proposed Reinforcement Learning Iterated Local Search is able to obtain statistically significant improvements compared to the other solution approaches featured in the computational experiments.}
}
@article{MA2023419,
title = {Deep reinforcement learning optimized double exponentially weighted moving average controller for chemical mechanical polishing processes},
journal = {Chemical Engineering Research and Design},
volume = {197},
pages = {419-433},
year = {2023},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2023.07.049},
url = {https://www.sciencedirect.com/science/article/pii/S026387622300494X},
author = {Zhu Ma and Tianhong Pan and Jiaqiang Tian},
keywords = {Deep reinforcement learning, Run-to-run control, Double exponentially weighted moving average, Chemical mechanical polishing, Parameter optimization},
abstract = {This study investigates a deep reinforcement learning (DRL)-assisted double exponentially weighted moving average (dEWMA) controller for run-to-run (RtR) control in the semiconductor manufacturing process. We focus on implementing parameter adaption of dEWMA controllers to achieve disturbance compensation and target tracking. Owing to the powerful adaptive decision-making capability of the DRL, the weight adjustment of dEWMA controller is formulated as a Markov decision process. Specifically, the DRL behaves as an assisted controller to derive appropriate weights that facilitate dEWMA to perform highly accurate disturbance estimation, whereas the standard dEWMA works as a baseline controller to provide suitable recipes for the manufacturing process. Consequently, a composite control strategy integrating DRL and dEWMA is developed. In addition, a twin-delayed deep deterministic policy gradient algorithm is employed to adjust the weights of dEWMA online. The effectiveness of the proposed scheme is validated in a chemical mechanical polishing process. Several disturbance rejection scenarios verify the benefits of the suggested approach.}
}
@article{LI2023102471,
title = {An AR-assisted Deep Reinforcement Learning-based approach towards mutual-cognitive safe human-robot interaction},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {80},
pages = {102471},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102471},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001533},
author = {Chengxi Li and Pai Zheng and Yue Yin and Yat Ming Pang and Shengzeng Huo},
keywords = {Smart manufacturing, Human robot interaction, Augmented reality, Deep reinforcement learning, Manufacturing safety},
abstract = {With the emergence of Industry 5.0, the human-centric manufacturing paradigm requires manufacturing equipment (robots, etc.) interactively assist human workers to deal with dynamic and complex production tasks. To achieve symbiotic humanârobot interaction (HRI), the safety issue serves as a prerequisite foundation. Regarding the growing individualized demand of manufacturing tasks, the conventional rule-based safe HRI measures could not well address the safety requirements due to inflexibility and lacking synergy. To fill the gap, this work proposes a mutual-cognitive safe HRI approach including worker visual augmentation, robot velocity control, Digital Twin-enabled motion preview and collision detection, and Deep Reinforcement Learning-based robot collision avoidance motion planning in the Augmented Reality-assisted manner. Finally, the feasibility of the system design and the performance of the proposed approach are validated by establishing and executing the prototype HRI system in a practical scene.}
}
@article{ANDERSEN2019171,
title = {Self-learning Processes in Smart Factories: Deep Reinforcement Learning for Process Control of Robot Brine Injection},
journal = {Procedia Manufacturing},
volume = {38},
pages = {171-177},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S235197892030024X},
author = {Rasmus E. Andersen and Steffen Madsen and Alexander B.K. Barlo and Sebastian B. Johansen and Morten NÃ¸r and Rasmus S. Andersen and Simon BÃ¸gh},
keywords = {Self-learning Smart Factories, Deep Reinforcement Learning, Process Control},
abstract = {The goal of this paper is to investigate the application of adaptive learning algorithms, which enables industrial robots to cope with natural variations exhibited in a brine injection process related to the production of bacon. Due to the variations in bacon meat, the traditional needle-based brine injection process is not capable of injecting the correct amount of brine, leading to either ruined or unflavored bacon. In the presented work a Deep Deterministic Policy Gradient (DDPG) reinforcement learning algorithm is introduced in the injection process to improve process control. To accelerate training of the reinforcement learning algorithm, a simulation environment of the brine absorption is generated based on 64 conducted experiments. The simulation environment estimates the amount of absorbed brine given injection pressure and injection time. Tests are run in the simulation where the starting mass is generated from a normal distribution with mean 80.5g, and a standard deviation of 4.8 g and 20.0 g respectively. With a target of 15 % mass increase, the agent can produce an average mass increase of 14.9 % for the first test and 14.6 % for the second test. This indicates that the model can successfully adapt to a high variety input, thereby showing potential for process control in brine injection, coping with natural variation in meat structure.}
}
@article{LIU202120,
title = {On deep reinforcement learning security for Industrial Internet of Things},
journal = {Computer Communications},
volume = {168},
pages = {20-32},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420320193},
author = {Xing Liu and Wei Yu and Fan Liang and David Griffith and Nada Golmie},
keywords = {Industrial Internet of Things, Deep reinforcement learning, Inverse reinforcement learning, Security risks, Edge computing},
abstract = {The Industrial Internet of Things (IIoT), also known as Industry 4.0, empowers manufacturing and production processes by leveraging automation and Internet of Things (IoT) technologies. In IIoT, the information communication technologies enabled by IoT could greatly improve the efficiency and timeliness of information exchanges between both vertical and horizontal system integrations. Likewise, machine learning algorithms, particularly Deep Reinforcement Learning (DRL), are viable for assisting in automated control of complex IIoT systems, with the support of distributed edge computing infrastructure. Despite noticeable performance improvements, the security threats brought by massive interconnections in IoT and the vulnerabilities of deep neural networks used in DRL must be thoroughly investigated and mitigated before widespread deployment. Thus, in this paper we first design a DRL-based controller that could be deployed at edge computing server to enable automated control in an IIoT context. We then investigate malicious behaviors of adversaries with two attacks: (i) function-based attacks that can be launched during training phase and (ii) performance-based attacks that can be launched after training phase, to study the security impacts of vulnerable DRL-based controllers. From the adversaryâs perspective, maximum entropy Inverse Reinforcement Learning (IRL) is used to approximate a reward function through observation of system trajectories under the control of trained DRL-based controllers. The approximated reward function is then used to launch attacks by the adversary against the Deep Q Network (DQN)-based controller. Via simulation, we evaluate the impacts of our two investigated attacks, finding that attacks are increasingly successful with increasing accuracy of the control model. Furthermore, we discuss some tradeoffs between control performance and security performance of DRL-based IIoT controllers, and outline several future research directions to secure machine learning use in IIoT systems.}
}
@article{ZHOU2021102202,
title = {Multi-agent reinforcement learning for online scheduling in smart factories},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {72},
pages = {102202},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102202},
url = {https://www.sciencedirect.com/science/article/pii/S0736584521000855},
author = {Tong Zhou and Dunbing Tang and Haihua Zhu and Zequn Zhang},
keywords = {Online scheduling, Smart factory, Composite reward, Multi-agent system, Reinforcement learning},
abstract = {Rapid advances in sensing and communication technologies connect isolated manufacturing units, which generates large amounts of data. The new trend of mass customization brings a higher level of disturbances and uncertainties to production planning. Traditional manufacturing systems analyze data and schedule orders in a centralized architecture, which is inefficient and unreliable for the overdependence on central controllers and limited communication channels. Internet of things (IoT) and cloud technologies make it possible to build a distributed manufacturing architecture such as the multi-agent system (MAS). Recently, artificial intelligence (AI) methods are used to solve scheduling problems in the manufacturing setting. However, it is difficult for scheduling algorithms to process high-dimensional data in a distributed system with heterogeneous manufacturing units. Therefore, this paper presents new cyber-physical integration in smart factories for online scheduling of low-volume-high-mix orders. First, manufacturing units are interconnected with each other through the cyber-physical system (CPS) by IoT technologies. Attributes of machining operations are stored and transmitted by radio frequency identification (RFID) tags. Second, we propose an AI scheduler with novel neural networks for each unit (e.g., warehouse, machine) to schedule dynamic operations with real-time sensor data. Each AI scheduler can collaborate with other schedulers by learning from their scheduling experiences. Third, we design new reward functions to improve the decision-making abilities of multiple AI schedulers based on reinforcement learning (RL). The proposed methodology is evaluated and validated in a smart factory by real-world case studies. Experimental results show that the new architecture for smart factories not only improves the learning and scheduling efficiency of multiple AI schedulers but also effectively deals with unexpected events such as rush orders and machine failures.}
}