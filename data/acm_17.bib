@article{10.1145/3610388.3610390,
author = {Custode, Leonardo Lucio},
title = {Thesis Report: Evolutionary Optimization of Decision Trees for Interpretable Reinforcement Learning},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
url = {https://doi.org/10.1145/3610388.3610390},
doi = {10.1145/3610388.3610390},
abstract = {This thesis focuses on the issue of interpretability in machine learning, proposing algorithms to evolve decision trees (DTs) for reinforcement learning (RL) tasks. The dissertation is available at the following link: https://iris.unitn.it/handle/11572/375447.},
journal = {SIGEVOlution},
month = {jul},
articleno = {2},
numpages = {4}
}

@inproceedings{10.1145/3365109.3368780,
author = {Dassanayake, Priyanthi M. and Anjum, Ashiq and Manning, Warren and Bower, Craig},
title = {A Deep Reinforcement Learning Based Homeostatic System for Unmanned Position Control},
year = {2019},
isbn = {9781450370165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365109.3368780},
doi = {10.1145/3365109.3368780},
abstract = {Deep Reinforcement Learning (DRL) has been proven to be capable of designing an optimal control theory by minimising the error in dynamic systems. However, in many of the real-world operations, the exact behaviour of the environment is unknown. In such environments, random changes cause the system to reach different states for the same action. Hence, application of DRL for unpredictable environments is difficult as the states of the world cannot be known for non-stationary transition and reward functions.In this paper, a mechanism to encapsulate the randomness of the environment is suggested using a novel bio-inspired homeostatic approach based on a hybrid of Receptor Density Algorithm (an artificial immune system based anomaly detection application) and a Plastic Spiking Neuronal model. DRL is then introduced to run in conjunction with the above hybrid model. The system is tested on a vehicle to autonomously re-position in an unpredictable environment. Our results show that the DRL based process control raised the accuracy of the hybrid model by 32\%.},
booktitle = {Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {127–136},
numpages = {10},
keywords = {deep reinforcement learning, homoeostasis-inspired, deep neural network, bio-inspired, receptor density algorithm, artificial immune system, plastic spiking neuron},
location = {Auckland, New Zealand},
series = {BDCAT '19}
}

@article{10.1145/2882969,
author = {Ganesan, Rajesh and Jajodia, Sushil and Shah, Ankit and Cam, Hasan},
title = {Dynamic Scheduling of Cybersecurity Analysts for Minimizing Risk Using Reinforcement Learning},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2882969},
doi = {10.1145/2882969},
abstract = {An important component of the cyber-defense mechanism is the adequate staffing levels of its cybersecurity analyst workforce and their optimal assignment to sensors for investigating the dynamic alert traffic. The ever-increasing cybersecurity threats faced by today’s digital systems require a strong cyber-defense mechanism that is both reactive in its response to mitigate the known risk and proactive in being prepared for handling the unknown risks. In order to be proactive for handling the unknown risks, the above workforce must be scheduled dynamically so the system is adaptive to meet the day-to-day stochastic demands on its workforce (both size and expertise mix). The stochastic demands on the workforce stem from the varying alert generation and their significance rate, which causes an uncertainty for the cybersecurity analyst scheduler that is attempting to schedule analysts for work and allocate sensors to analysts. Sensor data are analyzed by automatic processing systems, and alerts are generated. A portion of these alerts is categorized to be significant, which requires thorough examination by a cybersecurity analyst. Risk, in this article, is defined as the percentage of significant alerts that are not thoroughly analyzed by analysts. In order to minimize risk, it is imperative that the cyber-defense system accurately estimates the future significant alert generation rate and dynamically schedules its workforce to meet the stochastic workload demand to analyze them. The article presents a reinforcement learning-based stochastic dynamic programming optimization model that incorporates the above estimates of future alert rates and responds by dynamically scheduling cybersecurity analysts to minimize risk (i.e., maximize significant alert coverage by analysts) and maintain the risk under a pre-determined upper bound. The article tests the dynamic optimization model and compares the results to an integer programming model that optimizes the static staffing needs based on a daily-average alert generation rate with no estimation of future alert rates (static workforce model). Results indicate that over a finite planning horizon, the learning-based optimization model, through a dynamic (on-call) workforce in addition to the static workforce, (a) is capable of balancing risk between days and reducing overall risk better than the static model, (b) is scalable and capable of identifying the quantity and the right mix of analyst expertise in an organization, and (c) is able to determine their dynamic (on-call) schedule and their sensor-to-analyst allocation in order to maintain risk below a given upper bound. Several meta-principles are presented, which are derived from the optimization model, and they further serve as guiding principles for hiring and scheduling cybersecurity analysts. Days-off scheduling was performed to determine analyst weekly work schedules that met the cybersecurity system’s workforce constraints and requirements.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jul},
articleno = {4},
numpages = {21},
keywords = {dynamic scheduling, integer programming, risk mitigation, Cybersecurity analysts, reinforcement learning, genetic algorithm, resource allocation, optimization}
}

@inproceedings{10.1145/3528416.3530227,
author = {Meng, Yuan and Zhang, Chi and Prasanna, Viktor},
title = {FPGA Acceleration of Deep Reinforcement Learning Using On-Chip Replay Management},
year = {2022},
isbn = {9781450393386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528416.3530227},
doi = {10.1145/3528416.3530227},
abstract = {A major bottleneck in parallelizing deep reinforcement learning (DRL) is in the high latency to perform various operations used to update the Prioritized Replay Buffer on CPU. The low arithmetic intensity of these operations leads to severe under-utilization of the SIMT computation power of GPUs. In this work, we propose a high-throughput on-chip accelerator for Prioritized Replay Buffer and learner that efficient allocates computation and memory resources to saturate the FPGA computation power. Our design features hardware pipelining on FPGA such that the latency of replay operations is completely hidden. Our experimental results show that the performance of the key operations in managing Prioritized Replay Buffer including sampling and priority insertions are improved by factor of 21X ~ 40X compared with the state-of-the-art implementations on CPU and GPU. In addition, our system design leads to up to 4.3X improvement in overall throughput compared with the state-of-the-art CPU-GPU implementations.},
booktitle = {Proceedings of the 19th ACM International Conference on Computing Frontiers},
pages = {40–48},
numpages = {9},
keywords = {FPGA, deep reinforcement learning, prioritized replay buffer},
location = {Turin, Italy},
series = {CF '22}
}

@inproceedings{10.1145/3580305.3599951,
author = {Han, Weiguang and Zhang, Boyi and Xie, Qianqian and Peng, Min and Lai, Yanzhao and Huang, Jimin},
title = {Select and Trade: Towards Unified Pair Trading with Hierarchical Reinforcement Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599951},
doi = {10.1145/3580305.3599951},
abstract = {Pair trading is one of the most effective statistical arbitrage strategies which seeks a neutral profit by hedging a pair of selected assets. Existing methods generally decompose the task into two separate steps: pair selection and trading. However, the decoupling of two closely related sub-tasks can block information propagation and lead to limited overall performance. For pair selection, ignoring the trading performance results in the wrong assets being selected with irrelevant price movements, while the agent trained for trading can overfit to the selected assets without any historical information of other assets. To address it, in this paper, we propose a paradigm for automatic pair trading as a unified task rather than a two-step pipeline. We design a hierarchical reinforcement learning framework to jointly learn and optimize two sub-tasks. A high-level policy would select two assets from all possible combinations and a low-level policy would then perform a series of trading actions. Experimental results on real-world stock data demonstrate the effectiveness of our method on pair trading compared with both existing pair selection and trading methods.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4123–4134},
numpages = {12},
keywords = {automatic trading, pair selection, hierarchical reinforcement learning, pair trading},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@article{10.1145/3447878,
author = {He, Zhaoliang and Li, Hongshan and Wang, Zhi and Xia, Shutao and Zhu, Wenwu},
title = {Adaptive Compression for Online Computer Vision: An Edge Reinforcement Learning Approach},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3447878},
doi = {10.1145/3447878},
abstract = {With the growth of computer vision-based applications, an explosive amount of images have been uploaded to cloud servers that host such online computer vision algorithms, usually in the form of deep learning models. JPEG has been used as the de facto compression and encapsulation method for images. However, standard JPEG configuration does not always perform well for compressing images that are to be processed by a deep learning model—for example, the standard quality level of JPEG leads to 50\% of size overhead (compared with the best quality level selection) on ImageNet under the same inference accuracy in popular computer vision models (e.g., InceptionNet and ResNet). Knowing this, designing a better JPEG configuration for online computer vision-based services is still extremely challenging. First, cloud-based computer vision models are usually a black box to end-users; thus, it is challenging to design JPEG configuration without knowing their model structures. Second, the “optimal” JPEG configuration is not fixed; instead, it is determined by confounding factors, including the characteristics of the input images and the model, the expected accuracy and image size, and so forth. In this article, we propose a reinforcement learning (RL)-based adaptive JPEG configuration framework, AdaCompress. In particular, we design an edge (i.e., user-side) RL agent that learns the optimal compression quality level to achieve an expected inference accuracy and upload image size, only from the online inference results, without knowing details of the model structures. Furthermore, we design an explore-exploit mechanism to let the framework fast switch an agent when it detects a performance degradation, mainly due to the input change (e.g., images captured across daytime and night). Our evaluation experiments using real-world online computer vision-based APIs from Amazon Rekognition, Face++, and Baidu Vision show that our approach outperforms existing baselines by reducing the size of images by one-half to one-third while the overall classification accuracy only decreases slightly. Meanwhile, AdaCompress adaptively re-trains or re-loads the RL agent promptly to maintain the performance.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {nov},
articleno = {118},
numpages = {23},
keywords = {machine learning service, reinforcement learning, adaptive compression, Edge computing}
}

@inproceedings{10.1145/3511808.3557133,
author = {Yao, Zhiyuan and Ding, Zihan and Clausen, Thomas},
title = {Multi-Agent Reinforcement Learning for Network Load Balancing in Data Center},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557133},
doi = {10.1145/3511808.3557133},
abstract = {This paper presents the network load balancing problem, a challenging real-world task for multi-agent reinforcement learning (MARL) methods. Conventional heuristic solutions like Weighted-Cost Multi-Path (WCMP) and Local Shortest Queue (LSQ) are less flexible to the changing workload distributions and arrival rates, with a poor balance among multiple load balancers. The cooperative network load balancing task is formulated as a Dec-POMDP problem, which naturally induces the MARL methods. To bridge the reality gap for applying learning-based methods, all models are directly trained and evaluated on a real-world system from moderate- to large-scale setups. Experimental evaluations show that the independent and "selfish'' load balancing strategies are not necessarily the globally optimal ones, while the proposed MARL solution has a superior performance over different realistic settings. Additionally, the potential difficulties of the application and deployment of MARL methods for network load balancing are analysed, which helps draw the attention of the learning and network communities to such challenges.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {3594–3603},
numpages = {10},
keywords = {MARL, load balancing, distributed systems},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.1145/3582577,
author = {Wang, Ruihang and Cao, Zhiwei and Zhou, Xin and Wen, Yonggang and Tan, Rui},
title = {Green Data Center Cooling Control via Physics-Guided Safe Reinforcement Learning},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2378-962X},
url = {https://doi.org/10.1145/3582577},
doi = {10.1145/3582577},
abstract = {Deep reinforcement learning (DRL) has shown good performance in tackling Markov decision process (MDP) problems. As DRL optimizes a long-term reward, it is a promising approach to improving the energy efficiency of data center cooling. However, enforcement of thermal safety constraints during DRL’s state exploration is a main challenge. The widely adopted reward shaping approach adds negative reward when the exploratory action results in unsafety. Thus, it needs to experience sufficient unsafe states before it learns how to prevent unsafety. In this paper, we propose a safety-aware DRL framework for data center cooling control. It applies offline imitation learning and online post-hoc rectification to holistically prevent thermal unsafety during online DRL. In particular, the post-hoc rectification searches for the minimum modification to the DRL-recommended action such that the rectified action will not result in unsafety. The rectification is designed based on a thermal state transition model that is fitted using historical safe operation traces and able to extrapolate the transitions to unsafe states explored by DRL. Extensive evaluation for chilled water and direct expansion-cooled data centers in two climate conditions show that our approach saves 18\% to 26.6\% of total data center power compared with conventional control and reduces safety violations by 94.5\% to 99\% compared with reward shaping. We also extend the proposed framework to address data centers with non-uniform temperature distributions for detailed safety considerations. The evaluation shows that our approach saves 14\% power usage compared with the PID control while addressing safety compliance during the training.},
note = {Just Accepted},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {feb},
keywords = {energy efficiency, Data center, proper orthogonal decomposition, computational fluid dynamics, safe reinforcement learning}
}

@inproceedings{10.1145/1255047.1255073,
author = {Merrick, Kathryn Elizabeth and Maher, Mary Lou},
title = {Motivated Reinforcement Learning for Adaptive Characters in Open-Ended Simulation Games},
year = {2007},
isbn = {9781595936400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1255047.1255073},
doi = {10.1145/1255047.1255073},
abstract = {Recently a new generation of virtual worlds has emerged in which users are provided with open-ended modelling tools with which they can create and modify world content. The result is evolving virtual spaces for commerce, education and social interaction. In general, these virtual worlds are not games and have no concept of winning, however the open-ended modelling capacity is nonetheless compelling. The rising popularity of open-ended virtual worlds suggests that there may also be potential for a new generation of computer games situated in open-ended environments. A key issue with the development of such games, however, is the design of non-player characters which can respond autonomously to unpredictable, open-ended changes to their environment. This paper considers the impact of open-ended modelling on character development in simulation games. Motivated reinforcement learning using context-free grammars is proposed as a means of representing unpredictable, evolving worlds for character reasoning. This technique is used to design adaptive characters for the Second Life virtual world to create a new kind of open-ended simulation game.},
booktitle = {Proceedings of the International Conference on Advances in Computer Entertainment Technology},
pages = {127–134},
numpages = {8},
keywords = {motivated reinforcement learning, computer games, adaptive characters, context-free grammar},
location = {Salzburg, Austria},
series = {ACE '07}
}

@inproceedings{10.1145/3447555.3464855,
author = {Zhang, Tianyu and Baasch, Gaby and Ardakanian, Omid and Evins, Ralph},
title = {On the Joint Control of Multiple Building Systems with Reinforcement Learning},
year = {2021},
isbn = {9781450383332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447555.3464855},
doi = {10.1145/3447555.3464855},
abstract = {Commercial buildings are comprised of multiple mechanical and electrical systems that work in tandem to provide a healthy, safe, and comfortable environment for occupants. These systems have complex interactions with each other, and consume a large amount of energy. In this paper, we apply three model-free deep reinforcement learning algorithms to jointly control HVAC and blind systems in a multi-zone test building, in scenarios with and without automatic dimming of the lights in response to daylight levels. The control agents are trained through interactions with a building simulator that generates traces for the movement of occupants. We investigate the three-way trade-off between energy use, thermal comfort, and visual comfort, and discuss how the joint control of the building systems could provide a better trade-off compared to when they are controlled separately. We compare the performance of the proposed control algorithms assuming the availability of occupancy data with two spatial resolutions, and confirm through experiments that a better trade-off can be achieved should zone-level occupancy information become available. Incorporating zone-level occupancy information, we show that 11.0\% and 31.8\% more energy can be saved respectively in heating and cooling seasons over existing rule-based baselines that control the same building systems.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Future Energy Systems},
pages = {60–72},
numpages = {13},
keywords = {Building Controls, Deep Reinforcement Learning, Energy Efficiency},
location = {Virtual Event, Italy},
series = {e-Energy '21}
}

@inproceedings{10.5555/3463952.3464034,
author = {Jiang, Zhengyao and Minervini, Pasquale and Jiang, Minqi and Rockt\"{a}schel, Tim},
title = {Grid-to-Graph: Flexible Spatial Relational Inductive Biases for Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Although reinforcement learning has been successfully applied in many domains in recent years, we still lack agents that can systematically generalize. While relational inductive biases that fit a task can improve generalization of RL agents, these biases are commonly hard-coded directly in the agent's neural architecture. In this work, we show that we can incorporate relational inductive biases, encoded in the form of relational graphs, into agents. Based on this insight, we propose Grid-to-Graph (GTG), a mapping from grid structures to relational graphs that carry useful spatial relational inductive biases when processed through a Relational Graph Convolution Network (R-GCN). We show that, with GTG, R-GCNs generalize better both in terms of in-distribution and out-of-distribution compared to baselines based on Convolutional Neural Networks and Neural Logic Machines on challenging procedurally generated environments and MinAtar. Furthermore, we show that GTG produces agents that can jointly reason over observations and environment dynamics encoded in knowledge bases.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {674–682},
numpages = {9},
keywords = {relational inductive bias, reinforcement learning, graph neural network},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3586102.3586129,
author = {Luo, Shaojie and Jin, Minhao and Han, Rongjie and Sun, Kexin},
title = {Anti-Jamming Based on Reinforcement Learning in Power System Sensing Network},
year = {2023},
isbn = {9781450397520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586102.3586129},
doi = {10.1145/3586102.3586129},
abstract = {The construction of power system sensing network requires high reliability, long-distance and low-cost sensing network. The characteristics of Long Range (LoRa) are very suitable for the construction of new power systems. However, LoRa is vulnerable to be attacked by jammer. In order to ensure the security and stability of the network, it is necessary to study the anti-jamming technology of Long Range Wide Area Network (LoRaWAN). In practice, it is difficult to know the jamming mode of jammer clearly, but reinforcement learning can effectively deal with jamming attacks without knowing the jamming model. In this paper, an anti-jamming strategy based on reinforcement learning is proposed to cope with jamming in power system sensing network. The scheme takes the total system capacity as a utility function. Facing unknown jamming models in power system sensing network, reinforcement learning can obtain the optimal anti-jamming scheme through continuous trial and error, and can improve the total system capacity. The effectiveness of the algorithm is verified by simulation. Compared with the random anti-jamming strategy, anti-jamming scheme based on reinforcement learning is found to be effective against jamming and the total system capacity is greatly improved.},
booktitle = {Proceedings of the 2022 12th International Conference on Communication and Network Security},
pages = {180–184},
numpages = {5},
keywords = {LoRa, Power System Sensing Network, Reinforcement Learning, Anti-jamming},
location = {Beijing, China},
series = {ICCNS '22}
}

@inproceedings{10.1145/3386367.3431670,
author = {Sacco, Alessio and Esposito, Flavio and Marchetto, Guido},
title = {A Distributed Reinforcement Learning Approach for Energy and Congestion-Aware Edge Networks},
year = {2020},
isbn = {9781450379489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386367.3431670},
doi = {10.1145/3386367.3431670},
abstract = {The abiding attempt of automation has also pervaded computer networks, with the ability to measure, analyze, and control themselves in an automated manner, by reacting to changes in the environment (e.g., demand) while exploiting existing flexibilities. When provided with these features, networks are often referred to as "self-driving". Network virtualization and machine learning are the drivers. In this regard, the provision and orchestration of physical or virtual resources are crucial for both Quality of Service guarantees and cost management in the edge/cloud computing ecosystem. Auto-scaling mechanisms are hence essential to effectively manage the lifecycle of network resources. In this poster, we propose Relevant, a distributed reinforcement learning approach to enable distributed automation for network orchestrators. Our solution aims at solving the congestion control problem within Software-Defined Network infrastructures, while being mindful of the energy consumption, helping resources to scale up and down as traffic demands fluctuate and energy optimization opportunities arise.},
booktitle = {Proceedings of the 16th International Conference on Emerging Networking EXperiments and Technologies},
pages = {546–547},
numpages = {2},
keywords = {self-driving networks, reinforcement learning, auto-scaling},
location = {Barcelona, Spain},
series = {CoNEXT '20}
}

@article{10.1145/3596519,
author = {Cavenaghi, Emanuele and Sottocornola, Gabriele and Stella, Fabio and Zanker, Markus},
title = {A Systematic Study on Reproducibility of Reinforcement Learning in Recommendation Systems},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3596519},
doi = {10.1145/3596519},
abstract = {Reproducibility is a main principle in science and fundamental to ensure scientific progress. However, many recent works point out that there are widespread deficiencies for this aspect in the AI field, making the reproducibility of results impractical or even impossible. We therefore studied the state of reproducibility support on the topic of Reinforcement Learning \&amp; Recommender Systems to analyse the situation in this context. We collected a total of 60 papers and analysed them by defining a set of variables to inspect the most important aspects that enable reproducibility, such as dataset, pre-processing code, hardware specifications, software dependencies, algorithm implementation, algorithm hyperparameters, and experiment code. Furthermore, we used the ACM Badges definitions assigning them to the selected papers. We discovered that, like in many other AI domains, the Reinforcement Learning \&amp; Recommender Systems field is grappling with a reproducibility crisis, as none of the selected papers were reproducible when strictly applying the ACM Badges definitions according to our analysis.},
journal = {ACM Trans. Recomm. Syst.},
month = {jul},
articleno = {11},
numpages = {23},
keywords = {ACM badges, Reproducibility}
}

@inproceedings{10.5555/3306127.3332033,
author = {Miyashita, Yuki and Sugawara, Toshiharu},
title = {Coordination Structures Generated by Deep Reinforcement Learning in Distributed Task Executions},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We investigate the coordination structures generated by deep Q-network (DQN) in a distributed task execution. Cooperation and coordination are the crucial issues in multi-agent systems, and very sophisticated design or learning is required in order to achieve effective structures or regimes of coordination. In this paper, we show the results that agents establish the division of labor in a bottom-up manner by determining their implicit responsible area when input structure for DQN is constituted by their own observation and absolute location.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2129–2131},
numpages = {3},
keywords = {coordination, cooperation, multi-agent deep reinforcement learning, divisional cooperation},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.5555/3535850.3536073,
author = {Liu, Guan-Ting and Lin, Guan-Yu and Cheng, Pu-Jen},
title = {Improving Generalization with Cross-State Behavior Matching in Deep Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Representation learning on visualized input is an essential yet challenging task for deep reinforcement learning (RL). To help the RL agent learn more general and discriminative representation among various states, we present cross-state self-constraint (CSSC). This novel technique regularizes representation learning by comparing state embedding similarities across different state-action pairs. We test our proposed method on the OpenAI Procgen benchmark with Rainbow and PPO and demonstrate significant improvement across most Procgen environments.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1675–1677},
numpages = {3},
keywords = {reinforcement learning, behavior matching, generalization},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/2831296.2831338,
author = {Valerio, Valerio Di and Petrioli, Chiara and Pescosolido, Loreto and Van Der Shaar, Mihaela},
title = {A Reinforcement Learning-Based Data-Link Protocol for Underwater Acoustic Communications},
year = {2015},
isbn = {9781450340366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2831296.2831338},
doi = {10.1145/2831296.2831338},
abstract = {We consider an underwater acoustic link where a sender transmits a flow of packets to a receiver through a channel with time varying quality. We address the problem of scheduling packets transmission, forward error correction (FEC) code selection, and channel probing to achieve the best trade-off between energy consumption and latency. Unlike previous works, which assume complete knowledge of the statistics of the underwater acoustic environment, we make the protocol learn the optimal behavior based on experience, without relying on any prior knowledge on the environment. We design a Reinforcement-Learning (RL)-based protocol which learns how to minimize a cost function which is a combination of delay and energy consumption, at the same time ensuring packet delivery. Starting from a basic Q-learning strategy, we design two learning algorithms to speed up learning time, and compare the performance of the proposed solutions with the Q-learning-based strategy and with an aggressive strategy which always transmits all the packets in the buffer. The results show that the proposed techniques outperform the aggressive policy and Q-learning, and are successful in achieving good tradeoffs between energy consumption and packet delivery latency (PDL).},
booktitle = {Proceedings of the 10th International Conference on Underwater Networks \&amp; Systems},
articleno = {2},
numpages = {5},
keywords = {underwater networks, Underwater communications, reinforcement learning, adaptive protocols},
location = {Arlington, VA, USA},
series = {WUWNet '15}
}

@inproceedings{10.1145/3539618.3591899,
author = {Wang, Kai and Zou, Zhene and Zhao, Minghao and Deng, Qilin and Shang, Yue and Liang, Yile and Wu, Runze and Shen, Xudong and Lyu, Tangjie and Fan, Changjie},
title = {RL4RS: A Real-World Dataset for Reinforcement Learning Based Recommender System},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591899},
doi = {10.1145/3539618.3591899},
abstract = {Reinforcement learning based recommender systems (RL-based RS) aim at learning a good policy from a batch of collected data, by casting recommendations to multi-step decision-making tasks. However, current RL-based RS research commonly has a large reality gap. In this paper, we introduce the first open-source real-world dataset, RL4RS, hoping to replace the artificial datasets and semi-simulated RS datasets previous studies used due to the resource limitation of the RL-based RS domain. Unlike academic RL research, RL-based RS suffers from the difficulties of being well-validated before deployment. We attempt to propose a new systematic evaluation framework, including evaluation of environment simulation, evaluation on environments, and counterfactual policy evaluation. In summary, the RL4RS (Reinforcement Learning for Recommender Systems), a new resource with special concerns on the reality gaps, contains two real-world datasets, data understanding tools, tuned simulation environments, related advanced RL baselines, batch RL baselines, and counterfactual policy evaluation algorithms. The RL4RS suite can be found at https://github.com/fuxiAIlab/RL4RS.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2935–2944},
numpages = {10},
keywords = {datasets, applied reinforcement learning, recommender systems},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@article{10.1145/3498327,
author = {Shiri, Aidin and Kallakuri, Uttej and Rashid, Hasib-Al and Prakash, Bharat and Waytowich, Nicholas R. and Oates, Tim and Mohsenin, Tinoosh},
title = {E2HRL: An Energy-Efficient Hardware Accelerator for Hierarchical Deep Reinforcement Learning},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3498327},
doi = {10.1145/3498327},
abstract = {Recently, Reinforcement Learning (RL) has shown great performance in solving sequential decision-making and control in dynamic environment problems. Despite its achievements, deploying Deep Neural Network (DNN)-based RL is expensive in terms of time and power due to the large number of episodes required to train agents with high dimensional image representations. Additionally, at the interference the large energy footprint of deep neural networks can be a major drawback. Embedded edge devices as the main platform for deploying RL applications are intrinsically resource-constrained and deploying deep neural network-based RL on them is a challenging task. As a result, reducing the number of actions taken by the RL agent to learn desired policy, along with the energy-efficient deployment of RL, is crucial. In this article, we propose Energy Efficient Hierarchical Reinforcement Learning (E2HRL), which is a scalable hardware architecture for RL applications. E2HRL utilizes a cross-layer design methodology for achieving better energy efficiency, smaller model size, higher accuracy, and system integration at the software and hardware layers. Our proposed model for RL agent is designed based on the learning hierarchical policies, which makes the network architecture more efficient for implementation on mobile devices. We evaluated our model in three different RL environments with different level of complexity. Simulation results with our analysis illustrate that hierarchical policy learning with several levels of control improves RL agents training efficiency and the agent learns the desired policy faster compared to a non-hierarchical model. This improvement is specifically more observable as the environment or the task becomes more complex with multiple objective subgoals. We tested our model with different hyperparameters to achieve the maximum reward by the RL agent while minimizing the model size, parameters, and required number of operations. E2HRL model enables efficient deployment of RL agent on resource-constraint-embedded devices with the proposed custom hardware architecture that is scalable and fully parameterized with respect to the number of input channels, filter size, and depth. The number of processing engines (PE) in the proposed hardware can vary between 1 to 8, which provides the flexibility of tradeoff of different factors such as latency, throughput, power, and energy efficiency. By performing a systematic hardware parameter analysis and design space exploration, we implemented the most energy-efficient hardware architectures of E2HRL on Xilinx Artix-7 FPGA and NVIDIA Jetson TX2. Comparing the implementation results shows Jetson TX2 boards achieve 0.1 ∼ 1.3 GOP/S/W energy efficiency while Artix-7 FPGA achieves 1.1 ∼ 11.4 GOP/S/W, which denotes 8.8\texttimes{} ∼ 11\texttimes{} better energy efficiency of E2HRL when model is implemented on FPGA. Additionally, compared to similar works our design shows better performance and energy efficiency.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {sep},
articleno = {45},
numpages = {19},
keywords = {FPGA, CNN, Reinforcement Learning, CPU, energy efficient hardware accelerator}
}

@inproceedings{10.1145/3404835.3463012,
author = {Zhao, Pu and Luo, Chuan and Zhou, Cheng and Qiao, Bo and He, Jiale and Zhang, Liangjie and Lin, Qingwei},
title = {RLNF: Reinforcement Learning Based Noise Filtering for Click-Through Rate Prediction},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463012},
doi = {10.1145/3404835.3463012},
abstract = {Click-through rate (CTR) prediction aims to recall the advertisements that users are interested in and to lead users to click, which is of critical importance for a variety of online advertising systems. In practice, CTR prediction is generally formulated as a conventional binary classification problem, where the clicked advertisements are positive samples and the others are negative samples. However, directly treating unclicked advertisements as negative samples would suffer from the severe label noise issue, since there exist many reasons why users are interested in a few advertisements but do not click. To address such serious issue, we propose a reinforcement learning based noise filtering approach, dubbed RLNF, which employs a noise filter to select effective negative samples. In RLNF, such selected, effective negative samples can be used to enhance the CTR prediction model, and meanwhile the effectiveness of the noise filter can be enhanced through reinforcement learning using the performance of CTR prediction model as reward. Actually, by alternating the enhancements of the noise filter and the CTR prediction model, the performance of both the noise filter and the CTR prediction model is improved. In our experiments, we equip 7 state-of-the-art CTR prediction models with RLNF. Extensive experiments on a public dataset and an industrial dataset present that RLNF significantly improves the performance of all these 7 CTR prediction models, which indicates both the effectiveness and the generality of RLNF.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2268–2272},
numpages = {5},
keywords = {reinforcement learning, CTR prediction, noise filtering},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.5555/1351542.1351973,
author = {Mattila, Ville},
title = {Flight Time Allocation for a Fleet of Aircraft through Reinforcement Learning},
year = {2007},
isbn = {1424413060},
publisher = {IEEE Press},
abstract = {Fighter aircraft are typically maintained periodically on the basis of cumulated usage hours. In a fleet of aircraft, the timing of the maintenance therefore depends on the allocation of flight time. A fleet with limited maintenance resources is faced with a design problem in assigning the aircraft to flight missions so that the overall amount of maintenance needs will not exceed the maintenance capacity. We consider the assignment of aircraft to flight missions as a Markov Decision Problem over a finite time horizon. The average availability of aircraft is taken as the optimization criterion. An efficient assignment policy is solved using a Reinforcement Learning technique called Q-learning. We compare the performance of the Q-learning algorithm to a set of heuristic assignment rules using problem instances that involve varying number of aircraft and types of periodic maintenance. Moreover, we consider the possibilities of practical implementation of the produced solutions.},
booktitle = {Proceedings of the 39th Conference on Winter Simulation: 40 Years! The Best is yet to Come},
pages = {2373},
numpages = {1},
location = {Washington D.C.},
series = {WSC '07}
}

@inproceedings{10.1145/3576842.3582383,
author = {Fahmida, Sezana and Modekurthy‬, Venkata Prashant and Rahman, Mahbubur and Saifullah, Abusayeed},
title = {Handling Coexistence of LoRa with Other Networks through Embedded Reinforcement Learning},
year = {2023},
isbn = {9798400700378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576842.3582383},
doi = {10.1145/3576842.3582383},
abstract = {The rapid growth of various Low-Power Wide-Area Network (LPWAN) technologies in the limited spectrum brings forth the challenge of their coexistence. Today, LPWANs are not equipped to handle this impending challenge. It is difficult to employ sophisticated media access control protocol for low-power nodes. Coexistence handling for WiFi or traditional short-range wireless network will not work for LPWANs. Due to long range, their nodes can be subject to an unprecedented number of hidden nodes, requiring highly energy-efficient techniques to handle such coexistence. In this paper, we address the coexistence problem for LoRa, a leading LPWAN technology. To improve the performance of a LoRa network under coexistence with many independent networks, we propose the design of a novel embedded learning agent based on a lightweight reinforcement learning at LoRa nodes. This is done by developing a Q-learning framework while ensuring minimal memory and computation overhead at LoRa nodes. The framework exploits transmission acknowledgments as feedback from the network based on what a node makes transmission decisions. To our knowledge, this is the first Q-learning approach for handling coexistence of low-power networks. Considering various coexistence scenarios of a LoRa network, we evaluate our approach through experiments indoors and outdoors. The outdoor results show that our Q-learning approach on average achieves an improvement of 46\% in packet reception rate while reducing energy consumption by 66\% in a LoRa network. In indoor experiments, we have observed some coexistence scenarios where a current LoRa network loses all the packets while our approach enables 99\% packet reception rate with up to 90\% improvement in energy consumption.},
booktitle = {Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation},
pages = {410–423},
numpages = {14},
keywords = {Internet-of-Things, Low Power Wide-Area Networks, IoT, Reinforcement Learning, Q-learning, LoRa},
location = {San Antonio, TX, USA},
series = {IoTDI '23}
}

@inproceedings{10.5555/3378680.3378717,
author = {Tabrez, Aaquib and Agrawal, Shivendra and Hayes, Bradley},
title = {Explanation-Based Reward Coaching to Improve Human Performance via Reinforcement Learning},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {For robots to effectively collaborate with humans, it is critical to establish a shared mental model amongst teammates. In the case of incongruous models, catastrophic failures may occur unless mitigating steps are taken. To identify and remedy these potential issues, we propose a novel mechanism for enabling an autonomous system to detect model disparity between itself and a human collaborator, infer the source of the disagreement within the model, evaluate potential consequences of this error, and finally, provide human-interpretable feedback to encourage model correction. This process effectively enables a robot to provide a human with a policy update based on perceived model disparity, reducing the likelihood of costly or dangerous failures during joint task execution. This paper makes two contributions at the intersection of explainable AI (xAI) and human-robot collaboration: 1) The Reward Augmentation and Repair through Explanation (RARE) framework for estimating task understanding and 2) A human subjects study illustrating the effectiveness of reward augmentation-based policy repair in a complex collaborative task.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {249–257},
numpages = {9},
keywords = {reward estimation, policy explanation, explainable AI, human-robot collaboration, joint task execution},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.1145/3308557.3308704,
author = {Leino, Katri and Todi, Kashyap and Oulasvirta, Antti and Kurimo, Mikko},
title = {Computer-Supported Form Design Using Keystroke-Level Modeling with Reinforcement Learning},
year = {2019},
isbn = {9781450366731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308557.3308704},
doi = {10.1145/3308557.3308704},
abstract = {The Keystroke-Level Model (KLM) is commonly used to predict a user's task completion times with graphical user interfaces. With KLM, the user's behavior is modeled with a linear function of independent, elementary operators. Each task can be completed with a sequence of operators. The policy, or the assumed sequence that the user executes, is typically pre-specified by the analyst. Using Reinforcement Learning (RL), RL-KLM [4] proposes an algorithmic method to obtain this policy automatically. This approach yields user-like policies in simple but realistic interaction tasks, and offers a quick way to obtain an upper bound for user performance.In this demonstration, we show how a policy is automatically learned by RL-KLM in form-filling tasks. A user can interact with the system by placing form fields onto a UI canvas. The system learns the fastest filling order for the form template according to Fitts' Law operators, and computes estimates the time required to complete the form. Attendees are able to iterate over their designs to see how the changes in designs affect user's policy and the task completion time.},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces: Companion},
pages = {85–86},
numpages = {2},
keywords = {reinforcement learning, keystroke-level modelling, computational evaluation, computational design},
location = {Marina del Ray, California},
series = {IUI '19}
}

@inproceedings{10.1145/3534678.3539063,
author = {Xue, Siqiao and Qu, Chao and Shi, Xiaoming and Liao, Cong and Zhu, Shiyi and Tan, Xiaoyu and Ma, Lintao and Wang, Shiyu and Wang, Shijun and Hu, Yun and Lei, Lei and Zheng, Yangfei and Li, Jianguo and Zhang, James},
title = {A Meta Reinforcement Learning Approach for Predictive Autoscaling in the Cloud},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539063},
doi = {10.1145/3534678.3539063},
abstract = {Predictive autoscaling (autoscaling with workload forecasting) is an important mechanism that supports autonomous adjustment of computing resources in accordance with fluctuating workload demands in the Cloud. In recent works, Reinforcement Learning (RL) has been introduced as a promising approach to learn the resource management policies to guide the scaling actions under the dynamic and uncertain cloud environment. However, RL methods face the following challenges in steering predictive autoscaling, such as lack of accuracy in decision-making, inefficient sampling and significant variability in workload patterns that may cause policies to fail at test time. To this end, we propose an end-to-end predictive meta model-based RL algorithm, aiming to optimally allocate resource to maintain a stable CPU utilization level, which incorporates a specially-designed deep periodic workload prediction model as the input and embeds the Neural Process [11, 16] to guide the learning of the optimal scaling actions over numerous application services in the Cloud. Our algorithm not only ensures the predictability and accuracy of the scaling strategy, but also enables the scaling decisions to adapt to the changing workloads with high sample efficiency. Our method has achieved significant performance improvement compared to the existing algorithms and has been deployed online at Alipay, supporting the autoscaling of applications for the world-leading payment platform.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4290–4299},
numpages = {10},
keywords = {autoscaling, reinforcement learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.5555/3091125.3091408,
author = {Prasad, Vignesh and Jangir, Rishabh and Balaraman, Ravindran and Krishna, K. Madhava},
title = {Data Driven Strategies for Active Monocular SLAM Using Inverse Reinforcement Learning},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Learning a complex task such as low-level robot manoeuvres while preventing failure of monocular SLAM is a challenging problem for both robots and humans. The data-driven identification of basic motion strategies in preventing monocular SLAM failure is a largely unexplored problem. We devise a computational model for representing and inferring strategies, formulated as Markov decision processes, where the reward function models the goal of the task as well as information about the strategy. We show how this reward function can be learnt from expert demonstrations using Inverse Reinforcement Learning. The resulting framework allows one to identify the way in which a few chosen parameters affect the quality of monocular SLAM estimates. The estimated reward function was able to capture expert demonstration information and the inherent expert strategy and it was possible to give an intuitive explanation to the obtained reward structure. A significant improvement in performance as compared to an intuitive hand-crafted reward function is also shown.},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {1697–1699},
numpages = {3},
keywords = {learning from demonstration, inverse reinforcement learning, robot navigation, active monocular slam},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@inproceedings{10.5555/3545946.3598751,
author = {Huber, Tobias and Demmler, Maximilian and Mertes, Silvan and Olson, Matthew L. and Andr\'{e}, Elisabeth},
title = {GANterfactual-RL: Understanding Reinforcement Learning Agents' Strategies through Visual Counterfactual Explanations},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Counterfactual explanations are a common tool to explain artificial intelligence models. For Reinforcement Learning (RL) agents, they answer "Why not?" or "What if?" questions by illustrating what minimal change to a state is needed such that an agent chooses a different action. Generating counterfactual explanations for RL agents with visual input is especially challenging because of their large state spaces and because their decisions are part of an overarching policy, which includes long-term decision-making. However, research focusing on counterfactual explanations, specifically for RL agents with visual input, is scarce and does not go beyond identifying defective agents. It is unclear whether counterfactual explanations are still helpful for more complex tasks like analyzing the learned strategies of different agents or choosing a fitting agent for a specific task. We propose a novel but simple method to generate counterfactual explanations for RL agents by formulating the problem as a domain transfer problem which allows the use of adversarial learning techniques like StarGAN. Our method is fully model-agnostic and we demonstrate that it outperforms the only previous method in several computational metrics. Furthermore, we show in a user study that our method performs best when analyzing which strategies different agents pursue.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1097–1106},
numpages = {10},
keywords = {explainable artificial intelligence, interpretable machine learning, explainable deep reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/1402383.1402428,
author = {Metzen, Jan Hendrik and Edgington, Mark and Kassahun, Yohannes and Kirchner, Frank},
title = {Analysis of an Evolutionary Reinforcement Learning Method in a Multiagent Domain},
year = {2008},
isbn = {9780981738109},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Many multiagent problems comprise subtasks which can be considered as reinforcement learning (RL) problems. In addition to classical temporal difference methods, evolutionary algorithms are among the most promising approaches for such RL problems. The relative performance of these approaches in certain subdomains (e. g. multiagent learning) of the general RL problem remains an open question at this time. In addition to theoretical analysis, benchmarks are one of the most important tools for comparing different RL methods in certain problem domains. A recently proposed multiagent RL benchmark problem is the RoboCup Keepaway benchmark. This benchmark is one of the most challenging multiagent learning problems because its state-space is continuous and high dimensional, and both the sensors and the actuators are noisy. In this paper we analyze the performance of the neuroevolutionary approach called Evolutionary Acquisition of Neural Topologies (EANT) in the Keepaway benchmark, and compare the results obtained using EANT with the results of other algorithms tested on the same benchmark.},
booktitle = {Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {291–298},
numpages = {8},
keywords = {reinforcement learning, neuroevolution},
location = {Estoril, Portugal},
series = {AAMAS '08}
}

@inproceedings{10.1145/3219819.3219961,
author = {Wang, Lu and Zhang, Wei and He, Xiaofeng and Zha, Hongyuan},
title = {Supervised Reinforcement Learning with Recurrent Neural Network for Dynamic Treatment Recommendation},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219961},
doi = {10.1145/3219819.3219961},
abstract = {Dynamic treatment recommendation systems based on large-scale electronic health records (EHRs) become a key to successfully improve practical clinical outcomes. Prior relevant studies recommend treatments either use supervised learning (e.g. matching the indicator signal which denotes doctor prescriptions), or reinforcement learning (e.g. maximizing evaluation signal which indicates cumulative reward from survival rates). However, none of these studies have considered to combine the benefits of supervised learning and reinforcement learning. In this paper, we propose Supervised Reinforcement Learning with Recurrent Neural Network (SRL-RNN), which fuses them into a synergistic learning framework. Specifically, SRL-RNN applies an off-policy actor-critic framework to handle complex relations among multiple medications, diseases and individual characteristics. The "actor'' in the framework is adjusted by both the indicator signal and evaluation signal to ensure effective prescription and low mortality. RNN is further utilized to solve the Partially-Observed Markov Decision Process (POMDP) problem due to lack of fully observed states in real world applications. Experiments on the publicly real-world dataset, i.e., MIMIC-3, illustrate that our model can reduce the estimated mortality, while providing promising accuracy in matching doctors' prescriptions.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {2447–2456},
numpages = {10},
keywords = {supervised reinforcement learning, dynamic treatment regime, deep sequential recommendation},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3538969.3539006,
author = {Merzouk, Mohamed Amine and Delas, Jos\'{e}phine and Neal, Christopher and Cuppens, Fr\'{e}d\'{e}ric and Boulahia-Cuppens, Nora and Yaich, Reda},
title = {Evading Deep Reinforcement Learning-Based Network Intrusion Detection with Adversarial Attacks},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538969.3539006},
doi = {10.1145/3538969.3539006},
abstract = {An Intrusion Detection System (IDS) aims to detect attacks conducted over computer networks by analyzing traffic data. Deep Reinforcement Learning (Deep-RL) is a promising lead in IDS research, due to its lightness and adaptability. However, the neural networks on which Deep-RL is based can be vulnerable to adversarial attacks. By applying a well-computed modification to malicious traffic, adversarial examples can evade detection. In this paper, we test the performance of a state-of-the-art Deep-RL IDS agent against the Fast Gradient Sign Method (FGSM) and Basic Iterative Method (BIM) adversarial attacks. We demonstrate that the performance of the Deep-RL detection agent is compromised in the face of adversarial examples and highlight the need for future Deep-RL IDS work to consider mechanisms for coping with adversarial examples.},
booktitle = {Proceedings of the 17th International Conference on Availability, Reliability and Security},
articleno = {31},
numpages = {6},
keywords = {reinforcement learning, evasion attacks, adversarial machine learning, adversarial examples, intrusion detection},
location = {Vienna, Austria},
series = {ARES '22}
}

@inproceedings{10.1145/3573834.3574534,
author = {Wang, Huimin and Liang, Dong and Xi, Yuliang},
title = {Urban Path Planning Based on Improved Model-Based Reinforcement Learning Algorithm},
year = {2023},
isbn = {9781450397933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573834.3574534},
doi = {10.1145/3573834.3574534},
abstract = {With the development of the urban economy, and the continuous expansion of vehicle scale, trafﬁc congestion has become the most serious problem affecting contemporary urban development. Using advanced road network information perception and transmission technologies, path planning under real-time road conditions has become an important means to solve this problem. Previously, our proposed model-based reinforcement learning multipath planning algorithm realized the rapid response of the path planning result, alleviating congestion drift to a certain extent. However, further research shows that the model performs poorly in extreme road network environments (the road network trafﬁc pressure is 0) and cannot explore the complete path, the main reason is that the effect of model hyperparameters on the convergence of the algorithm was ignored. to solve this problems, we explore the hyperparameters in detail, especially discuss the discount factor γ and the finalReward to the model convergence by using Shenzhen road network data. the results show that when the discount factor γ and the finalReward value satisfy certain conditions, which is obtained in this study, the improved model-based method can guarantee the convergence stability of the algorithm under extreme road network environments. This paper reveals the importance of the design of hyperparameters γ and finalReward as well as their interrelationship on the convergence of reinforcement learning algorithms and we hope to give some insights in the field which explore hyperparameters of reinforcement learning algorithm.},
booktitle = {Proceedings of the 4th International Conference on Advanced Information Science and System},
articleno = {55},
numpages = {6},
keywords = {model convergence, path planning, model-based reinforcement learning, reward function},
location = {Sanya, China},
series = {AISS '22}
}

@inproceedings{10.5555/3398761.3398816,
author = {Ghiassian, Sina and Rafiee, Banafsheh and Lo, Yat Long and White, Adam},
title = {Improving Performance in Reinforcement Learning by Breaking Generalization in Neural Networks},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Reinforcement learning systems require good representations to work well. For decades practical success in reinforcement learning was limited to small domains. Deep reinforcement learning systems, on the other hand, are scalable, not dependent on domain specific prior knowledge and have been successfully used to play Atari, in 3D navigation from pixels, and to control high degree of freedom robots. Unfortunately, the performance of deep reinforcement learning systems is sensitive to hyper-parameter settings and architecture choices. Even well tuned systems exhibit significant instability both within a trial and across experiment replications. In practice, significant expertise and trial and error are usually required to achieve good performance. One potential source of the problem is known as catastrophic interference: when later training decreases performance by overriding previous learning. Interestingly, the powerful generalization that makes Neural Networks (NN) so effective in batch supervised learning might explain the challenges when applying them in reinforcement learning tasks. In this paper, we explore how online NN training and interference interact in reinforcement learning. We find that simply re-mapping the input observations to a high-dimensional space improves learning speed and parameter sensitivity. We also show this preprocessing reduces interference in prediction tasks. More practically, we provide a simple approach to NN training that is easy to implement, and requires little additional computation. We demonstrate that our approach improves performance in both prediction and control with an extensive batch of experiments in classic control domains.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {438–446},
numpages = {9},
keywords = {interference, neural networks, reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3594315.3594399,
author = {Wu, Keyu and Wu, Fengge and Lin, Yijun and Zhao, Junsuo},
title = {Stable Control Policy and Transferable Reward Function via Inverse Reinforcement Learning},
year = {2023},
isbn = {9781450399029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594315.3594399},
doi = {10.1145/3594315.3594399},
abstract = {Inverse reinforcement learning (IRL) can solve the problem of complex reward function shaping by learning from expert data. However, it is challenging to train when the expert data is insufficient, and its stability is difficult to guarantee. Moreover, the reward function of mainstream IRL can only adapt to subtle environmental changes. It cannot be directly transferred to a similar task scenario, so the generalization ability still needs to be improved. To address these issues, we propose an IRL algorithm to obtain a stable control policy and transferable reward function (ST-IRL). Firstly, by introducing the Wasserstein metric and adversarial training, we solve the problem that IRL is challenging to train in a new environment with little expert data. Secondly, we add state marginal matching (SMM), hyperparameter comparison and optimizer evaluation to address the model's generalisability problem. As a result, the control policy obtained by ST-IRL achieves outstanding control results in all four Mujoco benchmarks. Furthermore, in both the custom Ant and PointMaze environments, the reward function obtained by our algorithm exhibits promising transferability.},
booktitle = {Proceedings of the 2023 9th International Conference on Computing and Artificial Intelligence},
pages = {733–742},
numpages = {10},
keywords = {Adversarial training, Wasserstein metric, Inverse reinforcement learning, Reward function},
location = {Tianjin, China},
series = {ICCAI '23}
}

@inproceedings{10.1145/3357384.3357945,
author = {Zhang, Junqi and Mao, Jiaxin and Liu, Yiqun and Zhang, Ruizhe and Zhang, Min and Ma, Shaoping and Xu, Jun and Tian, Qi},
title = {Context-Aware Ranking by Constructing a Virtual Environment for Reinforcement Learning},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357945},
doi = {10.1145/3357384.3357945},
abstract = {Result ranking is one of the major concerns for Web search technologies. Most existing methodologies rank search results in descending order according to pointwise relevance estimation of single results. However, the dependency relationship between different search results are not taken into account. While search engine result pages contain more and more heterogenous components, a better ranking strategy should be a context-aware process and optimize result ranking globally. In this paper, we propose a novel framework which aims to improve context-aware listwise ranking performance by optimizing online evaluation metrics. The ranking problem is formalized as a Markov Decision Process (MDP) and solved with the reinforcement learning paradigm. To avoid the great cost to online systems during the training of the ranking model, we construct a virtual environment with millions of historical click logs to simulate the behavior of real users. Extensive experiments on both simulated and real datasets show that: 1) constructing a virtual environment can effectively leverage the large scale click logs and capture some important properties of real users. 2) the proposed framework can improve search ranking performance by a large margin.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1603–1612},
numpages = {10},
keywords = {context, heterogeneous result, ranking, reinforcement learning},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.5555/3432601.3432613,
author = {Ryu, Bon and An, Aijun and Rashidi, Zana and Liu, Junfeng and Hu, Yonggang},
title = {Towards Topology Aware Pre-Emptive Job Scheduling with Deep Reinforcement Learning},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {We present a topology aware Deep Reinforcement Learning (DRL) scheduler that simultaneously chooses jobs to run and elastically allocates resources to them for Distributed Deep Learning data parallel jobs in a multi-GPU, multi-machine cluster. This work addresses multiple limitations in the state-of-the-art methods: 1) Not sufficiently accounting for the bandwidth sharing between multiple jobs running simultaneously in a cluster, 2) Using overly simply heuristics to solve the resource allocation problem, 3) Pretending that job speed is not affected by the topology of allocated resources in simulation environments. This DRL method calculates unique job speeds by taking advantage of a graph representation of the cluster topology. This enables modeling realistic sharing of inter and intra machine bandwidths such as QPI speed, CPU-GPU speed, GPU-GPU speed, Infiniband card to Top-of-Rack Switch, etc. Our neural network model is trained using the REINFORCE algorithm which is a policy gradient method. The model outputs a multiple softmax designed to represent an assignment table that specifies the resource allocation of GPU's to Jobs. Using this design we can dynamically choose/change which GPUs to assign to which jobs at discrete time steps. Our simulation experiments show that our method can outperform baseline schedulers that use heuristics for job picking and resource allocation.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {83–92},
numpages = {10},
keywords = {reinforcement learning, GPU job scheduling, neural networks},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1145/3583131.3590376,
author = {France, Kordel K. and Sheppard, John W.},
title = {Factored Particle Swarm Optimization for Policy Co-Training in Reinforcement Learning},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590376},
doi = {10.1145/3583131.3590376},
abstract = {Uncertainty of the environment limits the circumstances with which any optimization problem can provide meaningful information. Multiple optimizers can combat this problem by communicating different information through cooperative coevolution. In reinforcement learning (RL), uncertainty can be reduced by applying learned policies collaboratively with another agent. Here, we propose policy Co-training with Factored Evolutionary Algorithms (CoFEA) to evolve an optimal policy for such scenarios. We hypothesize that self-paced co-training can allow factored particle swarms with imperfect knowledge to consolidate knowledge from each of their imperfect policies in order to approximate a single optimal policy. Additionally, we show how the performance of co-training swarms of RL agents can be maximized through the specific use of Expected SARSA as the policy learner. We evaluate CoFEA against comparable RL algorithms and attempt to establish limits for which our procedure does and does not provide benefit. Our results indicate that Particle Swarm Optimization (PSO) is effective in training multiple agents under uncertainty and that FEA reduces swarm and policy updates. This paper contributes to the field of cooperative co-evolutionary algorithms by proposing a method by which factored evolutionary techniques can significantly improve how multiple RL agents collaborate under extreme uncertainty to solve complex tasks faster than a single agent can under identical conditions.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {30–38},
numpages = {9},
keywords = {factored evolutionary algorithms, reinforcement learning, co-training},
location = {Lisbon, Portugal},
series = {GECCO '23}
}

@inproceedings{10.1145/1329125.1329247,
author = {Gomes, Eduardo Rodrigues and Kowalczyk, Ryszard},
title = {Reinforcement Learning with Utility-Aware Agents for Market-Based Resource Allocation},
year = {2007},
isbn = {9788190426275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1329125.1329247},
doi = {10.1145/1329125.1329247},
abstract = {In this paper we propose and investigate the use of Reinforcement Learning in a market-based resource allocation mechanism called Iterative Price Adjustment. Under standard assumptions, this mechanism uses demand functions that do not allow the agents to have preferences over the attributes of the allocation, e.g. the price of the resources. To address this limitation, we study the case where the agent's preferences in the resource allocation are described by utility functions and they learn the demand functions given their utility functions. The approach has been evaluated with extensive experiments.},
booktitle = {Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems},
articleno = {99},
numpages = {3},
keywords = {market-based resource allocation, reinforcement learning},
location = {Honolulu, Hawaii},
series = {AAMAS '07}
}

@inproceedings{10.1145/3563357.3566166,
author = {Nonaka, Shotaro and Watari, Daichi and Taniguchi, Ittetsu and Onoye, Takao},
title = {Deep Reinforcement Learning-Based SOH-Aware Battery Management for DER Aggregation},
year = {2022},
isbn = {9781450398909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563357.3566166},
doi = {10.1145/3563357.3566166},
abstract = {In smart energy systems, batteries, which assume an important role in filling the temporal gap between generation and consumption, are expected to be a potential distributed energy resource (DER). A resource aggregator (RA) has emerged to collect various DERs to extract demand-side flexibility, and various methods have been proposed based on reinforcement learning. Since battery degradation is unavoidable during utilization, battery management is required to minimize it. This paper proposes state-of-health (SOH)-aware battery management based on deep reinforcement learning. Our experimental results demonstrate an average battery lifetime improvement of 11.2\%.},
booktitle = {Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {471–474},
numpages = {4},
keywords = {SOH (state-of-health), DER (distributed energy resources), battery management, aggregation},
location = {Boston, Massachusetts},
series = {BuildSys '22}
}

@inproceedings{10.1145/3397271.3401196,
author = {Cao, Yuanjiang and Chen, Xiaocong and Yao, Lina and Wang, Xianzhi and Zhang, Wei Emma},
title = {Adversarial Attacks and Detection on Reinforcement Learning-Based Interactive Recommender Systems},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401196},
doi = {10.1145/3397271.3401196},
abstract = {Adversarial attacks pose significant challenges for detecting adversarial attacks at an early stage. We propose attack-agnostic detection on reinforcement learning-based interactive recommendation systems. We first craft adversarial examples to show their diverse distributions and then augment recommendation systems by detecting potential attacks with a deep learning-based classifier based on the crafted data. Finally, we study the attack strength and frequency of adversarial examples and evaluate our model on standard datasets with multiple crafting methods. Our extensive experiments show that most adversarial attacks are effective, and both attack strength and attack frequency impact the attack performance. The strategically-timed attack achieves comparative attack performance with only 1/3 to 1/2 attack frequency. Besides, our black-box detector trained with one crafting method has the generalization ability over several crafting methods.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1669–1672},
numpages = {4},
keywords = {reinforcement learning, adversarial attack, interactive recommender system, adversarial examples detection},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3308558.3313433,
author = {Li, Minne and Qin, Zhiwei and Jiao, Yan and Yang, Yaodong and Wang, Jun and Wang, Chenxi and Wu, Guobin and Ye, Jieping},
title = {Efficient Ridesharing Order Dispatching with Mean Field Multi-Agent Reinforcement Learning},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313433},
doi = {10.1145/3308558.3313433},
abstract = {A fundamental question in any peer-to-peer ridesharing system is how to, both effectively and efficiently, dispatch user's ride requests to the right driver in real time. Traditional rule-based solutions usually work on a simplified problem setting, which requires a sophisticated hand-crafted weight design for either centralized authority control or decentralized multi-agent scheduling systems. Although recent approaches have used reinforcement learning to provide centralized combinatorial optimization algorithms with informative weight values, their single-agent setting can hardly model the complex interactions between drivers and orders. In this paper, we address the order dispatching problem using multi-agent reinforcement learning (MARL), which follows the distributed nature of the peer-to-peer ridesharing problem and possesses the ability to capture the stochastic demand-supply dynamics in large-scale ridesharing scenarios. Being more reliable than centralized approaches, our proposed MARL solutions could also support fully distributed execution through recent advances in the Internet of Vehicles (IoV) and the Vehicle-to-Network (V2N). Furthermore, we adopt the mean field approximation to simplify the local interactions by taking an average action among neighborhoods. The mean field approximation is capable of globally capturing dynamic demand-supply variations by propagating many local interactions between agents and the environment. Our extensive experiments have shown the significant improvements of MARL order dispatching algorithms over several strong baselines on the accumulated driver income (ADI), and order response rate measures. Besides, the simulated experiments with real data have also justified that our solution can alleviate the supply-demand gap during the rush hours, thus possessing the capability of reducing traffic congestion.},
booktitle = {The World Wide Web Conference},
pages = {983–994},
numpages = {12},
keywords = {Order Dispatching, Multi-Agent Reinforcement Learning, Mean Field Reinforcement Learning},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3550471.3564762,
author = {Kim, Sunwoo and Sorokin, Maks and Lee, Jehee and Ha, Sehoon},
title = {HumanConQuad: Human Motion Control of Quadrupedal Robots Using Deep Reinforcement Learning},
year = {2022},
isbn = {9781450394727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550471.3564762},
doi = {10.1145/3550471.3564762},
abstract = {Robotic creatures are capable of entering hazardous environments instead of human workers, but it is challenging to develop a fully autonomous agent that can work independently in unstructured scenes. We propose a human motion-based control interface for quadrupedal robots that promises adaptable robot operations by reflecting the user’s intuition directly to the robot’s movements. Designing motion interface for different morphologies conveys tricky problems in solving dynamics and control strategies. We first retarget the captured human motion into the corresponding robot’s kinematic space with proper semantics using supervised learning and post-processing techniques. Second, we build the motion imitation controller to track the given retargeted motion using deep reinforcement learning with task-based curriculums. Finally, we apply domain randomization during training for real-world deployment. (Video1)},
booktitle = {SIGGRAPH Asia 2022 Emerging Technologies},
articleno = {5},
numpages = {2},
keywords = {HumanConQuad, Deep Reinforcement Learning, Hetero-Morphological Control, Physcis Simulation, Human Motion-Based Control},
location = {Daegu, Republic of Korea},
series = {SA '22}
}

@inproceedings{10.1145/3184066.3184094,
author = {Valdivia, Andr\'{e} and Quispe, Jose Herrera and Barrios-Aranibar, Dennis},
title = {A New Approach for Supervised Learning Based Influence Value Reinforcement Learning},
year = {2018},
isbn = {9781450363365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3184066.3184094},
doi = {10.1145/3184066.3184094},
abstract = {The neural self-organization, is an innate feature of the mammal's brains, and is necessary for its operation. The most known neuronal models that use this characteristic are the self-organized maps (SOM) and the adaptive resonance theory (ART), but those models, did not take the neuron as a processing unit, as the biological counterpart. On the other hand, the influence value learning paradigm [1], used in multi-agent environments, proof that agents can communicate with each other [2]; and they can self-organize to assign tasks; without any interference. Motivated by this missing feature in artificial networks, and with the influence value reinforcement learning algorithm; a new approach to supervised learning was modeled using the neuron as an agent learning by reinforcement.},
booktitle = {Proceedings of the 2nd International Conference on Machine Learning and Soft Computing},
pages = {24–28},
numpages = {5},
keywords = {neural networks, reinforcement learning, multi-agent},
location = {Phu Quoc Island, Viet Nam},
series = {ICMLSC '18}
}

@inproceedings{10.1145/3356464.3357712,
author = {Wang, Weixun and Hao, Jianye and Wang, Yixi and Taylor, Matthew},
title = {Achieving Cooperation through Deep Multiagent Reinforcement Learning in Sequential Prisoner's Dilemmas},
year = {2019},
isbn = {9781450376563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356464.3357712},
doi = {10.1145/3356464.3357712},
abstract = {The Iterated Prisoner's Dilemma has guided research on social dilemmas for decades. However, it distinguishes between only two atomic actions: cooperate and defect. In real-world prisoner's dilemmas, these choices are temporally extended and different strategies may correspond to sequences of actions, reflecting grades of cooperation. We introduce a Sequential Prisoner's Dilemma (SPD) game to better capture the aforementioned characteristics. In this work, we propose a deep multiagent reinforcement-learning approach that investigates the evolution of mutual cooperation in SPD games. Our approach consists of two phases. The first phase is offline: it synthesizes policies with different cooperation degrees and then trains a cooperation degree detection network. The second phase is online: an agent adaptively selects its policy based on the detected degree of opponent cooperation. The effectiveness of our approach is demonstrated in two representative SPD 2D games: the Apple-Pear game and the Fruit Gathering game. Experimental results show that our strategy can avoid being exploited by exploitative opponents and achieve cooperation with cooperative opponents.},
booktitle = {Proceedings of the First International Conference on Distributed Artificial Intelligence},
articleno = {11},
numpages = {7},
keywords = {mutual cooperation, sequential prisoner's dilemmas, opponent model, deep multiagent reinforcement learning},
location = {Beijing, China},
series = {DAI '19}
}

@inproceedings{10.1145/3472456.3473514,
author = {Wang, Haoyu and Shen, Haiying and Gao, Jiechao and Zheng, Kevin and Li, Xiaoying},
title = {Multi-Agent Reinforcement Learning Based Distributed Renewable Energy Matching for Datacenters},
year = {2021},
isbn = {9781450390682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472456.3473514},
doi = {10.1145/3472456.3473514},
abstract = {The rapid growth of cloud computing in cloud datacenters in recent decades greatly increases the brown energy consumption in datacenters, and hence significant increase of carbon emission that negatively impacts on the environment as well as the monetary cost. More and more cloud service providers are adopting renewable energy as the energy supply to offset the consumption of brown energy. Meanwhile, an increasing number of renewable energy generators have been built to meet the needs. However, the instability of the renewable energy cannot guarantee the support to the datacenter and the energy competition of different datacenters may lead to datacenter energy outage. In this paper, we focus on the problem of how to match different renewable energy generators to the datacenters from different cloud providers to minimize the carbon emission, monetary cost, and service level objective (SLO) violation due to renewable energy shortage. The challenges here are that the datacenters may compete in energy requesting, the renewable energy generation is not stable and the decision should be made quickly. There have been no previous efforts devoting to this problem. To solve the problem, we first test several machine learning techniques on long-term prediction accuracy on renewable energy generation and energy demand using real traces and identify SARIMA for the prediction. We then propose a multi-agent reinforcement learning based method (MARL) for each datacenter to determine how much renewable energy to request from each generator based on the predicted results. We also propose a deadline guaranteed job postponement method (DGJP) to postpone executing unurgent jobs upon insufficient renewable energy supply. The trace-driven experiments show that MARL outperforms other methods by increasing up to 35\% SLO satisfaction ratio, and reducing up to 19\% (0.33 billion dollars in 90 days) total monetary cost and 33\% total carbon emission, and DGJP further improves the performance.},
booktitle = {Proceedings of the 50th International Conference on Parallel Processing},
articleno = {37},
numpages = {10},
location = {Lemont, IL, USA},
series = {ICPP '21}
}

@inproceedings{10.1145/3579654.3579702,
author = {Yu, Jiali and Wu, Fengge and Zhao, Junsuo},
title = {Trust Region Method Using K-FAC in Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450398336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579654.3579702},
doi = {10.1145/3579654.3579702},
abstract = {A challenging problem in multi-agent reinforcement learning (MARL) is to ensure that the policy converges quickly and is effective with limited computing resources. This paper extends the second-order optimization to MARL using Kronecker-factored approximate curvature (K-FAC) to approximate the natural gradient update. And it solves the challenge of training policy networks in MARL which requires a lot of time and computing costs. We propose a Heterogeneous-agent Trust Region algorithm using K-FAC (HAKTR). Further more, we endow HAKTR with monotonic performance improvement based on the multi-agent advantage decomposition theorem. Our algorithm is evaluated on continuous tasks in the MuJoCo environment. The experimental results show that HAKTR can achieve higher rewards with less computing costs compared to the baselines such as HATRPO and HAPPO. Moreover, HAKTR has good scalability regarding the number of agents and can be applied to large-scale networks.},
booktitle = {Proceedings of the 2022 5th International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {45},
numpages = {7},
keywords = {Deep Reinforcement Learning, Kronecker-factored approximated curvature;, Multi-Agent Reinforcement Learning},
location = {Sanya, China},
series = {ACAI '22}
}

@inproceedings{10.1145/3568231.3568258,
author = {Setyawan, Gembong Edhi and Hartono, Pitoyo and Sawada, Hideyuki},
title = {An In-Depth Analysis of Cooperative Multi-Robot Hierarchical Reinforcement Learning},
year = {2023},
isbn = {9781450397117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568231.3568258},
doi = {10.1145/3568231.3568258},
abstract = {In most cases, it is known that Multi-Robot System (MRS) performs more effectively than a Single-Robot System (SRS) in complex environments. Therefore, Deep Reinforcement Learning (DRL) has been gaining interest in training robots to solve tasks in complex environments cooperatively. However, Deep Reinforcement Learning continues to be hampered by challenges in complex environments. Recently, the Multi-Agent Hierarchical Deep Deterministic Policy Gradient (MH-DDPG) has been proposed for implementing hierarchical learning for a group of robots engaged in collaboratively problem-solving. This study provides a more in-depth investigation of the performance of the MH-DDPG algorithm by examining the influence of variations in the number of robots, the sensitivity settings, and the hierarchical levels. The experimental results reveal performance tradeoffs regarding these parameters, which should be considered following the task and environment complexity.},
booktitle = {Proceedings of the 7th International Conference on Sustainable Information Engineering and Technology},
pages = {119–126},
numpages = {8},
location = {Malang, Indonesia},
series = {SIET '22}
}

@inproceedings{10.5555/2392800.2392815,
author = {Misu, Teruhisa and Georgila, Kallirroi and Leuski, Anton and Traum, David},
title = {Reinforcement Learning of Question-Answering Dialogue Policies for Virtual Museum Guides},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We use Reinforcement Learning (RL) to learn question-answering dialogue policies for a real-world application. We analyze a corpus of interactions of museum visitors with two virtual characters that serve as guides at the Museum of Science in Boston, in order to build a realistic model of user behavior when interacting with these characters. A simulated user is built based on this model and used for learning the dialogue policy of the virtual characters using RL. Our learned policy outperforms two baselines (including the original dialogue policy that was used for collecting the corpus) in a simulation setting.},
booktitle = {Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
pages = {84–93},
numpages = {10},
location = {Seoul, South Korea},
series = {SIGDIAL '12}
}

@inproceedings{10.1145/1143997.1144202,
author = {Taylor, Matthew E. and Whiteson, Shimon and Stone, Peter},
title = {Comparing Evolutionary and Temporal Difference Methods in a Reinforcement Learning Domain},
year = {2006},
isbn = {1595931864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143997.1144202},
doi = {10.1145/1143997.1144202},
abstract = {Both genetic algorithms (GAs) and temporal difference (TD) methods have proven effective at solving reinforcement learning (RL) problems. However, since few rigorous empirical comparisons have been conducted, there are no general guidelines describing the methods' relative strengths and weaknesses. This paper presents the results of a detailed empirical comparison between a GA and a TD method in Keepaway, a standard RL benchmark domain based on robot soccer. In particular, we compare the performance of NEAT [19], a GA that evolves neural networks, with Sarsa [16, 17], a popular TD method. The results demonstrate that NEAT can learn better policies in this task, though it requires more evaluations to do so. Additional experiments in two variations of Keepaway demonstrate that Sarsa learns better policies when the task is fully observable and NEAT learns faster when the task is deterministic. Together, these results help isolate the factors critical to the performance of each method and yield insights into their general strengths and weaknesses.},
booktitle = {Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation},
pages = {1321–1328},
numpages = {8},
keywords = {performance analysis, empirical study, genetic algorithms, machine learning},
location = {Seattle, Washington, USA},
series = {GECCO '06}
}

@inproceedings{10.1145/3140549.3140552,
author = {Venkatesan, Sridhar and Albanese, Massimiliano and Shah, Ankit and Ganesan, Rajesh and Jajodia, Sushil},
title = {Detecting Stealthy Botnets in a Resource-Constrained Environment Using Reinforcement Learning},
year = {2017},
isbn = {9781450351768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140549.3140552},
doi = {10.1145/3140549.3140552},
abstract = {Modern botnets can persist in networked systems for extended periods of time by operating in a stealthy manner. Despite the progress made in the area of botnet prevention, detection, and mitigation, stealthy botnets continue to pose a significant risk to enterprises. Furthermore, existing enterprise-scale solutions require significant resources to operate effectively, thus they are not practical. In order to address this important problem in a resource-constrained environment, we propose a reinforcement learning based approach to optimally and dynamically deploy a limited number of defensive mechanisms, namely honeypots and network-based detectors, within the target network. The ultimate goal of the proposed approach is to reduce the lifetime of stealthy botnets by maximizing the number of bots identified and taken down through a sequential decision-making process. We provide a proof-of-concept of the proposed approach, and study its performance in a simulated environment. The results show that the proposed approach is promising in protecting against stealthy botnets.},
booktitle = {Proceedings of the 2017 Workshop on Moving Target Defense},
pages = {75–85},
numpages = {11},
keywords = {botnets, reinforcement learning, intrusion detection},
location = {Dallas, Texas, USA},
series = {MTD '17}
}

@inproceedings{10.1145/3543873.3587623,
author = {Lin, Baihan and Cecchi, Guillermo and Bouneffouf, Djallel},
title = {Psychotherapy AI Companion with Reinforcement Learning Recommendations and Interpretable Policy Dynamics},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587623},
doi = {10.1145/3543873.3587623},
abstract = {We introduce a Reinforcement Learning Psychotherapy AI Companion that generates topic recommendations for therapists based on patient responses. The system uses Deep Reinforcement Learning (DRL) to generate multi-objective policies for four different psychiatric conditions: anxiety, depression, schizophrenia, and suicidal cases. We present our experimental results on the accuracy of recommended topics using three different scales of working alliance ratings: task, bond, and goal. We show that the system is able to capture the real data (historical topics discussed by the therapists) relatively well, and that the best performing models vary by disorder and rating scale. To gain interpretable insights into the learned policies, we visualize policy trajectories in a 2D principal component analysis space and transition matrices. These visualizations reveal distinct patterns in the policies trained with different reward signals and trained on different clinical diagnoses. Our system’s success in generating DIsorder-Specific Multi-Objective Policies (DISMOP) and interpretable policy dynamics demonstrates the potential of DRL in providing personalized and efficient therapeutic recommendations.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {932–939},
numpages = {8},
keywords = {deep reinforcement learning, recommendation system, psychotherapy, computational psychiatry, natural language processing},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

