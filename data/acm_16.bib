@inproceedings{10.1145/3543873.3587661,
author = {Naik, Abhishek and Chang, Bo and Karatzoglou, Alexandros and Mladenov, Martin and Chi, Ed H. and Chen, Minmin},
title = {Investigating Action-Space Generalization in Reinforcement Learning for Recommendation Systems},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587661},
doi = {10.1145/3543873.3587661},
abstract = {Recommender systems are used to suggest items to users based on the users’ preferences. Such systems often deal with massive item sets and incredibly sparse user-item interactions, which makes it very challenging to generate high-quality personalized recommendations. Reinforcement learning (RL) is a framework for sequential decision making and naturally formulates recommender-system tasks: recommending items as actions in different user and context states to maximize long-term user experience. We investigate two RL policy parameterizations that generalize sparse user-items interactions by leveraging the relationships between actions: parameterizing the policy over action features as a softmax or Gaussian distribution. Our experiments on synthetic problems suggest that the Gaussian parameterization—which is not commonly used on recommendation tasks—is more robust to the set of action features than the softmax parameterization. Based on these promising results, we propose a more thorough investigation of the theoretical properties and empirical benefits of the Gaussian parameterization for recommender systems.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {966–972},
numpages = {7},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3394810.3394815,
author = {Seetanadi, Gautham Nayak and \r{A}rz\'{e}n, Karl-Erik and Maggio, Martina},
title = {Adaptive Routing with Guaranteed Delay Bounds Using Safe Reinforcement Learning},
year = {2020},
isbn = {9781450375931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394810.3394815},
doi = {10.1145/3394810.3394815},
abstract = {Time-critical networks require strict delay bounds on the transmission time of packets from source to destination. Routes for transmissions are usually statically determined, using knowledge about worst-case transmission times between nodes. This is generally a conservative method, that guarantees transmission times but does not provide any optimization for the typical case. In real networks, the typical delays vary from those considered during static route planning. The challenge in such a scenario is to minimize the total delay from a source to a destination node, while adhering to the timing constraints. For known typical and worst-case delays, an algorithm was presented to (statically) determine the policy to be followed during the packet transmission in terms of edge choices.In this paper we relax the assumption of knowing the typical delay, and we assume only worst-case bounds are available. We present a reinforcement learning solution to obtain optimal routing paths from a source to a destination when the typical transmission time is stochastic and unknown. Our reinforcement learning policy is based on the observation of the state-space during each packet transmission and on adaptation for future packets to congestion and unpredictable circumstances in the network. We ensure that our policy only makes safe routing decisions, thus never violating pre-determined timing constraints. We conduct experiments to evaluate the routing in a congested network and in a network where the typical delays have a large variance. Finally, we analyze the application of the algorithm to large randomly generated networks.},
booktitle = {Proceedings of the 28th International Conference on Real-Time Networks and Systems},
pages = {149–160},
numpages = {12},
location = {Paris, France},
series = {RTNS '20}
}

@inproceedings{10.1145/3400302.3415636,
author = {Wang, Ying and Wang, Mengdi and Li, Bing and Li, Huawei and Li, Xiaowei},
title = {A Many-Core Accelerator Design for on-Chip Deep Reinforcement Learning},
year = {2020},
isbn = {9781450380263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400302.3415636},
doi = {10.1145/3400302.3415636},
abstract = {Deep Reinforcement Learning (DRL) is substantially resource-consuming, and it requires large-scale distributed computing-nodes to learn complicated tasks, like videogame and Go play. This work attempts to down-scale a distributed DRL system into a specialized many-core chip and achieve energy-efficient on-chip DRL. With the customized Network-on-Chip that handles the communication of on-chip data and control-signals, we proposed a Synchronous Asynchronous RL Architecture (SARLA) and the according many-core chip that completely avoids the unnecessary data duplication and synchronization activities in multi-node RL systems. In evaluation, the SARLA system achieves considerable energy-efficiency boost over the GPU-based implementations for typical DRL workloads built with OpenAI-gym.},
booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
articleno = {46},
numpages = {7},
keywords = {reinforcement learning, distributed learning, many-core chip, network-on-chip},
location = {Virtual Event, USA},
series = {ICCAD '20}
}

@inproceedings{10.5555/3408352.3408474,
author = {Pfeifer, N\'{\i}colas and Zimpel, Bruno V. and Andrade, Gabriel A. G. and Santos, Luiz C. V. dos},
title = {A Reinforcement Learning Approach to Directed Test Generation for Shared Memory Verification},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Multicore chips are expected to rely on coherent shared memory. Albeit the coherence hardware can scale gracefully, the protocol state space grows exponentially with core count. That is why design verification requires directed test generation (DTG) for dynamic coverage control under the tight time constraints resulting from slow simulation and short verification budgets. Next generation EDA tools are expected to exploit Machine Learning for reaching high coverage in less time. We propose a technique that addresses DTG as a decision process and tries to find a decision-making policy for maximizing the cumulative coverage, as a result of successive actions taken by an agent. Instead of simply relying on learning, our technique builds upon the legacy from constrained random test generation (RTG). It casts DTG as coverage-driven RTG, and it explores distinct RTG engines subject to progressively tighter constraints. We compared three Reinforcement Learning generators with a state-of-the-art generator based on Genetic Programming. The experimental results show that the proper enforcement of constraints is more efficient for guiding learning towards higher coverage than simply letting the generator learn how to select the most promising memory events for increasing coverage. For a 3-level MESI 32-core design, the proposed approach led to the highest observed coverage (95.81\%), and it was 2.4 times faster than the baseline generator to reach the latter's maximal coverage.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {538–543},
numpages = {6},
keywords = {multicore chips, design verification, decision process, shared memory, reinforcement learning},
location = {Grenoble, France},
series = {DATE '20}
}

@inproceedings{10.5555/3451906.3451929,
author = {Cicirelli, F. and Gentile, A. F. and Greco, E. and Guerrieri, A. and Spezzano, G. and Vinci, A.},
title = {An Energy Management System at the Edge Based on Reinforcement Learning},
year = {2021},
publisher = {IEEE Press},
abstract = {In this work, we propose an IoT edge-based energy management system devoted to minimizing the energy cost for the daily-use of in-home appliances. The proposed approach employs a load scheduling based on a load shifting technique, and it is designed to operate in an edge-computing environment naturally. The scheduling considers all together time-variable profiles for energy cost, energy production, and energy consumption for each shiftable appliance. Deadlines for load termination can also be expressed. In order to address these goals, the scheduling problem is formulated as a Markov decision process and then processed through a reinforcement learning technique. The approach is validated by the development of an agent-based real-world test case deployed in an edge context.},
booktitle = {Proceedings of the IEEE/ACM 24th International Symposium on Distributed Simulation and Real Time Applications},
pages = {155–162},
numpages = {8},
keywords = {reinforcement learning, multi-agent systems, internet of things, edge computing, energy management systems},
location = {Prague, Czech Republic},
series = {DS-RT '20}
}

@article{10.5555/3546258.3546440,
author = {Klink, Pascal and Abdulsamad, Hany and Belousov, Boris and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {A Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Across machine learning, the use of curricula has shown strong empirical potential to improve learning from data by avoiding local optima of training objectives. For reinforcement learning (RL), curricula are especially interesting, as the underlying optimization has a strong tendency to get stuck in local optima due to the exploration-exploitation trade-off. Recently, a number of approaches for an automatic generation of curricula for RL have been shown to increase performance while requiring less expert knowledge compared to manually designed curricula. However, these approaches are seldomly investigated from a theoretical perspective, preventing a deeper understanding of their mechanics. In this paper, we present an approach for automated curriculum generation in RL with a clear theoretical underpinning. More precisely, we formalize the well-known self-paced learning paradigm as inducing a distribution over training tasks, which trades off between task complexity and the objective to match a desired task distribution. Experiments show that training on this induced distribution helps to avoid poor local optima across RL algorithms in different tasks with uninformative rewards and challenging exploration requirements.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {182},
numpages = {52},
keywords = {curriculum learning, self-paced learning, reinforcement learning, rl-as-inference, tempered inference}
}

@article{10.1145/3058592,
author = {Wang, Hongbign and Chen, Xin and Wu, Qin and Yu, Qi and Hu, Xingguo and Zheng, Zibin and Bouguettaya, Athman},
title = {Integrating Reinforcement Learning with Multi-Agent Techniques for Adaptive Service Composition},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3058592},
doi = {10.1145/3058592},
abstract = {Service-oriented architecture is a widely used software engineering paradigm to cope with complexity and dynamics in enterprise applications. Service composition, which provides a cost-effective way to implement software systems, has attracted significant attention from both industry and research communities. As online services may keep evolving over time and thus lead to a highly dynamic environment, service composition must be self-adaptive to tackle uninformed behavior during the evolution of services. In addition, service composition should also maintain high efficiency for large-scale services, which are common for enterprise applications. This article presents a new model for large-scale adaptive service composition based on multi-agent reinforcement learning. The model integrates reinforcement learning and game theory, where the former is to achieve adaptation in a highly dynamic environment and the latter is to enable agents to work for a common task (i.e., composition). In particular, we propose a multi-agent Q-learning algorithm for service composition, which is expected to achieve better performance when compared with the single-agent Q-learning method and multi-agent SARSA (State-Action-Reward-State-Action) method. Our experimental results demonstrate the effectiveness and efficiency of our approach.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {may},
articleno = {8},
numpages = {42},
keywords = {multi-agent system, game theory, Service composition, reinforcement learning}
}

@inproceedings{10.1145/3575813.3576871,
author = {Wei, Zhuo and De Nijs, Frits and Li, Jinhao and Wang, Hao},
title = {Model-Free Approach to Fair Solar PV Curtailment Using Reinforcement Learning},
year = {2023},
isbn = {9798400700323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575813.3576871},
doi = {10.1145/3575813.3576871},
abstract = {The rapid adoption of residential solar photovoltaics (PV) has resulted in regular overvoltage events, due to correlated reverse power flows. Currently, PV inverters prevent damage to electronics by curtailing energy production in response to overvoltage. However, this disproportionately affects households at the far end of the feeder, leading to an unfair allocation of the potential value of energy produced. Globally optimizing for fair curtailment requires accurate feeder parameters, which are often unknown. This paper investigates reinforcement learning, which gradually optimizes a fair PV curtailment strategy by interacting with the system. We evaluate six fairness metrics on how well they can be learned compared to an optimal solution oracle. We show that all definitions permit efficient learning, suggesting that reinforcement learning is a promising approach to achieving both safe and fair PV coordination.},
booktitle = {Proceedings of the 14th ACM International Conference on Future Energy Systems},
pages = {14–21},
numpages = {8},
keywords = {Energy Fairness, Renewable Energy, PV Curtailment, Voltage Control, Reinforcement Learning},
location = {Orlando, FL, USA},
series = {e-Energy '23}
}

@inproceedings{10.5555/2936924.2937152,
author = {Mannion, Patrick and Mason, Karl and Devlin, Sam and Duggan, Jim and Howley, Enda},
title = {Multi-Objective Dynamic Dispatch Optimisation Using Multi-Agent Reinforcement Learning: (Extended Abstract)},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper, we examine the application of Multi-Agent Reinforcement Learning (MARL) to a Dynamic Economic Emissions Dispatch problem. This is a multi-objective problem domain, where the conflicting objectives of fuel cost and emissions must be minimised. We evaluate the performance of several different MARL credit assignment structures in this domain, and our experimental results show that MARL can produce comparable solutions to those computed by Genetic Algorithms and Particle Swarm Optimisation.},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {1345–1346},
numpages = {2},
keywords = {reinforcement learning, difference rewards, smart grid, multi-objective, multi-agent systems, reward shaping},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{10.1145/3442381.3449862,
author = {Luo, Ziyan and Zhao, Linfeng and Cheng, Wei and Chen, Sihao and Chen, Qi and Xue, Hui and Wang, Haidong and Liu, Chuanjie and Yang, Mao and Zhang, Lintao},
title = {Match Plan Generation in Web Search with Parameterized Action Reinforcement Learning},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449862},
doi = {10.1145/3442381.3449862},
abstract = {To achieve good result quality and short query response time, search engines use specific match plans on Inverted Index to help retrieve a small set of relevant documents from billions of web pages. A match plan is composed of a sequence of match rules, which contain discrete match rule types and continuous stopping quotas. Currently, match plans are manually designed by experts according to their several years’ experience, which encounters difficulty in dealing with heterogeneous queries and varying data distribution. In this work, we formulate the match plan generation as a Partially Observable Markov Decision Process (POMDP) with a parameterized action space, and propose a novel reinforcement learning algorithm Parameterized Action Soft Actor-Critic (PASAC) to effectively enhance the exploration in both spaces. In our scene, we also discover a skew prioritizing issue of the original Prioritized Experience Replay (PER) and introduce Stratified Prioritized Experience Replay (SPER) to address it. We are the first group to generalize this task for all queries as a learning problem with zero prior knowledge and successfully apply deep reinforcement learning in the real web search environment. Our approach greatly outperforms the well-designed production match plans by over 70\% reduction of index block accesses with the quality of documents almost unchanged, and 9\% reduction of query response time even with model inference cost. Our method also beats the baselines on some open-source benchmarks1.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {1040–1052},
numpages = {13},
keywords = {Search Engine, Parameterized Action Soft Actor-Critic, Information Retrieval, Deep Reinforcement Learning},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3558482.3590195,
author = {Guan, Chongqi and Liu, Heting and Cao, Guohong and Zhu, Sencun and La Porta, Thomas},
title = {HoneyIoT: Adaptive High-Interaction Honeypot for IoT Devices Through Reinforcement Learning},
year = {2023},
isbn = {9781450398596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558482.3590195},
doi = {10.1145/3558482.3590195},
abstract = {As IoT devices are becoming widely deployed, there exist many threats to IoT-based systems due to their inherent vulnerabilities. One effective approach to improving IoT security is to deploy IoT honeypot systems, which can collect attack information and reveal the methods and strategies used by attackers. However, building high-interaction IoT honeypots is challenging due to the heterogeneity of IoT devices. Vulnerabilities in IoT devices typically depend on specific device types or firmware versions, which encourages attackers to perform pre-attack checks to gather device information before launching attacks. Moreover, conventional honeypots are easily detected because their replying logic differs from that of the IoT devices they try to mimic.To address these problems, we develop an adaptive high-interaction honeypot for IoT devices, called em HoneyIoT. We first build a real device based attack trace collection system to learn how attackers interact with IoT devices. We then model the attack behavior through markov decision process and leverage reinforcement learning techniques to learn the best responses to engage attackers based on the attack trace. We also use differential analysis techniques to mutate response values in some fields to generate high-fidelity responses.HoneyIoT has been deployed on the public Internet. Experimental results show that HoneyIoT can effectively bypass the pre-attack checks and mislead the attackers into uploading malware. Furthermore, HoneyIoT is covert against widely used reconnaissance and honeypot detection tools.},
booktitle = {Proceedings of the 16th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {49–59},
numpages = {11},
keywords = {honeypot, reinforcement learning, internet of things, security},
location = {Guildford, United Kingdom},
series = {WiSec '23}
}

@inproceedings{10.1145/3371158.3371168,
author = {Verma, Shresth and Nair, Haritha S. and Agarwal, Gaurav and Dhar, Joydip and Shukla, Anupam},
title = {Deep Reinforcement Learning for Single-Shot Diagnosis and Adaptation in Damaged Robots},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371168},
doi = {10.1145/3371158.3371168},
abstract = {Robotics has proved to be an indispensable tool in many industrial as well as social applications, such as warehouse automation, manufacturing, disaster robotics, etc. In most of these scenarios, damage to the agent while accomplishing mission-critical tasks can result in failure. To enable robotic adaptation in such situations, the agent needs to adopt policies which are robust to a diverse set of damages and must do so with minimum computational complexity. We thus propose a damage aware control architecture which diagnoses the damage prior to gait selection while also incorporating domain randomization in the damage space for learning a robust policy. To implement damage awareness, we have used a Long Short Term Memory based supervised learning network which diagnoses the damage and predicts the type of damage. The main novelty of this approach is that only a single policy is trained to adapt against a wide variety of damages and the diagnosis is done in a single trial at the time of damage.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {82–89},
numpages = {8},
keywords = {LSTM, Damage recovery, Gait Selection, Domain Adaptation, Reinforcement Learning},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@inproceedings{10.1145/3545008.3545020,
author = {Zhang, Zining and He, Bingsheng and Zhang, Zhenjie},
title = {HARL: Hierarchical Adaptive Reinforcement Learning Based Auto Scheduler for Neural Networks},
year = {2023},
isbn = {9781450397339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545008.3545020},
doi = {10.1145/3545008.3545020},
abstract = {To efficiently perform inference with neural networks, the underlying tensor programs require sufficient tuning efforts before being deployed into production environments. Usually, enormous tensor program candidates need to be sufficiently explored to find the one with the best performance. This is necessary to make the neural network products meet the high demand of real-world applications such as natural language processing, auto-driving, etc. Auto-schedulers are being developed to avoid the need for human intervention. However, due to the gigantic search space and lack of intelligent search guidance, current auto-schedulers require hours to days of tuning time to find the best-performing tensor program for the entire neural network. In this paper, we propose HARL, a reinforcement learning (RL) based auto-scheduler specifically designed for efficient tensor program exploration. HARL uses a hierarchical RL architecture in which learning-based decisions are made at all different levels of search granularity. It also automatically adjusts exploration configurations in real-time for faster performance convergence. As a result, HARL improves the tensor operator performance by 22\% and the search speed by 4.3x compared to the state-of-the-art auto-scheduler. Inference performance and search speed are also significantly improved on end-to-end neural networks.},
booktitle = {Proceedings of the 51st International Conference on Parallel Processing},
articleno = {77},
numpages = {13},
keywords = {auto tuner, reinforcement learning, neural network optimization},
location = {Bordeaux, France},
series = {ICPP '22}
}

@article{10.1145/3453186,
author = {Chen, Guihong and Liu, Xi and Shorfuzzaman, Mohammad and Karime, Ali and Wang, Yonghua and Qi, Yuanhang},
title = {MEC-Based Jamming-Aided Anti-Eavesdropping with Deep Reinforcement Learning for WBANs},
year = {2021},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3453186},
doi = {10.1145/3453186},
abstract = {Wireless body area network (WBAN) suffers secure challenges, especially the eavesdropping attack, due to constraint resources. In this article, deep reinforcement learning (DRL) and mobile edge computing (MEC) technology are adopted to formulate a DRL-MEC-based jamming-aided anti-eavesdropping (DMEC-JAE) scheme to resist the eavesdropping attack without considering the channel state information. In this scheme, a MEC sensor is chosen to send artificial jamming signals to improve the secrecy rate of the system. Power control technique is utilized to optimize the transmission power of both the source sensor and the MEC sensor to save energy. The remaining energy of the MEC sensor is concerned to ensure routine data transmission and jamming signal transmission. Additionally, the DMEC-JAE scheme integrates with transfer learning for a higher learning rate. The performance bounds of the scheme concerning the secrecy rate, energy consumption, and the utility are evaluated. Simulation results show that the DMEC-JAE scheme can approach the performance bounds with high learning speed, which outperforms the benchmark schemes.},
journal = {ACM Trans. Internet Technol.},
month = {dec},
articleno = {60},
numpages = {17},
keywords = {power control, Wireless body area networks, anti-eavesdropping, mobile edge computing}
}

@inproceedings{10.1145/3511616.3513093,
author = {Hao, Daniel and Sweetser, Penny and Aitchison, Matthew},
title = {Curriculum Generation and Sequencing for Deep Reinforcement Learning in StarCraft II},
year = {2022},
isbn = {9781450396066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511616.3513093},
doi = {10.1145/3511616.3513093},
abstract = {Reinforcement learning has proven successful in games, but suffers from long training times when compared to other forms of machine learning. Curriculum learning, an optimisation technique that improves a model’s ability to learn by presenting training samples in a meaningful order, known as curricula, could offer a solution for reinforcement learning. Due to limitations involved with automating curriculum learning, curricula are usually manually designed. However, due to a lack of research into effective design of curricula, researchers often rely on intuition and the resulting performance can vary. In this paper, we explore different ways of manually designing curricula for reinforcement learning in real-time strategy game, StarCraft II. We propose three generalised methods of manually creating tasks for curriculum learning and verify their effectiveness through experiments. We also experiment with different curricula sequences, in addition to the most commonly used easy-to-hard order. Our results show that all three of our proposed methods can improve a reinforcement learning agent’s learning process when used correctly. We demonstrate that modifying the state space of the tasks is the most effective way to create training samples for StarCraft II and that reversed curricula can be beneficial to an agent’s convergence process under certain circumstances.},
booktitle = {Proceedings of the 2022 Australasian Computer Science Week},
pages = {1–11},
numpages = {11},
keywords = {Real-Time Strategy Games, Reinforcement Learning, StarCraft II, Game AI},
location = {Brisbane, Australia},
series = {ACSW '22}
}

@inproceedings{10.1145/3219819.3219846,
author = {Hu, Yujing and Da, Qing and Zeng, Anxiang and Yu, Yang and Xu, Yinghui},
title = {Reinforcement Learning to Rank in E-Commerce Search Engine: Formalization, Analysis, and Application},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219846},
doi = {10.1145/3219819.3219846},
abstract = {In E-commerce platforms such as Amazon and TaoBao , ranking items in a search session is a typical multi-step decision-making problem. Learning to rank (LTR) methods have been widely applied to ranking problems. However, such methods often consider different ranking steps in a session to be independent, which conversely may be highly correlated to each other. For better utilizing the correlation between different ranking steps, in this paper, we propose to use reinforcement learning (RL) to learn an optimal ranking policy which maximizes the expected accumulative rewards in a search session. Firstly, we formally define the concept of search session Markov decision process (SSMDP) to formulate the multi-step ranking problem. Secondly, we analyze the property of SSMDP and theoretically prove the necessity of maximizing accumulative rewards. Lastly, we propose a novel policy gradient algorithm for learning an optimal ranking policy, which is able to deal with the problem of high reward variance and unbalanced reward distribution of an SSMDP. Experiments are conducted in simulation and TaoBao search engine. The results demonstrate that our algorithm performs much better than the state-of-the-art LTR methods, with more than 40\% and 30\% growth of total transaction amount in the simulation and the real application, respectively.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {368–377},
numpages = {10},
keywords = {reinforcement learning, policy gradient, online learning to rank},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{10.1145/3610294,
author = {Xiao, Jie and Ge, Yingying and Wang, Ru and Lou, Jungang},
title = {ICP-RL: Identifying Critical Paths for Fault Diagnosis Using Reinforcement Learning},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3610294},
doi = {10.1145/3610294},
abstract = {Identifying the critical paths is crucial to reducing the complexity of performance analysis and reliability calculation for logic circuits. In this paper, we propose a method for identifying the critical path in a combination circuit using a reinforcement learning framework to enhance its applicability and compatibility. Initially, we configured the learning environment of the model based on circuit structure information to provide valuable information for decision-making on time. Subsequently, the upper confidence bound applied to trees (UCT) algorithm is employed to construct the behavior decision strategy of the model, which avoids invalid traversal and reduces computing costs. Then, a goal-oriented reward and punishment function is constructed based on the distance from the circuit primary outputs. Finally, based on the parallel computing strategy, we construct an adaptive training method to improve the model’s prediction accuracy by using finite sampling, which speeds up the convergence speed and enhances the quality of the model. Experimental results on benchmark circuits show that, with the functional timing analysis method as the reference, the average accuracy of the proposed method is as high as 99.39\% and the single average calculation speed is 18.07 times faster than that of the reference method. Compared with the Monte Carlo model, the proposed method has a higher critical path hit rate, and the average calculation speed is 928.75 times faster.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {jul},
keywords = {reinforcement learning, circuit reliability, critical path, upper confidence bound applied to tree (UCT), parallel training}
}

@inproceedings{10.1145/2815782.2815809,
author = {Frankland, Clive and Pillay, Nelishia},
title = {Evolving Game Playing Strategies for Othello Incorporating Reinforcement Learning and Mobility},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815809},
doi = {10.1145/2815782.2815809},
abstract = {Genetic programming is rapidly gaining popularity in research areas for the induction of complex game playing strategies for board games such as Othello, checkers, backgammon and chess endgames. Most of this research has focused on developing evaluation functions for use with standard game playing algorithms such as the alpha-beta algorithm or Monte Carlo tree search, supported by game specific knowledge bases. In previous work we have introduced a novel application of genetic programming to evolve game playing strategies composed of heuristics for board games. Each evolved strategy represents a player. Strategies are evolved in real time, during game play, while in other studies the strategies are generally created offline. The research presented in this paper builds on this work by investigating the use of reinforcement learning and mobility to further improve the evolved game playing strategies. An initial population of players created using the ramped half-and-half method is iteratively refined using reproduction, mutation and crossover. Tournament selection is used to choose parents. The board game Othello, also known as Reversi, is used to illustrate and evaluate this approach. The performance of the genetic programming approach incorporating reinforcement learning, the genetic programming approach incorporating mobility and the genetic programming approach incorporating both reinforcement learning and mobility were compared to that of the previous heuristic based approach. All three approaches were found to produce better results than the previous heuristic based approach, with the genetic programming approach incorporating just reinforcement learning performing the best.},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {16},
numpages = {9},
keywords = {heuristics, Othello, Genetic programming, board games, game-playing},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@inproceedings{10.1145/3604915.3608778,
author = {Nagrecha, Kabir and Liu, Lingyi and Delgado, Pablo and Padmanabhan, Prasanna},
title = {InTune: Reinforcement Learning-Based Data Pipeline Optimization for Deep Recommendation Models},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608778},
doi = {10.1145/3604915.3608778},
abstract = {Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- \&amp; time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning (DL) training jobs are dominated by model execution times, the most important factor in DLRM training performance is often online data ingestion.In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into the specific bottlenecks and challenges of the DLRM training pipeline at scale. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to both observe the performance impacts of online ingestion and to identify shortfalls in existing data pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization to adopt. Our studies lead us to design and build a new solution for data pipeline optimization, InTune. InTune&nbsp;employs a reinforcement learning (RL) agent to learn how to distribute the CPU resources of a trainer machine across a DLRM data pipeline to more effectively parallelize data-loading and improve throughput. Our experiments show that InTune&nbsp;can build an optimized data pipeline configuration within only a few minutes, and can easily be integrated into existing training workflows. By exploiting the responsiveness and adaptability of RL, InTune&nbsp;achieves significantly higher online data ingestion rates than existing optimizers, thus reducing idle times in model execution and increasing efficiency. We apply InTune&nbsp;to our real-world cluster, and find that it increases data ingestion throughput by as much as 2.29X versus current state-of-the-art data pipeline optimizers while also improving both CPU \&amp; GPU utilization.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {430–442},
numpages = {13},
keywords = {resource allocation, parallel computing, recommendation systems, data processing, deep learning},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@inproceedings{10.1109/ICSE48619.2023.00155,
author = {Haq, Fitash Ul and Shin, Donghwan and Briand, Lionel C.},
title = {Many-Objective Reinforcement Learning for Online Testing of DNN-Enabled Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00155},
doi = {10.1109/ICSE48619.2023.00155},
abstract = {Deep Neural Networks (DNNs) have been widely used to perform real-world tasks in cyber-physical systems such as Autonomous Driving Systems (ADS). Ensuring the correct behavior of such DNN-Enabled Systems (DES) is a crucial topic. Online testing is one of the promising modes for testing such systems with their application environments (simulated or real) in a closed loop, taking into account the continuous interaction between the systems and their environments. However, the environmental variables (e.g., lighting conditions) that might change during the systems' operation in the real world, causing the DES to violate requirements (safety, functional), are often kept constant during the execution of an online test scenario due to the two major challenges: (1) the space of all possible scenarios to explore would become even larger if they changed and (2) there are typically many requirements to test simultaneously.In this paper, we present MORLOT (Many-Objective Reinforcement Learning for Online Testing), a novel online testing approach to address these challenges by combining Reinforcement Learning (RL) and many-objective search. MORLOT leverages RL to incrementally generate sequences of environmental changes while relying on many-objective search to determine the changes so that they are more likely to achieve any of the uncovered objectives. We empirically evaluate MORLOT using CARLA, a high-fidelity simulator widely used for autonomous driving research, integrated with Transfuser, a DNN-enabled ADS for end-to-end driving. The evaluation results show that MORLOT is significantly more effective and efficient than alternatives with a large effect size. In other words, MORLOT is a good option to test DES with dynamically changing environments while accounting for multiple safety requirements.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1814–1826},
numpages = {13},
keywords = {many objective search, reinforcement learning, online testing, DNN testing, self-driving cars},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3575813.3595189,
author = {Wang, Ruihang and Cao, Zhiwei and Zhou, Xin and Wen, Yonggang and Tan, Rui},
title = {Phyllis: Physics-Informed Lifelong Reinforcement Learning for Data Center Cooling Control},
year = {2023},
isbn = {9798400700323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575813.3595189},
doi = {10.1145/3575813.3595189},
abstract = {Deep reinforcement learning (DRL) has shown good performance in data center cooling control for improving energy efficiency. The main challenge in deploying the DRL agent to real-world data centers is how to quickly adapt the agent to the ever-changing system with thermal safety compliance. Existing approaches rely on DRL’s native fine-tuning or a learned data-driven dynamics model to assist the adaptation. However, they require long-term unsafe exploration before the agent or the model can capture a new environment. This paper proposes Phyllis, a physics-informed reinforcement learning approach to assist the DRL agent’s lifelong learning under evolving data center environment. Phyllis first identifies a transition model to capture the data hall thermodynamics in the offline stage. When the environment changes in the online stage, Phyllis assists the adaptation by i) supervising safe data collection with the identified transition model, ii) fitting power usage and residual thermal models, iii) pretraining the agent by interacting with these models, and iv) deploying the agent for further fine-tuning. Phyllis uses known physical laws to inform the transition and power models for improving the extrapolation ability to unseen states. Extensive evaluation for two simulated data centers with different system changes shows that Phyllis saves 5.7\% to 13.8\% energy usage compared with feedback cooling control and adapts to new environments 8x to 10x faster than fine-tuning with at most 0.74°C temperature overshoot.},
booktitle = {Proceedings of the 14th ACM International Conference on Future Energy Systems},
pages = {114–126},
numpages = {13},
keywords = {safe exploration, lifelong reinforcement learning, domain adaptation, cooling control optimization, Data centers},
location = {Orlando, FL, USA},
series = {e-Energy '23}
}

@inproceedings{10.5555/3586210.3586445,
author = {Mohapatra, Deepak and Pal, Aritra and Ojha, Ankush and Ghosh, Supratim and Agarwal, Marichi and Sarkar, Chayan},
title = {A Simulation-Aided Deep Reinforcement Learning Approach for Optimization of Automated Sorting Center Processes},
year = {2023},
publisher = {IEEE Press},
abstract = {Operations in a parcel sorting center (SC) are multi-fold which lead to multiple NP-hard optimization problems, namely, parcel-chute assignment, online bin-packing, scheduling, and routing. The advent of multi-agent robotics has accelerated the process of automation in sorting centers which has led to the requirement of sophisticated algorithms to optimize these operations within an SC. To this end, we propose RL --- SORT: a simulation-aided deep reinforcement learning based algorithm which jointly optimizes the parcel-chute assignment and online roller-cage (RC) packing problems. Through experimentation on our simulation framework, we show that RL --- SORT not only outperforms baselines, but also has a low computational burden. Further, it is able to significantly reduce the number of RCs used, thereby, reducing the transportation costs.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2795–2806},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.5555/3535850.3535878,
author = {\c{C}elikok, Mustafa Mert and Oliehoek, Frans A. and Kaski, Samuel},
title = {Best-Response Bayesian Reinforcement Learning with Bayes-Adaptive POMDPs for Centaurs},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Centaurs are half-human, half-AI decision-makers where the AI's goal is to complement the human. To do so, the AI must be able to recognize the goals and constraints of the human and have the means to help them. We present a novel formulation of the interaction between the human and the AI as a sequential game where the agents are modelled using Bayesian best-response models. We show that in this case the AI's problem of helping bounded-rational humans make better decisions reduces to a Bayes-adaptive POMDP. In our simulated experiments, we consider an instantiation of our framework for humans who are subjectively optimistic about the AI's future behaviour. Our results show that when equipped with a model of the human, the AI can infer the human's bounds and nudge them towards better decisions. We discuss ways in which the machine can learn to improve upon its own limitations as well with the help of the human. We identify a novel trade-off for centaurs in partially observable tasks: for the AI's actions to be acceptable to the human, the machine must make sure their beliefs are sufficiently aligned, but aligning beliefs might be costly. We present a preliminary theoretical analysis of this trade-off and its dependence on task structure.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {235–243},
numpages = {9},
keywords = {computational rationality, hybrid intelligence, Bayesian reinforcement learning, multiagent learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3567445.3567454,
author = {Chabi Sika Boni, Abdel Kader and Hablatou, Youssef and Hassan, Hassan and Drira, Khalil},
title = {Distributed Deep Reinforcement Learning Architecture for Task Offloading in Autonomous IoT Systems},
year = {2023},
isbn = {9781450396653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3567445.3567454},
doi = {10.1145/3567445.3567454},
abstract = {Autonomous IoT systems require the development of good automation algorithms capable of handling a huge number of IoT devices such as in smart cities. Deep Reinforcement Learning (DRL) is a powerful automation technique that can be used in massive systems thanks to its ability to deal with big state spaces. Moreover, it adapts quickly to changes in the system by reinforcement learning, making the automation algorithm very flexible. However, using DRL relies generally on centralized agent architecture making it more exposed to communication failures. In this paper, we propose a distributed architecture to solve the task offloading problem in autonomous IoT systems where learning is achieved in a master agent while decision making is delegated to IoT devices. This architecture is more resilient as decisions are made locally and interactions between IoT devices and the master agent are less frequent and not blocking. We tested this architecture in the ns3-gym environment and our results show very good resilience of this architecture.},
booktitle = {Proceedings of the 12th International Conference on the Internet of Things},
pages = {112–118},
numpages = {7},
keywords = {Task offloading, Autonomous IoT systems, Deep Reinforcement Learning, Distributed},
location = {Delft, Netherlands},
series = {IoT '22}
}

@article{10.1145/3546952,
author = {Chen, Jinwei and Zong, Zefang and Zhuang, Yunlin and Yan, Huan and Jin, Depeng and Li, Yong},
title = {Reinforcement Learning for Practical Express Systems with Mixed Deliveries and Pickups},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3546952},
doi = {10.1145/3546952},
abstract = {In real-world express systems, couriers need to satisfy not only the delivery demands but also the pick-up demands of customers. Delivery and pickup tasks are usually mixed together within integrated routing plans. Such a mixed routing problem can be abstracted and formulated as Vehicle Routing Problem with Mixed Delivery and Pickup (VRPMDP), which is an NP-hard combinatorial optimization problem. To solve VRPMDP, there are three major challenges as below. (a) Even though successive pickup and delivery tasks are independent to accomplish, the inter-influence between choosing pickup task or delivery task to deal with still exists. (b) Due to the two-way flow of goods between the depot and customers, the loading rate of vehicles leaving the depot affects routing decisions. (c) The proportion of deliveries and pickups will change due to the complex demand situation in real-world scenarios, which requires robustness of the algorithm. To solve the challenges above, we design an encoder-decoder based framework to generate high-quality and robust VRPMDP solutions. First, we consider a VRPMDP instance as a graph and utilize a GNN encoder to extract the feature of the instance effectively. The detailed routing solutions are further decoded as a sequence by the decoder with attention mechanism. Second, we propose a Coordinated Decision of Loading and Routing (CDLR) mechanism to determine the loading rate dynamically after the vehicle returns to the depot, thus avoiding the influence of improper loading rate settings. Finally, the model equipped with a GNN encoder and CDLR simultaneously can adapt to the changes in the proportion of deliveries and pickups. We conduct the experiments to demonstrate the effectiveness of our model. The experiments show that our method achieves desirable results and generalization ability.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {feb},
articleno = {33},
numpages = {19},
keywords = {Practical express systems; vehicle routing problem with mixed deliveries and pickup; reinforcement learning}
}

@inproceedings{10.1145/3328905.3329506,
author = {Russo, Gabriele Russo and Cardellini, Valeria and Presti, Francesco Lo},
title = {Reinforcement Learning Based Policies for Elastic Stream Processing on Heterogeneous Resources},
year = {2019},
isbn = {9781450367943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328905.3329506},
doi = {10.1145/3328905.3329506},
abstract = {Data Stream Processing (DSP) has emerged as a key enabler to develop pervasive services that require to process data in a near real-time fashion. DSP applications keep up with the high volume of produced data by scaling their execution on multiple computing nodes, so as to process the incoming data flow in parallel. Workloads variability requires to elastically adapt the application parallelism at run-time in order to avoid over-provisioning. Elasticity policies for DSP have been widely investigated, but mostly under the simplifying assumption of homogeneous infrastructures. The resulting solutions do not capture the richness and inherent complexity of modern infrastructures, where heterogeneous computing resources are available on-demand. In this paper, we formulate the problem of controlling elasticity on heterogeneous resources as a Markov Decision Process (MDP). The resulting MDP is not easily solved by traditional techniques due to state space explosion, and thus we show how linear Function Approximation and Tile Coding can be used to efficiently compute elasticity policies at run-time. In order to deal with parameters uncertainty, we integrate the proposed approach with Reinforcement Learning algorithms. Our numerical evaluation shows the efficacy of the presented solutions compared to standard methods in terms of accuracy and convergence speed.},
booktitle = {Proceedings of the 13th ACM International Conference on Distributed and Event-Based Systems},
pages = {31–42},
numpages = {12},
keywords = {Function Approximation, Elasticity, Tile Coding, Reinforcement Learning, Markov Decision Process},
location = {Darmstadt, Germany},
series = {DEBS '19}
}

@inproceedings{10.1145/3366030.3366055,
author = {Dehury, Chinmaya Kumar and Srirama, Satish Narayana},
title = {Personalized Service Delivery Using Reinforcement Learning in Fog and Cloud Environment},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366055},
doi = {10.1145/3366030.3366055},
abstract = {The ability to fulfil the resource demand in runtime is encouraging the businesses to migrate to cloud. Recently, to provide real-time cloud services and to save network resources, fog computing is introduced. To further improve the quality of service in delivery process, Artificial Intelligence is being applied extensively. However, the state-of-the-art in this regard is still immature as it mainly focuses at either fog or cloud. To address this issue, a novel reinforcement learning-based personalized service delivery (RLPSD) mechanism is proposed in this paper, which allows the service provider to combine the fog and cloud environments, while providing the service. RLPSD distributes the user's service requests between fog and cloud, considering the users' constraints (e.g. the distance from fog), thus resulting in personalized service delivery. The proposed RLPSD algorithm is implemented and evaluated in terms of its success rate, percentage of service requests' distribution, learning rate, discount factor, etc.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {522–529},
numpages = {8},
keywords = {cloud computing, fog computing, Q-learn algorithm, Reinforcement learning, service delivery},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/3269206.3271748,
author = {Wu, Di and Chen, Xiujun and Yang, Xun and Wang, Hao and Tan, Qing and Zhang, Xiaoxun and Xu, Jian and Gai, Kun},
title = {Budget Constrained Bidding by Model-Free Reinforcement Learning in Display Advertising},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271748},
doi = {10.1145/3269206.3271748},
abstract = {Real-time bidding (RTB) is an important mechanism in online display advertising, where a proper bid for each page view plays an essential role for good marketing results. Budget constrained bidding is a typical scenario in RTB where the advertisers hope to maximize the total value of the winning impressions under a pre-set budget constraint. However, the optimal bidding strategy is hard to be derived due to the complexity and volatility of the auction environment. To address these challenges, in this paper, we formulate budget constrained bidding as a Markov Decision Process and propose a model-free reinforcement learning framework to resolve the optimization problem. Our analysis shows that the immediate reward from environment is misleading under a critical resource constraint. Therefore, we innovate a reward function design methodology for the reinforcement learning problems with constraints. Based on the new reward design, we employ a deep neural network to learn the appropriate reward so that the optimal policy can be learned effectively. Different from the prior model-based work, which suffers from the scalability problem, our framework is easy to be deployed in large-scale industrial applications. The experimental evaluations demonstrate the effectiveness of our framework on large-scale real datasets.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1443–1451},
numpages = {9},
keywords = {bid optimization, rtb, reinforcement learning, display advertising},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3580305.3599916,
author = {Timmaraju, Aditya Srinivas and Mashayekhi, Mehdi and Chen, Mingliang and Zeng, Qi and Fettes, Quintin and Cheung, Wesley and Xiao, Yihan and Kannadasan, Manojkumar Rangasamy and Tripathi, Pushkar and Gahagan, Sean and Bogen, Miranda and Roudani, Rob},
title = {Towards Fairness in Personalized Ads Using Impression Variance Aware Reinforcement Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599916},
doi = {10.1145/3580305.3599916},
abstract = {Variances in ad impression outcomes across demographic groups are increasingly considered to be potentially indicative of algorithmic bias in personalized ads systems. While there are many definitions of fairness that could be applicable in the context of personalized systems, we present a framework which we call the Variance Reduction System (VRS) for achieving more equitable outcomes in Meta's ads systems. VRS seeks to achieve a distribution of impressions with respect to selected protected class (PC) attributes that more closely aligns the demographics of an ad's eligible audience (a function of advertiser targeting criteria) with the audience who sees that ad, in a privacy-preserving manner. We first define metrics to quantify fairness gaps in terms of ad impression variances with respect to PC attributes including gender and estimated race. We then present the VRS for re-ranking ads in an impression variance-aware manner. We evaluate VRS via extensive simulations over different parameter choices and study the effect of the VRS on the chosen fairness metric. We finally present online A/B testing results from applying VRS to Meta's ads systems, concluding with a discussion of future work. We have deployed the VRS to all users in the US for housing ads, resulting in significant improvement in our fairness metric. VRS is the first large-scale deployed framework for pursuing fairness for multiple PC attributes in online advertising.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4937–4947},
numpages = {11},
keywords = {fairness, reinforcement learning, differential privacy, ads},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3486001.3486236,
author = {Shibu, Aebel Joe and S, Sadhana and N, Shilpa and Kumar, Pratyush},
title = {VeRLPy: Python Library for Verification of Digital Designs with Reinforcement Learning},
year = {2021},
isbn = {9781450385947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486001.3486236},
doi = {10.1145/3486001.3486236},
abstract = {Digital hardware is verified by comparing its behavior against a reference model on a range of randomly generated input signals. The random generation of the inputs hopes to achieve sufficient coverage of the different parts of the design. However, such coverage is often difficult to achieve, amounting to large verification efforts and delays. An alternative is to use Reinforcement Learning (RL) to generate the inputs by learning to prioritize those inputs which can more efficiently explore the design under test. In this work, we present VeRLPy [3], an open-source library to allow RL-driven verification with limited additional engineering overhead. This contributes to two broad movements within the EDA community of (a) moving to open-source tool chains and (b) reducing barriers for development with Python support. We also demonstrate the use of VeRLPy for a few designs and establish its value over randomly generated input signals.},
booktitle = {Proceedings of the First International Conference on AI-ML Systems},
articleno = {16},
numpages = {7},
location = {Bangalore, India},
series = {AIMLSystems '21}
}

@inproceedings{10.5555/2484920.2485069,
author = {Colby, Mitchell and Tumer, Kagan},
title = {Multiagent Reinforcement Learning in a Distributed Sensor Network with Indirect Feedback},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Highly accurate sensor measurements are crucial in order for power plants to effectively operate, as well as to predict and subsequently prevent any potentially catastrophic failures. As the cost of sensors decreases while their power increases, distributed sensor networks become a more attractive option for implementation in power plants. In this work, we investi- gate the use of a distributed sensor network to achieve highly accurate measurements. We apply shaped rewards to local components and use a simple learning algorithm at each sen- sor in order to maximize those rewards. Our results show that the measurements from a sensor network trained us- ing shaped rewards are up to two orders of magnitude more accurate than a sensor network trained with a traditional global reward. Further, the algorithm proposed scales well to large networks, and is robust to measurement noise and sensor failures.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {941–948},
numpages = {8},
keywords = {coordination, multiagent learning},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.1145/3299874.3318039,
author = {Singh, Karunveer and Gupta, Rishabh and Gupta, Vikram and Fayyazi, Arash and Pedram, Massoud and Nazarian, Shahin},
title = {A Hybrid Framework for Functional Verification Using Reinforcement Learning and Deep Learning},
year = {2019},
isbn = {9781450362528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299874.3318039},
doi = {10.1145/3299874.3318039},
abstract = {In this paper, we propose a novel hybrid verification framework (HVF) which uses Reinforcement Learning (RL) and Deep Neural Networks (DNNs) to accelerate the verification of complex systems. More precisely, our HVF incorporates RL to generate all possible sequences of vectors needed to approach a target state as well as the corresponding path to the target state which contains a potential design error. Furthermore, HVF utilizes DNNs to accelerate the verification of complex data paths in the target states. We have tested our framework on several circuits including multi-core designs as well as bus-arbiters and confirmed its significant verification speedup when compared to prior work. For example, HVF provides a total speedup of 4.5x for a quad-core MIPS processor verification.},
booktitle = {Proceedings of the 2019 on Great Lakes Symposium on VLSI},
pages = {367–370},
numpages = {4},
keywords = {deep neural networks, assertions, coverage directed test generation, sat solver, reinforcement learning},
location = {Tysons Corner, VA, USA},
series = {GLSVLSI '19}
}

@inproceedings{10.1109/CCGRID.2017.15,
author = {Arabnejad, Hamid and Pahl, Claus and Jamshidi, Pooyan and Estrada, Giovani},
title = {A Comparison of Reinforcement Learning Techniques for Fuzzy Cloud Auto-Scaling},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.15},
doi = {10.1109/CCGRID.2017.15},
abstract = {A goal of cloud service management is to design self-adaptable auto-scaler to react to workload fluctuations and changing the resources assigned. The key problem is how and when to add/remove resources in order to meet agreed service-level agreements. Reducing application cost and guaranteeing service-level agreements (SLAs) are two critical factors of dynamic controller design. In this paper, we compare two dynamic learning strategies based on a fuzzy logic system, which learns and modifies fuzzy scaling rules at runtime. A self-adaptive fuzzy logic controller is combined with two reinforcement learning (RL) approaches: (i) Fuzzy SARSA learning (FSL) and (ii) Fuzzy Q-learning (FQL). As an off-policy approach, Q-learning learns independent of the policy currently followed, whereas SARSA as an on-policy always incorporates the actual agent's behavior and leads to faster learning. Both approaches are implemented and compared in their advantages and disadvantages, here in the OpenStack cloud platform. We demonstrate that both auto-scaling approaches can handle various load traffic situations, sudden and periodic, and delivering resources on demand while reducing operating costs and preventing SLA violations. The experimental results demonstrate that FSL and FQL have acceptable performance in terms of adjusted number of virtual machine targeted to optimize SLA compliance and response time.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {64–73},
numpages = {10},
keywords = {SARSA, Controller, OpenStack, Fuzzy Logic, Orchestration, Q-Learning, Cloud Computing},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/3464509.3464884,
author = {Personnaz, Aur\'{e}lien and Amer-Yahia, Sihem and Berti-Equille, Laure and Fabricius, Maximilian and Subramanian, Srividya},
title = {Balancing Familiarity and Curiosity in Data Exploration with Deep Reinforcement Learning},
year = {2021},
isbn = {9781450385350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464509.3464884},
doi = {10.1145/3464509.3464884},
abstract = {The ability to find a set of records in Exploratory Data Analysis (EDA) hinges on the scattering of objects in the data set and the on users’ knowledge of data and their ability to express their needs. This yields a wide range of EDA scenarios and solutions that differ in the guidance they provide to users. In this paper, we investigate the interplay between modeling curiosity and familiarity in Deep Reinforcement Learning (DRL) and expressive data exploration operators. We formalize curiosity as intrinsic reward and familiarity as extrinsic reward. We examine the behavior of several policies learned for different weights for those rewards. Our experiments on SDSS, a very large sky survey data set1 provide several insights and justify the need for a deeper examination of combining DRL and data exploration operators that go beyond drill-downs and roll-ups.},
booktitle = {Fourth Workshop in Exploiting AI Techniques for Data Management},
pages = {16–23},
numpages = {8},
location = {Virtual Event, China},
series = {aiDM '21}
}

@inproceedings{10.1145/3533767.3543292,
author = {Zhang, Ziqian and Liu, Yulei and Yu, Shengcheng and Li, Xin and Yun, Yexiao and Fang, Chunrong and Chen, Zhenyu},
title = {UniRLTest: Universal Platform-Independent Testing with Reinforcement Learning via Image Understanding},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3543292},
doi = {10.1145/3533767.3543292},
abstract = {GUI testing has been prevailing in software testing. However, existing automated GUI testing tools mostly rely on frameworks of a specific platform. Testers have to fully understand platform features before developing platform-dependent GUI testing tools. Starting from the perspective of tester’s vision, we observe that GUIs on different platforms share commonalities of widget images and layout designs, which can be leveraged to achieve platform-independent testing. We propose UniRLTest, an automated software testing framework, to achieve platform independence testing. UniRLTest utilizes computer vision techniques to capture all the widgets in the screenshot and constructs a widget tree for each page. A set of all the executable actions in each tree will be generated accordingly. UniRLTest adopts a Deep Q-Network, a reinforcement learning (RL) method, to the exploration process and formalize the Android GUI testing problem to a Marcov Decision Process (MDP), where RL could work. We have conducted evaluation experiments on 25 applications from different platforms. The result shows that UniRLTest outperforms baselines in terms of efficiency and effectiveness.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {805–808},
numpages = {4},
keywords = {Reinforcement Learning, Image Analysis, Cross-platform Testing},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@inproceedings{10.1145/3404555.3404569,
author = {Zhao, Tian and Wang, Luyao and Chin, Kwan-Wu},
title = {Reinforcement Learning Based Routing in EH-WSNs with Dual Alternative Batteries},
year = {2020},
isbn = {9781450377089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404555.3404569},
doi = {10.1145/3404555.3404569},
abstract = {This paper considers an Energy Harvesting Wireless Sensor Network (EH-WSN) where nodes have a dual alternative battery system. We propose a stateless distributed reinforcement learning based routing algorithm, named QLRA, where each node learns the best next hop(s) to forward its data based on the battery and data information of its neighbors. We study how the number of sources and path exploration probability impacts the performance of QLRA. Numerical results show that after learning, QLRA is able to achieve minimal end-to-end delays in all tested scenarios, which is about 18\% lower than the average end-to-end delay of a competing routing algorithm.},
booktitle = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
pages = {439–443},
numpages = {5},
keywords = {charging and discharging, machine learning, routing, Battery},
location = {Tianjin, China},
series = {ICCAI '20}
}

@article{10.1145/3439332,
author = {Le, Duc Van and Wang, Rongrong and Liu, Yingbo and Tan, Rui and Wong, Yew-Wah and Wen, Yonggang},
title = {Deep Reinforcement Learning for Tropical Air Free-Cooled Data Center Control},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3439332},
doi = {10.1145/3439332},
abstract = {Air free-cooled data centers (DCs) have not existed in the tropical zone due to the unique challenges of year-round high ambient temperature and relative humidity (RH). The increasing availability of servers that can tolerate higher temperatures and RH due to the regulatory bodies’ prompts to raise DC temperature setpoints sheds light upon the feasibility of air free-cooled DCs in the tropics. However, due to the complex psychrometric dynamics, operating the air free-cooled DC in the tropics generally requires adaptive control of supply air condition to maintain the computing performance and reliability of the servers. This article studies the problem of controlling the supply air temperature and RH in a free-cooled tropical DC below certain thresholds. To achieve the goal, we formulate the control problem as Markov decision processes and apply deep reinforcement learning (DRL) to learn the control policy that minimizes the cooling energy while satisfying the requirements on the supply air temperature and RH. We also develop a constrained DRL solution for performance improvements. Extensive evaluation based on real data traces collected from an air free-cooled testbed and comparisons among the unconstrained and constrained DRL approaches as well as two other baseline approaches show the superior performance of our proposed solutions.},
journal = {ACM Trans. Sen. Netw.},
month = {jun},
articleno = {24},
numpages = {28},
keywords = {air free cooling, deep reinforcement learning, Data centers}
}

@inproceedings{10.1145/3534678.3539154,
author = {von Wahl, Leonie and Tempelmeier, Nicolas and Sao, Ashutosh and Demidova, Elena},
title = {Reinforcement Learning-Based Placement of Charging Stations in Urban Road Networks},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539154},
doi = {10.1145/3534678.3539154},
abstract = {The transition from conventional mobility to electromobility largely depends on charging infrastructure availability and optimal placement. This paper examines the optimal placement of charging stations in urban areas. We maximise the charging infrastructure supply over the area and minimise waiting, travel, and charging times while setting budget constraints. Moreover, we include the possibility of charging vehicles at home to obtain a more refined estimation of the actual charging demand throughout the urban area. We formulate the Placement of Charging Stations problem as a non-linear integer optimisation problem that seeks the optimal positions for charging stations and the optimal number of charging piles of different charging types. We design a novel Deep Reinforcement Learning approach to solve the charging station placement problem (PCRL). Extensive experiments on real-world datasets show how the PCRL reduces the waiting and travel time while increasing the benefit of the charging plan compared to five baselines. Compared to the existing infrastructure, we can reduce the waiting time by up to 97\% and increase the benefit up to 497\%.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3992–4000},
numpages = {9},
keywords = {reinforcement learning, location selection, electromobility},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3487075.3487177,
author = {Niu, Haoyi and Hu, Jianming and Cui, Zheyu and Zhang, Yi},
title = {DR2L: Surfacing Corner Cases to Robustify Autonomous Driving via Domain Randomization Reinforcement Learning},
year = {2021},
isbn = {9781450389853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487075.3487177},
doi = {10.1145/3487075.3487177},
abstract = {How to explore corner cases as efficiently and thoroughly as possible has long been one of the top concerns in the context of deep reinforcement learning (DeepRL) autonomous driving. Training with simulated data is less costly and dangerous than utilizing real-world data, but the inconsistency of parameter distribution and the incorrect system modeling in simulators always lead to an inevitable Sim2real gap, which probably accounts for the underperformance in novel, anomalous and risky cases that simulators can hardly generate. Domain Randomization(DR) is a methodology that can bridge this gap with little or no real-world data. Consequently, in this research, an adversarial model is put forward to robustify DeepRL-based autonomous vehicles trained in simulation to gradually surfacing harder events, so that the models could readily transfer to the real world.},
booktitle = {Proceedings of the 5th International Conference on Computer Science and Application Engineering},
articleno = {102},
numpages = {8},
keywords = {Reinforcement learning, Domain randomization, Autonomous driving, Corner cases},
location = {Sanya, China},
series = {CSAE '21}
}

@inproceedings{10.1145/3564625.3567969,
author = {Landen, Matthew and Chung, Keywhan and Ike, Moses and Mackay, Sarah and Watson, Jean-Paul and Lee, Wenke},
title = {DRAGON: Deep Reinforcement Learning for Autonomous Grid Operation and Attack Detection},
year = {2022},
isbn = {9781450397599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564625.3567969},
doi = {10.1145/3564625.3567969},
abstract = {As power grids have evolved, IT has become integral to maintaining reliable power. While providing operators improved situational awareness and the ability to rapidly respond to dynamic situations, IT concurrently increases the cyberattack threat surface – as recent grid attacks such as Blackenergy and Crashoverride illustrate. To defend against such attacks, modern power grids require a system that can maintain reliable power during attacks and detect when these attacks occur to allow for a timely response. To help address limitations of prior work, we propose DRAGON– deep reinforcement learning for autonomous grid operation and attack detection, which (i) autonomously learns how to maintain reliable power operations while (ii) simultaneously detecting cyberattacks. We implement DRAGON and evaluate its effectiveness by simulating different attack scenarios on the IEEE 14 bus power transmission system model. Our experimental results show that DRAGON can maintain safe grid operations 225.5\% longer than a state-of-the-art autonomous grid operator. Furthermore, on average, our detection method reports a true positive rate of 92.9\% and a false positive rate of 11.4\%, while also reducing the false negative rate by 63.1\% compared to a recent attack detection method.},
booktitle = {Proceedings of the 38th Annual Computer Security Applications Conference},
pages = {13–27},
numpages = {15},
keywords = {power grid reliability, deep reinforcement leearning, cyberattack detection},
location = {Austin, TX, USA},
series = {ACSAC '22}
}

@inproceedings{10.1145/3356250.3361935,
author = {DeChicchis, Joseph and Ahn, Surin and Gorlatova, Maria},
title = {Adaptive AR Visual Output Security Using Reinforcement Learning Trained Policies: Demo Abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361935},
doi = {10.1145/3356250.3361935},
abstract = {Augmented reality (AR) technologies have seen significant improvement in recent years with several consumer and commercial solutions being developed. New security challenges arise as AR becomes increasingly ubiquitous. Previous work has proposed techniques for securing the output of AR devices and used reinforcement learning (RL) to train security policies which can be difficult to define manually. However, whether such systems and policies can be deployed on a physical AR device without degrading performance was left an open question. We develop a visual output security application using a RL trained policy and deploy it on a Magic Leap One head-mounted AR device. The demonstration illustrates that RL based visual output security systems are feasible.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {380–381},
numpages = {2},
keywords = {magic leap AR headset, visual output security, reinforcement learning, augmented reality, policy optimization},
location = {New York, New York},
series = {SenSys '19}
}

@inproceedings{10.1145/3520304.3528766,
author = {Amaral, Ryan and Ianta, Alexandru and Bayer, Caleidgh and Smith, Robert J. and Heywood, Malcolm I.},
title = {Benchmarking Genetic Programming in a Multi-Action Reinforcement Learning Locomotion Task},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3528766},
doi = {10.1145/3520304.3528766},
abstract = {Reinforcement learning (RL) requires an agent to interact with an environment to maximize the cumulative rather than the immediate reward. Recently, there as been a significant growth in the availability of scalable RL tasks, e.g. OpenAI gym. However, most benchmarking studies concentrate on RL solutions based on some form of deep learning. In this work, we benchmark a family of linear genetic programming based approaches to the 2-d biped walker problem. The biped walker is an example of a RL environment described in terms of a multi-dimensional, real-valued 24-d input and 4-d action space. Specific recommendations are made regarding mechanisms to adopt that are able to consistently produce solutions, in this case using transfer from periodic restarts.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {522–525},
numpages = {4},
keywords = {real-valued actions, continuous control, reinforcement learning},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@article{10.1145/3368313,
author = {Huo, Yuchi and Wang, Rui and Zheng, Ruzahng and Xu, Hualin and Bao, Hujun and Yoon, Sung-Eui},
title = {Adaptive Incident Radiance Field Sampling and Reconstruction Using Deep Reinforcement Learning},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0730-0301},
url = {https://doi.org/10.1145/3368313},
doi = {10.1145/3368313},
abstract = {Serious noise affects the rendering of global illumination using Monte Carlo (MC) path tracing when insufficient samples are used. The two common solutions to this problem are filtering noisy inputs to generate smooth but biased results and sampling the MC integrand with a carefully crafted probability distribution function (PDF) to produce unbiased results. Both solutions benefit from an efficient incident radiance field sampling and reconstruction algorithm. This study proposes a method for training quality and reconstruction networks (Q- and R-networks, respectively) with a massive offline dataset for the adaptive sampling and reconstruction of first-bounce incident radiance fields. The convolutional neural network (CNN)-based R-network reconstructs the incident radiance field in a 4D space, whereas the deep reinforcement learning (DRL)-based Q-network predicts and guides the adaptive sampling process. The approach is verified by comparing it with state-of-the-art unbiased path guiding methods and filtering methods. Results demonstrate improvements for unbiased path guiding and competitive performance in biased applications, including filtering and irradiance caching.},
journal = {ACM Trans. Graph.},
month = {jan},
articleno = {6},
numpages = {17},
keywords = {deep neural network, Incident radiance field, adaptive sampling}
}

@inproceedings{10.1145/2909824.3020241,
author = {Daniele, Andrea F. and Bansal, Mohit and Walter, Matthew R.},
title = {Navigational Instruction Generation as Inverse Reinforcement Learning with Neural Machine Translation},
year = {2017},
isbn = {9781450343367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909824.3020241},
doi = {10.1145/2909824.3020241},
abstract = {Modern robotics applications that involve human-robot interaction require robots to be able to communicate with humans seamlessly and effectively. Natural language provides a flexible and efficient medium through which robots can exchange information with their human partners. Significant advancements have been made in developing robots capable of interpreting free-form instructions, but less attention has been devoted to endowing robots with the ability to generate natural language. We propose a model that enables robots to generate natural language instructions that allow humans to navigate a priori unknown environments. We first decide which information to share with the user according to their preferences, using a policy trained from human demonstrations via inverse reinforcement learning. We then "translate" this information into a natural language instruction using a neural sequence-to-sequence model that learns to generate free-form instructions from natural language corpora. We evaluate our method on a benchmark route instruction dataset and achieve a BLEU score of 72.18\% compared to human-generated reference instructions. We additionally conduct navigation experiments with human participants demonstrating that our method generates instructions that people follow as accurately and easily as those produced by humans.},
booktitle = {Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {109–118},
numpages = {10},
keywords = {human-robot interaction, selective generation, natural language generation},
location = {Vienna, Austria},
series = {HRI '17}
}

@inproceedings{10.1145/3308557.3308686,
author = {Jayarathne, Isuru and Cohen, Michael and Frishkopf, Michael and Mulyk, Gregory},
title = {Relaxation "Sweet Spot" Exploration in Pantophonic Musical Soundscape Using Reinforcement Learning},
year = {2019},
isbn = {9781450366731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308557.3308686},
doi = {10.1145/3308557.3308686},
abstract = {Musical relaxation is a common method to relieve personal stress. Particularly, nature sounds, instrumental music, voice (chanting), "easy listening" songs, etc. can be played for relaxation. Nevertheless, effectiveness of the sounds used for the relaxation is idiosyncratic, depending on personal taste. In our approach, computer-guided audition for spatial soundscapes is investigated, automatically exploring a polyphonic area while using biosignals as indicators of satisfaction. We propose a reinforcement learning (RL) method to discover the sound relaxation "sweet spot" in a polyphonic soundscape. An avatar roams within a pantophonic space, surrounded by six independent audio channels, while a human subject, listening through the avatar's ears, is connected to an electroencephalographic (EEG) headset. Besides the position of the avatar, pitch, reverberation, and filters can also be changed to find the most relaxing virtual standpoint and parameters for the listener. Instead of changing position manually, a Deep Q-Network (DQN) in reinforcement learning is used. An RL agent adjusts parameters according to reward values calculated by change of relative theta band (4--8 Hz) power.},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces: Companion},
pages = {55–56},
numpages = {2},
keywords = {RL (reinforcement learning), musical relaxation, EEG, ANN (artificial neural network), DQN (deep q-network)},
location = {Marina del Ray, California},
series = {IUI '19}
}

@inproceedings{10.1145/2493525.2493535,
author = {Ferreira, Emmanuel and Lef\`{e}vre, Fabrice},
title = {Social Signal and User Adaptation in Reinforcement Learning-Based Dialogue Management},
year = {2013},
isbn = {9781450320191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493525.2493535},
doi = {10.1145/2493525.2493535},
abstract = {This paper investigates the conditions under which cues from social signals can be used for user adaptation (or user tracking) of a learning agent. In this work we consider the case of the Reinforcement Learning (RL) of a dialogue management module. Social signals (gazes, postures, emotions, etc.) have an undeniable importance in human interactions and can be used as an additional and user-dependent (subjective) reinforcement signal during learning. In this paper, the Kalman Temporal Differences (KTD) framework is employed in combination with a potential-based shaping reward method to properly integrate the social information in the optimisation procedure and adapt the policy to user profiles. In a second step the ability of the method to track a new user profile (after self learning of the user or switch to a new user) is shown. Experiments carried out using a state-of-the-art goal-oriented dialogue management framework with simulations support our claims.},
booktitle = {Proceedings of the 2nd Workshop on Machine Learning for Interactive Systems: Bridging the Gap Between Perception, Action and Communication},
pages = {61–69},
numpages = {9},
keywords = {dialogue management, reward shaping, user adaptation, value function approximation, reinforcement learning, social signals},
location = {Beijing, China},
series = {MLIS '13}
}

@inproceedings{10.5555/1289189.1289246,
author = {Scheffler, Konrad and Young, Steve},
title = {Automatic Learning of Dialogue Strategy Using Dialogue Simulation and Reinforcement Learning},
year = {2002},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper describes a method for automatic design of human-computer dialogue strategies by means of reinforcement learning, using a dialogue simulation tool to model the user behaviour and system recognition performance. To the authors' knowledge this is the first application of a detailed simulation tool to this problem. The simulation tool is trained on a corpus of real user data. Compared to direct state transition modelling, it has the major advantage that different state space representations can be studied without collecting more training data.We applied Q-learning with eligibility traces to obtain policies for a telephone-based cinema information system, comparing the effect of different state space representations and evaluation functions. The policies outperformed handcrafted policies that operated in the same restricted state space, and gave performance similar to the original design that had been through several iterations of manual refinement.},
booktitle = {Proceedings of the Second International Conference on Human Language Technology Research},
pages = {12–19},
numpages = {8},
location = {San Diego, California},
series = {HLT '02}
}

@inproceedings{10.1145/3503161.3547737,
author = {Jin, Xin and Zhao, Shu and Zhang, Le and Zhao, Xin and Deng, Qiang and Xiao, Chaoen},
title = {Attribute Controllable Beautiful Caucasian Face Generation by Aesthetics Driven Reinforcement Learning},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547737},
doi = {10.1145/3503161.3547737},
abstract = {In recent years, image generation has made great strides in improving the quality of images, producing high-fidelity ones. Also, quite recently, there are architecture designs, which enable GAN to unsupervisedly learn the semantic attributes represented in different layers. However, there is still a lack of research on generating face images more consistent with human aesthetics. Based on EigenGAN [He et al., ICCV 2021], we build the techniques of reinforcement learning into the generator of EigenGAN. The agent tries to figure out how to alter the semantic attributes of the generated human faces towards more preferable ones. To accomplish this, we trained an aesthetics scoring model that can conduct facial beauty prediction. We also can utilize this scoring model to analyze the correlation between face attributes and aesthetics scores. Empirically, using off-the-shelf techniques from reinforcement learning would not work well. So instead, we present a new variant incorporating the ingredients emerging in the reinforcement learning communities in recent years. Compared to the original generated images, the adjusted ones show clear distinctions concerning various attributes. Experimental results using the MindSpore, show the effectiveness of the proposed method. Altered facial images are commonly more attractive, with significantly improved aesthetic levels.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {6964–6966},
numpages = {3},
keywords = {image aesthetics, face generation, reinforcement learning},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3397536.3422244,
author = {Pang, Yanbo and Tsubouchi, Kota and Yabe, Takahiro and Sekimoto, Yoshihide},
title = {Intercity Simulation of Human Mobility at Rare Events via Reinforcement Learning},
year = {2020},
isbn = {9781450380195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397536.3422244},
doi = {10.1145/3397536.3422244},
abstract = {Agent-based simulations, combined with large scale mobility data, have been an effective method for understanding urban scale human dynamics. However, collecting such large scale human mobility datasets are especially difficult during rare events (e.g., natural disasters), reducing the performance of agent-based simulations. To tackle this problem, we develop an agent-based model that can simulate urban dynamics during rare events by learning from other cities using inverse reinforcement learning. More specifically, in our framework, agents imitate real human-beings' travel behavior from areas where rare events have occurred in the past (source area) and produce synthetic people movement in different cities where such rare events have never occurred (target area). Our framework contains three main stages: 1) recovering the reward function, where the people's travel patterns and preferences are learned from the source areas; 2) transferring the model of the source area to the target areas; 3) simulating the people movement based on learned model in the target area. We apply our approach in various cities for both normal and rare situations using real-world GPS data collected from more than 1 million people in Japan, and show higher simulation performance than previous models.},
booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
pages = {293–302},
numpages = {10},
keywords = {Reinforcement Learning, Urban Computing, Human Mobility, People Flow Simulation},
location = {Seattle, WA, USA},
series = {SIGSPATIAL '20}
}

@article{10.1145/3469860,
author = {Tao, Shuo and Jiang, Jingang and Lian, Defu and Zheng, Kai and Chen, Enhong},
title = {Predicting Human Mobility with Reinforcement-Learning-Based Long-Term Periodicity Modeling},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3469860},
doi = {10.1145/3469860},
abstract = {Mobility prediction plays an important role in a wide range of location-based applications and services. However, there are three problems in the existing literature: (1) explicit high-order interactions of spatio-temporal features are not systemically modeled; (2) most existing algorithms place attention mechanisms on top of recurrent network, so they can not allow for full parallelism and are inferior to self-attention for capturing long-range dependence; (3) most literature does not make good use of long-term historical information and do not effectively model the long-term periodicity of users. To this end, we propose MoveNet and RLMoveNet. MoveNet is a self-attention-based sequential model, predicting each user’s next destination based on her most recent visits and historical trajectory. MoveNet first introduces a cross-based learning framework for modeling feature interactions. With self-attention on both the most recent visits and historical trajectory, MoveNet can use an attention mechanism to capture the user’s long-term regularity in a more efficient way. Based on MoveNet, to model long-term periodicity more effectively, we add the reinforcement learning layer and named RLMoveNet. RLMoveNet regards the human mobility prediction as a reinforcement learning problem, using the reinforcement learning layer as the regularization part to drive the model to pay attention to the behavior with periodic actions, which can help us make the algorithm more effective. We evaluate both of them with three real-world mobility datasets. MoveNet outperforms the state-of-the-art mobility predictor by around 10\% in terms of accuracy, and simultaneously achieves faster convergence and over 4x training speedup. Moreover, RLMoveNet achieves higher prediction accuracy than MoveNet, which proves that modeling periodicity explicitly from the perspective of reinforcement learning is more effective.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {dec},
articleno = {78},
numpages = {23},
keywords = {self-attention, reinforcement learning, Human mobility prediction}
}

