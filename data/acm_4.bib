@article{10.5555/1953048.2021028,
author = {Choi, Jaedeug and Kim, Kee-Eung},
title = {Inverse Reinforcement Learning in Partially Observable Environments},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behavior of an expert. Most of the existing IRL algorithms assume that the environment is modeled as a Markov decision process (MDP), although it is desirable to handle partially observable settings in order to handle more realistic scenarios. In this paper, we present IRL algorithms for partially observable environments that can be modeled as a partially observable Markov decision process (POMDP). We deal with two cases according to the representation of the given expert's behavior, namely the case in which the expert's policy is explicitly given, and the case in which the expert's trajectories are available instead. The IRL in POMDPs poses a greater challenge than in MDPs since it is not only ill-posed due to the nature of IRL, but also computationally intractable due to the hardness in solving POMDPs. To overcome these obstacles, we present algorithms that exploit some of the classical results from the POMDP literature. Experimental results on several benchmark POMDP domains show that our work is useful for partially observable settings.},
journal = {J. Mach. Learn. Res.},
month = {jul},
pages = {691–730},
numpages = {40}
}

@inproceedings{10.1145/3505688.3505699,
author = {Ma, Tengcong and Song, Ruizhuo},
title = {Event-Triggered Suboptimal Control Based Adaptive Reinforcement Learning},
year = {2022},
isbn = {9781450385855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505688.3505699},
doi = {10.1145/3505688.3505699},
abstract = {The paper presents event-triggered suboptimal control based adaptive reinforcement learning. Linear quadratic optimal control requires the use of all state variables feedback, but in engineering practice, not all states can be measured or easy to be measured. Therefore, suboptimal control becomes very significant. Under event-triggered (ET) mechanism, we give the expression of suboptimal control, propose a novel triggering condition and prove the stability of close-loop system. Adaptive Q-learning is a kind of reinforcement learning, which is used to structure critic network. Finally, simulation example is represented to show the proposed is valid.},
booktitle = {Proceedings of the 7th International Conference on Robotics and Artificial Intelligence},
pages = {64–69},
numpages = {6},
keywords = {Reinforcement Learning, Event-triggered, Suboptimal Control, Adaptive Q-learning},
location = {Guangzhou, China},
series = {ICRAI '21}
}

@inproceedings{10.5555/3545946.3598718,
author = {Daoudi, Paul and Robu, Bogdan and Prieur, Christophe and Dos Santos, Ludovic and Barlier, Merwan},
title = {Enhancing Reinforcement Learning Agents with Local Guides},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper addresses the problem of integrating local guide policies into a Reinforcement Learning agent. For this, we show how to adapt existing algorithms to this setting before introducing a novel algorithm based on a noisy policy-switching procedure. This approach builds on a proper Approximate Policy Evaluation (APE) scheme to provide a perturbation that carefully leads the local guides towards better actions. We evaluated our method on a set of classical Reinforcement Learning problems, including safety-critical systems where the agent cannot enter some areas at the risk of triggering catastrophic consequences. In all the proposed environments, our agent proved to be efficient at leveraging those policies to improve the performance of any APE-based Reinforcement Learning algorithm, especially in its first learning stages.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {829–838},
numpages = {10},
keywords = {optimal control, reinforcement learning, guide, expert, constrained optimization, sample efficiency},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.1145/2442087.2442095,
author = {Shen, Hao and Tan, Ying and Lu, Jun and Wu, Qing and Qiu, Qinru},
title = {Achieving Autonomous Power Management Using Reinforcement Learning},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/2442087.2442095},
doi = {10.1145/2442087.2442095},
abstract = {System level power management must consider the uncertainty and variability that come from the environment, the application and the hardware. A robust power management technique must be able to learn the optimal decision from past events and improve itself as the environment changes. This article presents a novel on-line power management technique based on model-free constrained reinforcement learning (Q-learning). The proposed learning algorithm requires no prior information of the workload and dynamically adapts to the environment to achieve autonomous power management. We focus on the power management of the peripheral device and the microprocessor, two of the basic components of a computer. Due to their different operating behaviors and performance considerations, these two types of devices require different designs of Q-learning agent. The article discusses system modeling and cost function construction for both types of Q-learning agent. Enhancement techniques are also proposed to speed up the convergence and better maintain the required performance (or power) constraint in a dynamic system with large variations. Compared with the existing machine learning based power management techniques, the Q-learning based power management is more flexible in adapting to different workload and hardware and provides a wider range of power-performance tradeoff.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {apr},
articleno = {24},
numpages = {32},
keywords = {machine learning, thermal management, Power management, computer}
}

@inproceedings{10.1145/3385032.3385041,
author = {Clark, Tony and Barn, Balbir and Kulkarni, Vinay and Barat, Souvik},
title = {Language Support for Multi Agent Reinforcement Learning},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385041},
doi = {10.1145/3385032.3385041},
abstract = {Software Engineering must increasingly address the issues of complexity and uncertainty that arise when systems are to be deployed into a dynamic software ecosystem. There is also interest in using digital twins of systems in order to design, adapt and control them when faced with such issues. The use of multi-agent systems in combination with reinforcement learning is an approach that will allow software to intelligently adapt to respond to changes in the environment. This paper proposes a language extension that encapsulates learning-based agents and system building operations and shows how it is implemented in ESL. The paper includes examples the key features and describes the application of agent-based learning implemented in ESL applied to a real-world supply chain.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference on Formerly Known as India Software Engineering Conference},
articleno = {7},
numpages = {12},
keywords = {Reinforcement Learning, Agents},
location = {Jabalpur, India},
series = {ISEC 2020}
}

@inproceedings{10.1145/2598394.2605694,
author = {Meseguer Llopis, Joan and Rajewski, \L{}ukasz and Kuklinski, S\l{}awomir},
title = {Reinforcement Learning Based Energy Efficient LTE RAN},
year = {2014},
isbn = {9781450328814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598394.2605694},
doi = {10.1145/2598394.2605694},
abstract = {Reducing power consumption in LTE networks has become an important issue for mobile network operators. The 3GPP organization has included such operation as one of SON (Self-Organizing Networks) functions [1][2]. Using the approach presented in this paper the decision about turning Radio Access Network (RAN) nodes off and on, according to the network load (which is typically low at night), is taken into account. The process is controlled using a combination of Fuzzy Logic and Q-Learning techniques (FQL). The effectiveness of the proposed approach has been evaluated using the LTE-Sim simulator with some extensions. The simulations are very close to real network implementation: we used the RAN node parameters that are defined by 3GPP and simulations take into account the network behaviour in the micro time scale.},
booktitle = {Proceedings of the Companion Publication of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1197–1204},
numpages = {8},
keywords = {reinforcement learning, son, energy savings, lte, machine learning},
location = {Vancouver, BC, Canada},
series = {GECCO Comp '14}
}

@inproceedings{10.1145/3489517.3530512,
author = {Jin, Zhou and Pei, Haojie and Dong, Yichao and Jin, Xiang and Wu, Xiao and Xing, Wei W. and Niu, Dan},
title = {Accelerating Nonlinear DC Circuit Simulation with Reinforcement Learning},
year = {2022},
isbn = {9781450391429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489517.3530512},
doi = {10.1145/3489517.3530512},
abstract = {DC analysis is the foundation for nonlinear electronic circuit simulation. Pseudo transient analysis (PTA) methods have gained great success among various continuation algorithms. However, PTA tends to be computationally intensive without careful tuning of parameters and proper stepping strategies. In this paper, we harness the latest advancing in machine learning to resolve these challenges simultaneously. Particularly, an active learning is leveraged to provide a fine initial solver environment, in which a TD3-based Reinforcement Learning (RL) is implemented to accelerate the simulation on the fly. The RL agent is strengthen with dual agents, priority sampling, and cooperative learning to enhance its robustness and convergence. The proposed algorithms are implemented in an out-of-the-box SPICElike simulator, which demonstrated a significant speedup: up to 3.1X for the initial stage and 234X for the RL stage.},
booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference},
pages = {619–624},
numpages = {6},
keywords = {DC analysis, pseudo transient analysis, reinforcement learning, circuit simulation, nonlinear equations},
location = {San Francisco, California},
series = {DAC '22}
}

@article{10.5555/2946645.3007080,
author = {Klenske, Edgar D. and Hennig, Philipp},
title = {Dual Control for Approximate Bayesian Reinforcement Learning},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {Control of non-episodic, finite-horizon dynamical systems with uncertain dynamics poses a tough and elementary case of the exploration-exploitation trade-off. Bayesian reinforcement learning, reasoning about the effect of actions and future observations, offers a principled solution, but is intractable. We review, then extend an old approximate approach from control theory--where the problem is known as dual control--in the context of modern regression methods, specifically generalized linear regression. Experiments on simulated systems show that this framework offers a useful approximation to the intractable aspects of Bayesian RL, producing structured exploration strategies that differ from standard RL approaches. We provide simple examples for the use of this framework in (approximate) Gaussian process regression and feedforward neural networks for the control of exploration.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {4354–4383},
numpages = {30},
keywords = {control, reinforcement learning, Bayesian inference, Gaussian processes filtering}
}

@inproceedings{10.1145/3522664.3528609,
author = {Kanso, Ali and Patra, Kinshuman},
title = {Engineering a Platform for Reinforcement Learning Workloads},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528609},
doi = {10.1145/3522664.3528609},
abstract = {Reinforcement Learning (RL) is an area of machine learning concerned with teaching intelligent agents to take desired actions in a specific environment. The teaching part can be performed in a simulated environment where the agent can learn how to react to the (simulated) current state in order to reach a desired state. Offering Reinforcement Learning as a service with stringent reliability and scalability requirements, entails a set of challenges at both the architectural and implementation level. In this paper we present the Bonsai platform for RL workloads. We discuss the requirements, design and implementation of the Bonsai platform.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {88–89},
numpages = {2},
keywords = {cloud computing, kubernetes, auto-scaling, machine learning, reinforcement learning, reliability},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@article{10.5555/1005332.1016794,
author = {Sallans, Brian and Hinton, Geoffrey E.},
title = {Reinforcement Learning with Factored States and Actions},
year = {2004},
issue_date = {12/1/2004},
publisher = {JMLR.org},
volume = {5},
issn = {1532-4435},
abstract = {A novel approximation method is presented for approximating the value function and selecting good actions for Markov decision processes with large state and action spaces. The method approximates state-action values as negative free energies in an undirected graphical model called a product of experts. The model parameters can be learned efficiently because values and derivatives can be efficiently computed for a product of experts. Actions can be found even in large factored action spaces by the use of Markov chain Monte Carlo sampling. Simulation results show that the product of experts approximation can be used to solve large problems. In one simulation it is used to find actions in action spaces of size 240.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {1063–1088},
numpages = {26}
}

@inproceedings{10.5555/3586210.3586438,
author = {Jiang, Jinyang and Peng, Yijie and Hu, Jiaqiao},
title = {Quantile-Based Policy Optimization for Reinforcement Learning},
year = {2023},
publisher = {IEEE Press},
abstract = {Classical reinforcement learning (RL) aims to optimize the expected cumulative rewards. In this work, we consider the RL setting where the goal is to optimize the quantile of the cumulative rewards. We parameterize the policy controlling actions by neural networks and propose a novel policy gradient algorithm called Quantile-Based Policy Optimization (QPO) and its variant Quantile-Based Proximal Policy Optimization (QPPO) to solve deep RL problems with quantile objectives. QPO uses two coupled iterations running at different time scales for simultaneously estimating quantiles and policy parameters. Our numerical results demonstrate that the proposed algorithms outperform the existing baseline algorithms under the quantile criterion.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2712–2723},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3386290.3396930,
author = {Huang, Tianchi and Zhang, Rui-Xiao and Sun, Lifeng},
title = {Self-Play Reinforcement Learning for Video Transmission},
year = {2020},
isbn = {9781450379458},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386290.3396930},
doi = {10.1145/3386290.3396930},
abstract = {Video transmission services adopt adaptive algorithms to ensure users' demands. Existing techniques are often optimized and evaluated by a function that linearly combines several weighted metrics. Nevertheless, we observe that the given function fails to describe the requirement accurately. Thus, such proposed methods might eventually violate the original needs. To eliminate this concern, we propose Zwei, a self-play reinforcement learning algorithm for video transmission tasks. Zwei aims to update the policy by straightforwardly utilizing the actual requirement. Technically, Zwei samples a number of trajectories from the same starting point, and instantly estimates the win rate w.r.t the competition outcome. Here the competition result represents which trajectory is closer to the assigned requirement. Subsequently, Zwei optimizes the strategy by maximizing the win rate. To build Zwei, we develop simulation environments, design adequate neural network models, and invent training methods for dealing with different requirements on various video transmission scenarios. Trace-driven analysis over two representative tasks demonstrates that Zwei optimizes itself according to the assigned requirement faithfully, outperforming the state-of-the-art methods under all considered scenarios.},
booktitle = {Proceedings of the 30th ACM Workshop on Network and Operating Systems Support for Digital Audio and Video},
pages = {7–13},
numpages = {7},
keywords = {self-play reinforcement learning, video transmission},
location = {Istanbul, Turkey},
series = {NOSSDAV '20}
}

@inproceedings{10.1145/2903220.2903257,
author = {Kofinas, Panagiotis and Vouros, George and Dounis, Anastasios I.},
title = {Energy Management in Solar Microgrid via Reinforcement Learning},
year = {2016},
isbn = {9781450337342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2903220.2903257},
doi = {10.1145/2903220.2903257},
abstract = {This paper proposes a single agent system towards solving energy management issues in solar microgrids. The system considered consists of a Photovoltaic (PV) source, a battery bank, a desalination unit (responsible for providing the demanded water) and a local consumer. The trade-offs and complexities involved in the operation of the different units, and the quality of services' demanded from energy consumer units (e.g. the desalination unit), makes the energy management a challenging task. The goal of the agent is to satisfy the energy demand in the solar microgrid, optimizing the battery usage, in conjunction to satisfying the quality of services provided. It is assumed that the solar microgrid operates in island-mode. Thus, no connection to the electrical grid is considered. The agent collects data from the elements of the system and learns the suitable policy towards optimizing system performance. Simulation results provided, show the performance of the agent.},
booktitle = {Proceedings of the 9th Hellenic Conference on Artificial Intelligence},
articleno = {12},
numpages = {7},
keywords = {microgrid, Reinforcement learning, Q-learning, energy management},
location = {Thessaloniki, Greece},
series = {SETN '16}
}

@inproceedings{10.1145/3448016.3452799,
author = {Sioulas, Panagiotis and Ailamaki, Anastasia},
title = {Scalable Multi-Query Execution Using Reinforcement Learning},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3452799},
doi = {10.1145/3448016.3452799},
abstract = {The growing demand for data-intensive decision support and the migration to multi-tenant infrastructures put databases under the stress of high analytical query load. The requirement for high throughput contradicts the traditional design of query-at-a-time databases that optimize queries for efficient serial execution. Sharing work across queries presents an opportunity to reduce the total cost of processing and therefore improve throughput with increasing query load. Systems can share work either by assessing all opportunities and restructuring batches of queries ahead of execution, or by inspecting opportunities in individual incoming queries at runtime: the former strategy scales poorly to large query counts, as it requires expensive sharing-aware optimization, whereas the latter detects only a subset of the opportunities. Both strategies fail to minimize the cost of processing for large and ad-hoc workloads. This paper presents RouLette, a specialized intelligent engine for multi-query execution that addresses, through runtime adaptation, the shortcomings of existing work-sharing strategies. RouLette scales by replacing sharing-aware optimization with adaptive query processing, and it chooses opportunities to explore and exploit by using reinforcement learning. RouLette also includes optimizations that reduce the adaptation overhead. RouLette increases throughput by 1.6-28.3x, compared to a state-of-the-art query-at-a-time engine, and up to 6.5x, compared to sharing-enabled prototypes, for multi-query workloads based on the schema of TPC-DS.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1651–1663},
numpages = {13},
keywords = {sharing, multi-query optimization, reinforcement learning, join},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.5555/1873738.1873746,
author = {Dethlefs, Nina and Cuay\'{a}huitl, Heriberto},
title = {Hierarchical Reinforcement Learning for Adaptive Text Generation},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We present a novel approach to natural language generation (NLG) that applies hierarchical reinforcement learning to text generation in the wayfinding domain. Our approach aims to optimise the integration of NLG tasks that are inherently different in nature, such as decisions of content selection, text structure, user modelling, referring expression generation (REG), and surface realisation. It also aims to capture existing interdependencies between these areas. We apply hierarchical reinforcement learning to learn a generation policy that captures these interdependencies, and that can be transferred to other NLG tasks. Our experimental results---in a simulated environment---show that the learnt wayfinding policy outperforms a baseline policy that takes reasonable actions but without optimization.},
booktitle = {Proceedings of the 6th International Natural Language Generation Conference},
pages = {37–45},
numpages = {9},
location = {Trim, Co. Meath, Ireland},
series = {INLG '10}
}

@inproceedings{10.5555/3586210.3586439,
author = {Belsare, Sahil and Badilla, Emily Diaz and Dehghanimohammadabadi, Mohammad},
title = {Reinforcement Learning with Discrete Event Simulation: The Premise, Reality, and Promise},
year = {2023},
publisher = {IEEE Press},
abstract = {Several studies have shown the success of Reinforcement Learning (RL) for solving sequential decision-making problems in domains like robotics, autonomous vehicles, manufacturing, supply chain, and health care. For such applications, uncertainty in real-life environments presents a significant challenge in training an RL agent. RL requires a large number of trials (training examples) to learn a good policy. One of the approaches to tackle these obstacles is augmenting RL with a Discrete Event Simulation (DES) model. Learning from a simulated environment, makes the training process of the RL agent more efficient, faster, and even safer by alleviating the need for expensive real-world trials. Therefore, integrating RL algorithms with simulation environments has inspired many researchers in recent years. In this paper, we analyze the existing literature on RL models using DES to put forward the benefits, application areas, challenges, and scope for future work in developing such models for industrial use cases.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2724–2735},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.5555/3237383.3237955,
author = {Adjodah, Dhaval},
title = {Decentralized Reinforcement Learning Inspired by Multiagent Systems},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Existence can perhaps be viewed an exercise of searching high-dimensional, rugged, and approximated (using training data) landscapes for (often time-delayed) rewards. Bounded rationality imposes limits on the success of solutions that can be found by agents acting alone, causing them to potentially get stuck in 'effective local minima'. To overcome these limits, agents can communicate and work together. Historically, machine learning problems and algorithms were far enough from these limits that all problems could be abstracted as a single agent/model which was optimizing a loss using signals from the environment. However, we are now entering an era where the theoretical and engineering insights from multiagent systems and collective intelligence are becoming, again, critical for the continued growth and usefulness of large-scale real-world machine learning. Theoretically, for example, modern reinforcement learning algorithms and problems are now high-dimensional and rugged enough that a collection of agents are often run in parallel (sometimes asynchronously) to speed up training because learning is fundamentally experience-based - the diversity and uniqueness of search trajectories of agents is of prime importance. Beyond reinforcement learning, machine learning models have gained from being trained as a collective through approaches ranging from student-teacher mechanisms (to transfer learning more effectively between agents), to population/evolutionary methods (to search more broadly the landscapes). Engineering-wise, because we are still far from replicating human intelligence, we must build better interfaces for how humans and algorithms could collaborate for increased performance. But because humans are known to have very low communication bandwidth and are prone to biases, work must be done to understand not only how humans learn individually (for which there is extensive neuroscience, cognitive science, etc) but also how they learn from each other - what kind of data and model approximations do they make (and the biases and ensue), what network topologies are best for collaboration or exploration, what cognitive models do they use to sample and update their beliefs, etc. For example, personal AI assistants need to be able to learn more seamlessly from humans interaction.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1729–1730},
numpages = {2},
keywords = {reinforcement learning, decentralized optimization, multiagent systems, collective intelligence},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/1389095.1389371,
author = {Metzen, Jan Hendrik and Kirchner, Frank and Edgington, Mark and Kassahun, Yohannes},
title = {Towards Efficient Online Reinforcement Learning Using Neuroevolution},
year = {2008},
isbn = {9781605581309},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1389095.1389371},
doi = {10.1145/1389095.1389371},
abstract = {For many complex Reinforcement Learning (RL) problems with large and continuous state spaces, neuroevolution has achieved promising results. This is especially true when there is noise in sensor and/or actuator signals. These results have mainly been obtained in offline learning settings, where the training and the evaluation phases of the systems are separated. In contrast, for online RL tasks, the actual performance of a system matters during its learning phase. In these tasks, neuroevolutionary systems are often impaired by their purely exploratory nature, meaning that they usually do not use (i.e. exploit) their knowledge of a single individual's performance to improve performance during learning. In this paper we describe modifications that significantly improve the online performance of the neuroevolutionary method Evolutionary Acquisition of Neural Topologies and discuss the results obtained in the Mountain Car benchmark.},
booktitle = {Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation},
pages = {1425–1426},
numpages = {2},
keywords = {neuroevolution, online-learning, reinforcement learning},
location = {Atlanta, GA, USA},
series = {GECCO '08}
}

@inproceedings{10.5555/3535850.3535956,
author = {Neustroev, Grigory and Andringa, Sytze P. E. and Verzijlbergh, Remco A. and De Weerdt, Mathijs M.},
title = {Deep Reinforcement Learning for Active Wake Control},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Wind farms suffer from so-called wake effects: when turbines are located in the wind shadows of other turbines, their power output is substantially reduced. These losses can be partially mitigated via actively changing the yaw from the individually optimal direction. Most existing wake control techniques have two major limitations: they use simplified wake models to optimize the control strategy, and they assume that the atmospheric conditions remain stable. In this paper, we address these limitations by applying reinforcement learning (RL). RL forgoes the wake model entirely and learns an optimal control strategy based on the observed atmospheric conditions and a reward signal, in this case the power output of the farm. It also accounts for random transitions in the observations, such as turbulent fluctuations in the wind. To evaluate RL for active wake control, we provide a simulator based on the state-of-the-art FLORIS model in the OpenAI gym format. Next, we propose three different state-action representations of the active wake control problem and investigate their effect on the performance of RL-based wake control. Finally, we compare RL to a state-of-the-art wake control strategy based on FLORIS and show that RL is less sensitive to changes in unobservable data.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {944–953},
numpages = {10},
keywords = {active wake control, deep reinforcement learning, wind energy},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/2739482.2756582,
author = {Drugan, Madalina M.},
title = {Synergies between Evolutionary Algorithms and Reinforcement Learning},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2756582},
doi = {10.1145/2739482.2756582},
abstract = {A recent trend in evolutionary algorithms (EAs) transfers expertise from and to other areas of machine learning. An interesting novel symbiosis considers: i) reinforcement learning (RL), which learns on-line and off-line difficult dynamic elaborated tasks requiring lots of computational resources, and ii) EAs with the main strength its eloquence and computational efficiency. These two techniques address the same problem of reward maximization in difficult environments that can include stochasticity. Sometimes, they exchange techniques in order to improve their theoretical and empirical efficiency, like computational speed for on-line learning, and robust behaviour for the off-line optimisation algorithms. For example, multi-objective RL uses tuples of rewards instead of a single reward value and techniques from multi-objective EAs should be integrated for an efficient exploration/exploitation trade-off. The problem of selecting the best genetic operator is similar to the problem an agent faces when choosing between alternatives in achieving its goal of maximising its cumulative expected reward. Practical approaches select the RL method that solve the best online operator selection problem.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {723–740},
numpages = {18},
keywords = {multi-objective optimisation, multi-armed bandits, reinforcement learning},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@article{10.1145/3592113,
author = {Cao, Hezhi and Xia, Xi and Wu, Guan and Hu, Ruizhen and Liu, Ligang},
title = {ScanBot: Autonomous Reconstruction via Deep Reinforcement Learning},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592113},
doi = {10.1145/3592113},
abstract = {Autoscanning of an unknown environment is the key to many AR/VR and robotic applications. However, autonomous reconstruction with both high efficiency and quality remains a challenging problem. In this work, we propose a reconstruction-oriented autoscanning approach, called ScanBot, which utilizes hierarchical deep reinforcement learning techniques for global region-of-interest (ROI) planning to improve the scanning efficiency and local next-best-view (NBV) planning to enhance the reconstruction quality. Given the partially reconstructed scene, the global policy designates an ROI with insufficient exploration or reconstruction. The local policy is then applied to refine the reconstruction quality of objects in this region by planning and scanning a series of NBVs. A novel mixed 2D-3D representation is designed for these policies, where a 2D quality map with tailored quality channels encoding the scanning progress is consumed by the global policy, and a coarse-to-fine 3D volumetric representation that embodies both local environment and object completeness is fed to the local policy. These two policies iterate until the whole scene has been completely explored and scanned. To speed up the learning of complex environmental dynamics and enhance the agent's memory for spatial-temporal inference, we further introduce two novel auxiliary learning tasks to guide the training of our global policy. Thorough evaluations and comparisons are carried out to show the feasibility of our proposed approach and its advantages over previous methods. Code and data are available at https://github.com/HezhiCao/Scanbot.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {157},
numpages = {16},
keywords = {hierarchical reinforcement learning, autonomous reconstruction, auxiliary learning tasks, indoor scene reconstruction}
}

@inproceedings{10.1145/1553374.1553512,
author = {Vlassis, Nikos and Toussaint, Marc},
title = {Model-Free Reinforcement Learning as Mixture Learning},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553512},
doi = {10.1145/1553374.1553512},
abstract = {We cast model-free reinforcement learning as the problem of maximizing the likelihood of a probabilistic mixture model via sampling, addressing both the infinite and finite horizon cases. We describe a Stochastic Approximation EM algorithm for likelihood maximization that, in the tabular case, is equivalent to a non-bootstrapping optimistic policy iteration algorithm like Sarsa(1) that can be applied both in MDPs and POMDPs. On the theoretical side, by relating the proposed stochastic EM algorithm to the family of optimistic policy iteration algorithms, we provide new tools that permit the design and analysis of algorithms in that family. On the practical side, preliminary experiments on a POMDP problem demonstrated encouraging results.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {1081–1088},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{10.1145/3511808.3557474,
author = {Zha, Daochen and Lai, Kwei-Herng and Tan, Qiaoyu and Ding, Sirui and Zou, Na and Hu, Xia Ben},
title = {Towards Automated Imbalanced Learning with Deep Hierarchical Reinforcement Learning},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557474},
doi = {10.1145/3511808.3557474},
abstract = {Imbalanced learning is a fundamental challenge in data mining, where there is a disproportionate ratio of training samples in each class. Over-sampling is an effective technique to tackle imbalanced learning through generating synthetic samples for the minority class. While numerous over-sampling algorithms have been proposed, they heavily rely on heuristics, which could be sub-optimal since we may need different sampling strategies for different datasets and base classifiers, and they cannot directly optimize the performance metric. Motivated by this, we investigate developing a learning-based over-sampling algorithm to optimize the classification performance, which is a challenging task because of the huge and hierarchical decision space. At the high level, we need to decide how many synthetic samples to generate. At the low level, we need to determine where the synthetic samples should be located, which depends on the high-level decision since the optimal locations of the samples may differ for different numbers of samples. To address the challenges, we propose AutoSMOTE, an automated over-sampling algorithm that can jointly optimize different levels of decisions. Motivated by the success of SMOTE and its extensions, we formulate the generation process as a Markov decision process (MDP) consisting of three levels of policies to generate synthetic samples within the SMOTE search space. Then we leverage deep hierarchical reinforcement learning to optimize the performance metric on the validation data. Extensive experiments on six real-world datasets demonstrate that AutoSMOTE significantly outperforms the state-of-the-art resampling algorithms. The code is at https://github.com/daochenzha/autosmote},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {2476–2485},
numpages = {10},
keywords = {automated machine learning, classification, reinforcement learning, imbalanced learning},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.5555/3463952.3464236,
author = {Nikou, Alexandros and Mujumdar, Anusha and Orli\'{c}, Marin and Vulgarakis Feljan, Aneta},
title = {Symbolic Reinforcement Learning for Safe RAN Control},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper, we demonstrate a Symbolic Reinforcement Learning (SRL) architecture for safe control in Radio Access Network (RAN) applications. In our automated tool, a user can select a high-level safety specifications expressed in Linear Temporal Logic (LTL) to shield an RL agent running in a given cellular network with aim of optimizing network performance, as measured through certain Key Performance Indicators (KPIs). In the proposed architecture, network safety shielding is ensured through model-checking techniques over combined discrete system models (automata) that are abstracted through reinforcement learning. We demonstrate the user interface (UI) helping the user set intent specifications to the architecture and inspect the difference in allowed and blocked actions.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1782–1784},
numpages = {3},
keywords = {safety, network control, reinforcement learning, ltl specifications, ran control},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3194718.3194720,
author = {Kim, Junhwi and Kwon, Minhyuk and Yoo, Shin},
title = {Generating Test Input with Deep Reinforcement Learning},
year = {2018},
isbn = {9781450357418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194718.3194720},
doi = {10.1145/3194718.3194720},
abstract = {Test data generation is a tedious and laborious process. Search-based Software Testing (SBST) automatically generates test data optimising structural test criteria using metaheuristic algorithms. In essence, metaheuristic algorithms are systematic trial-and-error based on the feedback of fitness function. This is similar to an agent of reinforcement learning which iteratively decides an action based on the current state to maximise the cumulative reward. Inspired by this analogy, this paper investigates the feasibility of employing reinforcement learning in SBST to replace human designed meta-heuristic algorithms. We reformulate the software under test (SUT) as an environment of reinforcement learning. At the same time, we present GunPowder, a novel framework for SBST which extends SUT to the environment. We train a Double Deep Q-Networks (DDQN) agent with deep neural network and evaluate the effectiveness of our approach by conducting a small empirical study. Finally, we find that agents can learn metaheuristic algorithms for SBST, achieving 100\% branch coverage for training functions. Our study sheds light on the future integration of deep neural network and SBST.},
booktitle = {Proceedings of the 11th International Workshop on Search-Based Software Testing},
pages = {51–58},
numpages = {8},
location = {Gothenburg, Sweden},
series = {SBST '18}
}

@article{10.1613/jair.1.13854,
author = {Mazoure, Bogdan and Doan, Thang and Li, Tianyu and Makarenkov, Vladimir and Pineau, Joelle and Precup, Doina and Rabusseau, Guillaume},
title = {Low-Rank Representation of Reinforcement Learning Policies},
year = {2022},
issue_date = {Dec 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {75},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13854},
doi = {10.1613/jair.1.13854},
abstract = {We propose a general framework for policy representation for reinforcement learning tasks. This framework involves finding a low-dimensional embedding of the policy on a reproducing kernel Hilbert space (RKHS). The usage of RKHS based methods allows us to derive strong theoretical guarantees on the expected return of the reconstructed policy. Such guarantees are typically lacking in black-box models, but are very desirable in tasks requiring stability and convergence guarantees. We conduct several experiments on classic RL domains. The results confirm that the policies can be robustly represented in a low-dimensional space while the embedded policy incurs almost no decrease in returns.},
journal = {J. Artif. Int. Res.},
month = {dec},
numpages = {40}
}

@inproceedings{10.1145/3449639.3459386,
author = {Ha, Myoung Hoon and Chi, Seung-geun and Lee, Sangyeop and Cha, Yujin and Byung-Ro, Moon},
title = {Evolutionary Meta Reinforcement Learning for Portfolio Optimization},
year = {2021},
isbn = {9781450383509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449639.3459386},
doi = {10.1145/3449639.3459386},
abstract = {Portfolio optimization is a control problem whose objective is to find the optimal strategy for the process of selecting the proportions of assets that can provide the maximum return. Conventional approaches formulate the problem as a single Markov decision process and apply reinforcement learning methods to provide solutions. However, it is well known that financial markets involve non-stationary processes, leading to violations of this assumption in these methods. In this work, we reformulate the portfolio optimization problem to deal with the non-stationary nature of financial markets. In our approach, we divide a long-term process into multiple short-term processes to adapt to context changes and consider the portfolio optimization problem as a multitask control problem. Thereafter, we propose an evolutionary meta reinforcement learning approach to search for an initial policy that can quickly adapt to the upcoming target tasks. We model the policies as convolutional networks that can score the match of the patterns in market data charts. Finally, we test our approach using real-world cryptographic currency data and show that it adapts well to the changes in the market and leads to better profitability.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {964–972},
numpages = {9},
keywords = {portfolio optimization, meta reinforcement learning, evolution strategies, baldwinian evolution, lamarckian evolution},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{10.1145/1329125.1329178,
author = {Burkov, Andriy and Chaib-draa, Brahim},
title = {Reducing the Complexity of Multiagent Reinforcement Learning},
year = {2007},
isbn = {9788190426275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1329125.1329178},
doi = {10.1145/1329125.1329178},
abstract = {It is known that the complexity of the reinforcement learning algorithms, such as Q-learning, may be exponential in the number of environment's states. It was shown, however, that the learning complexity for the goal-directed problems may be substantially reduced by initializing the Q-values with a "good" approximative function. In the multiagent case, there exists such a good approximation for a big class of problems, namely, for goal-directed stochastic games. These games, for example, can reflect coordination and common interest problems of cooperative robotics. The approximative function for these games is nothing but the relaxed, single-agent, problem solution, which can easily be found by each agent individually. In this article, we show that (1) an optimal single-agent solution is a "good" approximation for the goal-directed stochastic games with action-penalty representation and (b) the complexity is reduced when the learning is initialized with this approximative function, as compared to the uninformed case.},
booktitle = {Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems},
articleno = {44},
numpages = {3},
keywords = {Q-learning, initialization, multiagent learning, stochastic games},
location = {Honolulu, Hawaii},
series = {AAMAS '07}
}

@inproceedings{10.1145/3331453.3360975,
author = {Zhou, Wenhong and Chen, Yiting and Li, Jie},
title = {Competitive Evolution Multi-Agent Deep Reinforcement Learning},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3360975},
doi = {10.1145/3331453.3360975},
abstract = {As an effective method to solve the optimal policy in multi-agent systems, multi-agent deep reinforcement learning (MADRL) has achieved impressive results in many applications. However, the training of a deep neural network used in MADRL is time-consuming and laborious. In order to improve the training efficiency and accelerate the training speed of the neural network, this paper proposed a sharing learning framework to support the effectively training of multi-agent state-action value function neural network through making full use of sharing diverse experience from different agents to train a single shared network. Besides, a novel learning algorithm combined with competition and evolution idea was developed to accelerate the training process and improve the performance of the trained neural network. Finally, the experiment results prove the effectiveness of the proposed framework and the fact that the proposed algorithm with an appropriate update rate can indeed accelerate the training process and improve the performance of the trained network by providing a well-matched opponent.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {24},
numpages = {6},
keywords = {Sharing learning framework, Cooperation, Multi-agent deep reinforcement learning, Competitive evolution},
location = {Sanya, China},
series = {CSAE '19}
}

@inproceedings{10.5555/1687878.1687892,
author = {Branavan, S. R. K. and Chen, Harr and Zettlemoyer, Luke S. and Barzilay, Regina},
title = {Reinforcement Learning for Mapping Instructions to Actions},
year = {2009},
isbn = {9781932432459},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains --- Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.},
booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1},
pages = {82–90},
numpages = {9},
location = {Suntec, Singapore},
series = {ACL '09}
}

@article{10.1145/3608479,
author = {Lu, Sidi and Yuan, Xin and Katsaggelos, Aggelos K. and Shi, Weisong},
title = {Reinforcement Learning for Adaptive Video Compressive Sensing},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3608479},
doi = {10.1145/3608479},
abstract = {We apply reinforcement learning to video compressive sensing to adapt the compression ratio. Specifically, video snapshot compressive imaging (SCI), which captures high-speed video using a low-speed camera is considered in this work, in which multiple (B) video frames can be reconstructed from a snapshot measurement. One research gap in previous studies is how to adapt B in the video SCI system for different scenes. In this article, we fill this gap utilizing reinforcement learning (RL). An RL model, as well as various convolutional neural networks for reconstruction, are learned to achieve adaptive sensing of video SCI systems. Furthermore, the performance of an object detection network using directly the video SCI measurements without reconstruction is also used to perform RL-based adaptive video compressive sensing. Our proposed adaptive SCI method can thus be implemented in low cost and real time. Our work takes the technology one step further towards real applications of video SCI.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {aug},
articleno = {81},
numpages = {21},
keywords = {compressive sensing, Image processing, reinforcement learning}
}

@inproceedings{10.1145/1833349.1778859,
author = {Lee, Seong Jae and Popovi\'{c}, Zoran},
title = {Learning Behavior Styles with Inverse Reinforcement Learning},
year = {2010},
isbn = {9781450302104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1833349.1778859},
doi = {10.1145/1833349.1778859},
abstract = {We present a method for inferring the behavior styles of character controllers from a small set of examples. We show that a rich set of behavior variations can be captured by determining the appropriate reward function in the reinforcement learning framework, and show that the discovered reward function can be applied to different environments and scenarios. We also introduce a new algorithm to recover the unknown reward function that improves over the original apprenticeship learning algorithm. We show that the reward function representing a behavior style can be applied to a variety of different tasks, while still preserving the key features of the style present in the given examples. We describe an adaptive process where an author can, with just a few additional examples, refine the behavior so that it has better generalization properties.},
booktitle = {ACM SIGGRAPH 2010 Papers},
articleno = {122},
numpages = {7},
keywords = {data driven animation, human animation, inverse reinforcement learning, optimal control, apprenticeship learning},
location = {Los Angeles, California},
series = {SIGGRAPH '10}
}

@inproceedings{10.5555/3306127.3331700,
author = {Subramanian, Jayakumar and Mahajan, Aditya},
title = {Reinforcement Learning in Stationary Mean-Field Games},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent reinforcement learning has made significant progress in recent years, but it remains a hard problem. Hence, one often resorts to developing learning algorithms for specific classes of multi-agent systems. In this paper we study reinforcement learning in a specific class of multi-agent systems systems called mean-field games. In particular, we consider learning in stationary mean-field games. We identify two different solution concepts---stationary mean-field equilibrium and stationary mean-field social-welfare optimal policy---for such games based on whether the agents are non-cooperative or cooperative, respectively. We then generalize these solution concepts to their local variants using bounded rationality based arguments. For these two local solution concepts, we present two reinforcement learning algorithms. We show that the algorithms converge to the right solution under mild technical conditions and demonstrate this using two numerical examples.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {251–259},
numpages = {9},
keywords = {mean-field games, bounded rationality, multi-agent reinforcement learning, stationary mean-field games},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{10.1145/3607835,
author = {Varshosaz, Mahsa and Ghaffari, Mohsen and Johnsen, Einar Broch and W\k{a}sowski, Andrzej},
title = {Formal Specification and Testing for Reinforcement Learning},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {ICFP},
url = {https://doi.org/10.1145/3607835},
doi = {10.1145/3607835},
abstract = {The development process for reinforcement learning applications is still exploratory rather than systematic. This exploratory nature reduces reuse of specifications between applications and increases the chances of introducing programming errors. This paper takes a step towards systematizing the development of reinforcement learning applications. We introduce a formal specification of reinforcement learning problems and algorithms, with a particular focus on temporal difference methods and their definitions in backup diagrams. We further develop a test harness for a large class of reinforcement learning applications based on temporal difference learning, including SARSA and Q-learning. The entire development is rooted in functional programming methods; starting with pure specifications and denotational semantics, ending with property-based testing and using compositional interpreters for a domain-specific term language as a test oracle for concrete implementations. We demonstrate the usefulness of this testing method on a number of examples, and evaluate with mutation testing. We show that our test suite is effective in killing mutants (90\% mutants killed for 75\% of subject agents). More importantly, almost half of all mutants are killed by generic write-once-use-everywhere tests that apply to any reinforcement learning problem modeled using our library, without any additional effort from the programmer.},
journal = {Proc. ACM Program. Lang.},
month = {aug},
articleno = {193},
numpages = {34},
keywords = {reinforcement learning, specification-based testing, Scala}
}

@article{10.5555/2503308.2343689,
author = {Konidaris, George and Scheidwasser, Ilya and Barto, Andrew G.},
title = {Transfer in Reinforcement Learning via Shared Features},
year = {2012},
issue_date = {January 2012},
publisher = {JMLR.org},
volume = {13},
number = {1},
issn = {1532-4435},
abstract = {We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to significantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that significantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-specific skills.},
journal = {J. Mach. Learn. Res.},
month = {may},
pages = {1333–1371},
numpages = {39},
keywords = {transfer, reinforcement learning, skills, shaping}
}

@inproceedings{10.5555/3091125.3091262,
author = {Philipp, Patrick and Rettinger, Achim},
title = {Reinforcement Learning for Multi-Step Expert Advice},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Complex tasks for heterogeneous data sources, such as finding and linking named entities in text documents or detecting objects in images, often require multiple steps to be solved in a processing pipeline. In most of the cases, there exist numerous, exchangeable software components for a single step, each an "expert" for data with certain characteristics. Which expert to apply to which observed data instance in which step becomes a challenge that is even hard for humans to decide. In this work, we treat the problem as Single-Agent System (SAS) where a centralized agent learns how to best exploit experts. We therefore define locality-sensitive relational measures for experts and data points, so-called "meta-dependencies", to assess expert performances, and use them for decision-making via Online Model-Free- and Batch Reinforcement Learning (RL) approaches, building on techniques from Contextual Bandits (CBs) and Statistical Relational Learning (SRL). The resulting system automatically learns to pick the best pipeline of experts for a given set of data points. We evaluate our approach for Entity Linking on text corpora with heterogeneous characteristics (such as news articles or tweets). Our empirical results improve the estimation of expert accuracies as well as the out-of-the-box performance of the original experts without manual tuning.},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {962–971},
numpages = {10},
keywords = {reinforcement learning, expert processes, entity linking, decision-making with multi-step expert advice, collective learning},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@inproceedings{10.1145/2983323.2983379,
author = {Han, Miyoung and Senellart, Pierre and Bressan, St\'{e}phane and Wu, Huayu},
title = {Routing an Autonomous Taxi with Reinforcement Learning},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983379},
doi = {10.1145/2983323.2983379},
abstract = {Singapore's vision of a Smart Nation encompasses the development of effective and efficient means of transportation. The government's target is to leverage new technologies to create services for a demand-driven intelligent transportation model including personal vehicles, public transport, and taxis. Singapore's government is strongly encouraging and supporting research and development of technologies for autonomous vehicles in general and autonomous taxis in particular. The design and implementation of intelligent routing algorithms is one of the keys to the deployment of autonomous taxis. In this paper we demonstrate that a reinforcement learning algorithm of the Q-learning family, based on a customized exploration and exploitation strategy, is able to learn optimal actions for the routing autonomous taxis in a real scenario at the scale of the city of Singapore with pick-up and drop-off events for a fleet of one thousand taxis.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {2421–2424},
numpages = {4},
keywords = {reinforcement learning, exploration},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{10.5555/1838206.1838300,
author = {Comanici, Gheorghe and Precup, Doina},
title = {Optimal Policy Switching Algorithms for Reinforcement Learning},
year = {2010},
isbn = {9780982657119},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We address the problem of single-agent, autonomous sequential decision making. We assume that some controllers or behavior policies are given as prior knowledge, and the task of the agent is to learn how to switch between these policies. We formulate the problem using the framework of reinforcement learning and options (Sutton, Precup \&amp; Singh, 1999; Precup, 2000). We derive gradient-based algorithms for learning the termination conditions of options, with the goal of optimizing the expected long-term return. We incorporate the proposed approach into policy-gradient methods with linear function approximation.},
booktitle = {Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1 - Volume 1},
pages = {709–714},
numpages = {6},
keywords = {policy gradient, temporal abstraction, reinforcement learning, Markov decision processes},
location = {Toronto, Canada},
series = {AAMAS '10}
}

@inproceedings{10.1145/1835804.1835817,
author = {Abe, Naoki and Melville, Prem and Pendus, Cezar and Reddy, Chandan K. and Jensen, David L. and Thomas, Vince P. and Bennett, James J. and Anderson, Gary F. and Cooley, Brent R. and Kowalczyk, Melissa and Domick, Mark and Gardinier, Timothy},
title = {Optimizing Debt Collections Using Constrained Reinforcement Learning},
year = {2010},
isbn = {9781450300551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835804.1835817},
doi = {10.1145/1835804.1835817},
abstract = {The problem of optimally managing the collections process by taxation authorities is one of prime importance, not only for the revenue it brings but also as a means to administer a fair taxing system. The analogous problem of debt collections management in the private sector, such as banks and credit card companies, is also increasingly gaining attention. With the recent successes in the applications of data analytics and optimization to various business areas, the question arises to what extent such collections processes can be improved by use of leading edge data modeling and optimization techniques. In this paper, we propose and develop a novel approach to this problem based on the framework of constrained Markov Decision Process (MDP), and report on our experience in an actual deployment of a tax collections optimization system at New York State Department of Taxation and Finance (NYS DTF).},
booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {75–84},
numpages = {10},
keywords = {business analytics and optimization, constrained markov decision process, debt collection optimization, reinforcement learning},
location = {Washington, DC, USA},
series = {KDD '10}
}

@inproceedings{10.5555/3237383.3237850,
author = {Silva, Felipe Leno Da and Costa, Anna Helena Reali},
title = {Object-Oriented Curriculum Generation for Reinforcement Learning},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Autonomously learning a complex task takes a very long time for Reinforcement Learning (RL) agents. One way to learn faster is by dividing a complex task into several simple subtasks and organizing them into a Curriculum that guides Transfer Learning (TL) methods to reuse knowledge in a convenient sequence. However, previous works do not take into account the TL method to build specialized Curricula , leaving the burden of a careful subtask selection to a human. We here contribute novel procedures for: (i) dividing the target task into simpler ones under minimal human supervision; (ii) automatically generating Curricula based on object-oriented task descriptions; and (iii) using generated Curricula for reusing knowledge across tasks. Our experiments show that our proposal achieves a better performance using both manually given and generated subtasks when compared to the state-of-the-art technique in two different domains.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1026–1034},
numpages = {9},
keywords = {transfer learning, reinforcement learning, curriculum learning},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/3449301.3449311,
author = {Liu, Botong},
title = {Implementing Game Strategies Based on Reinforcement Learning},
year = {2021},
isbn = {9781450388597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449301.3449311},
doi = {10.1145/3449301.3449311},
abstract = {Artificial intelligence (AI) technology such as reinforcement learning is increasingly used in playing game in recent years. A deep reinforcement learning model was used to play the game Flappy Bird. This paper aimed to let the computer play a simple game and get the corresponding data for AI learning. Game image was sequentially scaled, grayed, and adjusted for brightness. Before the current frame entered a state, the multi-dimensional image data of several frames of image superposition and combination was processed. Deep Q Network algorithm realized the best action prediction of the game execution in a specific game state, and successfully converted a game decision problem into the classification and recognition problem of instant multi-dimensional images and solved it with a convolutional neural network. After analysis, computer players controlled by deep neural networks had better results than human players. This experiment was a model combined between a deep neural network model and reinforcement learning, and could be applied in other games.},
booktitle = {Proceedings of the 6th International Conference on Robotics and Artificial Intelligence},
pages = {53–56},
numpages = {4},
keywords = {Deep Q Network algorithm, Flappy bird, Multi-dimensional image, Reinforcement learning},
location = {Singapore, Singapore},
series = {ICRAI '20}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00058,
author = {Yerushalmi, Raz},
title = {Enhancing Deep Reinforcement Learning with Executable Specifications},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00058},
doi = {10.1109/ICSE-Companion58688.2023.00058},
abstract = {Deep reinforcement learning (DRL) has become a dominant paradigm for using deep learning to carry out tasks where complex policies are learned for reactive systems. However, these policies are "black-boxes", e.g., opaque to humans and known to be susceptible to bugs. For example, it is hard --- if not impossible --- to guarantee that the trained DRL agent adheres to specific safety and fairness properties that may be required. This doctoral dissertation's first and primary contribution is a novel approach to developing DRL agents, which will improve the DRL training process by pushing the learned policy toward high performance on its main task and compliance with such safety and fairness properties, guaranteeing a high probability of compliance while not compromising the performance of the resulting agent. The approach is realized by incorporating domain-specific knowledge captured as key properties defined by domain experts directly into the DRL optimization process while leveraging behavioral languages that are natural to the domain experts. We have validated the proposed approach by extending the AI-Gym Python framework [1] for training DRL agents and integrating it with the BP-Py framework [2] for specifying scenario-based models [3] in a way that allows scenario objects to affect the training process through reward and cost functions, demonstrating dramatic improvement in the safety and performance of the agent. In addition, we have validated the resulting DRL agents using the Marabou verifier [4], confirming that the resulting agents indeed comply (in full) with the required safety and fairness properties. We have applied the approach, training DRL agents for use cases from network communication and robotic navigation domains, exhibiting strong results. A second contribution of this doctoral dissertation is to develop and leverage probabilistic verification methods for deep neural networks to overcome the current scalability limitations of neural network verification technology, limiting the applicability of verification to practical DRL agents. We carried out an initial validation of the concept in the domain of image classification, showing promising results.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {213–217},
numpages = {5},
keywords = {machine learning, scenario-based modeling, deep reinforcement learning, rule-based specifications, domain expertise},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3310090,
author = {Garc\'{\i}a, Javier and Fern\'{a}ndez, Fernando},
title = {Probabilistic Policy Reuse for Safe Reinforcement Learning},
year = {2019},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3310090},
doi = {10.1145/3310090},
abstract = {This work introduces Policy Reuse for Safe Reinforcement Learning, an algorithm that combines Probabilistic Policy Reuse and teacher advice for safe exploration in dangerous and continuous state and action reinforcement learning problems in which the dynamic behavior is reasonably smooth and the space is Euclidean. The algorithm uses a continuously increasing monotonic risk function that allows for the identification of the probability to end up in failure from a given state. Such a risk function is defined in terms of how far such a state is from the state space known by the learning agent. Probabilistic Policy Reuse is used to safely balance the exploitation of actual learned knowledge, the exploration of new actions, and the request of teacher advice in parts of the state space considered dangerous. Specifically, the π-reuse exploration strategy is used. Using experiments in the helicopter hover task and a business management problem, we show that the π-reuse exploration strategy can be used to completely avoid the visit to undesirable situations while maintaining the performance (in terms of the classical long-term accumulated reward) of the final policy achieved.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {mar},
articleno = {14},
numpages = {24},
keywords = {Reinforcement learning, software agents, case-based reasoning}
}

@inproceedings{10.1145/1553374.1553504,
author = {Taylor, Gavin and Parr, Ronald},
title = {Kernelized Value Function Approximation for Reinforcement Learning},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553504},
doi = {10.1145/1553374.1553504},
abstract = {A recent surge in research in kernelized approaches to reinforcement learning has sought to bring the benefits of kernelized machine learning techniques to reinforcement learning. Kernelized reinforcement learning techniques are fairly new and different authors have approached the topic with different assumptions and goals. Neither a unifying view nor an understanding of the pros and cons of different approaches has yet emerged. In this paper, we offer a unifying view of the different approaches to kernelized value function approximation for reinforcement learning. We show that, except for different approaches to regularization, Kernelized LSTD (KLSTD) is equivalent to a modelbased approach that uses kernelized regression to find an approximate reward and transition model, and that Gaussian Process Temporal Difference learning (GPTD) returns a mean value function that is equivalent to these other approaches. We also discuss the relationship between our modelbased approach and the earlier Gaussian Processes in Reinforcement Learning (GPRL). Finally, we decompose the Bellman error into the sum of transition error and reward error terms, and demonstrate through experiments that this decomposition can be helpful in choosing regularization parameters.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {1017–1024},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{10.1145/3600211.3604669,
author = {Kasirzadeh, Atoosa and Evans, Charles},
title = {User Tampering in Reinforcement Learning Recommender Systems},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604669},
doi = {10.1145/3600211.3604669},
abstract = {In this paper, we introduce new formal methods and provide empirical evidence to highlight a unique safety concern prevalent in reinforcement learning (RL)-based recommendation algorithms – ’user tampering.’ User tampering is a situation where an RL-based recommender system may manipulate a media user’s opinions through its suggestions as part of a policy to maximize long-term user engagement. We use formal techniques from causal modeling to critically analyze prevailing solutions proposed in the literature for implementing scalable RL-based recommendation systems, and we observe that these methods do not adequately prevent user tampering. Moreover, we evaluate existing mitigation strategies for reward tampering issues, and show that these methods are insufficient in addressing the distinct phenomenon of user tampering within the context of recommendations. We further reinforce our findings with a simulation study of an RL-based recommendation system focused on the dissemination of political content. Our study shows that a Q-learning algorithm consistently learns to exploit its opportunities to polarize simulated users with its early recommendations in order to have more consistent success with subsequent recommendations that align with this induced polarization. Our findings emphasize the necessity for developing safer RL-based recommendation systems and suggest that achieving such safety would require a fundamental shift in the design away from the approaches we have seen in the recent literature.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {58–69},
numpages = {12},
keywords = {AI Ethics, Recommendation Systems, Reinforcement Learning, Recommender Systems, Value Alignment, AI Safety},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@inproceedings{10.1145/3308558.3313517,
author = {Fang, Zheng and Cao, Yanan and Li, Qian and Zhang, Dongjie and Zhang, Zhenyu and Liu, Yanbing},
title = {Joint Entity Linking with Deep Reinforcement Learning},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313517},
doi = {10.1145/3308558.3313517},
abstract = {Entity linking is the task of aligning mentions to corresponding entities in a given knowledge base. Previous studies have highlighted the necessity for entity linking systems to capture the global coherence. However, there are two common weaknesses in previous global models. First, most of them calculate the pairwise scores between all candidate entities and select the most relevant group of entities as the final result. In this process, the consistency among wrong entities as well as that among right ones are involved, which may introduce noise data and increase the model complexity. Second, the cues of previously disambiguated entities, which could contribute to the disambiguation of the subsequent mentions, are usually ignored by previous models. To address these problems, we convert the global linking into a sequence decision problem and propose a reinforcement learning model which makes decisions from a global perspective. Our model makes full use of the previous referred entities and explores the long-term influence of current selection on subsequent decisions. We conduct experiments on different types of datasets, the results show that our model outperforms state-of-the-art systems and has better generalization performance.},
booktitle = {The World Wide Web Conference},
pages = {438–447},
numpages = {10},
keywords = {knowledge base, joint disambiguation, reinforcement learning, Entity linking},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.5555/3463952.3463994,
author = {Chen, Kangjie and Guo, Shangwei and Zhang, Tianwei and Li, Shuxin and Liu, Yang},
title = {Temporal Watermarks for Deep Reinforcement Learning Models},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Watermarking has become a popular and attractive technique to protect the Intellectual Property (IP) of Deep Learning (DL) models. However, very few studies explore the possibility of watermarking Deep Reinforcement Learning (DRL) models. Common approaches in the DL context embed backdoors into the protected model and use special samples to verify the model ownership. These solutions are easy to be detected, and can potentially affect the performance and behaviors of the target model. Such limitations make existing solutions less applicable to safety- and security-critical tasks and scenarios, where DRL has been widely used.In this work, we propose a novel watermarking scheme for DRL protection. Instead of using spatial watermarks as in DL models, we introduce temporal watermarks, which can reduce the potential impact and damage to the target model, while achieving ownership verification with high fidelity. Specifically, (1) we design a new damage metric to select sequential states for watermark generation; (2) we introduce a new reward function to efficiently alter the model's behaviors for watermark embedding; (3) we propose to utilize a predefined probability density function of actions over the watermark states as the verification evidence. The integration of these techniques enables a DRL model owner to embed the watermarks for ownership verification and IP protection. Our method is general and can be applied to various DRL tasks with either deterministic or stochastic reinforcement learning algorithms. Extensive experimental results show that it can effectively preserve the functionality of DRL models and exhibit significant robustness against common model modifications, e.g., fine-tuning and model compression.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {314–322},
numpages = {9},
keywords = {deep reinforcement learning, intellectual property, watermarking},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@article{10.1145/3477050,
author = {Zhou, Yang and Ren, Jiaxiang and Jin, Ruoming and Zhang, Zijie and Zheng, Jingyi and Jiang, Zhe and Yan, Da and Dou, Dejing},
title = {Unsupervised Adversarial Network Alignment with Reinforcement Learning},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3477050},
doi = {10.1145/3477050},
abstract = {Network alignment, which aims at learning a matching between the same entities across multiple information networks, often suffers challenges from feature inconsistency, high-dimensional features, to unstable alignment results. This article presents a novel network alignment framework, Unsupervised Adversarial learning based Network Alignment(UANA), that combines generative adversarial network (GAN) and reinforcement learning (RL) techniques to tackle the above critical challenges. First, we propose a bidirectional adversarial network distribution matching model to perform the bidirectional cross-network alignment translations between two networks, such that the distributions of real and translated networks completely overlap together. In addition, two cross-network alignment translation cycles are constructed for training the unsupervised alignment without the need of prior alignment knowledge. Second, in order to address the feature inconsistency issue, we integrate a dual adversarial autoencoder module with an adversarial binary classification model together to project two copies of the same vertices with high-dimensional inconsistent features into the same low-dimensional embedding space. This facilitates the translations of the distributions of two networks in the adversarial network distribution matching model. Finally, we develop an RL based optimization approach to solve the vertex matching problem in the discrete space of the GAN model, i.e., directly select the vertices in target networks most relevant to the vertices in source networks, without unstable similarity computation that is sensitive to discriminative features and similarity metrics. Extensive evaluation on real-world graph datasets demonstrates the outstanding capability of UANA to address the unsupervised network alignment problem, in terms of both effectiveness and scalability.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {oct},
articleno = {50},
numpages = {29},
keywords = {adversarial classification, Unsupervised network alignment, reinforcement learning, adversarial network distribution matching, feature inconsistency, high-dimensional features}
}

@article{10.1145/3281032,
author = {Mu, Ting-Yu and Al-Fuqaha, Ala and Shuaib, Khaled and Sallabi, Farag M. and Qadir, Junaid},
title = {SDN Flow Entry Management Using Reinforcement Learning},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3281032},
doi = {10.1145/3281032},
abstract = {Modern information technology services largely depend on cloud infrastructures to provide their services. These cloud infrastructures are built on top of Datacenter Networks (DCNs) constructed with high-speed links, fast switching gear, and redundancy to offer better flexibility and resiliency. In this environment, network traffic includes long-lived (elephant) and short-lived (mice) flows with partitioned/aggregated traffic patterns. Although SDN-based approaches can efficiently allocate networking resources for such flows, the overhead due to network reconfiguration can be significant. With limited capacity of Ternary Content-Addressable Memory (TCAM) deployed in an OpenFlow enabled switch, it is crucial to determine which forwarding rules should remain in the flow table and which rules should be processed by the SDN controller in case of a table-miss on the SDN switch. This is needed in order to obtain the flow entries that satisfy the goal of reducing the long-term control plane overhead introduced between the controller and the switches. To achieve this goal, we propose a machine learning technique that utilizes two variations of Reinforcement Learning (RL) algorithms—the first of which is a traditional RL-based algorithm, while the other is deep reinforcement learning-based. Emulation results using the RL algorithm show around 60\% improvement in reducing the long-term control plane overhead and around 14\% improvement in the table-hit ratio compared to the Multiple Bloom Filters (MBF) method, given a fixed size flow table of 4KB.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {nov},
articleno = {11},
numpages = {23},
keywords = {machine learning, reinforcement learning, elephant and mice flows, ternary content addressable memory, MiniNet, Flow entry, big data, Openflow, software defined networking (SDN)}
}

@article{10.5555/1577069.1755867,
author = {Strehl, Alexander L. and Li, Lihong and Littman, Michael L.},
title = {Reinforcement Learning in Finite MDPs: PAC Analysis},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {We study the problem of learning near-optimal behavior in finite Markov Decision Processes (MDPs) with a polynomial number of samples. These "PAC-MDP" algorithms include the well-known E3 and R-MAX algorithms as well as the more recent Delayed Q-learning algorithm. We summarize the current state-of-the-art by presenting bounds for the problem in a unified theoretical framework. A more refined analysis for upper and lower bounds is presented to yield insight into the differences between the model-free Delayed Q-learning and the model-based R-MAX.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {2413–2444},
numpages = {32}
}

