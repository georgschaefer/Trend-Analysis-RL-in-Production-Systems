@inproceedings{10.5555/2772879.2773251,
author = {Efthymiadis, Kyriakos and Kudenko, Daniel},
title = {Knowledge Revision for Reinforcement Learning with Abstract MDPs},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Reward shaping is a method often used in RL so as to provide domain knowledge to agents and thus improve learning. An unrealistic assumption however is that the provided knowledge is always correct. This assumption can lead to poor performance in terms of total reward and convergence speed in case it is not met. Previous research demonstrated the use of plan-based reward shaping with knowledge revision in a single agent scenario where agents showed that they can quickly identify and revise erroneous knowledge and thus benefit from more accurate plans. This method however has no mechanism to deal with non-deterministic scenarios and is thus limited to deterministic domains. In this paper we present a method to provide heuristic knowledge via abstract MDPs, coupled with a revision algorithm to manage the cases where the provided domain knowledge is wrong. We show empirically that our method can efficiently revise erroneous knowledge even in the cases where the environment is non-deterministic and also removes the need for some of the assumptions present in plan-based reward shaping with knowledge revision.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {763–770},
numpages = {8},
keywords = {knowledge revision, reinforcement learning, reward shaping},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1145/3543873.3584651,
author = {Zhu, Menghui and Xia, Wei and Liu, Weiwen and Liu, Yifan and Tang, Ruiming and Zhang, Weinan},
title = {Integrated Ranking for News Feed with Reinforcement Learning},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3584651},
doi = {10.1145/3543873.3584651},
abstract = {With the development of recommender systems, it becomes an increasingly common need to mix multiple item sequences from different sources. Therefore, the integrated ranking stage is proposed to be responsible for this task with re-ranking models. However, existing methods ignore the relation between the sequences, thus resulting in local optimum over the interaction session. To resolve this challenge, in this paper, we propose a new model named NFIRank (News Feed Integrated Ranking with reinforcement learning) and formulate the whole interaction session as a MDP (Markov Decision Process). Sufficient offline experiments are provided to verify the effectiveness of our model. In addition, we deployed our model on Huawei Browser and gained 1.58\% improvements in CTR compared with the baseline in online A/B test. Code will be available at https://gitee.com/mindspore/models/tree/master/research/recommend/NFIRank.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {480–484},
numpages = {5},
keywords = {reinforcement learning, Recommender system, integrated ranking},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.5555/3545946.3598668,
author = {Sheng, Junjie and Wang, Xiangfeng and Jin, Bo and Li, Wenhao and Wang, Jun and Yan, Junchi and Chang, Tsung-Hui and Zha, Hongyuan},
title = {Learning Structured Communication for Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper investigates multi-agent reinforcement learning (MARL) communication mechanisms in large-scale scenarios. We propose a novel framework, Learning Structured Communication (LSC), that leverages a flexible and efficient communication topology. LSC enables adaptive agent grouping to create diverse hierarchical formations over episodes generated through an auxiliary task and a hierarchical routing protocol. We learn a hierarchical graph neural network with the formed topology that facilitates effective message generation and propagation between inter- and intra-group communications. Unlike state-of-the-art communication mechanisms, LSC possesses a detailed and learnable design for hierarchical communication. Numerical experiments on challenging tasks demonstrate that the proposed LSC exhibits high communication efficiency and global cooperation capability.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {436–438},
numpages = {3},
keywords = {learning to communicate, hierarchical structure, multi-agent reinforcement learning, graph neural networks},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3580305.3599831,
author = {Yu, Shuo and Xue, Hongyan and Ao, Xiang and Pan, Feiyang and He, Jia and Tu, Dandan and He, Qing},
title = {Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599831},
doi = {10.1145/3580305.3599831},
abstract = {In the field of quantitative trading, it is common practice to transform raw historical stock data into indicative signals for the market trend. Such signals are called alpha factors. Alphas in formula forms are more interpretable and thus favored by practitioners concerned with risk. In practice, a set of formulaic alphas is often used together for better modeling precision, so we need to find synergistic formulaic alpha sets that work well together. However, most traditional alpha generators mine alphas one by one separately, overlooking the fact that the alphas would be combined later. In this paper, we propose a new alpha-mining framework that prioritizes mining a synergistic set of alphas, i.e., it directly uses the performance of the downstream combination model to optimize the alpha generator. Our framework also leverages the strong exploratory capabilities of reinforcement learning (RL) to better explore the vast search space of formulaic alphas. The contribution to the combination models' performance is assigned to be the return used in the RL process. This return drives the alpha generator to find better alphas that improve upon the current set. Experimental evaluations on real-world stock market data demonstrate both the effectiveness and the efficiency of our framework for stock trend forecasting. The investment simulation results show that our framework is able to achieve higher returns compared to previous approaches.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5476–5486},
numpages = {11},
keywords = {stock trend forecasting, computational finance, reinforcement learning},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@article{10.1162/evco_a_00232,
author = {Kelly, Stephen and Heywood, Malcolm I.},
title = {Emergent Solutions to High-Dimensional Multitask Reinforcement Learning},
year = {2018},
issue_date = {Fall 2018},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {26},
number = {3},
issn = {1063-6560},
url = {https://doi.org/10.1162/evco_a_00232},
doi = {10.1162/evco_a_00232},
abstract = {Algorithms that learn through environmental interaction and delayed rewards, or reinforcement learning RL, increasingly face the challenge of scaling to dynamic, high-dimensional, and partially observable environments. Significant attention is being paid to frameworks from deep learning, which scale to high-dimensional data by decomposing the task through multilayered neural networks. While effective, the representation is complex and computationally demanding. In this work, we propose a framework based on genetic programming which adaptively complexifies policies through interaction with the task. We make a direct comparison with several deep reinforcement learning frameworks in the challenging Atari video game environment as well as more traditional reinforcement learning frameworks based on a priori engineered features. Results indicate that the proposed approach matches the quality of deep learning while being a minimum of three orders of magnitude simpler with respect to model complexity. This results in real-time operation of the champion RL agent without recourse to specialized hardware support. Moreover, the approach is capable of evolving solutions to multiple game titles simultaneously with no additional computational cost. In this case, agent behaviours for an individual game as well as single agents capable of playing all games emerge from the same evolutionary run.},
journal = {Evol. Comput.},
month = {sep},
pages = {347–380},
numpages = {34},
keywords = {cooperative coevolution, reinforcement learning, multitask learning., genetic programming, Emergent modularity}
}

@article{10.1613/jair.1.11263,
author = {Narasimhan, Karthik and Barzilay, Regina and Jaakkola, Tommi},
title = {Grounding Language for Transfer in Deep Reinforcement Learning},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11263},
doi = {10.1613/jair.1.11263},
abstract = {In this paper, we explore the utilization of natural language to drive transfer for reinforcement learning (RL). Despite the wide-spread application of deep RL techniques, learning generalized policy representations that work across domains remains a challenging problem. We demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. Specifically, by learning to ground the meaning of text to the dynamics of the environment such as transitions and rewards, an autonomous agent can effectively bootstrap policy learning on a new domain given its description. We employ a model-based RL approach consisting of a differentiable planning module, a model-free component and a factorized state representation to effectively use entity descriptions. Our model outperforms prior work on both transfer and multi-task scenarios in a variety of different environments. For instance, we achieve up to 14\% and 11.5\% absolute improvement over previously existing models in terms of average and initial rewards, respectively.},
journal = {J. Artif. Int. Res.},
month = {sep},
pages = {849–874},
numpages = {26}
}

@inproceedings{10.1145/3520304.3533980,
author = {Marchesini, Enrico and Amato, Christopher},
title = {Safety-Informed Mutations for Evolutionary Deep Reinforcement Learning},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3533980},
doi = {10.1145/3520304.3533980},
abstract = {Evolutionary Algorithms have been combined with Deep Reinforcement Learning (DRL) to address the limitations of the two approaches while leveraging their benefits. In this paper, we discuss objective-informed mutations to bias the evolutionary population toward exploring the desired objective. We focus on Safe DRL domains to show how these mutations exploit visited unsafe states to search for safer actions. Empirical evidence on a 12 degrees of freedom locomotion benchmark and a practical navigation task, confirm that we improve the safety of the policy while maintaining comparable return with the original DRL algorithm.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1966–1970},
numpages = {5},
keywords = {reinforcement learning, mutations, deep, robotics, evolutionary algorithms},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@article{10.1613/jair.1.14386,
author = {Lyu, Xueguang and Baisero, Andrea and Xiao, Yuchen and Daley, Brett and Amato, Christopher},
title = {On Centralized Critics in Multi-Agent Reinforcement Learning},
year = {2023},
issue_date = {Jun 2023},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {77},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.14386},
doi = {10.1613/jair.1.14386},
abstract = {Centralized Training for Decentralized Execution, where agents are trained offline in a centralized fashion and execute online in a decentralized manner, has become a popular approach in Multi-Agent Reinforcement Learning (MARL). In particular, it has become popular to develop actor-critic methods that train decentralized actors with a centralized critic where the centralized critic is allowed access global information of the entire system, including the true system state. Such centralized critics are possible given offline information and are not used for online execution. While these methods perform well in a number of domains and have become a de facto standard in MARL, using a centralized critic in this context has yet to be sufficiently analyzed theoretically or empirically. In this paper, we therefore formally analyze centralized and decentralized critic approaches, and analyze the effect of using state-based critics in partially observable environments. We derive theories contrary to the common intuition: critic centralization is not strictly beneficial, and using state values can be harmful. We further prove that, in particular, state-based critics can introduce unexpected bias and variance compared to history-based critics. Finally, we demonstrate how the theory applies in practice by comparing different forms of critics on a wide range of common multi-agent benchmarks. The experiments show practical issues such as the difficulty of representation learning with partial observability, which highlights why the theoretical problems are often overlooked in the literature.},
journal = {J. Artif. Int. Res.},
month = {jun},
numpages = {60}
}

@inproceedings{10.1145/2330163.2330178,
author = {Doucette, John A. and Lichodzijewski, Peter and Heywood, Malcolm I.},
title = {Hierarchical Task Decomposition through Symbiosis in Reinforcement Learning},
year = {2012},
isbn = {9781450311779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330163.2330178},
doi = {10.1145/2330163.2330178},
abstract = {Adopting a symbiotic model of evolution separates context for deploying an action from the action itself. Such a separation provides a mechanism for task decomposition in temporal sequence learning. Moreover, previously learned policies are taken to be synonymous with meta actions (actions that are themselves policies). Should solutions to the task not be forthcoming in an initial round of evolution, then solutions from the earlier round represent the 'meta' actions for a new round of evolution. This provides the basis for evolving policy trees. A benchmarking study is performed using the Acrobot handstand task. Solutions to date from reinforcement learning have not been able to approach the performance of those established 14 years ago using an A* search and a priori knowledge regarding the Acrobot energy equations. The proposed symbiotic approach is able to match and, for the first time, better these results. Moreover, unlike previous work, solutions are tested under a broad range of Acrobot initial conditions, with hierarchical solutions providing significantly better generalization performance.},
booktitle = {Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation},
pages = {97–104},
numpages = {8},
keywords = {symbiosis, reinforcement learning, task decomposition, genetic programming, meta actions},
location = {Philadelphia, Pennsylvania, USA},
series = {GECCO '12}
}

@inproceedings{10.5555/3545946.3598870,
author = {Fan, Ziming and Peng, Nianli and Tian, Muhang and Fain, Brandon},
title = {Welfare and Fairness in Multi-Objective Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We study fair multi-objective reinforcement learning in which an agent must learn a policy that simultaneously achieves high reward on multiple dimensions of a vector-valued reward. Motivated by the fair resource allocation literature, we model this as an expected welfare maximization problem, for some non-linear fair welfare function of the vector of long-term cumulative rewards. One canonical example of such a function is the Nash Social Welfare, or geometric mean, the log transform of which is also known as the Proportional Fairness objective. We show that even approximately optimal optimization of the expected Nash Social Welfare is computationally intractable even in the tabular case. Nevertheless, we provide a novel adaptation of Q-learning that combines non-linear scalarized learning updates and non-stationary action selection to learn effective policies for optimizing nonlinear welfare functions. We show that our algorithm is provably convergent, and we demonstrate experimentally that our approach outperforms techniques based on linear scalarization, mixtures of optimal linear scalarizations, or stationary action selection for the Nash Social Welfare Objective.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1991–1999},
numpages = {9},
keywords = {algorithmic fairness, multi-objective reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3528535.3565249,
author = {Pan, Lichen and Qian, Jun and Xia, Wei and Mao, Hangyu and Yao, Jun and Li, Pengze and Xiao, Zhen},
title = {Optimizing Communication in Deep Reinforcement Learning with XingTian},
year = {2022},
isbn = {9781450393409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528535.3565249},
doi = {10.1145/3528535.3565249},
abstract = {Deep Reinforcement Learning (DRL) achieves great success in various domains. Communication in today's DRL algorithms takes non-negligible time compared to the computation. However, prior DRL frameworks usually focus on computation management while paying little attention to communication optimization, and fail to utilize the opportunity of the communication-computation overlap that hides the communication from the critical path of DRL algorithms. Consequently, communication can take more time than the computation in prior DRL frameworks. In this paper, we present XingTian, a novel DRL framework that co-designs the management of communication and computation in DRL algorithms. XingTian organizes the computation in DRL algorithms in a decentralized way and provides an asynchronous communication channel. XingTian makes the communication execute asynchronously and aggressively and takes advantage of the communication-computation overlapping opportunity from DRL algorithms. Experimental results show that XingTian improves data transmission efficiency and can transmit at least twice as much data per second as the state-of-the-art DRL framework RLLib. DRL algorithms based on XingTian achieve up to 70.71\% more throughput than RLLib-based ones with better or similar convergent performance. XingTian maintains high communication efficiency under different scale deployments and the XingTian-based DRL algorithm achieves 91.12\% higher throughput than the RLLib-based one when deployed in four machines. XingTian is open-sourced and publicly available at https://github.com/huawei-noah/xingtian.},
booktitle = {Proceedings of the 23rd ACM/IFIP International Middleware Conference},
pages = {255–268},
numpages = {14},
keywords = {decentralized computation, communication-computation overlap, asynchronous communication, deep reinforcement learning},
location = {Quebec, QC, Canada},
series = {Middleware '22}
}

@article{10.5555/3291125.3291134,
author = {De Bruin, Tim and Kober, Jens and Tuyls, Karl and Babu\v{s}ka, Robert},
title = {Experience Selection in Deep Reinforcement Learning for Control},
year = {2018},
issue_date = {January 2018},
publisher = {JMLR.org},
volume = {19},
number = {1},
issn = {1532-4435},
abstract = {Experience replay is a technique that allows off-policy reinforcement-learning methods to reuse past experiences. The stability and speed of convergence of reinforcement learning, as well as the eventual performance of the learned policy, are strongly dependent on the experiences being replayed. Which experiences are replayed depends on two important choices. The first is which and how many experiences to retain in the experience replay buffer. The second choice is how to sample the experiences that are to be replayed from that buffer. We propose new methods for the combined problem of experience retention and experience sampling. We refer to the combination as experience selection. We focus our investigation specifically on the control of physical systems, such as robots, where exploration is costly. To determine which experiences to keep and which to replay, we investigate different proxies for their immediate and long-term utility. These proxies include age, temporal difference error and the strength of the applied exploration noise. Since no currently available method works in all situations, we propose guidelines for using prior knowledge about the characteristics of the control problem at hand to choose the appropriate experience replay strategy.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {347–402},
numpages = {56},
keywords = {experience replay, robotics, deep learning, reinforcement learning, control}
}

@inproceedings{10.1145/3584871.3584874,
author = {Du, Kai and Lu, Guoming and Qin, Ke},
title = {An Extractive&nbsp;Text&nbsp;Summarization Based on Reinforcement Learning},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584871.3584874},
doi = {10.1145/3584871.3584874},
abstract = {Abstract: In recent years, with the rapid development of network information technology, network text information also presents an explosive growth trend. As an efficient information processing technology in the digital age, text summarization can bring the advantage of focusing on key information in all directions in massive text information. However, text summarization is still faced with some problems such as difficulty in extracting long text and information redundancy. Therefore, combining with the deep learning framework, this paper proposes an extractive text summarization that uses reinforcement learning to optimize the long text extraction process and uses the attention mechanism to achieve the effect of redundancy removal. On CNN/Daily Mail datasets, the automatic evaluation shows that our model outperforms the previous on ROUGE, and the ablation experiment proves the effectiveness of the de-redundant attention module.},
booktitle = {Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
pages = {19–25},
numpages = {7},
keywords = {Reinforcement learning, Deep learning, Text summarization},
location = {Palmerston North, New Zealand},
series = {ICSIM '23}
}

@inproceedings{10.1145/3503161.3548277,
author = {Ma, Zhixin and Ngo, Chong Wah},
title = {Interactive Video Corpus Moment Retrieval Using Reinforcement Learning},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548277},
doi = {10.1145/3503161.3548277},
abstract = {Known-item video search is effective with human-in-the-loop to interactively investigate the search result and refine the initial query. Nevertheless, when the first few pages of results are swamped with visually similar items, or the search target is hidden deep in the ranked list, finding the know-item target usually requires a long duration of browsing and result inspection. This paper tackles the problem by reinforcement learning, aiming to reach a search target within a few rounds of interaction by long-term learning from user feedbacks. Specifically, the system interactively plans for navigation path based on feedback and recommends a potential target that maximizes the long-term reward for user comment. We conduct experiments for the challenging task of video corpus moment retrieval (VCMR) to localize moments from a large video corpus. The experimental results on TVR and DiDeMo datasets verify that our proposed work is effective in retrieving the moments that are hidden deep inside the ranked lists of CONQUER and HERO, which are the state-of-the-art auto-search engines for VCMR.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {296–306},
numpages = {11},
keywords = {interactive search, reinforcement learning, user simulation, video corpus moment retrieval},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3459637.3482347,
author = {Chen, Xiaocong and Yao, Lina and Sun, Aixin and Wang, Xianzhi and Xu, Xiwei and Zhu, Liming},
title = {Generative Inverse Deep Reinforcement Learning for Online Recommendation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482347},
doi = {10.1145/3459637.3482347},
abstract = {Deep reinforcement learning enables an agent to capture users' interest through dynamic interactions with the environment. It uses a reward function to learn user's interest and to control the learning process, attracting great interest in recommendation research. However, most reward functions are manually designed; they are either too unrealistic or imprecise to reflect the variety, dimensionality, and non-linearity of the recommendation problem. This impedes the agent from learning an optimal policy in highly dynamic online recommendation scenarios. To address the above issue, we propose a generative inverse reinforcement learning approach that avoids the need of defining an elaborative reward function. In particular, we model the recommendation problem as an automatic policy learning problem. We first generate policies based on observed users' preferences and then evaluate the learned policy by a measurement based on a discriminative actor-critic network. We conduct experiments on an online platform, VirtualTB, and demonstrate the feasibility and effectiveness of our proposed approach via comparisons with several state-of-the-art methods.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {201–210},
numpages = {10},
keywords = {deep learning, deep reinforcement learning, recommender system},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3472538.3472583,
author = {Versaw, Rachael and Schultz, Samantha and Lu, Kevin and Zhao, Richard},
title = {Modular Reinforcement Learning Framework for Learners and Educators},
year = {2021},
isbn = {9781450384223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472538.3472583},
doi = {10.1145/3472538.3472583},
abstract = {Reinforcement learning algorithms have been applied to many research areas in gaming and non-gaming applications. However, in gaming artificial intelligence (AI), existing reinforcement learning tools are aimed at experienced developers and not readily accessible to learners. The unpredictable nature of online learning is also a barrier for casual users. This paper proposes the EasyGameRL framework, a novel approach to the education of reinforcement learning in games using modular visual design patterns. The EasyGameRL framework and its software implementation in Unreal Engine are modular, reusable, and applicable to multiple game scenarios. The pattern-based approach allows users to effectively utilize reinforcement learning in their games and visualize the components of the process. This would be helpful to AI learners, educators, designers and casual users alike.},
booktitle = {Proceedings of the 16th International Conference on the Foundations of Digital Games},
articleno = {42},
numpages = {5},
keywords = {computer game, reinforcement learning, design pattern, visual design, education},
location = {Montreal, QC, Canada},
series = {FDG '21}
}

@inproceedings{10.1145/3448891.3448919,
author = {Lee, Isabella and Babu, Vignesh and Caesar, Matthew and Nicol, David},
title = {Deep Reinforcement Learning for UAV-Assisted Emergency Response},
year = {2021},
isbn = {9781450388405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448891.3448919},
doi = {10.1145/3448891.3448919},
abstract = {In the aftermath of a disaster, the ability to reliably communicate and coordinate emergency response could make a meaningful difference in the number of lives saved or lost. However, post-disaster areas tend to have limited functioning communication network infrastructure while emergency response teams are carrying increasingly more devices, such as sensors and video transmitting equipment, which can be low-powered with limited transmission ranges. In such scenarios, unmanned aerial vehicles (UAVs) can be used as relays to connect these devices with each other. Since first responders are likely to be constantly mobile, the problem of where these UAVs are placed and how they move in response to the changing environment could have a large effect on the number of connections this UAV relay network is able to maintain. In this work, we propose DroneDR, a reinforcement learning framework for UAV positioning that uses information about connectivity requirements and user node positions to decide how to move each UAV in the network while maintaining connectivity between UAVs. The proposed approach is shown to outperform other greedy heuristics across a broad range of scenarios and demonstrates the potential in using reinforcement learning techniques to aid communication during disaster relief operations.},
booktitle = {MobiQuitous 2020 - 17th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {327–336},
numpages = {10},
keywords = {reinforcement learning, IoT network, disaster relief, UAV network},
location = {Darmstadt, Germany},
series = {MobiQuitous '20}
}

@inproceedings{10.5555/3466184.3466305,
author = {van Heeswijk, Wouter and La Poutr\'{e}, Han},
title = {Deep Reinforcement Learning in Linear Discrete Action Spaces},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Problems in operations research are typically combinatorial and high-dimensional. To a degree, linear programs may efficiently solve such large decision problems. For stochastic multi-period problems, decomposition into a sequence of one-stage decisions with approximated downstream effects is often necessary, e.g., by deploying reinforcement learning to obtain value function approximations (VFAs). When embedding such VFAs into one-stage linear programs, VFA design is restricted by linearity. This paper presents an integrated simulation approach for such complex optimization problems, developing a deep reinforcement learning algorithm that combines linear programming and neural network VFAs. Our proposed method embeds neural network VFAs into one-stage linear decision problems, combining the nonlinear expressive power of neural networks with the efficiency of solving linear programs. As a proof of concept, we perform numerical experiments on a transportation problem. The neural network VFAs consistently outperform polynomial VFAs as well as other benchmarks, with limited design and tuning effort.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1063–1074},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.5555/3545946.3599151,
author = {R\"{o}pke, Willem},
title = {Reinforcement Learning in Multi-Objective Multi-Agent Systems},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {For effective decision-making in the real world, artificial agents need to take both the multi-agent as well as multi-objective nature of their environments into account. These environments are formalised as multi-objective games and introduce numerous challenges compared to their single-objective counterpart. For my main contributions so far, I have established a theoretical guarantee that a bidirectional link always exists that maps a finite multi-objective game to an equivalent single-objective game with an infinite number of actions. Additionally, I presented an extensive study of Nash equilibria in multi-objective games, culminating in existence guarantees under certain assumptions. From a reinforcement learning perspective, I explored how communication and commitment can help agents to learn adequate policies in these challenging environments. In this paper, I summarise my ongoing research and discuss several promising directions for future work.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2999–3001},
numpages = {3},
keywords = {reinforcement learning, game theory, multi-objective},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3463952.3464026,
author = {He, Keyang and Banerjee, Bikramjit and Doshi, Prashant},
title = {Cooperative-Competitive Reinforcement Learning with History-Dependent Rewards},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Consider a typical organization whose worker agents seek to collectively cooperate for its general betterment. However, each individual agent simultaneously seeks to act to secure a larger chunk than its co-workers of the annual increment in compensation, which usually comes from a fixed pot. As such, the agents in an organization must cooperate and compete. Another feature of many organizations is that a worker receives a bonus, which is often a fraction of previous year's total profit. As such, the agent derives a reward that is also partly dependent on historical performance. How should the individual agent decide to act in this context? Few methods for the mixed cooperative-competitive setting have been presented in recent years, but these are challenged by problem domains whose reward functions additionally depend on historical information. Recent deep multi-agent reinforcement learning (MARL) methods using long short-term memory (LSTM) may be used, but these adopt a joint perspective to the interaction or require explicit exchange of information among the agents to promote cooperation, which may not be possible under competition. In this paper, we first show that the agent's decision-making problem can be modeled as an interactive partially observable Markov decision process (I-POMDP) that captures the dynamic of a history-dependent reward. We present an interactive advantage actor-critic method (IA2C+), which combines the independent advantage actor-critic network with a belief filter that maintains a belief distribution over other agents' models. Empirical results show that IA2C+ learns the optimal policy faster and more robustly than several baselines.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {602–610},
numpages = {9},
keywords = {organization, mixed setting, actor-critic, reinforcement learning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.5555/1838206.1838494,
author = {Wilson, Aaron and Fern, Alan and Tadepalli, Prasad},
title = {Bayesian Role Discovery for Multi-Agent Reinforcement Learning},
year = {2010},
isbn = {9780982657119},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper we develop a Bayesian policy search approach for Multi-Agent RL (MARL), which is model-free and allows for priors on policy parameters. We present a novel optimization algorithm based on hybrid MCMC, which leverages both the prior and gradient information estimated from trajectories. Our experiments demonstrate the automatic discovery of roles through reinforcement learning in a real-time strategy game.},
booktitle = {Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1 - Volume 1},
pages = {1587–1588},
numpages = {2},
keywords = {reinforcement learning},
location = {Toronto, Canada},
series = {AAMAS '10}
}

@inproceedings{10.1145/3582437.3587190,
author = {Chetitah, Mounsif and M\"{u}ller, Julian and Deserno, Lorenz and Waltmann, Maria and von Mammen, Sebastian},
title = {Gamification Framework for Reinforcement Learning-Based Neuropsychology Experiments},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3587190},
doi = {10.1145/3582437.3587190},
abstract = {Reinforcement learning (RL) is an adaptive process where an agent relies on its experience to improve the outcome of its performance. It learns by taking actions to maximize its rewards, and by minimizing the gap between predicted and received rewards. In experimental neuropsychology, RL algorithms are used as a conceptual basis to account for several aspects of human motivation and cognition. A number of neuropsychological experiments, such as reversal learning, sequential decision-making, and go-no-go tasks, are required to validate the decisive RL algorithms. The experiments are conducted in digital environments and are comprised of numerous trials that lead to participants’ frustration and fatigue. This paper presents a gamification framework for reinforcement-based neuropsychology experiments that aims to increase participant engagement and provide them with appropriate testing environments.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {51},
numpages = {4},
keywords = {reinforcement learning, gamification, neuropsychology, serious games},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3580305.3599901,
author = {Zheng, Yu and Su, Hongyuan and Ding, Jingtao and Jin, Depeng and Li, Yong},
title = {Road Planning for Slums via Deep Reinforcement Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599901},
doi = {10.1145/3580305.3599901},
abstract = {Millions of slum dwellers suffer from poor accessibility to urban services due to inadequate road infrastructure within slums, and road planning for slums is critical to the sustainable development of cities. Existing re-blocking or heuristic methods are either time-consuming which cannot generalize to different slums, or yield sub-optimal road plans in terms of accessibility and construction costs. In this paper, we present a deep reinforcement learning based approach to automatically layout roads for slums. We propose a generic graph model to capture the topological structure of a slum, and devise a novel graph neural network to select locations for the planned roads. Through masked policy optimization, our model can generate road plans that connect places in a slum at minimal construction costs. Extensive experiments on real-world slums in different countries verify the effectiveness of our model, which can significantly improve accessibility by 14.3\% against existing baseline methods. Further investigations on transferring across different tasks demonstrate that our model can master road planning skills in simple scenarios and adapt them to much more complicated ones, indicating the potential of applying our model in real-world slum upgrading. The code and data are available at https://github.com/tsinghua-fib-lab/road-planning-for-slums.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5695–5706},
numpages = {12},
keywords = {road planning, reinforcement learning, slum upgrading},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3411764.3445497,
author = {Todi, Kashyap and Bailly, Gilles and Leiva, Luis and Oulasvirta, Antti},
title = {Adapting User Interfaces with Model-Based Reinforcement Learning},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445497},
doi = {10.1145/3411764.3445497},
abstract = {Adapting an interface requires taking into account both the positive and negative effects that changes may have on the user. A carelessly picked adaptation may impose high costs to the user – for example, due to surprise or relearning effort – or “trap” the process to a suboptimal design immaturely. However, effects on users are hard to predict as they depend on factors that are latent and evolve over the course of interaction. We propose a novel approach for adaptive user interfaces that yields a conservative adaptation policy: It finds beneficial changes when there are such and avoids changes when there are none. Our model-based reinforcement learning method plans sequences of adaptations and consults predictive HCI models to estimate their effects. We present empirical and simulation results from the case of adaptive menus, showing that the method outperforms both a non-adaptive and a frequency-based policy.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {573},
numpages = {13},
keywords = {Reinforcement Learning, Monte Carlo Tree Search, Predictive Models, Adaptive User Interfaces},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.5555/3306127.3331915,
author = {Anastassacos, Nicolas and Musolesi, Mirco},
title = {Towards Decentralized Reinforcement Learning Architectures for Social Dilemmas},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent reinforcement learning has received significant interest in recent years notably due to the advancements made in deep reinforcement learning which have allowed for the developments of new architectures and learning algorithms. In this extended abstract we present our initial efforts towards the development of decentralized architectures for multi-agent systems in order to understand and model societies. More specifically, using social dilemmas as the training ground, we present a novel learning architecture, Learning through Probing (LTP), where agents utilize a probing mechanism to incorporate how their opponent's behavior changes when an agent takes an action.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1776–1777},
numpages = {2},
keywords = {multi-agent systems, reinforcement learning, cooperation},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.5555/3545946.3598615,
author = {Jiang, Jiechuan and Lu, Zongqing},
title = {Adaptive Learning Rates for Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In multi-agent reinforcement learning (MARL), the learning rates of actors and critic are mostly hand-tuned and fixed. This not only requires heavy tuning but more importantly limits the learning. With adaptive learning rates according to gradient patterns, some optimizers have been proposed for general optimizations, which however do not take into consideration the characteristics of MARL. In this paper, we propose AdaMa to bring adaptive learning rates to cooperative MARL. AdaMa evaluates the contribution of actors' updates to the improvement of Q-value and adaptively updates the learning rates of actors to the direction of maximally improving the Q-value. AdaMa could also dynamically balance the learning rates between the critic and actors according to their varying effects on the learning. Moreover, AdaMa can incorporate the second-order approximation to capture the contribution of pairwise actors' updates and thus more accurately updates the learning rates of actors. Empirically, we show that AdaMa could accelerate learning and improve performance in a variety of multi-agent scenarios. More importantly, AdaMa does not require heavy hyperparameter tuning and thus significantly reduces the training cost. The visualizations of learning rates during training clearly explain how and why AdaMa works.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {23–30},
numpages = {8},
keywords = {multi-agent reinforcement learning, adaptive learning rates, reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3211954.3211958,
author = {Milo, Tova and Somech, Amit},
title = {Deep Reinforcement-Learning Framework for Exploratory Data Analysis},
year = {2018},
isbn = {9781450358514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211954.3211958},
doi = {10.1145/3211954.3211958},
abstract = {Deep Reinforcement Learning (DRL) is unanimously considered as a breakthrough technology, used in solving a growing number of AI challenges previously considered to be intractable. In this work, we aim to set the ground for employing DRL techniques in the context of Exploratory Data Analysis (EDA), an important yet challenging, that is critical in many application domains. We suggest an end-to-end framework architecture, coupled with an initial implementation of each component. The goal of this short paper is to encourage the exploration of DRL models and techniques for facilitating a full-fledged, autonomous solution for EDA.},
booktitle = {Proceedings of the First International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
articleno = {4},
numpages = {4},
keywords = {Exploratory Data Analysis, Deep Reinforcement Learning},
location = {Houston, TX, USA},
series = {aiDM'18}
}

@article{10.1145/2735392.2735396,
author = {Faust, Aleksandra},
title = {Reinforcement Learning and Planning for Preference Balancing Tasks},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/2735392.2735396},
doi = {10.1145/2735392.2735396},
abstract = {Many robotic motion tasks, such as UAV control, have non-linear and high-dimensional dynamics. Difficult for both human demonstration and explicit solutions, these tasks can be described with opposing preferences. This thesis develops PEARL, a real-time solution for such tasks on acceleration-controlled systems with unknown dynamics, and finds PEARL's safety conditions.},
journal = {AI Matters},
month = {mar},
pages = {8–12},
numpages = {5}
}

@inproceedings{10.1145/3397271.3401148,
author = {Xu, Jun and Wei, Zeng and Xia, Long and Lan, Yanyan and Yin, Dawei and Cheng, Xueqi and Wen, Ji-Rong},
title = {Reinforcement Learning to Rank with Pairwise Policy Gradient},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401148},
doi = {10.1145/3397271.3401148},
abstract = {This paper concerns reinforcement learning~(RL) of the document ranking models for information retrieval~(IR). One branch of the RL approaches to ranking formalize the process of ranking with Markov decision process~(MDP) and determine the model parameters with policy gradient. Though preliminary success has been shown, these approaches are still far from achieving their full potentials. Existing policy gradient methods directly utilize the absolute performance scores (returns) of the sampled document lists in its gradient estimations, which may cause two limitations: 1) fail to reflect the relative goodness of documents within the same query, which usually is close to the nature of IR ranking; 2) generate high variance gradient estimations, resulting in slow learning speed and low ranking accuracy. To deal with the issues, we propose a novel policy gradient algorithm in which the gradients are determined using pairwise comparisons of two document lists sampled within the same query. The algorithm, referred to as Pairwise Policy Gradient (PPG), repeatedly samples pairs of document lists, estimates the gradients with pairwise comparisons, and finally updates the model parameters. Theoretical analysis shows that PPG makes an unbiased and low variance gradient estimations. Experimental results have demonstrated performance gains over the state-of-the-art baselines in search result diversification and text retrieval.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {509–518},
numpages = {10},
keywords = {learning to rank, policy gradient, reinforcement learning},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.1145/2897824.2925881,
author = {Peng, Xue Bin and Berseth, Glen and van de Panne, Michiel},
title = {Terrain-Adaptive Locomotion Skills Using Deep Reinforcement Learning},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925881},
doi = {10.1145/2897824.2925881},
abstract = {Reinforcement learning offers a promising methodology for developing skills for simulated characters, but typically requires working with sparse hand-crafted features. Building on recent progress in deep reinforcement learning (DeepRL), we introduce a mixture of actor-critic experts (MACE) approach that learns terrain-adaptive dynamic locomotion skills using high-dimensional state and terrain descriptions as input, and parameterized leaps or steps as output actions. MACE learns more quickly than a single actor-critic approach and results in actor-critic experts that exhibit specialization. Additional elements of our solution that contribute towards efficient learning include Boltzmann exploration and the use of initial actor biases to encourage specialization. Results are demonstrated for multiple planar characters and terrain classes.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {81},
numpages = {12},
keywords = {reinforcement learning, physics-based characters}
}

@inproceedings{10.1145/1390156.1390240,
author = {Melo, Francisco S. and Meyn, Sean P. and Ribeiro, M. Isabel},
title = {An Analysis of Reinforcement Learning with Function Approximation},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390240},
doi = {10.1145/1390156.1390240},
abstract = {We address the problem of computing the optimal Q-function in Markov decision problems with infinite state-space. We analyze the convergence properties of several variations of Q-learning when combined with function approximation, extending the analysis of TD-learning in (Tsitsiklis \&amp; Van Roy, 1996a) to stochastic control settings. We identify conditions under which such approximate methods converge with probability 1. We conclude with a brief discussion on the general applicability of our results and compare them with several related works.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {664–671},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/3580305.3599393,
author = {Xu, Jiacheng and Chen, Chao and Zhang, Fuxiang and Yuan, Lei and Zhang, Zongzhang and Yu, Yang},
title = {Internal Logical Induction for Pixel-Symbolic Reinforcement Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599393},
doi = {10.1145/3580305.3599393},
abstract = {Reinforcement Learning (RL) has experienced rapid advancements in recent years. The widely studied RL algorithms mainly focus on a single input form, such as pixel-based image input or symbolic vector input. These two forms have different characteristics and, in many scenarios, will appear together, while few RL algorithms have studied the problems with mixed input types. Specifically, in the scenario where both pixel and symbolic inputs are available, symbolic input usually offers abstract features with specific semantics, which is more conducive to the agent's focus. Conversely, pixel input provides more comprehensive information, enabling the agent to make well-informed decisions. Tailoring the processing approach based on the properties of these two input types can contribute to solving the problem more effectively. To tackle the above issue, we propose an Internal Logical Induction (ILI) framework that integrates deep RL and rule learning into one system. ILI utilizes the deep RL algorithm to process the pixel input and the rule learning algorithm to induce propositional logic knowledge from symbolic input. To efficiently combine these two mechanisms, we further adopt a reward shaping technique by treating valuable knowledge as intrinsic rewards for the RL procedure. Experimental results demonstrate that the ILI framework outperforms baseline approaches in RL problems with pixel-symbolic input, and its inductive knowledge exhibits transferability advantages when pixel input semantics change.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2825–2837},
numpages = {13},
keywords = {rule learning, reinforcement learning},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/1943403.1943452,
author = {Chali, Yllias and Hasan, Sadid A. and Imam, Kaisar},
title = {A Reinforcement Learning Framework for Answering Complex Questions},
year = {2011},
isbn = {9781450304191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1943403.1943452},
doi = {10.1145/1943403.1943452},
abstract = {Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we use extractive multi-document summarization techniques to perform complex question answering and formulate it as a reinforcement learning problem. We use a reward function that measures the relatedness of the candidate (machine generated) summary sentences with abstract summaries. In the training stage, the learner iteratively selects original document sentences to be included in the candidate summary, analyzes the reward function and updates the related feature weights accordingly. The final weights found in this phase are used to generate summaries as answers to complex questions given unseen test data. We use a modified linear, gradient-descent version of Watkins' Q(») algorithm with µ-greedy policy to determine the best possible action i.e. selecting the most important sentences. We compare the performance of this system with a Support Vector Machine (SVM) based system. Evaluation results show that the reinforcement method advances the SVM system improving the ROUGE scores by &lt; 28\%.},
booktitle = {Proceedings of the 16th International Conference on Intelligent User Interfaces},
pages = {307–310},
numpages = {4},
keywords = {complex question answering, reinforcement learning, reward function, multi-document summarization, support vector machines},
location = {Palo Alto, CA, USA},
series = {IUI '11}
}

@article{10.5555/1953048.2021066,
author = {van Seijen, Harm and Whiteson, Shimon and van Hasselt, Hado and Wiering, Marco},
title = {Exploiting Best-Match Equations for Efficient Reinforcement Learning},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {This article presents and evaluates best-match learning, a new approach to reinforcement learning that trades off the sample efficiency of model-based methods with the space efficiency of model-free methods. Best-match learning works by approximating the solution to a set of best-match equations, which combine a sparse model with a model-free Q-value function constructed from samples not used by the model. We prove that, unlike regular sparse model-based methods, best-match learning is guaranteed to converge to the optimal Q-values in the tabular case. Empirical results demonstrate that best-match learning can substantially outperform regular sparse model-based methods, as well as several model-free methods that strive to improve the sample efficiency of temporal-difference methods. In addition, we demonstrate that best-match learning can be successfully combined with function approximation.},
journal = {J. Mach. Learn. Res.},
month = {jul},
pages = {2045–2094},
numpages = {50}
}

@inproceedings{10.5555/2343576.2343636,
author = {Teacy, W. T. L. and Chalkiadakis, G. and Farinelli, A. and Rogers, A. and Jennings, N. R. and McClean, S. and Parr, G.},
title = {Decentralized Bayesian Reinforcement Learning for Online Agent Collaboration},
year = {2012},
isbn = {0981738117},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Solving complex but structured problems in a decentralized manner via multiagent collaboration has received much attention in recent years. This is natural, as on one hand, multiagent systems usually possess a structure that determines the allowable interactions among the agents; and on the other hand, the single most pressing need in a cooperative multiagent system is to coordinate the local policies of autonomous agents with restricted capabilities to serve a system-wide goal. The presence of uncertainty makes this even more challenging, as the agents face the additional need to learn the unknown environment parameters while forming (and following) local policies in an online fashion. In this paper, we provide the first Bayesian reinforcement learning (BRL) approach for distributed coordination and learning in a cooperative multiagent system by devising two solutions to this type of problem. More specifically, we show how the Value of Perfect Information (VPI) can be used to perform efficient decentralised exploration in both model-based and model-free BRL, and in the latter case, provide a closed form solution for VPI, correcting a decade old result by Dearden, Friedman and Russell. To evaluate these solutions, we present experimental results comparing their relative merits, and demonstrate empirically that both solutions outperform an existing multiagent learning method, representative of the state-of-the-art.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {417–424},
numpages = {8},
keywords = {Bayesian techniques, uncertainty, multiagent learning},
location = {Valencia, Spain},
series = {AAMAS '12}
}

@inproceedings{10.1145/3447548.3467420,
author = {Li, Jiahui and Kuang, Kun and Wang, Baoxiang and Liu, Furui and Chen, Long and Wu, Fei and Xiao, Jun},
title = {Shapley Counterfactual Credits for Multi-Agent Reinforcement Learning},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467420},
doi = {10.1145/3447548.3467420},
abstract = {Centralized Training with Decentralized Execution (CTDE) has been a popular paradigm in cooperative Multi-Agent Reinforcement Learning (MARL) settings and is widely used in many real applications. One of the major challenges in the training process is credit assignment, which aims to deduce the contributions of each agent according to the global rewards. Existing credit assignment methods focus on either decomposing the joint value function into individual value functions or measuring the impact of local observations and actions on the global value function. These approaches lack a thorough consideration of the complicated interactions among multiple agents, leading to an unsuitable assignment of credit and subsequently mediocre results on MARL. We propose Shapley Counterfactual Credit Assignment, a novel method for explicit credit assignment which accounts for the coalition of agents. Specifically, Shapley Value and its desired properties are leveraged in deep MARL to credit any combinations of agents, which grants us the capability to estimate the individual credit for each agent. Despite this capability, the main technical difficulty lies in the computational complexity of Shapley Value who grows factorially as the number of agents. We instead utilize an approximation method via Monte Carlo sampling, which reduces the sample complexity while maintaining its effectiveness. We evaluate our method on StarCraft II benchmarks across different scenarios. Our method outperforms existing cooperative MARL algorithms significantly and achieves the state-of-the-art, with especially large margins on tasks with more severe difficulties.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {934–942},
numpages = {9},
keywords = {reinforcement learning, credit assignment, shapley value, multi-agent systems, counterfactual thinking},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.5555/3398761.3399006,
author = {Gupta, Shubham and Hazra, Rishi and Dukkipati, Ambedkar},
title = {Networked Multi-Agent Reinforcement Learning with Emergent Communication},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We develop a Multi-Agent Reinforcement Learning (MARL) method that finds approximately optimal policies for cooperative agents that co-exist in an environment. Central to achieving this is how the agents learn to communicate with each other. Can they together develop a language while learning to perform a common task? We formulate and study a MARL problem where cooperative agents are connected via a fixed underlying network. These agents communicate along the edges of this network by exchanging discrete symbols. However, the semantics of these symbols are not predefined and have to be learned during the training process. We propose a method for training these agents using emergent communication. We demonstrate the applicability of the proposed framework by applying it to the problem of managing traffic controllers, where we achieve state-of-the-art performance (as compared to several strong baselines) and perform a detailed analysis of the emergent communication.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1858–1860},
numpages = {3},
keywords = {traffic, multi-agent reinforcement learning, emergent communication},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1109/ASONAM49781.2020.9381416,
author = {Yang, Zhou and Nguyen, Long and Zhu, Jiazhen and Pan, Zhenhe and Li, Jia and Jin, Fang},
title = {Coordinating Disaster Emergency Response with Heuristic Reinforcement Learning},
year = {2021},
isbn = {9781728110561},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASONAM49781.2020.9381416},
doi = {10.1109/ASONAM49781.2020.9381416},
abstract = {A crucial and time-sensitive task when any disaster occurs is to rescue victims and distribute resources to the right groups and locations. This task is challenging in populated urban areas, due to a huge burst of help requests made in a very short period. To improve the efficiency of the emergency response in the immediate aftermath of a disaster, we propose a heuristic multi-agent reinforcement learning scheduling algorithm, named as ResQ, which can effectively schedule a rapid deployment of volunteers to rescue victims in dynamic settings. The core concept is to quickly identify victims and volunteers from social network data and then schedule rescue parties with an adaptive learning algorithm. This framework performs two key functions: 1) identify trapped victims and volunteers, and 2) optimize the volunteers' rescue strategy in a complex time-sensitive environment. The proposed ResQ algorithm can speed up the training processes through a heuristic function which reduces the state-action space by identifying a set of particular actions over others. Experimental results showed that the proposed heuristic multi-agent reinforcement learning based scheduling outperforms several state-of-art methods, in terms of both reward rate and response times.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {565–572},
numpages = {8},
location = {Virtual Event, Netherlands},
series = {ASONAM '20}
}

@inproceedings{10.5555/3378680.3378879,
author = {Tabrez, Aaquib and Hayes, Bradley},
title = {Improving Human-Robot Interaction through Explainable Reinforcement Learning},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Gathering the most informative data from humans without overloading them remains an active research area in AI, and is closely coupled with the problems of determining how and when information should be communicated to others [12]. Current decision support systems (DSS) are still overly simple and static, and cannot adapt to changing environments we expect to deploy in modern systems [3], [4], [9], [11]. They are intrinsically limited in their ability to explain rationale versus merely listing their future behaviors, limiting a human's understanding of the system [2], [7]. Most probabilistic assessments of a task are conveyed after the task/skill is attempted rather than before [10], [14], [16]. This limits failure recovery and danger avoidance mechanisms. Existing work on predicting failures relies on sensors to accurately detect explicitly annotated and learned failure modes [13]. As such, important non-obvious pieces of information for assessing appropriate trust and/or course-of-action (COA) evaluation in collaborative scenarios can go overlooked, while irrelevant information may instead be provided that increases clutter and mental workload. Understanding how AI models arrive at specific decisions is a key principle of trust [8]. Therefore, it is critically important to develop new strategies for anticipating, communicating, and explaining justifications and rationale for AI driven behaviors via contextually appropriate semantics.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {751–753},
numpages = {3},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/2484920.2485093,
author = {Zhang, Chongjie and Lesser, Victor},
title = {Coordinating Multi-Agent Reinforcement Learning with Limited Communication},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Coordinated multi-agent reinforcement learning (MARL) provides a promising approach to scaling learning in large cooperative multi-agent systems. Distributed constraint optimization (DCOP) techniques have been used to coordinate action selection among agents during both the learning phase and the policy execution phase (if learning is off-line) to ensure good overall system performance. However, running DCOP algorithms for each action selection through the whole system results in significant communication among agents, which is not practical for most applications with limited communication bandwidth. In this paper, we develop a learning approach that generalizes previous coordinated MARL approaches that use DCOP algorithms and enables MARL to be conducted over a spectrum from independent learning (without communication) to fully coordinated learning depending on agents' communication bandwidth. Our approach defines an interaction measure that allows agents to dynamically identify their beneficial coordination set (i.e., whom to coordinate with) in different situations and to trade off its performance and communication cost. By limiting their coordination set, agents dynamically decompose the coordination network in a distributed way, resulting in dramatically reduced communication for DCOP algorithms without significantly affecting overall learning performance. Essentially, our learning approach conducts co-adaptation of agents' policy learning and coordination set identification, which outperforms approaches that sequence them.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1101–1108},
numpages = {8},
keywords = {multiagent learning, distributed constraint optimization, coordinated learning},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.1145/3578741.3578807,
author = {Dou, Lintao and Huang, Jian},
title = {Cross-Domain Sentiment Classification via Deep Reinforcement Learning},
year = {2023},
isbn = {9781450399067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578741.3578807},
doi = {10.1145/3578741.3578807},
abstract = {Cross-domain sentiment classification aims to transferring the labeled knowledge of the source domain to the target domain for sentiment classification. In the case of lacking labeled target domain data, it reduces manual marking cost of labeling data. Most existing works perform transfer learning by extracting pivot and non-pivot features between domains. These methods are susceptible to noise data and ignore important sentiment information. To solve the above problem, we propose the cross-domain sentiment classification via deep reinforcement learning. We propose a deep reinforcement learning framework, which formulates feature selection policy to solve the noise data problem and pay attention to important data features. The framework is integrated with a policy network and a classification network. The policy network is applied to the feature selection. The classification network makes predictions based on feature selection and calculate the reward of the policy network. Experiment results on the Amazon review datasets demonstrate that the proposed method considerably outperforms other state-of-the-art methods.},
booktitle = {Proceedings of the 2022 5th International Conference on Machine Learning and Natural Language Processing},
pages = {337–341},
numpages = {5},
keywords = {Cross-domain, Deep Reinforcement Learning, Policy Learning, Sentiment Classification},
location = {Sanya, China},
series = {MLNLP '22}
}

@inproceedings{10.1145/3397271.3401250,
author = {Liu, Jianfeng and Pan, Feiyang and Luo, Ling},
title = {GoChat: Goal-Oriented Chatbots with Hierarchical Reinforcement Learning},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401250},
doi = {10.1145/3397271.3401250},
abstract = {A chatbot that converses like a human should be goal-oriented (i.e., be purposeful in conversation), which is beyond language generation. However, existing goal-oriented dialogue systems often heavily rely on cumbersome hand-crafted rules or costly labelled datasets, which limits the applicability. In this paper, we propose Goal-oriented Chatbots (GoChat), a framework for end-to-end training the chatbot to maximize the long-term return from offline multi-turn dialogue datasets. Our framework utilizes hierarchical reinforcement learning (HRL), where the high-level policy determines some sub-goals to guide the conversation towards the final goal, and the low-level policy fulfills the sub-goals by generating the corresponding utterance for response. In our experiments conducted on a real-world dialogue dataset for anti-fraud in financial, our approach outperforms previous methods on both the quality of response generation as well as the success rate of accomplishing the goal.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1793–1796},
numpages = {4},
keywords = {reinforcement learning, dialogue system, goal-oriented chatbot},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/1082473.1082482,
author = {Taylor, Matthew E. and Stone, Peter},
title = {Behavior Transfer for Value-Function-Based Reinforcement Learning},
year = {2005},
isbn = {1595930930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1082473.1082482},
doi = {10.1145/1082473.1082482},
abstract = {Temporal difference (TD) learning methods [22] have become popular reinforcement learning techniques in recent years. TD methods have had some experimental successes and have been shown to exhibit some desirable properties in theory, but have often been found very slow in practice. A key feature of TD methods is that they represent policies in terms of value functions. In this paper we introduce behavior transfer, a novel approach to speeding up TD learning by transferring the learned value function from one task to a second related task. We present experimental results showing that autonomous learners are able to learn one multiagent task and then use behavior transfer to markedly reduce the total training time for a more complex task.},
booktitle = {Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multiagent Systems},
pages = {53–59},
numpages = {7},
location = {The Netherlands},
series = {AAMAS '05}
}

@inproceedings{10.5555/3535850.3536076,
author = {Ma, Jinming and Chen, Yingfeng and Wu, Feng and Ji, Xianpeng and Ding, Yu},
title = {Multimodal Reinforcement Learning with Effective State Representation Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Many real-world applications require an agent to make robust and deliberate decisions with multimodal information (e.g., robots with multi-sensory inputs). However, it is very challenging to train the agent via reinforcement learning (RL) due to the heterogeneity and dynamic importance of different modalities. Specifically, we observe that these issues make conventional RL methods difficult to learn a useful state representation in the end-to-end training with multimodal information. To address this, we propose a novel multimodal RL approach that can do multimodal alignment and importance enhancement according to their similarity and importance in terms of RL tasks respectively. By doing so, we are able to learn an effective state representation and consequentially improve the RL training process. We test our approach on several multimodal RL domains, showing that it outperforms state-of-the-art methods in terms of learning speed and policy quality.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1684–1686},
numpages = {3},
keywords = {multimodal learning, deep reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.5555/3091125.3091280,
author = {da Silva, Felipe Leno and Glatt, Ruben and Costa, Anna Helena Reali},
title = {Simultaneously Learning and Advising in Multiagent Reinforcement Learning},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Reinforcement Learning has long been employed to solve sequential decision-making problems with minimal input data. However, the classical approach requires a large number of interactions with an environment to learn a suitable policy. This problem is further intensified when multiple autonomous agents are simultaneously learning in the same environment. The teacher-student approach aims at alleviating this problem by integrating an advising procedure in the learning process, in which an experienced agent (human or not) can advise a student to guide her exploration. Even though previous works reported that an agent can learn faster when receiving advice, their proposals require that the teacher is an expert in the learning task. Sharing successful episodes can also accelerate learning, but this procedure requires a lot of communication between agents, which is unfeasible for domains in which communication is limited. Thus, we here propose a multiagent advising framework where multiple agents can advise each other while learning in a shared environment. If in any state an agent is unsure about what to do, it can ask for advice to other agents and may receive answers from agents that have more confidence in their actuation for that state. We perform experiments in a simulated Robot Soccer environment and show that the learning process is improved by incorporating this kind of advice.},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {1100–1108},
numpages = {9},
keywords = {transfer learning, multiagent reinforcement learning, autonomous advice taking, cooperative learning},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@inproceedings{10.1145/2330163.2330286,
author = {Loscalzo, Steven and Wright, Robert and Acunto, Kevin and Yu, Lei},
title = {Sample Aware Embedded Feature Selection for Reinforcement Learning},
year = {2012},
isbn = {9781450311779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330163.2330286},
doi = {10.1145/2330163.2330286},
abstract = {Reinforcement learning (RL) is designed to learn optimal control policies from unsupervised interactions with the environment. Many successful RL algorithms have been developed, however, none of them can efficiently tackle problems with high-dimensional state spaces due to the "curse of dimensionality," and so their applicability to real-world scenarios is limited. Here we propose a Sample Aware Feature Selection algorithm embedded in NEAT, or SAFS-NEAT, to help address this challenge. This algorithm builds upon the powerful evolutionary policy search algorithm NEAT, by exploiting data samples collected during the learning process. This data permits feature selection techniques from the supervised learning domain to be used to help RL scale to problems with high-dimensional state spaces. We show that by exploiting previously observed samples, on-line feature selection can enable NEAT to learn near optimal policies for such problems, and also outperform an existing feature selection algorithm which does not explicitly make use of this available data.},
booktitle = {Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation},
pages = {887–894},
numpages = {8},
keywords = {feature selection, reinforcement learning, evolutionary policy search},
location = {Philadelphia, Pennsylvania, USA},
series = {GECCO '12}
}

@inproceedings{10.5555/3398761.3398875,
author = {Oller, Declan and Glasmachers, Tobias and Cuccu, Giuseppe},
title = {Analyzing Reinforcement Learning Benchmarks with Random Weight Guessing},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We propose a novel method for analyzing and visualizing the complexity of standard reinforcement learning (RL) benchmarks based on score distributions. A large number of policy networks are generated by randomly guessing their parameters, and then evaluated on the benchmark task; the study of their aggregated results provide insights into the benchmark complexity. Our method guarantees objectivity of evaluation by sidestepping learning altogether: the policy network parameters are generated using Random Weight Guessing (RWG), making our method agnostic to (i) the classic RL setup, (ii) any learning algorithm, and (iii) hyperparameter tuning. We show that this approach isolates the environment complexity, highlights specific types of challenges, and provides a proper foundation for the statistical analysis of the task's difficulty. We test our approach on a variety of classic control benchmarks from the OpenAI Gym, where we show that small untrained networks can provide a robust baseline for a variety of tasks. The networks generated often show good performance even without gradual learning, incidentally highlighting the triviality of a few popular benchmarks.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {975–982},
numpages = {8},
keywords = {learning agent capabilities, machine learning, evolutionary algorithms},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@article{10.5555/3207692.3207712,
author = {Vodopivec, Tom and Samothrakis, Spyridon and \v{S}ter, Branko},
title = {On Monte Carlo Tree Search and Reinforcement Learning},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Fuelled by successes in Computer Go, Monte Carlo tree search (MCTS) has achieved wide-spread adoption within the games community. Its links to traditional reinforcement learning (RL) methods have been outlined in the past; however, the use of RL techniques within tree search has not been thoroughly studied yet. In this paper we re-examine in depth this close relation between the two fields; our goal is to improve the cross-awareness between the two communities. We show that a straightforward adaptation of RL semantics within tree search can lead to a wealth of new algorithms, for which the traditional MCTS is only one of the variants. We confirm that planning methods inspired by RL in conjunction with online search demonstrate encouraging results on several classic board games and in arcade video game competitions, where our algorithm recently ranked first. Our study promotes a unified view of learning, planning, and search.},
journal = {J. Artif. Int. Res.},
month = {sep},
pages = {881–936},
numpages = {56}
}

@inproceedings{10.1145/1390156.1390187,
author = {Diuk, Carlos and Cohen, Andre and Littman, Michael L.},
title = {An Object-Oriented Representation for Efficient Reinforcement Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390187},
doi = {10.1145/1390156.1390187},
abstract = {Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, which is a natural way of modeling environments and offers important generalization opportunities. We introduce a learning algorithm for deterministic OO-MDPs and prove a polynomial bound on its sample complexity. We illustrate the performance gains of our representation and algorithm in the well-known Taxi domain, plus a real-life videogame.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {240–247},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@article{10.1109/TASLP.2018.2852739,
author = {Lee, Hung-Yi and Chung, Pei-Hung and Wu, Yen-Chen and Lin, Tzu-Hsiang and Wen, Tsung-Hsien},
title = {Interactive Spoken Content Retrieval by Deep Reinforcement Learning},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2852739},
doi = {10.1109/TASLP.2018.2852739},
abstract = {For text content retrieval, the user can easily scan through and select from a list of retrieved items. This is impossible for spoken content retrieval, because the retrieved items are not easily displayed on-screen. In addition, due to the high degree of uncertainty for speech recognition, retrieval results can be very noisy. One way to counter such difficulties is through user-machine interaction. The machine can take different actions to interact with the user to obtain better retrieval results before showing them to the user. For example, the machine can request extra information from the user, return a list of topics for the user to select from, and so on. In this paper, we propose using deep-Q-network DQN to determine the machine actions for interactive spoken content retrieval. DQN bypasses the need to estimate hand-crafted states, and directly determines the best action based on the present retrieval results even without any human knowledge. It is shown to achieve significantly better performance as compared with the previous hand-crafted states. We further find that double DQN and dueling DQN improve the naive version.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {2447–2459},
numpages = {13}
}

