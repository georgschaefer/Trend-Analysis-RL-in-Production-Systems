"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Energy Efficient Relay in UAV Networks Against Jamming: A Reinforcement Learning Based Approach","W. Wang; X. Lu; S. Liu; L. Xiao; B. Yang","Dept. of Information and Communication Engineering, Xiamen University, Xiamen, China; Dept. of Information and Communication Engineering, Xiamen University, Xiamen, China; Dept. of Information and Communication Engineering, Xiamen University, Xiamen, China; Dept. of Information and Communication Engineering, Xiamen University, Xiamen, China; Dept. of Automation, Shanghai Jiao Tong University, Shanghai, China","2020 IEEE 91st Vehicular Technology Conference (VTC2020-Spring)","30 Jun 2020","2020","","","1","5","Unmanned aerial vehicle (UAV) networks are vulnerable to jamming attacks because of the high mobility, limited battery and scarce spectrum resources of UAVs. In this paper, we propose a reinforcement learning based UAV relay scheme to improve the anti-jamming capability and save energy consumption of the UAV network. Based on the real-time channel conditions and the historical relay experiences, the proposed scheme enables UAVs to improve the policy of relay power and strategies without knowing the UAV network and channel model. Simulation results show that the proposed UAV relay scheme reduces the bit error rate of the messages and reduces the energy consumption of the UAV network compared with the state-of-the-art benchmark.","2577-2465","978-1-7281-5207-3","10.1109/VTC2020-Spring48590.2020.9128716","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9128716","Jamming;unmanned aerial vehicles;relay;reinforcement learning","Relays;Jamming;Unmanned aerial vehicles;Batteries;Power control;Channel models;Energy consumption","aircraft communication;autonomous aerial vehicles;energy conservation;energy consumption;error statistics;jamming;learning (artificial intelligence);relay networks (telecommunication);telecommunication computing;telecommunication power management","UAV network;reinforcement learning;unmanned aerial vehicle networks;jamming attacks;UAV relay scheme;antijamming capability;energy consumption;historical relay experiences;energy efficient relay;real-time channel conditions;spectrum resources;relay power","","3","","19","IEEE","30 Jun 2020","","","IEEE","IEEE Conferences"
"Grid Voltage Control Method Based on Generator Reactive Power Regulation Using Reinforcement Learning","Y. Wang","School of Electrical Engineering and Automation, Harbin Institute of Technology, Harbin, China","2020 IEEE/IAS Industrial and Commercial Power System Asia (I&CPS Asia)","29 Sep 2020","2020","","","1060","1065","Too high or too low grid voltage will greatly affect the operation safety of the power system. This paper proposes a voltage regulation method based on generator reactive power regulation using reinforcement learning. First, the members of the agent are selected based on the reactive power of the generator, and then the Q tables are trained corresponding to the agent. According to the voltage amplitude state of the point to be adjusted, the regulation action is given. In order to solve the problem of complex combination of action sets and difficult convergence of Q table in a single agent, multi-agents are selected to decompose complex regulation actions into a series of continuous simple actions. At the same time, in order to improve the effectiveness of agent actions, the “progress” reward mechanism is proposed. The method based on reinforcement learning proposed in this paper can effectively regulate the reactive voltage of the system.","","978-1-7281-4303-3","10.1109/ICPSAsia48933.2020.9208556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9208556","reactive voltage regulation;reinforcement learning;reactive power;multi-agents","Industrial power systems;Asia;Conferences","control engineering computing;learning (artificial intelligence);multi-agent systems;power engineering computing;power generation control;power grids;reactive power control;voltage control","reinforcement learning;reactive voltage;grid voltage control method;reactive power regulation;power system;voltage regulation method","","3","","10","IEEE","29 Sep 2020","","","IEEE","IEEE Conferences"
"Simulation of mobile robot navigation utilizing reinforcement and unsupervised weightless neural network learning algorithm","Y. Yusof; H. M. A. H. Mansor; H. M. Dani Baba","Industrial Automation Section, Universiti Kuala Lumpur Malaysia France Institute, Selangor, Malaysia; Faculty of Electrical Engineering, University Technology MARA Shah Alam, Selangor, Malaysia; Faculty of Electrical Engineering, University Technology MARA, Selangor, Malaysia","2015 IEEE Student Conference on Research and Development (SCOReD)","9 Apr 2016","2015","","","123","128","The approach of transforming human expert knowledge into computer program only allow a system to solve foreseen and tested outcomes compared to a system having self-learning capabilities. This paper will summarize and discuss the research, design and implementation of a novel self-learning algorithm which combines: (a) Q-Learning - A reinforcement learning algorithm; and (b) AutoWiSARD - An unsupervised weightless neural network learning algorithm. The self-learning algorithm was implemented in an autonomous mobile robot navigation and obstacle avoidance system in a simulated environment. The AutoWiSARD algorithm identifies, differentiates and classifies the obstacles and the Q-learning algorithm learns and tries to maneuver through these obstacles. This novel hybrid technique allows the autonomous system to acquire knowledge, learn and record experience thus attaining self-learning state. The final result shows the simulated mobile robot was able to differentiate various shapes of obstacles such as corners and walls; and create complex control sequences of movements to maneuver through these obstacles.","","978-1-4673-9572-4","10.1109/SCORED.2015.7449308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7449308","reinforcement learning;Q-learning;AutoWiSARD;autonomous navigation;unsupervised learning;weightless neural network;robot simulation","","collision avoidance;control engineering computing;digital simulation;mobile robots;neural nets;unsupervised learning","mobile robot navigation simulation;obstacle avoidance system;reinforcement learning algorithm;Q-Learning algorithm;unsupervised weightless neural network learning algorithm;AutoWiSARD algorithm;self-learning capability","","2","","28","IEEE","9 Apr 2016","","","IEEE","IEEE Conferences"
"A Law of Iterated Logarithm for Multi-Agent Reinforcement Learning","G. Thoppe; B. Kumar","Computer Science and Automation, Indian Institute of Science, Bengaluru, Karnataka, India; Electrical and Computer Engineering, University of Wisconsin at Madison, Madison, WI, USA","2021 Seventh Indian Control Conference (ICC)","14 Feb 2022","2021","","","19","20","In Multi-Agent Reinforcement Learning (MARL), multiple agents interact with a common environment, as also with each other, for solving a shared problem in sequential decision-making. In this work, we derive a novel law of iterated logarithm for a family of distributed nonlinear stochastic approximation schemes that is useful in MARL. In particular, our result describes the convergence rate on almost every sample path where the algorithm converges. This result is the first of its kind in the distributed setup and provides deeper insights than the existing ones, which only discuss convergence rates in the expected or the CLT sense. Importantly, our result holds under significantly weaker assumptions: neither the gossip matrix needs to be doubly stochastic nor the stepsizes square summable.","","978-1-6654-0978-0","10.1109/ICC54714.2021.9702912","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9702912","","Decision making;Reinforcement learning;Approximation algorithms;Convergence","approximation theory;convergence;decision making;learning (artificial intelligence);matrix algebra;multi-agent systems;stochastic processes","multiagent reinforcement learning;MARL;sequential decision-making;iterated logarithm;distributed nonlinear stochastic approximation schemes;convergence rate;multiple agents interaction;shared problem solving;CLT sense","","2","","7","IEEE","14 Feb 2022","","","IEEE","IEEE Conferences"
"Motion Planning and Control with Randomized Payloads Using Deep Reinforcement Learning","A. Demir; V. Sezer","Department of Mechatronics Engineering, Istanbul Technical University, Istanbul, Turkey; Department of Control and Automation Engineering, Istanbul Technical University, Istanbul, Turkey","2019 Third IEEE International Conference on Robotic Computing (IRC)","28 Mar 2019","2019","","","32","37","In this study, we present a unified motion planner with low-level controller for continuous control of a differential drive mobile robot under variable payload values. Our deep reinforcement agent takes 11 dimensional state vector as input and calculates each wheel's torque value as a 2 dimensional output vector. These torque values are fed into the dynamic model of the robot, and lastly steering commands are gathered. In previous studies, intersection navigation solutions that uses deep - RL methods, have not been considered with variable payloads. Our study is focused specifically on service robotic applications where payload is subject to change. To the best of our knowledge, this is the first study in the literature that investigates intersection - navigation problem under variable payloads using deep-RL. In this paper, deep-RL based motion planning is performed by considering both kinematic and dynamic constraints. According to the simulations in a dynamic environment, the agent successfully navigates to target with 98.2% success rate in test time with unseen payload masses during training. Another agent is also trained without payload randomization for comparison. Results show that our agent outperforms the other agent, that is not aware of its own payload, with more than 40% gap.","","978-1-5386-9245-5","10.1109/IRC.2019.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675660","Reinforcement Learning;Motion Planning;Navigation","Mathematical model;Payloads;Robot kinematics;Torque;Mobile robots;Navigation","learning (artificial intelligence);mobile robots;motion control;navigation;path planning;robot dynamics;robot kinematics;service robots;torque control;wheels","differential drive mobile robot;variable payload values;deep reinforcement agent;torque values;intersection navigation solutions;service robotic applications;deep-RL based motion planning;kinematic constraints;dynamic constraints;payload randomization;deep reinforcement learning;continuous control","","2","","32","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Dynamic Adaptive Streaming Control based on Deep Reinforcement Learning in Named Data Networking","S. Qiu; X. Tan; J. Zhu","Laboratory for Future Networks, University of Science and Technology of China, Hefei, P. R. China; Laboratory for Future Networks, University of Science and Technology of China, Hefei, P. R. China; Department of Automation of University of Science and Technology of China, Hefei, P. R. China","2018 37th Chinese Control Conference (CCC)","7 Oct 2018","2018","","","9478","9482","Named Data Networking (NDN) is a general proposed network layer protocol which offers a set of rich functionality: in-network storage, multi-path forwarding, and data-centric security. The cache and multi-path feature improves the efficiency of transmission but increases the difficulty of the bandwidth estimation. In dynamic adaptive streaming, video of different quality is segmented in server, clients could select the most appropriate segment to download due to the network status. In this paper, a deep reinforcement learning method is proposed for dynamic adaptive video streaming over NDN to maximum user-perceived quality-of-experience(QoE). As a model-free method, our algorithm doesn't need accurate bandwidth estimation. It can use all kinds of information during the video playback process to make a sensitive decision. Experimental results indicate that our algorithm performs better than others in NDN.","1934-1768","978-988-15639-5-8","10.23919/ChiCC.2018.8483332","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8483332","Named Data Networking;Dynamic Adaptive Streaming;Bitrate Adaptation;Deep Reinforcement Learning","Streaming media;Bit rate;Machine learning;Adaptive systems;Bandwidth;Heuristic algorithms;Training","cache storage;client-server systems;Internet;learning (artificial intelligence);quality of experience;video streaming","multipath forwarding;data-centric security;multipath feature;network status;deep reinforcement learning method;dynamic adaptive video;model-free method;video playback process;NDN;dynamic adaptive streaming control;general proposed network layer protocol;in-network storage;bandwidth estimation;named data networking;maximum user-perceived quality-of-experience","","2","","21","","7 Oct 2018","","","IEEE","IEEE Conferences"
"Reinforcement-learning based fault-tolerant control","D. Zhang; Z. Lin; Z. Gao","School of Electrical Engineering & Automation, Tianjin University, Tianjin, China; School of Electrical Engineering, Tianjin University of Technology, Tianjin, China; Faculty of Engineering and Environment, University of Northumbria, Newcastle upon Tyne, UK","2017 IEEE 15th International Conference on Industrial Informatics (INDIN)","13 Nov 2017","2017","","","671","676","Engineering systems are always subjected to faults or malfunctions due to age or unexpected events, which would degrade the operation performance and even lead to the operation failure-Therefore, there is a strong motivation to develop fault-tolerant control strategy so that the system can operate with tolerated perform ance de ggr ad ation-In this p ap er, a novel approach based on reinforcement leaning is proposed to design a fault-tolerant controller without need of the information on faults-T simulation example.","2378-363X","978-1-5386-0837-1","10.1109/INDIN.2017.8104852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8104852","Fault-tolerant control;reinforcement learning;performance index","","failure analysis;fault diagnosis;fault tolerant control;learning (artificial intelligence)","engineering systems;operation performance;operation failure;fault-tolerant control;performance degradation;reinforcement learning","","2","","19","IEEE","13 Nov 2017","","","IEEE","IEEE Conferences"
"Behavior-based reinforcement learning control for robotic rehabilitation training","F. Meng; K. Fan","School of Automation, Beijing Institute of Technology, Beijing, China; Zoucheng forestry administration, Shandong, China","The 27th Chinese Control and Decision Conference (2015 CCDC)","20 Jul 2015","2015","","","4330","4334","A behavior-based reinforcement learning controller (BRL) is developed for robotic rehabilitation training systems with time-varying properties. This adaptive BRL control system consists of an event-based planner layer, a behavior decision-making layer, and a control execution layer. In the adaptive BRL, the event-based planner layer is used to create a representative database and generate a real optimal desired trajectory for each patient, and then the behavior decision-making layer utilizes multiple behavior modules to select an optimal control behavior, which is transmitted to the control execution layer. In addition, to avoid the conflicts and the competition of different control behaviors, a self-adjusting shaping algorithm is proposed for BRL. Simulation experiments verify that the feasibility of the proposed BRL framework.","1948-9447","978-1-4799-7017-9","10.1109/CCDC.2015.7162691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7162691","Robotic rehabilitation system;Behavior-based Reinforcement-learning control;Actor critic;Shaping","Training;Medical treatment;Decision making;Trajectory;Robot kinematics;Learning (artificial intelligence)","adaptive control;learning systems;medical robotics;patient rehabilitation;time-varying systems;trajectory control","self-adjusting shaping algorithm;trajectory generation;control execution layer;behavior decision-making layer;event-based planner layer;adaptive BRL control system;time-varying property;robotic rehabilitation training;behavior-based reinforcement learning control","","1","","21","IEEE","20 Jul 2015","","","IEEE","IEEE Conferences"
"A Bilevel Graph Reinforcement Learning Method for Electric Vehicle Fleet Charging Guidance","Q. Xing; Y. Xu; Z. Chen","School of Automation and the School of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Nanyang Ave, Singapore; School of Electrical Engineering, Southeast University (Sipailou), Nanjing, China","IEEE Transactions on Smart Grid","21 Jun 2023","2023","14","4","3309","3312","This letter proposes a bilevel graph reinforcement learning method for electric vehicle (EV) fleet charging guidance, achieving collaborative optimization of the transportation-electrification coupled system. A dual-agent architecture is first constructed, where the upper-level is used for charging and the lower-level is used for routing. The EV traveling and charging behavior is characterized as a graph-structured interaction process. A graph attention network (GAT) is leveraged to extract the topology correlation and feature information. Then the extracted topology embedded with knowledge, as intermediate latent environment states, is fed into the underlying network of deep reinforcement learning (DRL). A DRL-based sequential scheduling pattern is developed to realize the guidance of multiple EVs. Extensive experimental results verify the superiority and adaptability of our proposed methodology.","1949-3061","","10.1109/TSG.2023.3240580","National Natural Science Foundation of China(grant numbers:52077035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10029881","Electric vehicle fleet charging guidance;transportation electrification coupled system;bilevel graph reinforcement learning;graph attention network","Roads;Navigation;Feature extraction;Topology;Routing;Decision making;Costs","battery powered vehicles;deep learning (artificial intelligence);electric vehicle charging;electric vehicles;graph theory;optimisation;reinforcement learning;transportation","bilevel graph reinforcement learning method;charging behavior;collaborative optimization;deep reinforcement learning;DRL-based sequential scheduling pattern;dual-agent architecture;electric vehicle fleet charging guidance;EV traveling;GAT;graph attention network;graph-structured interaction process;transportation-electrification coupled system","","1","","9","IEEE","30 Jan 2023","","","IEEE","IEEE Journals"
"Mixing Update Q-value for Deep Reinforcement Learning","Z. Li; X. Hou","School of Artificial Intelligence, University of Chinese Academy of Sciences; Center for Research on Intelligent System and Engineering, Institute of Automation, Chinese Academy of Sciences","2019 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2019","2019","","","1","6","The value-based reinforcement learning methods are known to overestimate action values such as deep Q-learning, which could lead to suboptimal policies. This problem also persists in an actor-critic algorithm. In this paper, we propose a novel mechanism to minimize its effects on both the critic and the actor. Our mechanism builds on Double Q-learning, by mixing update action value based on the minimum and maximum between a pair of critics to limit the overestimation. We then propose a specific adaptation to the Twin Delayed Deep Deterministic policy gradient algorithm (TD3) and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several tasks.","2161-4407","978-1-7281-1985-4","10.1109/IJCNN.2019.8852397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8852397","","Reinforcement learning;Space exploration;Neural networks;Games;Estimation;Task analysis;Approximation algorithms","deterministic algorithms;estimation theory;gradient methods;learning (artificial intelligence);neural nets","mixing update Q-value;Deep reinforcement learning;value-based reinforcement learning methods;deep Q-learning;actor-critic algorithm;Double Q-learning;Twin Delayed Deep Deterministic policy gradient algorithm","","1","","15","IEEE","30 Sep 2019","","","IEEE","IEEE Conferences"
"Multi-agent Reinforcement Learning with Knowledge Constraints and Its Application in Scene Confrontation","J. Wang; Z. Wu; J. Li; F. Cheng","China Academy of Launch Vehicle Technology, Beijing, China; China Academy of Launch Vehicle Technology, Beijing, China; China Academy of Launch Vehicle Technology, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China","2022 4th International Conference on Frontiers Technology of Information and Computer (ICFTIC)","27 Mar 2023","2022","","","838","841","Multi-agent reinforcement learning (MARL) can be used in complex scene confrontation involving multiple intelligent operable units. Each intelligent operable unit can be regarded as an agent. However, as the complexity of the scene, there are a large number of decision points with different characteristics. It is difficult to solve this problem by a single MARL algorithm. As a result, a multi-agent reinforcement learning method with knowledge constraints is proposed. The prior information is used as constraints of the MARL model according to the properties of different agents in this paper. The performance of this method is proved in a scene confrontation platform called “Jueshengqianli” (JSQL), which is developed for naval warfare.","","979-8-3503-2195-1","10.1109/ICFTIC57696.2022.10075131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10075131","Multi agent;Reinforcement Learning;Knowledge Constraints;Scene Confrontation","Optimization methods;Reinforcement learning;Complexity theory;Convergence","learning (artificial intelligence);multi-agent systems;reinforcement learning","complex scene confrontation;intelligent operable unit;knowledge constraints;multiagent reinforcement learning method;multiple intelligent operable units;scene confrontation platform;single MARL algorithm","","","","19","IEEE","27 Mar 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Attitude Tracking Control of Spacecraft with Actuator Saturation and Inertial Uncertainty","Y. Xiong; C. Wei; Z. Yin","School of Aeronautics and Astronautics, Central South University, Changsha, China; School of Aeronautics and Astronautics, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China","2021 IEEE 7th International Conference on Control Science and Systems Engineering (ICCSSE)","24 Sep 2021","2021","","","98","102","This paper investigates an attitude tracking control problem of spacecraft subject to actuator saturation and inertial uncertainties. Aiming at alleviating the negative effects of actuator saturation and parameter uncertainties, an adaptive prescribed performance attitude control scheme is proposed via exploring the reinforcement learning policy. By constructing a critic network and an action network, the approximations of the optimal objective function and the optimal control gains of the spacecraft attitude tracking error system are realized. A group of numerical examples is organized to validate the effectiveness and robustness of the proposed attitude control scheme with respect to actuator saturation and inertia uncertainties.","","978-1-6654-4405-7","10.1109/ICCSSE52761.2021.9545116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9545116","spacecraft attitude control;prescribed performance;reinforcement learning","Space vehicles;Actuators;Uncertain systems;Uncertainty;Attitude control;Space missions;Simulation","actuators;adaptive control;attitude control;learning (artificial intelligence);nonlinear control systems;optimal control;robust control;space vehicles;tracking;uncertain systems","actuator saturation;inertial uncertainty;attitude tracking control problem;spacecraft subject;parameter uncertainties;adaptive prescribed performance attitude control scheme;reinforcement learning policy;optimal control gains;spacecraft attitude;inertia uncertainties","","","","15","IEEE","24 Sep 2021","","","IEEE","IEEE Conferences"
"Catastrophic Interference in Reinforcement Learning: A Solution Based on Context Division and Knowledge Distillation","T. Zhang; X. Wang; B. Liang; B. Yuan","Intelligent Computing Laboratory, Shenzhen International Graduate School, Tsinghua University, Shenzhen 518055, China.; Center for Artificial Intelligence and Robotics, Shenzhen International Graduate School, Tsinghua University, Shenzhen 518055, China.; Research Center for Navigation and Control, Department of Automation, Tsinghua University, Beijing 100084, China.; Intelligent Computing Laboratory, Shenzhen International Graduate School, Tsinghua University, Shenzhen 518055, China.","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","15","The powerful learning ability of deep neural networks enables reinforcement learning (RL) agents to learn competent control policies directly from continuous environments. In theory, to achieve stable performance, neural networks assume identically and independently distributed (i.i.d.) inputs, which unfortunately does not hold in the general RL paradigm where the training data are temporally correlated and nonstationary. This issue may lead to the phenomenon of ``catastrophic interference'' and the collapse in performance. In this article, we present interference-aware deep Q-learning (IQ) to mitigate catastrophic interference in single-task deep RL. Specifically, we resort to online clustering to achieve on-the-fly context division, together with a multihead network and a knowledge distillation regularization term for preserving the policy of learned contexts. Built upon deep Q networks (DQNs), IQ consistently boosts the stability and performance when compared to existing methods, verified with extensive experiments on classic control and Atari tasks. The code is publicly available at https://github.com/Sweety-dm/Interference-aware-Deep-Q-learning.","2162-2388","","10.1109/TNNLS.2022.3162241","National Natural Science Foundation of China(grant numbers:U1713214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9760159","Catastrophic interference;context division;knowledge distillation;reinforcement learning (RL).","Task analysis;Interference;Training;Neural networks;Optimization;Knowledge engineering;Games","","","","","","","IEEE","19 Apr 2022","","","IEEE","IEEE Early Access Articles"
"An Online Reinforcement Learning Offloading Method for Delay-Sensitive Vehicular Service","W. Liu; X. Shao; C. Wang; X. Gu; F. Jiang; J. Peng","School of Computer Science, Central South University, Changsha, China; School of Computer Science, Central South University, Changsha, China; School of Computer Science, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Computer Science, Central South University, Changsha, China; School of Computer Science, Central South University, Changsha, China","2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)","26 Apr 2021","2020","","","973","978","Nowadays, various advanced vehicular applications are developed for road safety enhancement and traffic optimization, while the limited computation capability of vehicles can not always meet the delay requirements of these applications. Offloading the computation tasks to edge servers is regarded as a promising solution. However, how to select the optimal edge server to offload so as to support many delay-sensitive vehicular services is a challenging problem, especially when the server status changes. To overcome this challenge, this paper proposes an online reinforcement learning offloading method. The offloading decision process is divided into two steps to reduce the computation cost. Firstly, the edge server candidates are obtained through analysis on the historical server data, using the reinforcement learning method. Secondly, the optimal edge server is selected from the candidates online through observing current condition of the status of servers. Simulation results verify the superiority of the proposed method and it can effectively reduce running time.","","978-1-7281-7649-9","10.1109/HPCC-SmartCity-DSS50907.2020.00130","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407985","vehicular network;edge computing;computation offloading;reinforcement learning","Simulation;High performance computing;Reinforcement learning;Switches;Road safety;Delays;Trajectory","cloud computing;file servers;learning (artificial intelligence);mobile computing;road safety;road traffic;traffic engineering computing;vehicular ad hoc networks","optimal edge server;road safety enhancement;traffic optimization;delay requirements;computation tasks;offloading decision process;online reinforcement learning offloading;delay sensitive vehicular service;mobile devices;cloud server","","","","16","IEEE","26 Apr 2021","","","IEEE","IEEE Conferences"
"Desynchronizing of noisy neuron networks using reinforcement learning","M. Lu; X. Wei","School of Information Technology Engineering, Tianjin University of Technology and Education, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University, Tianjin, China","2017 8th International IEEE/EMBS Conference on Neural Engineering (NER)","14 Aug 2017","2017","","","296","299","Mitigating pathological synchrony of neurons in basal ganglia networks was considered as one of the potential mechanisms of deep brain stimulation (DBS) in treating Parkinson's disease. Motivated by reducing the energy of external stimuli, optimal control strategies are presented to regulate DBS waveform so as to mitigate synchronous oscillations of neural networks with fewer energy expenditure. In this paper, the adaptive optimal control of DBS based on reinforcement learning (RL) is designed to desynchronizing phase models of neural populations in the presence of noise. Numerical simulations show the effectiveness of the proposed method. Moreover, the influence of noise intensity on the control performance of the controller is analyzed.","1948-3554","978-1-5090-4603-4","10.1109/NER.2017.8008349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008349","","Neurons;Satellite broadcasting;Biological neural networks;Mathematical model;Synchronization;Learning (artificial intelligence);Biological system modeling","cellular biophysics;diseases;learning (artificial intelligence);medical computing;neural nets;neurophysiology;numerical analysis;prosthetics","noisy neuron networks;reinforcement learning;pathological synchrony;basal ganglia networks;deep brain stimulation;Parkinson disease;external stimuli;optimal control strategies;synchronous oscillations;energy expenditure;desynchronizing phase models;neural populations;numerical simulations","","","","26","IEEE","14 Aug 2017","","","IEEE","IEEE Conferences"
"Path Planning with Autonomous Obstacle Avoidance Using Reinforcement Learning for Six-axis Arms","Y. Jia; Y. Li; B. Xin; C. Chen","School of Electric and Automation Engineering, Nanjing Normal University, Nanjing, China; School of Management and Engineering, Nanjing University, Nanjing, China; School of Management and Engineering, Nanjing University, Nanjing, China; School of Management and Engineering, Nanjing University, Nanjing, China","2020 IEEE International Conference on Networking, Sensing and Control (ICNSC)","4 Nov 2020","2020","","","1","6","In this paper, a strategy of path planning for autonomous obstacle avoidance using reinforcement learning for six-axis arms is proposed. This strategy gives priority to planning the obstacle avoidance path for the terminal of the mechanical arm, and then uses the calculated terminal path to plan the poses of the mechanical arm. For the points on the terminal path that the mechanical arm cannot avoid obstacles within the limit of the safe distance, this strategy will record these points as new obstacles and plan a new obstacle avoidance path for the terminal of mechanical arm. The above process is accelerated by the assisted learning strategies and looped until the correct path being calculated. The method proposed in this paper has been applied to a six-axis mechanical arm, and the simulation results show that this method can effectively plan an optimal path and poses for the mechanical arm.","","978-1-7281-6855-5","10.1109/ICNSC48988.2020.9238112","National Natural Science Foundation of China(grant numbers:71732003); National Key Research and Development Program of China(grant numbers:2016YFD0702100); Fundamental Research Funds for the Central Universities(grant numbers:011814380035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238112","Q-learning;obstacle avoidance;path planning;six-axis arms;assisted learning strategies","Simulation;Reinforcement learning;Path planning;Sensors;Planning;Collision avoidance;Task analysis","collision avoidance;control engineering computing;learning (artificial intelligence);manipulators","path planning;autonomous obstacle avoidance;reinforcement learning;obstacle avoidance path;calculated terminal path;assisted learning strategies;correct path;six-axis mechanical arm;optimal path","","","","18","IEEE","4 Nov 2020","","","IEEE","IEEE Conferences"
"Reinforcement Q-Learning and Non-Zero-Sum Games Optimal Tracking Control for Discrete-Time Linear Multi-Input Systems","J. -G. Zhao","School of Machinery and Automation, Weifang University, Weifang, Shandong, P. R. China","2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)","7 Jul 2023","2023","","","277","282","This paper studies the optimal tracking control problem of discrete-time linear multi-input systems from the perspective of Non-Zero-Sum Games (NZSG) using reinforcement Q-learning technique. Firstly, an augmented multi-input systems is constructed by combining the original multi-input systems and the reference trajectory dynamics. Then, the original optimal tracking control problem can be transformed into the NZSG optimal control problem of the constructed augmented multi-input systems. In order to obtain the Nash equilibrium solution of the NZSG optimal control problem, a Q-function is introduced and an reinforcement Q-learning algorithm is designed to learn the Nash equilibrium solution. The convergence of the reinforcement Q-learning algorithm is also given. Finally, a simulation example is given to verify the effectiveness of the proposed reinforcement Q-learning algorithm.","2767-9861","979-8-3503-2105-0","10.1109/DDCLS58216.2023.10166093","Shandong Provincial Natural Science Foundation(grant numbers:ZR2022QF096); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10166093","Reinforcement Learning;Q-Learning;Optimal Tracking Control;Non-Zero-Sum Games (NZSG);Multi-Input Systems","Linear systems;Learning systems;Q-learning;System dynamics;Heuristic algorithms;Optimal control;Games","discrete time systems;game theory;linear systems;optimal control;tracking","constructed augmented multiinput systems;discrete-time linear multiinput systems;Nash equilibrium solution;NonZero-Sum Games optimal tracking control;NZSG optimal control problem;original multiinput systems;original optimal tracking control problem;reinforcement Q-learning algorithm","","","","26","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"Freeway network traffic management based on distributed reinforcement learning","K. Wen; W. Yang; S. Qu","School of Electronics and Control Engineering, Chang'an University, Xi'an, Shaanxi, China; School of Electronics and Control Engineering, Chang'an University, Xi'an, Shaanxi, China; School of Automation, Northwestern Polytechnical University, Xi'an, Shaanxi, China","2010 2nd IEEE International Conference on Information Management and Engineering","3 Jun 2010","2010","","","684","687","A distributed machine learning approach in traffic flow control and dynamic route guidance is presented. The problem domain, a freeway network traffic flow integration control application considers multiple objectives of system, is formulated as a distributed reinforcement learning problem. The Gini coefficient is adopted in this study as an indicator of equity. The DRL approach was implemented via a multi-agent control architecture where the decision agent was assigned to each of the on-ramp or VMS. The reward of each agent is simultaneously updating a single shared policy. The control strategy's effect is demonstrated through its application to the simple freeway network. Analyses of simulation results using this approach show the equity of the system have a significant improvement over traditional control, especially for the case of traffic peak hour. Using the DRL approach, the Gini coefficient of the network has been reduced by 28.99% compared to traditional method.","","978-1-4244-5263-7","10.1109/ICIME.2010.5477875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477875","traffic flow;traffic control;guidance;equity;reinforcement learning;freeway","Traffic control;Telecommunication traffic;Communication system traffic control;Engineering management;Control engineering;Electronic mail;Machine learning;Control systems;Voice mail;Analytical models","control engineering computing;learning (artificial intelligence);multi-agent systems;road traffic;traffic engineering computing","freeway network traffic management;distributed reinforcement learning;distributed machine learning;traffic flow control;dynamic route guidance;Gini coefficient;multi-agent control architecture","","","","10","IEEE","3 Jun 2010","","","IEEE","IEEE Conferences"
"Using Deep Reinforcement Learning to Solve a Navigation Problem for a Swarm Robotics System","A. Iskandar; H. M. Rostum; B. Kovács","Faculty of Mechanical Engineering and Informatics, University of Miskolc, Miskolc, Hungary; Institute of Automation and Info-Communication, University of Miskolc, Miskolc, Hungary; Faculty of Mechanical Engineering and Informatics, University of Miskolc, Miskolc, Hungary","2023 24th International Carpathian Control Conference (ICCC)","19 Jul 2023","2023","","","185","189","Five mobile robots in a swarm are trained in this paper using the deep reinforcement learning method to solve a navigation problem with two targets without prior knowledge of the environment. The Proximal Policy Optimization method trains E-puck mobile robots to avoid obstacles while completing tasks in the shortest distance for each robot. A Webots simulator is used to model the environment in three-dimension space. The suggested algorithm works with continuous states derived from eight infrared sensors and continuous action spaces that reflect the velocities of two motors for each robot in the swarm. Then the robot's behavior will be examined in light of two categories of rewards: Spares and Shaping rewards. If the environment's complexity is not decreased, proximal policy optimization with spare rewards will not be able to train every robot in the system how to accomplish its goal. Compared to the spare rewards technique, shaping rewards aid the robot in gaining experience from prior knowledge during the training process. This speeds up learning and aids navigation in more complex environments.","","979-8-3503-1022-1","10.1109/ICCC57093.2023.10178888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10178888","Reinforcement Learning;Swarm Robotic;Proximal Policy Optimization;Spares Reward;Shaping Reward","Deep learning;Training;Navigation;Swarm robotics;Optimization methods;Reinforcement learning;Robot sensing systems","collision avoidance;control engineering computing;deep learning (artificial intelligence);mobile robots;navigation;optimisation;reinforcement learning","complex environments;deep reinforcement learning method;E-puck mobile robots;infrared sensors;navigation problem;obstacle avoidance;proximal policy optimization method;spare rewards technique;swarm robotics system;three-dimension space;training process;Webots simulator","","","","27","IEEE","19 Jul 2023","","","IEEE","IEEE Conferences"
"Double inverted pendulum control by linear quadratic regulator and reinforcement learning","S. Biro; R. -E. Precup; D. Todinca","Crabel Capital Research, Timisoara, Romania; Department of Automation and Applied Informatics, Politehnica University, Timisoara, Timisoara, Romania; Department of Computer Science and Engineering, Politehnica University, Timisoara, Timisoara, Romania","2010 International Joint Conference on Computational Cybernetics and Technical Informatics","21 Jun 2010","2010","","","159","164","The paper gives an original combination of linear quadratic regulator and reinforcement learning dedicated to the position control of a double inverted pendulum system. An agent based on a modified Sarsa algorithm is applied to swing up the pendulum. The linear quadratic regulator is applied to the linearized mathematical model of the process in the vicinity of upright position. Digital simulation results show the performance of the new approach.","","978-1-4244-7433-2","10.1109/ICCCYB.2010.5491309","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5491309","","Regulators;Learning;Mathematical model;Process control;Control systems;Position control;Informatics;Digital simulation;Shape control;Fuzzy control","learning (artificial intelligence);linear quadratic control;linear systems;nonlinear systems;pendulums;position control","double inverted pendulum control;linear quadratic regulator;reinforcement learning;position control;modified Sarsa algorithm;linearized mathematical model;upright position","","","","31","IEEE","21 Jun 2010","","","IEEE","IEEE Conferences"
"Cluster Synchronization of Boolean Control Networks with Reinforcement Learning","Z. Zhou; Y. Liu; J. Cao; M. Abdel-Aty","School of Computer Science and Technology, Zhejiang Normal University, Jinhua, China; the School of Mathematical Sciences, Key Laboratory of Intelligent Education Technology and Application of Zhejiang Province, Zhejiang Normal University, Jinhua, China; School of Mathematics, Southeast University, Nanjing, China; Center for Photonics and Smart Materials, Zewail City of Science and Technology, Egypt","IEEE Transactions on Circuits and Systems II: Express Briefs","","2023","PP","99","1","1","State-flipped control for cluster synchronization of Boolean control networks (BCNs) is considered in this paper. When the reachable set of the target set is empty or the reachable set does not cover all states of the network, state-flipped control is used to achieve cluster synchronization and a corresponding theorem is given. When dealing with large-scale networks, Q-learning (QL) is a model-free reinforcement learning method that can be used to find optimal control sequences. It can stabilize the network to a target set with minimum cost for cluster synchronization. The theoretical results are verified by a numerical example in the end.","1558-3791","","10.1109/TCSII.2023.3293803","National Natural Science Foundation of China(grant numbers:62173308); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10178111","Boolean control networks;Q-learning;cluster synchronization;state-flipped-transition matrix","Synchronization;Matrix converters;Clustering algorithms;State feedback;Biological system modeling;Sensitivity;Optimal control","","","","","","","IEEE","11 Jul 2023","","","IEEE","IEEE Early Access Articles"
"Reinforcement Learning-Based Inertia and Droop Control for Wind Farm Frequency Regulation","Y. Liang; L. Sun; X. Zhao","School of Engineering, University of Warwick, Coventry, United Kingdom; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, Shenzhen, China; School of Engineering, University of Warwick, Coventry, United Kingdom","2022 IEEE Power & Energy Society General Meeting (PESGM)","27 Oct 2022","2022","","","1","5","As more and more wind turbines (WTs) are installed, there is an increasing interest in actively controlling their power output to meet power set-points and to participate in the frequency regulation for the utility grid. Conventional inertial and droop control loops use fixed gains, making it difficult to utilise the kinetic energy of WTs in a wind farm in a synergistic manner based on real-time information. In this paper, the fixed gains are modified to adaptive gains to improve frequency support performance and reduce the impact on mechanical structures. The cooperative frequency control problem for all WTs in a wind farm is modelled as a decentralised partially observable Markov decision process (Dec-POMDP) and solved using a multi-agent deep reinforcement learning (MADRL) algorithm. MATLAB/Simulink and FAST are run in connection to simulate the frequency response of a wind farm, where FAST simulates the mechanical part of WTs and Simulink simulates the electrical part. Simulation results show that the proposed method is effective in reducing frequency drops and the impact of frequency control on the mechanical structure.","1944-9933","978-1-6654-0823-3","10.1109/PESGM48719.2022.9917075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9917075","Frequency regulation;wind generation;inertia and droop control;multi-agent deep reinforcement learning","Software packages;Simulation;Process control;Reinforcement learning;Wind farms;Real-time systems;Mathematical models","","","","","","20","IEEE","27 Oct 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-based Data-Driven load Frequency Control for Microgrid","J. Chen","School of Automation, Nanjing University of Science and Technology, Nanjing, China","2023 8th Asia Conference on Power and Electrical Engineering (ACPEE)","31 May 2023","2023","","","1717","1721","In order to overcome the frequency fluctuations caused by the uncertain random disturbances of microgrids, a data-driven load frequency control (DDB-LFC) method is proposed to achieve the multi-objective optimal control of control and economic performance. In addition, the soft actor-critic algorithm is adopted, and replaces the original LFC controller with an agent that can make independent decisions. It employs the soft actor-critic algorithm with strong robustness to realize the adaptive control of microgrids by reasonably setting the reward function of the agent. The simulation for the Zhuzhou isolated microgrid of Southern Grid (CSG) proves that the proposed algorithm can effectively reduce frequency deviation and generation cost.","","979-8-3503-4552-0","10.1109/ACPEE56931.2023.10135764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10135764","Keywords-deep reinforcement learning;microgrid;soft actor-critic algorithm;load frequency control;controller","Electrical engineering;Adaptation models;Costs;Fluctuations;Optimal control;Microgrids;Robustness","adaptive control;deep learning (artificial intelligence);distributed power generation;frequency control;learning (artificial intelligence);load regulation;optimal control;power engineering computing;power generation control;power grids;power system control;power system interconnection;reinforcement learning","data-driven load frequency control;frequency deviation;frequency fluctuations;generation cost;original LFC controller;soft actor-critic algorithm","","","","21","IEEE","31 May 2023","","","IEEE","IEEE Conferences"
"Does Explicit Prediction Matter in Deep Reinforcement Learning-Based Energy Management?","Z. Qin; H. Zhang; Y. Zhao; H. Xie; J. Cao","Department of Automation, Tsinghua University, Beijing, China; New Smart City High-quality Power Supply Joint Laboratory, China Southern Power Grid, Shenzhen, China; New Smart City High-quality Power Supply Joint Laboratory, China Southern Power Grid, Shenzhen, China; New Smart City High-quality Power Supply Joint Laboratory, China Southern Power Grid, Shenzhen, China; Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China","2021 IEEE International Conference on Energy Internet (ICEI)","8 Feb 2022","2021","","","13","19","As a model-free optimization and decision-making method, deep reinforcement learning (DRL) has been widely applied to the field of energy management in energy Internet. While, some DRL-based energy management schemes also incorporate the prediction module used by the traditional model-based methods, which seems to be unnecessary and even adverse. In this work, we implement the standard energy management scheme with prediction using supervised learning and DRL, and the counterpart without prediction using end-to-end DRL. Then, these two schemes are compared in the unified energy management framework. The simulation results demonstrate that the energy management scheme without prediction is superior over the scheme with prediction. This work intends to rectify the misuse of DRL methods in the field of energy management.","","978-1-6654-0734-2","10.1109/ICEI52466.2021.00009","China Southern Power Grid; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9701049","deep reinforcement learning;energy management;prediction;recurrent neural network","Recurrent neural networks;Simulation;Supervised learning;Decision making;Reinforcement learning;Predictive models;Internet","decision making;deep learning (artificial intelligence);energy management systems;optimisation;power engineering computing;reinforcement learning;supervised learning","deep reinforcement learning-based energy management;model-free optimization;decision-making method;energy Internet;DRL-based energy management schemes;supervised learning","","","","24","IEEE","8 Feb 2022","","","IEEE","IEEE Conferences"
"Modeling Socially Normative Navigation Behaviors from Demonstrations with Inverse Reinforcement Learning","X. Gao; X. Zhao; M. Tan","University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)","19 Sep 2019","2019","","","1333","1340","Navigation in an efficient and socially normative manner is essential for the robot to operate in human populated environments. Traditional methods treat the pedestrians as dynamic obstacles and design a manual cost function for collision avoidance, but neglect social norms in navigation and do not generalize well to new environments. In this paper, we propose a mixture model to capture the human navigation behaviors in terms of the features of the continuous trajectories and discrete navigation decisions, such as passing on the left or right. The lower level of the model aims to generate socially normative trajectories. To this end, we extend inverse reinforcement learning (IRL) framework to a motion planner called Timed Elastic Band to learn from demonstrations. The upper level comprises a discrete distribution over the homotopy classes of the trajectories. IRL algorithm is employed to find the parameters of distribution that match demonstrations best. Experiments demonstrate that our learning algorithm has the capacity to recover the human navigation behaviors that respect social norms, which makes our approach outperform state-of-the-art methods in social navigation scenarios.","2161-8089","978-1-7281-0356-3","10.1109/COASE.2019.8843123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8843123","","Trajectory;Navigation;Robots;Cost function;Planning;Collision avoidance;Mixture models","collision avoidance;control engineering computing;human-robot interaction;learning (artificial intelligence);mixture models;mobile robots;navigation","IRL algorithm;homotopy classes;motion planner;timed elastic band;robot;socially normative navigation behavior modeling;discrete navigation decisions;continuous trajectories;mixture model;collision avoidance;manual cost function;dynamic obstacles;human populated environments;respect social norms;human navigation;learning algorithm;discrete distribution;inverse reinforcement learning framework;socially normative trajectories","","","","23","IEEE","19 Sep 2019","","","IEEE","IEEE Conferences"
"Task Assignment with Minimum Cost for Multi-UAV System via Reinforcement Learning*","W. Lu; H. Liu; Z. Ren; Q. Gao; D. Liu; X. Wang; M. Guo","School of Astronautics, Beihang University, Beijing, P.R. China; Institute of Artificial Intelligence, Beihang University, Beijing, P.R. China; School of Astronautics, Beihang University, Beijing, P.R. China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, P.R. China; China Research and Development Academy of Machinery Equipment, Beijing, P.R. China; Norinco Group Air Ammunition Research Institute, Harbin, P.R. China; China Research and Development Academy of Machinery Equipment, Beijing, P.R. China","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","1654","1658","This paper addresses the task assignment problem for multi-UAV system in pursuit-evasion game via reinforcement learning. The targets are assigned for agents based on the principle of minimizing the total execution cost of multiple tasks. The cost and the corresponding optimal control policy of agent executing each task are solved before the task assignment process. Reinforcement learning algorithm without knowledge of the agent dynamics is proposed to solve the problems arising from high nonlinearities of agent model and parameter uncertainties caused by external disturbances. Simulation results are given to verify the effectiveness of the proposed task assignment algorithm.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10240489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240489","","Uncertain systems;Costs;Heuristic algorithms;Simulation;Optimal control;Reinforcement learning;Games","autonomous aerial vehicles;multi-agent systems;multi-robot systems;optimal control;reinforcement learning","agent dynamics;agent model;multiUAV system;parameter uncertainties;pursuit-evasion game;reinforcement learning algorithm;task assignment algorithm;task assignment problem;task assignment process;total execution cost","","","","21","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Neural Reinforcement Learning Controllers for a Real Robot Application","R. Hafner; M. Riedmiller","Neuroinformatics, University of Osnabrück, Germany; Neuroinformatics, University of Osnabrück, Germany","Proceedings 2007 IEEE International Conference on Robotics and Automation","21 May 2007","2007","","","2098","2103","Accurate and fast control of wheel speeds in the presence of noise and nonlinearities is one of the crucial requirements for building fast mobile robots, as they are required in the MiddleSize League of RoboCup. We will describe, how highly effective speed controllers can be learned from scratch on the real robot directly. The use of our recently developed neural fitted Q iteration scheme allows reinforcement learning of neural controllers with only a limited amount of training data seen. In the described application, less than 5 minutes of interaction with the real robot were sufficient, to learn fast and accurate control to arbitrary target speeds.","1050-4729","1-4244-0601-3","10.1109/ROBOT.2007.363631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4209395","","Learning;Mobile robots;Wheels;DC motors;Control nonlinearities;Robotics and automation;Automatic control;Optimal control;Training data;Shape","control nonlinearities;iterative methods;learning (artificial intelligence);learning systems;mobile robots;multi-robot systems;neurocontrollers;velocity control","neural reinforcement learning controller;control nonlinearities;mobile robots;RoboCup;speed controller;neural fitted Q iteration","","27","1","13","IEEE","21 May 2007","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach to Lift Generation in Flapping MAVs: Experimental Results","M. Motamed; J. Yan","Motion Metrics International Corporation, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada","Proceedings 2007 IEEE International Conference on Robotics and Automation","21 May 2007","2007","","","748","754","We proposed an RL framework for control of flapping-wing MAVs (2006). The algorithm has been discussed and simulation results using a quasi-steady model showed initial promise. In this paper, the results from an experiment on a Drosophila-based dynamically scaled model are presented and are used to verify the control framework. Moreover, a comparison between a biological Drosophila melanogaster and the experimental results shows the actual possibility of employing the proposed approach to MAV control problem","1050-4729","1-4244-0601-3","10.1109/ROBOT.2007.363076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4209180","","Learning;Biological system modeling;Aerodynamics;Insects;Force measurement;Prototypes;Computational fluid dynamics;Solid modeling;Motion control;Robotics and automation","aerodynamics;aerospace robotics;learning (artificial intelligence);microrobots;mobile robots;robot dynamics","reinforcement learning;lift generation;flapping microaerial vehicles;quasisteady model;Drosophila-based dynamically scaled model;biological Drosophila melanogaster","","12","","27","IEEE","21 May 2007","","","IEEE","IEEE Conferences"
"Boosting Reinforcement Learning of Robotic Assembly Tasks by Constraining the Actionspace in a Task-Specific Manner","M. Braun; S. Wrede",NA; NA,"ISR Europe 2022; 54th International Symposium on Robotics","19 Aug 2022","2022","","","1","8","Autonomous learning of robotic assembly tasks is a promising approach for the future of industrial manufacturing. The Reinforcement Learning (RL) framework provides a possibility to autonomous learning based on interaction with the environment but although much research has been done, poor trial efficiency is a problem for learning-based methods. Learning robust strategies requires many costly interactions with the environment, which severely limits the potential applications in an industrial context. We propose a grey-box learning approach that allows process experts to provide a partial behavioral description based on the Task Frame Formalism. The potential to speed up the learning progress by restricting the action space in a task-specific manner is demonstrated. We evaluate how much trial efficiency is increased by comparing different variations of constraints in a simulated Peg-in-Hole task. Moreover, we show that our method enables learning how to skillfully assemble a light bulb under positional uncertainties with comparatively few real-world trials. This shows the potential to automate industrial assembly processes efficiently.","","978-3-8007-5891-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9861827","","","","","","","","","","19 Aug 2022","","","VDE","VDE Conferences"
"Single-Site Perishable Inventory Management Under Uncertainties: A Deep Reinforcement Learning Approach","K. Wang; C. Long; D. J. Ong; J. Zhang; X. -M. Yuan","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Singapore Institute of Manufacturing Technology, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Singapore Institute of Manufacturing Technology, Singapore","IEEE Transactions on Knowledge and Data Engineering","14 Sep 2023","2023","35","10","10807","10813","Online lot sizing for perishable materials in an uncertain environment is a fundamental problem for inventory planning and has been studied for several decades. In this article, we study a novel setting of the lot sizing problem, considering perishable materials, multiple suppliers, uncertain demands and lead time (LS-PMU), which captures the inventory planning task in real life better than existing lot sizing problems. We present theoretical results of the best possible competitive ratio an online algorithm can achieve for the LS-PMU problem. We then develop a reinforcement learning-based algorithm called RL4LS to intelligently choose the supplier and decide the order quantity in each time period. We conduct extensive experiments on both real and synthetic datasets to verify that RL4LS outperforms existing algorithms in terms of effectiveness and efficiency, e.g., RL4LS improves the effectiveness by 44% and runs two orders of magnitude faster than the state-of-the-art algorithm IBFA.","1558-2191","","10.1109/TKDE.2023.3250249","Ministry of Education - Singapore(grant numbers:Tier 1 Award RG77/21); A*STAR Cyber-Physical Production System(grant numbers:RIE2020 IAF-PP,A19C1a0018); Model Factory@SIMTech; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10056278","Deep reinforcement learning;inventory management;lot sizing;supply chain optimization","Costs;Planning;Lot sizing;Reinforcement learning;Task analysis;Decision making;Synthetic data","deep learning (artificial intelligence);inventory management;learning (artificial intelligence);lot sizing;reinforcement learning","calledRL4LSto;deep reinforcement learning approach;fundamental problem;inventory planning task;lead time;lot sizing problems;LS-PMU problem;multiple suppliers;online algorithm;perishable materials;possible competitive ratio;reinforcement learning-based;single-site perishable inventory management;thatRL4LSoutperforms;thelotsizing problem;time period;uncertain demands;uncertain environment","","","","27","IEEE","28 Feb 2023","","","IEEE","IEEE Journals"
"Natural Residual Reinforcement Learning for Bicycle Robot Control","X. Zhu; X. Zheng; Q. Zhang; Z. Chen; Y. Liu; B. Liang","Department of Mechanical and Electrical Engineering, Harbin Institute of Technology, Harbin, Heilongjiang Province, China; Tsinghua Shenzhen International Graduate School Tsinghua University, Shenzhen, China; Department of Mechanical and Electrical Engineering, Harbin Institute of Technology, Harbin, Heilongjiang Province, China; Department of Automation, Tsinghua University, Beijing, China; Department of Mechanical and Electrical Engineering, Harbin Institute of Technology, Harbin, Heilongjiang Province, China; Department of Automation, Tsinghua University, Beijing, China","2021 IEEE International Conference on Mechatronics and Automation (ICMA)","27 Aug 2021","2021","","","1201","1206","This work focuses on motion control of the bicycle robot by using the proposed NRRL algorithm. Unlike the traditional RL algorithm, decomposing the main tasks into subtasks manually and introducing qualitative prior knowledge to the agent have been applied in the NRRL algorithm. Simulation results show that better performance and better sample efficiency of the proposed NRRL algorithm have been achieved in terms of balance control and path tracking of bicycle robot. It's believed that the NRRL algorithm is available on the real physical bicycle robot, and the deployment of the algorithm will be realized soon, as the real physical bicycle robot has been constructed currently.","2152-744X","978-1-6654-4101-8","10.1109/ICMA52036.2021.9512587","National Natural Science Foundation of China(grant numbers:62073183,61903219); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9512587","NRRL algorithm;bicycle robot;balance control;path tracking","Mechatronics;Tracking;Simulation;Conferences;Robot control;Bicycles;Reinforcement learning","bicycles;learning (artificial intelligence);mobile robots;motion control;path planning;robot dynamics","balance control;NRRL algorithm;physical bicycle robot;natural residual reinforcement learning;bicycle robot control;motion control;traditional RL algorithm","","3","","17","IEEE","27 Aug 2021","","","IEEE","IEEE Conferences"
"Intelligent Mission Supervisor Design for Null-space-based Behavioral Control System: A Reinforcement Learning Approach","J. Huang; H. Mei; Z. Zhang","College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Artificial Intelligence, Fuzhou University, Fuzhou, China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","5861","5866","In this paper, the intelligent mission supervisor for null-space-based behavioral control (NS- BC) is developed. A novel reinforcement learning (RL) based approach is proposed to overcome the lack ability of the mission priority adjustment in the classic NS- BC methodologies. The traditional mission supervisor design of NSBC rely on human's ability to determine every individual mission's priority in advance. By elaborately combining the NSBC concept and the Deep Q Network (DQN) method, we develops a new mission management approach which can adjust the agent's behavior priorities dynamically. This approach is implemented without manual design rules and avoiding online calculations after offline training. In order to achieve better learning effect and faster network convergence performance, the experience replay and network structure are optimized by applying priority replay and dueling structure, respectively. Finally, simulation results show that the proposed reinforcement learning mission supervisor (RLMS) can provide better mission priority adjustment performance than the traditional mission supervisor using a much lower sampling frequency and much fewer numbers of dynamic mission priority adjustment.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9326520","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326520","Behavioral control;mission supervisor;reinforcement learning;deep Q network","Switches;Reinforcement learning;Neural networks;Jacobian matrices;Convergence;Training;Indexes","learning (artificial intelligence);multi-robot systems","reinforcement learning based approach;deep Q network method;mission management approach;NS- BC methodologies;reinforcement learning approach;null-space-based behavioral control system;intelligent mission supervisor design;dynamic mission priority adjustment;mission priority adjustment performance;reinforcement learning mission supervisor;priority replay;network structure;experience replay;learning effect;manual design rules","","2","","15","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Event-Triggered Optimal Formation Tracking Control Using Reinforcement Learning for Large-Scale UAV Systems","Z. Yan; L. Han; X. Li; J. Li; Z. Ren","Sino-French Engineer School, Beihang University, Beijing, China; Sino-French Engineer School, Beihang University, Beijing, China; Sino-French Engineer School, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","3233","3239","Large-scale UAV switching formation tracking control has been widely applied in many fields such as search and rescue, cooperative transportation, and UAV light shows. In order to optimize the control performance and reduce the computational burden of the system, this study proposes an event-triggered optimal formation tracking controller for discrete-time large-scale UAV systems (UASs). And an optimal decision - optimal control framework is completed by introducing the Hungarian algorithm and actor-critic neural networks (NNs) implementation. Finally, a large-scale mixed reality experimental platform is built to verify the effectiveness of the proposed algorithm, which includes large-scale virtual UAV nodes and limited physical UAV nodes. This compensates for the limitations of the experimental field and equipment in real-world scenario, ensures the experimental safety, significantly reduces the experimental cost, and is suitable for realizing large-scale UAV formation light shows.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160532","National Natural Science Foundation of China(grant numbers:61803014,61873011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160532","","Costs;Heuristic algorithms;Transportation;Optimal control;Mixed reality;Switches;Artificial neural networks","autonomous aerial vehicles;control engineering computing;multi-robot systems;neurocontrollers;optimal control;reinforcement learning","control performance;event-triggered optimal formation tracking control;event-triggered optimal formation tracking controller;large-scale mixed reality experimental platform;large-scale UAV formation light shows;large-scale UAV systems;large-scale virtual UAV nodes;optimal control framework;physical UAV nodes;UAV light shows","","","","25","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Development of a Simulation Environment for Robot Soccer Game with Deep Reinforcement Learning and Role Assignment","H. Zhong; H. Zhu; X. Li","Department of Automation, Tsinghua University, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, China; Department of Automation, Tsinghua University, China","2023 WRC Symposium on Advanced Robotics and Automation (WRC SARA)","27 Sep 2023","2023","","","213","218","The robot soccer game has been recognized as an excellent scenario to test the gaming algorithm of multi-agent systems. This paper develops a new simulation platform for the robot soccer game, and it has the advantage of open architecture, such that the formation control scheme, the path planning strategy for multiple robots, and many other algorithms can be implemented and tested. Specifically, both a Deep Reinforcement Learning (DRL) scheme and a role-assignment-based method have been successfully realized in this platform to drive multiple robots to play the soccer game, including 2V2,3V3,4V4, and so on. It is believed that the developed simulation environment can be used for data collection and transfer learning (TL), hence bridging the gap of Sim2Real technique in actual implementations.","2835-3358","979-8-3503-0732-0","10.1109/WRCSARA60131.2023.10261865","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261865","","Deep learning;Training;Transfer learning;Games;Reinforcement learning;Robustness;Path planning","deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;path planning;reinforcement learning","deep reinforcement learning;developed simulation environment;gaming algorithm;multiple robots;robot soccer game;role assignment;role-assignment-based method;simulation platform","","","","23","IEEE","27 Sep 2023","","","IEEE","IEEE Conferences"
"Robotic Disassembly Task Training and Skill Transfer Using Reinforcement Learning","M. Qu; Y. Wang; D. T. Pham","Department of Mechanical Engineering, University of Birmingham, Birmingham, U.K.; Department of Mechanical Engineering, University of Birmingham, Birmingham, U.K.; Department of Mechanical Engineering, University of Birmingham, Birmingham, U.K.","IEEE Transactions on Industrial Informatics","19 Sep 2023","2023","19","11","10934","10943","This article proposes a platform for robots to learn disassembly tasks based on reinforcement learning (RL) techniques. The platform is demonstrated by a robot learning the skill of removing a bolt along a door-chain groove in a data-driven way, where the clearance between the bolt and the groove is less than 1 mm. Furthermore, the relationship between the performance of the learned skills and the precision of the robot is studied with a method to control the robot's precision by adding uncorrelated zero-mean Gaussian noise to the robot's actions. Finally, the transferability of the learned skills among robots with different precisions is empirically studied. It has been found that skills learned by a low-precision robot can perform better on a robot with higher precision, and skills learned by a high-precision robot have worse performance on robots with lower precision.","1941-0050","","10.1109/TII.2023.3242831","Engineering and Physical Sciences Research Council(grant numbers:EP/N018524/1,EP/W00206X/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10038536","Machine learning;reinforcement learning (RL);remanufacturing;robotic disassembly","Robots;Task analysis;Robot kinematics;Training;Service robots;Reinforcement learning;Fasteners","assembling;control engineering computing;Gaussian noise;production engineering computing;reinforcement learning;robot programming;robotic assembly;training","bolt;door-chain groove;high-precision robot;low-precision robot;reinforcement learning techniques;RL techniques;robot precision control;robotic disassembly task training;skill transfer;zero-mean Gaussian noise","","","","33","IEEE","6 Feb 2023","","","IEEE","IEEE Journals"
"Driver Behavior Modeling via Inverse Reinforcement Learning Based on Particle Swarm Optimization","Z. -J. Liu; H. -N. Wu","The Science and Technology on Aircraft Control Laboratory, School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; The Science and Technology on Aircraft Control Laboratory, School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","7232","7237","In this paper, an inverse reinforcement learning method based on particle swarm optimization (PSO) is proposed to model driver's steering behavior. Initially, the vehicle dynamics is represented by a Takagi-Sugeno (T-S) fuzzy model which provides a method of approximating Q-function. Then the driver behavior model is described as an optimal control policy with decision-making model which illustrates the driving style. Subsequently, the Q-function is approximated by a quadratic polynomial-in-memberships form and the PSO algorithm is used to obtain the decision-making model from the driving data. And the corresponding optimal control policy is obtained by using the Q-learning policy iteration method. Finally, a numerical simulation is carried to show the effectiveness of the proposed method.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327174","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327174","Driver behavior modeling;T-S fuzzy model;inverse reinforcement learning;particle swarm optimization","Vehicles;Cost function;Vehicle dynamics;Optimal control;Mathematical model;TV;Safety","behavioural sciences computing;decision making;decision theory;fuzzy set theory;iterative methods;learning (artificial intelligence);optimal control;particle swarm optimisation;vehicle dynamics","driver behavior modeling;particle swarm optimization;inverse reinforcement learning method;fuzzy model;Q-function;driver behavior model;decision-making model;corresponding optimal control policy;Q-learning policy iteration method;driver steering behavior;PSO algorithm;vehicle dynamics;Takagi-Sugeno fuzzy model;T-S fuzzy model","","","","19","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Bridging Scenarios in Reinforcement Learning with Continuously Generated Relaying Predictive Models","K. Li; Q. -S. Jia","Department of Automation, Beijing National Research Center for Information Science and Technology (BN-Rist), Center for Intelligent and Networked System (CFINS), Tsinghua University, Beijing, P.R. China; Department of Automation, Beijing National Research Center for Information Science and Technology (BN-Rist), Center for Intelligent and Networked System (CFINS), Tsinghua University, Beijing, P.R. China","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","10","15","Transfer learning is an effective way to reduce expensive interactions with the physical environment in reinforcement learning (RL). Based on the correlation between scenarios, both the prior policy and historical experiences collected in the source domain may help to accelerate policy optimization in the target domain. However, without setting proper relaying scenarios, the discrepancy between domains may lead to sub-optimal policies or even negative transfer. In this paper, we firstly propose a continuously generated relaying predictive model (CRPM), which autonomously bridges the source domain and target domain with a series of gradually modi ed relaying scenarios. Then, we experimentally show that CRPM effectively reduces interactions required for policy optimization in the target domain. Besides, we combine CRPM with model-based RL, which further improves the performance. The CRPM also helps to improve the classical model-free algorithm by considering it as a particular case of transfer learning in the same domain. Experimental results show that CRPM helps to avoid sub-optimal policies and outperforms other algorithms in both the source and target scenarios.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926635","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926635","","Bridges;Q-learning;Transfer learning;Predictive models;Power system stability;Prediction algorithms;Stability analysis","reinforcement learning","reinforcement learning;continuously generated relaying predictive model;transfer learning;source domain;policy optimization;sub-optimal policies;negative transfer;CRPM;model-based RL;classical model-free algorithm;gradually modifed relaying scenarios","","","","31","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"An effective deep reinforcement learning approach for adaptive traffic signal control","M. Yu; J. Chai; Y. Lv; G. Xiong","School of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China; School of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","6419","6425","Intelligent traffic signal timing is critical to reduce traffic congestion and vehicle delay. Recent studies have shown promising results of deep reinforcement learning for traffic signal control. However, existing studies have only focused on selecting which direction (phase) to let vehicles go, not on phase duration. In this paper, we propose a deep reinforcement learning algorithm that automatically learns an optimal policy to adaptively determine phase duration. To improve algorithm performance and stability, we propose a phase sensitive neural network structure based on the deep deterministic policy gradient (DDPG) model, i.e. we design a deep neural network controller for each specific traffic signal phase with DDPG; we develop some interesting training techniques to improve training efficiency, i.e. dividing the training process into three stages and introducing the episode-break mechanism. We test the proposed methods on an isolated intersection under diverse traffic demands. Experiments show that our method is more effective.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327396","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327396","","Training;Mathematical model;Neural networks;Reinforcement learning;Vehicle dynamics;Stability analysis;Numerical stability","adaptive control;control system synthesis;deep learning (artificial intelligence);learning systems;neurocontrollers;road traffic control;stability","optimal policy learning;phase duration;stability;phase sensitive neural network structure;deep deterministic policy gradient model;DDPG;deep neural network controller;traffic signal phase;traffic demand;adaptive traffic signal control;intelligent traffic signal timing;traffic congestion reduction;vehicle delay;deep reinforcement learning algorithm;episode-break mechanism","","","","27","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Continuous Policy Multi-Agent Deep Reinforcement Learning with Generalizable Episodic Memory","W. Ni; B. Wang; H. Zhong; X. Guo","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; Beijing Aerospace Control Center, Beijing, China; China Certification & Inspection Group Beijing Co. Ltd, Beijing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","1699","1704","Multi-agent reinforcement learning (MARL) has been plagued by low sample efficiency. It needs far more samples than human learning to achieve convergence and learn successful strategies. And this situation is more serious in continuous state and policy space. Episodic memory (EM), as an effective method to improve the sample efficiency of reinforcement learning (RL) by imitating the ability of human rapid learning, has currently made little effort in continuous policy space and MARL. Therefore, we propose a continuous policy multi-agent reinforcement learning method with generalizable episodic memory (ECM). It establishes a centralized memory parameter network and memory buffer for each agent, and updates memory through implicit planning, so that the episodic memory model can use neural networks to learn successful strategies from the past successful experience. Thus, the model can adapt to the continuous policy space. Moreover, ECM combines MARL's idea of decentralized execution and centralized training (CTDE) with episodic memory model to make the model adapt to multi-agent task environment. Simulation results show that ECM method can effectively improve the sample efficiency of MARL algorithm, and the learned strategy has higher accuracy.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055953","multi-agent reinforcement learning;generalizable episodic memory;continuous policy;sample efficiency","Training;Deep learning;Adaptation models;Simulation;Memory management;Neural networks;Reinforcement learning","deep learning (artificial intelligence);multi-agent systems;reinforcement learning;storage management","centralized memory parameter network;continuous policy multiagent deep reinforcement learning;continuous policy multiagent reinforcement learning method;CTDE;decentralized execution and centralized training;ECM method;EM;episodic memory;episodic memory model;generalizable episodic memory;human rapid learning;learned strategy;MARL algorithm;memory buffer;multiagent task environment;neural networks;RL;updates memory","","","","14","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"OSSP-PTA: An Online Stochastic Stepping Policy for PTA on Reinforcement Learning","D. Niu; Y. Dong; Z. Jin; C. Zhang; Q. Li; C. Sun","School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; Super Scientific Software Laboratory, China University of Petroleum-Beijing, Beijing, China; LEADS, National Mobile Communications Research Laboratory, Frontiers Science Center for Mobile Information Communications and Security of MoE, Quantum Information Center, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Artificial Intelligence, Anhui University, Hefei, China","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","19 Oct 2023","2023","42","11","4310","4323","The dc analysis is essential and still quite challenging in large-scale nonlinear circuit simulation. Pseudo transient analysis (PTA) is a widely used and has great potential solver in the industry. However, the PTA convergence and simulation efficiency is still seriously affected by its stepping policy. This article proposes an online stochastic stepping policy (OSSP) for PTA based on deep reinforcement learning (DRL). To achieve better policy evaluation and stronger stepping exploration ability, the dual soft Actor–Critic agents work with the proposed valuation splitting and online momental scaling, enabling our OSSP to intelligently encode PTA iteration status and online further adjust forward and backward time-step size for unseen test circuits without human intervention and domain knowledge, trained solely by reinforcement learning from self-search. Our public sample buffer and priority sampling are also introduced to overcome the sparsity and imbalance of sample data. Numerical examples demonstrate that the proposed OSSP achieves a significant efficiency speedup (up to  $47.0\times $  less Newton–Raphson iterations) and convergence enhancement on unseen test circuits compared with the previous iter-based and switched evolution/relaxation-based stepping methods, in just one stepping iteration.","1937-4151","","10.1109/TCAD.2023.3251731","Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20202006,BK20211512); Key Program of the National Natural Science Foundation of China(grant numbers:62204265,62234010,62122020); National Key Research and Development Program of China(grant numbers:2022YFB4400400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10057469","DC analysis;momental scaling;pseudo transient analysis (PTA);stochastic stepping;valuation splitting","Convergence;Integrated circuit modeling;Mathematical models;Transient analysis;Steady-state;Cost accounting;Reinforcement learning","","","","","","45","IEEE","2 Mar 2023","","","IEEE","IEEE Journals"
"Devs Model Construction As A Reinforcement Learning Problem","I. David; E. Syriani","Department of Computer Science and Operations Research (DIRO), Université de Montréal 2920 Chemin de la Tour, Montreal, (Quebec), CANADA; Department of Computer Science and Operations Research (DIRO), Université de Montréal 2920 Chemin de la Tour, Montreal, (Quebec), CANADA","2022 Annual Modeling and Simulation Conference (ANNSIM)","23 Aug 2022","2022","","","30","41","Simulators are crucial components in many software-intensive systems, such as cyber-physical systems and digital twins. The inherent complexity of such systems renders the manual construction of simulators an error-prone and costly endeavor, and automation techniques are much sought after. However, current automation techniques are typically tailored to a particular system and cannot be easily transposed to other settings. In this paper, we propose an approach for the automated construction of simulators that can overcome this limitation, based on the inference of Discrete Event System Specifications (DEVS) models by reinforcement learning. Reinforcement learning allows inferring knowledge on the construction process of the simulator, instead of inferring the simulator itself. This, in turn, fosters reuse across different systems. DEVS further improves the reusability of this knowledge, as the vast majority of simulation formalisms can be efficiently translated to DEVS. We demonstrate the performance and generalizability of our approach on an illustrative example implemented in Python and Tensorforce.","","978-1-71-385288-9","10.23919/ANNSIM55834.2022.9859369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9859369","DEVS;reinforcement learning;machine learning;automation","Automation;Transfer learning;Reinforcement learning;Manuals;Markov processes;Human in the loop;Complexity theory","discrete event simulation;formal specification;reinforcement learning;software maintenance","cyber-physical systems;digital twins;discrete event system specification;construction process;devs model construction;reinforcement learning problem;software-intensive systems;DEVS model construction","","","","23","","23 Aug 2022","","","IEEE","IEEE Conferences"
"Smart Train Operation Algorithms Based on Expert Knowledge and Reinforcement Learning","K. Zhou; S. Song; A. Xue; K. You; H. Wu","Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Key Laboratory for IOT and Information Fusion Technology of Zhejiang, Institute of Information and Control, Hangzhou Dianzi University, Hangzhou, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","14 Jan 2022","2022","52","2","716","727","During decades, the automatic train operation (ATO) system has been gradually adopted in many subway systems for its low-cost and intelligence. This article proposes two smart train operation (STO) algorithms by integrating the expert knowledge with reinforcement learning algorithms. Compared with previous works, the proposed algorithms can realize the control of continuous action for the subway system and optimize multiple critical objectives without using an offline speed profile. First, through learning historical data of experienced subway drivers, we extract the expert knowledge rules and build inference methods to guarantee the riding comfort, the punctuality, and the safety of the subway system. Then we develop two algorithms for optimizing the energy efficiency of train operation. One is the STO algorithm based on deep deterministic policy gradient named (STOD) and the other is the STO algorithm based on normalized advantage function (STON). Finally, we verify the performance of proposed algorithms via some numerical simulations with the real field data from the Yizhuang Line of the Beijing Subway and illustrate that the developed STO algorithm are better than expert manual driving and existing ATO algorithms in terms of energy efficiency. Moreover, STOD and STON can adapt to different trip times and different resistance conditions.","2168-2232","","10.1109/TSMC.2020.3000073","Key Project of the National Natural Science Foundation of China(grant numbers:61936009); National Major Research Program(grant numbers:2018AAA0101604); Department of Automation, Tsinghua University, under the supervision of Prof. Shiji Song and Dr. Keyou You; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9144488","Expert knowledge;reinforcement learning;smart train operation (STO);subway","Public transportation;Resistance;Inference algorithms;Rail transportation;Mathematical model;Force;Safety","control engineering computing;deep learning (artificial intelligence);energy conservation;expert systems;gradient methods;inference mechanisms;intelligent transportation systems;rail traffic control;railway safety;railways;reinforcement learning;traffic engineering computing","automatic train operation system;reinforcement learning;subway drivers;expert knowledge rules;energy efficiency;Beijing Subway;ATO;smart train operation;inference methods;riding comfort;subway system safety;deep deterministic policy gradient;STOD;normalized advantage function;STON;numerical simulations;Yizhuang Line;trip times","","16","","38","IEEE","20 Jul 2020","","","IEEE","IEEE Journals"
"A sensor-based navigation for a mobile robot using fuzzy logic and reinforcement learning","Hee Rak Beom; Hyung Suck Cho","Laboratory for Control Systems and Automation, Department of Precision Engineering and Mechatronics, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Center for Robotics and Automation, Department of Precision Engineering and Mechatronics, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Systems, Man, and Cybernetics","6 Aug 2002","1995","25","3","464","477","The proposed navigator consists of an avoidance behavior and goal-seeking behavior. Two behaviors are independently designed at the design stage and then combined them by a behavior selector at the running stage. A behavior selector using a bistable switching function chooses a behavior at each action step so that the mobile robot can go for the goal position without colliding with obstacles. Fuzzy logic maps the input fuzzy sets representing the mobile robot's state space determined by sensor readings to the output fuzzy sets representing the mobile robot's action space. Fuzzy rule bases are built through the reinforcement learning which requires simple evaluation data rather than thousands of input-output training data. Since the fuzzy rules for each behavior are learned through a reinforcement learning method, the fuzzy rule bases can be easily constructed for more complex environments. In order to find the mobile robot's present state, ultrasonic sensors mounted at the mobile robot are used. The effectiveness of the proposed method is verified by a series of simulations.<>","2168-2909","","10.1109/21.364859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=364859","","Navigation;Mobile robots;Fuzzy logic;Learning;Path planning;Robot sensing systems;Orbital robotics;Fuzzy sets;Switches;Robotics and automation","mobile robots;path planning;navigation;fuzzy logic;fuzzy control;learning (artificial intelligence);state-space methods;knowledge based systems;ultrasonic transducers;fuzzy set theory","sensor-based navigation;mobile robot;fuzzy logic;reinforcement learning;goal-seeking behavior;avoidance behavior;bistable switching function;behavior selector;input fuzzy sets;state space;action space;fuzzy rule bases;ultrasonic sensors","","193","1","16","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Direct-reinforcement-adaptive-learning neural network control for nonlinear systems","Y. H. Kim; F. L. Lewis","Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA; Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA","Proceedings of the 1997 American Control Conference (Cat. No.97CH36041)","6 Aug 2002","1997","3","","1804","1808 vol.3","The paper is concerned with the application of reinforcement learning techniques to feedback control of nonlinear systems using neural networks (NN). Even if a good model of the nonlinear system is known, it is often difficult to formulate a control law. The work in this paper addresses this problem by showing how a NN can cope with nonlinearities through reinforcement learning with no preliminary off-line learning phase required. The learning is performed online based on a binary reinforcement signal from a critic without knowing the nonlinearity appearing in the system. The algorithm is derived from Lyapunov stability analysis, so that both system tracking stability and error convergence can be guaranteed in the closed-loop system.","0743-1619","0-7803-3832-4","10.1109/ACC.1997.610896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=610896","","Neural networks;Nonlinear control systems;Control systems;Nonlinear systems;Learning;Stability analysis;Robotics and automation;Electronic mail;Feedback control;Control nonlinearities","nonlinear control systems;adaptive control;neurocontrollers;learning (artificial intelligence);feedback;Lyapunov methods;closed loop systems;convergence;tracking","direct-reinforcement-adaptive-learning neural network control;nonlinear systems;feedback control;binary reinforcement signal;Lyapunov stability analysis;system tracking stability;error convergence;closed-loop system","","7","","17","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Effect of force load in hand reaching movement acquired by reinforcement learning","K. Shibata; K. Ito","Dept of Electr. & Electronic Eng., Oita Univ., Japan; Dept. of Camp. Intelli. & Sys. Sci., Tokyo Inst, of Tech., Japan","Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.","5 Jun 2003","2002","3","","1444","1448 vol.3","It has been known that when a human moves its hand to a target, the trajectory becomes almost a straight line from the start point to the target. When a viscosity force field is loaded to the hand unexpectedly, it is pulled toward the force direction once and then goes back to the target. However, after the learning in the force field, the trajectory becomes a straight line again, and when the force field is removed, it is pulled toward the opposite direction of the force that was loaded to the hand. This is called after-effect. In this paper, a neural network, whose inputs are visual sensory signals and state of manipulator, and whose outputs are joint torques, was trained by reinforcement learning. The effect of the first force field exposure and after-effect could be observed. This means that the system obtains inverse dynamics of its hand and environment in the neural network through reinforcement learning. Further, when the neural network learned with a random force at every trial, it became to control its hand based on feedback control rather than feedforward control.","","981-04-7524-1","10.1109/ICONIP.2002.1202859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1202859","","Learning;Force sensors;Neural networks;Force control;Humans;Trajectory;Viscosity;Manipulator dynamics;Feedforward neural networks;Force feedback","biomechanics;learning (artificial intelligence);neural nets;neurophysiology;feedback","hand reaching movement;reinforcement learning;viscosity force field;neural network;visual sensory signals;inverse dynamics;feedback","","3","","9","","5 Jun 2003","","","IEEE","IEEE Conferences"
"Accelerated reinforcement learning control using modified CMAC neural networks","Xin Xu; Dewen Hu; Han-gen He","Department of Automatic Control, National University of Defense Technology, Changsha, China; Department of Automatic Control, National University of Defense Technology, Changsha, China; Department of Automatic Control, National University of Defense Technology, Changsha, China","Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.","5 Jun 2003","2002","5","","2575","2578 vol.5","Reinforcement learning is a class of model-free learning control methods that can solve Markov decision problems. One difficulty for the application of reinforcement learning control is its slow convergence, especially in MDPs with continuous state space. In this paper, a modified structure of CMAC neural networks is proposed to accelerate reinforcement learning control. The modified structure is designed by incorporating a priori knowledge of learning control problems so that the efficiency and generalization ability of reinforcement learning can be improved. Simulation results on the cart-pole balancing problem illustrate the effectiveness of the proposed method.","","981-04-7524-1","10.1109/ICONIP.2002.1201960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201960","","Acceleration;Learning;Neural networks;State-space methods;Automatic control;Convergence;Helium;Space technology;Control engineering;Operations research","cerebellar model arithmetic computers;learning (artificial intelligence);state-space methods;Markov processes;decision theory;neurocontrollers;generalisation (artificial intelligence)","accelerated reinforcement learning control;modified CMAC neural networks;model-free learning control;Markov decision problem;slow convergence;continuous state space;a priori knowledge;generalization ability;cart-pole balancing problem;inverted pendulum;probability distribution;optimal policy","","3","","9","","5 Jun 2003","","","IEEE","IEEE Conferences"
"A novel artificial neural network trained using evolutionary algorithms for reinforcement learning","A. Reddipogu; G. Maxwell; C. MacLeod; M. Simpson","Schoolhill, Robert Gordon University, Aberdeen, UK; Schoolhill, Robert Gordon University, Aberdeen, UK; Schoolhill, Robert Gordon University, Aberdeen, UK; Schoolhill, Robert Gordon University, Aberdeen, UK","Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.","21 May 2003","2002","4","","1946","1950 vol.4","This paper discusses the development of a novel pattern recognition system using artificial neural networks (ANNs) and evolutionary algorithms for reinforcement learning (EARL). The network is based on neuronal interactions involved in identification of prey and predator in toads. The distributed neural network (DNN) is capable of recognizing and classifying various features. The lateral inhibition between the output neurons helps the network in the classification process - similar to the gate in gating network. The results obtained are compared with standard neural network architectures and learning algorithms.","","981-04-7524-1","10.1109/ICONIP.2002.1199013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199013","","Artificial neural networks;Evolutionary computation;Learning;Neurons;Biological neural networks;Visual system;Computer vision;Shape;Computer architecture;Voting","neural net architecture;evolutionary computation;learning (artificial intelligence);pattern recognition;predator-prey systems;multilayer perceptrons","novel artificial neural network;evolutionary algorithms;reinforcement learning;pattern recognition system;neuronal interactions;prey;predator;toads;distributed neural network;lateral inhibition;classification;neural network architectures","","3","","14","","21 May 2003","","","IEEE","IEEE Conferences"
"Application of direct-vision-based reinforcement learning to a real mobile robot","M. Iida; M. Sugisaka; K. Shibata","Department of Electrical & Electronic Engineering, Oita University, Japan; Department of Electrical & Electronic Engineering, Oita University, Japan; Department of Electrical & Electronic Engineering, Oita University, Japan","Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.","5 Jun 2003","2002","5","","2556","2560 vol.5","In this paper, it was confirmed that a real mobile robot with a simple visual sensor could learn appropriate actions to reach a target by Direct-Vision-Based reinforcement learning (RL). In Direct-Vision-Based RL, raw visual sensory signals are put into a layered neural network directly, an the neural network is trained by Back Propagation using the training signal that is generated based on reinforcement learning. Considering the time delay to get the visual sensory signals, it was proposed that the actor outputs are trained using the critic output at two time steps ahead. It was shown that the robot with a monochrome visual sensor could obtain reaching actions to a target object through the learning from scratch without any advance knowledge and any help of humans.","","981-04-7524-1","10.1109/ICONIP.2002.1201956","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201956","","Learning;Mobile robots;Robot sensing systems;Neural networks;Robot kinematics;Intelligent robots;Orbital robotics;State-space methods;Signal generators;Humans","mobile robots;robot vision;learning (artificial intelligence);backpropagation;neural nets;intelligent robots","direct-vision-based reinforcement learning;real mobile robot;simple visual sensor;raw visual sensory signals;layered neural network;backpropagation;time delay;actor outputs;critic output;monochrome sensor;autonomous robots;robot intelligence","","1","","9","","5 Jun 2003","","","IEEE","IEEE Conferences"
"Anticipative reinforcement learning","F. Maire","Smart Devices Laboratory, School of Computing Science and Software Engineering, Queensland University of Technology, Brisbane, QLD, Australia","Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.","5 Jun 2003","2002","3","","1428","1432 vol.3","This paper introduces anticipative reinforcement learning (ARL), a method that addresses the problem of the breakdown of value based algorithms for problems with small time steps and continuous action and state spaces when the algorithms are implemented with neural networks. In ARL, an agent is made of three components; the actor, the critic and the model (the model is as in Dyna but we use it differently). The main originality of ARL lies in the action selection process; the agent builds a set of candidate actions that includes the action recommended by the actor plus some random actions. Once the set of candidate actions is built, the candidate actions are ranked by considering what would happen if these actions were taken and followed by a sequence of actions using only the current policy (anticipation using iteratively the model with a finite look-ahead). We demonstrate the benefits of looking ahead with experiments on a Khepera robot.","","981-04-7524-1","10.1109/ICONIP.2002.1202856","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1202856","","Learning;Australia;Books;Laboratories;Software engineering;Space technology;Electric breakdown;State-space methods;Software algorithms;Robots","learning (artificial intelligence);neural nets;function approximation;generalisation (artificial intelligence);state-space methods;mobile robots","anticipative reinforcement learning;state spaces;neural networks;Khepera robot;function approximation;generalisation","","1","","16","","5 Jun 2003","","","IEEE","IEEE Conferences"
"Reinforcement learning based on a statistical value function and its application to a board game","I. Nishikawa; T. Nakanishi","Department of Computer Science, Ritsumeikan University, Shiga, Japan; Department of Computer Science, Ritsumeikan University, Shiga, Japan","Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.","5 Jun 2003","2002","3","","1449","1453 vol.3","A statistical method for reinforcement learning is proposed to cope with a large number of discrete states. As a coarse-graining of a large number of states, less number of sets of states are defined as a group of neighbouring states. State sets partly overlap, and one state is included in a multiple sets. The learning is based on an action-value function for each state set, and an action-value function on an individual state is derived by a statistical average of multiple value functions on state sets. The proposed method is applied to a board game Dots-and-Boxes. Simulations show a successful learning through the training games competing with a mini-max method of the search depth 2 to 5, and the winning rate against a depth-3 mini-max attains about 80%. An action-value function derived by a weighted average with the weight given by the variance of rewards shows the advantage compared with the one derived by a simple average.","","981-04-7524-1","10.1109/ICONIP.2002.1202860","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1202860","","Learning;Application software;Statistical analysis;State-space methods;Computer science;Computational efficiency;Bellows","games of skill;learning (artificial intelligence);search problems;statistical analysis;minimax techniques","reinforcement learning;statistical value function;board game;statistical method;action-value function;minimax method;search depth","","","","3","","5 Jun 2003","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Assisted Federated Learning Algorithm for Data Management of IIoT","P. Zhang; C. Wang; C. Jiang; Z. Han","College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; National Research Center for Information Science and Technology, Tsinghua University and the Tsinghua Space Center, Beijing, China; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA","IEEE Transactions on Industrial Informatics","26 Aug 2021","2021","17","12","8475","8484","The continuous expanded scale of the industrial Internet of Things (IIoT) leads to IIoT equipments generating massive amounts of user data every moment. According to the different requirement of end users, these data usually have high heterogeneity and privacy, while most of users are reluctant to expose them to the public view. How to manage these time series data in an efficient and safe way in the field of IIoT is still an open issue, such that it has attracted extensive attention from academia and industry. As a new machine learning paradigm, federated learning (FL) has great advantages in training heterogeneous and private data. This article studies the FL technology applications to manage IIoT equipment data in wireless network environments. In order to increase the model aggregation rate and reduce communication costs, we apply deep reinforcement learning (DRL) to IIoT equipment selection process, specifically to select those IIoT equipment nodes with accurate models. Therefore, we propose a FL algorithm assisted by DRL, which can take into account the privacy and efficiency of data training of IIoT equipment. By analyzing the data characteristics of IIoT equipments, we use MNIST, fashion MNIST, and CIFAR-10 datasets to represent the data generated by IIoT. During the experiment, we employ the deep neural network model to train the data, and experimental results show that the accuracy can reach more than 97%, which corroborates the effectiveness of the proposed algorithm.","1941-0050","","10.1109/TII.2021.3064351","National Key Research and Development Program of China(grant numbers:2020YFB1804800); National Natural Science Foundation of China(grant numbers:61922050); Natural Science Foundation of Shandong Province(grant numbers:ZR2020MF006); NSF(grant numbers:EARS-1839818,CNS1717454,CNS-1731424,CNS-1702850); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9372789","Data training;deep reinforcement learning (DRL);federated learning (FL);industrial Internet of Things (IIoT);IIoT equipment","Informatics","data privacy;deep learning (artificial intelligence);Internet of Things;production engineering computing;time series","deep reinforcement learning assisted federated learning algorithm;data management;continuous expanded scale;time series data;machine learning;private data;FL technology applications;IIoT equipment data;IIoT equipment selection process;IIoT equipment nodes;FL algorithm;data characteristics;deep neural network model;data training privacy;DRL;MNIST;industrial Internet of Things","","81","","36","IEEE","8 Mar 2021","","","IEEE","IEEE Journals"
"Online Recursive Power Management Strategy Based on the Reinforcement Learning Algorithm With Cosine Similarity and a Forgetting Factor","X. Lin; B. Zhou; Y. Xia","School of Mechanical Engineering and Automation, Fuzhou University, Fuzhou, China; School of Mechanical Engineering and Automation, Fuzhou University, Fuzhou, China; School of Mechanical Engineering and Automation, Fuzhou University, Fuzhou, China","IEEE Transactions on Industrial Electronics","18 Feb 2021","2021","68","6","5013","5023","In this article, an online recursive power management strategy based on the reinforcement learning technique is proposed to achieve an optimal power distribution and excellent economic performance of a plug-in fuel cell hybrid electric vehicle. First, the optimal control algorithm of the power management strategy is formulated. Then, an online recursive algorithm using cosine similarity and a forgetting factor to achieve the adaptability of the proposed strategy to various driving conditions is formulated. The parameters updating framework is triggered once the change rate of cosine similarity exceeds the threshold value of the corresponding driving cycle. Some of the important parameters such as the learning rate, discount factor, forgetting factor, and the change rate of cosine similarity are analyzed to optimize the online updating capability of the strategy. Finally, an experimental study is conducted to validate the effectiveness of the proposed strategy. Both a rule-based strategy and an equivalent consumption minimization strategy are implemented to establish the benchmark framework used to analyze the performance of the proposed strategy. The simulation and experimental results corroborate the effectiveness and economic performance of the proposed strategy. A comparison with the existing standard benchmark strategy indicates the superior performance of the developed strategy reaching a remarkable reduction in energy consumption at a long driving distance.","1557-9948","","10.1109/TIE.2020.2988189","National Natural Science Foundation of China(grant numbers:51505086); Opening Foundation of Key Laboratory of Advanced Manufacture Technology for Automobile Parts, Ministry of Education(grant numbers:2019KLMT06); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9075414","Energy management;forgetting factor;fuel cell vehicle;online updating","Energy management;Batteries;Fuel cells;State of charge;Hybrid electric vehicles;Power demand;Optimization","fuel cell vehicles;hybrid electric vehicles;learning (artificial intelligence);optimal control","online recursive power management strategy;reinforcement learning algorithm;cosine similarity;forgetting factor;reinforcement learning technique;optimal power distribution;plug-in fuel cell hybrid electric vehicle;optimal control algorithm;online recursive algorithm;change rate;learning rate;online updating capability;rule-based strategy;equivalent consumption minimization strategy;existing standard benchmark strategy","","23","","29","IEEE","21 Apr 2020","","","IEEE","IEEE Journals"
"Intelligent Energy Management Strategy Based on an Improved Reinforcement Learning Algorithm With Exploration Factor for a Plug-in PHEV","X. Lin; K. Zhou; L. Mo; H. Li","College of Mechanical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Mechanical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Mechanical Engineering and Automation, Fuzhou University, Fuzhou, China; Department of Mechanical and Aerospace Engineering, West Virginia University, Morgantown, WV, USA","IEEE Transactions on Intelligent Transportation Systems","11 Jul 2022","2022","23","7","8725","8735","An intelligent energy management strategy (EMS) based on an improved Reinforcement Learning (RL) algorithm is developed to enhance the adaptability of the EMS and to further improve the fuel efficiency of a Plug-in Parallel Hybrid Electric Vehicle (PHEV). Both the numerical model and the energy management strategy of a plug-in PHEV are described. The improved RL with Q-learning algorithm is implemented to acquire the optimal control strategies for improving fuel economy. The Markov Chain is employed to calculate the Transition Probability Matrix of the required power. A Kullback-Leibler (KL) divergence rate is designed to activate the update of EMS, when a new corresponding driving cycle is expected. An Exploration Factor (EF) is proposed to overcome the disadvantages of the normal RL algorithm in convergence rate and reward cost evaluation. The diverse KL divergence rates are examined to seek optimal solutions. The normal-RL strategy, rule-based strategy, and dynamic programming strategy are implemented as benchmark strategies to verify the effectiveness of the proposed strategy. The validation results indicate that the improved RL algorithm with EF makes it possible to promote the EMS capable of significantly improving the energy efficiency of a plug-in PHEV.","1558-0016","","10.1109/TITS.2021.3085710","Natural Science Foundation of Fujian Province, China(grant numbers:2020J01449); National Natural Science Foundation of China(grant numbers:51505086); Opening Foundation of Key Laboratory of Advanced Manufacture Technology for Automobile Parts, Ministry of Education(grant numbers:2019KLMT06); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9457142","Hybrid electric vehicle;energy management strategy;reinforcement learning algorithm;self-adaptive exploration factor","Energy management;Engines;State of charge;Batteries;Torque;Real-time systems;Hybrid electric vehicles","control engineering computing;convergence of numerical methods;dynamic programming;energy conservation;energy management systems;fuel economy;hybrid electric vehicles;knowledge based systems;Markov processes;optimal control;power engineering computing;probability;reinforcement learning","energy efficiency;intelligent energy management strategy;reinforcement learning;exploration factor;EMS;Q-learning;optimal control;Kullback-Leibler divergence rate;convergence rate;reward cost evaluation;KL divergence rates;normal-RL strategy;rule-based strategy;dynamic programming;plug-in PHEV;fuel efficiency;plug-in parallel hybrid electric vehicle;numerical model;fuel economy;Markov chain;transition probability matrix;driving cycle","","8","","40","IEEE","16 Jun 2021","","","IEEE","IEEE Journals"
"Geometric-Feature Representation Based Pre-Training Method for Reinforcement Learning of Peg-in-Hole Tasks","Y. Zang; P. Wang; F. Zha; W. Guo; S. Ruan; L. Sun","School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, China; School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, China; School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, China; School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, China; School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, China; School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, China","IEEE Robotics and Automation Letters","28 Apr 2023","2023","8","6","3478","3485","Recently, reinforcement learning (RL) is often used for learning the strategy of peg-in-hole tasks. However, traditional state representation of PiH RL might be either redundant or abstract, which leads to unnecessary learning steps and incompatibility with mathematical training optimization. To issue these problems, a geometric-feature (GF) state representation method for peg-in-hole DDPG (Deep Deterministic Policy Gradient) RL is proposed to get rid of the redundant states. Also, a network pre-training method for peg-in-hole DDPG based on GF representation is provided to help DDPG acquire basic assembly skills before training. Finally, we executed the simulation experiments in the environment with Open AI Gym + Pybullet to test the performance of the proposed GF representation based pre-training method. We also executed the lenrned strategy on the real-world Panda robot.","2377-3766","","10.1109/LRA.2023.3261759","National Natural Science Foundation of China(grant numbers:U2013602,52075115,51521003,61911530250); National Key R&D Program of China(grant numbers:2020YFB13134); Self-Planned Task of the State Key Laboratory of Robotics and System(grant numbers:SKLRS202001B,SKLRS202110B); Shenzhen Science and Technology Research and Development Foundation(grant numbers:JCYJ20190813171009236); Basic Research on Free Exploration of Shenzhen Virtual University Park(grant numbers:2021Szvup085); Basic Scientific Research of Technology(grant numbers:JCKY2020603C009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10081033","Reinforcement learning;contact modeling;task planning;assembly","Task analysis;Codes;Axles;Robot sensing systems;Encoding;Decoding;Robots","control engineering computing;deep learning (artificial intelligence);gradient methods;learning (artificial intelligence);reinforcement learning;robot programming","Deep Deterministic Policy Gradient;geometric-feature representation;geometric-feature state representation method;GF representation;mathematical training optimization;network pre-training method;peg-in-hole DDPG RL;peg-in-hole tasks;PiH RL;redundant states;reinforcement learning;traditional state representation;unnecessary learning steps","","1","","19","IEEE","24 Mar 2023","","","IEEE","IEEE Journals"
"Towards Pick and Place Multi Robot Coordination Using Multi-agent Deep Reinforcement Learning","X. Lan; Y. Qiao; B. Lee","Software Research Institute, Athlone Institute of Technology, Athlone, Ireland; Software Research Institute, Athlone Institute of Technology, Athlone, Ireland; Software Research Institute, Athlone Institute of Technology, Athlone, Ireland","2021 7th International Conference on Automation, Robotics and Applications (ICARA)","17 Mar 2021","2021","","","85","89","Recent advances in deep reinforcement learning are enabling the creation and use of powerful multi-agent systems in complex areas such as multi-robot coordination. These show great promise to help solve many of the difficult challenges of rapidly growing domains such as smart manufacturing. In this position paper we describe our early-stage work on the use of multi-agent deep reinforcement learning to optimise coordination in a multi-robot pick and place system. Our goal is to evaluate the feasibility of this new approach in a manufacturing environment. We propose to adopt a decentralised partially observable Markov Decision Process approach and to extend an existing cooperative game work to suitably formulate the problem as a multiagent system. We describe the centralised training/decentralised execution multi-agent learning approach which allows a group of agents to be trained simultaneously but to exercise decentralised control based on their local observations. We identify potential learning algorithms and architectures that we will investigate as a base for our implementation and we outline our open research questions. Finally we identify next steps in our research program.","","978-1-6654-0469-3","10.1109/ICARA51699.2021.9376433","Science Foundation Ireland (SFI)(grant numbers:SFI/16/RC/3918); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376433","multi-agent system;pick and place;deep reinforcement learning;multi-robot system;Dec-POMDP","Robot kinematics;Reinforcement learning;Games;Markov processes;Optimization;Multi-agent systems;Smart manufacturing","decentralised control;game theory;Markov processes;multi-agent systems;multi-robot systems;neurocontrollers","multiagent deep reinforcement learning;multiagent systems;pick and place multirobot coordination;decentralised partially observable Markov Decision process;cooperative game","","3","","20","IEEE","17 Mar 2021","","","IEEE","IEEE Conferences"
"Reinforcement-Learning-Based Signal Integrity Optimization and Analysis of a Scalable 3-D X-Point Array Structure","K. Son; M. Kim; H. Park; D. Lho; K. Son; K. Kim; S. Lee; S. Jeong; S. Park; S. Hong; G. Park; J. Kim","TeraByte Interconnection and Package Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; TeraByte Interconnection and Package Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; TeraByte Interconnection and Package Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; TeraByte Interconnection and Package Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; TeraByte Interconnection and Package Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; TeraByte Interconnection and Package Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; TeraByte Interconnection and Package Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; TeraByte Interconnection and Package Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; TeraByte Interconnection and Package Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; TeraByte Interconnection and Package Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; TeraByte Interconnection and Package Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; TeraByte Interconnection and Package Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Components, Packaging and Manufacturing Technology","17 Jan 2022","2022","12","1","100","110","In this article, we, for the first time, propose a reinforcement learning (RL) model to design an optimal 3-D cross-point (X-Point) array structure considering signal integrity issues. The interconnection design problem is modeled to the Markov decision process (MDP). The proposed RL model designs the 3-D X-Point array structure based on three reward factors: the number of bits, the crosstalk, and the IR drop. We applied multilayer perceptron (MLP) and long short-term memory (LSTM) to parameterize the policy. Proximal policy optimization (PPO) is used to optimize the parameters to train the policy. The reward of the proposed RL model is well-converged with variations in the array structure size and hyperparameters of the reward factors. We verified the scalability and sensitivity of the proposed RL model. With the optimal 3-D X-Point array structure design, we analyzed the reward factor and signal integrity issues. The optimal design of the 3-D X-Point array structure shows 17%–26.5% better signal integrity performance than the conventional design in finer process technology. In addition, we suggest a range of possible directions for improvement of the proposed model with variations in MDP tuples, reward factors, and learning algorithms, among other factors. Using the proposed model, we can easily design an optimal 3-D X-Point array structure with a certain size, performance capabilities, and specifications based on reward factors and hyperparameters.","2156-3985","","10.1109/TCPMT.2021.3129502","National Research Foundation of Korea funded by the Korean Government(grant numbers:NRF-2019M3F3A1A03079612,NRF-2020M3F3A2A01081587); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622276","3-D cross-point (X-Point) array structure;crosstalk;interconnection;IR drop;long short-term memory (LSTM);proximal policy optimization (PPO);reinforcement learning (RL);signal integrity","Three-dimensional displays;Integrated circuit interconnections;Crosstalk;Signal integrity;Solid modeling;Optimization;Manufacturing","electronic engineering computing;integrated circuit design;integrated circuit interconnections;learning (artificial intelligence);Markov processes;multilayer perceptrons;neural nets;optimisation;three-dimensional integrated circuits","reward factor;proximal policy optimization;RL model;array structure size;X-Point array structure design;reinforcement-learning-based signal integrity optimization;interconnection design problem;scalable 3D X-Point Array Structure;Markov decision process;multilayer perceptron;long short-term memory","","3","","24","IEEE","19 Nov 2021","","","IEEE","IEEE Journals"
"Introduction to UAV swarm utilization for communication on the move terminals tracking evaluation with reinforcement learning technique","S. Omi; H. -S. Shin; A. Tsourdos; J. Espeland; A. Buchi","The School of Aerospace, Transport and Manufacturing, Cranfield University, Cranfield, U.K.; The School of Aerospace, Transport and Manufacturing, Cranfield University, Cranfield, U.K.; The School of Aerospace, Transport and Manufacturing, Cranfield University, Cranfield, U.K.; QuadSAT, Odense, Denmark; QuadSAT, Odense, Denmark","2021 15th European Conference on Antennas and Propagation (EuCAP)","27 Apr 2021","2021","","","1","5","As the growth of communication and satellite industry, the demand of satellite antenna evaluation is increasing. Particularly Communication On The Move (COTM) terminal antenna, including electronically steerable antennas (ESA) and for the communication between new constellations on LEO and MEO, requires tracking accuracy test for the communication on moving vehicles. The measurement capability of conventional methodologies have been limited due to their location fixed facilities and non-adjustable sensor’s positions during the measurement. To overcome this drawbacks, we will present how multi-agent system of UAVs could be utilized for COTM tracking accuracy evaluation. This measurement needs instant actions for UAVs to keep them navigating in order to achieve accurate and stable measurement. Reinforcement learning (RL) techniques are investigated for this purpose in this paper. The performance improvement is demonstrated with the system using RL technique to adjust UAVs with sensors during the measurement.","","978-88-31299-02-2","10.23919/EuCAP51087.2021.9411153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9411153","Communication On The Move;de-pointing;antenna measurements;UAV;multi-agent reinforcement learning","Antenna measurements;Training;Satellite antennas;Satellites;Tracking;Navigation;Reinforcement learning","autonomous aerial vehicles;learning (artificial intelligence);location based services;multi-agent systems;multi-robot systems;satellite antennas;vehicular ad hoc networks","UAV swarm utilization;reinforcement learning;satellite antenna evaluation;electronically steerable antennas;location fixed facilities;multiagent system;UAV navigation;COTM tracking accuracy evaluation;communication on the move terminal antenna;COTM terminal antenna","","1","","22","","27 Apr 2021","","","IEEE","IEEE Conferences"
"Anti-Adaptive Harmful Birds Repelling Method Based on Reinforcement Learning Approach","C. W. Lee; A. Muminov; M. -C. Ko; H. -J. Oh; J. D. Lee; Y. -A. Kwon; D. Na; S. -P. Heo; H. S. Jeon","Department of Smart Farm Application Research, Electronics and Telecommunications Research Institute, Daegu, Republic of Korea; Department of Software, College of Science Technology, Konkuk University, Chungju, Republic of Korea; Department of Software, College of Science Technology, Konkuk University, Chungju, Republic of Korea; Department of Computer Information, Yeungnam University College, Daegu, Republic of Korea; Department of Multimedia Engineering, Gangneung-Wonju National University, Wonju, Republic of Korea; Institute for Innovative Education, Konkuk University, Chungju, Republic of Korea; School of Global Leadership, Handong Global University, Pohang, Republic of Korea; Department of Information and Communication Engineering, Gangneung-Wonju National University, Wonju, Republic of Korea; Department of Software, College of Science Technology, Konkuk University, Chungju, Republic of Korea","IEEE Access","23 Apr 2021","2021","9","","60553","60563","To prevent crop damage from harmful birds, various repelling methods have been studied. However, harmful birds are still causing damage in the orchard by adapting to the repelling device according to their biological characteristics. This paper proposes a method called Anti-adaptive Harmful Birds Repelling (AHBR) that uses the model-free learning idea of the Reinforcement Learning (RL) approach to repell harmful birds that can effectively prevent bird adaptation problems. To prevent adaptation, the AHBR method uses a method of learning the bird’s reaction to the available threat sounds and playing them in patterns that are difficult to adapt through the RL approach. We also proposed the Long-term and Short-term (LaS) policy to meet the Markov assumptions that make RL difficult to implement in the real world. The LaS policy enable learning of the actual bird’s reaction to the sound of a threat. The performance of the AHBR method was evaluated in a closed environment to experiment real harmful bird such as Brown-eared Bulbul, Great Tit, and Eurasian Magpie captured in orchards. Results obtained from the experiment showed that the AHBR method was on average 43.5% better than the threat sound patterns(One, Sequential, Reverse Sequential, Random) used in commercial products.","2169-3536","","10.1109/ACCESS.2021.3073205","Intelligent Software Convergence Research Institute in Konkuk University; Electronics and Telecommunications Research Institute (ETRI) Grant through the Korean Government (Regional Industry ICT Convergence Technology Advancement and Support Project in Daegu-GyeongBuk)(grant numbers:21ZD1100); KIAT Grant funded by the Korean Government (The Establishment Project of Industry-University Fusion District and Ministry of Trade, industry and Energy)(grant numbers:N/P0011930); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9404183","Agricultural engineering;machine learning;intelligent systems;automation;Anti-adaptive repeller","Birds;Wind;Software;Markov processes;Animal behavior;Telecommunications","agricultural engineering;crops;learning (artificial intelligence);Markov processes","orchards;Eurasian Magpie bird;Brown-eared Bulbul bird;Great Tit bird;Markov assumptions;biological characteristics;crop damage;anti-adaptive harmful birds repelling;bird adaptation;RL approach;model-free learning idea;reinforcement learning;AHBR method","","","","22","CCBY","14 Apr 2021","","","IEEE","IEEE Journals"
"Adaptive Stepping PTA for DC Analysis Based on Reinforcement Learning","Y. Dong; D. Niu; Z. Jin; C. Zhang; Q. Li; C. Sun","School of Automation, Southeast University, Nanjing, China; School of Automation and the Key Laboratory of Measurement and Control of CSE, Ministry of Education, Southeast University, Nanjing, China; Super Scientific Software Laboratory, China University of Petroleum (Beijing), Beijing, China; LEADS and the National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation and the Key Laboratory of Measurement and Control of CSE, Ministry of Education, Southeast University, Nanjing, China","IEEE Transactions on Circuits and Systems II: Express Briefs","21 Dec 2022","2023","70","1","266","270","Solving the DC operating point efficiently for large-scale nonlinear circuit is crucial and quite challenging. Pseudo transient analysis (PTA) is a widely-used and promising DC solver in the industry, in which the stepping policy is of great importance for PTA convergence and simulation efficiency. In this brief, a reinforcement learning (RL)-enhanced stepping policy is proposed. It designs dual Actor-Critic agents with stochastic policy and online adaptive scaling to intelligently evaluate PTA convergence status, and adaptively adjust forward and backward time-step size. Numerical examples demonstrate that a significant efficiency speedup and convergence improvement over the previous stepping methods is achieved by the proposed RL-enhanced stepping policy.","1558-3791","","10.1109/TCSII.2022.3207356","Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20202006); Commercialization of Scientific and Research findings of Jiangsu Province(grant numbers:BA2021012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893885","Circuit simulation;DC analysis;pseudo transient analysis;reinforcement learning","Convergence;Steady-state;Integrated circuit modeling;Reinforcement learning;Iterative methods;Training;Mathematical models","nonlinear network analysis;reinforcement learning;stochastic processes;transient analysis","adaptive stepping PTA;convergence improvement;DC analysis;DC operating point;DC solver;dual actor-critic agents;large-scale nonlinear circuit;online adaptive scaling;previous stepping methods;pseudotransient analysis;PTA convergence status;reinforcement learning-enhanced stepping policy;RL-enhanced stepping policy;significant efficiency speedup;simulation efficiency;stochastic policy;time-step size","","1","","26","IEEE","16 Sep 2022","","","IEEE","IEEE Journals"
"Safe Reinforcement Learning and Adaptive Optimal Control With Applications to Obstacle Avoidance Problem","K. Wang; C. Mu; Z. Ni; D. Liu","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Department of Electrical Engineering and Computer Science, Florida Atlantic University, Boca Raton, FL, USA; School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, Shenzhen, China","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","14","This paper presents a novel composite obstacle avoidance control method to generate safe motion trajectories for autonomous systems in an adaptive manner. First, system safety is described using forward invariance, and the barrier function is encoded into the cost function such that the obstacle avoidance problem can be characterized by an infinite-horizon optimal control problem. Next, a safe reinforcement learning framework is proposed by combining model-based policy iteration and state-following-based approximation. Upon real-time data and extrapolated experience data, this learning design is implemented through the actor-critic structure, in which critic networks are tuned by gradient-descent adaption and actor networks produce adaptive control policies via gradient projection. Then, system stability and weight convergence are theoretically analyzed using Lyapunov method. Finally, the proposed learning-based controller is demonstrated on a two-dimensional single integrator system and a nonlinear unicycle kinematic system. Simulation results reveal that the system or agent can smoothly reach the target point while keeping a safe distance from each obstacle; at the same time, other three avoidance control methods are used to provide side-by-side comparisons and to verify some claimed advantages of the present method. Note to Practitioners—This paper is motivated by the obstacle avoidance problem of real-time navigation of an agent to the target point, which applies to practical autonomous systems such as vehicles and robots. Pre-generative methods and reactive methods have been widely employed to generate safe motion trajectories in the obstacle environment. However, these methods cannot strike a good balance between safety and optimality. In this paper, the obstacle avoidance problem is formulated in the sense of optimal control, and a safe reinforcement learning method is designed to generate safe motion trajectories. This method combines the advantages of model-based policy iteration and state-following-based approximation, in which the former ensures regional optimality while the latter ensures local safety. Based on the proposed adaptive tuning laws, engineers are able to design learning-based avoidance controllers in the environment with static obstacles. In future research, we will address the dynamic avoidance problem against moving obstacles.","1558-3783","","10.1109/TASE.2023.3299275","National Key Research and Development Program of China(grant numbers:2021YFB1714700); National Natural Science Foundation of China(grant numbers:62022061,62333016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10239217","Adaptive dynamic programming;actor-critic reinforcement learning;safe reinforcement learning;obstacle avoidance;optimal control;neural networks","Collision avoidance;Safety;Optimal control;Reinforcement learning;Real-time systems;Trajectory;Switches","","","","","","","IEEE","4 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Efficient Multilevel Federated Compressed Reinforcement Learning of Smart Homes Using Deep Learning Methods","P. Ravichandran; C. Saravanakumar; J. D. Rose; M. Vijayakumar; V. M. Lakshmi","Department of Information Technology, St. Joseph's Institute of Technology, Chennai; Department of Information Technology, St. Joseph's Institute of Technology, Chennai; Department of Computer Science and Engineering, St. Joseph's Institute of Technology, Chennai; Department of Computer Science and Engineering, St. Joseph's Institute of Technology, Chennai; Department of Information Technology, St. Joseph's Institute of Technology, Chennai","2021 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)","16 Dec 2021","2021","","","1","11","Internet of Things connects all real time devices using the wireless nature for collecting, sharing and processing of data. These data are analyzed using machine learning models based on the structure of data. Reinforcement learning is a type of learning method which performs with past experience of data. Traditional algorithms use data with a specific environment with a learning process for prediction. Federated Learning (FL) is achieved through the integration of the various learning models for achieving accuracy. The proposed learning algorithm uses multilevel FL over the smart homes with two house data for analysis of the user behavior. Various kinds of sensors are used for analyzing the behavior, namely local and global. The data is shared with agents and servers with the use of communication networks. It suffers the bandwidth issues because of heterogeneity in the data, so this is overcome by using FL compression method. Multilevel FL compression method achieves reduced latency with efficient interaction. The proposed technique achieves better accuracy when compared to existing RL method with maximum performance and reliability.","","978-1-6654-3521-5","10.1109/ICSES52305.2021.9633785","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9633785","Machine Learning;Reinforcement Learning;Federated Learning;IoT;Deep Learning","Wireless communication;Performance evaluation;Wireless sensor networks;Machine learning algorithms;Smart homes;Reinforcement learning;Prediction algorithms","deep learning (artificial intelligence);home automation;reinforcement learning","efficient multilevel federated compressed reinforcement learning;smart homes;deep learning methods;Internet of Things;wireless nature;sharing processing;machine learning models;federated learning;house data;RL method;multilevel FL compression method","","11","","35","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Meta-Reinforcement Learning of Machining Parameters for Energy-Efficient Process Control of Flexible Turning Operations","Q. Xiao; C. Li; Y. Tang; L. Li","State Key Laboratory of Mechanical Transmission, Chongqing University, Chongqing, China; State Key Laboratory of Mechanical Transmission, Chongqing University, Chongqing, China; Department of Electrical and Computer Engineering, Rowan University, Glassboro, NJ, USA; College of Engineering and Technology, Southwest University, Chongqing, China","IEEE Transactions on Automation Science and Engineering","7 Jan 2021","2021","18","1","5","18","Energy-efficient machining has become imperative for energy conservation, emission reduction, and cost saving of manufacturing sectors. Optimal machining parameter decision is regarded as an effective way to achieve energy efficient turning. For flexible machining, it is of utmost importance to determine the optimal parameters adaptive to various machines, workpieces, and tools. However, very little research has focused on this issue. Hence, this paper undertakes this challenge by integrated meta-reinforcement learning (MRL) of machining parameters to explore the commonalities of optimization models and use the knowledge to respond quickly to new machining tasks. Specifically, the optimization problem is first formulated as a finite Markov decision process (MDP). Then, the continuous parametric optimization is approached with actor-critic (AC) framework. On the basis of the framework, meta-policy training is performed to improve the generalization capacity of the optimizer. The significance of the proposed method is exemplified and elucidated by a case study with a comparative analysis.","1558-3783","","10.1109/TASE.2019.2924444","National Natural Science Foundation of China(grant numbers:51975075); Chongqing Technology Innovation and Application Program(grant numbers:cstc2018jszx-cyzdX0183); Fundamental Research Funds for the Central Universities of China(grant numbers:cqu2018CDHB1B07); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8770304","Energy efficiency;meta-reinforcement learning (MRL);parametric optimization;turning operations","Optimization;Turning;Task analysis;Tools;Machine tools","energy conservation;learning (artificial intelligence);Markov processes;optimisation;process control","machining parameters;energy-efficient process control;flexible turning operations;energy-efficient machining;energy conservation;emission reduction;manufacturing sectors;optimal machining parameter decision;energy efficient turning;flexible machining;optimal parameters;paper undertakes this challenge;integrated meta-reinforcement learning;optimization models;machining tasks;optimization problem;finite Markov decision process;continuous parametric optimization;meta-policy training","","25","","36","IEEE","23 Jul 2019","","","IEEE","IEEE Journals"
"Indirect Multi-Energy Transactions of Energy Internet With Deep Reinforcement Learning Approach","L. Yang; Q. Sun; N. Zhang; Y. Li","School of Artificial Intelligence, Anhui University, Hefei, China; State Key Laboratory of Synthetical Automation for Processes Industries and the School of Information Science and Engineering, Northeastern University, Shenyang, China; School of Electrical Engineering and Automation, Anhui University, Hefei, China; School of Information Science and Engineering, Northeastern University, Shenyang, China","IEEE Transactions on Power Systems","18 Aug 2022","2022","37","5","4067","4077","With the new feature of multi-energy coupling and the advancement of the energy market, Energy Internet (EI) has higher requirements for the efficiency and applicability of integrated energy response. This paper proposes an indirect multi-energy transaction (IMET) to promote multi-energy collaborative optimization in local energy market (LEM) and improve energy utilization through personalized responses from We-Energies (WEs). Firstly, an indirect customer-to-customer multi-energy transaction is modeled for local multi-energy coupling market which can satisfy privacy, preference and autonomy of users. The efficiency of energy matching can be promoted through the participation of conversion devices. In addition, multi-time scale hybrid trading mechanism is constructed with the consideration of the transmission speed of different energy sources. Meanwhile, energy transaction process is built as a Markov decision process (MDP) with deep reinforcement learning algorithm so that the system modeling error can be successfully avoided. Furthermore, a distributed training structure is utilized to obtain more experience for a wider range of scenarios. The results of numerical simulations demonstrate the performance of the proposed method.","1558-0679","","10.1109/TPWRS.2022.3142969","National Key R&D Program of China(grant numbers:2018YFA0702200); National Natural Science Foundation of China(grant numbers:62073065); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681381","Indirect multi-energy transactions;market mechanism;deep reinforcement learning;energy internet","Indexes;Reinforcement learning;Games;Privacy;Load modeling;Internet;Companies","deep learning (artificial intelligence);Internet;Markov processes;numerical analysis;optimisation;power engineering computing;power markets;reinforcement learning","multienergy collaborative optimization;local energy market;energy utilization;customer-to-customer multienergy transaction;energy matching;multitime scale hybrid trading mechanism;deep reinforcement learning algorithm;energy internet;IMET;indirect multienergy transactions;LEM;Markov decision process;MDP;numerical simulations;distributed training structure","","44","","33","IEEE","13 Jan 2022","","","IEEE","IEEE Journals"
"Data-Driven Inverse Reinforcement Learning Control for Linear Multiplayer Games","B. Lian; V. S. Donge; F. L. Lewis; T. Chai; A. Davoudi","Department of Electrical Engineering, University of Texas at Arlington, Arlington, TX, USA; Department of Electrical Engineering, University of Texas at Arlington, Arlington, TX, USA; Department of Electrical Engineering, University of Texas at Arlington, Arlington, TX, USA; State Key Laboratory of Synthetical Automation for Process Industries and the International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; Department of Electrical Engineering, University of Texas at Arlington, Arlington, TX, USA","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","14","This article proposes a data-driven inverse reinforcement learning (RL) control algorithm for nonzero-sum multiplayer games in linear continuous-time differential dynamical systems. The inverse RL problem in the games is solved by a learner reconstructing the unknown expert players’ cost functions from demonstrated expert’s optimal state and control input trajectories. The learner, thus, obtains the same control feedback gains and trajectories as the expert, only using data along system trajectories without knowing system dynamics. This article first proposes a model-based inverse RL policy iteration framework that has: 1) policy evaluation step for reconstructing cost matrices using Lyapunov functions; 2) state-reward weight improvement step using inverse optimal control (IOC); and 3) policy improvement step using optimal control. Based on the model-based policy iteration algorithm, this article further develops an online data-driven off-policy inverse RL algorithm without knowing any knowledge of system dynamics or expert control gains. Rigorous convergence and stability analysis of the algorithms are provided. It shows that the off-policy inverse RL algorithm guarantees unbiased solutions while probing noises are added to satisfy the persistence of excitation (PE) condition. Finally, two different simulation examples validate the effectiveness of the proposed algorithms.","2162-2388","","10.1109/TNNLS.2022.3186229","Office of Naval Research(grant numbers:N00014-18-1-2221); Army Research Office(grant numbers:W911NF-20-1-0132); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815022","Inverse optimal control (IOC);inverse RL;nonzero-sum Nash games;off-policy;optimal control","Games;Cost function;Optimal control;Heuristic algorithms;Trajectory;System dynamics;Costs","","","","5","","","IEEE","4 Jul 2022","","","IEEE","IEEE Early Access Articles"
"Inverse Reinforcement Learning Control for Linear Multiplayer Games","B. Lian; V. S. Donge; F. L. Lewis; T. Chai; A. Davoudi","University of Texas at Arlington, Arlington, TX, USA; University of Texas at Arlington, Arlington, TX, USA; University of Texas at Arlington, Arlington, TX, USA; The State Key Laboratory of Synthetical Automation for Process Industries and International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; University of Texas at Arlington, Arlington, TX, USA","2022 IEEE 61st Conference on Decision and Control (CDC)","10 Jan 2023","2022","","","2839","2844","This paper proposes model-based and model-free inverse reinforcement learning (RL) control algorithms for multiplayer game systems described by linear continuous-time differential equations. Both algorithms find the learner the same optimal control policies and trajectories as the expert, by inferring the unknown expert players’ cost functions from the expert’s trajectories. This paper first discusses a model-based inverse RL policy iteration that consists of 1) policy evaluation for cost matrices using a Lyapunov equation, 2) state-reward weight improvement using inverse optimal control (IOC), and 3) policy improvement using optimal control. Based on the model-based algorithm, an online data-driven inverse RL algorithm is proposed without knowing system dynamics or expert control gains. Rigorous convergence and stability analysis of these algorithms are provided. Finally, a simulation example verifies our approach.","2576-2370","978-1-6654-6761-2","10.1109/CDC51059.2022.9993367","Office of Naval Research; Arm; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9993367","","Costs;System dynamics;Heuristic algorithms;Optimal control;Reinforcement learning;Games;Differential equations","differential equations;game theory;iterative methods;Lyapunov methods;matrix algebra;optimal control;reinforcement learning","continuous-time differential equations;cost matrices;expert player cost functions;inverse optimal control;inverse reinforcement learning control;inverse RL policy iteration;IOC;linear multiplayer games;Lyapunov equation;optimal control policies;policy improvement;state-reward weight improvement","","","","26","IEEE","10 Jan 2023","","","IEEE","IEEE Conferences"
"Attention-Based Multi-Agent Graph Reinforcement Learning for Service Restoration","B. Fan; X. Liu; G. Xiao; Y. Kang; D. Wang; P. Wang","School of Electrical Engineering, Xi'an University of Technology, Xi'an, China; School of Electrical Engineering, Xi'an University of Technology, Xi'an, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Automation, School of Information Science and Technology, University of Science and Technology of China, Hefei, China; Artificial Intelligence Research Institute, China University of Mining and Technology, Xuzhou, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Artificial Intelligence","","2023","PP","99","1","15","With the ongoing integration of distributed energy resources, modern distribution systems are getting sufficient generation capacity to perform active restoration after outages without transmission system support. The model-based approaches are widely used in resolving service restoration problems, relying on accurate system models. Deep reinforcement learning is believed as an alternative solution for problem solving, although it has not been sufficiently explored. In this paper, the service restoration process is described as a partially observable Markov decision process, and a multi-agent graph reinforcement learning approach based on attention is proposed to train multiple agents to co-achieve the restoration goal to reinforce the system resilience in coping with extreme events. To consider the connections and correlations between nodes during the service restoration, the state of the active distribution network is defined by graph data that contains features of both topology and nodes. The perceived ability of the agents is empowered by graph convolutional networks during the feature extraction, supplying agents with more comprehensive data to learn more reasonable restoration strategies. In addition, the centralized training with attention is developed for multi-agent systems, focusing on the relations between the agents to strengthen teamwork capability. The performance of the proposed method is verified by a set of comparative studies on the IEEE-118 system with dispatchable generators, rooftop photovoltaics, and energy storage systems simultaneously.","2691-4581","","10.1109/TAI.2023.3314395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247632","Active distribution network;distributed energy resource;multi-agent graph reinforcement learning;self-attention;service restoration","Reinforcement learning;Artificial intelligence;Reactive power;Distribution networks;Deep learning;Load modeling;Active distribution networks","","","","","","","IEEE","12 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Efficiently Training On-Policy Actor-Critic Networks in Robotic Deep Reinforcement Learning with Demonstration-like Sampled Exploration","Z. Chen; B. Chen; S. Xie; L. Gong; C. Liu; Z. Zhang; J. Zhang","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University; School of Mechanical Engineering, Shanghai Jiao Tong University; School of Mechanical Engineering, Shanghai Jiao Tong University; School of Mechanical Engineering, Shanghai Jiao Tong University; School of Mechanical Engineering, Shanghai Jiao Tong University; Shanghai Key Lab of Intelligent Information Processing, Fudan University; Shanghai Key Lab of Intelligent Information Processing, Fudan University","2021 3rd International Symposium on Robotics & Intelligent Manufacturing Technology (ISRIMT)","15 Nov 2021","2021","","","292","298","In complex environments with high dimension, training a reinforcement learning (RL) model from scratch often suffers from lengthy and tedious collection of agent-environment interactions. Instead, leveraging expert demonstration to guide RL agent can boost sample efficiency and improve final convergence. In order to better integrate expert prior with on-policy RL models, we propose a generic framework for Learning from Demonstration (LfD) based on actor-critic algorithms. Technically, we first employ K-Means clustering to evaluate the similarity of sampled exploration with demonstration data. Then we increase the likelihood of actions in similar frames by modifying the gradient update strategy to leverage demonstration. We conduct experiments on 4 standard benchmark environments in Mujoco and 2 self-designed robotic environments. Results show that, under certain condition, our algorithm can improve sample efficiency by 20%~ 40%. By combining our framework with on-policy algorithms, RL models can accelerate convergence and obtain better final mean episode rewards especially in complex robotic context where interactions are expensive.","","978-1-6654-3718-9","10.1109/ISRIMT53730.2021.9597110","National Natural Science Foundation of China(grant numbers:51775333); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9597110","deep learning;deep reinforcement learning;learning from demonstration (LfD);actor-critic framework;robotics","Training;Clustering algorithms;Reinforcement learning;Benchmark testing;Stability analysis;Manufacturing;Robots","deep learning (artificial intelligence);gradient methods;mobile robots;network theory (graphs);pattern clustering;reinforcement learning;robot programming","expert demonstration;RL agent;final convergence;demonstration data;gradient update strategy;complex environments;on-policy actor-critic networks;demonstration-like sampled exploration;learning from demonstration;LfD;k-means clustering;Mujoco;self-designed robotic environments;robotic deep reinforcement learning","","","","25","IEEE","15 Nov 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Power-Saving Algorithm for Video Traffics Considering Network Delay Jitter","D. Ron; J. -R. Lee","Department of Intelligent Energy and Industry, Chung-Ang University, Seoul, Republic of Korea; Department of Intelligent Energy and Industry, Chung-Ang University, Seoul, Republic of Korea","IEEE Access","12 Sep 2022","2022","10","","92946","92958","Recent studies on energy efficiency and scheduling of power-saving mode have been considered as key technologies for reducing the energy consumption of device-to-device (D2D) communication. Wi-Fi Direct (P2P), one of the key protocols for D2D communication, defines the on-off power saving mechanic called the notice of absence (NoA) power-saving mode that can be applied to the multimedia video traffic. The on-off power saving mechanic enables the user to transmit or receive the real-time video frame during the awake interval in which the video frame rate should meet the requirement. When the user can wholly transmit one video frame before the end time of a required inter-frame interval, it can switch to the sleep mode to save the power consumption. However, the challenge remaining for the NoA method is the fixed length of awake/sleep interval, even if the traffic load is varied. Therefore, in this study, we proposed a reinforcement learning-based power saving (RLPS) method to enhance the performance of the notice of absence (NoA) power-saving mode in Wi-Fi direct with taking the multimedia video transmission and the network delay jitter into consideration. The proposed RLPS method enables the Wi-Fi direct device to dynamically estimate the length of awake interval for transmitting the future video frame in real-time. In addition, the Wi-Fi direct device may wake up too early before the arrival of the video frame, which is caused by the network delay jitter. Thus, the client device has to wait for receiving the video frame. To tackle this challenge, the proposed RLPS method enables the device to predict the start time of awake interval for the purpose of reducing the delay time for receiving the upcoming video frame. Results show that the proposed RLPS method outperforms the existing NoA power-saving mode in terms of the outage probability, energy consumption, and transmission delay of Wi-Fi Direct devices.","2169-3536","","10.1109/ACCESS.2022.3201866","Ministry of Science and ICT (MSIT), South Korea, through the ITRC Support Program, supervised by the Institute for Information and Communications Technology Planning and Evaluation (IITP)(grant numbers:IITP-2022-2018-0-01799); Korea Institute of Energy Technology Evaluation and Planning (KETEP) and the Ministry of Trade, Industry and Energy (MOTIE), Republic of Korea(grant numbers:20214000000280); National Research Foundation of Korea (NRF) grant funded by the Korea Government (MEST)(grant numbers:NRF-2020R1A2C1010929); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9868002","Wi-Fi direct;opportunistic power saving;NoA;reinforcement learning;network delay jitter","Delays;Wireless fidelity;Streaming media;Jitter;Energy consumption;Scheduling;Device-to-device communication;Reinforcement learning;Energy consumption","delays;jitter;learning (artificial intelligence);mobile radio;multimedia communication;peer-to-peer computing;probability;protocols;quality of service;scheduling;telecommunication traffic;video coding;video streaming;wireless LAN","required inter-frame interval;sleep mode;power consumption;NoA method;traffic load;reinforcement learning-based power saving method;absence power-saving mode;multimedia video transmission;RLPS method;Wi-Fi direct device;awake interval;future video frame;client device;delay time;upcoming video frame;existing NoA power-saving mode;energy consumption;Wi-Fi Direct devices;reinforcement learning-based power-saving algorithm;video traffics considering network delay jitter;energy efficiency;scheduling;device-to-device communication;key protocols;power saving mechanic;multimedia video traffic;real-time video frame;video frame rate","","","","31","CCBY","26 Aug 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Offloading and Resource Allocation in Vehicle Edge Computing and Networks","Y. Liu; H. Yu; S. Xie; Y. Zhang","School of Automation, Guangdong University of Technology, Key Lab. of Ministry of Education and Guangdong Province Key Lab. of IoT Information Technology, Guangzhou, China; College of Information Science and Engineering, Hunan Normal University, Changsha, China; School of Automation, Guangdong University of Technology, and State Key Lab. of Precision Electronic Manufacturing Technology and Equipment, Guangzhou, China; University of Oslo, Oslo, Norway","IEEE Transactions on Vehicular Technology","12 Nov 2019","2019","68","11","11158","11168","Mobile Edge Computing (MEC) is a promising technology to extend the diverse services to the edge of Internet of Things (IoT) system. However, the static edge server deployment may cause “service hole” in IoT networks in which the location and service requests of the User Equipments (UEs) may be dynamically changing. In this paper, we firstly explore a vehicle edge computing network architecture in which the vehicles can act as the mobile edge servers to provide computation services for nearby UEs. Then, we propose as vehicle-assisted offloading scheme for UEs while considering the delay of the computation task. Accordingly, an optimization problem is formulated to maximize the long-term utility of the vehicle edge computing network. Considering the stochastic vehicle traffic, dynamic computation requests and time-varying communication conditions, the problem is further formulated as a semi-Markov process and two reinforcement learning methods: Q-learning based method and deep reinforcement learning (DRL) method, are proposed to obtain the optimal policies of computation offloading and resource allocation. Finally, we analyze the effectiveness of the proposed scheme in the vehicular edge computing network by giving numerical results.","1939-9359","","10.1109/TVT.2019.2935450","National Natural Science Foundation of China(grant numbers:61773126,61727810,61701125,61603099,61973087); Pearl River S and T Nova Program of Guangzhou(grant numbers:201806010176); The European Unions Horizon 2020 research and innovation programme(grant numbers:824019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798668","Vehicle edge computing;resource allocation;IoT;deep reinforcement learning","Task analysis;Edge computing;Servers;Internet of Things;Computational modeling;Iron;Reinforcement learning","cellular radio;cloud computing;Internet of Things;learning (artificial intelligence);Markov processes;mobile computing;resource allocation;telecommunication traffic","vehicle-assisted offloading scheme;computation task;vehicle edge computing network;stochastic vehicle traffic;mobile edge computing;computation services;mobile edge servers;service requests;IoT networks;service hole;diverse services;vehicular edge computing network;resource allocation;computation offloading;deep reinforcement learning method;reinforcement learning methods;time-varying communication conditions;dynamic computation requests","","297","","37","IEEE","14 Aug 2019","","","IEEE","IEEE Journals"
"Manifold-Based Reinforcement Learning via Locally Linear Reconstruction","X. Xu; Z. Huang; L. Zuo; H. He","College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; Department of Electrical, Computer, and Biomedical Engineering, The University of Rhode Island, Kingston, RI, USA","IEEE Transactions on Neural Networks and Learning Systems","20 May 2017","2017","28","4","934","947","Feature representation is critical not only for pattern recognition tasks but also for reinforcement learning (RL) methods to solve learning control problems under uncertainties. In this paper, a manifold-based RL approach using the principle of locally linear reconstruction (LLR) is proposed for Markov decision processes with large or continuous state spaces. In the proposed approach, an LLR-based feature learning scheme is developed for value function approximation in RL, where a set of smooth feature vectors is generated by preserving the local approximation properties of neighboring points in the original state space. By using the proposed feature learning scheme, an LLR-based approximate policy iteration (API) algorithm is designed for learning control problems with large or continuous state spaces. The relationship between the value approximation error of a new data point and the estimated values of its nearest neighbors is analyzed. In order to compare different feature representation and learning approaches for RL, a comprehensive simulation and experimental study was conducted on three benchmark learning control problems. It is illustrated that under a wide range of parameter settings, the LLR-based API algorithm can obtain better learning control performance than the previous API methods with different feature representation schemes.","2162-2388","","10.1109/TNNLS.2015.2505084","U.S. National Science Foundation, the NSFC-Innovation and Development Joint Foundation of Chinese Automobile Industry(grant numbers:1053717,U1564214); National Natural Science Foundation of China(grant numbers:51529701,91220301); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7393833","Adaptive dynamic programming;learning control;manifold learning;Markov decision processes (MDPs);reinforcement learning (RL);value function approximation (VFA)","Approximation algorithms;Manifolds;Aerospace electronics;Laplace equations;Function approximation;Heuristic algorithms;Economic indicators","approximation theory;iterative methods;learning (artificial intelligence);Markov processes;vectors","manifold-based reinforcement learning;manifold-based RL;locally linear reconstruction;LLR;feature representation;Markov decision process;feature learning scheme;value function approximation;smooth feature vector;approximate policy iteration;API algorithm","","36","","43","IEEE","27 Jan 2016","","","IEEE","IEEE Journals"
"Physical Layer Security Enhancement in Energy Harvesting-based Cognitive Internet of Things: A GAN-Powered Deep Reinforcement Learning Approach","R. Lin; H. Qiu; J. Wang; Z. Zhang; L. Wu; F. Shu","College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Frontiers Science Center for Mobile Information Communication and Security, National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; Frontiers Science Center for Mobile Information Communication and Security, National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; School of Information and Communication Engineering, Hainan University, Haikou, China","IEEE Internet of Things Journal","","2023","PP","99","1","1","Cognitive Radio (CR) is regarded as the key technology of the 6th-Generation (6G) wireless network. Because 6G CR networks are anticipated to offer worldwide coverage, increase cost efficiency, enhance spectrum utilization, improve device intelligence and network safety. This paper studies the secrecy communication in an Energy Harvesting (EH)-enabled Cognitive Internet of Things (EH-CIoT) network with a cooperative jammer. The Secondary Transmitters (STs) and the jammer first harvest the energy from the received Radio Frequency (RF) signals in the EH phase. Then, in the subsequent Wireless Information Transfer (WIT) phase, the STs transmit secrecy information to their intended receivers in the presence of eavesdroppers while the jammer sends the jamming signal to confuse the eavesdroppers. To evaluate the system secrecy performance, we derive the instantaneous secrecy rate and the closed-form expression of Secrecy Outage Probability (SOP). Furthermore, we propose a Deep Reinforcement Learning (DRL)-based framework for the joint EH time and transmission power allocation problems. Specifically, a pair of ST and jammer over each time block is modeled as an agent which is dynamically interacting with the environment by the state, action and reward mechanisms. To better find the optimal solutions to the proposed problems, the Long Short-Term Memory Network (LSTM) and the Generative Adversarial Networks (GAN) are combined with the classical DRL algorithm. The simulation results show that our proposed method is highly effective in maximizing the secrecy rate while minimizing the SOP compared with other existing schemes.","2327-4662","","10.1109/JIOT.2023.3300770","Industry-Academia Collaboration Program of Fujian Universities(grant numbers:2020H6006); National Natural Science Foundation of China(grant numbers:61871133); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10198554","Cognitive Radio Network;energy harvesting;physical layer security enhancement;deep reinforcement learning","Jamming;Internet of Things;Wireless communication;Resource management;Communication system security;6G mobile communication;Mobile handsets","","","","1","","","IEEE","1 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Self-Learning Chatbots using Reinforcement Learning","M. B. Lone; N. Nazir; N. Kaur; D. Pradeep; A. U. Ashraf; P. Asrar Ul Haq; N. B. Dar; A. Sarwar; M. Rakhra; O. Dahiya","School of Computer Science and Engineering, Lovely Professional University, Phagwara, Punjab, India; School of Computer Science and Engineering, Lovely Professional University, Phagwara, Punjab, India; School of Computer Science and Engineering, Lovely Professional University, Phagwara, Punjab, India; School of Computer Science and Engineering, Lovely Professional University, Phagwara, Punjab, India; School of Computer Science and Engineering, Lovely Professional University, Phagwara, Punjab, India; School of Computer Science and Engineering, Lovely Professional University, Phagwara, Punjab, India; School of Computer Science and Engineering, Lovely Professional University, Phagwara, Punjab, India; University of Jammu, Gujarbasti, Jammu Jammu & Kashmir; School of Computer Science and Engineering, Lovely Professional University, Phagwara, Punjab, India; School of Computer Science and Engineering, Lovely Professional University, Phagwara, Punjab, India","2022 3rd International Conference on Intelligent Engineering and Management (ICIEM)","17 Aug 2022","2022","","","802","808","There has been a surge in the development of dialogue generation systems also called chatbots in Recent years. Research on sequence-to-sequence architectural chatbots has led to a number of efficient and adaptive chatbots. Several of them worked on goal-oriented chatbots, some worked on open domain chatbots and others developed emotional chatbots called CheerBots that respond to people’s emotions. Be it in the IT field or in Education, these Dialogue systems have proven their worth in every industry. This paper discusses, the proposal of a SEQ2SEQ architecture-based chatbot that uses a Reinforcement Learning Algorithm to respond to user queries. This allows the model to explore the domain of all possible responses that can be generated by the SEQ2SEQ model by learning novel utterances/queries. The model answered all the queries, making it a suitable chitchat system for open-domain applications like SIRI and ALEXA. It responded in an engaging and interactive manner which led to an interesting conversation. The model, despite being complex, showed remarkable accuracy.","","978-1-6654-6756-8","10.1109/ICIEM54221.2022.9853156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9853156","Neural Response Generation (NRG);Dialogue Systems;Reinforcement learning;SEQUENCE-TO-SEQUENCE;LSTM (Long Short-Term Memory);Goal Oriented Systems;Open Domain Systems;Encoder-Decoder;Deep Reinforcement Learning;Chatbots;Artificial Intelligence;Human-Robot-Interaction (HRI);Recurrent Neural Networks (RNN);Deep Neural Networks;Natural Language Understanding (NLU);CheerBots;Deep Q-Networks (DQN)","Industries;Training;Virtual assistants;Reinforcement learning;Predictive models;Chatbots;Proposals","interactive systems;learning (artificial intelligence);natural language interfaces;software agents;speech-based user interfaces","suitable chitchat system;open-domain applications;dialogue generation systems;sequence-to-sequence architectural chatbots;efficient chatbots;adaptive chatbots;goal-oriented chatbots;open domain chatbots;emotional chatbots;people;Dialogue systems;SEQ2SEQ architecture-based chatbot;Reinforcement Learning Algorithm;user queries;SEQ2SEQ model","","","","44","IEEE","17 Aug 2022","","","IEEE","IEEE Conferences"
"Low-Level Control of a Quadrotor With Deep Model-Based Reinforcement Learning","N. O. Lambert; D. S. Drew; J. Yaconelli; S. Levine; R. Calandra; K. S. J. Pister","Department of Electrical Engineering and Computer Sciences, University of California–Berkeley, Berkeley, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California–Berkeley, Berkeley, CA, USA; University of Oregon, Eugene, OR, USA; Department of Electrical Engineering and Computer Sciences, University of California–Berkeley, Berkeley, CA, USA; Facebook AI Research, Menlo Park, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California–Berkeley, Berkeley, CA, USA","IEEE Robotics and Automation Letters","15 Aug 2019","2019","4","4","4224","4230","Designing effective low-level robot controllers often entail platform-specific implementations that require manual heuristic parameter tuning, significant system knowledge, or long design times. With the rising number of robotic and mechatronic systems deployed across areas ranging from industrial automation to intelligent toys, the need for a general approach to generating low-level controllers is increasing. To address the challenge of rapidly generating low-level controllers, we argue for using model-based reinforcement learning (MBRL) trained on relatively small amounts of automatically generated (i.e., without system simulation) data. In this letter, we explore the capabilities of MBRL on a Crazyflie centimeter-scale quadrotor with rapid dynamics to predict and control at ≤50 Hz. To our knowledge, this is the first use of MBRL for controlled hover of a quadrotor using only on-board sensors, direct motor input signals, and no initial dynamics knowledge. Our controller leverages rapid simulation of a neural network forward dynamics model on a graphic processing unit enabled base station, which then transmits the best current action to the quadrotor firmware via radio. In our experiments, the quadrotor achieved hovering capability of up to 6 s with 3 min of experimental training data.","2377-3766","","10.1109/LRA.2019.2930489","Berkeley Sensors & Actuator Center SUPERB REU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8769882","Deep learning in robotics and automation;aerial systems: mechanics and control","Data models;Vehicle dynamics;Robots;Pulse width modulation;Attitude control;Trajectory;Predictive models","aircraft control;control engineering computing;helicopters;learning (artificial intelligence);mechatronics;mobile robots;multi-robot systems;path planning;trajectory control","rapid simulation;platform-specific implementations;on-board sensors;direct motor input signals;neural network forward dynamics model;quadrotor firmware;robotic systems;low-level robot controllers;deep model-based reinforcement learning;low-level control;controlled hover;Crazyflie centimeter-scale quadrotor;MBRL;low-level controllers;mechatronic systems;frequency 50.0 Hz;time 6.0 s;time 3.0 min","","81","","30","IEEE","23 Jul 2019","","","IEEE","IEEE Journals"
"Learning to Schedule Joint Radar-Communication With Deep Multi-Agent Reinforcement Learning","J. Lee; D. Niyato; Y. L. Guan; D. I. Kim","Energy Research Institute @ NTU, Nanyang Technological University, Singapore, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea","IEEE Transactions on Vehicular Technology","20 Jan 2022","2022","71","1","406","422","Radar detection and communication are two essential sub-tasks for the operation of next-generation autonomous vehicles (AVs). The forthcoming proliferation of faster 5G networks utilizing mmWave has raised concerns on interference with automotive radar sensors, which has led to a body of research on Joint Radar-Communication (JRC). This paper considers the problem of time-sharing for JRC, with the additional simultaneous objective of minimizing the average age of information (AoI) transmitted by a JRC-equipped AV. We first formulate the problem as a Markov Decision Process (MDP). We then propose a more general multi-agent system, with an appropriate medium access control (MAC) protocol, which is formulated as a partially observed Markov game (POMG). To solve the POMG, we propose a multi-agent extension of the Proximal Policy Optimization (PPO) algorithm, along with algorithmic features to enhance learning from raw observations. Simulations are run with a range of environmental parameters to mimic variations in real-world operation. The results show that the chosen deep reinforcement learning methods allow the agents to obtain strong performance with minimal a priori knowledge about the environment.","1939-9359","","10.1109/TVT.2021.3124810","Energy Research Institute @ NTU; Computer Networks and Communications Lab in Nanyang Technological University; National Research Foundation; Prime Minister's Office, Singapore; Campus for Research Excellence and Technological Enterprise; Alibaba Innovative Research; Program and Alibaba-NTU Singapore Joint Research Institute; National Research Foundation Singapore; AI Singapore Programme(grant numbers:AISG2-RP-2020-019); WASP/NTU(grant numbers:M4082187 (4080)); Singapore Ministry of Education (MOE) Tier 1(grant numbers:RG16/20); Agency for Science, Technology and Research(grant numbers:RIE2020); Advanced Manufacturing and Engineering; Industry Alignment Fund-Pre Positioning(grant numbers:A19D6a0053); National Research Foundation of Korea(grant numbers:2021R1A2C2007638); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9601214","Reinforcement learning;deep learning;task scheduling;vehicle safety;communication","Sensors;Radar;Automotive engineering;Accidents;Sensor systems;Reinforcement learning;Cameras","access protocols;control engineering computing;decision theory;learning (artificial intelligence);Markov processes;multi-agent systems;radar detection","Proximal Policy Optimization algorithm;real-world operation;chosen deep reinforcement learning methods;schedule Joint Radar-Communication;deep multiagent reinforcement learning;essential sub-tasks;next-generation autonomous vehicles;forthcoming proliferation;faster 5G networks utilizing mmWave;automotive radar sensors;time-sharing;additional simultaneous objective;JRC-equipped AV;Markov Decision Process;general multiagent system;appropriate medium access control protocol;partially observed Markov game;POMG;multiagent extension","","5","","48","IEEE","4 Nov 2021","","","IEEE","IEEE Journals"
"A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning","M. Vecerik; O. Sushkov; D. Barker; T. Rothörl; T. Hester; J. Scholz","DeepMind, London, UK; DeepMind, London, UK; DeepMind, London, UK; DeepMind, London, UK; DeepMind, London, UK; DeepMind, London, UK","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","754","760","Insertion is a challenging haptic and visual control problem with significant practical value for manufacturing. Existing approaches in the model-based robotics community can be highly effective when task geometry is known, but are complex and cumbersome to implement, and must be tailored to each individual problem by a qualified engineer. Within the learning community there is a long history of insertion research, but existing approaches are either too sample-inefficient to run on real robots, or assume access to high-level object features, e.g. socket pose. In this paper we show that relatively minor modifications to an off-the-shelf Deep-RL algorithm (DDPG), combined with a small number of human demonstrations, allows the robot to quickly learn to solve these tasks efficiently and robustly. Our approach requires no modeling or simulation, no parameterized search or alignment behaviors, no vision system aside from raw images, and no reward shaping. We evaluate our approach on a narrow-clearance peg-insertion task and a deformable clip-insertion task, both of which include variability in the socket position. Our results show that these tasks can be solved reliably on the real robot in less than 10 minutes of interaction time, and that the resulting policies are robust to variance in the socket position and orientation.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794074","","Task analysis;Robots;Sockets;Visualization;Training;Plugs;Feature extraction","control engineering computing;industrial robots;learning (artificial intelligence);neural nets;production engineering computing","variable socket position;visual control problem;model-based robotics community;task geometry;off-the-shelf Deep-RL algorithm;narrow-clearance peg-insertion task;deformable clip-insertion task;deep reinforcement learning;haptic control problem","","39","","21","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Industrial Power Load Forecasting Method Based on Reinforcement Learning and PSO-LSSVM","Q. Ge; C. Guo; H. Jiang; Z. Lu; G. Yao; J. Zhang; Q. Hua","School of Electronics and Information Engineering, Tongji University, Shanghai, China; Logistics Engineering College, Shanghai Maritime University, Shanghai, China; School of Automation, Southeast University, Nanjing, China; Jiangsu Key Laboratory of Meteorological Observation and Information Processing, Nanjing University, Nanjing, China; Logistics Engineering College, Shanghai Maritime University, Shanghai, China; School of Automation, Hangzhou Dianzi University, Hangzhou, China; Hangzhou Zhonhen Power Energy Co. Ltd., Hangzhou, China","IEEE Transactions on Cybernetics","16 Feb 2022","2022","52","2","1112","1124","Influenced by many complex factors, it is very difficult to obtain high-performance industrial power load forecasting. The industrial power load forecasting is deeply studied by fusing some machine-learning methods for industrial enterprise power consumers. As a result, a novel power load forecasting method is proposed by taking into account the variation of load characteristics in different regions, industries, and production patterns. First, through the improved  $K$ -means clustering analysis, the historical load data are classified as the production patterns to which they belong. Then, the prediction algorithm combining reinforcement learning with particle swarm optimization and the least-squares support vector machine is proposed. Finally, the improved algorithm in this article is used for short-term load forecasting separately by the load data in different patterns after the above processing. The forecasting method in this article is based on data driven with real datasets. The results of the simulation experiment show that the improved prediction algorithm can distinguish the changes in different production patterns and identify the load characteristics of different regions and industries with high prediction accuracy, which has practical application value.","2168-2275","","10.1109/TCYB.2020.2983871","Zhejiang Provincial Nature Science Foundation of China(grant numbers:LR17F030005); Nature Science Foundation of China(grant numbers:61803136,51677047,U1866209); National Key Research and Development Program of China(grant numbers:2017YFB040350); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9086457","Clustering analysis;data-driven;optimization algorithm;power load forecasting;reinforcement learning","Load forecasting;Prediction algorithms;Reinforcement learning;Particle swarm optimization;Clustering algorithms;Support vector machines","","","","34","","44","IEEE","5 May 2020","","","IEEE","IEEE Journals"
"Multi-Objective Reinforcement Learning for Sustainable Supply Chain Optimization","I. E. Shar; H. Wang; C. Gupta","R&D, Industrial AI Lab, Hitachi America, Ltd.; R&D, Industrial AI Lab, Hitachi America, Ltd.; R&D, Industrial AI Lab, Hitachi America, Ltd.","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","7","Building sustainable supply chains (SC) has become a strategic focus for many organizations due to the increasing regulatory requirements and pressure from their conscientious customers. These SC focus on integrating social and environmental considerations into the design and management of the system, with reducing greenhouse gas (GHG) emissions being one of the primary goals. Reducing GHG emissions requires a holistic approach that tracks and considers emissions throughout the SC, from manufacturing to delivering the products to end customers. Toward this goal, we study a finite-horizon multi-echelon SC with emission considerations under carbon cap-and-trade legislation policy and formulate it as a multi-objective Markov decision process, where inventory replenishment and carbon management decisions are made at each period. One of the main technical challenges in creating sustainable multi-echelon SC is balancing environmental objectives with traditional economic metrics, such as cost and service level. Applying deep multi-objective reinforcement learning algorithms, we obtain a set of Pareto near-optimal policies that achieve a wide range of tradeoffs between cost and environmental objectives. We further analyze these policies and provide managerial insights into their decisions and performance. The performance of our derived sustainable inventory policies is also compared with that of a traditional inventory policy without emission considerations. In addition, we perform a sensitivity analysis to assess how changes in environmental factors, including the emission allowance and the carbon credit price, would impact the policies.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260354","","Measurement;Costs;Sensitivity analysis;Supply chains;Emissions trading;Carbon dioxide;Reinforcement learning","air pollution control;decision making;deep learning (artificial intelligence);environmental economics;environmental legislation;inventory management;Markov processes;Pareto optimisation;production engineering computing;reinforcement learning;sensitivity analysis;supply chain management;supply chains;sustainable development","carbon cap-and-trade legislation policy;carbon credit price;carbon management decisions;deep multiobjective reinforcement learning algorithms;economic metrics;emission allowance;environmental factors;finite-horizon multiechelon SC;GHG emission reduction;greenhouse gas emissions;inventory replenishment;multiobjective Markov decision process;Pareto near-optimal policies;sensitivity analysis;sustainable inventory policies;sustainable multiechelon SC;sustainable supply chain optimization","","","","17","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Optimal Trajectory Learning for UAV-BS Video Provisioning System: A Deep Reinforcement Learning Approach","D. Kwon; J. Kim","School of Computer Science and Engineering, Chung-Ang University, Seoul, Korea; School of Computer Science and Engineering, Chung-Ang University, Seoul, Korea","2019 International Conference on Information Networking (ICOIN)","20 May 2019","2019","","","372","374","The unmanned aerial vehicle (UAV) based data transmission is highlighted for next-generation communication system by both academia and industry. The UAV, which is dynamically associated with mobile users, can take a role of base station (BS) as service provider (SP), for various types of scenarios. For this sake, it is important that the UAV-BS should be hovered in the air with obeying optimal trajectory for minimizing delay, which is caused by enormous computation of data transmission, and the trajectory can be controlled by centralized macro base station (MBS). In this paper, we propose deep reinforcement learning approach for computing optimal trajectories of distributed UAV-BS with low-latency overhead to enable efficient UAV communication in next generation wireless system.","1976-7684","978-1-5386-8350-7","10.1109/ICOIN.2019.8718194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718194","","Trajectory;Reinforcement learning;Unmanned aerial vehicles;Wireless networks;Streaming media;Next generation networking","aircraft communication;autonomous aerial vehicles;data communication;learning (artificial intelligence);mobile radio;next generation networks;telecommunication computing;video signal processing","optimal trajectory learning;UAV-BS video provisioning system;unmanned aerial vehicle based data transmission;next-generation communication system;service provider;centralized macro base station;deep reinforcement learning approach;distributed UAV-BS;efficient UAV communication;next generation wireless system;delay minimization;low-latency overhead","","5","","11","IEEE","20 May 2019","","","IEEE","IEEE Conferences"
"Poster Abstract: Deep Learning Workloads Scheduling with Reinforcement Learning on GPU Clusters","Z. Chen; L. Luo; W. Quan; M. Wen; C. Zhang","College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China","IEEE INFOCOM 2019 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)","23 Sep 2019","2019","","","1023","1024","With the recent widespread adoption of deep learning (DL) in academia and industry, more attention are attracted by DL platform, which can support research and development (R&D) of AI firms, institutes and universities. Towards an off-the-shelf distributed GPU cluster, prior work propose prediction-based schedulers to allocate resources for diverse DL workloads. However, the prediction-based schedulers have disadvantages on prediction accuracy and offline-profiling costs. In this paper, we propose a learning-based scheduler, which models the scheduling problem as a reinforcement learning problem, achieving minimum average job completion time and maximum system utilization. The scheduler contains the designs of state space, action space, reward function and update scheme. Furthermore, we will evaluate our proposed scheduler implemented as a plugin of Tensorflow on real cluster and large-scale simulation.","","978-1-7281-1878-9","10.1109/INFCOMW.2019.8845276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8845276","DL platform;Reinforcement Learning;Scheduling;GPU clusters","Task analysis;Research and development;Processor scheduling;Predictive models;Graphics processing units;Deep learning","graphics processing units;learning (artificial intelligence);neural nets;resource allocation;scheduling;state-space methods","deep learning workloads scheduling;GPU clusters;prediction-based schedulers;learning-based scheduler;reinforcement learning problem;DL workloads;state space;action space;reward function;update scheme;Tensorflow plugin","","3","","5","IEEE","23 Sep 2019","","","IEEE","IEEE Conferences"
"Combining Decision Making and Trajectory Planning for Lane Changing Using Deep Reinforcement Learning","S. Li; C. Wei; Y. Wang","MOT Key Laboratory of Transport Industry of Big Data Application Technologies for Comprehensive Transport, School of Traffic and Transportation, Beijing Jiaotong University, Beijing, China; MOT Key Laboratory of Transport Industry of Big Data Application Technologies for Comprehensive Transport, School of Traffic and Transportation, Beijing Jiaotong University, Beijing, China; MOT Key Laboratory of Transport Industry of Big Data Application Technologies for Comprehensive Transport, School of Traffic and Transportation, Beijing Jiaotong University, Beijing, China","IEEE Transactions on Intelligent Transportation Systems","14 Sep 2022","2022","23","9","16110","16136","In the context of Automated Vehicles, the Automated Lane Change system, is fundamentally based upon the separate constructs of Perception, Decision making, Trajectory Planning, and Execution. However, in existing works there are many simplistic and unplausible assumptions in applying these constructs that severely restrict their operational effectiveness in realistic and complex driving scenarios. For instance, there are rigid assumptions about the disposition of vehicles and that lane-changing maneuvers can occur instantaneously, but that highly desirable features such as the ability for real-time trajectory re-planning are lacking. In this paper, we address these limitations through an integrated methodology for lane-change decision making and trajectory planning, in which a deep Reinforcement Learning algorithm with a safe action set technique is employed in decision making that is effectively coupled to a specially devised trajectory planning model. The proposed new methodology is computationally efficient, supporting real-time implementation, and provides for lane-changing maneuvers that can be made simultaneously with other vehicles and can be dynamically re-planned; thus, enabling flexible, robust, and safe lane-changing maneuvers under the guidance of a new decision-making module. Finally, the veracity of the proposed methodology in guiding a vehicle to improve travel times and accomplish high-level driving behaviors such as overtaking and desired-speed maintenance in a range of road traffic scenarios is demonstrated in a number of numerical experiments.","1558-0016","","10.1109/TITS.2022.3148085","National Natural Science Foundation of China(grant numbers:71971017,91746201,71621001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9726894","Decision making;trajectory planning;trajectory replanning;reinforcement learning;priority DQN;safety action set technique","Decision making;Trajectory planning;Trajectory;Vehicles;Reinforcement learning;Planning;Safety","decision making;driver information systems;learning (artificial intelligence);path planning;road safety;road traffic;road traffic control;road vehicles;traffic engineering computing","Lane changing;Automated Vehicles;Automated Lane Change system;separate constructs;simplistic assumptions;unplausible assumptions;realistic driving scenarios;complex driving scenarios;rigid assumptions;real-time trajectory re-planning;lane-change decision making;deep Reinforcement Learning algorithm;specially devised trajectory planning model;flexible lane-changing maneuvers;robust, lane-changing maneuvers;safe lane-changing maneuvers;decision-making module","","5","","70","IEEE","3 Mar 2022","","","IEEE","IEEE Journals"
"Feeling of Presence Maximization: mmWave-Enabled Virtual Reality Meets Deep Reinforcement Learning","P. Yang; T. Q. S. Quek; J. Chen; C. You; X. Cao","School of Electronic and Information Engineering, Beihang University, Beijing, China; Information Systems Technology and Design, Singapore University of Technology and Design, Singapore; School of Electronic and Information Engineering, Beihang University, Beijing, China; Information Systems Technology and Design, Singapore University of Technology and Design, Singapore; School of Electronic and Information Engineering, Beihang University, Beijing, China","IEEE Transactions on Wireless Communications","10 Nov 2022","2022","21","11","10005","10019","This paper investigates the problem of providing ultra-reliable and power-efficient virtual reality (VR) experiences for wireless mobile users. To ensure reliable ultra-high-definition (UHD) video frame delivery to mobile users and enhance their immersive visual experiences, a coordinated multipoint (CoMP) transmission technique and millimeter wave (mmWave) communications are exploited. Owing to user movement and time-varying wireless channels, the wireless VR experience enhancement problem is formulated as a sequence-dependent and mixed-integer problem with a goal of maximizing users’ feeling of presence (FoP) in the virtual world, subject to power consumption constraints on access points (APs) and users’ head-mounted displays (HMDs). The problem, however, is hard to be directly solved due to the lack of users’ accurate tracking information and the sequence-dependent and mixed-integer characteristics. To overcome this challenge, we develop a parallel echo state network (ESN) learning method to predict users’ tracking information by training fresh and historical tracking samples separately collected by APs. With the learnt results, we propose a deep reinforcement learning (DRL) based optimization algorithm to solve the formulated problem. In this algorithm, we implement deep neural networks (DNNs) as a scalable solution to produce integer decision variables and solve a continuous power control problem to criticize the integer decision variables. Finally, the performance of the proposed algorithm is compared with various benchmark algorithms, and the impact of different design parameters is also discussed. Simulation results demonstrate that the proposed algorithm is more 4.14% power-efficient than the benchmark algorithms.","1558-2248","","10.1109/TWC.2022.3181674","National Natural Science Foundation of China(grant numbers:91738301,61827901); National Research Foundation, Singapore and Infocomm Media Development Authority, under its Future Communications Research and Development Programme; Singapore University of Technology and Design (SUTD) Growth Plan Grant for AI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9798771","Virtual reality;coordinated multipoint transmission;feeling of presence;parallel echo state network;deep reinforcement learning","Streaming media;Wireless communication;Visualization;Uplink;Servers;Resists;Reliability","deep learning (artificial intelligence);helmet mounted displays;integer programming;mobile radio;reinforcement learning;telecommunication computing;telecommunication network reliability;video communication;virtual reality;wireless channels","access points;benchmark algorithms;continuous power control problem;deep neural networks;deep reinforcement learning;fresh tracking samples;historical tracking samples;immersive visual experiences;integer decision variables;mixed-integer characteristics;mixed-integer problem;optimization algorithm;parallel echo state network learning method;power consumption constraints;reliable ultra high definition video frame delivery;time-varying wireless channels;ultra reliable power efficient;virtual reality meets deep reinforcement learning;virtual world;wireless mobile users;wireless VR experience enhancement problem","","3","","53","IEEE","16 Jun 2022","","","IEEE","IEEE Journals"
"Partially Distributed Channel and Power Management Based on Reinforcement Learning","Z. Jiang; C. Hao; Y. Huang; Q. Wu; F. Zhou","Key Laboratory of Dynamic Cognitive System of Electromagnetic Spectrum Space, Ministry of Industry and Information Technology, Nanjing University of Aeronautics and Astronautics (NUAA), Nanjing 210016, China; Shenzhen Station of State Radio Monitoring Center, Shenzhen 518000, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing 210016, China; Key Laboratory of Dynamic Cognitive System of Electromagnetic Spectrum Space, Ministry of Industry and Information Technology, Nanjing University of Aeronautics and Astronautics (NUAA), Nanjing 210016, China; Key Laboratory of Dynamic Cognitive System of Electromagnetic Spectrum Space, Ministry of Industry and Information Technology, Nanjing University of Aeronautics and Astronautics (NUAA), Nanjing 210016, China","Journal of Communications and Information Networks","23 Dec 2020","2020","5","4","423","437","This paper studies a dynamic multi-user wireless network, where users have no knowledge of the arrival rate and size of data block and suffer from a constraint on long-term average power consumption. Considering such a network, we address the problem of dynamically optimizing channel/power allocation, so as to minimize the long-term average data backlog. The design problem is shown to be a constrained Markov decision process. In order to solve the problem without knowledge on dynamics of the system, we introduce post-decision states and propose a resource allocation algorithm based on reinforcement learning. Since the channel/power allocation problem is coupled, the multiuser decision problem suffers from curses of dimensions (of state/action/outcome space). This makes centralized decision-making and optimization on channel/power allocation suffer from a long convergence time. As a countermeasure, a partially distributed resource allocation framework is proposed. The multiuser power allocation problem is decoupled into single-user decision problems, while channel allocation optimization is performed in a centralized manner. In order to further reduce computational complexity, we propose a low-complexity reinforcement learning method. Simulation results reveal that the proposed algorithm outperforms the state-of-the-art myopic optimizations in terms of energy efficiency and the backlog performance.","2509-3312","","10.23919/JCIN.2020.9306016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9306016","constrained Markov decision processes;multi-user optimization;reinforcement learning;the Internet of things","Resource management;Frequency-domain analysis;Wireless networks;Heuristic algorithms;Dynamic scheduling;Vehicle dynamics;Optimization","","","","3","","","","23 Dec 2020","","","PTP","PTP Journals"
"Adaptive LoRaWAN Transmission exploiting Reinforcement Learning: the Industrial Case","T. Fedullo; A. Morato; F. Tramarin; P. Bellagente; P. Ferrari; E. Sisinni","Dept. of Management and Engineering, University of Padova, Italy; Dept. of Information Engineering, CMZ Sistemi Elettronici s. r.l., University of Padova, Italy; Department of Engineering, University of Modena and Reggio Emilia, Modena, Italy; Dept. of Information Engineering, University of Brescia, Brescia, Italy; Dept. of Information Engineering, University of Brescia, Brescia, Italy; Dept. of Information Engineering, University of Brescia, Brescia, Italy","2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT)","27 Jul 2021","2021","","","671","676","Wireless technologies play a key role in the Industrial Internet of Things (IIoT) scenario, for the development of increasingly flexible and interconnected factory systems. A significant opportunity in this context is represented by the advent of Low Power Wide Area Network (LPWAN) wireless technologies, that enable a reliable, secure, and effective transmission of measurement data over long communication ranges and with very low power consumption. Nevertheless, reliability in harsh environments (as typically occurs in the industrial scenario) is a significant issue to deal with. Focusing on LoRaWAN, adaptive strategies can be profitably devised concerning the above tradeoff. To this aim, this paper proposes to exploit Reinforcement Learning (RL) techniques to design an adaptive LoRaWAN strategy for industrial applications. The RL is spreading in many fields since it allows the design of intelligent systems using a stochastic discrete-time system approach. The proposed technique has been implemented within a purposely designed simulator, allowing to draw a preliminary performance assessment in a real-world scenario. A high density of independent nodes per square km has been considered, showing a significant improvement (about 10%) of the overall reliability in terms of data extraction rate (DER) without compromising full compatibility with the standard specifications.","","978-1-6654-1980-2","10.1109/MetroInd4.0IoT51437.2021.9488498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9488498","Reinforcement Learning;LoRa;LPWANs;ADR;Machine Learning;Artificial Intelligence","Wireless communication;Power measurement;Power demand;Reinforcement learning;Production facilities;Reliability;Communication system security","computer network reliability;computer network security;data communication;discrete time systems;Internet of Things;learning (artificial intelligence);power consumption;radio networks;stochastic systems;telecommunication power management;wide area networks","adaptive LoRaWAN transmission;interconnected factory systems;low power wide area network wireless technology;reliable transmission;secure transmission;effective transmission;measurement data;long communication ranges;low power consumption;harsh environments;reinforcement learning technique;RL technique;adaptive LoRaWAN strategy;industrial applications;intelligent systems;purposely designed simulator;flexible factory systems;industrial Internet of Things scenario;data extraction rate;IIoT system;LPWAN wireless technology;stochastic discrete-time system approach;DER reliability","","5","","12","IEEE","27 Jul 2021","","","IEEE","IEEE Conferences"
"Visual Spatial Attention and Proprioceptive Data-Driven Reinforcement Learning for Robust Peg-in-Hole Task Under Variable Conditions","A. Y. Yasutomi; H. Ichiwara; H. Ito; H. Mori; T. Ogata","R&D Group, Hitachi, Ltd, Hitachinaka, Japan; R&D Group, Hitachi, Ltd, Hitachinaka, Japan; R&D Group, Hitachi, Ltd, Hitachinaka, Japan; Future Robotics Organization, Waseda University, Tokyo, Japan; Graduate School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan","IEEE Robotics and Automation Letters","16 Feb 2023","2023","8","3","1834","1841","Anchor-bolt insertion is a peg-in-hole task performed in the construction field for holes in concrete. Efforts have been made to automate this task, but the variable lighting and hole surface conditions, as well as the requirements for short setup and task execution time make the automation challenging. In this study, we introduce a vision and proprioceptive data-driven robot control model for this task that is robust to challenging lighting and hole surface conditions. This model consists of a spatial attention point network (SAP) and a deep reinforcement learning (DRL) policy that are trained jointly end-to-end to control the robot. The model is trained in an offline manner, with a sample-efficient framework designed to reduce training time and minimize the reality gap when transferring the model to the physical world. Through evaluations with an industrial robot performing the task in 12 unknown holes, starting from 16 different initial positions, and under three different lighting conditions (two with misleading shadows), we demonstrate that SAP can generate relevant attention points of the image even in challenging lighting conditions. We also show that the proposed model enables task execution with higher success rate and shorter task completion time than various baselines. Due to the proposed model's high effectiveness even in severe lighting, initial positions, and hole conditions, and the offline training framework's high sample-efficiency and short training time, this approach can be easily applied to construction.","2377-3766","","10.1109/LRA.2023.3243526","Hitachi, Ltd.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10040732","Robotics and automation in construction;reinforcement learning;deep learning for visual perception","Task analysis;Robots;Feature extraction;Training;Lighting;Robot kinematics;Visualization","deep learning (artificial intelligence);industrial robots;learning (artificial intelligence);reinforcement learning","12 unknown holes;16 different initial positions;anchor-bolt insertion;challenging lighting conditions;construction field;deep reinforcement learning policy;hole conditions;hole surface conditions;industrial robot;offline training framework;proprioceptive data-driven reinforcement learning;proprioceptive data-driven robot control model;relevant attention points;robust peg-in-hole task;sample-efficient framework;SAP;severe lighting;short setup;short training time;shorter task completion time;spatial attention point network;task execution;variable conditions;variable lighting;visual spatial attention","","2","","26","IEEE","8 Feb 2023","","","IEEE","IEEE Journals"
"Reconfiguration of Distribution Networks for Resilience Enhancement: A Deep Reinforcement Learning-based Approach","M. Gautam; M. Abdelmalak; M. MansourLakouraj; M. Benidris; H. Livani","Department of Electrical & Biomedical Engineering, University of Nevada, Reno; Department of Electrical & Biomedical Engineering, University of Nevada, Reno; Department of Electrical & Biomedical Engineering, University of Nevada, Reno; Department of Electrical & Biomedical Engineering, University of Nevada, Reno; Department of Electrical & Biomedical Engineering, University of Nevada, Reno","2022 IEEE Industry Applications Society Annual Meeting (IAS)","17 Nov 2022","2022","","","1","6","This paper proposes a deep reinforcement learning (DRL)-based approach for optimal Reconfiguration of Distribution Networks to improve their Resilience (R-DNR) against extreme events and multiple line outages. The objective of the proposed framework is to minimize the amount of critical load curtailments. The distribution network is represented as a graph network, and the optimal network configuration is obtained by searching for the optimal spanning forest. The constraints to the optimization problem are the radial topology constraint and the power balance constraints. Unlike existing analytical and population-based approaches, which require the entire analysis and computation to be repeated to find the optimal network configuration for each system operating state, DRL-based R-DNR, once properly trained, can quickly determine optimal or near-optimal configuration even when system states change. The proposed R-DNR forms microgrids with distributed energy resources to reduce the critical load curtailment when multiple line outages occur in the system because of extreme events. The proposed DRL-based model learns the action-value function utilizing Q-learning, which is a model-free reinforcement learning technique. A case study on a 33-node distribution test system demonstrates the effectiveness and efficacy of the proposed approach for R-DNR.","2576-702X","978-1-6654-7815-1","10.1109/IAS54023.2022.9939854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9939854","Deep Q Network;distribution system;network reconfiguration;reinforcement learning;resilience","Q-learning;Network topology;Industry applications;Distribution networks;Microgrids;Forestry;Topology","deep learning (artificial intelligence);distributed power generation;graph theory;optimisation;power distribution reliability;power engineering computing;reinforcement learning","33-node distribution test system;action-value function;critical load curtailment;deep reinforcement learning-based approach;distributed energy resources;distribution network;DRL-based model;DRL-based R-DNR;graph network;microgrids;model-free reinforcement learning technique;multiple line outages;near-optimal configuration;optimal network configuration;optimal Reconfiguration;optimal spanning forest;optimization problem;population-based approaches;power balance constraints;Q-learning;radial topology constraint;Resilience enhancement","","1","","21","IEEE","17 Nov 2022","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning-based Approach to Post-Disaster Routing of Movable Energy Resources","M. Gautam; N. Bhusal; M. Benidris","Department of Electrical & Biomedical Engineering, University of Nevada, Reno; Department of Electrical & Biomedical Engineering, University of Nevada, Reno; Department of Electrical & Biomedical Engineering, University of Nevada, Reno","2022 IEEE Industry Applications Society Annual Meeting (IAS)","17 Nov 2022","2022","","","1","6","After the occurrence of an extreme event, movable energy resources (MERs) can be an effective way to restore criti-cal loads to enhance power system resilience when no other forms of energy sources are available. Since the optimal locations of MERs after an extreme event are dependent on system operating states (e.g., loads at each node, on/off status of system branches, etc.), existing analytical and population-based approaches must repeat the entire analysis and computation when the system operating states change. Conversely, deep reinforcement learning (DRL)-based approaches can quickly determine optimal or near-optimal locations despite changes in system states if they are adequately trained with a variety of scenarios. The optimal deployment of MERs to improve power system resilience is proposed using a Deep Q-Learning-based approach. If they are available, MERs can also be used to supplement other types of resources. Following an extreme event, the proposed approach operates in two stages. The distribution network is modeled as a graph in the first stage, and Kruskal's spanning forest search algorithm (KSFSA) is used to reconfigure the network using tie-switches. The optimal or near-optimal locations of MERs are determined in the second stage to maximize critical load recovery. A case study on a 33-node distribution test system demonstrates the effectiveness and efficacy of the proposed approach for post-disaster routing of MERs.","2576-702X","978-1-6654-7815-1","10.1109/IAS54023.2022.9940073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9940073","Deep Q Network;distribution system;movable energy resources;reinforcement learning;resilience","Energy resources;Industry applications;Distribution networks;Forestry;Reinforcement learning;Microgrids;Routing","disasters;distributed power generation;distribution networks;learning (artificial intelligence);optimisation;power system restoration;search problems","33-node distribution test system;criti-cal loads;critical load recovery;Deep Q-Learning-based approach;Deep reinforcement Learning-based approach;deep reinforcement learning-based approaches;energy sources;entire analysis;extreme event;MERs;movable energy resources;near-optimal locations;optimal deployment;population-based approaches;post-disaster routing;power system resilience;system branches;system operating states change;system states","","1","","17","IEEE","17 Nov 2022","","","IEEE","IEEE Conferences"
"Position-Agnostic Autonomous Navigation in Vineyards with Deep Reinforcement Learning","M. Martini; S. Cerrato; F. Salvetti; S. Angarano; M. Chiaberge","Department of Electronics and Telecommunications, Politecnico di Torino, Turin, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Turin, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Turin, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Turin, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Turin, Italy","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","477","484","Precision agriculture is rapidly attracting research to efficiently introduce automation and robotics solutions to support agricultural activities. Robotic navigation in vineyards and orchards offers competitive advantages in autonomously monitoring and easily accessing crops for harvesting, spraying and performing time-consuming necessary tasks. Nowadays, autonomous navigation algorithms exploit expensive sensors which also require heavy computational cost for data processing. Nonetheless, vineyard rows represent a challenging outdoor scenario where GPS and Visual Odometry techniques often struggle to provide reliable positioning information. In this work, we combine Edge AI with Deep Reinforcement Learning to propose a cutting-edge lightweight solution to tackle the problem of autonomous vineyard navigation with-out exploiting precise localization data and overcoming task-tailored algorithms with a flexible learning-based approach. We train an end-to-end sensorimotor agent which directly maps noisy depth images and position-agnostic robot state information to velocity commands and guides the robot to the end of a row, continuously adjusting its heading for a collision-free central trajectory. Our extensive experimentation in realistic simulated vineyards demonstrates the effectiveness of our solution and the generalization capabilities of our agent.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926582","Politecnico di Torino; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926582","","Automation;Navigation;Reinforcement learning;Robot sensing systems;Agriculture;Trajectory;Task analysis","agriculture;distance measurement;Global Positioning System;learning (artificial intelligence);mobile robots;navigation;path planning;robot vision","position-agnostic autonomous navigation;Deep Reinforcement Learning;precision agriculture;robotics solutions;agricultural activities;robotic navigation;competitive advantages;autonomously monitoring;time-consuming necessary tasks;autonomous navigation algorithms;expensive sensors;heavy computational cost;data processing;vineyard rows;challenging outdoor scenario;Visual Odometry techniques;reliable positioning information;Edge AI;cutting-edge lightweight solution;autonomous vineyard navigation;precise localization data;overcoming task-tailored algorithms;flexible learning-based approach;end-to-end sensorimotor agent;maps noisy depth images;position-agnostic robot state information;realistic simulated vineyards","","9","","40","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"MA-Opt: Reinforcement Learning-based Analog Circuit Optimization using Multi-Actors","Y. Choi; M. Choi; K. Lee; S. Kang","Department of EE, POSTECH, Pohang, South Korea; Department of EE, POSTECH, Pohang, South Korea; Department of EE, POSTECH, Pohang, South Korea; Department of EE, POSTECH, Pohang, South Korea","2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)","2 Jun 2023","2023","","","1","5","Analog circuit design requires significant human efforts and expertise; therefore, electronic design automation (EDA) tools for analog design are needed. This study presents MA-Opt that is an analog circuit optimizer using reinforcement learning (RL)-inspired framework. MA-Opt using multiple actors is proposed to provide various predictions of optimized circuit designs in parallel. Sharing a specific memory that affects the loss function of network training is proposed to exploit multiple actors effectively, accelerating circuit optimization. Moreover, we devise a novel method to tune the most optimized design in previous simulations into a more optimized design. To demonstrate the efficiency of the proposed framework, MA-Opt was simulated for three analog circuits and the results were compared with those of other methods. The experimental results indicated the strength of using multiple actors with a shared elite solution set and the near-sampling method. Within the same number of simulations, while satisfying all given constraints, MA-Opt obtained minimum target metrics up to 24% better than DNN-Opt. Furthermore, MA-Opt obtained better Figure of Merits (FoMs) than DNN-Opt at the same runtime.","1558-1101","979-8-3503-9624-9","10.23919/DATE56975.2023.10136894","IITP; IC Design Education Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10136894","Analog circuit optimization;RL-inspired;multiple actors;shared elite solution set;near-sampling method","Training;Measurement;Runtime;Circuit optimization;Design automation;Reinforcement learning;Analog circuits","","","","","","21","","2 Jun 2023","","","IEEE","IEEE Conferences"
"Autonomous Navigation of Wheel Loaders using Task Decomposition and Reinforcement Learning","C. Borngrund; U. Bodin; F. Sandin; H. Andreasson","EISLAB, Luleå University of Technology, Luleå, Sweden; EISLAB, Luleå University of Technology, Luleå, Sweden; EISLAB, Luleå University of Technology, Luleå, Sweden; Centre for Applied Autonomous Sensor Systems, Örebro University, Sweden","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","8","The short-loading cycle is a repetitive task performed in high quantities making it a good candidate for automation. Expert operators perform this task to upkeep high productivity while minimizing the environmental impact of using energy to propel the wheel loader. The need to balance productivity and environmental performance is essential for the sub-task of navigating the wheel loader between the pile of material and a dump truck receiving the material. This task is further complicated by behaviours of the wheel loader such as wheel slip depending on the tire-to-surface friction that is hard to model. Such uncertainties motivate the use of data-driven and adaptable approaches like reinforcement learning to automate navigation. In this paper, we examine the possibility to use reinforcement learning for the navigation sub-task. We focus on the process of developing a solution to the complete sub-task by decomposing it into two distinct steps and training two different agents to perform them separately. These steps are reversing from the pile and approaching the dump truck. The agents are trained in a simulation environment in which the wheel loader is modelled. Our results indicate that task decomposition can be helpful in performing the navigation compared to training a single agent for the entire sub-task. We present unsuccessful experiments using a single agent for the entire sub-task to illustrate difficulties associated with such an approach. A video of the results is available online11Video available at https://youtu.be/IZbgvHvSltI.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260481","","Training;Productivity;Automation;Uncertainty;Navigation;Wheels;Reinforcement learning","friction;learning (artificial intelligence);mobile robots;multi-agent systems;path planning;reinforcement learning;road vehicles;tyres;wheels","autonomous navigation;complete sub-task;dump truck;entire sub-task;environmental performance;productivity;reinforcement learning;repetitive task;task decomposition;wheel loader;wheel slip","","","","30","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Learning to Schedule Job-Shop Problems via Hierarchical Reinforcement Learning","Z. Liao; Q. Li; Y. Dai; Z. Zhang","School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Department of Computer Science, Brown University, Providence, RI, USA; School of Software Engineering, Sun Yat-sen University, Zhuhai, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China","2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","18 Nov 2022","2022","","","3222","3227","The job-shop scheduling problem (JSSP) is a classic combinatorial optimization problem in the areas of computer science and operations research. It is closely associated with many industrial scenarios. In today’s society, the demand for efficient and stable scheduling algorithms has significantly increased. More and more researchers have recently tried new methods to solve JSSP. In this paper, we effectively formulate the scheduling process of JSSP as a Semi-Markov Decision Process. We then propose a method of using hierarchical reinforcement learning with graph neural networks to solve JSSP. We also demonstrate that larger-sized instances require the support of a bigger number of sub-policies and different scheduling phases require using different sub-policies.","2577-1655","978-1-6654-5258-8","10.1109/SMC53654.2022.9945585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9945585","Hierarchical Reinforcement Learning;Job Shop Scheduling Problem;Graph Neural Network","Computer science;Schedules;Job shop scheduling;Operations research;Scheduling algorithms;Reinforcement learning;Graph neural networks","graph theory;job shop scheduling;Markov processes;neural nets;optimisation;production engineering computing;reinforcement learning","classic combinatorial optimization problem;graph neural network;hierarchical reinforcement learning;job-shop scheduling problem;JSSP;semiMarkov decision process","","1","","40","IEEE","18 Nov 2022","","","IEEE","IEEE Conferences"
"Optimizing Network Slicing in Distributed Large Scale Infrastructures: From Heuristics to Controlled Deep Reinforcement Learning","J. J. A. Esteves; A. Boubendir; F. Guillemin; P. Sens","Mercado Livre, Brazil; Airbus Defence and Space, France; Orange Labs, France; Sorbonne Université / CNRS / Inria, France","NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium","21 Jun 2023","2023","","","1","6","This paper summarizes the PhD thesis and the 10 associated publications on the optimization of network slice placement in large-scale distributed infrastructures by focusing on online heuristics and approaches based on Deep Reinforcement Learning (DRL). First, we rely on Integer Linear Programming (ILP) to propose a data model for on-Edge and on-network slice placement. Second, we leverage an approach called Power of Two Choices (P2C) to propose an online heuristic adapted to support placement on large-scale distributed infrastructures while incorporating Edge-specific constraints like latency. Finally, we investigate the use of Machine Learning (ML) methods, specifically DRL, to increase the scalability and automation of network slice placement by considering a multi-objective optimization approach to the problem. We will go through the extensive evaluation work that provide encouraging results about the advantages of the proposed approaches when used in realistic network scenarios.","2374-9709","978-1-6654-7716-1","10.1109/NOMS56928.2023.10154353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10154353","Network Functions Virtualization;Network Slicing;Placement;Large-scale infrastructures;Optimization;Heuristics;Automation;Deep Reinforcement Learning.","Deep learning;Automation;Scalability;Network slicing;Focusing;Reinforcement learning;Integer linear programming","deep learning (artificial intelligence);integer programming;learning (artificial intelligence);linear programming;optimisation;reinforcement learning;telecommunication computing","10 associated publications;controlled Deep Reinforcement Learning;distributed large scale infrastructures;DRL;Edge-specific constraints;Integer Linear Programming;large-scale distributed infrastructures;multiobjective optimization approach;network slicing;on-network slice placement;online heuristics;PhD thesis;realistic network scenarios","","","","26","IEEE","21 Jun 2023","","","IEEE","IEEE Conferences"
"RTAW: An Attention Inspired Reinforcement Learning Method for Multi-Robot Task Allocation in Warehouse Environments","A. Agrawal; A. S. Bedi; D. Manocha","The Department of Computer Science, University of Maryland, College Park, MD, USA; The Department of Computer Science, University of Maryland, College Park, MD, USA; The Department of Computer Science, University of Maryland, College Park, MD, USA","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","1393","1399","We present a novel reinforcement learning based algorithm for multi-robot task allocation problem in ware-house environments. We formulate it as a Markov Decision Process and solve via a novel deep multi-agent reinforcement learning method (called RTAW) with attention inspired policy architecture. Hence, our proposed policy network uses global embeddings that are independent of the number of robots/tasks. We utilize proximal policy optimization algorithm for training and use a carefully designed reward to obtain a converged policy. The converged policy ensures cooperation among different robots to minimize total travel delay (TTD) which ultimately improves the makespan for a sufficiently large task-list. In our extensive experiments, we compare the performance of our RTAW algorithm to state of the art methods such as myopic pickup distance minimization (greedy) and regret based baselines on different navigation schemes. We show an improvement of upto 14% (25–1000 seconds) in TTD on scenarios with hundreds or thousands of tasks for different challenging warehouse layouts and task generation schemes. We also demonstrate the scalability of our approach by showing performance with up to 1000 robots in simulations.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161310","ARO(grant numbers:W911NF1910069,W911NF2110026); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161310","","Training;Navigation;Scalability;Layout;Reinforcement learning;Minimization;Resource management","control engineering computing;deep learning (artificial intelligence);Markov processes;multi-agent systems;multi-robot systems;optimisation;reinforcement learning;warehouse automation","attention inspired policy architecture;deep multiagent reinforcement learning method;global embeddings;Markov decision process;multirobot task allocation problem;myopic pickup distance minimization;proximal policy optimization algorithm;RTAW algorithm;task generation schemes;TTD;warehouse environments","","","","36","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Toward a Superintelligent Action Recommender for Network Operation Centers Using Reinforcement Learning","S. Altamimi; B. Altamimi; D. Côté; S. Shirmohammadi","School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada; Blue Planet Analytics, Ciena Corporation, Ottawa, ON, Canada; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada","IEEE Access","2 Mar 2023","2023","11","","20216","20229","Today’s Network Operation Centres (NOC) consist of teams of network professionals responsible for monitoring and taking actions for their network’s health. Most of these NOC actions are relatively complex and executed manually; only the simplest tasks can be automated with rules-based software. But today’s networks are getting larger and more complex. Therefore, deciding what action to take in the face of non-trivial problems has essentially become an art that depends on collective human intelligence of NOC technicians, specialized support teams organized by technology domains, and vendors’ technical support. But this model is getting increasingly expensive and inefficient; hence, the automation of all or at least some NOC tasks is now considered a desirable step towards autonomous and self-healing networks. In this article, we investigate whether an autonomous NOC can achieve superintelligence; i.e., recommend or take actions that lead to better results than those achieved by rules designed by human experts. Our investigation is inspired by the superintelligence achieved in computer games recently. Specifically, we build an Action Recommendation Engine using Reinforcement Learning, train it with expert rules, and let it explore actions by itself. We then show that it can learn new and more efficient strategies that outperform expert rules designed by humans. This can be used in the face of network problems to either quickly recommend actions to NOC technicians or autonomously take actions for fast recovery.","2169-3536","","10.1109/ACCESS.2023.3248652","Mitacs Accelerate Cluster(grant numbers:IT12571); Ciena Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10051839","Network automation;network operation center;reinforcement learning;self-healing networks","Automation;Reinforcement learning;Task analysis;Data models;Quality of experience;Routing","computer games;recommender systems;reinforcement learning","autonomous NOC technicians;collective human intelligence;network operation centers;nontrivial problems;reinforcement learning;rules-based software;self-healing networks;superintelligent action recommendation engine;vendors","","","","28","CCBYNCND","24 Feb 2023","","","IEEE","IEEE Journals"
"Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones","B. Thananjeyan; A. Balakrishna; S. Nair; M. Luo; K. Srinivasan; M. Hwang; J. E. Gonzalez; J. Ibarz; C. Finn; K. Goldberg","Department of Electrical Engineering and Computer Science, University of California, Santa Clara, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Santa Clara, CA, USA; Department of Computer Science, Stanford University, Stanford, CA, U.S.; Department of Electrical Engineering and Computer Science, University of California, Santa Clara, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Santa Clara, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Santa Clara, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Santa Clara, CA, USA; Google AI, Sunnyvale, CA, U.S.; Department of Computer Science, Stanford University, Stanford, CA, U.S.; Department of Electrical Engineering and Computer Science, University of California, Santa Clara, CA, USA","IEEE Robotics and Automation Letters","23 Apr 2021","2021","6","3","4915","4922","Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 2–20 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.","2377-3766","","10.1109/LRA.2021.3070252","AUTOLAB at UC Berkeley in affiliation with the Berkeley AI Research (BAIR) Lab; Real-Time Intelligent Secure Execution (RISE) Lab; Google Brain Robotics; Stanford AI Research Lab.; SAIL-Toyota Research initiative; Scalable Collaborative Human-Robot Learning (SCHooL) Project; NSF National Robotics Initiative(grant numbers:1734633); Google; Siemens; Amazon Robotics; Toyota Research Institute; Nvidia; NSF GRFPs; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9392290","Reinforcement learning;safety","Safety;Navigation;Optimization;Reinforcement learning","collision avoidance;intelligent robots;learning systems;mobile robots;optimisation;reinforcement learning;robot vision;safety","image-based obstacle avoidance task;image-based navigation task;contact-rich manipulation tasks;recovery policy;task policy;task performance;policy learning;learned recovery zones;constraint violation;Recovery RL","","50","","34","IEEE","31 Mar 2021","","","IEEE","IEEE Journals"
"Learning Variable Impedance Control via Inverse Reinforcement Learning for Force-Related Tasks","X. Zhang; L. Sun; Z. Kuang; M. Tomizuka","Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Research Institute of Intelligent Control and Systems, Harbin Institute of Technology, Harbin, China; Department of Mechanical Engineering, University of California, Berkeley, CA, USA","IEEE Robotics and Automation Letters","11 Mar 2021","2021","6","2","2225","2232","Many manipulation tasks require robots to interact with unknown environments. In such applications, the ability to adapt the impedance according to different task phases and environment constraints is crucial for safety and performance. Although many approaches based on deep reinforcement learning (RL) and learning from demonstration (LfD) have been proposed to obtain variable impedance skills on contact-rich manipulation tasks, these skills are typically task-specific and could be sensitive to changes in task settings. This letter proposes an inverse reinforcement learning (IRL) based approach to recover both the variable impedance policy and reward function from expert demonstrations. We explore different action space of the reward functions to achieve a more general representation of expert variable impedance skills. Experiments on two variable impedance tasks (Peg-in-Hole and Cup-on-Plate) were conducted in both simulations and on a real FANUC LR Mate 200iD/7 L industrial robot. The comparison results with behavior cloning and force-based IRL proved that the learned reward function in the gain action space has better transferability than in the force space. Experiment videos are available at https://msc.berkeley.edu/research/impedance-irl.html.","2377-3766","","10.1109/LRA.2021.3061374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361101","Compliance and impedance control;learning from demonstration;machine learning for robot control","Impedance;Task analysis;Robots;Aerospace electronics;Trajectory;Force;Reinforcement learning","deep learning (artificial intelligence);force control;learning by example;manipulators;robot programming","deep reinforcement learning;contact rich manipulation;inverse reinforcement learning;force related tasks;variable impedance control learning;robot interaction;learning from demonstration","","32","","21","IEEE","23 Feb 2021","","","IEEE","IEEE Journals"
"Learn-to-Recover: Retrofitting UAVs with Reinforcement Learning-Assisted Flight Control Under Cyber-Physical Attacks","F. Fei; Z. Tu; D. Xu; X. Deng","School of Mechanical Engineering, Purdue University; School of Mechanical Engineering, Purdue University; Department of Computer Science, Purdue University; School of Mechanical Engineering, Purdue University","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","7358","7364","In this paper, we present a generic fault-tolerant control (FTC) strategy via reinforcement learning (RL). We demonstrate the effectiveness of this method on quadcopter unmanned aerial vehicles (UAVs). The fault-tolerant control policy is trained to handle actuator and sensor fault/attack. Unlike traditional FTC, this policy does not require fault detection and diagnosis (FDD) nor tailoring the controller for specific attack scenarios. Instead, the policy is running simultaneously alongside the stabilizing controller without the need for on- detection activation. The effectiveness of the policy is compared with traditional active and passive FTC strategies against actuator and sensor faults. We compare their performance in position control tasks via simulation and experiments on quadcopters. The result shows that the strategy can effectively tolerate different types of attacks/faults and maintain the vehicle's position, outperforming the other two methods.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196611","","Actuators;Fault tolerance;Fault tolerant systems;Vehicle dynamics;Learning (artificial intelligence);Training;Solid modeling","actuators;aerospace computing;autonomous aerial vehicles;control engineering computing;fault diagnosis;fault tolerant control;helicopters;learning (artificial intelligence);position control;security of data;stability","fault-tolerant control policy;actuator;stabilizing controller;detection activation;sensor faults;position control;learn-to-recover;UAVs;reinforcement learning-assisted flight control;cyber-physical attacks;quadcopter unmanned aerial vehicles;sensor attack","","26","","35","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Comparing Task Simplifications to Learn Closed-Loop Object Picking Using Deep Reinforcement Learning","M. Breyer; F. Furrer; T. Novkovic; R. Siegwart; J. Nieto","Autonomous Systems Lab, ETH, Zurich, Switzerland; Autonomous Systems Lab, ETH, Zurich, Switzerland; Autonomous Systems Lab, ETH, Zurich, Switzerland; Autonomous Systems Lab, ETH, Zurich, Switzerland; Autonomous Systems Lab, ETH, Zurich, Switzerland","IEEE Robotics and Automation Letters","20 Feb 2019","2019","4","2","1549","1556","Enabling autonomous robots to interact in unstructured environments with dynamic objects requires manipulation capabilities that can deal with clutter, changes, and objects' variability. This letter presents a comparison of different reinforcement learning-based approaches for object picking with a robotic manipulator. We learn closed-loop policies mapping depth camera inputs to motion commands and compare different approaches to keep the problem tractable, including reward shaping, curriculum learning, and using a policy pre-trained on a task with a reduced action set to warm-start the full problem. For efficient and more flexible data collection, we train in simulation and transfer the policies to a real robot. We show that using curriculum learning, policies learned with a sparse reward formulation can be trained at similar rates as with a shaped reward. These policies result in success rates comparable to the policy initialized on the simplified task. We could successfully transfer these policies to the real robot with only minor modifications of the depth image filtering. We found that using a heuristic to warm-start the training was useful to enforce desired behavior, while the policies trained from scratch using a curriculum learned better to cope with unseen scenarios where objects are removed.","2377-3766","","10.1109/LRA.2019.2896467","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; Luxembourg National Research Fund(grant numbers:12571953); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630008","Grasping;visual servoing;deep reinforcement learning;curriculum learning","Task analysis;Training;Robots;Grippers;Cameras;Data models;Reinforcement learning","closed loop systems;image filtering;learning (artificial intelligence);manipulators;motion control;robot programming;robot vision","task simplifications;object picking;deep reinforcement learning;manipulation capabilities;robotic manipulator;closed-loop policies;reward shaping;curriculum learning;autonomous robots;reinforcement learning;data collection;depth camera inputs;depth image filtering","","24","","46","IEEE","30 Jan 2019","","","IEEE","IEEE Journals"
"Learning Kinematic Feasibility for Mobile Manipulation Through Deep Reinforcement Learning","D. Honerkamp; T. Welschehold; A. Valada","Department of Computer Science, University of Freiburg, Germany; Department of Computer Science, University of Freiburg, Germany; Department of Computer Science, University of Freiburg, Germany","IEEE Robotics and Automation Letters","20 Jul 2021","2021","6","4","6289","6296","Mobile manipulation tasks remain one of the critical challenges for the widespread adoption of autonomous robots in both service and industrial scenarios. While planning approaches are good at generating feasible whole-body robot trajectories, they struggle with dynamic environments as well as the incorporation of constraints given by the task and the environment. On the other hand, dynamic motion models in the action space struggle with generating kinematically feasible trajectories for mobile manipulation actions. We propose a deep reinforcement learning approach to learn feasible dynamic motions for a mobile base while the end-effector follows a trajectory in task space generated by an arbitrary system to fulfill the task at hand. This modular formulation has several benefits: it enables us to readily transform a broad range of end-effector motions into mobile applications, it allows us to use the kinematic feasibility of the end-effector trajectory as a dense reward signal and its modular formulation allows it to generalise to unseen end-effector motions at test time. We demonstrate the capabilities of our approach on multiple mobile robot platforms with different kinematic abilities and different types of wheeled platforms in extensive simulated as well as real-world experiments.","2377-3766","","10.1109/LRA.2021.3092685","European Union's Horizon 2020 Research and Innovation Program(grant numbers:871449-OpenDR); Eva Mayr-Stihl Stiftung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9465645","Mobile manipulation;reinforcement learning","End effectors;Task analysis;Kinematics;Robots;Trajectory;Collision avoidance;Planning","collision avoidance;end effectors;learning (artificial intelligence);manipulators;mobile robots;multi-robot systems;path planning","kinematic feasibility;mobile manipulation tasks;autonomous robots;industrial scenarios;planning approaches;whole-body robot trajectories;dynamic environments;dynamic motion models;action space struggle;kinematically feasible trajectories;mobile manipulation actions;deep reinforcement learning approach;feasible dynamic motions;mobile base;task space;modular formulation;mobile applications;end-effector trajectory;unseen end-effector motions;multiple mobile robot platforms;different kinematic abilities","","19","","25","IEEE","25 Jun 2021","","","IEEE","IEEE Journals"
"Learning Generalizable Locomotion Skills with Hierarchical Reinforcement Learning","T. Li; N. Lambert; R. Calandra; F. Meier; A. Rai","Facebook, Menlo Park, CA, USA; Work done during an internship at Facebook AI Research, University of California, Berkeley, USA; Facebook, Menlo Park, CA, USA; Facebook, Menlo Park, CA, USA; Facebook, Menlo Park, CA, USA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","413","419","Learning to locomote to arbitrary goals on hardware remains a challenging problem for reinforcement learning. In this paper, we present a hierarchical framework that improves sample-efficiency and generalizability of learned locomotion skills on real-world robots. Our approach divides the problem of goal-oriented locomotion into two sub-problems: learning diverse primitives skills, and using model-based planning to sequence these skills. We parametrize our primitives as cyclic movements, improving sample-efficiency of learning from scratch on a 18 degrees of freedom robot. Then, we learn coarse dynamics models over primitive cycles and use them in a model predictive control framework. This allows us to learn to walk to arbitrary goals up to 12m away, after about two hours of training from scratch on hardware. Our results on a Daisy hexapod hardware and simulation demonstrate the efficacy of our approach at reaching distant targets, in different environments, and with sensory noise.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196642","","Hardware;Legged locomotion;Training;Task analysis;Planning;Heuristic algorithms","learning (artificial intelligence);legged locomotion;path planning;predictive control;robot dynamics;robot kinematics","generalizable locomotion skills;hierarchical reinforcement learning;arbitrary goals;hierarchical framework;sample-efficiency;generalizability;learned locomotion skills;real-world robots;goal-oriented locomotion;diverse primitives skills;freedom robot;coarse dynamics models;primitive cycles;model predictive control framework;Daisy hexapod hardware;size 12.0 m","","18","","40","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Tactile Robotics: Learning to Type on a Braille Keyboard","A. Church; J. Lloyd; R. Hadsell; N. F. Lepora","Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Google DeepMind, London, U.K.; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.","IEEE Robotics and Automation Letters","5 Aug 2020","2020","5","4","6145","6152","Artificial touch would seem well-suited for Reinforcement Learning (RL), since both paradigms rely on interaction with an environment. Here we propose a new environment and set of tasks to encourage development of tactile reinforcement learning: learning to type on a braille keyboard. Four tasks are proposed, progressing in difficulty from arrow to alphabet keys and from discrete to continuous actions. A simulated counterpart is also constructed by sampling tactile data from the physical environment. Using state-of-the-art deep RL algorithms, we show that all of these tasks can be successfully learnt in simulation, and 3 out of 4 tasks can be learned on the real robot. A lack of sample efficiency currently makes the continuous alphabet task impractical on the robot. To the best of our knowledge, this work presents the first demonstration of successfully training deep RL agents in the real world using observations that exclusively consist of tactile images. To aid future research utilising this environment, the code for this project has been released along with designs of the braille keycaps for 3D printing and a guide for recreating the experiments.","2377-3766","","10.1109/LRA.2020.3010461","Leverhulme Trust(grant numbers:RL-2016-39); EPSRC CASE Award; Google DeepMind; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9144378","Force and tactile sensing;reinforecment learning;biomimetics","Task analysis;Tactile sensors;Keyboards;Training","handicapped aids;learning (artificial intelligence);neural nets;tactile sensors","artificial touch;tactile reinforcement learning;Braille keyboard;tactile data;physical environment;deep RL algorithms;sample efficiency;continuous alphabet task;tactile images;deep reinforcement;tactile robotics","","14","","40","IEEE","20 Jul 2020","","","IEEE","IEEE Journals"
