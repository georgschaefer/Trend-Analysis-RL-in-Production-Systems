"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Reinforcement Learning for Operational Space Control","J. Peters; S. Schaal","University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA","Proceedings 2007 IEEE International Conference on Robotics and Automation","21 May 2007","2007","","","2111","2116","While operational space control is of essential importance for robotics and well-understood from an analytical point of view, it can be prohibitively hard to achieve accurate control in face of modeling errors, which are inevitable in complex robots, e.g., humanoid robots. In such cases, learning control methods can offer an interesting alternative to analytical control algorithms. However, the resulting supervised learning problem is ill-defined as it requires to learn an inverse mapping of a usually redundant system, which is well known to suffer from the property of non-convexity of the solution space, i.e., the learning system could generate motor commands that try to steer the robot into physically impossible configurations. The important insight that many operational space control algorithms can be reformulated as optimal control problems, however, allows addressing this inverse learning problem in the framework of reinforcement learning. However, few of the known optimization or reinforcement learning algorithms can be used in online learning control for robots, as they are either prohibitively slow, do not scale to interesting domains of complex robots, or require trying out policies generated by random search, which are infeasible for a physical system. Using a generalization of the EM-based reinforcement learning framework suggested by Dayan and Hinton, we reduce the problem of learning with immediate rewards to a reward-weighted regression problem with an adaptive, integrated reward transformation for faster convergence. The resulting algorithm is efficient, learns smoothly without dangerous jumps in solution space, and works well in applications of complex high degree-of-freedom robots.","1050-4729","1-4244-0601-3","10.1109/ROBOT.2007.363633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4209397","","Orbital robotics;Humanoid robots;Optimal control;Face;Error correction;Algorithm design and analysis;Supervised learning;Learning systems;Robot control;Control systems","adaptive control;learning (artificial intelligence);learning systems;optimal control;regression analysis;robots;search problems","reinforcement learning;operational space control;humanoid robots;supervised learning;learning system;optimal control;inverse learning problem;online learning control;random search;reward-weighted regression problem;adaptive integrated reward transformation","","12","","14","IEEE","21 May 2007","","","IEEE","IEEE Conferences"
"A novel dynamic priority-based action-selection-mechanism integrating a reinforcement learning","Il Hong Suh; Min Jo Kim; Sanghoon Lee; Byung Ju Yi","The Graduate School of Information and Communications, Hanyang University, Seoul, South Korea; The Graduate School of Information and Communications, Hanyang University, Seoul, South Korea; School of E.E.C.S., Hanyang University, Seoul, South Korea; School of E.E.C.S., Hanyang University, Seoul, South Korea","IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004","6 Jul 2004","2004","3","","2639","2646 Vol.3","A novel action-selection-mechanism is proposed to deal with sequential behaviors, where associations between some of stimulus and behaviors would be learned by a shortest-path-finding-based reinforcement learning technique. To be specific, we define behavioral motivation as a primitive node for action selection, and then sequentially construct a network with behavioral motivations. The vertical path of the network represents a behavioral sequence. Here, such a tree for our proposed ASM can be newly generated and/or updated, whenever a new sequential behaviors is learned. To show the validity of our proposed ASM, some experimental results on a ""pushing-box-into-a-goal (PBIG) task"" of a mobile robot is illustrated.","1050-4729","0-7803-8232-3","10.1109/ROBOT.2004.1307459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1307459","","Learning;Animals;Animation;Application specific processors;Artificial intelligence;Actuators;Robot sensing systems;Genetics;Sensor systems;Tree data structures","learning (artificial intelligence);path planning;mobile robots","dynamic priority action selection mechanism;reinforcement learning;sequential behavior;shortest path finding;pushing box into a goal task;mobile robot","","4","","23","IEEE","6 Jul 2004","","","IEEE","IEEE Conferences"
"Adaptive organization of generalized behavioral concepts for autonomous robots: schema-based modular reinforcement learning","T. Taniguchi; T. Sawaragi","Department of Precision Engineering, Graduate School of Engineering, Kyoto University, Kyoto, Japan; Department of Precision Engineering, Graduate School of Engineering, Kyoto University, Kyoto, Japan","2005 International Symposium on Computational Intelligence in Robotics and Automation","12 Dec 2005","2005","","","601","606","In this paper, we introduce a reinforcement learning method for autonomous robots to obtain generalized behavioral concepts. Reinforcement learning is a well formulated method for autonomous robots to obtain a new behavioral concept by themselves. However, these behavioral concepts cannot be applied to other environments that are different from the place where the robots have learned the concepts. On the contrary, we, human beings, can apply our behavioral concepts to some different environments, objects, and/or situations. We think this ability owes to some memory structure like schema system that was originally proposed by J. Piaget. We previously proposed a modular-learning method called Dual-Schemata model. In this paper, we add a reinforcement learning mechanism to this model. By being provided with this structure, autonomous robots become able to obtain new generalized behavioral concepts by themselves. We also show this kind of structure enables autonomous robots to behave appropriately even in a novel socially interactive environment.","","0-7803-9355-4","10.1109/CIRA.2005.1554342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1554342","schema;hierarchical reinforcement learning;modular reinforcement learning;generalizes behavioral concept","Learning;Humans;Intelligent robots;Precision engineering;Central nervous system;Material storage;Cultural differences;Laboratories;Animation;Computational modeling","robots;learning (artificial intelligence);interactive systems","adaptive organization;generalized behavioral concept;autonomous robot;modular reinforcement learning;memory structure;schema system;modular-learning method;Dual-Schemata model;interactive environment;hierarchical reinforcement learning","","2","","12","IEEE","12 Dec 2005","","","IEEE","IEEE Conferences"
"Intelligent Path Planning of Underwater Robot Based on Reinforcement Learning","J. Yang; J. Ni; M. Xi; J. Wen; Y. Li","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China","IEEE Transactions on Automation Science and Engineering","30 Jun 2023","2023","20","3","1983","1996","As one of the commonly used vehicles for underwater detection, underwater robots are facing a series of problems. The real underwater environment is large-scale, complex, real-time and dynamic, and many unknown obstacles may exist in the underwater environment. Under such complex conditions and lack of prior knowledge, the existing path planning methods are difficult to plan, therefore they cannot effectively meet the actual demands. In response to these problems, a three-dimensional marine environment including multiple obstacles is established with the real ocean current data in this paper, which is consistent with the actual application scenarios. Then, we propose an N-step Priority Double DQN (NPDDQN) path planning algorithm, which potently realizes obstacle avoidance in the complex environment. In addition, this study proposes an experience screening mechanism, which screens the explored positive experience and improves its reuse rate, thus efficiently improving the algorithm stability in the dynamic environment. This paper verifies the better performance of reinforcement learning compared with a variety of traditional methods in three-dimensional underwater path planning. Underwater robots based on the proposed method have good autonomy and stability, which provides a new method for path planning of underwater robots. Note to Practitioners—The goal of this study is to provide a new solution for obstacle avoidance in path planning of underwater robots, which is consistent with the dynamic and real-time demands of the real environment. Existing underwater path planning researches lack a consistent environment with the actual application, and therefore we firstly construct a three-dimensional ocean environment with real ocean current data to provide support for the algorithms. Additionally, most of the algorithms are pre-planning methods or require long-time calculation, and there is little research on obstacle avoidance. In the face of obstacle changes, underwater robots with poor adaptability will cause performance decline and even economic losses. The proposed algorithm learns through interaction with the environment, and therefore it does not require any prior experience, and has good adaptability as well as fast inference speed. Especially, in the dynamic environment, algorithm performance is difficult to guarantee due to less positive experience in exploration. The proposed experience screening mechanism improves the stability of the algorithm, so that the underwater robot maintains stable performance in different dynamic environments.","1558-3783","","10.1109/TASE.2022.3190901","National Natural Science Foundation of China(grant numbers:61871283); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9832616","Reinforcement learning;path planning;obstacle avoidance;underwater robot","Path planning;Heuristic algorithms;Vehicle dynamics;Autonomous underwater vehicles;Reinforcement learning;Oceans;Collision avoidance","collision avoidance;mobile robots;path planning;reinforcement learning","complex environment;dynamic environment;existing path planning methods;intelligent path planning;N-step Priority Double DQN path planning algorithm;obstacle avoidance;pre-planning methods;three-dimensional marine environment;three-dimensional ocean environment;three-dimensional underwater path planning;underwater detection;underwater environment;underwater path planning researches;underwater robot","","2","","46","IEEE","18 Jul 2022","","","IEEE","IEEE Journals"
"Adaptive Fault-Tolerant Tracking Control for Affine Nonlinear Systems With Unknown Dynamics via Reinforcement Learning","S. Roshanravan; S. Shamaghdari","Electrical Engineering Department, Iran University of Science and Technology (IUST), Tehran, Iran; Electrical Engineering Department, Iran University of Science and Technology (IUST), Tehran, Iran","IEEE Transactions on Automation Science and Engineering","","2022","PP","99","1","12","This paper investigates the optimal fault-tolerant tracking control (FTTC) problem for unknown affine nonlinear continuous-time systems with process and actuator faults in the framework of reinforcement learning (RL). The proposed novel active FTTC scheme is based on adaptive optimal control theory. In this way, the FTTC problem is formulated as an optimal regulation problem for the augmented system, which consists of the controlled system and the reference trajectory. To solve the Hamilton-Jacobi-Bellman (HJB) equation of the augmented system, an identifier-critic-based online RL strategy is employed with a dual neural network (NN) approximation structure. Initially, in order to remove the requirement of prior knowledge of the system dynamics, an adaptive NN identifier is designed. The forgetting factor in the proposed identifier update law is variable and a function of the filtered state estimation error and filtered state error. Choosing this variable forgetting factor increases the convergence speed and decreases the estimation error of identifier NN weights compared to the constant one while maintaining its robustness. When a fault occurs, the system continues to operate under the former FTTC until the fault is detected. Meanwhile, the optimal FTTC design in the RL framework requires the initial admissible control condition. In order to make it possible to initiate the FTTC learning process from the former FTTC, we employed a stabilizing term in the critical update rule. The Uniformly Ultimately Boundedness (UUB) of identifier and critic NN weight errors and, as a result, the convergence of the control input to the neighborhood of the optimal solution are all proved by Lyapunov theory. In the proposed method, changes in the values of faults are detected by comparing the HJB error to a predefined threshold. Finally, the simulation results are given to validate the effectiveness of the developed method. Note to Practitioners—long-time operations and the influence of external perturbations often make the faults inevitable for many practical engineering systems which can lead to unpredictable behaviors and catastrophic impacts. In general, the faults are naturally uncertain in time, value, and pattern, that is, it is unknown when, how much, and which system components fail. Therefore, the control system must be able to tolerate an extensive set of component faults. The design of optimal model-free FTTC strategies in an adaptive manner is challenging in nonlinear systems. The proposed method is suitable for a large class of nonlinear systems with input-affine form, and guarantees the system stability in the presence of process and actuator faults.","1558-3783","","10.1109/TASE.2022.3223702","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9994664","Fault detection;fault-tolerant tracking control;reinforcement learning;affine nonlinear systems;process and actuator faults","Actuators;Convergence;Artificial neural networks;System dynamics;Optimal control;Nonlinear dynamical systems;Adaptive systems","","","","1","","","IEEE","20 Dec 2022","","","IEEE","IEEE Early Access Articles"
"RL-TEE: Autonomous Probe Guidance for Transesophageal Echocardiography Based on Attention-Augmented Deep Reinforcement Learning","K. Li; A. Li; Y. Xu; H. Xiong; M. Q. . -H. Meng","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, Hong Kong; Department of Ultrasound, Shenzhen Second People’s Hospital, Shenzhen, China; Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","13","Ultrasound image acquisition in conventional transesophageal echocardiography (TEE) requires complex manual operation of the probe in the esophagus based on the interpretation of ultrasound images and in-depth knowledge of the cardiac anatomy. In this work, we formulate the TEE probe guidance task as a reinforcement learning (RL) problem, and present the first learning-based solution to 3-DOF control of a TEE probe based on the ultrasound image feedback, named RL-TEE, in order to mimic the visual search and navigation strategies of expert echocardiographers. The probe-tissue interaction in TEE is carefully modeled in our framework by considering both the requirements for navigation towards the standard views and compliance in the esophageal environment. Furthermore, we propose a hybrid deep Q-network model that augments a convolutional neural network backbone with self-attention mechanisms to better capture spatial information in ultrasound images to guide navigation decisions. The presented methods are preliminarily validated in a TEE simulation environment built with data from 25 subjects to acquire four standard views of the heart. Our results show that the proposed method can effectively learn to accurately and compliantly guide the probe movement for TEE standard view acquisition tasks and has a good generalization ability to unseen patient data. Note to Practitioners—The motivation of this paper is to realize 3-DOF movement guidance of a TEE probe to acquire the standard views of the heart based on the real-time images, which can be applied to existing robotic control systems or used to assist novice echocardiographers in TEE examination, thereby relieving operator workload and improving ease of use. This paper suggests a novel approach that uses the deep RL technique to achieve automatic interpretation of TEE images and intelligent guidance of the probe movement. The RL framework is designed to take into account both the navigation efficiency and compliance with the esophageal environment for the targeted intracorporeal application. A hybrid deep Q-network model that augments a convolutional neural network with attention mechanisms is designed to better capture spatial information from ultrasound images to predict the probe movement. The effectiveness of the framework is preliminarily validated in extensive experiments in a simulation environment built with real patient data. The proposed method can be applied in clinical use to provide real-time TEE probe guidance for novice echocardiographers, and can be integrated with a robotic system to fully automate the TEE acquisition, thereby relieving the doctors from tedious manual operation to focus on the diagnosis and treatment.","1558-3783","","10.1109/TASE.2023.3246089","National Key Research and Development Program of China(grant numbers:2019YFB1312400); Hong Kong Research Grants Council Collaborative Research Fund (RGC CRF)(grant numbers:C4063-18G); Hong Kong RGC General Research Fund (GRF)(grant numbers:14211420); Shenzhen Second People’s Hospital Clinical Research Fund of Guangdong Province High-Level Hospital Construction Project(grant numbers:20213357016); Guangdong Natural Science Funds(grant numbers:2020B1515120061); Shenzhen Key Medical Discipline Construction Fund(grant numbers:SZXK052); Department of Shenzhen Local Science and Technology Development(grant numbers:2021Szvup052); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049752","Transesophageal echocardiography;robot decision-making;reinforcement learning","Probes;Ultrasonic imaging;Standards;Navigation;Robots;Heart;Task analysis","","","","1","","","IEEE","22 Feb 2023","","","IEEE","IEEE Early Access Articles"
"On-Device Personalized Charging Strategy With an Aging Model for Lithium-Ion Batteries Using Deep Reinforcement Learning","A. Yadu; S. S. Brahmadathan; S. Agarwal; S. D. B.; S. Lee; Y. Kim","On-DeviceAI Mobile Battery Laboratory, Samsung Research and Development Institute India-Bengaluru, Bengaluru, Karnataka, India; On-DeviceAI Mobile Battery Laboratory, Samsung Research and Development Institute India-Bengaluru, Bengaluru, Karnataka, India; On-DeviceAI Mobile Battery Laboratory, Samsung Research and Development Institute India-Bengaluru, Bengaluru, Karnataka, India; On-DeviceAI Mobile Battery Laboratory, Samsung Research and Development Institute India-Bengaluru, Bengaluru, Karnataka, India; Advanced Battery Laboratory, Samsung Electronics, Suwon, South Korea; Advanced Battery Laboratory, Samsung Electronics, Suwon, South Korea","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","11","The charging protocol design is a control problem between charge time and capacity retention solved with numerous methods. However, the outcome is an optimal yet rigid charge protocol. Ever changing user behaviour limits the acceptability of one rigid optimal protocol to affect the growing market of LIBs -electric vehicles (EVs), embedded systems etc. It is imperative to redefine optimal charging by incorporating the user behaviour resulting in a dynamic charging strategy. We have formulated the personalized charging strategy problem as a Markov Decision Process (MDP). Q-learning and Deep Deterministic Policy Gradient (DDPG) method is used to solve the MDP. We then present a full spectrum of charging strategies based on perceived user requirement. Three representative charge protocols are demonstrated. The aggressive protocol can charge 77% SOC in  $\sim$ 15 minutes faster than MSCCCV (baseline).  $\sim$ 1.5 years of extra battery life is offered by the life-saver protocol which takes only 30 minutes more than MSCCCV to fully charge. The balanced protocol provides a quick boost and yet maintains a similar charge time and health as MSCCCV. We present an online methodology to retune the protocol on-device based on battery dynamics and user behavior. Finally, the claims are validated using real-world experiments. Note to Practitioners—This work was motivated by the problem of dynamic optimization of the battery-charging trajectory for each user of a mobile device or electric vehicle. The existing mechanism of an optimal charging problem revolves around finding a trade-off between charge time and battery health, not taking into account every user’s unique perspective. This offers only a fixed way of charging a device. This paper proposes to design an optimal charging protocol by taking user behaviour also into account. At each charging phase, the trajectory used for battery charging is optimized w.r.t expected charge time (user behaviour), battery health, and safety constraints of battery at that point of time. The method is trained to learn the battery ageing mechanism for individual user. Existing methodologies finds it tough to accommodate the process of optimal charging for fresh battery characteristics while also considering the constraints of an ageing battery. The proposed solution improves productivity because existing ways of optimizing require significant man-months. An automatic way of optimizing the charging protocol for each charging cycles is an efficient and reliable way to capture degradation as well as the battery-to-battery variability. The outcome of our work is a spectrum of charging protocols each optimized w.r.t battery characteristics and user expectations. Thus, each user can be catered to, with a unique charging protocol optimally designed for their immediate requirement. The practical limitation is to ensure the model converges during on-device retraining. The future direction of the work will be to incorporate more detailed battery model for optimizing the charging protocol.","1558-3783","","10.1109/TASE.2023.3307168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10238462","Battery charging;reinforcement learning;battery management system (BMS);personalized charging;real-time charge protocol optimization","Batteries;Protocols;Optimization;Aging;Voltage measurement;Battery charge measurement;Read only memory","","","","","","","IEEE","4 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Reinforcement-Learning-Informed Prescriptive Analytics for Air Traffic Flow Management","Y. Wang; W. Cai; Y. Tu; J. Mao","School of Data Science, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), Shenzhen, China; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), Shenzhen, China; School of Data Science, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), Shenzhen, China; School of Data Science, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), Shenzhen, China","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","15","Air Traffic Flow Management (ATFM) is a complex sequential decision-making problem that involves dynamically matching flights with sectors under changing environmental conditions. Finding an optimal solution for ATFM is challenging due to its dynamic nature and operational constraints. Reinforcement learning is a well-suited approach for sequential decision-making problems. However, ATFM poses three potential challenges: 1) large state space, 2) combinatorial action space, and 3) variational feasible action set, resulting from numerous agents with tightly-coupled constraints. These challenges can hinder the effectiveness of direct application of reinforcement learning methods. While prescriptive analytics can readily handle hard constraints via a mathematical optimization model, but it is computationally intractable for online sequential decision-making problems under changing environments. To address these challenges, we propose a novel framework, Reinforcement-Learning-Informed Prescriptive Analytics (RLIPA), in which an “informing” scheme is devised to integrate reinforcement learning and prescriptive analytics and leverage their strengths in predicting future reward and coping with hard constraints respectively. RLIPA is a general framework that can be adapted to other problems beyond ATFM, which typically involves many agents with tightly-coupled hard constraints. We demonstrate the usage and performance of RLIPA using numerical results and a real case study in comparison to two baseline approaches. Note to Practitioners—To improve Air Traffic Flow Management (ATFM) and reduce flight congestion, we propose a new method called reinforcement-learning-informed prescriptive analytics (RLIPA). RLIPA is a general framework that facilitates online sequential decision-making problems with multiple agents coupled with hard constraints. The approach consists of two stages: first, estimating future potential rewards for each agent via reinforcement learning, and second, informing the potential rewards to the following prescriptive analysis and using the information to construct and solve the downstream optimization problem dealing with hard coupling constraints among agents. Numerical experiments demonstrate the efficiency and effectiveness of RLIPA in the application of ATFM. In the most cases, RLIPA can offer more than 10x improvement in computational efficiency while maintaining or improving the level of optimality. The framework of RLIPA can be further extended to problems such as order dispatch in ride-hailing systems and food delivery.","1558-3783","","10.1109/TASE.2023.3292921","National Natural Science Foundation of China(grant numbers:U1733102); Guangdong Provincial Key Laboratory of Big Data Computing, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen)(grant numbers:B10120210117); Science, Technology and Innovation Commission of Shenzhen Municipality(grant numbers:ZDSYS2017072-5140921348); Chinese University of Hong Kong, Shenzhen(grant numbers:PF.01.000404); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10258410","Air traffic flow management;reinforcement learning;prescriptive analytics;sequential decision-making problem","Reinforcement learning;Decision making;Optimization;Delays;Airports;Mathematical models;Transportation","","","","","","","IEEE","22 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Reinforcement Learning-Based Switching Controller for a Milliscale Robot in a Constrained Environment","A. Tariverdi; U. Côté-Allard; K. Mathiassen; O. J. Elle; H. Kalvoy; Ø. G. Martinsen; J. Torresen","Department of Physics, University of Oslo, Oslo, Norway; Department of Informatics and RITMO, University of Oslo, Oslo, Norway; Department of Technology Systems, University of Oslo, Kjeller, Norway; Department of Informatics, University of Oslo, Oslo, Norway; Department of Clinical and Biomedical Engineering, Oslo University Hospital, Oslo, Norway; Department of Physics, University of Oslo, Oslo, Norway; Department of Informatics and RITMO, University of Oslo, Oslo, Norway","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","17","This work presents a reinforcement learning-based switching control mechanism to autonomously move a ferromagnetic object (representing a milliscale robot) around obstacles within a constrained environment in the presence of disturbances. This mechanism can be used to navigate objects (e.g., capsule endoscopy, swarms of drug particles) through complex environments when active control is a necessity but where direct manipulation can be hazardous. The proposed control scheme consists of a switching control architecture implemented by two sub-controllers. The first sub-controller is designed to employ the robot’s inverse kinematic solutions to do an environment search for the to-be-carried ferromagnetic particle while being robust to disturbances. The second sub-controller uses a customized rainbow algorithm to control a robotic arm, i.e., the UR5 robot, to carry a ferromagnetic particle to a desired position through a constrained environment. For the customized Rainbow algorithm, Quantile Huber loss from the Implicit Quantile Networks (IQN) algorithm and ResNet are employed. The proposed controller is first trained and tested in a real-time physics simulation engine (PyBullet). Afterward, the trained controller is transferred to a UR5 robot to remotely transport a ferromagnetic particle in a real-world scenario to demonstrate the applicability of the proposed approach. The experimental results on the UR5 robot show an average success rate of 98.86% over 30 episodes for randomly generated trajectories, demonstrating the viability of the proposed approach for real-life applications. In addition, two classical path finding approaches, Attractor Dynamics and the execution extended Rapidly-Exploring Random Trees (ERRT), are also investigated and compared to the RL-based method. The proposed RL-based algorithm is shown to achieve performance comparable to that of the tested classical path planners whilst being more robust to deploy in dynamical environments. Note to Practitioners—Deep reinforcement learning methods have been widely applied in computer games and simulations. However, employing these algorithms for practical, real-world applications such as robotics becomes challenging due to the difficulty of obtaining training samples. This paper predominantly focuses on bridging the gap between simulations and the real-world implementation of a reinforcement learning algorithm for a robotic application in the context of miniaturized drug delivery robots and robotic capsule endoscopes. This paper presents the derivation and experimental validation of a reinforcement learning-based algorithm for controlling a magnetically-actuated small-scale robot within a simplified model of the large intestine in the presence of disturbances. We demonstrate the possibility of training a high-fidelity reinforcement learning algorithm fully within a simulated environment before deploying it as-is in a real-world scenario by carrying out different experiments and simulations. Implementing the presented control framework complements a large body of this work, and the results offer a feasibility study of using reinforcement learning algorithms in practice.","1558-3783","","10.1109/TASE.2023.3259905","Research Council of Norway (RCN) as a part of the Vulnerability in the Robot Society (VIROS) Project(grant numbers:288285); Predictive and Intuitive Robot Companion (PIRC)(grant numbers:312333); Centres of Excellence Scheme(grant numbers:262762); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10081280","Magnetic manipulation;reinforcement learning;rapidly-exploring random trees;attractor dynamics;ERRT;switching control;targeted drug delivery","Robots;Drugs;Task analysis;Navigation;Control systems;Heuristic algorithms;Switches","","","","","","","IEEE","27 Mar 2023","","","IEEE","IEEE Early Access Articles"
"Power Regulation and Load Mitigation of Floating Wind Turbines via Reinforcement Learning","J. Xie; H. Dong; X. Zhao","School of Engineering, Intelligent Control and Smart Energy (ICSE) Research Group, The University of Warwick, Coventry, U.K.; School of Engineering, Intelligent Control and Smart Energy (ICSE) Research Group, The University of Warwick, Coventry, U.K.; School of Engineering, Intelligent Control and Smart Energy (ICSE) Research Group, The University of Warwick, Coventry, U.K.","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","12","Floating offshore wind turbines (FOWTs) are often subjected to heavy structural loads due to challenging operating conditions, which can negatively impact power generation and lead to structural fatigue. This paper proposes a novel reinforcement learning (RL)-based control scheme to address this issue. It combines individual pitch control (IPC) and collective pitch control (CPC) to balance two key objectives: load reduction and power regulation. Specifically, a novel incremental model-based dual heuristic programming (IDHP) strategy is developed as the IPC solution to reduce structural loads. It integrates the online-learned FOWT dynamics into the dual heuristic programming process, making the entire control scheme data-driven and free from dependence on analytical models. Furthermore, the proposed method differs from existing IDHP methods in that only partial system dynamics need to be learned, resulting in a simplified design structure and improved training efficiency. Tests using a high-fidelity FOWT simulator demonstrate the effectiveness of the proposed method. Note to Practitioners—This work achieves power regulation and load reduction simultaneously for FOWTs to guarantee the reliability of wind turbine operations. Such a task is still an open problem because existing FOWT controllers commonly rely on accurate turbine models and lack adaptability to potential uncertainties and errors in practical situations. A new data-driven, model-free control strategy based on the RL technique is developed to address these issues. Our method has the ability to capture potential changes in system dynamics by updating a so-called incremental model via online measurements. Unlike current advances in this direction that need to approximate the whole system dynamics, the proposed control algorithm only needs to update partial system information for the incremental model. This naturally simplifies the design structure and enhances learning effectiveness while providing adaptability and robustness against uncertainties and errors. The proposed control strategy can also be extended and implemented in other systems, such as autonomous systems and other renewable energy systems.","1558-3783","","10.1109/TASE.2023.3295576","Engineering and Physical Sciences Research Council (EPSRC) Supergen Offshore Renewable Energy (ORE) Hub Early Career Researcher (ECR) Research Fund; U(grant numbers:0000DONOTUSETHIS0000.K,EP/S000747/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190112","Intelligent control;wind turbine control;reinforcement learning;wind energy","Wind turbines;Regulation;Load modeling;Blades;Poles and towers;System dynamics;Reinforcement learning","","","","","","","IEEE","21 Jul 2023","","","IEEE","IEEE Early Access Articles"
"Multi-USV Cooperative Chasing Strategy Based on Obstacles Assistance and Deep Reinforcement Learning","W. Gan; X. Qu; D. Song; P. Yao","College of Engineering, Ocean University of China, Qingdao, China; College of Engineering, Ocean University of China, Qingdao, China; College of Engineering, Ocean University of China, Qingdao, China; College of Engineering, Ocean University of China, Qingdao, China","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","16","How to use environmental information to improve the cooperative chasing efficiency of multiple unmanned surface vehicles (multi-USV) is a problem worthy of attention. This article considers an archipelago scenario and proposes an obstacle-assisted chasing framework based on reinforcement learning, in which a multi-USV system chases a smart evader through autonomous environmental awareness and target state prediction. When there are obstacles near the evader, the chasing group can use them to quickly complete the chase. Otherwise, within the range of perception, the group can drive the evader closer to the obstacles or directly cooperate to catch the evader. At the same time, the problem of group credit allocation is also considered, so as to achieve a relatively stable encircling structure with the least number of vehicles. Simulation experiments in the multi-obstacle environment demonstrate the flexibility of the proposed framework. Compared with the traditional force-based and learning-based methods in untrained archipelago environments, the model trained by this framework is proven to have higher efficiency while ensuring real-time performance and generalization. Note to Practitioners—The motivation of this paper is to provide guidance to the USV team in learning how to collaboratively chase and capture target through multi-agent reinforcement learning. Meanwhile, this method can also be applied to other chasing scenarios for vehicles, such as ground robots. This paper proposes a new method that leverages the MA-POCA learning framework and RSA mechanism to enhance cooperation and credit allocation among USVs. Additionally, a novel reward mechanism is designed to encourage USVs to fully utilize the advantages of the surrounding environment and accelerate cooperative chasing. With this method, multiple USVs can effectively pursue intelligent evaders through autonomous environmental perception and target state prediction. To validate the proposed method, we developed a high-fidelity simulation environment using Unity3D, incorporating USV dynamics, irregular obstacles, and sea surface disturbances. Furthermore, our method demonstrates low computational cost, making it suitable for practical USV navigation and control applications in the future.","1558-3783","","10.1109/TASE.2023.3319510","Shandong Provincial Natural Science Foundation, China(grant numbers:ZR2023ME009); National Natural Science Foundation of China(grant numbers:51909252); Fundamental Research Funds for the Central Universities(grant numbers:201964013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10269079","Group chasing;obstacles assistance;reinforcement learning;unmanned surface vehicles (USVs);multi-agent systems (MASs)","Robot kinematics;Vehicle dynamics;Training;Task analysis;Surface waves;Wind speed;Reinforcement learning","","","","","","","IEEE","2 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Data-Driven Control of COVID-19 in Buildings: A Reinforcement-Learning Approach","A. H. Hosseinloo; S. Nabi; A. Hosoi; M. A. Dahleh","Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","0","In addition to its public health crisis, COVID-19 pandemic has led to the shutdown and closure of workplaces with an estimated total cost of more than $16 trillion. Given the long hours an average person spends in buildings and indoor environments, this research article proposes data-driven control strategies to design optimal indoor airflow to minimize the exposure of occupants to viral pathogens in built environments. A general control framework is put forward for designing an optimal velocity field and proximal policy optimization, a reinforcement learning algorithm is employed to solve the control problem in a data-driven fashion. The same framework is used for optimal placement of disinfectants to neutralize the viral pathogens as an alternative to the airflow design when the latter is practically infeasible or hard to implement. We show, via computational simulations, that the control agent learns the optimal policy in both scenarios within a reasonable time. The proposed data-driven control framework in this study will have significant societal and economic benefits by setting the foundation for an improved methodology in designing case-specific infection control guidelines that can be realized by affordable ventilation devices and disinfectants. Note to Practitioners—This paper is motivated by the problem of COVID-19 infection spread in enclosed spaces but it also applies to other airborne pathogens. Airborne disease contagion often takes place in indoor environments; however, ventilation systems are almost never designed to take this into account so as to contain the spread of the pathogens. This is mainly because airflow design requires solving high-dimensional nonlinear partial differential equations known as Navier Stokes equations in fluid dynamics. In this paper, we propose a data-driven approach for solving the control problem of pathogen containment without solving the fluid dynamics equations. To this end, we first mathematically formulate the problem as an optimal control problem and then cast it as a reinforcement learning (RL) task. Reinforcement learning is the data-driven science of sequential decision-making and control in which the controller finds an optimal solution by systematic trial and error and without access to the system dynamics, i.e. fluid and pathogen dynamics in this paper. We employ an state-of-the-art RL algorithm, called PPO, to solve for optimal airflow in a room so as to minimize the exposure risk of occupants. Once it is calculated, the optimal airflow could be realized, via reverse engineering, by proper placement of the ventilation equipment, e.g. inlets, outlets, and fans. As an alternative to the airflow design, we use the same proposed data-driven techniques to find an optimal placement for pathogen disinfectants if there exists one, such as, hydrogen peroxide for COVID-19. Our results show the efficacy of our data-driven approach in designing an steady-state controller with full access to the system states. In future research, we will address the controller design with sparse measurements of the system states.","1558-3783","","10.1109/TASE.2023.3315549","NSF Modeling and Control of COVID-19 Transmission in Indoor Environments (EAGER); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10265235","Disease control;COVID-19;reinforcement learning;data-driven control;HVAC system","Atmospheric modeling;Pathogens;COVID-19;Mathematical models;Computational modeling;Aerospace electronics;Reinforcement learning","","","","","","","IEEE","27 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Cooperative reinforcement learning: exploring communication and cooperation problems","C. Oliveira e Sousa; L. Custodio","Institute for Systems and Robotics, Instituto Superior Técnico, Lisboa, Portugal; Institute for Systems and Robotics, Instituto Superior Técnico, Lisboa, Portugal","2005 International Symposium on Computational Intelligence in Robotics and Automation","12 Dec 2005","2005","","","445","450","In this paper some methods of dealing with the problem of cooperative learning in a multi-agent environment are presented. A system is developed that learns by reinforcement and deals with the problems of communication and cooperation in environments without noise.","","0-7803-9355-4","10.1109/CIRA.2005.1554317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1554317","Reinforcement Learning;multi-agent system;communication;cooperation","Learning;Robots;Multiagent systems;Stochastic processes;Search problems;Working environment noise;Accelerated aging;State-space methods;Costs;Decision making","learning (artificial intelligence);multi-agent systems","cooperative reinforcement learning;multi-agent environment","","","","16","IEEE","12 Dec 2005","","","IEEE","IEEE Conferences"
"Digital Twin-Driven Reinforcement Learning Method for Marine Equipment Vehicles Scheduling Problem","X. Shen; S. Liu; B. Zhou; T. Wu; Q. Zhang; J. Bao","College of Mechanical Engineering, Donghua University, Shanghai, China; Department of Industrial and Systems Engineering, The Hong Kong Polytechnic University, Hong Kong, China; School of Mechanical Engineering, University of Shanghai for Science and Technology, Shanghai, China; College of Mechanical Engineering, Donghua University, Shanghai, China; College of Mechanical Engineering, Donghua University, Shanghai, China; College of Mechanical Engineering, Donghua University, Shanghai, China","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","11","In the traditional marine equipment construction process, the material transportation vehicle scheduling method dominated by manual experience has shown great limitations, which is inefficient, costly, wasteful of human resources, and unable to cope with complex and changing scheduling scenarios. The existing scheduling system cannot realize the information interaction and collaborative integration between the physical world and the virtual world, while the digital twin (DT) technology can effectively solve the problem of real-time information interaction and the reinforcement learning (RL) method can cope with dynamic scenarios. Therefore, this paper proposed a DT-driven RL method to solve the marine equipment vehicle scheduling problem. Given the dynamic nature of transportation tasks, the diversity of transported goods, and the optimization characteristics of transportation requirements, a framework for scheduling transportation vehicle operations based on DT is constructed, and a RL-based vehicle scheduling method in a dynamic task environment is proposed. A Markov decision process (MDP) model of the vehicle scheduling process is established to realize one-to-one mapping between information and physical elements. An improved RL method based on Q-learning is proposed to solve the MDP model, and the value function approximation and convergence enhancement methods are applied to optimize the solving process. Finally, a case study is used for example verification to prove the superiority and effectiveness of the proposed method in this paper. Note to Practitioners—The motivation of this paper is to optimize material transportation vehicle scheduling in dynamic task environments and to improve logistics transportation efficiency. Therefore, a DT-based vehicle scheduling method for marine equipment is proposed. Firstly, a framework of vehicle scheduling based on DT is designed to establish a MDP model of the vehicle scheduling process, and the dynamic task characteristics are described by mathematical methods in the design of the elements of the model. A RL-based vehicle scheduling method is proposed. The value function approximation method and the convergence enhancement method of the algorithm are investigated for the characteristics of continuous dynamic action features leading to huge state space and non-convergence of the algorithm. The algorithm performance is verified and analyzed through data validation of actual cases.","1558-3783","","10.1109/TASE.2023.3289915","National Key Research and Development Plan of China(grant numbers:2019YFB1706300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177865","Digital twin;Q-learning;vehicle scheduling;marine equipment","Job shop scheduling;Task analysis;Dynamic scheduling;Transportation;Marine equipment;Vehicle dynamics;Optimization","","","","","","","IEEE","10 Jul 2023","","","IEEE","IEEE Early Access Articles"
"A Hierarchical Framework for Quadruped Omnidirectional Locomotion Based on Reinforcement Learning","W. Tan; X. Fang; W. Zhang; R. Song; T. Chen; Y. Zheng; Y. Li","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; Tencent Robotics X Laboratory, Shenzhen, China; School of Control Science and Engineering, Shandong University, Jinan, China","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","12","Quadruped locomotion is challenging for many learning-based algorithms. This is because it requires tedious manual tuning to cope with different types of terrains and is difficult to deploy in reality due to the sim-to-real gap between the training and the testing scenarios. This paper proposes a quadruped robot learning system for agile locomotion which does not require any pre-training and works well in various terrains. We introduce a hierarchical framework that uses reinforcement learning as the high-level policy to adjust the low-level trajectory generator for a better adaptability to various terrains. We compact the observation and the action spaces of reinforcement learning to deploy the proposed framework on a host computer interfaced with the robot. Besides, we design an omnidirectional trajectory generator guided by robot posture, which generates omnidirectional foot trajectories to interact with the environment. Experimental results and the supplementary video demonstrate that our hierarchical framework only trained in simulation can be easily deployed in the real world, and also has the advantages of fast convergence and good terrain adaptability. Note to Practitioners—This paper presents a hierarchical framework for quadruped robots. It combines a high-level reinforcement learning controller with a posture-guided trajectory generator to adaptively generate omnidirectional motions. Our method is easy to train as it converges fast and does not need to adjust a dozen or so of rewards. The quadruped robot can be deployed in a real environment directly after being trained in simulation. With the trained hierarchical framework deployed on a remote host computer, the robot works well in a variety of real-world environments unseen in the simulation.","1558-3783","","10.1109/TASE.2023.3310945","National Key Research and Development Plan of China(grant numbers:2021ZD0112002,2018AAA0102504); National Natural Science Foundation of China(grant numbers:U1913204,U22A2057); Natural Science Foundation of Shandong Province for Distinguished Young Scholars(grant numbers:ZR2020JQ29); Project for Self-Developed Innovation Team of Jinan City(grant numbers:2021GXRC038); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254481","Quadruped robot;reinforcement learning;trajectory generator;robotic manipulation","Robots;Quadrupedal robots;Legged locomotion;Trajectory;Generators;Actuators;Training","","","","","","","IEEE","18 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Brain-Inspired Deep Meta-Reinforcement Learning for Active Coordinated Fault-Tolerant Load Frequency Control of Multi-Area Grids","J. Li; T. Zhou; H. Cui","School of Electronic and Information Engineering, Shanghai University of Electric Power, Shanghai, China; School of Electronic and Information Engineering, Shanghai University of Electric Power, Shanghai, China; School of Electronic and Information Engineering, Shanghai University of Electric Power, Shanghai, China","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","13","This paper proposes an active coordinated fault tolerance load frequency control (AFCT-LFC) method, which effectively prevents sudden frequency changes caused by unit actuator failures or unplanned decommissioning in a multi-area interconnected grid subject to the performance-based frequency regulation market mechanism. It can also reduce regulation mileage payments and achieve multi-objective active fault-tolerant control. In addition, this paper proposes a brain-Inspired deep meta-deterministic policy gradient algorithm (BIMA-DMDPG), which adopts multi-agent centralized training, equates the controller of each area as an agent capable of independent decision making, and implements distributed training by dividing the environment into multiple environments. In addition, meta-reinforcement learning is employed to realize multi-task collaborative learning. The optimal policy is actively selected under different fault conditions to achieve active fault-tolerant control. The superior performance of the method is verified in a four-area LFC model of the China Southern Grid (CSG), in which it is tested alongside a selection of existing algorithms. Note to Practitioners—AFCT-LFC is based on advanced artificial intelligence algorithm, which can effectively identify any fault in multi-area grids and make rapid response to achieve active fault-tolerant control. Compared with the existing model-based fault-tolerant control methods, the BIMA-DMDPG algorithm proposed in this paper does not need to rely on accurate mathematical models, and can be applied to practice through simple training, which is very suitable for practical applications. Therefore, AFCT-LFC is an advanced adaptive active fault-tolerant control method that can be truly applied in practice because of its fast-decision-making ability and performance.","1558-3783","","10.1109/TASE.2023.3263005","National Natural Science Foundation of China(grant numbers:52177185); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10091207","Load frequency control;fault tolerance control;performance-based frequency regulation;meta-deterministic policy gradient algorithm;brain-inspired","Frequency control;Fault tolerant systems;Fault tolerance;Fluctuations;Robust control;Power systems;Actuators","","","","","","","IEEE","3 Apr 2023","","","IEEE","IEEE Early Access Articles"
"Efficient Hierarchical Reinforcement Learning for Mapless Navigation With Predictive Neighbouring Space Scoring","Y. Gao; J. Wu; X. Yang; Z. Ji","School of Engineering, Cardiff University, Cardiff, U.K.; School of Computer Science and Informatics, Cardiff University, Cardiff, U.K.; School of Engineering, Cardiff University, Cardiff, U.K.; School of Engineering, Cardiff University, Cardiff, U.K.","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","16","Solving reinforcement learning (RL)-based mapless navigation tasks is challenging due to their sparse reward and long decision horizon nature. Hierarchical reinforcement learning (HRL) has the ability to leverage knowledge at different abstract levels and is thus preferred in complex mapless navigation tasks. However, it is computationally expensive and inefficient to learn navigation end-to-end from raw high-dimensional sensor data, such as Lidar or RGB cameras. The use of subgoals based on a compact intermediate representation is therefore preferred for dimension reduction. This work proposes an efficient HRL-based framework to achieve this with a novel scoring method, named Predictive Neighbouring Space Scoring (PNSS). The PNSS model estimates the explorable space for a given position of interest based on the current robot observation. The PNSS values for a few candidate positions around the robot provide a compact and informative state representation for subgoal selection. We study the effects of different candidate position layouts and demonstrate that our layout design facilitates higher performances in longer-range tasks. Moreover, a penalty term is introduced in the reward function for the high-level (HL) policy, so that the subgoal selection process takes the performance of the low-level (LL) policy into consideration. Comprehensive evaluations demonstrate that using the proposed PNSS module consistently improves performances over the use of Lidar only or Lidar and encoded RGB features Note to Practitioners—This paper seeks to improve robot mapless navigation capabilities where the robot is expected to navigate to a goal location without knowing the map of the environment. This ability is highly demanded in many applications that require autonomous operations in unstructured environments, including both indoor and outdoor scenarios, involving tasks such as service robots for domestic and public environments, logistics in industrial warehouses, urban search and rescue missions, and disaster relief efforts, where detailed and accurate maps are difficult to obtain in advance. In this work, we focus on reinforcement learning-based mapless navigation. It is known that such methods struggle in complex long-range tasks, e.g. stuck in a local region by multiple objects. Therefore, this paper proposes a novel mapless navigation method inspired by human navigation behaviours. We enable a robot to split a long-range navigation task into multiple segments, by selecting and navigating to short-term goals. These subgoals are selected each time from a number of candidate positions located around the robot. The process stops when the robot reaches the final target location. When selecting a short-term goal, we use a deep neural network to predict the openness around each candidate subgoal position, named the Predictive Neighbouring Space Scoring (PNSS), from raw images and Lidar scans. In addition, we study the effects of different arrangements of candidate subgoal locations and select the optimal one. Experiments conducted in photo-realistic simulation environments demonstrate the effectiveness of our method, showcasing superior performance over baselines. It is worth noting that our agent is only trained in domestic environments using the iGibson simulator. For applications in other environments, additional training in more representative settings specific to corresponding scenarios will be necessary. In the future, our intention is to validate our methods in complex real-world environments and narrow the simulation-to-reality gap for long-horizon navigation tasks.","1558-3783","","10.1109/TASE.2023.3312237","Royal Academy of Engineering(grant numbers:IF2223-199); China Scholarship Council (CSC) for providing the living stipend for his Ph(grant numbers:0000DONOTUSETHIS0000.D,202008230171); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10248030","Mapless navigation;deep reinforcement learning;collision avoidance;motion planning;hierarchical reinforcement learning","Navigation;Robots;Task analysis;Laser radar;Robot kinematics;Reinforcement learning;Layout","","","","","","","IEEE","12 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Towards Generalization in Target-Driven Visual Navigation by Using Deep Reinforcement Learning","A. Devo; G. Mezzetti; G. Costante; M. L. Fravolini; P. Valigi","Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy","IEEE Transactions on Robotics","1 Oct 2020","2020","36","5","1546","1561","Among the main challenges in robotics, target-driven visual navigation has gained increasing interest in recent years. In this task, an agent has to navigate in an environment to reach a user specified target, only through vision. Recent fruitful approaches rely on deep reinforcement learning, which has proven to be an effective framework to learn navigation policies. However, current state-of-the-art methods require to retrain, or at least fine-tune, the model for every new environment and object. In real scenarios, this operation can be extremely challenging or even dangerous. For these reasons, we address generalization in target-driven visual navigation by proposing a novel architecture composed of two networks, both exclusively trained in simulation. The first one has the objective of exploring the environment, while the other one of locating the target. They are specifically designed to work together, while separately trained to help generalization. In this article, we test our agent in both simulated and real scenarios, and validate its generalization capabilities through extensive experiments with previously unseen goals and unknown mazes, even much larger than the ones used for training.","1941-0468","","10.1109/TRO.2020.2994002","Università degli Studi di Perugia(grant numbers:RICBA17MRF,RICBA18MF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9102361","Deep learning in robotics and automation;target-driven visual navigation;visual-based navigation;visual learning","Navigation;Visualization;Task analysis;Training;Machine learning;Simultaneous localization and mapping","learning (artificial intelligence);mobile robots;path planning;robot vision","target-driven visual navigation;deep reinforcement learning;generalization","","38","","54","IEEE","27 May 2020","","","IEEE","IEEE Journals"
"Optical Coherence Tomography-Guided Robotic Ophthalmic Microsurgery via Reinforcement Learning from Demonstration","B. Keller; M. Draelos; K. Zhou; R. Qian; A. N. Kuo; G. Konidaris; K. Hauser; J. A. Izatt","Department of Biomedical Engineering, Duke University, Durham, USA; Department of Biomedical Engineering, Duke University, Durham, USA; Department of Biomedical Engineering, Duke University, Durham, USA; Department of Biomedical Engineering, Duke University, Durham, USA; Department of Ophthalmology, Duke University Medical Center, Durham, USA; Department of Computer Science Brown University, Providence, USA; Department of Electrical and Computer Engineering, Duke University, Durham, USA; Department of Biomedical Engineering, Duke University, Durham, USA","IEEE Transactions on Robotics","5 Aug 2020","2020","36","4","1207","1218","Ophthalmic microsurgery is technically difficult because the scale of required surgical tool manipulations challenge the limits of the surgeon's visual acuity, sensory perception, and physical dexterity. Intraoperative optical coherence tomography (OCT) imaging with micrometer-scale resolution is increasingly being used to monitor and provide enhanced real-time visualization of ophthalmic surgical maneuvers, but surgeons still face physical limitations when manipulating instruments inside the eye. Autonomously controlled robots are one avenue for overcoming these physical limitations. In this article, we demonstrate the feasibility of using learning from demonstration and reinforcement learning with an industrial robot to perform OCT-guided corneal needle insertions in an ex vivo model of deep anterior lamellar keratoplasty (DALK) surgery. Our reinforcement learning agent trained on ex vivo human corneas, then outperformed surgical fellows in reaching a target needle insertion depth in mock corneal surgery trials. This article shows the combination of learning from demonstration and reinforcement learning is a viable option for performing OCT-guided robotic ophthalmic surgery.","1941-0468","","10.1109/TRO.2020.2980158","National Institutes of Health(grant numbers:R01 EY023039,R21EY029877); Duke Coulter Translational Partnership(grant numbers:2016–2018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9069310","Deep learning in robotics and automation;learning from demonstration;medical robots and systems;microsurgery","Needles;Robots;Microsurgery;Cornea;Learning (artificial intelligence);Task analysis","biological tissues;biomedical equipment;biomedical optical imaging;eye;laser applications in medicine;learning (artificial intelligence);medical robotics;needles;optical tomography;surgery","optical coherence tomography-guided robotic ophthalmic microsurgery;reinforcement learning;required surgical tool manipulations;surgeon;sensory perception;physical dexterity;intraoperative optical coherence tomography;micrometer-scale resolution;real-time visualization;ophthalmic surgical maneuvers;manipulating instruments;autonomously controlled robots;industrial robot;OCT-guided corneal needle insertions;ex vivo model;deep anterior lamellar keratoplasty surgery;surgical fellows;target needle insertion depth;OCT-guided robotic ophthalmic surgery","","20","","69","CCBY","16 Apr 2020","","","IEEE","IEEE Journals"
"RLOC: Terrain-Aware Legged Locomotion Using Reinforcement Learning and Optimal Control","S. Gangapurwala; M. Geisert; R. Orsolino; M. Fallon; I. Havoutis","Dynamic Robots Systems Group, Oxford Robotics Institute, University of Oxford, Oxford, U.K.; Dynamic Robots Systems Group, Oxford Robotics Institute, University of Oxford, Oxford, U.K.; Dynamic Robots Systems Group, Oxford Robotics Institute, University of Oxford, Oxford, U.K.; Dynamic Robots Systems Group, Oxford Robotics Institute, University of Oxford, Oxford, U.K.; Dynamic Robots Systems Group, Oxford Robotics Institute, University of Oxford, Oxford, U.K.","IEEE Transactions on Robotics","4 Oct 2022","2022","38","5","2908","2927","We present a unified model-based and data-driven approach for quadrupedal planning and control to achieve dynamic locomotion over uneven terrain. We utilize on-board proprioceptive and exteroceptive feedback to map sensory information and desired base velocity commands into footstep plans using a reinforcement learning (RL) policy. This RL policy is trained in simulation over a wide range of procedurally generated terrains. When run online, the system tracks the generated footstep plans using a model-based motion controller. We evaluate the robustness of our method over a wide variety of complex terrains. It exhibits behaviors that prioritize stability over aggressive locomotion. Additionally, we introduce two ancillary RL policies for corrective whole-body motion tracking and recovery control. These policies account for changes in physical parameters and external perturbations. We train and evaluate our framework on a complex quadrupedal system, ANYmal version B, and demonstrate transferability to a larger and heavier robot, ANYmal C, without requiring retraining.","1941-0468","","10.1109/TRO.2022.3172469","UKRI/EPSRC RAIN Hub(grant numbers:EP/R026084/1); EU H2020; Engineering and Physical Sciences Research Council(grant numbers:EP/S002383/1); Royal Society University Research Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779429","AI-based methods;deep learning in robotics and automation;legged robots;robust/adaptive control of robotic systems","Robots;Quadrupedal robots;Planning;Training;Computational modeling;Legged locomotion;Tracking","learning (artificial intelligence);legged locomotion;motion control;reinforcement learning;robot dynamics","data-driven approach;quadrupedal planning;dynamic locomotion;uneven terrain;on-board proprioceptive;exteroceptive feedback;sensory information;base velocity;reinforcement learning policy;procedurally generated terrains;generated footstep plans;model-based motion controller;complex terrains;aggressive locomotion;ancillary RL policies;whole-body motion tracking;recovery control;complex quadrupedal system;RLOC;terrain-aware legged locomotion;optimal control;prioritized stability;unified model-based approach","","17","","50","IEEE","20 May 2022","","","IEEE","IEEE Journals"
"Grasping Living Objects With Adversarial Behaviors Using Inverse Reinforcement Learning","Z. Hu; Y. Zheng; J. Pan","Department of Biomedical Engineering, The City University of Hong Kong, Hong Kong; Tencent Robotics X, Shenzhen, China; Department of Computer Science, The University of Hong Kong, Hong Kong","IEEE Transactions on Robotics","5 Apr 2023","2023","39","2","1151","1163","Living objects are difficult to grasp since they can actively elude capture by adopting adversarial behaviors that are extremely hard to model or predict. In this case, an inappropriately strong contact force may hurt the struggling living objects and a grasping algorithm that can minimize the contact force whenever possible is required. To solve this challenging task, in this article, we present a reinforcement-learning (RL)-based algorithm with two stages: the pregrasp stage and the in-hand stage. In the pregrasp stage, the robot focuses on the living object's adversarial behavior and approaches it in a reliable manner. In particular, we use inverse RL to encode the living object's adversarial behavior into a reward function. The negative value of the learned reward function is then used to train a high-quality grasping policy that can compete with the living object's adversarial behavior with the RL framework. In the in-hand stage, we use RL to train a grasp policy such that the dexterous hand can grab the living object with the minimal force. A set of dense rewards are also specifically designed to encourage the robot to grasp and hold the living object persistently. To further improve the grasp performance, we explicitly take into account the structure of the dexterous robot hand by treating the hand as a graph and adopting graph convolutional network to formulate the grasping policy. We conduct a set of experiments to demonstrate the performance of our proposed method, in which the robot can grasp living objects with the success rate of 90% and 95% in the pregrasp and in-hand stages, respectively. The contact force applied by the robotic hand to the living object is dramatically reduced in comparison with the baseline grasping policy.","1941-0468","","10.1109/TRO.2022.3226108","HKSAR Research Grants Council; General Research Fund(grant numbers:HKU 11202119,11207818); Innovation and Technology Commission of the HKSAR Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024050","Deep learning in robotics and automation;dexterous manipulation;grasping;reinforcement learning (RL)","Grasping;Robots;Behavioral sciences;Robot kinematics;Force;Task analysis;Manipulators","control engineering computing;convolutional neural nets;dexterous manipulators;graph theory;grippers;learning (artificial intelligence);reinforcement learning;robot vision","adversarial behaviors;baseline grasping policy;grasp policy;grasping algorithm;high-quality grasping policy;in-hand stage;inappropriately strong contact force;inverse reinforcement learning;living object;pregrasp stage;reinforcement-learning-based algorithm;struggling living objects","","1","","33","IEEE","20 Jan 2023","","","IEEE","IEEE Journals"
"Target Search Control of AUV in Underwater Environment With Deep Reinforcement Learning","X. Cao; C. Sun; M. Yan","School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; Lab Underwater Vehicles & Intelligent System, Shanghai Maritime University, Shanghai, China","IEEE Access","31 Jul 2019","2019","7","","96549","96559","The autonomous underwater vehicle (AUV) is widely used to search for unknown targets in the complex underwater environment. Due to the unpredictability of the underwater environment, this paper combines the traditional frontier exploration method with deep reinforcement learning (DRL) to enable the AUV to explore the unknown underwater environment autonomously. In this paper, a grid map of the search environment is built by the grid method. The designed asynchronous advantage actor-critic (A3C) network structure is used in the traditional frontier exploration method for target search tasks. This network structure enables the AUV to learn from its own experience and generate search strategies for the various unknown environment. At the same time, DRL and dual-stream Q-learning algorithms are used for AUV navigation to further optimize the search path. The simulations and experiments in an unknown underwater environment with different layouts show that the proposed algorithm can accomplish target search tasks with a high success rate, and it can adapt to different environments. In addition, compared to other search methods, the frontier exploration algorithm based on DRL can search a wider environment faster, which results in a higher search efficiency and reduced search time.","2169-3536","","10.1109/ACCESS.2019.2929120","National Natural Science Foundation of China(grant numbers:61773177,61520106009); Natural Science Foundation of Jiangsu Province(grant numbers:BK20171270); China Postdoctoral Science Foundation(grant numbers:2017M621587); Jiangsu Planned Projects for Postdoctoral Research Funds(grant numbers:1701076B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8764329","Target search;frontier exploration;deep learning;reinforcement learning","Robot sensing systems;Task analysis;Sonar;Reinforcement learning;Heuristic algorithms;Sonar navigation","autonomous underwater vehicles;learning (artificial intelligence);mobile robots;multi-robot systems;path planning;remotely operated vehicles;search problems;underwater vehicles","target search control;deep reinforcement learning;autonomous underwater vehicle;unknown targets;complex underwater environment;traditional frontier exploration method;DRL;unknown underwater environment;search environment;grid method;designed asynchronous advantage actor-critic network structure;target search tasks;search strategies;unknown environment;Q-learning algorithms;AUV navigation;search path;search methods;frontier exploration algorithm;wider environment;higher search efficiency;search time","","31","","43","CCBY","16 Jul 2019","","","IEEE","IEEE Journals"
"UAV Path Planning Based on Multi-Layer Reinforcement Learning Technique","Z. Cui; Y. Wang","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","IEEE Access","23 Apr 2021","2021","9","","59486","59497","Unmanned aerial vehicles (UAVs) have been widely used in many applications due to its small size, swift mobility and low cost. Therefore, the study of guidance, navigation and control (GNC) system of UAV has becoming a popular research direction. Path planning plays an important role in the GNC system. In this paper, a multi-layer path planning algorithm based on reinforcement learning (RL) technique is proposed. Compared to the classic Q-learning, the proposed multi-layer algorithm has a distinct advantage that it collects both global and local information which greatly improves overall performance. The proposed RL algorithm has two layers, the higher layer deals with the local information, which could be considered as a short-term strategy. The lower layer deals with the global information, which could be considered as a long-term strategy. Both the higher layer and lower layer are working in harmony to plan a collision-free path. B-spline curve approach is applied for on-line path smoothing. Simulation results in different scenarios prove the effectiveness of multi-layer Q-learning algorithm.","2169-3536","","10.1109/ACCESS.2021.3073704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406006","Path planning;reinforcement learning;multi-layer Q-learning;B-spline curve","Path planning;Heuristic algorithms;Unmanned aerial vehicles;Navigation;Collision avoidance;Reinforcement learning;Fuzzy logic","autonomous aerial vehicles;collision avoidance;learning (artificial intelligence);mobile robots;search problems;splines (mathematics)","UAV path planning;unmanned aerial vehicles;swift mobility;navigation;GNC system;multilayer path;RL algorithm;global information;collision-free path;on-line path smoothing;multilayer Q-learning algorithm;multilayer reinforcement learning","","30","","39","CCBYNCND","16 Apr 2021","","","IEEE","IEEE Journals"
"Guided Soft Actor Critic: A Guided Deep Reinforcement Learning Approach for Partially Observable Markov Decision Processes","M. Haklidir; H. Temeltaş","Department of Control and Automation Engineering, Istanbul Technical University, Maslak, Istanbul, Turkey; Department of Control and Automation Engineering, Istanbul Technical University, Maslak, Istanbul, Turkey","IEEE Access","10 Dec 2021","2021","9","","159672","159683","Most real-world problems are essentially partially observable, and the environmental model is unknown. Therefore, there is a significant need for reinforcement learning approaches to solve them, where the agent perceives the state of the environment partially and noisily. Guided reinforcement learning methods solve this issue by providing additional state knowledge to reinforcement learning algorithms during the learning process, allowing them to solve a partially observable Markov decision process (POMDP) more effectively. However, these guided approaches are relatively rare in the literature, and most existing approaches are model-based, meaning that they require learning an appropriate model of the environment first. In this paper, we propose a novel model-free approach that combines the soft actor-critic method and supervised learning concept to solve real-world problems, formulating them as POMDPs. In experiments performed on OpenAI Gym, an open-source simulation platform, our guided soft actor-critic approach outperformed other baseline algorithms, gaining 7~20% more maximum average return on five partially observable tasks constructed based on continuous control problems and simulated in MuJoCo.","2169-3536","","10.1109/ACCESS.2021.3131772","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9631278","Deep reinforcement learning;guided policy search;POMDP","Reinforcement learning;Markov processes;Task analysis;Training;Taxonomy;Supervised learning","deep learning (artificial intelligence);Markov processes;reinforcement learning","partially observable Markov decision processes;environmental model;additional state knowledge;partially observable Markov decision process;guided soft actor-critic approach;partially observable tasks;guided deep reinforcement learning;POMDP;OpenAI Gym;open-source simulation platform;continuous control problems;MuJoCo","","1","","75","CCBY","30 Nov 2021","","","IEEE","IEEE Journals"
"Combined DR Pricing and Voltage Control Using Reinforcement Learning Based Multi-Agents and Load Forecasting","D. A. Khan; A. Arshad; M. Lehtonen; K. Mahmoud","Ghulam Ishaq Khan Institute of Engineering Sciences and Technology, Topi, Pakistan; Ghulam Ishaq Khan Institute of Engineering Sciences and Technology, Topi, Pakistan; Department of Electrical Engineering and Automation, School of Electrical Engineering, Aalto University, Espoo, Finland; Department of Electrical Engineering and Automation, School of Electrical Engineering, Aalto University, Espoo, Finland","IEEE Access","21 Dec 2022","2022","10","","130839","130849","The demand for energy around the world continues to increase at a very high rate. To sufficiently supply this high demand, it is imperative to employ efficient methods so that the total costs for fulfilling such high demand in energy are minimized. To achieve this ambitious goal, this paper proposes a multi-agent reinforcement learning system for time of use pricing based combined demand response and voltage control. For this purpose, a long short term memory network is employed for day-ahead load forecasting in order to remove future uncertainties. The Q-learning algorithm is used which is a model free algorithm and hence, doesn’t require the agent(s) to have prior knowledge of the environment. The role of reinforcement learning in this work is very important since it allows the agent(s) to determine their respective optimal behavior(s) autonomously without explicit training by the end user. To allow effective cooperation among multiple agents, each household is controlled by its own agent, whereas all the household agents are directed by a master agent or service provider. Accordingly, the voltage control agent serves the purpose of checking voltage level violations in the system and removing them through optimal decision making. The proposed system yields very good results, whereby, not only is the overall cost of electricity reduced, but voltage level violations are also removed from the entire system. The implementation of this mechanism reduces the total average aggregated load demand from 5.23 kW to 3.86 kW, while reducing the total aggregated average cost from 94.01 Rs to 60.80 Rs, thanks to the proposed effective multi-agent based system.","2169-3536","","10.1109/ACCESS.2022.3228836","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982611","Reinforcement learning;long short term memory;demand response;multi-agent system;voltage control","Home appliances;Pricing;Voltage control;Costs;Load modeling;Reinforcement learning;Load forecasting","decision making;learning (artificial intelligence);load forecasting;multi-agent systems;pricing;voltage control","ambitious goal;combined demand response;combined DR pricing;day-ahead load forecasting;household agents;long short term memory network;master agent;model free algorithm;multiagent based system;multiagent reinforcement;multiple agents;power 3.86 kW to 5.23 kW;Q-learning algorithm;reinforcement learning based multiagents;respective optimal behavior;system yields;total aggregated average cost;total average aggregated load demand;use pricing;voltage control agent;voltage level violations","","","","39","CCBY","12 Dec 2022","","","IEEE","IEEE Journals"
"Coordination of multiple behaviors acquired by a vision-based reinforcement learning","M. Asada; E. Uchibe; S. Noda; S. Tawaratsumida; K. Hosoda","Department of Mech. Engineering for Computer-Controlled Machinery, Osaka University, Suita, Osaka, Japan; Department of Mech. Engineering for Computer-Controlled Machinery, Osaka University, Suita, Osaka, Japan; Department of Mech. Engineering for Computer-Controlled Machinery, Osaka University, Suita, Osaka, Japan; Department of Mech. Engineering for Computer-Controlled Machinery, Osaka University, Suita, Osaka, Japan; Department of Mech. Engineering for Computer-Controlled Machinery, Osaka University, Suita, Osaka, Japan","Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS'94)","6 Aug 2002","1994","2","","917","924 vol.2","A method is proposed which accomplishes a whole task consisting of plural subtasks by coordinating multiple behaviors acquired by a vision-based reinforcement learning. First, individual behaviors which achieve the corresponding subtasks are independently acquired by Q-learning, a widely used reinforcement learning method. Each learned behavior can be represented by an action-value function in terms of state of the environment and robot action. Next, three kinds of coordinations of multiple behaviors are considered; simple summation of different action-value functions, switching action-value functions according to situations, and learning with previously obtained action-value functions as initial values of a new action-value function. A task of shooting a ball into the goal avoiding collisions with an enemy is examined. The task can be decomposed into a ball shooting subtask and a collision avoiding subtask. These subtasks should be accomplished simultaneously, but they are not independent of each other.<>","","0-7803-1933-8","10.1109/IROS.1994.407484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=407484","","Robot kinematics;Robotics and automation;Robot sensing systems;Machinery;Computer simulation;Learning automata;Convergence;Autonomous agents","robot vision;learning (artificial intelligence);object recognition;intelligent control;mobile robots","multiple behaviors coordination;vision-based reinforcement learning;Q-learning;action-value function;robot action;ball shooting task;collision avoidance;mobile robots","","26","","16","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Automating GUI Testing with Image-Based Deep Reinforcement Learning","J. Eskonen; J. Kahles; J. Reijonen","Ericsson, Jorvas, Finland; Ericsson, Jorvas, Finland; Ericsson, Jorvas, Finland","2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)","15 Sep 2020","2020","","","160","167","Users interact with modern applications and devices through graphical user interfaces (GUIs). To ensure intuitive and easy usability, the GUIs need to be tested, where developers aim at finding possible bugs and inconsistent functionality. Manual GUI testing requires time and effort, and thus, its efficiency can be improved with automation. Conventional automation tools for GUI testing reduce the burden of manual testing but also introduce challenges in the maintenance of test cases. In order to overcome these issues, we propose a deep-reinforcement-learning-based (DRL) solution for automated and adaptive GUI testing. Specifically, we propose and evaluate the performance of an image-based DRL solution. We adapt the asynchronous advantage actor-critic (A3C) algorithm to GUI testing inspired by how a human uses a GUI. We feed screenshots of the GUI as the input and let the algorithm decide how to interact with GUI components. We observe that our solution can achieve up to six times higher exploration efficiency compared to selected baseline algorithms. Moreover, our solution is more efficient than inexperienced human users and almost as efficient as an experienced human user in our experimental GUI testing scenario. For these reasons, image-based DRL exploration can be considered as a viable GUI testing method.","","978-1-7281-7277-4","10.1109/ACSOS49614.2020.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196452","GUI testing;deep reinforcement learning;image processing;automation","Graphical user interfaces;Testing;Humanoid robots;Manuals;Task analysis;Kernel;Machine learning","graphical user interfaces;image processing;learning (artificial intelligence);neural nets;program testing","A3C algorithm;asynchronous advantage actor-critic algorithm;image-based DRL exploration;GUI components;adaptive GUI testing;automated testing;graphical user interfaces;image-based deep reinforcement learning","","16","","30","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Reinforcement learning and adaptive dynamic programming for feedback control","F. L. Lewis; D. Vrabie","Automation & Robotics Research Institute, University of Texas, Arlington, USA; Automation & Robotics Research Institute, University of Texas, Arlington, USA","IEEE Circuits and Systems Magazine","28 Aug 2009","2009","9","3","32","50","Living organisms learn by acting on their environment, observing the resulting reward stimulus, and adjusting their actions accordingly to improve the reward. This action-based or reinforcement learning can capture notions of optimal behavior occurring in natural systems. We describe mathematical formulations for reinforcement learning and a practical implementation method known as adaptive dynamic programming. These give us insight into the design of controllers for man-made engineered systems that both learn and exhibit optimal behavior.","1558-0830","","10.1109/MCAS.2009.933854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5227780","","Learning;Programmable control;Adaptive control;Dynamic programming;Feedback control;Organisms;Optimal control;Control systems;Design engineering;Systems engineering and theory","control engineering computing;control system synthesis;dynamic programming;feedback;learning (artificial intelligence);optimal control","reinforcement learning;adaptive dynamic programming;feedback control;reward stimulus;optimal behavior;natural system;man-made engineered system;controller design","","1023","1","92","IEEE","28 Aug 2009","","","IEEE","IEEE Magazines"
"Reinforcement Learning for Partially Observable Dynamic Processes: Adaptive Dynamic Programming Using Measured Output Data","F. L. Lewis; K. G. Vamvoudakis","Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA; Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","13 Jan 2011","2011","41","1","14","25","Approximate dynamic programming (ADP) is a class of reinforcement learning methods that have shown their importance in a variety of applications, including feedback control of dynamical systems. ADP generally requires full information about the system internal states, which is usually not available in practical situations. In this paper, we show how to implement ADP methods using only measured input/output data from the system. Linear dynamical systems with deterministic behavior are considered herein, which are systems of great interest in the control system community. In control system theory, these types of methods are referred to as output feedback (OPFB). The stochastic equivalent of the systems dealt with in this paper is a class of partially observable Markov decision processes. We develop both policy iteration and value iteration algorithms that converge to an optimal controller that requires only OPFB. It is shown that, similar to Q-learning, the new methods have the important advantage that knowledge of the system dynamics is not needed for the implementation of these learning algorithms or for the OPFB control. Only the order of the system, as well as an upper bound on its ""observability index,"" must be known. The learned OPFB controller is in the form of a polynomial autoregressive moving-average controller that has equivalent performance with the optimal state variable feedback gain.","1941-0492","","10.1109/TSMCB.2010.2043839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439950","Approximate dynamic programming (ADP);data-based optimal control;policy iteration (PI);output feedback (OPFB);value iteration (VI)","Dynamic programming;Learning;Control systems;Optimal control;Feedback control;Output feedback;Stochastic systems;Upper bound;Polynomials;State feedback","autoregressive moving average processes;control engineering computing;data handling;decision theory;dynamic programming;feedback;iterative methods;knowledge based systems;learning (artificial intelligence);Markov processes;observability;optimal control;polynomial approximation","reinforcement learning;adaptive dynamic programming;linear dynamical system;deterministic behavior;control system community;decision processes;output feedback;chastic equivalent system;observable Markov decision process;iteration algorithm;optimal controller;learning algorithm;polynomial autoregressive moving average controller;optimal state variable feedback gain;control system theory;partially observable dynamic processes","Algorithms;Artificial Intelligence;Feedback;Learning;Markov Chains;Reinforcement (Psychology)","355","4","44","IEEE","29 Mar 2010","","","IEEE","IEEE Journals"
"Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition","Y. Tang; Y. Tian; J. Lu; P. Li; J. Zhou","Beijing National Research Center for Information Science and Technology, China; Department of Automation, Tsinghua University, China; Beijing National Research Center for Information Science and Technology, China; Department of Automation, Tsinghua University, China; Beijing National Research Center for Information Science and Technology, China","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","16 Dec 2018","2018","","","5323","5332","In this paper, we propose a deep progressive reinforcement learning (DPRL) method for action recognition in skeleton-based videos, which aims to distil the most informative frames and discard ambiguous frames in sequences for recognizing actions. Since the choices of selecting representative frames are multitudinous for each video, we model the frame selection as a progressive process through deep reinforcement learning, during which we progressively adjust the chosen frames by taking two important factors into account: (1) the quality of the selected frames and (2) the relationship between the selected frames to the whole video. Moreover, considering the topology of human body inherently lies in a graph-based structure, where the vertices and edges represent the hinged joints and rigid bones respectively, we employ the graph-based convolutional neural network to capture the dependency between the joints for action recognition. Our approach achieves very competitive performance on three widely used benchmarks.","2575-7075","978-1-5386-6420-9","10.1109/CVPR.2018.00558","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578656","","Joints;Videos;Biological system modeling;Three-dimensional displays;Computer vision","convolutional neural nets;graph theory;image capture;image motion analysis;image recognition;image representation;image sequences;learning (artificial intelligence);video signal processing","skeleton-based action recognition;deep progressive reinforcement learning method;skeleton-based videos;frame selection;graph-based structure;graph-based convolutional neural network","","256","","55","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Adaptive Fault-Tolerant Tracking Control for Discrete-Time Multiagent Systems via Reinforcement Learning Algorithm","H. Li; Y. Wu; M. Chen","School of Automation, Guangdong University of Technology, Guangzhou, China; College of Engineering, Bohai University, Jinzhou, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IEEE Transactions on Cybernetics","17 Feb 2021","2021","51","3","1163","1174","This article investigates the adaptive fault-tolerant tracking control problem for a class of discrete-time multiagent systems via a reinforcement learning algorithm. The action neural networks (NNs) are used to approximate unknown and desired control input signals, and the critic NNs are employed to estimate the cost function in the design procedure. Furthermore, the direct adaptive optimal controllers are designed by combining the backstepping technique with the reinforcement learning algorithm. Comparing the existing reinforcement learning algorithm, the computational burden can be effectively reduced by using the method of less learning parameters. The adaptive auxiliary signals are established to compensate for the influence of the dead zones and actuator faults on the control performance. Based on the Lyapunov stability theory, it is proved that all signals of the closed-loop system are semiglobally uniformly ultimately bounded. Finally, some simulation results are presented to illustrate the effectiveness of the proposed approach.","2168-2275","","10.1109/TCYB.2020.2982168","Local Innovative and Research Teams Project of Guangdong Special Support Program of 2019; Innovative Research Team Program of Guangdong Province Science Foundation(grant numbers:2018B030312006); Science and Technology Program of Guangzhou(grant numbers:201904020006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9086141","Discrete-time multiagent systems (MASs);fault-tolerant control;neural networks (NNs);reinforcement learning algorithm","Reinforcement learning;Artificial neural networks;Actuators;Fault tolerance;Fault tolerant systems;Estimation;Multi-agent systems","actuators;adaptive control;closed loop systems;control nonlinearities;control system synthesis;discrete time systems;estimation theory;fault tolerant control;learning (artificial intelligence);Lyapunov methods;multi-agent systems;neurocontrollers;nonlinear control systems;optimal control;stability","control input signals;direct adaptive optimal controllers;reinforcement learning algorithm;adaptive auxiliary signals;discrete-time multiagent systems;adaptive fault-tolerant tracking control;Lyapunov stability theory;closed-loop system;neural networks;cost function estimation","","236","","45","IEEE","4 May 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning With Function Approximation for Traffic Signal Control","P. LA; S. Bhatnagar","Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India","IEEE Transactions on Intelligent Transportation Systems","31 May 2011","2011","12","2","412","421","We propose, for the first time, a reinforcement learning (RL) algorithm with function approximation for traffic signal control. Our algorithm incorporates state-action features and is easily implementable in high-dimensional settings. Prior work, e.g., the work of Abdulhai , on the application of RL to traffic signal control requires full-state representations and cannot be implemented, even in moderate-sized road networks, because the computational complexity exponentially grows in the numbers of lanes and junctions. We tackle this problem of the curse of dimensionality by effectively using feature-based state representations that use a broad characterization of the level of congestion as low, medium, or high. One advantage of our algorithm is that, unlike prior work based on RL, it does not require precise information on queue lengths and elapsed times at each lane but instead works with the aforementioned described features. The number of features that our algorithm requires is linear to the number of signaled lanes, thereby leading to several orders of magnitude reduction in the computational complexity. We perform implementations of our algorithm on various settings and show performance comparisons with other algorithms in the literature, including the works of Abdulhai  and Cools , as well as the fixed-timing and the longest queue algorithms. For comparison, we also develop an RL algorithm that uses full-state representation and incorporates prioritization of traffic, unlike the work of Abdulhai  We observe that our algorithm outperforms all the other algorithms on all the road network settings that we consider.","1558-0016","","10.1109/TITS.2010.2091408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5658157","Q-learning with full-state representation (QTLC-FS);Q-learning with function approximation (QTLC-FA);reinforcement learning (RL);traffic signal control","Approximation algorithms;Roads;Function approximation;Junctions;Timing;Equations;Software algorithms","function approximation;learning (artificial intelligence);traffic control;traffic engineering computing","reinforcement learning;function approximation;traffic signal control;full-state representation;moderate-sized road networks;computational complexity;curse of dimensionality;feature-based state representations;magnitude reduction","","185","","29","IEEE","6 Dec 2010","","","IEEE","IEEE Journals"
"NN Reinforcement Learning Adaptive Control for a Class of Nonstrict-Feedback Discrete-Time Systems","W. Bai; T. Li; S. Tong","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; Navigation College, Dalian Maritime University, Dalian, China","IEEE Transactions on Cybernetics","26 Oct 2020","2020","50","11","4573","4584","This article investigates an adaptive reinforcement learning (RL) optimal control design problem for a class of nonstrict-feedback discrete-time systems. Based on the neural network (NN) approximating ability and RL control design technique, an adaptive backstepping RL optimal controller and a minimal learning parameter (MLP) adaptive RL optimal controller are developed by establishing a novel strategic utility function and introducing external function terms. It is proved that the proposed adaptive RL optimal controllers can guarantee that all signals in the closed-loop systems are semiglobal uniformly ultimately bounded (SGUUB). The main feature is that the proposed schemes can solve the optimal control problem that the previous literature cannot deal with. Furthermore, the proposed MPL adaptive optimal control scheme can reduce the number of adaptive laws, and thus the computational complexity is decreased. Finally, the simulation results illustrate the validity of the proposed optimal control schemes.","2168-2275","","10.1109/TCYB.2020.2963849","National Natural Science Foundation of China(grant numbers:51939001,61903092,61976033,61903174,61751202,U1813203); Science and Technology Innovation Funds of Dalian(grant numbers:2018J11CY022); Natural Science Foundation of Liaoning Province(grant numbers:2019-ZD-0151); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8968420","Adaptive control;neural network (NN);nonstrict-feedback systems;optimal controller;reinforcement learning (RL)","Optimal control;Adaptive systems;Backstepping;Nonlinear systems;Discrete-time systems;Control design;Reinforcement learning","adaptive control;closed loop systems;control nonlinearities;control system synthesis;discrete time systems;feedback;learning (artificial intelligence);neurocontrollers;nonlinear control systems;optimal control","adaptive reinforcement learning optimal control design problem;nonstrict-feedback discrete-time systems;neural network approximating ability;adaptive backstepping;minimal learning parameter;strategic utility function;adaptive RL optimal controllers;closed-loop systems;adaptive laws;NN reinforcement learning adaptive control;external function terms;semiglobal uniformly ultimately bounded;SGUUB;MPL adaptive optimal control scheme","","166","","52","IEEE","24 Jan 2020","","","IEEE","IEEE Journals"
"Multiobjective Reinforcement Learning: A Comprehensive Overview","C. Liu; X. Xu; D. Hu","College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; Institute of Unmanned Systems, College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; Department of Automatic Control, National University of Defense Technology, Changsha, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","20 May 2017","2015","45","3","385","398","Reinforcement learning (RL) is a powerful paradigm for sequential decision-making under uncertainties, and most RL algorithms aim to maximize some numerical value which represents only one long-term objective. However, multiple long-term objectives are exhibited in many real-world decision and control systems, so recently there has been growing interest in solving multiobjective reinforcement learning (MORL) problems where there are multiple conflicting objectives. The aim of this paper is to present a comprehensive overview of MORL. The basic architecture, research topics, and naïve solutions of MORL are introduced at first. Then, several representative MORL approaches and some important directions of recent research are comprehensively reviewed. The relationships between MORL and other related research are also discussed, which include multiobjective optimization, hierarchical RL, and multiagent RL. Moreover, research challenges and open problems of MORL techniques are suggested.","2168-2232","","10.1109/TSMC.2014.2358639","Program for New Century Excellent Talents in Universities(grant numbers:NCET-10-0901); National Fundamental Research Program of China(grant numbers:2013CB329401); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918520","Markov decision process (MDP);multiobjective reinforcement learning (MORL);Pareto front;reinforcement learning (RL);sequential decision-making;Markov decision process (MDP);multiobjective reinforcement learning (MORL);Pareto front;reinforcement learning (RL);sequential decision-making","Decision making;Approximation algorithms;Linear programming;Approximation methods;Optimization;Equations;Vectors","decision making;learning (artificial intelligence);multi-agent systems;optimisation","multiobjective reinforcement learning;sequential decision-making;RL algorithms;MORL;multiobjective optimization;hierarchical RL;multiagent RL","","142","","105","IEEE","8 Oct 2014","","","IEEE","IEEE Journals"
"Combining Planning and Deep Reinforcement Learning in Tactical Decision Making for Autonomous Driving","C. -J. Hoel; K. Driggs-Campbell; K. Wolff; L. Laine; M. J. Kochenderfer","Department of Vehicle Automation, Volvo Group, Gothenburg, Sweden; Electrical and Computer Engineering Department, University of Illinois at Urbana-Champaign, USA; Department of Mechanics and Maritime Sciences, Chalmers University of Technology, Gothenburg, Sweden; Department of Vehicle Automation, Volvo Group, Gothenburg, Sweden; Aeronautics and Astronautics Department at Stanford University, Stanford, USA","IEEE Transactions on Intelligent Vehicles","25 May 2020","2020","5","2","294","305","Tactical decision making for autonomous driving is challenging due to the diversity of environments, the uncertainty in the sensor information, and the complex interaction with other road users. This article introduces a general framework for tactical decision making, which combines the concepts of planning and learning, in the form of Monte Carlo tree search and deep reinforcement learning. The method is based on the AlphaGo Zero algorithm, which is extended to a domain with a continuous state space where self-play cannot be used. The framework is applied to two different highway driving cases in a simulated environment and it is shown to perform better than a commonly used baseline method. The strength of combining planning and learning is also illustrated by a comparison to using the Monte Carlo tree search or the neural network policy separately.","2379-8904","","10.1109/TIV.2019.2955905","Knut och Alice Wallenbergs Stiftelse; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911507","Autonomous driving;tactical decision making;reinforcement learning;Monte Carlo tree search","Planning;Decision making;Autonomous vehicles;Monte Carlo methods;Reinforcement learning;Road transportation;Neural networks","control engineering computing;decision making;learning (artificial intelligence);mobile robots;Monte Carlo methods;neural nets;road vehicles;tree searching","deep reinforcement learning;tactical decision making;autonomous driving;Monte Carlo tree search;highway driving cases;neural network;AlphaGo Zero algorithm;sensor information","","123","","32","IEEE","25 Nov 2019","","","IEEE","IEEE Journals"
"Multi-Agent Deep Reinforcement Learning for HVAC Control in Commercial Buildings","L. Yu; Y. Sun; Z. Xu; C. Shen; D. Yue; T. Jiang; X. Guan","College of Automation and College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China; Ministry of Education Key Lab for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, China; Ministry of Education Key Lab for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, China; Institute of Advanced Technology, College of Automation and College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; Wuhan National Laboratory for Optoelectronics, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Ministry of Education Key Lab for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, China","IEEE Transactions on Smart Grid","21 Dec 2020","2021","12","1","407","419","In commercial buildings, about 40%-50% of the total electricity consumption is attributed to Heating, Ventilation, and Air Conditioning (HVAC) systems, which places an economic burden on building operators. In this paper, we intend to minimize the energy cost of an HVAC system in a multi-zone commercial building with the consideration of random zone occupancy, thermal comfort, and indoor air quality comfort. Due to the existence of unknown thermal dynamics models, parameter uncertainties (e.g., outdoor temperature, electricity price, and number of occupants), spatially and temporally coupled constraints associated with indoor temperature and CO2 concentration, a large discrete solution space, and a non-convex and non-separable objective function, it is very challenging to achieve the above aim. To this end, the above energy cost minimization problem is reformulated as a Markov game. Then, an HVAC control algorithm is proposed to solve the Markov game based on multi-agent deep reinforcement learning with attention mechanism. The proposed algorithm does not require any prior knowledge of uncertain parameters and can operate without knowing building thermal dynamics models. Simulation results based on real-world traces show the effectiveness, robustness and scalability of the proposed algorithm.","1949-3061","","10.1109/TSG.2020.3011739","National Key Research and Development Program of China(grant numbers:2019YFB1312); National Natural Science Foundation of China(grant numbers:61972214,61803297,61822309,61773310,61771258); China Postdoctoral Science Foundation(grant numbers:2020M673406); 1311 Talent Project of Nanjing University of Posts and Telecommunications; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9146920","Commercial buildings;HVAC systems;energy cost;multi-zone coordination;random occupancy;thermal comfort;indoor air quality comfort;multi-agent deep reinforcement learning","Buildings;Air quality;Atmospheric modeling;Temperature;Heuristic algorithms;Machine learning;Fans","air conditioning;building management systems;HVAC;learning (artificial intelligence);Markov processes;power consumption;ventilation","multiagent deep reinforcement learning;commercial buildings;total electricity consumption;Air Conditioning systems;economic burden;building operators;HVAC system;multizone commercial building;random zone occupancy;thermal comfort;indoor air quality comfort;unknown thermal dynamics models;outdoor temperature;electricity price;indoor temperature;energy cost minimization problem;Markov game;HVAC control algorithm","","122","","42","IEEE","24 Jul 2020","","","IEEE","IEEE Journals"
"Two-Timescale Voltage Control in Distribution Grids Using Deep Reinforcement Learning","Q. Yang; G. Wang; A. Sadeghi; G. B. Giannakis; J. Sun","State Key Laboratory of Intelligent Control and Decision of Complex Systems, School of Automation, Beijing Institute of Technology, Beijing, China; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, USA; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, USA; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, USA; State Key Laboratory of Intelligent Control and Decision of Complex Systems, School of Automation, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Smart Grid","21 Apr 2020","2020","11","3","2313","2323","Modern distribution grids are currently being challenged by frequent and sizable voltage fluctuations, due mainly to the increasing deployment of electric vehicles and renewable generators. Existing approaches to maintaining bus voltage magnitudes within the desired region can cope with either traditional utility-owned devices (e.g., shunt capacitors), or contemporary smart inverters that come with distributed generation units (e.g., photovoltaic plants). The discrete on-off commitment of capacitor units is often configured on an hourly or daily basis, yet smart inverters can be controlled within milliseconds, thus challenging joint control of these two types of assets. In this context, a novel two-timescale voltage regulation scheme is developed for distribution grids by judiciously coupling data-driven with physics-based optimization. On a faster timescale, say every second, the optimal setpoints of smart inverters are obtained by minimizing instantaneous bus voltage deviations from their nominal values, based on either the exact alternating current power flow model or a linear approximant of it; whereas, on the slower timescale (e.g., every hour), shunt capacitors are configured to minimize the long-term discounted voltage deviations using a deep reinforcement learning algorithm. Extensive numerical tests on a real-world 47-bus distribution network as well as the IEEE 123-bus test feeder using real data corroborate the effectiveness of the novel scheme.","1949-3061","","10.1109/TSG.2019.2951769","National Natural Science Foundation of China(grant numbers:61522303,61720106011,61621063); China Scholarship Council; National Science Foundation(grant numbers:1509040,1711471,1901134); National Natural Science Foundation of China(grant numbers:61522303,61720106011,61621063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8892476","Two timescales;voltage control;inverters;capacitors;deep reinforcement learning","Inverters;Capacitors;Voltage control;Reactive power;Reinforcement learning;Solar power generation;Optimization","distributed power generation;invertors;learning (artificial intelligence);load flow;neural nets;optimisation;power capacitors;power distribution control;power engineering computing;power generation control;power grids;voltage control","electric vehicles;renewable generators;utility-owned devices;shunt capacitors;smart inverters;numerical tests;voltage fluctuations;distribution grids;voltage control;IEEE 123-bus test feeder;47-bus distribution network;deep reinforcement learning algorithm;alternating current power flow model;physics-based optimization;two-timescale voltage regulation scheme;distributed generation units","","121","","38","IEEE","6 Nov 2019","","","IEEE","IEEE Journals"
"3D UAV Trajectory Design and Frequency Band Allocation for Energy-Efficient and Fair Communication: A Deep Reinforcement Learning Approach","R. Ding; F. Gao; X. S. Shen","Institute for Artificial Intelligence, Tsinghua University (THUAI), Beijing, China; Institute for Artificial Intelligence, Tsinghua University (THUAI), Beijing, China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada","IEEE Transactions on Wireless Communications","10 Dec 2020","2020","19","12","7796","7809","Unmanned Aerial Vehicle (UAV)-assisted communication has drawn increasing attention recently. In this paper, we investigate 3D UAV trajectory design and band allocation problem considering both the UAV's energy consumption and the fairness among the ground users (GUs). Specifically, we first formulate the energy consumption model of a quad-rotor UAV as a function of the UAV's 3D movement. Then, based on the fairness and the total throughput, the fair throughput is defined and maximized within limited energy. We propose a deep reinforcement learning (DRL)-based algorithm, named as EEFC-TDBA (energy-efficient fair communication through trajectory design and band allocation) that chooses the state-of-the-art DRL algorithm, deep deterministic policy gradient (DDPG), as its basis. EEFC-TDBA allows the UAV to: 1) adjust the flight speed and direction so as to enhance the energy efficiency and reach the destination before the energy is exhausted; and 2) allocate frequency band to achieve fair communication service. Simulation results are provided to demonstrate that EEFC-TDBA outperforms the baseline methods in terms of the fairness, the total throughput, as well as the minimum throughput.","1558-2248","","10.1109/TWC.2020.3016024","National Key Research and Development Program of China(grant numbers:2018AAA0102401); National Natural Science Foundation of China(grant numbers:61831013,61771274,61531011); Beijing Municipal Natural Science Foundation(grant numbers:4182030,L182042); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9171468","3D UAV trajectory;band allocation;energy-efficient;fair communication;deep reinforcement learning","Mathematical model;Maintenance engineering;Biological system modeling;Numerical models;Weibull distribution;Unmanned aerial vehicles;Atmospheric modeling","autonomous aerial vehicles;energy conservation;frequency allocation;gradient methods;learning (artificial intelligence);telecommunication control;telecommunication power management;trajectory control;velocity control","Unmanned Aerial Vehicle-assisted communication;UAV's energy consumption;energy consumption model;quad-rotor UAV;UAV's 3D movement;fair throughput;deep reinforcement learning-based algorithm;EEFC-TDBA;energy-efficient fair communication;trajectory design;frequency band allocation;DRL algorithm;deep deterministic policy gradient;energy efficiency;fair communication service","","110","","43","IEEE","19 Aug 2020","","","IEEE","IEEE Journals"
"Cooperative Deep Reinforcement Learning for Large-Scale Traffic Grid Signal Control","T. Tan; F. Bao; Y. Deng; A. Jin; Q. Dai; J. Wang","Department of Civil and Environmental Engineering, Stanford University, Stanford, USA; Department of Automation, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; School of Astronautics, Beihang University, Beijing, China; Computer Science Department, Stanford University, Stanford, USA; Department of Automation, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Department of Civil and Environmental Engineering, Stanford University, Stanford, USA","IEEE Transactions on Cybernetics","7 May 2020","2020","50","6","2687","2700","Exploiting reinforcement learning (RL) for traffic congestion reduction is a frontier topic in intelligent transportation research. The difficulty in this problem stems from the inability of the RL agent simultaneously monitoring multiple signal lights when taking into account complicated traffic dynamics in different regions of a traffic system. Such challenge is even more outstanding when forming control decisions on a large-scale traffic grid, where the RL action space grows exponentially with the number of intersections within the traffic grid. In this paper, we tackle such a problem by proposing a cooperative deep reinforcement learning (Coder) framework. The intuition behind Coder is to decompose the original difficult RL task as a number of subproblems with relatively easy RL goals. Accordingly, we implement Coder with multiple regional agents and a centralized global agent. Each regional agent learns its own RL policy and value functions over a small region with limited actions. Then, the centralized global agent hierarchically aggregates RL achievements from different regional agents and forms the final Q-function over the entire large-scale traffic grid. The experimental investigations demonstrate that the proposed Coder could reduce on average 30% congestions in terms of the number of waiting vehicles during high density traffic flows in simulations.","2168-2275","","10.1109/TCYB.2019.2904742","Stanford Center for Sustainable Development and Global Competitiveness; Project of Beijing Municipal Science and Technology Commission(grant numbers:Z181100003118014); Project of NSFC(grant numbers:61327902); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8676356","Deep reinforcement learning (DRL);intelligent transportation systems;traffic signal control","Reinforcement learning;Aerospace electronics;Transportation;Games;Vehicle dynamics;Task analysis","control engineering computing;learning (artificial intelligence);multi-agent systems;road traffic control;traffic engineering computing","value functions;large-scale traffic grid signal control;traffic congestion reduction;intelligent transportation research;RL agent;multiple signal lights;traffic dynamics;traffic system;control decisions;RL action space;deep reinforcement learning framework;RL task;RL goals","","110","","51","IEEE","29 Mar 2019","","","IEEE","IEEE Journals"
"Leader–Follower Output Synchronization of Linear Heterogeneous Systems With Active Leader Using Reinforcement Learning","Y. Yang; H. Modares; D. C. Wunsch; Y. Yin","Key Laboratory of Knowledge Automation for Industrial Process, University of Science and Technology Beijing, Ministry of Education, Beijing, China; Department of Electrical and Computer Engineering, Missouri University of Science and Technology, Rolla, MO, USA; Department of Electrical and Computer Engineering, Missouri University of Science and Technology, Rolla, MO, USA; Key Laboratory of Knowledge Automation for Industrial Process, University of Science and Technology Beijing, Ministry of Education, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","16 May 2018","2018","29","6","2139","2153","This paper develops optimal control protocols for the distributed output synchronization problem of leader-follower multiagent systems with an active leader. Agents are assumed to be heterogeneous with different dynamics and dimensions. The desired trajectory is assumed to be preplanned and is generated by the leader. Other follower agents autonomously synchronize to the leader by interacting with each other using a communication network. The leader is assumed to be active in the sense that it has a nonzero control input so that it can act independently and update its control to keep the followers away from possible danger. A distributed observer is first designed to estimate the leader's state and generate the reference signal for each follower. Then, the output synchronization of leader-follower systems with an active leader is formulated as a distributed optimal tracking problem, and inhomogeneous algebraic Riccati equations (AREs) are derived to solve it. The resulting distributed optimal control protocols not only minimize the steady-state error but also optimize the transient response of the agents. An off-policy reinforcement learning algorithm is developed to solve the inhomogeneous AREs online in real time and without requiring any knowledge of the agents' dynamics. Finally, two simulation examples are conducted to illustrate the effectiveness of the proposed algorithm.","2162-2388","","10.1109/TNNLS.2018.2803059","National Natural Science Foundation of China(grant numbers:61333002); China Scholarship Council(grant numbers:201406460057); Mary K. Finley Endowment; Missouri S&T Intelligent Systems Center; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8306304","Active leader;heterogeneous system;inhomogeneous algebraic Riccati equations (AREs);output synchronization;reinforcement learning (RL);unknown follower","Synchronization;Protocols;Trajectory;Observers;Heuristic algorithms;Nonhomogeneous media;Learning (artificial intelligence)","distributed control;learning (artificial intelligence);multi-agent systems;optimal control;Riccati equations;synchronisation","linear heterogeneous systems;synchronization;leader-follower multiagent systems;reinforcement learning;communication network;algebraic Riccati equation;steady-state error;ARE;distributed optimal control protocol","","98","","65","IEEE","2 Mar 2018","","","IEEE","IEEE Journals"
"Memory-Based Deep Reinforcement Learning for Obstacle Avoidance in UAV With Limited Environment Knowledge","A. Singla; S. Padakandla; S. Bhatnagar","Robert Bosch Centre for Cyber-Physical Systems, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bengaluru, India; Department of Computer Science and Automation, Indian Institute of Science, Bengaluru, India","IEEE Transactions on Intelligent Transportation Systems","25 Dec 2020","2021","22","1","107","118","This paper presents our method for enabling a UAV quadrotor, equipped with a monocular camera, to autonomously avoid collisions with obstacles in unstructured and unknown indoor environments. When compared to obstacle avoidance in ground vehicular robots, UAV navigation brings in additional challenges because the UAV motion is no more constrained to a well-defined indoor ground or street environment. Unlike ground vehicular robots, a UAV has to navigate across more types of obstacles - for e.g., objects like decorative items, furnishings, ceiling fans, sign-boards, tree branches, etc., are also potential obstacles for a UAV. Thus, methods of obstacle avoidance developed for ground robots are clearly inadequate for UAV navigation. Current control methods using monocular images for UAV obstacle avoidance are heavily dependent on environment information. These controllers do not fully retain and utilize the extensively available information about the ambient environment for decision making. We propose a deep reinforcement learning based method for UAV obstacle avoidance (OA) which is capable of doing exactly the same. The crucial idea in our method is the concept of partial observability and how UAVs can retain relevant information about the environment structure to make better future navigation decisions. Our OA technique uses recurrent neural networks with temporal attention and provides better results compared to prior works in terms of distance covered without collisions. In addition, our technique has a high inference rate and reduces power wastage as it minimizes oscillatory motion of UAV.","1558-0016","","10.1109/TITS.2019.2954952","Robert Bosch Centre for Cyber Physical Systems, Indian Institute of Science, as well as the Department of Science and Technology, through the ICPS Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8917687","Unmanned aerial vehicle (UAV) obstacle avoidance (OA);deep reinforcement learning (DRL);partial observability;deep Q-networks (DQN)","Collision avoidance;Navigation;Cameras;Unmanned aerial vehicles;Simultaneous localization and mapping;Visualization","autonomous aerial vehicles;collision avoidance;decision making;learning (artificial intelligence);mobile robots;recurrent neural nets;robot vision","memory-based deep reinforcement;limited environment knowledge;UAV quadrotor;unstructured environments;unknown indoor environments;ground vehicular robots;UAV navigation;UAV motion;indoor ground;street environment;potential obstacles;ground robots;current control methods;UAV obstacle avoidance;environment information;deep reinforcement learning;environment structure","","84","","42","IEEE","28 Nov 2019","","","IEEE","IEEE Journals"
"Path Planning for UAV Ground Target Tracking via Deep Reinforcement Learning","B. Li; Y. Wu","State Key Laboratory of Virtual Reality Technology and System, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and System, Beihang University, Beijing, China","IEEE Access","14 Feb 2020","2020","8","","29064","29074","In this paper, we focus on the study of UAV ground target tracking under obstacle environments using deep reinforcement learning, and an improved deep deterministic policy gradient (DDPG) algorithm is presented. A reward function based on line of sight and artificial potential field is constructed to guide the behavior of UAV to achieve target tracking, and a penalty term of action makes the trajectory smooth. In order to improve the exploration ability, multiple UAVs, which controlled by the same policy network, are used to perform tasks in each episode. Taking into account that the history observations have a great degree of correlation with the policy, long short-term memory networks are used to approximate the state of environments, which improve the approximation accuracy and the efficiency of data utilization. The simulation results show that the propose method can make the UAV keep target tracking and obstacle avoidance effectively.","2169-3536","","10.1109/ACCESS.2020.2971780","National Natural Science Foundation of China(grant numbers:91216304); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8984371","DDPG;deep reinforcement learning;obstacle avoidance;target tracking;UAV","Target tracking;Unmanned aerial vehicles;Collision avoidance;Path planning;Heuristic algorithms;Sensors;Reinforcement learning","autonomous aerial vehicles;collision avoidance;learning (artificial intelligence);mobile robots;multi-robot systems;neurocontrollers;path planning","policy network;long short-term memory networks;path planning;UAV ground target tracking;deep reinforcement learning;deep deterministic policy gradient algorithm;artificial potential field;obstacle avoidance","","71","","41","CCBY","5 Feb 2020","","","IEEE","IEEE Journals"
"MEC—A Near-Optimal Online Reinforcement Learning Algorithm for Continuous Deterministic Systems","D. Zhao; Y. Zhu","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","20 May 2017","2015","26","2","346","356","In this paper, the first probably approximately correct (PAC) algorithm for continuous deterministic systems without relying on any system dynamics is proposed. It combines the state aggregation technique and the efficient exploration principle, and makes high utilization of online observed samples. We use a grid to partition the continuous state space into different cells to save samples. A near-upper Q operator is defined to produce a near-upper Q function using samples in each cell. The corresponding greedy policy effectively balances between exploration and exploitation. With the rigorous analysis, we prove that there is a polynomial time bound of executing nonoptimal actions in our algorithm. After finite steps, the final policy reaches near optimal in the framework of PAC. The implementation requires no knowledge of systems and has less computation complexity. Simulation studies confirm that it is a better performance than other similar PAC algorithms.","2162-2388","","10.1109/TNNLS.2014.2371046","National Natural Science Foundation of China(grant numbers:61273136,61034002); Natural Science Foundation of Beijing(grant numbers:4122083); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6971146","Efficient exploration;probably approximately correct (PAC);reinforcement learning (RL);state aggregation.;Efficient exploration;probably approximately correct (PAC);reinforcement learning (RL);state aggregation","Heuristic algorithms;Algorithm design and analysis;Approximation algorithms;Polynomials;Upper bound;Learning systems;Partitioning algorithms","computational complexity;continuous systems;learning (artificial intelligence)","MEC algorithm;near-optimal online reinforcement learning algorithm;continuous deterministic system;probably approximately correct algorithm;PAC algorithm;system dynamics;state aggregation technique;exploration principle;near-upper Q operator;near-upper Q function;greedy policy;polynomial time bound","","69","","43","IEEE","2 Dec 2014","","","IEEE","IEEE Journals"
"Game Theory and Reinforcement Learning Based Secure Edge Caching in Mobile Social Networks","Q. Xu; Z. Su; R. Lu","School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; Faculty of Computer Science, University of New Brunswick, Fredericton, Canada","IEEE Transactions on Information Forensics and Security","29 Jul 2020","2020","15","","3415","3429","Edge caching has become one of promising technologies in mobile social networks (MSNs) to proximally provide popular contents for mobile users. However, since caching contents inevitably consume resources (e.g., power, bandwidth, storage, etc.), edge caching devices maybe selfish to cheat the content provider for earning service fees. In addition, due to the open access of edge caching devices, the edge caching service is vulnerable to various attacks, such as man-in-the-middle attack and content tamper attack, etc., resulting in the degradation of content delivery performance. To efficiently tackle the above problems, in this paper, we propose a secure edge caching scheme for the content provider and mobile users in MSNs. Specifically, we first develop a secure edge caching framework consisting of the content provider, multiple edge caching devices, and some mobile users. To motivate the participation of edge caching devices, Stackelberg game is exploited to model the interactions between the content provider and edge caching devices. The content provider serves as the game-leader to determine the payment strategy of secure caching service and each edge caching device is the game-follower to make the strategy on the quality of secure caching service. Especially, the zero payment mechanism is adopted to suppress the selfish behaviors of edge caching devices. Apart from this, for lack of the knowledge on interactions between the content provider and edge caching devices in dynamic network scenarios, we also employ the Q-leaning to derive the optimal payment strategy of the content provider and the security strategy of edge caching device. Extensive simulations are conducted, and results demonstrate that the proposed scheme can efficiently motivate edge caching devices to provide the content provider and mobile users with high-quality secure caching services.","1556-6021","","10.1109/TIFS.2020.2980823","NSFC(grant numbers:U1808207,91746114); Higher Education Discipline Innovation Project; Project of Shanghai Municipal Science and Technology Commission(grant numbers:18510761000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9036917","Mobile social networks (MSNs);secure edge caching service;Stackelberg game;reinforcement learning;zero payment punishment","Games;Social networking (online);Performance evaluation;Knowledge engineering;Security;Wireless communication;Quality of experience","cache storage;game theory;learning (artificial intelligence);mobile computing;security of data;social networking (online);telecommunication security","security strategy;Q-learning;zero payment mechanism;game-follower;game-leader;Stackelberg game;content delivery performance;mobile social networks;reinforcement learning;game theory;secure edge caching;secure caching service;edge caching service;content provider;edge caching device;mobile users","","56","","48","IEEE","16 Mar 2020","","","IEEE","IEEE Journals"
"Approximate Nash Solutions for Multiplayer Mixed-Zero-Sum Game With Reinforcement Learning","Y. Lv; X. Ren","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","19 Nov 2019","2019","49","12","2739","2750","Inspired by Nash game theory, a multiplayer mixed-zero-sum (MZS) nonlinear game considering both two situations [zero-sum and nonzero-sum (NZS) Nash games] is proposed in this paper. A synchronous reinforcement learning (RL) scheme based on the identifier-critic structure is developed to learn the Nash equilibrium solution of the proposed MZS game. First, the MZS game formulation is presented, where the performance indexes for players 1 to N - 1 and N NZS Nash game are presented, and another performance index for players N and N + 1 zero-sum game is presented, such that player N cooperates with players 1 to N - 1, while competes with player N + 1, which leads to a Nash equilibrium of all players. A single-layer neural network (NN) is then used to approximate the unknown dynamics of the nonlinear game system. Finally, an RL scheme based on NNs is developed to learn the optimal performance indexes, which can be used to produce the optimal control policy of every player such that Nash equilibrium can be obtained. Thus, the widely used actor NN in RL literature is not needed. To this end, a recently proposed adaptive law is used to estimate the unknown identifier coefficient vectors, and an improved adaptive law with the error performance index is further developed to update the critic coefficient vectors. Both linear and nonlinear simulations are presented to demonstrate the existence of Nash equilibrium for MZS game and performance of the proposed algorithm.","2168-2232","","10.1109/TSMC.2018.2861826","National Natural Science Foundation of China(grant numbers:61433003,61573174); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8438886","Approximate dynamic programming (ADP);Nash games;neural networks (NNs);reinforcement learning (RL);system identification","Games;Nash equilibrium;Performance analysis;Artificial neural networks;Heuristic algorithms;Approximation algorithms","adaptive control;approximation theory;game theory;learning systems;neurocontrollers;optimal control;performance index","approximate Nash solutions;multiplayer mixed-zero-sum game;Nash game theory;multiplayer mixed-zero-sum nonlinear game;nonzero-sum;synchronous reinforcement learning scheme;identifier-critic structure;Nash equilibrium solution;MZS game formulation;approximate the unknown dynamics;nonlinear game system;RL scheme;optimal performance indexes;unknown identifier coefficient vectors;error performance index;NZS Nash game","","47","","47","IEEE","17 Aug 2018","","","IEEE","IEEE Journals"
"Integral Reinforcement Learning for online computation of feedback Nash strategies of nonzero-sum differential games","D. Vrabie; F. Lewis","Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA; Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA","49th IEEE Conference on Decision and Control (CDC)","22 Feb 2011","2010","","","3066","3071","This paper presents an Approximate/Adaptive Dynamic Programming (ADP) algorithm that finds online the Nash equilibrium for two-player nonzero-sum differential games with linear dynamics and infinite horizon quadratic cost. Each of the game players is using the procedure of Integral Reinforcement Learning (IRL) to calculate online the infinite horizon value function that it associates with every given set of feedback control policies. It will be shown that the online algorithm is mathematically equivalent to an offline iterative method, previously introduced in the literature, that solves the set of coupled algebraic Riccati equations (ARE) underlying the game problem using complete knowledge on the system dynamics. Here we show how the ADP techniques will enhance the capabilities of the offline method allowing an online solution without the requirement of complete knowledge of the system dynamics. The two participants in the continuous-time differential game are competing in real-time and the feedback Nash control strategies will be determined based on online measured data from the system. The algorithm is built on interplay between a learning phase, where each of the players is learning online the value that they associate with a given set of play policies, and a policy update step, performed by each of the payers towards decreasing the value of their cost. The players are learning concurrently. The feasibility of the ADP scheme is demonstrated in simulation.","0191-2216","978-1-4244-7746-3","10.1109/CDC.2010.5718152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718152","","Games;Heuristic algorithms;Learning;Nash equilibrium;Cost function;Infinite horizon","differential games;dynamic programming;iterative methods;learning (artificial intelligence);Riccati equations","integral reinforcement learning;online computation;feedback Nash strategies;approximate-adaptive dynamic programming algorithm;two-player nonzero-sum differential games;linear dynamics;infinite horizon quadratic cost;infinite horizon value function;offline iterative method;coupled algebraic Riccati equations","","47","","24","IEEE","22 Feb 2011","","","IEEE","IEEE Conferences"
"Privacy Preserving Load Control of Residential Microgrid via Deep Reinforcement Learning","Z. Qin; D. Liu; H. Hua; J. Cao","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; College of Energy and Electrical Engineering, Hohai University, Nanjing, China; Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Smart Grid","20 Aug 2021","2021","12","5","4079","4089","Demand side management has been proved to be effective in improving the operating efficiency of microgrids, while posing a severe threat to user privacy. This paper proposes a novel privacy preserving load control scheme for the residential microgrid, in which the microgrid operator manages a multitude of home appliances including electric vehicles (EVs) and air conditioners (ACs). This problem is formulated as a partially observable Markov decision process, since users' privacy information including indoor temperatures associated with ACs and arrival/departure times of EVs cannot be observed by microgrid operator. To address the formulated problem with high-dimensional continuous action space caused by massive controllable appliances, we develop a novel deep reinforcement learning algorithm by introducing credit assignment mechanism. Moreover, we integrate recurrent neural network to accommodate the partial observability of state due to privacy issues. Simulation results demonstrate the superiority and flexibility of the developed algorithm and verify the advantages of the proposed scheme compared with prior privacy preserving load control method.","1949-3061","","10.1109/TSG.2021.3088290","Tsinghua-Toyota Joint Research Institute Cross-discipline Program; Fundamental Research Funds for the Central Universities of China(grant numbers:B200201071); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9451164","Deep reinforcement learning;load control;privacy preserving;residential microgrid","Microgrids;Privacy;Load flow control;Smart homes;Aerospace electronics;User experience;Temperature control","data privacy;demand side management;distributed power generation;domestic appliances;electric vehicles;learning (artificial intelligence);load regulation;Markov processes;recurrent neural nets","air conditioners;ACs;partially observable Markov decision process;users;microgrid operator;high-dimensional continuous action space;massive controllable appliances;deep reinforcement learning algorithm;privacy issues;prior privacy preserving load control method;residential microgrid;demand side management;severe threat;user privacy;novel privacy;load control scheme;home appliances","","42","","37","IEEE","10 Jun 2021","","","IEEE","IEEE Journals"
"Using Reinforcement Learning to Minimize the Probability of Delay Occurrence in Transportation","Z. Cao; H. Guo; W. Song; K. Gao; Z. Chen; L. Zhang; X. Zhang","Department of Industrial Systems Engineering and Management, National University of Singapore, Singapore; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; Institute of Marine Science and Technology, Shandong University, Jinan, China; Macau Institute of Systems Engineering, Macau University of Science and Technology, Taipa, China; Institute for Infocomm Research (I2R), Singapore; Institute for Infocomm Research (I2R), Singapore; School of Automation, Guangdong University of Technology, Guangzhou, China","IEEE Transactions on Vehicular Technology","13 Mar 2020","2020","69","3","2424","2436","Reducing traffic delay is of crucial importance for the development of sustainable transportation systems, which is a challenging task in the studies of stochastic shortest path (SSP) problem. Existing methods based on the probability tail model to solve the SSP problem, seek for the path that minimizes the probability of delay occurrence, which is equal to maximizing the probability of reaching the destination before a deadline (i.e., arriving on time). However, they suffer from low accuracy or high computational cost. Therefore, we design a novel and practical Q-learning approach where the converged Q-values have the practical meaning as the actual probabilities of arriving on time so as to improve the accuracy of finding the real optimal path. By further adopting dynamic neural networks to learn the value function, our approach can scale well to large road networks with arbitrary deadlines. Moreover, our approach is flexible to implement in a time dependent manner, which further improves the performance of returned path. Experimental results on some road networks with real mobility data, such as Beijing, Munich and Singapore, demonstrate the significant advantages of the proposed approach over other methods.","1939-9359","","10.1109/TVT.2020.2964784","National Natural Science Foundation of China(grant numbers:61803104,61603169,61772290); Fundamental Research Funds for the Central Universities(grant numbers:63192616); Shandong University(grant numbers:62420079614084); National Research Foundation Singapore(grant numbers:NRF-RSS2016-004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952783","Reinforcement learning;transportation;arriving on time;vehicle routing;Q-learning","Roads;Stochastic processes;Delays;Reinforcement learning;Computational modeling;Shortest path problem;Adaptation models","delays;learning (artificial intelligence);neural nets;probability;road traffic;stochastic processes;traffic engineering computing;transportation","arbitrary deadlines;road networks;reinforcement learning;delay occurrence;traffic delay;sustainable transportation systems;stochastic shortest path problem;probability tail model;SSP problem;computational cost;converged Q-values;actual probabilities;optimal path;dynamic neural networks;value function","","39","","48","IEEE","8 Jan 2020","","","IEEE","IEEE Journals"
"Routing Protocol Design for Underwater Optical Wireless Sensor Networks: A Multiagent Reinforcement Learning Approach","X. Li; X. Hu; R. Zhang; L. Yang","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Key Laboratory of Image Processing and Intelligent Control, and Ministry of Education, Huazhong University of Science and Technology, Wuhan, China; School of Software Engineering, Tongji University, Shanghai, China; Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, CO, USA","IEEE Internet of Things Journal","9 Oct 2020","2020","7","10","9805","9818","Underwater optical wireless sensor networks (UOWSNs) have been attracting many interests for the advantages of high transmission rate, ultrawide bandwidth, and low latency. However, due to limited energy resources and highly dynamic topology caused by the water flow movement, it is challenging to provide a low-consumption and reliable routing in UOWSNs. To tackle this issue, in this article, we propose an efficient routing protocol based on multiagent reinforcement learning, termed as DMARL, for UOWSNs. The network is first modeled as a distributed multiagent system, and residual energy and link quality are considered into the routing protocol design to improve the adaptation to a dynamic environment and the support of prolonging network life. Additionally, two optimization strategies are proposed to accelerate the convergence of the reinforcement learning algorithm. On the basis, a reward mechanism is provided for the distributed system. The simulation results show that the DMARL-based routing protocol has low energy consumption and high packet delivery ratio (over 90%), and it is suitable for networks where the average number of neighbor nodes is less than 14.","2327-4662","","10.1109/JIOT.2020.2989924","National Natural Science Foundation of China(grant numbers:61471177,61901302); Open Research Fund from Shenzhen Research Institute of Big Data(grant numbers:2019ORF01014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076600","Dynamic environment;energy consumption;multiagent reinforcement learning (MARL);routing protocol;underwater optical wireless sensor network (UOWSN)","Routing;Routing protocols;Wireless sensor networks;Reinforcement learning;Heuristic algorithms;Data communication;Optical sensors","energy conservation;learning (artificial intelligence);multi-agent systems;routing protocols;telecommunication computing;telecommunication network reliability;telecommunication network topology;telecommunication power management;underwater optical wireless communication;wireless sensor networks","reward mechanism;DMARL;water flow movement;highly dynamic topology;energy resources;underwater optical wireless sensor networks;energy consumption;routing protocol design;link quality;residual energy;distributed multiagent system;multiagent reinforcement learning;UOWSN","","34","","39","IEEE","23 Apr 2020","","","IEEE","IEEE Journals"
"Reinforcement learning based data fusion method for multi-sensors","T. Zhou; M. Chen; J. Zou","College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Science and Technology on Electron-Optic Control Laboratory, Luoyang Institute of Electro-Optical Equipment of Avic, Luoyang, China","IEEE/CAA Journal of Automatica Sinica","20 Oct 2020","2020","7","6","1489","1497","In order to improve detection system robustness and reliability, multi-sensors fusion is used in modern air combat. In this paper, a data fusion method based on reinforcement learning is developed for multi-sensors. Initially, the cubic B-spline interpolation is used to solve time alignment problems of multi-source data. Then, the reinforcement learning based data fusion ( RLBDF ) method is proposed to obtain the fusion results. With the case that the priori knowledge of target is obtained, the fusion accuracy reinforcement is realized by the error between fused value and actual value. Furthermore, the Fisher information is instead used as the reward if the priori knowledge is unable to be obtained. Simulations results verify that the developed method is feasible and effective for the multi-sensors data fusion in air combat.","2329-9274","","10.1109/JAS.2020.1003180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106866","","Data integration;Reinforcement learning;Splines (mathematics);Interpolation;Curve fitting;Mathematical model","data analysis;interpolation;learning (artificial intelligence);military computing;sensor fusion;splines (mathematics)","Fisher information;multisensors data fusion;fused value;fusion accuracy reinforcement;multisource data;time alignment problems;cubic B-spline interpolation;reinforcement learning;modern air combat","","30","","","","2 Jun 2020","","","IEEE","IEEE Journals"
"A Clustering-Based Graph Laplacian Framework for Value Function Approximation in Reinforcement Learning","X. Xu; Z. Huang; D. Graves; W. Pedrycz","College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland","IEEE Transactions on Cybernetics","20 May 2017","2014","44","12","2613","2625","In order to deal with the sequential decision problems with large or continuous state spaces, feature representation and function approximation have been a major research topic in reinforcement learning (RL). In this paper, a clustering-based graph Laplacian framework is presented for feature representation and value function approximation (VFA) in RL. By making use of clustering-based techniques, that is,  $K$ -means clustering or fuzzy  $C$ -means clustering, a graph Laplacian is constructed by subsampling in Markov decision processes (MDPs) with continuous state spaces. The basis functions for VFA can be automatically generated from spectral analysis of the graph Laplacian. The clustering-based graph Laplacian is integrated with a class of approximation policy iteration algorithms called representation policy iteration (RPI) for RL in MDPs with continuous state spaces. Simulation and experimental results show that, compared with previous RPI methods, the proposed approach needs fewer sample points to compute an efficient set of basis functions and the learning control performance can be improved for a variety of parameter settings.","2168-2275","","10.1109/TCYB.2014.2311578","National Natural Science Foundation of China(grant numbers:61075072,91220301); New Century Excellent Talent Plan(grant numbers:NCET-10-0901); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805593","Approximate dynamic programming;clustering;learning control;Markov decision processes;reinforcement learning;value function approximation;Approximate dynamic programming;clustering;learning control;Markov decision processes;reinforcement learning;value function approximation","Laplace equations;Clustering algorithms;Economic indicators;Approximation algorithms;Function approximation;Aerospace electronics","approximation theory;fuzzy set theory;graph theory;learning (artificial intelligence);Markov processes;pattern clustering","graph Laplacian framework;clustering-based technique;value function approximation;reinforcement learning;sequential decision problem;feature representation;K-means clustering;fuzzy C-means clustering;Markov decision process;continuous state space;spectral analysis;approximation policy iteration algorithm","Algorithms;Artificial Intelligence;Computer Simulation;Models, Theoretical;Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Reinforcement (Psychology)","30","","40","IEEE","25 Apr 2014","","","IEEE","IEEE Journals"
"Reinforcement Learning With Task Decomposition for Cooperative Multiagent Systems","C. Sun; W. Liu; L. Dong","School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; College of Electronics and Information Engineering, Tongji University, Shanghai, China","IEEE Transactions on Neural Networks and Learning Systems","3 May 2021","2021","32","5","2054","2065","In this article, we study cooperative multiagent systems (MASs) with multiple tasks by using reinforcement learning (RL)-based algorithms. The target for a single-agent RL system is represented by its scalar reward signals. However, for an MAS with multiple cooperative tasks, the holistic reward signal consists of multiple parts to represent the tasks, which makes the problem complicated. Existing multiagent RL algorithms search distributed policies with holistic reward signals directly, making it difficult to obtain an optimal policy for each task. This article provides efficient learning-based algorithms such that each agent can learn a joint optimal policy to accomplish these multiple tasks cooperatively with other agents. The main idea of the algorithms is to decompose the holistic reward signal for each agent into multiple parts according to the subtasks, and then the proposed algorithms learn multiple value functions with the decomposed reward signals and update the policy with the sum of distributed value functions. In addition, this article presents a theoretical analysis of the proposed approach. Finally, the simulation results for both discrete decision-making and continuous control problems have demonstrated the effectiveness of the proposed algorithms.","2162-2388","","10.1109/TNNLS.2020.2996209","National Natural Science Foundation of China(grant numbers:61921004,U1713209,61803085,62041301); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9119863","Cooperative multiagent system (MAS);multitask;reinforcement learning (RL);task decomposition","Task analysis;Reinforcement learning;Sun;Training;Learning systems;Multi-agent systems;Navigation","decision making;learning (artificial intelligence);multi-agent systems","task decomposition;cooperative multiagent systems;single-agent RL system;scalar reward signals;multiagent RL algorithms search;efficient learning-based algorithms;joint optimal policy;multiple value functions;reinforcement learning;discrete decision-making;continuous control problems","","24","","40","IEEE","17 Jun 2020","","","IEEE","IEEE Journals"
"Trajectory Design and Access Control for Air–Ground Coordinated Communications System With Multiagent Deep Reinforcement Learning","R. Ding; Y. Xu; F. Gao; X. Shen","Institute for Artificial Intelligence, State Key Laboratory of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology, and Department of Automation, Tsinghua University, Beijing, China; Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; Institute for Artificial Intelligence, State Key Laboratory of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology, and Department of Automation, Tsinghua University, Beijing, China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Internet of Things Journal","6 Apr 2022","2022","9","8","5785","5798","Unmanned-aerial-vehicle (UAV)-assisted communications has attracted increasing attention recently. This article investigates air–ground coordinated communications system, in which trajectories of air UAV base stations (UAV-BSs) and access control of ground users (GUs) are jointly optimized. We formulated this optimization problem as a mixed cooperative–competitive game, where each GU competes for the limited resources of UAV-BSs to maximize its own throughput by accessing a suitable UAV-BS, and UAV-BSs cooperate with each other and design their trajectories to maximize the defined fair throughput to improve the total throughput and keep the GU fairness. Moreover, the action space of GUs is discrete, while that of UAV-BS is continuous. To tackle this hybrid action space issue, we transform the discrete actions into continuous action probabilities and propose a multiagent deep reinforcement learning (MADRL) approach, named air–ground probabilistic multiagent deep deterministic policy gradient (AG-PMADDPG). With well-designed rewards, AG-PMADDPG can coordinate two types of agents, UAV-BSs and GUs, to achieve their own objectives based on local observations. Simulation results demonstrate that AG-PMADDPG can outperform the benchmark algorithms in terms of throughput and fairness.","2327-4662","","10.1109/JIOT.2021.3062091","National Key Research and Development Program of China(grant numbers:2018AAA0102401); National Natural Science Foundation of China(grant numbers:61831013,61771274); Beijing Municipal Natural Science Foundation(grant numbers:L182042,4212002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363308","Air–ground coordinated communications;fair communication;multiagent deep reinforcement learning (MADRL);unmanned aerial vehicle (UAV) trajectory design;user access control","Trajectory;Throughput;Access control;Optimization;Communication systems;Training;Reinforcement learning","aerospace communication;authorisation;autonomous aerial vehicles;deep learning (artificial intelligence);game theory;multi-agent systems;optimisation;reinforcement learning;trajectory control","air UAV base stations;access control;ground users;UAV-BS;hybrid action space;multiagent deep reinforcement learning;trajectory design;air-ground coordinated communications system;unmanned-aerial-vehicle-assisted communications;air-ground probabilistic multiagent deep deterministic policy gradient;mixed cooperative-competitive game;discrete action space;continuous action probabilities","","22","","46","IEEE","25 Feb 2021","","","IEEE","IEEE Journals"
"Multi-Agent Reinforcement Learning for Decentralized Resilient Secondary Control of Energy Storage Systems Against DoS Attacks","P. Chen; S. Liu; B. Chen; L. Yu","Department of Electronics, Carleton University, Ottawa, ON, Canada; Department of Electronics, Carleton University, Ottawa, ON, Canada; Department of Automation and the Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou, China; Department of Automation and the Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou, China","IEEE Transactions on Smart Grid","21 Apr 2022","2022","13","3","1739","1750","While distributed secondary controllers have been studied for multiple energy storage systems in islanded microgrids, information infrastructure has to be added for the extensive information transmission among these secondary controllers and the additional communication among distributed controllers is costly and increases the vulnerability surface to cyberattacks. In this work, a data-driven decentralized secondary control scheme is proposed for multiple heterogeneous battery energy storage systems (BESSs). The proposed secondary control scheme can achieve frequency regulation and the state-of-charge (SoC) balancing simultaneously for BESSs without requiring accurate BESS models. This scheme leverages an asynchronous advantage actor-critic (A3C) based multi-agent deep reinforcement learning (MA-DRL) algorithm where the centralized off-line learning with shared convolutional neural networks (CNN) is designed to maximize global rewards and ensure the performance of the entire system and a decentralized online execution mechanism is applied to each BESS. Furthermore, in view of possible denial-of-service (DoS) attack on local communication networks used for signal transfer between secondary controllers and remote sensors, a signal-to-interference-plus-noise ratio (SINR)-based dynamic and proactive event-triggered communication mechanism is proposed to alleviate the impact of DoS attacks and reduce the occupation of communication resources. Simulation results on a four-bus multiple BESS system show that the proposed decentralized secondary controller can achieve simultaneous frequency regulation and SoC balancing. Comparison results with other event-triggered mechanisms and MA-DRL algorithms show the A3C based MA-DRL algorithm with CNN can obtain a comparatively optimal policy through training and the designed event-triggered strategy can dynamically adapt the release frequency based on real-time SINR and significantly reduce the occupied network bandwidth and packet loss rate (PER) induced by DoS attacks.","1949-3061","","10.1109/TSG.2022.3142087","NSERC Discovery Grant; National Natural Science Funds of China(grant numbers:62073292); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LR20F030004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9676705","Multi-agent deep reinforcement learning;microgrid;secondary frequency control;DoS attacks;battery energy storage systems","Frequency control;Microgrids;Signal to noise ratio;Power system dynamics;Interference;Denial-of-service attack;Transmission line measurements","battery storage plants;computer network security;convolutional neural nets;decentralised control;deep learning (artificial intelligence);distributed control;distributed power generation;energy storage;frequency control;multi-agent systems;optimisation;power distribution control;power distribution faults;power generation control;reinforcement learning","decentralized resilient secondary control;DoS attacks;distributed secondary controllers;information infrastructure;information transmission;multiple heterogeneous battery energy storage systems;asynchronous advantage actor-critic based multiagent deep reinforcement learning algorithm;shared convolutional neural networks;decentralized online execution mechanism;denial-of-service attack;local communication networks;four-bus multiple BESS system;simultaneous frequency regulation;A3C based MA-DRL algorithm;centralized off-line learning;signal-to-interference-plus-noise ratio-based dynamic mechanism;islanded microgrids;vulnerability surface;cyberattacks;data-driven decentralized secondary control scheme;state-of-charge balancing;SoC;signal transfer;remote sensors;proactive event-triggered communication mechanism;SINR;CNN;packet loss rate;PER","","21","","40","IEEE","11 Jan 2022","","","IEEE","IEEE Journals"
"GraphBit: Bitwise Interaction Mining via Deep Reinforcement Learning","Y. Duan; Z. Wang; J. Lu; X. Lin; J. Zhou","Beijing National Research Center for Information Science and Technology, China; Department of Automation, Tsinghua University, China; Beijing National Research Center for Information Science and Technology, China; Department of Automation, Tsinghua University, China; Beijing National Research Center for Information Science and Technology, China","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","16 Dec 2018","2018","","","8270","8279","In this paper, we propose a GraphBit method to learn deep binary descriptors in a directed acyclic graph unsupervisedly, representing bitwise interactions as edges between the nodes of bits. Conventional binary representation learning methods enforce each element to be binarized into zero or one. However, there are elements lying in the boundary which suffer from doubtful binarization as ""ambiguous bits"". Ambiguous bits fail to collect effective information for confident binarization, which are unreliable and sensitive to noise. We argue that there are implicit inner relationships between bits in binary descriptors, where the related bits can provide extra instruction as prior knowledge for ambiguity elimination. Specifically, we design a deep reinforcement learning model to learn the structure of the graph for bitwise interaction mining, reducing the uncertainty of binary codes by maximizing the mutual information with inputs and related bits, so that the ambiguous bits receive additional instruction from the graph for confident binarization. Due to the reliability of the proposed binary codes with bitwise interaction, we obtain an average improvement of 9.64%, 8.84% and 3.22% on the CIFAR-10, Brown and HPatches datasets respectively compared with the state-of-the-art unsupervised binary descriptors.","2575-7075","978-1-5386-6420-9","10.1109/CVPR.2018.00863","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578961","","Reliability;Binary codes;Computer vision;Linear programming;Training;Mutual information","binary codes;data mining;directed graphs;unsupervised learning","bitwise interaction mining;GraphBit method;deep binary descriptors;directed acyclic graph;conventional binary representation learning methods;doubtful binarization;ambiguous bits;confident binarization;ambiguity elimination;deep reinforcement learning model;binary codes;unsupervised binary descriptors","","20","","50","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Manifold Regularized Reinforcement Learning","H. Li; D. Liu; D. Wang","Tencent Inc., Shenzhen, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","16 Mar 2018","2018","29","4","932","943","This paper introduces a novel manifold regularized reinforcement learning scheme for continuous Markov decision processes. Smooth feature representations for value function approximation can be automatically learned using the unsupervised manifold regularization method. The learned features are data-driven, and can be adapted to the geometry of the state space. Furthermore, the scheme provides a direct basis representation extension for novel samples during policy learning and control. The performance of the proposed scheme is evaluated on two benchmark control tasks, i.e., the inverted pendulum and the energy storage problem. Simulation results illustrate the concepts of the proposed scheme and show that it can obtain excellent performance.","2162-2388","","10.1109/TNNLS.2017.2650943","National Natural Science Foundation of China(grant numbers:61233001,61273140,61304086,61533017,61503377,U1501251); Beijing Natural Science Foundation(grant numbers:4162065); Early Career Development Award of SKLMCCS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7835715","Adaptive dynamic programming;approximate dynamic programming;approximate policy iteration (API);manifold regularization;reinforcement learning (RL)","Manifolds;Approximation algorithms;Mathematical model;Algorithm design and analysis;Function approximation;Geometry;Aerospace electronics","energy storage;function approximation;learning (artificial intelligence);Markov processes;nonlinear control systems;sampling methods;state-space methods","reinforcement learning scheme;continuous Markov decision processes;smooth feature representations;value function approximation;unsupervised manifold regularization method;learned features;direct basis representation extension;policy learning;benchmark control tasks;manifold regularized reinforcement learning;inverted pendulum;energy storage","","19","","59","IEEE","27 Jan 2017","","","IEEE","IEEE Journals"
"Optimal adaptive control for unknown systems using output feedback by reinforcement learning methods","F. L. Lewis; K. G. Vamvoudakis","Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA; Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA","IEEE ICCA 2010","26 Jul 2010","2010","","","2138","2145","Optimal feedback controllers are generally computed offline assuming full knowledge of the system dynamics. Adaptive controllers, on the other hand, are online schemes that effectively learn to compensate for unknown system dynamics and disturbances. Generally, direct adaptive schemes do not converge to optimal control solutions for user-prescribed performance measures. During the past years, it has been shown that reinforcement learning techniques from computational intelligence can be used to learn optimal feedback controllers online using direct adaptive control techniques without knowing the system dynamics. Most reinforcement learning methods require full measurements of the system internal state. In this paper we develop reinforcement learning methods which require only output feedback and yet converge to an optimal controller. Deterministic linear time-invariant systems are considered. Both policy iteration (PI) and value iteration (VI) algorithms are derived. This corresponds to optimal control for a class of partially observable Markov decision processes (POMDPs). It is shown that, similar to Q-learning, the new output-feedback optimal learning methods have the important advantage that knowledge of the system dynamics is not needed for their implementation. Only the order of the system must be known and an upper bound on its `observability index'. The learned output feedback controller is in the form of a polynomial ARMA controller that has equivalent performance with the optimal state variable feedback gain.","1948-3457","978-1-4244-5195-1","10.1109/ICCA.2010.5524211","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5524211","Output feedback Approximate Dynamic Programming;Policy Iteration;Value Iteration","Adaptive control;Output feedback;Optimal control;Programmable control;Control systems;Computational intelligence;Learning systems;Upper bound;Polynomials;State feedback","adaptive control;control engineering computing;feedback;intelligent control;Internet;iterative methods;learning (artificial intelligence);linear systems;Markov processes;observability;optimal control;uncertain systems","optimal adaptive control technique;reinforcement learning method;optimal feedback controller;system dynamics;computational intelligence;linear time invariant system;policy iteration algorithm;value iteration algorithm;partially observable Markov decision process;Q learning;optimal learning method;observability index;polynomial ARMA controller;optimal state variable feedback gain;system internal state","","18","","41","IEEE","26 Jul 2010","","","IEEE","IEEE Conferences"
"Model-Free Reinforcement Learning for Fully Cooperative Consensus Problem of Nonlinear Multiagent Systems","H. Wang; M. Li","Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China","IEEE Transactions on Neural Networks and Learning Systems","4 Apr 2022","2022","33","4","1482","1491","This article presents an off-policy model-free algorithm based on reinforcement learning (RL) to optimize the fully cooperative (FC) consensus problem of nonlinear continuous-time multiagent systems (MASs). First, the optimal FC consensus problem is transformed into solving the coupled Hamilton–Jacobian–Bellman (HJB) equation. Then, we propose a policy iteration (PI)-based algorithm, which is further proved to be effective to solve the coupled HJB equation. To implement this scheme in a model-free way, a model-free Bellman equation is derived to find the optimal value function and the optimal control policy for each agent. Then, based on the least-squares approach, the tuning law for actor and critic weights is derived by employing actor and critic neural networks into the model-free Bellman equation to approximate the target policies and the value function. Finally, we propose an off-policy model-free integral RL (IRL) algorithm, which can be used to optimize the FC consensus problem of the whole system in real time by using measured data. The effectiveness of this proposed algorithm is verified by the simulation results.","2162-2388","","10.1109/TNNLS.2020.3042508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9298954","Fully cooperative~(FC) consensus;integral reinforcement learning (IRL);model-free reinforcement learning (RL);multiagent systems (MASs)","Mathematical model;Optimal control;Heuristic algorithms;Reinforcement learning;Decentralized control;Cost function;Artificial neural networks","continuous time systems;control system synthesis;dynamic programming;iterative methods;learning (artificial intelligence);Markov processes;multi-agent systems;neurocontrollers;nonlinear control systems;optimal control;optimisation;stochastic systems","reinforcement learning;fully cooperative consensus problem;continuous-time multiagent systems;optimal FC consensus problem;coupled Hamilton-Jacobian-Bellman equation;policy iteration-based algorithm;coupled HJB equation;model-free way;model-free Bellman equation;optimal value function;optimal control policy;target policies;off-policy model-free integral RL algorithm;model-free reinforcement;nonlinear multiagent systems;off-policy model-free algorithm","","16","","39","IEEE","18 Dec 2020","","","IEEE","IEEE Journals"
"Multiplayer Stackelberg–Nash Game for Nonlinear System via Value Iteration-Based Integral Reinforcement Learning","M. Li; J. Qin; N. M. Freris; D. W. C. Ho","Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Department of Mathematics, City University of Hong Kong, Hong Kong","IEEE Transactions on Neural Networks and Learning Systems","4 Apr 2022","2022","33","4","1429","1440","In this article, we study a multiplayer Stackelberg–Nash game (SNG) pertaining to a nonlinear dynamical system, including one leader and multiple followers. At the higher level, the leader makes its decision preferentially with consideration of the reaction functions of all followers, while, at the lower level, each of the followers reacts optimally to the leader’s strategy simultaneously by playing a Nash game. First, the optimal strategies for the leader and the followers are derived from down to the top, and these strategies are further shown to constitute the Stackelberg–Nash equilibrium points. Subsequently, to overcome the difficulty in calculating the equilibrium points analytically, we develop a novel two-level value iteration-based integral reinforcement learning (VI-IRL) algorithm that relies only upon partial information of system dynamics. We establish that the proposed method converges asymptotically to the equilibrium strategies under the weak coupling conditions. Moreover, we introduce effective termination criteria to guarantee the admissibility of the policy (strategy) profile obtained from a finite number of iterations of the proposed algorithm. In the implementation of our scheme, we employ neural networks (NNs) to approximate the value functions and invoke the least-squares methods to update the involved weights. Finally, the effectiveness of the developed algorithm is verified by two simulation examples.","2162-2388","","10.1109/TNNLS.2020.3042331","National Natural Science Foundation of China(grant numbers:61922076,61873252); Fok Ying-Tong Education Foundation for Young Teachers in Higher Education Institutions of China(grant numbers:161059); Anhui Department of Science and Technology(grant numbers:201903a05020049); Tencent Holdings Ltd(grant numbers:FR202003); Research Grants Council of the Hong Kong Special Administrative Region of China(grant numbers:CityU 11202819,CityU 11200717); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9302711","Integral reinforcement learning (IRL);multiplayer Stackelberg–Nash game (SNG);neural networks (NNs);nonlinear system;value iteration (VI)","Games;Heuristic algorithms;Mathematical model;Approximation algorithms;Decision making;System dynamics;Nonlinear dynamical systems","decision making;function approximation;game theory;iterative methods;least squares approximations;neural nets;reinforcement learning","multiplayer Stackelberg-Nash game;value iteration-based integral reinforcement learning;nonlinear dynamical system;Stackelberg-Nash equilibrium points;equilibrium strategies;value functions approximation;policy profile admissibility;VI-IRL;neural networks;least-squares methods;decision making;weak coupling conditions","","15","","37","IEEE","22 Dec 2020","","","IEEE","IEEE Journals"
"Task-Oriented Deep Reinforcement Learning for Robotic Skill Acquisition and Control","G. Xiang; J. Su","Department of Automation, Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Cybernetics","15 Jan 2021","2021","51","2","1056","1069","Reinforcement learning (RL) and imitation learning (IL), especially equipped with deep neural networks, have been widely studied for autonomous robotic skill acquisition and control tasks. However, these methods and their extensions require extensive environmental interactions during training, which greatly prevents them from being applied to real-world robots. To alleviate this problem, we present an efficient model-free off-policy actor-critic algorithm for robotic skill acquisition and continuous control, by fusing the task reward with a task-oriented guiding reward, which is formulated by leveraging few and imperfect expert demonstrations. In this framework, the agent can explore the environment more intentionally, thus sampling efficiency can be achieved; moreover, the agent can also exploit the experience more effectively, thereby substantially improved performance can be realized simultaneously. The empirical results on robotic locomotion tasks show that the proposed scheme can lower sample complexity by 2-10 times in contrast with the state-of-the-art baseline deep RL (DRL) algorithms, while achieving performance better than that of the expert. Furthermore, the proposed algorithm achieves significant improvement in both sampling efficiency and asymptotic performance on tasks with sparse and delayed reward, wherein those baseline DRL algorithms struggle to make progress. This takes a substantial step forward to implement these methods to acquire skills autonomously for real robots.","2168-2275","","10.1109/TCYB.2019.2949596","Key Projects of Natural Science Foundation of China(grant numbers:61533012,91748120,61521063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8897016","Continuous control;deep neural networks (DNNs);exploration;imitation learning (IL);reinforcement learning (RL);robotics;skill acquisition","Task analysis;Complexity theory;Robot sensing systems;Training;Entropy;Optimization","learning (artificial intelligence);neural nets;robots","extensive environmental interactions;real-world robots;efficient model-free off-policy actor-critic algorithm;continuous control;task reward;imperfect expert demonstrations;sampling efficiency;substantially improved performance;robotic locomotion tasks;state-of-the-art baseline deep RL algorithms;baseline DRL algorithms struggle;task-oriented deep reinforcement;imitation learning;deep neural networks;autonomous robotic skill acquisition;control tasks","","14","","52","IEEE","12 Nov 2019","","","IEEE","IEEE Journals"
"Fuzzy reinforcement learning control for compliance tasks of robotic manipulators","S. G. Tzafestas; G. G. Rigatos","Intelligent Robotics and Automation Laboratory, Department of Electrical and Computer Engineering, National and Technical University of Athens, Athens, Greece; Intelligent Robotics and Automation Laboratory, Department of Electrical and Computer Engineering, National and Technical University of Athens, Athens, Greece","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","7 Aug 2002","2002","32","1","107","113","A fuzzy reinforcement learning (FRL) scheme which is based on the principles of sliding-mode control and fuzzy logic is proposed. The FRL uses only immediate reward. Sufficient conditions for the convergence of the FRL to the optimal task performance are studied. The validity of the method is tested through simulation examples of a robot which deburrs a metal surface.","1941-0492","","10.1109/3477.979965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=979965","","Fuzzy control;Learning;Manipulators;Sliding mode control;Fuzzy logic;Convergence;Service robots;Deburring;Control systems;Iterative algorithms","manipulators;learning (artificial intelligence);intelligent control;fuzzy control;fuzzy logic;compliance control;variable structure systems","fuzzy reinforcement learning control;compliance tasks;robotic manipulators;FRL scheme;sliding-mode control;fuzzy logic;immediate reward;sufficient conditions;optimal task performance;metal surface deburring;fuzzy reinforcement learning;hybrid hierarchical control;impedance control","","14","","14","IEEE","7 Aug 2002","","","IEEE","IEEE Journals"
"A Collaborative Multiagent Reinforcement Learning Method Based on Policy Gradient Potential","Z. Zhang; Y. -S. Ong; D. Wang; B. Xue","School of Automation, Qingdao University, Qingdao, China; School of Science and Computer Engineering, Nanyang Technological University, Singapore; School of Electrical Engineering, Qingdao University, Qingdao, China; School of Automation, Qingdao University, Qingdao, China","IEEE Transactions on Cybernetics","15 Jan 2021","2021","51","2","1015","1027","Gradient-based method has been extensively used in today's multiagent reinforcement learning (MARL). In a gradient-based MARL algorithm, each agent updates its parameterized strategy in the direction of the gradient of some performance index. However, studies on the convergence of the existing gradient-based MARL algorithms for identical interest games are quite few. In this article, we propose a policy gradient potential (PGP) algorithm that takes PGP as the source of information for guiding the strategy update, as opposed to the gradient itself, to learn the optimal joint strategy that has a maximal global reward. Since the payoff matrix and the joint strategy are often unavailable to the learning agents in reality, we consider the probability of obtaining the maximal reward as the performance index. Theoretical analysis of the PGP algorithm on the continuous model involving an identical interest repeated game shows that if the component action of every optimal joint action is unique, the critical points corresponding to all optimal joint actions are asymptotically stable. The PGP algorithm is experimentally studied and compared against other MARL algorithms on two commonly used collaborative tasks-the robots leaving a room task and the distributed sensor network task, as well as a real-world minefield navigation problem where only local state and local reward information are available. The results show that the PGP algorithm outperforms the other algorithms in terms of the cumulative reward and the number of time steps used in an episode.","2168-2275","","10.1109/TCYB.2019.2932203","Qingdao Post-Doctoral Applied Research Project with Name (AGV Road Network Design and Path Planning Method Based on Multi-Agent Reinforcement Learning); International Cooperation Training Project for Outstanding Young and Middle-Aged Teachers of Universities in Shandong Province in 2017; Shandong Provincial Natural Science Foundation of China(grant numbers:ZR2017PF005); National Natural Science Foundation of China(grant numbers:61873138,61573205,61603205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809379","Gradient method;multiagent reinforcement learning (MARL);multiagent system;reinforcement learning","Games;Task analysis;Reinforcement learning;Performance analysis;Stochastic processes;Convergence;Collaboration","game theory;learning (artificial intelligence);multi-agent systems","collaborative multiagent reinforcement learning method;gradient-based method;gradient-based MARL algorithm;parameterized strategy;performance index;existing gradient-based MARL algorithms;identical interest games;policy gradient potential algorithm;strategy update;optimal joint strategy;maximal global reward;learning agents;maximal reward;PGP algorithm;identical interest repeated game;optimal joint action;commonly used collaborative tasks-the;local state;local reward information","","13","","36","IEEE","21 Aug 2019","","","IEEE","IEEE Journals"
"A New Multi-Agent Reinforcement Learning Method Based on Evolving Dynamic Correlation Matrix","X. Gan; H. Guo; Z. Li","The 54th Research Institute of China Electronics Technology Group Corporation, Shijiazhuang, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Access","13 Nov 2019","2019","7","","162127","162138","Multi-agent reinforcement learning approaches can be roughly classified into two categories. One is the agent-based approach which can be implemented in real distributed systems, though most approaches of this type cannot provide meaningful theoretical verifications. The other can be seen as the more formalized approach, which can provide theoretical results. However, most of current algorithms usually require unrealistic global communication, which makes them impractical for real distributed systems. In this article, we propose a dynamic correlation matrix based multi-agent reinforcement learning approach where the meta-parameters are evolved using an evolutionary algorithm. We believe that our approach is able to fill the gap between the two kinds of traditional multi-agent reinforcement learning approaches by providing both agent-level implementation and system-level convergence verification. The basic idea of this approach is that agents learn not only from local environmental feedback, i.e., their own experiences and rewards, but also from other agents' experiences. In this way, the agents' learning speed can be increased significantly. The performance of the proposed algorithm is demonstrated on a number of application scenarios, including blackjack games, urban traffic control systems and multi-robot foraging.","2169-3536","","10.1109/ACCESS.2019.2946848","National Key Research and Development Plan of China Project: Indoor Hybrid Intelligent Positioning and Indoor GIS Technology(grant numbers:2016YFB0502100,2016YFB0502101); Open Project of State Key Laboratory of Satellite Navigation System and Equipment Technology; National Natural Science Foundation of China(grant numbers:61603078); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8864051","Multi-agent reinforcement learning;dynamic correlation matrix;convergence;meta-parameter evolution","Reinforcement learning;Heuristic algorithms;Correlation;Evolutionary computation;Convergence;Tuning;Roads","evolutionary computation;learning (artificial intelligence);matrix algebra;multi-agent systems;multi-robot systems","new multiagent reinforcement learning method;evolving dynamic correlation matrix;multiagent reinforcement learning approaches;agent-based approach;distributed systems;agent-level implementation;system-level convergence verification","","10","","60","CCBY","11 Oct 2019","","","IEEE","IEEE Journals"
"Data-Driven Guaranteed Cost Control Design via Reinforcement Learning for Linear Systems With Parameter Uncertainties","H. -N. Wu; Z. -y. Liu","Science and Technology on Aircraft Control Laboratory, School of Automation Science and Electrical Engineering, Beihang University (Beijing University of Aeronautics and Astronautics), Beijing, China; Science and Technology on Aircraft Control Laboratory, School of Automation Science and Electrical Engineering, Beihang University (Beijing University of Aeronautics and Astronautics), Beijing, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","14 Oct 2020","2020","50","11","4151","4159","Controllers learned from data are more practical and promising than the existing model-based ones and their capability can be enhanced if a priori information about the controlled plant is available. Under this viewpoint, a data-driven guaranteed cost control (GCC) design is investigated for linear systems with time-varying parameter uncertainties. Initially, the GCC design is shown to be equivalent to an H∞ state feedback control problem subject to a specific disturbance attenuation performance requirement. Then such an H∞ control problem is regarded as a zero-sum game and reduces to seek the stabilizing solution of a parameterized algebraic Riccati equation (ARE). Furthermore, to solve the ARE approximately, a modified simultaneous policy update algorithm (SPUA) and the corresponding data-driven variant based on off-policy reinforcement learning (RL) and experience replay technique is proposed. Finally, a numerical simulation for aircrafts with harsh uncertainties is illustrated to validate the merits of the proposed methods.","2168-2232","","10.1109/TSMC.2019.2931332","National Natural Science Foundation for Distinguished Young Scholars of China(grant numbers:61625302); National Natural Science Foundation of China(grant numbers:61721091); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8795580","H∞ control;guaranteed cost control (GCC);parameterized algebraic Riccati equation (ARE);reinforcement learning (RL);uncertain system","Uncertainty;Uncertain systems;Linear systems;Time-varying systems;Nonlinear systems;Game theory;Games","control system synthesis;cost optimal control;game theory;learning (artificial intelligence);learning systems;linear systems;Riccati equations;robust control;state feedback;time-varying systems;uncertain systems","stabilization;simultaneous policy update algorithm;parameterized algebraic Riccati equation;zero-sum game;H∞ state feedback control problem;disturbance attenuation performance requirement;harsh uncertainties;experience replay technique;state feedback control problem;GCC design;time-varying parameter uncertainties;data-driven guaranteed cost control design;linear systems;off-policy reinforcement learning","","10","","46","IEEE","13 Aug 2019","","","IEEE","IEEE Journals"
"Identity-Preserving Face Hallucination via Deep Reinforcement Learning","X. Cheng; J. Lu; B. Yuan; J. Zhou","Graduate School at Shenzhen, Tsinghua University, Shenzhen, China; Department of Automation, State Key Laboratory of Intelligent Technologies and Systems, Beijing Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Graduate School at Shenzhen, Tsinghua University, Shenzhen, China; Department of Automation, State Key Laboratory of Intelligent Technologies and Systems, Beijing Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","3 Dec 2020","2020","30","12","4796","4809","In this paper, we propose an identity-preserving face hallucination (IPFH) method via deep reinforcement learning. Most existing methods ultra-resolve facial visual information in guidance of appearance similarity which rarely attend to recovering the semantic property, undermining further face analysis (e.g., recognition). We present a visual-semantic hallucinator relying on deep reinforcement learning to adaptively repair local details for the restoration of both identity and appearance characteristics. Specifically, we first capture the facial global topology structure to roughly recover the visual information with the pixel-wise similarity constraint. To super-resolve more photo-realistic faces, we explore the contextual interdependency to reconstruct facial local textural details (e.g., over-smoothed edges) with the constraints of visual and identity similarity. In terms of the visual similarity constraint, we develop the dual domain network with bidirectional consistency on both HR domain and LR domain to improve the appearance quality. Moreover, we introduce the identity constraint to encourage hallucinated faces to satisfy the identity property. Experimental results on several benchmarks demonstrate our method achieves promising performance on the recovery of visual and semantic information.","1558-2205","","10.1109/TCSVT.2019.2961629","National Key Research and Development Program of China(grant numbers:2016YFB1001001); National Natural Science Foundation of China(grant numbers:61822603,U1813218,U1713214,61672306); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8939465","Face hallucination;deep reinforcement learning;visual-semantic hallucinator;dual consistency;identity property","Face recognition;Visualization;Reinforcement learning;Image restoration;Semantics","face recognition;image reconstruction;image texture;learning (artificial intelligence)","visual identity similarity;visual similarity constraint;identity constraint;identity property;semantic information;deep reinforcement learning;identity-preserving face hallucination method;appearance similarity;semantic property;visual-semantic hallucinator;appearance characteristics;facial global topology structure;pixel-wise similarity constraint;photo-realistic faces;facial local textural details;ultra-resolve facial visual information;ultra-resolve facial visual information;dual domain network","","10","","71","IEEE","23 Dec 2019","","","IEEE","IEEE Journals"
"User-Guided Personalized Image Aesthetic Assessment Based on Deep Reinforcement Learning","P. Lv; J. Fan; X. Nie; W. Dong; X. Jiang; B. Zhou; M. Xu; C. Xu","School of Computers and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; School of Computers and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; School of Computers and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Computers and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; School of Computers and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; School of Computers and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Multimedia","9 Mar 2023","2023","25","","736","749","Personalized image aesthetic assessment (PIAA) has recently become a hot topic due to its wide applications, such as photography, film, television, e-commerce, fashion design, and so on. This task is more seriously affected by subjective factors and samples provided by users. In order to acquire precise personalized aesthetic distribution by small amount of samples, we propose a novel user-guided personalized image aesthetic assessment framework. This framework leverages user interactions to retouch and rank images for aesthetic assessment based on deep reinforcement learning (DRL), and generates personalized aesthetic distribution that is more in line with the aesthetic preferences of different users. It mainly consists of two stages. In the first stage, personalized aesthetic ranking is generated by interactive image enhancement and manual ranking, meanwhile, two policy networks will be trained. These two networks will be trained iteratively and alternatively to facilitate the final personalized aesthetic assessment. In the second stage, these modified images are labeled with aesthetic attributes by one style-specific classifier, and then the personalized aesthetic distribution is generated based on the multiple aesthetic attributes of these images, which conforms to the aesthetic preference of users better. Compared with other existing methods, our approach has achieved new state-of-the-art in the task of personalized image aesthetic assessment on the public AVA and FLICKR-AES datasets.","1941-0077","","10.1109/TMM.2021.3130752","National Natural Science Foundation of China(grant numbers:61772474,61872324,62072136,62172371); Program for Science and Technology Innovation Talents; Universities of Henan Province(grant numbers:20HASTIT021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9627535","Deep reinforcement learning;image aesthetic assessment;personalized aesthetic distribution;personalized image enhancement;user interaction","Image enhancement;Task analysis;Reinforcement learning;Feature extraction;Visualization;Training;Neural networks","deep learning (artificial intelligence);electronic commerce;image enhancement;learning (artificial intelligence);photography;reinforcement learning","aesthetic preference;deep reinforcement learning;final personalized aesthetic assessment;interactive image enhancement;manual ranking;modified images;multiple aesthetic attributes;novel user-guided personalized image aesthetic assessment framework;personalized aesthetic ranking;precise personalized aesthetic distribution;user interactions","","9","","80","IEEE","25 Nov 2021","","","IEEE","IEEE Journals"
"Reinforcement Learning-Aided Performance-Driven Fault-Tolerant Control of Feedback Control Systems","C. Hua; L. Li; S. X. Ding","Institute for Automatic Control, and Complex Systems (AKS), University of Duisburg-Essen, Verl, Germany; Key Laboratory of Knowledge Automation for Industrial Processes of Ministry of Education, School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, P. R. China; Institute for Automatic Control, and Complex Systems (AKS), University of Duisburg-Essen, Verl, Germany","IEEE Transactions on Automatic Control","27 May 2022","2022","67","6","3013","3020","This article is concerned with a fault-tolerant control (FTC) scheme for feedback control systems with multiplicative faults by optimizing system performance with the aid of a reinforcement learning (RL) approach. To be specific, initially, based on the Youla–Kučera (YK) and dual YK parameterizations, a new performance-driven FTC method is proposed and its capability in dealing with multiplicative faults is proven. Then, data-driven implementation of this method using RL is elaborated. This implementation shows that RL can be applied efficiently by utilizing both plant model and data to recover the fault-induced system performance degradation. Finally, a benchmark study on an inverted pendulum system demonstrates the application of the proposed performance-driven FTC method.","1558-2523","","10.1109/TAC.2021.3088397","National Natural Science Foundation of China(grant numbers:62073029,62073029); Beijing Natural Science Foundation(grant numbers:4202045); Fundamental Research Funds for the Central Universities(grant numbers:FRF-TP-20-012A3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9451628","Data-driven;fault-tolerant control (FTC);performance degradation recovery;reinforcement learning (RL)","Degradation;System performance;Fault tolerant systems;Fault tolerance;Trajectory;Stochastic processes;Estimation","control system synthesis;fault diagnosis;fault tolerance;feedback;learning (artificial intelligence);nonlinear control systems;pendulums","reinforcement learning-aided performance-driven fault-tolerant control;feedback control systems;fault-tolerant control scheme;multiplicative faults;optimizing system performance;reinforcement learning approach;RL;performance-driven FTC method;data-driven implementation;fault-induced system performance degradation;inverted pendulum system","","9","","24","IEEE","10 Jun 2021","","","IEEE","IEEE Journals"
"A Cooperative Multi-Agent Reinforcement Learning Method Based on Coordination Degree","H. Cui; Z. Zhang","Shandong Key Laboratory of Industrial Control Technology, School of Automation, Qingdao University, Qingdao, China; Shandong Key Laboratory of Industrial Control Technology, School of Automation, Qingdao University, Qingdao, China","IEEE Access","14 Sep 2021","2021","9","","123805","123814","Multi-agent reinforcement learning (MARL) has become a prevalent method for solving cooperative problems owing to its tractable implementation and task distribution. The goal of the MARL algorithms for fully cooperative scenarios is to obtain the optimal joint strategy that maximizes the expected common cumulative reward for all agents. However, to date, the analysis of MARL dynamics has focused on repeated games with few agents and actions. To this end, we propose a cooperative MARL algorithm based on the coordination degree (CMARL-CD) and analyze its dynamics in more general cases in which repeated games with more agents and actions are considered. Theoretical analysis shows that if the component action of every optimal joint action is unique, all optimal joint actions are asymptotically stable critical points. The CMARL-CD algorithm realizes coordination among agents without the need to estimate the global Q-value function. Each agent estimates the coordination degree of its own action, which represents the potential of being the optimal action. The efficacy of the CMARL-CD algorithm is studied through repeated games and stochastic games.","2169-3536","","10.1109/ACCESS.2021.3110255","National Natural Science Foundation of China(grant numbers:61903209); Qingdao Postdoctoral Applied Research Project with Name (AGV Road Network Design and Path Planning Method Based on Multi-Agent Reinforcement Learning); Science and Technology Support Plan for Youth Innovation of Universities in Shandong Province(grant numbers:2019KJN033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529182","Multi-agent reinforcement learning;multi-agent system;reinforcement learning;independent learner","Games;Task analysis;Reinforcement learning;Heuristic algorithms;Approximation algorithms;Training","learning (artificial intelligence);multi-agent systems;stochastic games","tractable implementation;MARL algorithm;fully cooperative scenarios;optimal joint strategy;MARL dynamics;repeated games;coordination degree;component action;optimal joint action;CMARL-CD algorithm;optimal action;prevalent method;cooperative multiagent reinforcement learning method;common cumulative reward;stochastic games","","8","","44","CCBY","3 Sep 2021","","","IEEE","IEEE Journals"
"A Reinforcement Learning Neural Network for Robotic Manipulator Control","Y. Hu; B. Si","State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang 110016, P.R.C., and University of Chinese Academy of Sciences, Beijing 100049, P.R.C.; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang 110016, Shenyang, P.R.C.","Neural Computation","26 Sep 2019","2018","30","7","1983","2004","We propose a neural network model for reinforcement learning to control a robotic manipulator with unknown parameters and dead zones. The model is composed of three networks. The state of the robotic manipulator is predicted by the state network of the model, the action policy is learned by the action network, and the performance index of the action policy is estimated by a critic network. The three networks work together to optimize the performance index based on the reinforcement learning control scheme. The convergence of the learning methods is analyzed. Application of the proposed model on a simulated two-link robotic manipulator demonstrates the effectiveness and the stability of the model.","0899-7667","","10.1162/neco_a_01079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8850567","","","","","","8","","","Traditional","26 Sep 2019","","","MIT Press","MIT Press Journals"
"A Multi-Target Trajectory Planning of a 6-DoF Free-Floating Space Robot via Reinforcement Learning","S. Wang; X. Zheng; Y. Cao; T. Zhang","Department of Automation, Institute of Navigation and Control, Tsinghua University, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China; Beijing Institute of Control Engineering, Beijing, China; Department of Automation, Institute of Navigation and Control, Tsinghua University, Beijing, China","2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","16 Dec 2021","2021","","","3724","3730","Space robots have played an essential role in space junk removal. Compared with traditional model-based methods, model-free reinforcement learning methods are promising in tackling space capture missions, which is challenging due to the dynamic singular problem and measuring errors of dynamics parameters. Nevertheless, current research mostly focus on the single-target environment. In this paper, we propose a multi-target trajectory planning strategy of a 6-DoF free-floating space robot optimized by the Proximal Policy Optimization (PPO) algorithm. Furthermore, we adopt some augmentation techniques to improve the PPO algorithm on precision and stability of reaching multiple targets. In particular, we introduce an Action Ensembles Based on Poisson Distribution (AEP) method, which facilitates the policy to efficiently approximate the optimal policy. Our method can be easily extended to realize the task that the end-effector tracks a specific trajectory. We evaluate our approach on four tasks: circle trajectory tracking, external disturbances at joints, different masses of the base, and even single joint failure, without any further fine-tuning. The results suggest that the planning strategy has comparably high adaptability and anti-inference capacity. Qualitative results (videos) are available at [36].","2153-0866","978-1-6654-1714-3","10.1109/IROS51168.2021.9636681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636681","","Trajectory planning;Heuristic algorithms;Reinforcement learning;Approximation algorithms;Stability analysis;Robustness;End effectors","aerospace robotics;end effectors;manipulator dynamics;manipulator kinematics;motion control;optimisation;Poisson distribution;reinforcement learning;robot kinematics;robot programming;target tracking;trajectory control","circle trajectory tracking;free-floating space robot;space robots;space junk removal;model-free reinforcement learning methods;space capture missions;dynamic singular problem;single-target environment;multitarget trajectory planning strategy;proximal policy optimization algorithm;action ensembles based on Poisson distribution method","","7","","28","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Salience-Aware Face Presentation Attack Detection via Deep Reinforcement Learning","B. Yu; J. Lu; X. Li; J. Zhou","Tsinghua Shenzhen International Graduate School, Shenzhen, China; Department of Automation, Beijing Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Tsinghua Shenzhen International Graduate School, Shenzhen, China; Department of Automation, Beijing Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China","IEEE Transactions on Information Forensics and Security","25 Jan 2022","2022","17","","413","427","In this paper, we propose a salience-aware face presentation attack detection (SAFPAD) approach, which takes advantage of deep reinforcement learning to exploit the salient local part information in face images. Most existing deep face presentation attack detection approaches extract features from the entire image or several fixed regions. However, the discriminative information beneficial for presentation attack detection is unevenly distributed in the image due to the illumination and presentation attack instrument variation, so treating all regions equally fails to highlight the most discriminative information which is important for more accurate and robust face presentation attack detection. To address this, we propose to identify the discriminative salient parts using deep reinforcement learning and focus on them to alleviate the adverse effects of redundant information in the face images. We fuse the high-level features and the local features which guide the policy network to exploit discriminative patches and assist the classification network to predict more accurate results. We jointly train the SAFPAD model with deep reinforcement learning to generate salient locations. Extensive experiments on five public datasets demonstrate that our approach achieves very competitive performance due to the concentrated employment of salient local information.","1556-6021","","10.1109/TIFS.2021.3135748","National Key Research and Development Program of China(grant numbers:2020AAA0108302); Shenzhen Science and Technology Project(grant numbers:JCYJ20200109143041798); Beijing Academy of Artificial Intelligence (BAAI); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650907","Face presentation attack detection;deep reinforcement learning;multiscale feature fusion","Face recognition;Feature extraction;Reinforcement learning;Faces;Videos;Deep learning;Thermal sensors","deep learning (artificial intelligence);face recognition;feature extraction;image representation;reinforcement learning","deep reinforcement learning;salient local part information;face images;discriminative information;presentation attack instrument variation;face presentation attack detection;discriminative salient parts;salient local information;salience-aware face presentation attack detection approach;deep face presentation;SAFPAD model","","7","","80","IEEE","14 Dec 2021","","","IEEE","IEEE Journals"
"Three-Dimensional Path Following Control of an Underactuated Robotic Dolphin Using Deep Reinforcement Learning","J. Liu; Z. Liu; Z. Wu; J. Yu","Department of Mechanics and Engineering Science, State Key Laboratory for Turbulence and Complex Systems, BIC-ESAT, College of Engineering, Peking University, Beijing, China; Shandong Labor Vocational and Technical College, Jinan, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Mechanics and Engineering Science, State Key Laboratory for Turbulence and Complex Systems, BIC-ESAT, College of Engineering, Peking University, Beijing, China","2020 IEEE International Conference on Real-time Computing and Robotics (RCAR)","30 Dec 2020","2020","","","315","320","In this paper, a novel improved deep reinforcement learning (DRL) strategy is utilized to solve the three-dimensional path-following control problem for an underactuated robotic dolphin in order to meet the complex and intractable hydrodynamic model. Firstly, a brief overview of the developed robotic dolphin consisting of two-degree-of-freedom paired pectoral fins and the slider-crank-based flapping two-joint tail is introduced. After that, path-following controller includes a novel lookahead-based guidance law, decoupling motion strategy, and a deep deterministic policy gradient algorithm based on the the prior knowledge is proposed. The novel lookahead based 3-D LOS guidance law is employed to transform the 3-D waypoints to the desired heading and pitch angles with its simplicity, intuitiveness, and small computational footprint. Finally, the effectiveness of the proposed controller is demonstrated after numerous simulations. It will offer a rich vein of insight for the real-time task execution in bioinspired underwater robots.","","978-1-7281-7293-4","10.1109/RCAR49640.2020.9303309","National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303309","","Robots;Dolphins;Sports;Oscillators;Biological system modeling;Turning;Aerospace electronics","gradient methods;learning (artificial intelligence);marine control;marine propulsion;mobile robots;motion control;path planning;underwater vehicles","underactuated robotic dolphin;novel improved deep reinforcement learning strategy;three-dimensional path;complex model;intractable hydrodynamic model;developed robotic dolphin;two-degree-of-freedom paired pectoral fins;slider-crank-based;novel lookahead-based guidance law;motion strategy;deep deterministic policy gradient algorithm;3-D LOS guidance law;bioinspired underwater robots","","7","","20","IEEE","30 Dec 2020","","","IEEE","IEEE Conferences"
"Model-Based Reinforcement Learning Control of Electrohydraulic Position Servo Systems","Z. Yao; X. Liang; G. -P. Jiang; J. Yao","College of Automation & College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu Province, China; School of Mechanical Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu Province, China; College of Automation & College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu Province, China; School of Mechanical Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu Province, China","IEEE/ASME Transactions on Mechatronics","15 Jun 2023","2023","28","3","1446","1455","Even though the unprecedented success of AlphaGo Zero demonstrated reinforcement learning as a feasible complex problem solver, the research on reinforcement learning control of hydraulic systems is still void. We are motivated by the challenges presented in hydraulic systems to develop a new model-based reinforcement learning controller that achieves high-accuracy tracking at performance level and with asymptotic stability guarantees at system level. In this article, the proposed design consists of two frameworks: A recursive robust integral of the sign of the error (RISE) control approach to providing closed-loop system stability framework, and a reinforcement learning approach with actor-critic structure to dealing with the unknown dynamics or more specifically, the actor neural network is used to reduce the high feedback gain of the recursive RISE control approach by compensating the unknown dynamic while the critic neural network is integrated to improve the control performance by evaluating the filtered tracking error. A theoretical guarantee for the stability of the overall dynamic system is provided by using Lyapunov stability theory. Simulation and experimental results are provided to demonstrate improved control performance of the proposed controller.","1941-014X","","10.1109/TMECH.2022.3219115","National Key R&D Program of China(grant numbers:2021YFB2011300); National Natural Science Foundation of China(grant numbers:61873326,52075262); Key University Science Research Project of Jiangsu Province(grant numbers:22KJB460005); Natural Science Research Startup Foundation of Recruiting Talents of Nanjing University of Posts and Telecommunications(grant numbers:NY221127); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955370","Hydraulic systems;neural networks;reinforcement learning;robust control","Reinforcement learning;Vehicle dynamics;Feedback control;Asymptotic stability;Friction;Uncertainty;System dynamics","asymptotic stability;closed loop systems;control engineering computing;control system synthesis;electrohydraulic control equipment;feedback;Lyapunov methods;neurocontrollers;position control;reinforcement learning;robust control;servomechanisms","actor neural network;AlphaGo Zero;closed-loop system stability framework;dynamic system;electrohydraulic position servo systems;error control approach;hydraulic systems;improved control performance;Lyapunov stability theory;model-based reinforcement learning control;model-based reinforcement learning controller;recursive RISE control approach;recursive robust integral-of-the sign-of-the error control approach;reinforcement learning approach;RISE","","6","","43","IEEE","18 Nov 2022","","","IEEE","IEEE Journals"
"Clique-based cooperative multiagent reinforcement learning using factor graphs","Z. Zhang; D. Zhao","Institute of Automation Chinese Academy of Sciences, Beijing, Beijing, CN; Institute of Automation Chinese Academy of Sciences, Beijing, Beijing, CN","IEEE/CAA Journal of Automatica Sinica","15 Jan 2015","2014","1","3","248","256","In this paper, we propose a clique-based sparse reinforcement learning (RL) algorithm for solving cooperative tasks. The aim is to accelerate the learning speed of the original sparse RL algorithm and to make it applicable for tasks decomposed in a more general manner. First, a transition function is estimated and used to update the Q-value function, which greatly reduces the learning time. Second, it is more reasonable to divide agents into cliques, each of which is only responsible for a specific subtask. In this way, the global Q-value function is decomposed into the sum of several simpler local Q-value functions. Such decomposition is expressed by a factor graph and exploited by the general maxplus algorithm to obtain the greedy joint action. Experimental results show that the proposed approach outperforms others with better performance.","2329-9274","","10.1109/JAS.2014.7004682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004682","Multiagent reinforcement learning;factor graph;maxplus algorithm;clique-based decomposition","Sensors;Games;Learning (artificial intelligence);Approximation algorithms;Algorithm design and analysis;Heuristic algorithms;Sparse matrices","graph theory;learning (artificial intelligence);multi-agent systems","clique-based cooperative multiagent reinforcement learning;factor graph;clique-based sparse reinforcement learning algorithm;original sparse RL algorithm;cooperative tasks;transition function;learning time reduction;global Q-value function;local Q-value functions;general maxplus algorithm;greedy joint action","","6","","","","15 Jan 2015","","","IEEE","IEEE Journals"
"Feature Search in the Grassmanian in Online Reinforcement Learning","S. Bhatnagar; V. S. Borkar; P. K. J.","Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Electrical Engineering, Indian Institute of Technology, Powai, Mumbai, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India","IEEE Journal of Selected Topics in Signal Processing","11 Sep 2013","2013","7","5","746","758","We consider the problem of finding the best features for value function approximation in reinforcement learning and develop an online algorithm to optimize the mean square Bellman error objective. For any given feature value, our algorithm performs gradient search in the parameter space via a residual gradient scheme and, on a slower timescale, also performs gradient search in the Grassman manifold of features. We present a proof of convergence of our algorithm. We show empirical results using our algorithm as well as a similar algorithm that uses temporal difference learning in place of the residual gradient scheme for the faster timescale updates.","1941-0484","","10.1109/JSTSP.2013.2255022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6488714","Feature adaptation;Grassman manifold;online learning;residual gradient scheme;stochastic approximation;temporal difference learning","Approximation algorithms;Vectors;Signal processing algorithms;Convergence;Learning (artificial intelligence);Function approximation","approximation theory;gradient methods;learning (artificial intelligence);search problems","feature search;online reinforcement learning;value function approximation;online algorithm;mean square Bellman error objective;gradient search;parameter space;residual gradient scheme;Grassman manifold;temporal difference learning","","6","","35","IEEE","27 Mar 2013","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Effective Coverage Control With Connectivity Constraints","S. Meng; Z. Kan","Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China","IEEE Control Systems Letters","24 Jun 2021","2022","6","","283","288","This work studies dynamic coverage control of a multi-agent system using deep reinforcement learning. Dynamic coverage control is a type of cooperative control which requires a multi-agent system to dynamically monitor an area of interest over time. To develop motion control laws, most of previous works highly rely on the knowledge of system models, such as the environment model and agent kinematics/dynamics. However, acquiring an accurate model can be restrictive and even impossible in many practical applications. Another challenge is that agent often has a limited communication capability in practice. Two agents may only exchange information when they are within a certain distance. To address these challenges, a multi-agent deep reinforcement learning (MADRL) based control framework is developed to enable agents to learn control policies directly from interactions with the environment to achieve dynamic coverage control while preserving network connectivity. The developed MADRL is model free and employs decentralized execution and centralized training, in which agents coordinate using only local information and do not need to know other agents' strategies at execution phase. Numerical simulations demonstrate the effectiveness of the developed control strategy.","2475-1456","","10.1109/LCSYS.2021.3070850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9395182","Coverage control;deep reinforcement learning;multi-agent systems","Sensors;Monitoring;Dynamics;Task analysis;Reinforcement learning;Vehicle dynamics;Training","control system analysis computing;control system synthesis;deep learning (artificial intelligence);motion control;multi-agent systems","effective coverage control;dynamic coverage control;multiagent system;motion control laws;system models;environment model;multiagent deep reinforcement learning based control framework;control policies;developed control strategy;MADRL;agent kinematics-dynamics","","6","","30","IEEE","5 Apr 2021","","","IEEE","IEEE Journals"
"WRFMR: A Multi-Agent Reinforcement Learning Method for Cooperative Tasks","H. Liu; Z. Zhang; D. Wang","School of Automation, Qingdao University, Qingdao, China; School of Automation, Qingdao University, Qingdao, China; School of Electrical Engineering, Qingdao University, Qingdao, China","IEEE Access","8 Dec 2020","2020","8","","216320","216331","Multi-agent reinforcement learning (MARL) for cooperative tasks has been extensively studied in recent years. The balance of exploration and exploitation is crucial to MARL algorithms' performance in terms of the learning speed and the quality of the obtained strategy. To this end, we propose an algorithm known as the weighted relative frequency of obtaining the maximal reward (WRFMR), which uses a weight parameter and the action probability to balance exploration and exploitation and accelerate convergence to the optimal joint action. For the WRFMR algorithm, each agent needs to share the state and the immediate reward and does not need to observe the actions of the other agents. Theoretical analysis on the model of WRFMR in cooperative repeated games shows that each optimal joint action is an asymptotically stable critical point if the component action of every optimal joint action is unique. The box-pushing task, the distributed sensor network (DSN) task, and a strategy game known as blood battlefield are used for empirical studies. Both the DSN task and the box-pushing task involve full cooperation, while blood battle comprises both cooperation and competition. The simulation results show that the WRFMR algorithm outperforms the other algorithms regarding the success rate and the learning speed.","2169-3536","","10.1109/ACCESS.2020.3040985","National Natural Science Foundation of China(grant numbers:61903209,61873138,61573205); Qingdao Postdoctoral Applied Research Project with Name (AGV Road Network Design and Path Planning Method Based on Multi-Agent Reinforcement Learning); Shandong Provincial Natural Science Foundation of China(grant numbers:ZR2017PF005); Science and Technology Support Plan for Youth Innovation of Universities in Shandong Province(grant numbers:2019KJN033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272805","Multi-agent reinforcement learning;reinforcement learning;multi-agent system;repeated game","Games;Task analysis;Reinforcement learning;Convergence;Approximation algorithms;Blood;Prediction algorithms","cooperative systems;distributed sensors;game theory;learning (artificial intelligence);multi-agent systems","strategy game;DSN task;box-pushing task;WRFMR algorithm;learning speed;multiagent reinforcement learning method;MARL algorithms;weighted relative frequency;maximal reward;weight parameter;action probability;balance exploration;optimal joint action;immediate reward;component action;distributed sensor network task","","6","","38","CCBY","27 Nov 2020","","","IEEE","IEEE Journals"
"Composite Observer-Based Optimal Attitude-Tracking Control With Reinforcement Learning for Hypersonic Vehicles","S. Zhao; J. Wang; H. Xu; B. Wang","Department of Automation, Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai Jiao Tong University, Shanghai, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China","IEEE Transactions on Cybernetics","13 Jan 2023","2023","53","2","913","926","This article proposes an observer-based reinforcement learning (RL) control approach to address the optimal attitude-tracking problem and application for hypersonic vehicles in the reentry phase. Due to the unknown uncertainty and nonlinearity caused by parameter perturbation and external disturbance, accurate model information of hypersonic vehicles in the reentry phase is generally unavailable. For this reason, a novel synchronous estimation is proposed to construct a composite observer for hypersonic vehicles, which consists of a neural-network (NN)-based Luenberger-type observer and a synchronous disturbance observer. This solves the identification problem of nonlinear dynamics in the reference control and realizes the estimation of the system state when unknown nonlinear dynamics and unknown disturbance exist at the same time. By synthesizing the information from the composite observer, an RL tracking controller is developed to solve the optimal attitude-tracking control problem. To improve the convergence performance of critic network weights, concurrent learning is employed to replace the traditional persistent excitation condition with a historical experience replay manner. In addition, this article proves that the weight estimation error is bounded when the learning rate satisfies the given sufficient condition. Finally, the numerical simulation demonstrates the effectiveness and superiority of the proposed approaches to attitude-tracking control systems for hypersonic vehicles.","2168-2275","","10.1109/TCYB.2022.3192871","National Natural Science Foundation of China(grant numbers:61533013,61633019,61903290,62273234); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857673","Attitude-tracking control;near-optimal control;observer design;reinforcement learning (RL)","Hypersonic vehicles;Nonlinear dynamical systems;Optimal control;Observers;Attitude control;Aerodynamics;Vehicle dynamics","adaptive control;attitude control;closed loop systems;control engineering computing;convergence;Lyapunov methods;neurocontrollers;nonlinear control systems;observers;optimal control;reinforcement learning;tracking;uncertain systems","composite observer-based optimal attitude-tracking control;hypersonic vehicles;neural-network-based Luenberger-type observer;nonlinearity;observer-based reinforcement learning control;optimal attitude-tracking problem;reentry phase;reference control;RL tracking controller;synchronous disturbance observer;unknown disturbance;unknown nonlinear dynamics;unknown uncertainty","","6","","51","IEEE","16 Aug 2022","","","IEEE","IEEE Journals"
"Data-Driven Dynamic Multiobjective Optimal Control: An Aspiration-Satisfying Reinforcement Learning Approach","M. Mazouchi; Y. Yang; H. Modares","Mechanical Engineering Department, Michigan State University, East Lansing, MI, USA; Key Laboratory of Knowledge Automation for Industrial Processes of Ministry of Education, School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Mechanical Engineering Department, Michigan State University, East Lansing, MI, USA","IEEE Transactions on Neural Networks and Learning Systems","27 Oct 2022","2022","33","11","6183","6193","This article presents an iterative data-driven algorithm for solving dynamic multiobjective (MO) optimal control problems arising in control of nonlinear continuous-time systems. It is first shown that the Hamiltonian functional corresponding to each objective can be leveraged to compare the performance of admissible policies. Hamiltonian inequalities are then used for which their satisfaction guarantees satisfying the objectives’ aspirations. Relaxed Hamilton–Jacobi–Bellman (HJB) equations in terms of HJB inequalities are then solved in a dynamic constrained MO framework to find Pareto optimal solutions. Relation to satisficing (good enough) decision-making framework is shown. A sum-of-square (SOS)-based iterative algorithm is developed to solve the formulated aspiration-satisfying MO optimization. To obviate the requirement of complete knowledge of the system dynamics, a data-driven satisficing reinforcement learning approach is proposed to solve the SOS optimization problem in real time using only the information of the system trajectories measured during a time interval without having full knowledge of the system dynamics. Finally, two simulation examples are utilized to verify the analytical results of the proposed algorithm.","2162-2388","","10.1109/TNNLS.2021.3072571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9411709","Multiobjective (MO) optimization;reinforcement learning (RL);satisficing control;of-square program","Linear programming;Optimal control;Pareto optimization;System dynamics;Cost function;Trajectory;Nonlinear systems","continuous time systems;iterative methods;learning (artificial intelligence);nonlinear control systems;optimal control;optimisation;Pareto optimisation","aspiration-satisfying reinforcement learning approach;continuous-time systems;data-driven dynamic multiobjective optimal control;data-driven satisficing reinforcement learning approach;dynamic constrained MO framework;formulated aspiration-satisfying;Hamiltonian inequalities;HJB inequalities;iterative data-driven algorithm;objectives;optimal control problems;Pareto optimal solutions;Relaxed Hamilton-Jacobi-Bellman equations;satisficing decision-making framework;SOS optimization problem;sum-of-square-based;system dynamics;system trajectories","","5","","33","IEEE","22 Apr 2021","","","IEEE","IEEE Journals"
"Two-Timescale Voltage Regulation in Distribution Grids Using Deep Reinforcement Learning","Q. Yang; G. Wang; A. Sadeghi; G. B. Giannakis; J. Sun","School of Automation Beijing Institute of Technology, Beijing, China; ECE Dept. and Digital Tech. Center University of Minnesota, Minneapolis, USA; ECE Dept. and Digital Tech. Center University of Minnesota, Minneapolis, USA; ECE Dept. and Digital Tech. Center University of Minnesota, Minneapolis, USA; School of Automation Beijing Institute of Technology, Beijing, China","2019 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)","25 Nov 2019","2019","","","1","6","Frequent and sizeable voltage fluctuations become more pronounced with the increasing penetration of distributed renewable generation, and they considerably challenge distribution grids. Voltage regulation schemes so far have relied on either utility-owned devices (e.g., voltage transformers, and shunt capacitors), or more recently, smart power inverters that come with contemporary distributed generation units (e.g., photovoltaic systems, and wind turbines). Nonetheless, due to the distinct response times of those devices, as well as the discrete on-off commitment of capacitor units, joint control of both types of assets is challenging. In this context, a novel two-timescale voltage regulation scheme is developed here by coupling optimization with reinforcement learning advances. Shunt capacitors are configured on a slow timescale (e.g., daily basis) leveraging a deep reinforcement learning algorithm, while optimal setpoints of the power inverters are computed using a linearized distribution flow model on a fast timescale (e.g., every few seconds or minutes). Numerical experiments using a real-world 47-bus distribution feeder showcase the remarkable performance of the novel scheme.","","978-1-5386-8099-5","10.1109/SmartGridComm.2019.8909764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8909764","two-timescale;voltage regulation;inverter;capacitor;deep reinforcement learning.","Capacitors;Inverters;Voltage control;Reinforcement learning;Reactive power;Optimization;Smart grids","distributed power generation;invertors;learning (artificial intelligence);optimisation;power capacitors;power distribution control;power generation control;power grids;renewable energy sources;voltage control","real-world 47-bus distribution feeder;distribution grids;voltage fluctuations;distributed renewable generation;utility-owned devices;voltage transformers;smart power inverters;contemporary distributed generation units;distinct response times;capacitor units;two-timescale voltage regulation scheme;deep reinforcement learning algorithm;linearized distribution flow model;discrete on-off commitment;coupling optimization","","5","","21","IEEE","25 Nov 2019","","","IEEE","IEEE Conferences"
"A Comparison of Deep Reinforcement Learning Models for Isolated Traffic Signal Control","F. Mao; Z. Li; L. Li","Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Intelligent Transportation Systems Magazine","11 Jan 2023","2023","15","1","160","180","Traditional control methods may not be adaptive enough for ever-changing traffic dynamics. Hence, extensive deep reinforcement learning (DRL) methods have been utilized to solve the traffic signal timing control problem because DRL can adaptively learn optimal policy through analyzing experience samples generated by interaction with the environment. However, there is an urgent need for researchers to answer which DRL algorithm should be adopted in practice and how to select model settings. Therefore, we introduce a reasonable simulation platform to test and compare different DRL methods. Specifically, we evaluate seven prevailing DRL algorithms under our defined model settings from two aspects: training and execution performance. Testing results indicate that the soft actor–critic (SAC) outperforms other DRL algorithms and the maximum pressure method in most cases. To our best knowledge, this is also the first study to apply SAC and value distribution methods for traffic signal control. To answer how to select model settings, we compare the execution performance of the DRL algorithms with different state, action, and reward settings. Experimental results reveal the superiority of our model-setting choices. All these findings have enlightening effects on other traffic decision management problems, such as ramp and multi-intersection control.","1941-1197","","10.1109/MITS.2022.3144797","Key Research and Development Program of Guangdong Province(grant numbers:2020B0909050003); Shenzhen Municipal Science and Technology Innovation Committee(grant numbers:CJGJZD20200617102801005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712430","","Task analysis;Heuristic algorithms;Biological system modeling;Classification algorithms;Timing;Adaptation models;Traffic control;Reinforcement learning","reinforcement learning;road traffic control;traffic engineering computing","deep reinforcement learning models;DRL algorithm;DRL methods;isolated traffic signal control;maximum pressure method;multiintersection control;SAC;soft actor-critic;traffic decision management problems;traffic signal timing control problem;value distribution methods","","5","","56","IEEE","14 Feb 2022","","","IEEE","IEEE Magazines"
"Multiple-Pilot Collaboration for Advanced Remote Intervention using Reinforcement Learning","Z. Wang; W. Bai; Z. Chen; B. Xiao; B. Liang; E. M. Yeatman","Department of Bioengineering, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Department of Automation, Tsinghua University, Beijing, China; Department of Computing, Imperial College London, London, United Kingdom; Department of Automation, Tsinghua University, Beijing, China; Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom","IECON 2021 – 47th Annual Conference of the IEEE Industrial Electronics Society","10 Nov 2021","2021","","","1","6","The traditional master-slave teleoperation relies on human expertise without correction mechanisms, resulting in excessive physical and mental workloads. To address these issues, a co-pilot-in-the-loop control framework is investigated for cooperative teleoperation. A deep deterministic policy gradient (DDPG) based agent is realised to effectively restore the master operators' intents without prior knowledge on time delay. The proposed framework allows for introducing an operator (i.e., copilot) to generate commands at the slave side, whose weights are optimally assigned online through DDPG-based arbitration, thereby enhancing the command robustness in the case of possible human operational errors. With the help of interval type-2 (IT2) Takagi-Sugeno (T-S) fuzzy identification, force feedback can be reconstructed at the master side without a sense of delay, thus ensuring the telepresence performance in the force-sensor-free scenarios. Two experimental applications validate the effectiveness of the proposed framework.","2577-1647","978-1-6654-3554-3","10.1109/IECON48115.2021.9589570","Engineering and Physical Sciences Research Council; European Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9589570","Collaborative teleoperation;IT2 fuzzy system;Reinforcement learning;Time delay;Kalman filter","Training;Telepresence;Target tracking;Delay effects;Force feedback;Collaboration;Reinforcement learning","delays;force feedback;fuzzy control;fuzzy set theory;learning (artificial intelligence);robust control;telecontrol;telerobotics","DDPG-based arbitration;command robustness;possible human operational errors;interval type-2 Takagi-Sugeno;master side;force-sensor-free scenarios;multiple-pilot collaboration;advanced remote intervention;reinforcement learning;traditional master-slave teleoperation;human expertise;correction mechanisms;excessive physical workloads;mental workloads;co-pilot-in-the-loop control framework;deep deterministic policy gradient based agent;master operators;time delay;slave side","","4","","23","IEEE","10 Nov 2021","","","IEEE","IEEE Conferences"
"Solver–Critic: A Reinforcement Learning Method for Discrete-Time-Constrained-Input Systems","X. Yuan; L. Dong; C. Sun","School of Automation, Southeast University, Nanjing, China; College of Electronics and Information Engineering, Tongji University, Shanghai, China; School of Automation, Southeast University, Nanjing, China","IEEE Transactions on Cybernetics","9 Nov 2021","2021","51","11","5619","5630","In this article, a solver–critic (SC) architecture is developed for optimal control problems of discrete-time (DT)-constrained-input systems. The proposed design consists of three parts: 1) a critic network; 2) an action solver; and 3) a target network. The critic network first approximates the action-value function using the sum-of-squares (SOS) polynomial. Then, the action solver adopts the SOS programming to obtain control inputs within the constraint set. The target network introduces the soft update mechanism into policy evaluation to stabilize the learning process. By using the proposed architecture, the constrained-input control problem can be solved without adding the nonquadratic functionals into the reward function. In this article, the theoretical analysis of the convergence property is presented. Besides, the effects of both different initial  $Q$ -functions and different discount factors are investigated. It is proven that the learned policy converges to the optimal solution of the Hamilton–Jacobi–Bellman equation. Four numerical examples are provided to validate the theoretical analysis and also demonstrate the effectiveness of our approach.","2168-2275","","10.1109/TCYB.2020.2978088","National Natural Science Foundation of China(grant numbers:61921004,U1713209,61803085); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9043890","Input constraints;optimal control;reinforcement learning (RL);sum-of-squares (SOS) programming","Optimal control;Convergence;Estimation;Programming profession;Cost function","convergence of numerical methods;discrete time systems;learning systems;optimal control","nonquadratic functionals;reward function;theoretical analysis;discrete-time-constrained-input systems;solver-critic architecture;optimal control problems;critic network;action solver;action-value function;sum-of-squares polynomial;SOS programming;control inputs;soft update mechanism;learning process;initial Q-functions;reinforcement learning method;discount factors;Hamilton-Jacobi-Bellman equation;convergence property","","4","","44","IEEE","20 Mar 2020","","","IEEE","IEEE Journals"
"Jamming-Resilient Path Planning for Multiple UAVs via Deep Reinforcement Learning","X. Wang; M. Cenk Gursoy; T. Erpek; Y. E. Sagduyu","Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY; Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY; Intelligent Automation, Inc., Rockville, MD; Intelligent Automation, Inc., Rockville, MD","2021 IEEE International Conference on Communications Workshops (ICC Workshops)","9 Jul 2021","2021","","","1","6","Unmanned aerial vehicles (UAVs) are expected to be an integral part of wireless networks. In this paper, we aim to find collision-free paths for multiple cellular-connected UAVs, while satisfying requirements of connectivity with ground base stations (GBSs) in the presence of a dynamic jammer. We first formulate the problem as a sequential decision making problem in discrete domain, with connectivity, collision avoidance, and kinematic constraints. We, then, propose an offline temporal difference (TD) learning algorithm with online signal-to-interference-plus-noise ratio (SINR) mapping to solve the problem. More specifically, a value network is constructed and trained offline by TD method to encode the interactions among the UAVs and between the UAVs and the environment; and an online SINR mapping deep neural network (DNN) is designed and trained by supervised learning, to encode the influence and changes due to the jammer. Numerical results show that, without any information on the jammer, the proposed algorithm can achieve performance levels close to that of the ideal scenario with the perfect SINR-map. Real-time navigation for multi-UAVs can be efficiently performed with high success rates, and collisions are avoided.","2694-2941","978-1-7281-9441-7","10.1109/ICCWorkshops50388.2021.9473587","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9473587","Jamming resiliency;multi-UAV path planning;wireless connectivity;collision avoidance;decentralized deep reinforcement learning","Navigation;Conferences;Supervised learning;Interference;Reinforcement learning;Real-time systems;Trajectory","autonomous aerial vehicles;collision avoidance;decision making;jamming;learning (artificial intelligence);mobile robots;neural nets;path planning;remotely operated vehicles","jamming-resilient path planning;multiple UAVs;deep reinforcement learning;unmanned aerial vehicles;wireless networks;collision-free paths;cellular-connected UAVs;satisfying requirements;ground base stations;dynamic jammer;sequential decision making problem;discrete domain;collision avoidance;kinematic constraints;offline temporal difference learning algorithm;online signal-to-interference-plus-noise ratio mapping;value network;online SINR mapping deep neural network;supervised learning;perfect SINR-map;multiUAVs","","4","","22","IEEE","9 Jul 2021","","","IEEE","IEEE Conferences"
"Efficient Exploration for Multi-Agent Reinforcement Learning via Transferable Successor Features","W. Liu; L. Dong; D. Niu; C. Sun","School of Artificial Intelligence, Anhui University, Hefei; School of Cyber Science and Engineering, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China","IEEE/CAA Journal of Automatica Sinica","23 Aug 2022","2022","9","9","1673","1686","In multi-agent reinforcement learning (MARL), the behaviors of each agent can influence the learning of others, and the agents have to search in an exponentially enlarged joint-action space. Hence, it is challenging for the multi-agent teams to explore in the environment. Agents may achieve suboptimal policies and fail to solve some complex tasks. To improve the exploring efficiency as well as the performance of MARL tasks, in this paper, we propose a new approach by transferring the knowledge across tasks. Differently from the traditional MARL algorithms, we first assume that the reward functions can be computed by linear combinations of a shared feature function and a set of task-specific weights. Then, we define a set of basic MARL tasks in the source domain and pre-train them as the basic knowledge for further use. Finally, once the weights for target tasks are available, it will be easier to get a well-performed policy to explore in the target domain. Hence, the learning process of agents for target tasks is speeded up by taking full use of the basic knowledge that was learned previously. We evaluate the proposed algorithm on two challenging MARL tasks: cooperative box-pushing and non-monotonic predator-prey. The experiment results have demonstrated the improved performance compared with state-of-the-art MARL algorithms.","2329-9274","","10.1109/JAS.2022.105809","National Natural Science Foundation of China(grant numbers:62173251,61921004,U1713209); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9865022","Knowledge transfer;multi-agent systems;reinforcement learning;successor features","Reinforcement learning;Predator prey systems;Behavioral sciences;Task analysis","multi-agent systems;reinforcement learning","multiagent reinforcement learning;transferable successor features;multiagent teams;MARL algorithms;shared feature function;cooperative box-pushing;nonmonotonic predator-prey","","4","","49","","23 Aug 2022","","","IEEE","IEEE Journals"
"Hierarchical reinforcement learning guidance with threat avoidance","B. Li; Y. Wu; G. Li","State Key Laboratory of Virtual Reality Technology and System, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and System, Beihang University, Beijing, China; School of Astronautics, Northwestern Polytechnical University, Xi'an, China","Journal of Systems Engineering and Electronics","4 Nov 2022","2022","33","5","1173","1185","The guidance strategy is an extremely critical factor in determining the striking effect of the missile operation. A novel guidance law is presented by exploiting the deep reinforcement learning (DRL) with the hierarchical deep deterministic policy gradient (DDPG) algorithm. The reward functions are constructed to minimize the line-of-sight (LOS) angle rate and avoid the threat caused by the opposed obstacles. To attenuate the chattering of the acceleration, a hierarchical reinforcement learning structure and an improved reward function with action penalty are put forward. The simulation results validate that the missile under the proposed method can hit the target successfully and keep away from the threatened areas effectively.","1004-4132","","10.23919/JSEE.2022.000113","National Natural Science Foundation of China(grant numbers:62003021,91212304); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9940160","guidance law;deep reinforcement learning (DRL);threat avoidance;hierarchical reinforcement learning","Training;Simulation;Decision making;Line-of-sight propagation;Reinforcement learning;Aerospace electronics;Systems engineering and theory","aerospace computing;control engineering computing;deep learning (artificial intelligence);gradient methods;military computing;missile guidance;reinforcement learning","missile operation;guidance law;deep reinforcement learning;hierarchical deep deterministic policy gradient algorithm;line-of-sight angle rate;hierarchical reinforcement learning guidance;threat avoidance;striking effect;reward function;DRL;DDPG;LOS;action penalty","","3","","36","","4 Nov 2022","","","BIAI","BIAI Journals"
"Optimal Tracking Control of Heterogeneous MASs Using Event-Driven Adaptive Observer and Reinforcement Learning","Y. Xu; J. Sun; Y. -J. Pan; Z. -G. Wu","School of Automation, Beijing Institute of Technology, Beijing, China; State Key Laboratory of Intelligent Control and Decision of Complex Systems and the School of Automation, Beijing Institute of Technology, Beijing, China; Department of Mechanical Engineering, Dalhousie University, Halifax, Canada; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","11","This article considers the output tracking control problem of nonidentical linear multiagent systems (MASs) using a model-free reinforcement learning (RL) algorithm, where partial followers have no prior knowledge of the leader’s information. To lower the communication and computing burden among agents, an event-driven adaptive distributed observer is proposed to predict the leader’s system matrix and state, which consists of the estimated value of relative states governed by an edge-based predictor. Meanwhile, the integral input-based triggering condition is exploited to decide whether to transmit its private control input to its neighbors. Then, an RL-based state feedback controller for each agent is developed to solve the output tracking control problem, which is further converted into the optimal control problem by introducing a discounted performance function. Inhomogeneous algebraic Riccati equations (AREs) are derived to obtain the optimal solution of AREs. An off-policy RL algorithm is used to learn the solution of inhomogeneous AREs online without requiring any knowledge of the system dynamics. Rigorous analysis shows that under the proposed event-driven adaptive observer mechanism and RL algorithm, all followers are able to synchronize the leader’s output asymptotically. Finally, a numerical simulation is demonstrated to verify the proposed approach in theory.","2162-2388","","10.1109/TNNLS.2022.3208237","National Natural Science Foundation of China(grant numbers:61925303,62088101,U20B2073,62103047,U1966202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9908586","Adaptive observer;event-triggered control;multiagent systems (MASs);reinforcement learning (RL)","Observers;Vehicle dynamics;Privacy;Heuristic algorithms;Reinforcement learning;Nonhomogeneous media;Synchronization","","","","3","","","IEEE","3 Oct 2022","","","IEEE","IEEE Early Access Articles"
"Adaptive Optimal Tracking Control of an Underactuated Surface Vessel Using Actor–Critic Reinforcement Learning","L. Chen; S. -L. Dai; C. Dong","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Ministry of Education, and Unmanned Aerial Vehicle Systems Engineering Technology Research Center of Guangdong, School of Automation Science and Engineering, the Key Laboratory of Autonomous Systems and Networked Control, South China University of Technology, Guangzhou, China; Southern Marine Science and Engineering Guangdong Laboratory (Zhuhai), Zhuhai, China","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","14","In this article, we present an adaptive reinforcement learning optimal tracking control (RLOTC) algorithm for an underactuated surface vessel subject to modeling uncertainties and time-varying external disturbances. By integrating backstepping technique with the optimized control design, we show that the desired optimal tracking performance of vessel control is guaranteed due to the fact that the virtual and actual control inputs are designed as optimized solutions of every subsystem. To enhance the robustness of vessel control systems, we employ neural network (NN) approximators to approximate uncertain vessel dynamics and present adaptive control technique to estimate the upper boundedness of external disturbances. Under the reinforcement learning framework, we construct actor–critic networks to solve the Hamilton–Jacobi–Bellman equations corresponding to subsystems of surface vessel to achieve the optimized control. The optimized control algorithm can synchronously train the adaptive parameters not only for actor–critic networks but also for NN approximators and adaptive control. By Lyapunov stability theorem, we show that the RLOTC algorithm can ensure the semiglobal uniform ultimate boundedness of the closed-loop systems. Compared with the existing reinforcement learning control results, the presented RLOTC algorithm can compensate for uncertain vessel dynamics and unknown disturbances, and obtain the optimized control performance by considering optimization in every backstepping design. Simulation studies on an underactuated surface vessel are given to illustrate the effectiveness of the RLOTC algorithm.","2162-2388","","10.1109/TNNLS.2022.3214681","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B1111010002); National Natural Science Foundation of China(grant numbers:61973129,42227901,62273156); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515012004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9967790","Actor–critic networks;adaptive optimal tracking control;reinforcement learning;underactuated surface vessel","Heuristic algorithms;Optimal control;Artificial neural networks;Backstepping;Adaptation models;Control design;Sea surface","","","","3","","","IEEE","30 Nov 2022","","","IEEE","IEEE Early Access Articles"
"Solving Robotic Manipulation With Sparse Reward Reinforcement Learning Via Graph-Based Diversity and Proximity","Z. Bing; H. Zhou; R. Li; X. Su; F. O. Morin; K. Huang; A. Knoll","Department of Informatics, Technical University of Munich, Munich, Germany; Department of Informatics, Technical University of Munich, Munich, Germany; School of Automation, Chongqing University, Chongqing, China; School of Automation, Chongqing University, Chongqing, China; Department of Informatics, Technical University of Munich, Munich, Germany; School of Computer Science, Sun Yat-sen University, Guangzhou, China; Department of Informatics, Technical University of Munich, Munich, Germany","IEEE Transactions on Industrial Electronics","18 Nov 2022","2023","70","3","2759","2769","In multigoal reinforcement learning (RL), algorithms usually suffer from inefficiency in the collection of successful experiences in tasks with sparse rewards. By utilizing the ideas of relabeling hindsight experience and curriculum learning, some prior works have greatly improved the sample efficiency in robotic manipulation tasks, such as hindsight experience replay (HER), hindsight goal generation (HGG), graph-based HGG (G-HGG), and curriculum-guided HER (CHER). However, none of these can learn efficiently to solve challenging manipulation tasks with distant goals and obstacles, since they rely either on heuristic or simple distance-guided exploration. In this article, we introduce graph-curriculum-guided HGG (GC-HGG), an extension of CHER and G-HGG, which works by selecting hindsight goals on the basis of graph-based proximity and diversity. We evaluated GC-HGG in four challenging manipulation tasks involving obstacles in both simulations and real-world experiments, in which significant enhancements in both sample efficiency and overall success rates over prior works were demonstrated. Videos and codes can be viewed at this link: https://videoviewsite.wixsite.com/gc-hgg.","1557-9948","","10.1109/TIE.2022.3172754","European Union’s Horizon 2020 Framework Programme for Research and Innovation(grant numbers:945539); Human Brain Project SGA3; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9772990","Hindsight experience replay (HER);path planning;reinforcement learning;robotic arm manipulation","Task analysis;Training;Robots;Euclidean distance;Trajectory;Reinforcement learning;Optimization","graph theory;learning (artificial intelligence);learning systems;manipulators","CHER;curriculum learning;curriculum-guided HER;distance-guided exploration;G-HGG;graph-based diversity;graph-based HGG;graph-based proximity;graph-curriculum-guided HGG;hindsight experience replay;hindsight goal generation;multigoal reinforcement learning;robotic manipulation task;sparse reward reinforcement learning","","3","","27","IEEE","11 May 2022","","","IEEE","IEEE Journals"
"Shortest-Path-Based Deep Reinforcement Learning for EV Charging Routing Under Stochastic Traffic Condition and Electricity Prices","J. Jin; Y. Xu","Department of Mechanical and Automation Engineering, Shun Hing Institute of Advanced Engineering, Chinese University of Hong Kong, Hong Kong; Department of Mechanical and Automation Engineering, Chinese University of Hong Kong, Hong Kong SAR","IEEE Internet of Things Journal","4 Nov 2022","2022","9","22","22571","22581","We study the charging routing problem faced by a smart electric vehicle (EV) that looks for an EV charging station (EVCS) to fulfill its battery charging demand. Leveraging the real-time information from both the power and intelligent transportation systems, the EV seeks to minimize the sum of the travel cost (to a selected EVCS) and the charging cost (a weighted sum of electricity cost and waiting time cost at the EVCS). We formulate the problem as a Markov decision process with unknown dynamics of system uncertainties (in traffic conditions, charging prices, and waiting time). To mitigate the curse of dimensionality, we reformulate the deterministic charging routing problem (a mixed-integer program) as a two-level shortest-path (SP)-based optimization problem that can be solved in polynomial time. Its low dimensional solution is input into a state-of-the-art deep reinforcement learning (DRL) algorithm, the advantage actor–critic (A2C) method, to make efficient online routing decisions. Numerical results (on a real-world transportation network) demonstrate that the proposed SP-based A2C approach outperforms the classical A2C method and two alternative SP-based DRL methods.","2327-4662","","10.1109/JIOT.2022.3181613","General Research Fund (GRF) Project through the Hong Kong University Grants Committee(grant numbers:14200720); National Natural Science Foundation of China (NSFC) Project(grant numbers:62073273); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792244","Deep reinforcement learning (DRL);electric vehicle (EV) charging routing;Markov decision process (MDP);shortest path (SP);smart city","Routing;Costs;Electric vehicle charging;Transportation;Real-time systems;Heuristic algorithms;Batteries","battery powered vehicles;electric vehicles;graph theory;integer programming;learning (artificial intelligence);Markov processes;optimisation;pricing;search problems;smart power grids;stochastic processes;telecommunication network routing;transportation","advantage actor-critic method;battery charging demand;charging cost;deterministic charging routing problem;efficient online routing decisions;electricity cost;electricity prices;EV charging routing;EV charging station;intelligent transportation systems;low dimensional solution;Markov decision process;mixed-integer program;polynomial time;real-time information;selected EVCS;shortest-path-based deep reinforcement learning;smart electric vehicle;state-of-the-art deep reinforcement learning algorithm;stochastic traffic condition;system uncertainties;traffic conditions;travel cost;two-level shortest-path-based optimization problem;waiting time cost;weighted sum","","3","","48","IEEE","9 Jun 2022","","","IEEE","IEEE Journals"
"MAC Contention Protocol Based on Reinforcement Learning for IoV Communication Environments","Z. Pei; W. Chen; L. Du; H. Zheng","School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Automation Wuhan University of Technology, Wuhan, China; School of Automation Wuhan University of Technology, Wuhan, China; Shanghai Engineering Technology Research Center for ICVT, Shanghai, China","2021 IEEE 6th International Conference on Computer and Communication Systems (ICCCS)","21 Jun 2021","2021","","","546","552","The Medium Access Control (MAC) layer contention protocol is closely related to the performance of network throughput, end-to-end delay, and access fairness on the Internet of Vehicles (IoV) communication. Based on the MAC layer protocol of the Wireless Access in Vehicular Environments (WAVE) standard system, this paper proposes a MAC layer contention window adaptive adjustment policy using Reinforcement Learning. Through the detection of the number of neighbors and the application of the Q-Learning algorithm, the vehicle can adjust the contention window according to the number of nodes competing for the same channel to adapt to the changing environments of the IoV. Three different MAC protocols are simulated and analyzed under the Vehicle in Network Simulation (Veins) platform. The results show that the proposed MAC protocol based on neighbor detection and Q-Learning performs better than WAVE MAC protocol and general MAC protocol based on Q-Learning.","","978-1-6654-1256-8","10.1109/ICCCS52626.2021.9449308","National Key R&D Program of China(grant numbers:2018YFB0105205); China Scholarship Council(grant numbers:202006950024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9449308","internet of vehicles;MAC protocol;Q-Learning;number of neighbors","Analytical models;Adaptation models;Protocols;Wireless Access in Vehicular Environments;Veins;Reinforcement learning;Media Access Protocol","access protocols;Internet of Things;learning (artificial intelligence);mobile computing;signal detection","MAC contention protocol;Reinforcement Learning;IoV communication Environments;Medium Access Control layer contention protocol;network throughput;end-to-end delay;access fairness;MAC layer protocol;Vehicular Environments standard system;MAC layer contention window adaptive adjustment policy;Network Simulation platform;Q-learning algorithm;WAVE MAC protocol;wireless access in vehicular environments standard system;Internet of Vehicles communication;WAVE standard system;Veins platform;neighbor detection","","3","","19","IEEE","21 Jun 2021","","","IEEE","IEEE Conferences"
"Joint Energy and Workload Scheduling for Fog-Assisted Multimicrogrid Systems: A Deep Reinforcement Learning Approach","T. Zhang; D. Yue; L. Yu; C. Dou; X. Xie","College of Automation & College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Jiangsu Province, China; Institute of Advanced Technology, Nanjing University of Posts and Telecommunications, Jiangsu Province, China; College of Automation & College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Jiangsu Province, China; Institute of Advanced Technology, Nanjing University of Posts and Telecommunications, Jiangsu Province, China; Institute of Advanced Technology, Nanjing University of Posts and Telecommunications, Jiangsu Province, China","IEEE Systems Journal","23 Feb 2023","2023","17","1","164","175","Due to the fluctuation of renewable energy, the uncertainty of electrical loads, and the complexity of networked microgrids, it is challenging to dispatch multiple resources to minimize the operating cost of multi-microgrid system (MMGS). In this article, a fog-assisted operating cost minimization problem of MMGS is investigated with the consideration of source-grid-load-storage-computing coordination. Since there are strong couplings among multiple resources, it is difficult to solve the problem directly. Therefore, the above problem is reformulated as a Markov game. Then, a novel energy management algorithm is proposed to solve Markov game based on multiagent deep reinforcement learning. It is worth mentioning that the proposed energy management algorithm can support local real-time decisions for each microgrid without knowing any prior knowledge of uncertain parameters and private information of other microgrids. Simulation results indicate that the proposed algorithm can reduce the long-term cost of each microgrid by 0.09$\%$–8.02$\%$ compared with baselines.","1937-9234","","10.1109/JSYST.2022.3171534","National Key R & D Program of China(grant numbers:2018YFA0702200); Leading Technology Foundation Research Project of Jiangsu Province(grant numbers:BK20202011); Postgraduate Research and Practice Innovation Program of Jiangsu Province(grant numbers:KYCX21_0784); National Natural Science Foundation of China(grant numbers:61903199,61972214,62192751); China Postdoctoral Science Foundation(grant numbers:2020M673406); 1311 Talent Project of Nanjing University of Posts and Telecommunications; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9778182","Deep reinforcement learning;energy management;fog-assisted;Markov game;multimicrogrid;operating cost","Microgrids;Task analysis;Costs;Energy management;Renewable energy sources;Reinforcement learning;Optimization","deep learning (artificial intelligence);distributed power generation;energy management systems;game theory;Markov processes;minimisation;multi-agent systems;power engineering computing;power generation control;power generation economics;power grids;reinforcement learning","electrical loads;fog-assisted multimicrogrid systems;fog-assisted operating cost;long-term cost;Markov game;microgrid;MMGS;multiagent deep reinforcement learning;multimicrogrid system;multiple resources;networked microgrids;novel energy management algorithm;renewable energy;source-grid-load-storage-computing coordination;strong couplings","","3","","40","IEEE","19 May 2022","","","IEEE","IEEE Journals"
"Federated Multiagent Deep Reinforcement Learning Approach via Physics-Informed Reward for Multimicrogrid Energy Management","Y. Li; S. He; Y. Li; Y. Shi; Z. Zeng","School of Artificial Intelligence and Automation, Key Laboratory on Image Information Processing and Intelligent Control of Ministry of Education, Huazhong University of Science and Technology, Wuhan, China; China-EU Institute for Clean and Renewable Energy, Huazhong University of Science and Technology, Wuhan, China; School of Electrical Engineering, Northeast Electric Power University, Jilin, China; Department of Mechanical Engineering, University of Victoria, Victoria, Canada; School of Artificial Intelligence and Automation, Key Laboratory on Image Information Processing and Intelligent Control of Ministry of Education, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","13","The utilization of large-scale distributed renewable energy (RE) promotes the development of the multimicrogrid (MMG), which raises the need of developing an effective energy management method to minimize economic costs and keep self energy sufficiency. The multiagent deep reinforcement learning (MADRL) has been widely used for the energy management problem because of its real-time scheduling ability. However, its training requires massive energy operation data of microgrids (MGs), while gathering these data from different MGs would threaten their privacy and data security. Therefore, this article tackles this practical yet challenging issue by proposing a federated MADRL (F-MADRL) algorithm via the physics-informed reward. In this algorithm, the federated learning (FL) mechanism is introduced to train the F-MADRL algorithm, thus ensures the privacy and the security of data. In addition, a decentralized MMG model is built, and the energy of each participated MG is managed by an agent, which aims to minimize economic costs and keep self energy sufficiency according to the physics-informed reward. At first, MGs individually execute the self-training based on local energy operation data to train their local agent models. Then, these local models are periodically uploaded to a server and their parameters are aggregated to build a global agent, which will be broadcasted to MGs and replace their local agents. In this way, the experience of each MG agent can be shared and the energy operation data are not explicitly transmitted, thus protecting the privacy and ensuring data security. Finally, experiments are conducted on Oak Ridge National Laboratory distributed energy control communication laboratory MG (ORNL-MG) test system, and the comparisons are carried out to verify the effectiveness of introducing the FL mechanism and the outperformance of our proposed F-MADRL.","2162-2388","","10.1109/TNNLS.2022.3232630","National Natural Science Foundation of China(grant numbers:62233006,62073148); Key Scientific and Technological Research Project of State Grid Corporation of China(grant numbers:1400-202099523A-0-0-00); Technology Innovation Project of Hubei Province of China(grant numbers:2019AEA171); 111 Project on Computational Intelligence and Intelligent Control(grant numbers:B18024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10005155","Federated learning (FL);multiagent deep reinforcement learning (MADRL);multimicrogrid (MMG);proximal policy optimization (PPO)","Energy management;Data privacy;Servers;Reinforcement learning;Deep learning;Renewable energy sources;Data security","","","","3","","","IEEE","3 Jan 2023","","","IEEE","IEEE Early Access Articles"
"Deep Reinforcement Learning based Adaptive Real-Time Path Planning for UAV","J. Li; Y. Liu","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","2021 8th International Conference on Dependable Systems and Their Applications (DSA)","1 Dec 2021","2021","","","522","530","Real-time path planning typically aims to obtain a collision-free and shorter path with lower computational complexity for UAVs in unknown environment. Apart from the above basic objective, kinematic constraints and the smoothness of path should be further considered especially for fixed-wing UAVs restricted by their maneuverability. In this paper, we propose an adaptive real-time path planning method based on Deep Reinforcement Learning. Taking the sensor data of obstacles nearby and the target’s position relative to the UAV as the decision information, and designing the action satisfying kinematic constraints of fixed-wing UAV, the proposed method can plan a feasible path for fixed-wing UAV in real-time. Experimental results show that the adaptive action devised combining with greedy reward, granularity reward and smoothness reward can accelerate the convergence speed of the algorithm and enhance the smoothness of the planned path.","2767-6684","978-1-6654-4391-3","10.1109/DSA52907.2021.00077","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623051","adaptive;real-time path planning;deep reinforcement learning;UAV;fixed-wing","Adaptive systems;Reinforcement learning;Kinematics;Real-time systems;Path planning;Computational complexity;Convergence","autonomous aerial vehicles;collision avoidance;computational complexity;learning (artificial intelligence);mobile robots;path planning;robot kinematics","fixed-wing UAV;adaptive action;planned path;Deep Reinforcement Learning;shorter path;kinematic constraints;adaptive real-time path planning method","","2","","21","IEEE","1 Dec 2021","","","IEEE","IEEE Conferences"
"Velocity Regulation for Automatic Train Operation via Meta-Reinforcement Learning","F. Zhao; K. You; Y. Fan; G. Yan","Department of Automation and BNRist, Tsinghua University, Beijing, China; Department of Automation and BNRist, Tsinghua University, Beijing, China; CRRC Zhuzhou Locomotive Co. Ltd., Zhuzhou, China; CRRC Zhuzhou Locomotive Co. Ltd., Zhuzhou, China","2020 39th Chinese Control Conference (CCC)","9 Sep 2020","2020","","","1969","1974","In this paper, we consider the velocity regulation for automatic train operation to maintain the velocity of the train at a desired value. Due to complicated railway conditions and uncertain dynamics of the system, the problem cannot be well solved by most of the model-based controllers. To this purpose, we formulate the velocity regulation problem as a sequence of stationary Markov decision processes (MDP) with unknown transition probabilities. Based on the meta-learning framework, we propose a model-free algorithm to learn an adaptive controller, which only requires a ""small"" amount of sampled data from the corresponding MDP. We illustrate with simulations that our model-free controller performs well and can well adapt to the dynamical environments.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9188581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9188581","Velocity regulation;Markov decision procession;Reinforcement learning;Meta-learning;Neural network","Adaptation models;Rail transportation;Markov processes;Task analysis;Control systems;Learning (artificial intelligence);Uncertainty","adaptive control;learning (artificial intelligence);Markov processes;railways","velocity regulation;stationary Markov decision;meta-learning framework;adaptive controller;model-free controller;automatic train operation;meta-reinforcement learning;railway conditions;uncertain dynamics;model-based controllers","","2","","17","","9 Sep 2020","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Algorithm based on Neural Network for Economic Dispatch","L. Yu; N. Li","the department of Automation, Shanghai Jiao Tong University; the department of Automation, Shanghai Jiao Tong University","2020 39th Chinese Control Conference (CCC)","9 Sep 2020","2020","","","1637","1642","Economic Dispatch (ED) problem for power system as an important yet challenging task, is commonly solved by recent methods under the assumption that values stored in Q-table. While with the computational complexity and mass data, it is no longer suitable to calculate the reward in Q-learning. To address the above problem, we propose a new Q-learning model based on neural networks and find the proposed model play an important role in our task. Therefore we adopt the proposed an improved strategy to solve the issue. By using our model, the cost function can be effectively learned and directly predicted the result instead of looking up table stored Q-values. In addition, our model can solve the ED problem with load demand varying with time and have good applications in the power system. So it reveals that our proposed model is time-saving and easily mounted on the power system. The experimental results show that our method is effective and efficient compared with state-of-the-art works. Finally, we argue our method has great potential for further research.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9188641","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9188641","Economic Dispatch;Multi-stage Decision Making Problem;Reinforcement Learning;Neural Network","Neural networks;Economics;Learning (artificial intelligence);Cost function;Power systems;Training;Load modeling","learning (artificial intelligence);load forecasting;neural nets;power engineering computing;power generation dispatch;power generation economics","neural network;power system economic dispatch;Q-table;computational complexity;Q-learning model;cost function;table stored Q-values;reinforcement learning algorithm;load demand","","2","","25","","9 Sep 2020","","","IEEE","IEEE Conferences"
"Relational reinforcement learning and recurrent neural network with state classification to solve joint attention","R. R. d. Silva; R. A. F. Romero","Department of Mechanical and Automation Engineering, Institute of Mathematics and Computer Sciences, University of Sao Paulo, Sao Carlos, Sao Paulo, Brazil; Department of Mechanical and Automation Engineering, Institute of Mathematics and Computer Sciences, University of Sao Paulo, Sao Carlos, Sao Paulo, Brazil","The 2011 International Joint Conference on Neural Networks","3 Oct 2011","2011","","","1222","1229","Joint attention is an important non verbal communication learned by humans in a period of childhood. One learning method has been explored to provide this learning ability in robots is known as reinforcement learning. However, the use of this method using a Markov Decision Process model has problems. In this article, we have enhanced our robotic architecture, which is inspired on behavior analysis, to provide to the robot or agent, the capacity of joint attention using combination of relational reinforcement learning and recurrent neural network with state classification. We have incorporated this improvement as learning mechanism in our architecture to simulate joint attention. Then, a set of empirical evaluations has been conducted in the social interactive simulator for performing the task of joint attention. The performance of this algorithm have been compared with the Q-Learning algorithm, contingency learning algorithm and ETG algorithm. The experimental results show that this new method is better than other algorithms evaluated by us for joint attention problem.","2161-4407","978-1-4244-9637-2","10.1109/IJCNN.2011.6033363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6033363","","Humans;Joints;Computer architecture;Learning;Learning systems;Robot kinematics","cognition;learning (artificial intelligence);Markov processes;recurrent neural nets","relational reinforcement learning;recurrent neural network;state classification;joint attention problem;Markov decision process model;social interactive simulator;Q-learning algorithm;contingency learning algorithm;ETG algorithm","","2","","32","IEEE","3 Oct 2011","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Based Algorithm for Finite Horizon Markov Decision Processes","S. Bhatnagar; M. S. Abdulla","Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India","Proceedings of the 45th IEEE Conference on Decision and Control","7 May 2007","2006","","","5519","5524","We develop a simulation based algorithm for finite horizon Markov decision processes with finite state and finite action space. Illustrative numerical experiments with the proposed algorithm are shown for problems in flow control of communication networks and capacity switching in semiconductor fabrication","0191-2216","1-4244-0171-2","10.1109/CDC.2006.377190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4177082","Finite horizon Markov decision processes;reinforcement learning;two timescale stochastic approximation;actor-critic algorithms;normalized Hadamard matrices","Learning;Stochastic processes;Computational modeling;Communication system control;Recursive estimation;Poisson equations;Convergence;Costs;Communication networks;Approximation algorithms","decision theory;learning (artificial intelligence);Markov processes","reinforcement learning;finite horizon Markov decision process;finite state space;finite action space;flow control;communication network;capacity switching;semiconductor fabrication;timescale stochastic approximation;actor-critic algorithms;normalized Hadamard matrix","","2","","17","IEEE","7 May 2007","","","IEEE","IEEE Conferences"
"Day-ahead scheduling based on reinforcement learning with hybrid action space","J. Cao; L. Dong; C. Sun","School of Automation, Southeast University, Nanjing, China; School of Cyber Science and Engineering, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China","Journal of Systems Engineering and Electronics","1 Jul 2022","2022","33","3","693","705","Driven by the improvement of the smart grid, the active distribution network (ADN) has attracted much attention due to its characteristic of active management. By making full use of electricity price signals for optimal scheduling, the total cost of the ADN can be reduced. However, the optimal day-ahead scheduling problem is challenging since the future electricity price is unknown. Moreover, in ADN, some schedulable variables are continuous while some schedulable variables are discrete, which increases the difficulty of determining the optimal scheduling scheme. In this paper, the day-ahead scheduling problem of the ADN is formulated as a Markov decision process (MDP) with continuous-discrete hybrid action space. Then, an algorithm based on multi-agent hybrid reinforcement learning (HRL) is proposed to obtain the optimal scheduling scheme. The proposed algorithm adopts the structure of centralized training and decentralized execution, and different methods are applied to determine the selection policy of continuous scheduling variables and discrete scheduling variables. The simulation experiment results demonstrate the effectiveness of the algorithm.","1004-4132","","10.23919/JSEE.2022.000064","National Key R&D Program of China(grant numbers:2018AAA0101400); National Natural Science Foundation of China(grant numbers:62173251,61921004,U1713209); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20202006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9813510","day-ahead scheduling;active distribution network (ADN);reinforcement learning;hybrid action space","Training;Costs;Uncertainty;Simulation;Optimal scheduling;Reinforcement learning;Predictive models","learning (artificial intelligence);Markov processes;multi-agent systems;optimisation;power generation scheduling;power markets;pricing;scheduling;smart power grids","smart grid;active distribution network;ADN;active management;electricity price signals;optimal day-ahead scheduling problem;future electricity price;schedulable variables;optimal scheduling scheme;continuous-discrete hybrid action space;multiagent hybrid reinforcement learning;continuous scheduling variables;discrete scheduling variables","","2","","39","","1 Jul 2022","","","BIAI","BIAI Journals"
"A Novel Multi-agent Cooperative Reinforcement Learning Method for Home Energy Management under a Peak Power-limiting","F. Jiang; C. Zheng; D. Gao; X. Zhang; W. Liu; Y. Cheng; C. Hu; J. Peng","School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China","2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","14 Dec 2020","2020","","","350","355","Home energy management plays a key role in demand response for residential customers to reduce the total cost via scheduling household loads energy consumption. However, excessive energy consumption by customers will bring a great challenge to the stability of the grid. To address the challenge, a day-ahead multi-agent reinforcement learning method is proposed for home energy management under a peak power-limiting. We first formulate the total cost minimization problem as a Markov game, and then a novel household loads energy consumption scheduling algorithm is proposed based on Mutil-agent Deep Deterministic Policy Gradient (MADDPG). It is worth mentioning that the proposed algorithm can achieve cooperation between agents so that it can meet the peak power-limiting constraint. Simulation results are provided in this paper to show the effectiveness of the proposed method.","2577-1655","978-1-7281-8526-2","10.1109/SMC42975.2020.9282976","National Natural Science Foundation of China; National Natural Science Foundation of China; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282976","home energy management;reinforcement learning;energy cost;Markov game;demand response","Energy consumption;Scheduling algorithms;Simulation;Neural networks;Reinforcement learning;Power system stability;Stability analysis","energy consumption;energy management systems;game theory;gradient methods;home computing;learning (artificial intelligence);Markov processes;minimisation;multi-agent systems;power engineering computing;power generation scheduling","home energy management;total cost minimization problem;day-ahead multiagent reinforcement learning;Markov game;household loads energy consumption scheduling algorithm;mutilagent deep deterministic policy gradient;multiagent cooperative reinforcement learning;MADDPG;peak power-limiting","","2","","19","IEEE","14 Dec 2020","","","IEEE","IEEE Conferences"
