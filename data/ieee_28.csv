"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Tensegrity Robot Locomotion Under Limited Sensory Inputs via Deep Reinforcement Learning","J. Luo; R. Edmunds; F. Rice; A. M. Agogino","Department of Mechanical Engineering, University of California, Berkeley, CA, Berkeley; Department of EECS, University of California, Berkeley, CA; Department of EECS, University of California, Berkeley, CA; Department of Mechanical Engineering, University of California, Berkeley, CA, Berkeley","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","6260","6267","Tensegrity robots are composed of rigid rods connected by elastic cables, and their unique light-weight yet compliant structure makes them an appealing choice for space exploration. However, locomotion control for these robotic systems remains difficult due to their nonlinear dynamics and high-dimensional state space. We demonstrate that in the domain of tensegrity robotics, it is possible to efficiently learn end-to-end locomotion policies using mirror descent guided policy search (MDGPS) even with limited sensory inputs. We compare learned neural network policies with other locomotion control policies in various testing environments; and results show that neural network policies consistently outperform others. We also shed light to the policy learning process by analyzing different choices of observation inputs to the robot. Moreover these findings motivate exploration of deep reinforcement learning algorithms in the domain of tensegrity robotics. We show preliminary results with one such locomotion example on discontinuous rough terrains.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8463144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463144","","Robot sensing systems;Neural networks;Aerospace electronics;Hardware;NASA;Training","learning systems;mobile robots;motion control;neurocontrollers;nonlinear dynamical systems;search problems;state-space methods","nonlinear dynamics;high-dimensional state space;robotic systems;space exploration;tensegrity robot locomotion;deep reinforcement learning algorithms;policy learning process;locomotion control policies;neural network policies;mirror descent guided policy search;end-to-end locomotion policies;tensegrity robotics","","16","","32","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Model-based and model-free reinforcement learning for visual servoing","A. m. Farahmand; A. Shademan; M. Jagersand; C. Szepesvari","Department of Computing Science, University of Alberta, Canada; Department of Computing Science, University of Alberta, Canada; Department of Computing Science, University of Alberta, Canada; Department of Computing Science, University of Alberta, Canada","2009 IEEE International Conference on Robotics and Automation","6 Jul 2009","2009","","","2917","2924","To address the difficulty of designing a controller for complex visual-servoing tasks, two learning-based uncalibrated approaches are introduced. The first method starts by building an estimated model for the visual-motor forward kinematic of the vision-robot system by a locally linear regression method. Afterwards, it uses a reinforcement learning method named Regularized Fitted Q-Iteration to find a controller (i.e. policy) for the system (model-based RL). The second method directly uses samples coming from the robot without building any intermediate model (model-free RL). The simulation results show that both methods perform comparably well despite not having any a priori knowledge about the robot.","1050-4729","978-1-4244-2788-8","10.1109/ROBOT.2009.5152834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5152834","","Learning;Visual servoing;Robot vision systems;Jacobian matrices;Cameras;Control systems;Kinematics;Linear regression;Calibration;Manipulators","control system synthesis;intelligent robots;iterative methods;learning (artificial intelligence);regression analysis;robot kinematics;robot vision;visual servoing","model-free reinforcement learning;visual servoing;vision-robot system;linear regression method;regularized fitted Q-iteration;visual-motor forward kinematic;controller design","","16","","31","IEEE","6 Jul 2009","","","IEEE","IEEE Conferences"
"Autonomous Overtaking in Gran Turismo Sport Using Curriculum Reinforcement Learning","Y. Song; H. Lin; E. Kaufmann; P. Dürr; D. Scaramuzza","Dep. of Informatics, Robotics and Perception Group, University of Zurich; Sony AI Zurich; Dep. of Informatics, Robotics and Perception Group, University of Zurich; Sony AI Zurich; Dep. of Informatics, Robotics and Perception Group, University of Zurich","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","9403","9409","Professional race-car drivers can execute extreme overtaking maneuvers. However, existing algorithms for autonomous overtaking either rely on simplified assumptions about the vehicle dynamics or try to solve expensive trajectory-optimization problems online. When the vehicle approaches its physical limits, existing model-based controllers struggle to handle highly nonlinear dynamics, and cannot leverage the large volume of data generated by simulation or real-world driving. To circumvent these limitations, we propose a new learning-based method to tackle the autonomous overtaking problem. We evaluate our approach in the popular car racing game Gran Turismo Sport, which is known for its detailed modeling of various cars and tracks. By leveraging curriculum learning, our approach leads to faster convergence as well as increased performance compared to vanilla reinforcement learning. As a result, the trained controller outperforms the built-in model-based game AI and achieves comparable over-taking performance with an experienced human driver.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561049","","Solid modeling;Reinforcement learning;Games;Data models;Automobiles;Vehicle dynamics;Task analysis","automobiles;computer games;drives;learning (artificial intelligence);optimisation;road vehicles;sport;vehicle dynamics","model-based game AI;curriculum reinforcement learning;professional race-car drivers;extreme overtaking maneuvers;simplified assumptions;vehicle dynamics;expensive trajectory-optimization problems;model-based controllers struggle;highly nonlinear dynamics;learning-based method;autonomous overtaking problem;popular car racing game Gran Turismo Sport;cars;leveraging curriculum learning;vanilla reinforcement learning","","16","","27","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"A reinforcement-learning approach to reactive control policy design for autonomous robots","A. H. Fagg; D. Lotspeich; G. A. Bekey","Center for Neural Engineering, University of Southern California, Los Angeles, CA, USA; Center for Neural Engineering, University of Southern California, Los Angeles, CA, USA; Center for Neural Engineering, University of Southern California, Los Angeles, CA, USA","Proceedings of the 1994 IEEE International Conference on Robotics and Automation","6 Aug 2002","1994","","","39","44 vol.1","Within the field of robotics, much recent attention has been given to control techniques that have been termed reactive or behavior-based. The design of such control systems for even a remotely interesting task is typically a laborious effort, requiring many hours of experimental ""tweaking"" as the actual behavior of the system is observed by the system designer. In this paper, the authors present a neural-based reinforcement learning approach to the design of reactive control policies in which the designer specifies the the desired behavior of the system, rather than the control program that produces the desired behavior.<>","","0-8186-5330-2","10.1109/ROBOT.1994.351013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=351013","","Control systems;Intelligent robots;Robot sensing systems;Service robots;Robot control;Intelligent systems;Computational and artificial intelligence;Optimal control;Application specific integrated circuits;Data mining","unsupervised learning;intelligent control;neural net architecture","reactive control policy design;autonomous robots;neural-based reinforcement learning","","16","","23","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"ReLMoGen: Integrating Motion Generation in Reinforcement Learning for Mobile Manipulation","F. Xia; C. Li; R. Martín-Martín; O. Litany; A. Toshev; S. Savarese",Stanford University; Stanford University; Stanford University; Nvidia; Robotics at Google; Stanford University,"2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","4583","4590","Many Reinforcement Learning (RL) approaches use joint control signals (positions, velocities, torques) as action space for continuous control tasks. We propose to lift the action space to a higher level in the form of subgoals for a motion generator (a combination of motion planner and trajectory executor). We argue that, by lifting the action space and by leveraging sampling-based motion planners, we can efficiently use RL to solve complex, long-horizon tasks that could not be solved with existing RL methods in the original action space. We propose ReLMoGen – a framework that combines a learned policy to predict subgoals and a motion generator to plan and execute the motion needed to reach these subgoals. To validate our method, we apply ReLMoGen to two types of tasks: 1) Interactive Navigation tasks, navigation problems where interactions with the environment are required to reach the destination, and 2) Mobile Manipulation tasks, manipulation tasks that require moving the robot base. These problems are challenging because they are usually long-horizon, hard to explore during training, and comprise alternating phases of navigation and interaction. Our method is benchmarked on a diverse set of seven robotics tasks in photo-realistic simulation environments. In all settings, ReLMoGen outperforms state-of-the-art RL and Hierarchical RL baselines. ReLMoGen also shows outstanding transferability between different motion generators at test time, indicating a great potential to transfer to real robots. For more information, please visit project website: http://svl.stanford.edu/projects/relmogen.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561315","","Training;Navigation;Conferences;Reinforcement learning;Aerospace electronics;Benchmark testing;Generators","manipulators;mobile robots;motion control;path planning;position control;reinforcement learning","motion generation;reinforcement learning;mobile manipulation;joint control signals;continuous control tasks;motion generator;sampling-based motion planners;RL methods;original action space;learned policy;navigation problems;manipulation tasks;robotics tasks;motion generators;hierarchical RL baselines;ReLMoGen","","16","","50","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Alextrac: Affinity learning by exploring temporal reinforcement within association chains","A. Bewley; L. Ott; F. Ramos; B. Upcroft","Queensland University of Technology (QUT); School of Information Technologies, The University of Sydney, Australia; School of Information Technologies, The University of Sydney, Australia; Queensland University of Technology (QUT)","2016 IEEE International Conference on Robotics and Automation (ICRA)","9 Jun 2016","2016","","","2212","2218","This paper presents a self-supervised approach for learning to associate object detections in a video sequence as often required in tracking-by-detection systems. In this paper we focus on learning an affinity model to estimate the data association cost, which can adapt to different situations by exploiting the sequential nature of video data. We also propose a framework for gathering additional training samples at test time with high variation in visual appearance, naturally inherent in large temporal windows. Reinforcing the model with these difficult samples greatly improves the affinity model compared to standard similarity measures such as cosine similarity. We experimentally demonstrate the efficacy of the resulting affinity model on several multiple object tracking (MOT) benchmark sequences. Using the affinity model alone places this approach in the top 25 state-of-the-art trackers with an average rank of 21.3 across 11 test sequences and an overall multiple object tracking accuracy (MOTA) of 17%. This is considerable as our simple approach only uses the appearance of the detected regions in contrast to other techniques with global optimisation or complex motion models.","","978-1-4673-8026-3","10.1109/ICRA.2016.7487371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487371","","Visualization;Tracking;Feature extraction;Training;Adaptation models;Data models;Image color analysis","image sequences;learning (artificial intelligence);object detection;object tracking;video signal processing","ALExTRAC;affinity learning;temporal reinforcement;association chains;self-supervised approach;object detections;video sequence;tracking-by-detection systems;affinity model;data association cost estimation;video data sequence;visual appearance;temporal windows;similarity measures;cosine similarity;MOT benchmark;multiple object tracking accuracy;MOTA","","16","","28","IEEE","9 Jun 2016","","","IEEE","IEEE Conferences"
"Vision-Based Mobile Robotics Obstacle Avoidance With Deep Reinforcement Learning","P. Wenzel; T. Schön; L. Leal-Taixé; D. Cremers","Technical University of Munich, Germany; Technische Hochschule Ingolstadt, Ingolstadt, Germany; Technical University of Munich, Germany; Technical University of Munich, Germany","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","14360","14366","Obstacle avoidance is a fundamental and challenging problem for autonomous navigation of mobile robots. In this paper, we consider the problem of obstacle avoidance in simple 3D environments where the robot has to solely rely on a single monocular camera. In particular, we are interested in solving this problem without relying on localization, mapping, or planning techniques. Most of the existing work consider obstacle avoidance as two separate problems, namely obstacle detection, and control. Inspired by the recent advantages of deep reinforcement learning in Atari games and understanding highly complex situations in Go, we tackle the obstacle avoidance problem as a data-driven end-to-end deep learning approach. Our approach takes raw images as input and generates control commands as output. We show that discrete action spaces are outperforming continuous control commands in terms of expected average reward in maze-like environments. Furthermore, we show how to accelerate the learning and increase the robustness of the policy by incorporating predicted depth maps by a generative adversarial network.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560787","","Training;Navigation;Robot vision systems;Reinforcement learning;Gray-scale;Aerospace electronics;Cameras","collision avoidance;learning (artificial intelligence);mobile robots;robot vision","vision-based mobile robotics obstacle avoidance;deep reinforcement learning;fundamental problem;mobile robots;simple 3D environments;separate problems;obstacle detection;understanding highly complex situations;obstacle avoidance problem;data-driven end-to-end deep learning approach","","16","","27","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Reset-free guided policy search: Efficient deep reinforcement learning with stochastic initial states","W. Montgomery; A. Ajay; C. Finn; P. Abbeel; S. Levine","Department of Computer Science and Engineering, University of Washington, Seattle, WA; Department of Electrical Engineering and Computer Sciences, University of California Berkeley, Berkeley, CA; Department of Electrical Engineering and Computer Sciences, University of California Berkeley, Berkeley, CA; Department of Electrical Engineering and Computer Sciences, University of California Berkeley, Berkeley, CA; Department of Electrical Engineering and Computer Sciences, University of California Berkeley, Berkeley, CA","2017 IEEE International Conference on Robotics and Automation (ICRA)","24 Jul 2017","2017","","","3373","3380","Autonomous learning of robotic skills can allow general-purpose robots to learn wide behavioral repertoires without extensive manual engineering. However, robotic skill learning must typically make trade-offs to enable practical real-world learning, such as requiring manually designed policy or value function representations, initialization from human demonstrations, instrumentation of the training environment, or extremely long training times. We propose a new reinforcement learning algorithm that can train general-purpose neural network policies with minimal human engineering, while still allowing for fast, efficient learning in stochastic environments. We build on the guided policy search (GPS) algorithm, which transforms the reinforcement learning problem into supervised learning from a computational teacher (without human demonstrations). In contrast to prior GPS methods, which require a consistent set of initial states to which the system must be reset after each episode, our approach can handle random initial states, allowing it to be used even when deterministic resets are impossible. We compare our method to existing policy search algorithms in simulation, showing that it can train high-dimensional neural network policies with the same sample efficiency as prior GPS methods, and can learn policies directly from image pixels. We also present real-world robot results that show that our method can learn manipulation policies with visual features and random initial states.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989383","","Robots;Global Positioning System;Neural networks;Learning (artificial intelligence);Supervised learning;Training;Optimization","learning (artificial intelligence);neurocontrollers;robots","reset-free guided policy search;deep reinforcement learning;stochastic initial states;autonomous learning;robotic skill learning;general-purpose neural network policies;GPS algorithm;high-dimensional neural network policies;image pixels","","15","","23","IEEE","24 Jul 2017","","","IEEE","IEEE Conferences"
"NavRep: Unsupervised Representations for Reinforcement Learning of Robot Navigation in Dynamic Human Environments","D. Dugas; J. Nieto; R. Siegwart; J. J. Chung","Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","7829","7835","Robot navigation is a task where reinforcement learning approaches are still unable to compete with traditional path planning. State-of-the-art methods differ in small ways, and do not all provide reproducible, openly available implementations. This makes comparing methods a challenge. Recent research has shown that unsupervised learning methods can scale impressively, and be leveraged to solve difficult problems. In this work, we design ways in which unsupervised learning can be used to assist reinforcement learning for robot navigation. We train two end-to-end, and 18 unsupervised-learning-based architectures, and compare them, along with existing approaches, in unseen test cases. We demonstrate our approach working on a real life robot. Our results show that unsupervised learning methods are competitive with end-to-end methods. We also highlight the importance of various components such as input representation, predictive unsupervised learning, and latent features. We make all our models publicly available, as well as training and testing environments, and tools 1. This release also includes OpenAI-gym-compatible environments designed to emulate the training conditions described by other papers, with as much fidelity as possible. Our hope is that this helps in bringing together the field of RL for robot navigation, and allows meaningful comparisons across state-of-the-art methods.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560951","","Training;Navigation;Computational modeling;Reinforcement learning;Tools;Robot sensing systems;Safety","control engineering computing;mobile robots;navigation;path planning;reinforcement learning;unsupervised learning","robot navigation;unsupervised-learning-based architectures;life robot;end-to-end methods;predictive unsupervised learning;unsupervised representations;dynamic human environments;reinforcement learning;path planning;NavRep;latent features;OpenAI-gym-compatible environments","","15","","20","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement learning for balancing a flying inverted pendulum","R. Figueroa; A. Faust; P. Cruz; L. Tapia; R. Fierro","Department of Electrical and Computer Engineering, University of New Mexico, Albuquerque, NM, United States; Department of Computer Science, University of New Mexico, Albuquerque, NM, United States; Department of Electrical and Computer Engineering, University of New Mexico, Albuquerque, NM, United States; Department of Computer Science, University of New Mexico, Albuquerque, NM, United States; Department of Electrical and Computer Engineering, University of New Mexico, Albuquerque, NM, United States","Proceeding of the 11th World Congress on Intelligent Control and Automation","5 Mar 2015","2014","","","1787","1793","The problem of balancing an inverted pendulum on an unmanned aerial vehicle (UAV) has been achieved using linear and nonlinear control approaches. However, to the best of our knowledge, this problem has not been solved using learning methods. On the other hand, the classical inverted pendulum is a common benchmark problem to evaluate learning techniques. In this paper we demonstrate a novel solution to the inverted pendulum problem extended to UAVs, specifically quadrotors. This complex system is underactuated and sensitive to small acceleration changes of the quadrotor. The solution is provided by reinforcement learning (RL), a platform commonly applied to solve nonlinear control problems. We generate a control policy to balance the pendulum using Continuous Action Fitted Value Iteration (CAFVI) [1] which is a RL algorithm for high-dimensional input-spaces. This technique combines learning of both state and state-action value functions in an approximate value iteration setting with continuous inputs. Simulations verify the performance of the generated control policy for varying initial conditions. The results show the control policy is computationally fast enough to be appropriate of real-time control.","","978-1-4799-5825-2","10.1109/WCICA.2014.7052991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052991","Aerial robotics;quadrotor control;inverted pendulum;approximate value iteration;reinforcement learning","Vectors;Control systems;Noise;Learning (artificial intelligence);Mathematical model;Acceleration;Aerospace electronics","aircraft control;approximation theory;autonomous aerial vehicles;helicopters;iterative methods;learning (artificial intelligence);nonlinear control systems;pendulums","reinforcement learning;flying inverted pendulum balancing;unmanned aerial vehicle;UAV;benchmark problem;quadrotor;underactuated complex system;acceleration changes sensitive system;control policy;continuous action fitted value iteration;CAFVI;RL algorithm;high-dimensional input-spaces;state-action value function learning;approximate value iteration;continuous inputs;varying initial conditions;real-time control","","15","","14","IEEE","5 Mar 2015","","","IEEE","IEEE Conferences"
"Offline Policy Iteration Based Reinforcement Learning Controller for Online Robotic Knee Prosthesis Parameter Tuning","M. Li; X. Gao; Y. Wen; J. Si; H. H. Huang","NCSU/UNC Department of Biomedical Engineering, NC State University, Raleigh, NC, 27695-7115; University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; NCSU/UNC Department of Biomedical Engineering, NC State University, Raleigh, NC, 27695-7115; University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; NCSU/UNC Department of Biomedical Engineering, NC State University, Raleigh, NC, 27695-7115; University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","2831","2837","This paper aims to develop an optimal controller that can automatically provide personalized control of robotic knee prosthesis in order to best support gait of individual prosthesis wearers. We introduced a new reinforcement learning (RL) controller for this purpose based on the promising ability of RL controllers to solve optimal control problems through interactions with the environment without requiring an explicit system model. However, collecting data from a human-prosthesis system is expensive and thus the design of a RL controller has to take into account data and time efficiency. We therefore propose an offline policy iteration based reinforcement learning approach. Our solution is built on the finite state machine (FSM) impedance control framework, which is the most used prosthesis control method in commercial and prototypic robotic prosthesis. Under such a framework, we designed an approximate policy iteration algorithm to devise impedance parameter update rules for 12 prosthesis control parameters in order to meet individual users' needs. The goal of the reinforcement learning-based control was to reproduce near-normal knee kinematics during gait. We tested the RL controller obtained from offline learning in real time experiment involving the same able-bodied human subject wearing a robotic lower limb prosthesis. Our results showed that the RL control resulted in good convergent behavior in kinematic states, and the offline learning control policy successfully adjusted the prosthesis control parameters to produce near-normal knee kinematics in 10 updates of the impedance control parameters.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794212","","Prosthetics;Knee;Impedance;Kinematics;Reinforcement learning;Legged locomotion","biomechanics;finite state machines;iterative methods;learning systems;medical robotics;optimal control;prosthetics","offline policy iteration;online robotic knee prosthesis parameter;optimal controller;personalized control;optimal control problems;human-prosthesis system;prototypic robotic prosthesis;approximate policy iteration algorithm;reinforcement learning-based control;near-normal knee kinematics;offline learning;robotic lower limb prosthesis;RL control;control policy;impedance control parameters;prosthesis control parameters","","15","","34","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Mapless Navigation of a Hybrid Aerial Underwater Vehicle with Medium Transition","R. B. Grando; J. C. de Jesus; V. A. Kich; A. H. Kolling; N. P. Bortoluzzi; P. M. Pinheiro; A. A. Neto; P. L. J. Drews","NAUTEC, Centro de Ciencias Computacionais, Univ. Fed. do Rio Grande - FURG, RS, Brazil; NAUTEC, Centro de Ciencias Computacionais, Univ. Fed. do Rio Grande - FURG, RS, Brazil; Universidade Federal de Santa Maria - UFSM, RS, Brazil; Universidade Federal de Santa Maria - UFSM, RS, Brazil; NAUTEC, Centro de Ciencias Computacionais, Univ. Fed. do Rio Grande - FURG, RS, Brazil; NAUTEC, Centro de Ciencias Computacionais, Univ. Fed. do Rio Grande - FURG, RS, Brazil; Electronic Engineering Dep, Univ. Fed. de Minas Gerais, MG, Brazil; NAUTEC, Centro de Ciencias Computacionais, Univ. Fed. do Rio Grande - FURG, RS, Brazil","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","1088","1094","Since the application of Deep Q-Learning to the continuous action domain in Atari-like games, Deep Reinforcement Learning (Deep-RL) techniques for motion control have been qualitatively enhanced. Nowadays, modern Deep-RL can be successfully applied to solve a wide range of complex decision-making tasks for many types of vehicles. Based on this context, in this paper, we propose the use of Deep-RL to perform autonomous mapless navigation for Hybrid Unmanned Aerial Underwater Vehicles (HUAUVs), robots that can operate in both, air or water media. We developed two approaches, one deterministic and the other stochastic. Our system uses the relative localization of the vehicle and simple sparse range data to train the network. We compared our approaches with an adapted version of the BUG2 algorithm for mapless navigation of aerial vehicles. Based on experimental results, we can conclude that Deep-RL-based approaches can be successfully used to perform mapless navigation and obstacle avoidance for HUAUVs. Our vehicle accomplished the navigation in two scenarios, being capable to achieve the desired target through both environments, and even outperforming the behavior-based algorithm on the obstacle-avoidance capability.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561188","","Location awareness;Navigation;Reinforcement learning;Games;Media;Robot sensing systems;Sensors","autonomous aerial vehicles;autonomous underwater vehicles;collision avoidance;decision making;deep learning (artificial intelligence);mobile robots;motion control;multi-robot systems;robot dynamics;robot kinematics;robot programming","Atari-like games;motion control;autonomous mapless navigation;HUAUVs;simple sparse range data;behavior-based algorithm;hybrid aerial underwater vehicle;deep Q-learning;continuous action domain;deep-RL-based approach;deep reinforcement learning;BUG2 algorithm;mapless navigation;obstacle avoidance","","14","","37","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"PrefixRL: Optimization of Parallel Prefix Circuits using Deep Reinforcement Learning","R. Roy; J. Raiman; N. Kant; I. Elkin; R. Kirby; M. Siu; S. Oberman; S. Godil; B. Catanzaro","NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA","2021 58th ACM/IEEE Design Automation Conference (DAC)","8 Nov 2021","2021","","","853","858","In this work, we present a reinforcement learning (RL) based approach to designing parallel prefix circuits such as adders or priority encoders that are fundamental to high-performance digital design. Unlike prior methods, our approach designs solutions tabula rasa purely through learning with synthesis in the loop. We design a grid-based state-action representation and an RL environment for constructing legal prefix circuits. Deep Convolutional RL agents trained on this environment produce prefix adder circuits that Pareto-dominate existing baselines with up to 16.0% and 30.2% lower area for the same delay in the 32b and 64b settings respectively. We observe that agents trained with open-source synthesis tools and cell library can design adder circuits that achieve lower area and delay than commercial tool adders in an industrial cell library.","0738-100X","978-1-6654-3274-0","10.1109/DAC18074.2021.9586094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586094","machine learning;reinforcement learning;datapath optimization","Training;Machine learning algorithms;Reinforcement learning;Tools;Libraries;Delays;Task analysis","adders;learning (artificial intelligence);logic design","parallel prefix circuits;deep reinforcement learning;reinforcement learning based approach;priority encoders;high-performance digital design;approach designs solutions tabula rasa;grid-based state-action representation;RL environment;legal prefix circuits;Convolutional RL agents;prefix adder circuits;commercial tool adders","","14","","27","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Model-based reinforcement learning with parametrized physical models and optimism-driven exploration","C. Xie; S. Patil; T. Moldovan; S. Levine; P. Abbeel","Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA","2016 IEEE International Conference on Robotics and Automation (ICRA)","9 Jun 2016","2016","","","504","511","In this paper, we present a robotic model-based reinforcement learning method that combines ideas from model identification and model predictive control. We use a feature-based representation of the dynamics that allows the dynamics model to be fitted with a simple least squares procedure, and the features are identified from a high-level specification of the robot's morphology, consisting of the number and connectivity structure of its links. Model predictive control is then used to choose the actions under an optimistic model of the dynamics, which produces an efficient and goal-directed exploration strategy. We present real time experimental results on standard benchmark problems involving the pendulum, cartpole, and double pendulum systems. Experiments indicate that our method is able to learn a range of benchmark tasks substantially faster than the previous best methods. To evaluate our approach on a realistic robotic control task, we also demonstrate real time control of a simulated 7 degree of freedom arm.","","978-1-4673-8026-3","10.1109/ICRA.2016.7487172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487172","","Robots;Mathematical model;Dynamics;Computational modeling;Real-time systems;Heuristic algorithms;Predictive control","control engineering computing;learning (artificial intelligence);manipulators;nonlinear systems;optimisation;pendulums;predictive control","model-based reinforcement learning;model-based RL;model identification;model predictive control;optimism-driven exploration;feature-based representation;robot morphology;connectivity structure;cartpole system;double pendulum system;robotic control task;7 degree of freedom arm","","14","","32","IEEE","9 Jun 2016","","","IEEE","IEEE Conferences"
"Motion capture and reinforcement learning of dynamically stable humanoid movement primitives","R. Vuga; M. Ogrinc; A. Gams; T. Petrič; N. Sugimoto; A. Ude; J. Morimoto","IES, UMR CNRS 5214, University of Montpellier II, Montpellier, France; IES, UMR CNRS 5214, University of Montpellier II, Montpellier, France; IES, UMR CNRS 5214, University of Montpellier II, Montpellier, France; Semiconductor Physics Institute, Center for Physical Sciences and Technology, Vilnius, Lithuania; Semiconductor Physics Institute, Center for Physical Sciences and Technology, Vilnius, Lithuania; Semiconductor Physics Institute, Center for Physical Sciences and Technology, Vilnius, Lithuania; Dept. of Brain Robot Interface, ATR Computational Neuroscience Laboratories, Kyoto, Japan","2013 IEEE International Conference on Robotics and Automation","17 Oct 2013","2013","","","5284","5290","Direct transfer of human motion trajectories to humanoid robots does not result in dynamically stable robot movements due to the differences in human and humanoid robot kinematics and dynamics. We developed a system that converts human movements captured by a low-cost RGB-D camera into dynamically stable humanoid movements. The transfer of human movements occurs in real-time. As need arises, the developed system can smoothly transition between unconstrained movement imitation and imitation with balance control, where movement reproduction occurs in the null space of the balance controller. The developed balance controller is based on an approximate model of the robot dynamics, which is sufficient to stabilize the robot during on-line imitation. However, the resulting movements cannot be guaranteed to be optimal because the model of the robot dynamics is not exact. The initially acquired movement is therefore subsequently improved by model-free reinforcement learning, both with respect to the accuracy of reproduction and balance control. We present experimental results in simulation and on a real humanoid robot.","1050-4729","978-1-4673-5643-5","10.1109/ICRA.2013.6631333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6631333","","Humanoid robots;Robot kinematics;Dynamics;Learning (artificial intelligence);Trajectory;Null space","cameras;humanoid robots;learning (artificial intelligence);mechanical stability;mechanical variables control;motion control;robot dynamics;robot kinematics","motion capture;dynamically stable humanoid movement primitives;human motion trajectory transfer;humanoid robot kinematics;humanoid robot dynamics;low-cost RGB-D camera;red-green-blue-depth camera;balance control;movement reproduction;robot stability;model-free reinforcement learning","","14","","24","IEEE","17 Oct 2013","","","IEEE","IEEE Conferences"
"Decision Making for Autonomous Driving via Augmented Adversarial Inverse Reinforcement Learning","P. Wang; D. Liu; J. Chen; H. Li; C. -Y. Chan","University of California, Berkeley; Zenseact, Chalmers University of Technology; Peking University; Google Research; University of California, Berkeley","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","1036","1042","Making decisions in complex driving environments is a challenging task for autonomous agents. Imitation learning methods have great potentials for achieving such a goal. Adversarial Inverse Reinforcement Learning (AIRL) is one of the state-of-art imitation learning methods that can learn both a behavioral policy and a reward function simultaneously, yet it is only demonstrated in simple and static environments where no interactions are introduced. In this paper, we improve and stabilize AIRL’s performance by augmenting it with semantic rewards in the learning framework. Additionally, we adapt the augmented AIRL to a more practical and challenging decision-making task in a highly interactive environment in autonomous driving. The proposed method is compared with four baselines and evaluated by four performance metrics. Simulation results show that the augmented AIRL outperforms all the baseline methods, and its performance is comparable with that of the experts on all of the four metrics.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560907","Inverse Reinforcement Learning;Decision Making;Lane Change;Autonomous Driving","Measurement;Learning systems;Simulation;Conferences;Decision making;Semantics;Reinforcement learning","decision making;reinforcement learning;traffic engineering computing","decision making;autonomous driving;complex driving environments;autonomous agents;behavioral policy;reward function;semantic rewards;augmented AIRL;interactive environment;augmented adversarial inverse reinforcement learning","","14","","24","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Bimanual Regrasping for Suture Needles using Reinforcement Learning for Rapid Motion Planning","Z. -Y. Chiu; F. Richter; E. K. Funk; R. K. Orosco; M. C. Yip","Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, CA, USA; Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, CA, USA; Department of Surgery - Division of Head and Neck Surgery, University of California San Diego, La Jolla, CA, USA; Department of Surgery - Division of Head and Neck Surgery, University of California San Diego, La Jolla, CA, USA; Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, CA, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","7737","7743","Regrasping a suture needle is an important yet time-consuming process in suturing. To bring efficiency into regrasping, prior work either designs a task-specific mechanism or guides the gripper toward some specific pick-up point for proper grasping of a needle. Yet, these methods are usually not deployable when the working space is changed. Therefore, in this work, we present rapid trajectory generation for bimanual needle regrasping via reinforcement learning (RL). Demonstrations from a sampling-based motion planning algorithm is incorporated to speed up the learning. In addition, we propose the ego-centric state and action spaces for this bimanual planning problem, where the reference frames are on the end-effectors instead of some fixed frame. Thus, the learned policy can be directly applied to any feasible robot configuration. Our experiments in simulation show that the success rate of a single pass is 97%, and the planning time is 0.0212s on average, which outperforms other widely used motion planning algorithms. For the real-world experiments, the success rate is 73.3% if the needle pose is reconstructed from an RGB image, with a planning time of 0.0846s and a run time of 5.1454s. If the needle pose is known beforehand, the success rate becomes 90.5%, with a planning time of 0.0807s and a run time of 2.8801s.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561673","Telemedicine and Advanced Technology Research Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561673","","Reinforcement learning;Grasping;Needles;End effectors;Planning;Trajectory;Task analysis","collision avoidance;dexterous manipulators;end effectors;grippers;image colour analysis;medical robotics;mobile robots;path planning;reinforcement learning;robot vision;surgery;trajectory control","needle pose;planning time;bimanual regrasping;suture needle;reinforcement learning;rapid motion planning;time-consuming process;suturing;task-specific mechanism;specific pick-up point;proper grasping;working space;rapid trajectory generation;bimanual needle regrasping;sampling-based motion planning algorithm;ego-centric state;bimanual planning problem;reference frames;fixed frame;learned policy;motion planning algorithms","","14","","31","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Tendon-driven control of biomechanical and robotic systems: A path integral reinforcement learning approach","E. Rombokas; E. Theodorou; M. Malhotra; E. Todorov; Y. Matsuoka","Computer Science & Engineering, University of Washington, Seattle, USA; Department of Computer Science and Engineering, University of Washington, Seattle, USA; Computer Science & Engineering, University of Washington, Seattle, USA; Computer Science & Engineering, University of Washington, Seattle, USA; Computer Science & Engineering, University of Washington, Seattle, USA","2012 IEEE International Conference on Robotics and Automation","28 Jun 2012","2012","","","208","214","We apply path integral reinforcement learning to a biomechanically accurate dynamics model of the index finger and then to the Anatomically Correct Testbed (ACT) robotic hand. We illustrate the applicability of Policy Improvement with Path Integrals (PI2) to parameterized and non-parameterized control policies. This method is based on sampling variations in control, executing them in the real world, and minimizing a cost function on the resulting performance. Iteratively improving the control policy based on real-world performance requires no direct modeling of tendon network nonlinearities and contact transitions, allowing improved task performance.","1050-4729","978-1-4673-1405-3","10.1109/ICRA.2012.6224650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224650","","Biology;Biomechanics;Robots;Switches","biomechanics;dexterous manipulators;iterative methods;learning (artificial intelligence);multi-robot systems;nonlinear control systems;sampling methods","tendon driven control;biomechanical model;robotic system;path integral reinforcement learning;index finger;anatomically correct testbed;ACT robotic hand;policy improvement;parameterized control policy;nonparameterised control policy;sampling variation;cost minimizing function;iterative method;tendon network nonlinearity modeling","","13","","13","IEEE","28 Jun 2012","","","IEEE","IEEE Conferences"
"Using reinforcement learning to improve exploration trajectories for error minimization","T. Kollar; N. Roy","Computer Science and AI Laboratory, MIT, Cambridge, MA, USA; Computer Science and AI Laboratory, MIT, Cambridge, MA, USA","Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.","26 Jun 2006","2006","","","3338","3343","The mapping and localization problems have received considerable attention in robotics recently. The exploration problem that drives mapping has started to generate similar attention, as the ease of construction and quality of map is strongly dependent on the strategy used to acquire sensor data for the map. Most exploration strategies concentrate on selecting the next best measurement to take, trading off information gathering for regular relocalization. What has not been studied so far is the effect the robot controller has on the map quality while executing exploration plans. Certain kinds of robot motion (e.g, sharp turns) are hard to estimate correctly, and increase the likelihood of errors in the mapping process. We show how reinforcement learning can be used to generate good motion control while executing a simple information gathering exploration strategy. We show that the learned policy reduces the overall map uncertainty by reducing the amount of uncertainty generated by robot motion","1050-4729","0-7803-9505-0","10.1109/ROBOT.2006.1642211","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1642211","","Learning;Trajectory;Simultaneous localization and mapping;Robot sensing systems;Current measurement;Gain measurement;Computer errors;Computer science;Artificial intelligence;Robot control","learning (artificial intelligence);mobile robots;motion control;path planning;position control","reinforcement learning;exploration trajectories;error minimization;regular relocalization;robot motion;motion control;mapping quality","","13","","14","IEEE","26 Jun 2006","","","IEEE","IEEE Conferences"
"A proactive link-failure resilient routing protocol for MANETs based on reinforcement learning","G. Oddi; D. Macone; A. Pietrabissa; F. Liberati","University of Roma La Sapienza, Rome, Italy; University of Roma La Sapienza, Rome, Italy; University of Roma La Sapienza, Rome, Italy; University of Roma La Sapienza, Rome, Italy","2012 20th Mediterranean Conference on Control & Automation (MED)","13 Aug 2012","2012","","","1259","1264","Mobile-Ad-Hoc-Networks (MANET) are self-configuring networks of mobile nodes, which communicate through wireless links. One of the main issues in MANETs is the mobility of the network nodes: routing protocols should explicitly consider network changes into the algorithm design. MANETs are particularly suited to guarantee connectivity in disaster relief scenarios, which are often impaired by the absence of network infrastructures. This work proposes a proactive routing protocol, developed via Reinforcement Learning (RL) techniques, to dynamically choose the most stable path, basing on GPS information, among the feasible ones and to consequently increase resiliency to link failures. Simulations show the effectiveness of the proposed protocol, through comparison with the Optimized Link State Routing (OLSR) protocol.","","978-1-4673-2531-8","10.1109/MED.2012.6265812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6265812","","Ad hoc networks;Mobile computing;Global Positioning System;Availability;Routing","learning (artificial intelligence);mobile ad hoc networks;mobility management (mobile radio);radio links;routing protocols;telecommunication computing;telecommunication network reliability","proactive link-failure resilient routing protocol;MANET;reinforcement learning technique;mobile ad hoc networks;self-configuring networks;mobile nodes;wireless links;network node mobility;disaster relief scenario;GPS information;optimized link state routing protocol;OLSR protocol","","13","","14","IEEE","13 Aug 2012","","","IEEE","IEEE Conferences"
"Regularizing Action Policies for Smooth Control with Reinforcement Learning","S. Mysore; B. Mabsout; R. Mancuso; K. Saenko","Department of Computer Science, Boston University, Boston, MA; Department of Computer Science, Boston University, Boston, MA; Department of Computer Science, Boston University, Boston, MA; Department of Computer Science, Boston University, Boston, MA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","1810","1816","A critical problem with the practical utility of controllers trained with deep Reinforcement Learning (RL) is the notable lack of smoothness in the actions learned by the RL policies. This trend often presents itself in the form of control signal oscillation and can result in poor control, high power consumption, and undue system wear. We introduce Conditioning for Action Policy Smoothness (CAPS), an effective yet intuitive regularization on action policies, which offers consistent improvement in the smoothness of the learned state-to-action mappings of neural network controllers, reflected in the elimination of high-frequency components in the control signal. Tested on a real system, improvements in controller smoothness on a quadrotor drone resulted in an almost 80% reduction in power consumption while consistently training flight-worthy controllers. Project website: http://ai.bu.edu/caps","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561138","","Training;Heating systems;Power demand;Neural networks;Reinforcement learning;System improvement;Market research","aircraft control;autonomous aerial vehicles;control engineering computing;deep learning (artificial intelligence);helicopters;neurocontrollers;oscillations;reinforcement learning","quadrotor drone;CAPS;Conditioning for Action Policy Smoothness;deep reinforcement learning;neural network controllers;state-to-action mappings;control signal oscillation;RL policies;smooth control;action policies","","13","","37","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction","M. Okada; T. Taniguchi","Digitan & AI Technology Center, Technology Division, Panasonic Corporation, Japan; Digitan & AI Technology Center, Technology Division, Panasonic Corporation, Japan","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","4209","4215","In the present paper, we propose a decoder-free extension of Dreamer, a leading model-based reinforcement learning (MBRL) method from pixels. Dreamer is a sample- and cost-efficient solution to robot learning, as it is used to train latent state-space models based on a variational autoencoder and to conduct policy optimization by latent trajectory imagination. However, this autoencoding based approach often causes object vanishing, in which the autoencoder fails to perceives key objects for solving control tasks, and thus significantly limiting Dreamer's potential. This work aims to relieve this Dreamer's bottleneck and enhance its performance by means of removing the decoder. For this purpose, we firstly derive a likelihood- free and InfoMax objective of contrastive learning from the evidence lower bound of Dreamer. Secondly, we incorporate two components, (i) independent linear dynamics and (ii) the random crop data augmentation, to the learning scheme so as to improve the training performance. In comparison to Dreamer and other recent model-free reinforcement learning methods, our newly devised Dreamer with InfoMax and without generative decoder (Dreaming) achieves the best scores on 5 difficult simulated robotics tasks, in which Dreamer suffers from object vanishing.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560734","","Training;Service robots;Crops;Reinforcement learning;Robot learning;Decoding;Trajectory","image segmentation;neural nets;reinforcement learning;robot programming","decoder-free extension;robot learning;latent state-space models;variational autoencoder;latent trajectory imagination;object vanishing;Dreamer;model free reinforcement learning method;InfoMax;Dreaming;image region","","13","","37","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning in Topology-based Representation for Human Body Movement with Whole Arm Manipulation","W. Yuan; K. Hang; H. Song; D. Kragic; M. Y. Wang; J. A. Stork","Department of Electronic and Computer Engineering, Department of Mechanical and Aerospace Engineering, Hong Kong University of Science and Technology; Department of Mechanical Engineering and Material Science, Yale University, New Haven, Connecticut, USA; Department of Electronic and Computer Engineering, Department of Mechanical and Aerospace Engineering, Hong Kong University of Science and Technology; Centre for Autonomous Systems, EECS, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Electronic and Computer Engineering, Department of Mechanical and Aerospace Engineering, Hong Kong University of Science and Technology; Center for Applied Autonomous Sensor Systems, Örebro University, Örebro, Sweden","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","2153","2160","Moving a human body or a large and bulky object may require the strength of whole arm manipulation (WAM). This type of manipulation places the load on the robot's arms and relies on global properties of the interaction to succeed- rather than local contacts such as grasping or non-prehensile pushing. In this paper, we learn to generate motions that enable WAM for holding and transporting of humans in certain rescue or patient care scenarios. We model the task as a reinforcement learning problem in order to provide a robot behavior that can directly respond to external perturbation and human motion. For this, we represent global properties of the robot-human interaction with topology-based coordinates that are computed from arm and torso positions. These coordinates also allow transferring the learned policy to other body shapes and sizes. For training and evaluation, we simulate a dynamic sea rescue scenario and show in quantitative experiments that the policy can solve unseen scenarios with differently-shaped humans, floating humans, or with perception noise. Our qualitative experiments show the subsequent transporting after holding is achieved and we demonstrate that the policy can be directly transferred to a real world setting.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794160","","Robot kinematics;Humanoid robots;Manipulators;Laplace equations;Torso;Shape","humanoid robots;human-robot interaction;learning (artificial intelligence);manipulator dynamics;medical robotics;path planning;patient care;position control","topology-based representation;human body movement;bulky object;WAM;manipulation places;global properties;local contacts;grasping;reinforcement learning problem;robot behavior;human motion;robot-human interaction;topology-based coordinates;torso positions;learned policy;body shapes;dynamic sea rescue scenario;unseen scenarios;differently-shaped humans;whole arm manipulation","","13","","38","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"GA3C Reinforcement Learning for Surgical Steerable Catheter Path Planning","A. Segato; L. Sestini; A. Castellano; E. De Momi","Department of Electronics, Politecnico di Milano, Milano, Italy; Department of Electronics, Politecnico di Milano, Milano, Italy; Neuroradiology Unit and CERMAC, Vita-Salute San Raffaele University and IRCCS Ospedale San Raffaele, Milan, Italy; Department of Electronics, Politecnico di Milano, Milano, Italy","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","2429","2435","Path planning algorithms for steerable catheters, must guarantee anatomical obstacles avoidance, reduce the insertion length and ensure the compliance with needle kinematics. The majority of the solutions in literature focuses on graph based or sampling based methods, both limited by the impossibility to directly obtain smooth trajectories. In this work we formulate the path planning problem as a reinforcement learning problem and show that the trajectory planning model, generated from the training, can provide the user with optimal trajectories in terms of obstacle clearance and kinematic constraints. We obtain 2D and 3D environments from MRI images processing and we implement a GA3C algorithm to create a path planning model, able to generalize on different patients anatomies. The curvilinear trajectories obtained from the model in 2D and 3D environments are compared to the ones obtained by A* and RRT* algorithms. Our method achieves state-of-the-art performances in terms of obstacle avoidance, trajectory smoothness and computational time proving this algorithm as valid planning method for complex environments.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196954","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196954","","Trajectory;Needles;Three-dimensional displays;Learning (artificial intelligence);Catheters;Kinematics","biomedical MRI;catheters;collision avoidance;learning (artificial intelligence);medical image processing;medical robotics;path planning;robot kinematics","trajectory smoothness;GA3C Reinforcement Learning;surgical steerable catheter path planning;path planning algorithms;steerable catheters;anatomical obstacles avoidance;insertion length;needle kinematics;smooth trajectories;path planning problem;reinforcement learning problem;trajectory planning model;optimal trajectories;obstacle clearance;kinematic constraints;MRI images processing;path planning model;curvilinear trajectories;RRT* algorithms;obstacle avoidance","","12","","34","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"APPLR: Adaptive Planner Parameter Learning from Reinforcement","Z. Xu; G. Dhamankar; A. Nair; X. Xiao; G. Warnell; B. Liu; Z. Wang; P. Stone","Department of Physics, University of Texas at Austin, Austin, Texas; Department of Computer Science, University of Texas at Austin, Austin, Texas; Mathematics, University of Texas at Austin, Austin, Texas; Department of Computer Science, University of Texas at Austin, Austin, Texas; Department of Computer Science, University of Texas at Austin, Austin, Texas; Department of Computer Science, University of Texas at Austin, Austin, Texas; Electrical and Computer Engineering, University of Texas at Austin, Austin, Texas; Department of Computer Science, University of Texas at Austin, Austin, Texas","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","6086","6092","Classical navigation systems typically operate using a fixed set of hand-picked parameters (e.g. maximum speed, sampling rate, inflation radius, etc.) and require heavy expert re-tuning in order to work in new environments. To mitigate this requirement, it has been proposed to learn parameters for different contexts in a new environment using human demonstrations collected via teleoperation. However, learning from human demonstration limits deployment to the training environment, and limits overall performance to that of a potentially-suboptimal demonstrator. In this paper, we introduce APPLR, Adaptive Planner Parameter Learning from Reinforcement, which allows existing navigation systems to adapt to new scenarios by using a parameter selection scheme discovered via reinforcement learning (RL) in a wide variety of simulation environments. We evaluate APPLR on a robot in both simulated and physical experiments, and show that it can outperform both a fixed set of hand-tuned parameters and also a dynamic parameter tuning scheme learned from human demonstration.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561647","","Training;Visualization;Costs;Navigation;Velocity control;Semantics;Switches","learning (artificial intelligence);mobile robots;navigation;path planning;telerobotics","APPLR;Adaptive Planner Parameter Learning;classical navigation systems;fixed set;hand-picked parameters;heavy expert re-tuning;learn parameters;human demonstration;training environment;potentially-suboptimal demonstrator;parameter selection scheme;reinforcement learning;simulation environments;hand-tuned parameters;dynamic parameter tuning scheme","","12","","21","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Avoiding moving obstacles with stochastic hybrid dynamics using PEARL: PrEference Appraisal Reinforcement Learning","A. Faust; H. -T. Chiang; N. Rackley; L. Tapia","Computer Science Department, University of New Mexico, USA; Computer Science Department, University of New Mexico, USA; Computer Science Department, University of New Mexico, USA; Computer Science Department, University of New Mexico, USA","2016 IEEE International Conference on Robotics and Automation (ICRA)","9 Jun 2016","2016","","","484","490","Manual derivation of optimal robot motions for task completion is difficult, especially when a robot is required to balance its actions between opposing preferences. One solution has been proposed to automatically learn near optimal motions with Reinforcement Learning (RL). This has been successful for several tasks including swing-free UAV flight, table tennis, and autonomous driving. However, high-dimensional problems remain a challenge. We address this dimensionality constraint with PrEference Appraisal Reinforcement Learning (PEARL), which solves tasks with opposing preferences for acceleration controlled robots. PEARL projects the high-dimensional continuous robot state space to a low dimensional preference feature space resulting in efficient and adaptable planning. We demonstrate that on a dynamic obstacle avoidance robotic task, a single learning on a much simpler problem performs real-time decision-making for significantly larger, high-dimensional problems working in unbounded continuous states and actions. We trained the agent with 4 static obstacles, while the trained agent avoids up to 900 moving obstacles with complex hybrid stochastic obstacle dynamics in a highly constrained space using only limited information about the environment. We compare these tasks to traditional, often manually tuned solutions for these high-dimensional problems.","","978-1-4673-8026-3","10.1109/ICRA.2016.7487169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487169","","Dynamics;Planning;Robot kinematics;Collision avoidance;Aerospace electronics;Acceleration","acceleration control;collision avoidance;control engineering computing;learning (artificial intelligence);motion control;optimal control;robot dynamics;stochastic systems","complex hybrid stochastic obstacle dynamics;static obstacle;dynamic obstacle avoidance robotic task;high-dimensional continuous robot state space;acceleration control;dimensionality constraint;optimal robot motion;preference appraisal reinforcement learning;PEARL;stochastic hybrid dynamics","","12","","28","IEEE","9 Jun 2016","","","IEEE","IEEE Conferences"
"Model Predictive Actor-Critic: Accelerating Robot Skill Acquisition with Deep Reinforcement Learning","A. S. Morgan; D. Nandha; G. Chalvatzaki; C. D’Eramo; A. M. Dollar; J. Peters","Department of Mechanical Engineering & Materials Science, Yale University, USA; Intelligent Autonomous Systems, Technische Universität, Darmstadt, Germany; Intelligent Autonomous Systems, Technische Universität, Darmstadt, Germany; Intelligent Autonomous Systems, Technische Universität, Darmstadt, Germany; Department of Mechanical Engineering & Materials Science, Yale University, USA; Intelligent Autonomous Systems, Technische Universität, Darmstadt, Germany","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","6672","6678","Substantial advancements to model-based reinforcement learning algorithms have been impeded by the model-bias induced by the collected data, which generally hurts performance. Meanwhile, their inherent sample efficiency warrants utility for most robot applications, limiting potential damage to the robot and its environment during training. Inspired by information theoretic model predictive control and advances in deep reinforcement learning, we introduce Model Predictive Actor-Critic (MoPAC)†, a hybrid model-based/model-free method that combines model predictive rollouts with policy optimization as to mitigate model bias. MoPAC leverages optimal trajectories to guide policy learning, but explores via its model-free method, allowing the algorithm to learn more expressive dynamics models. This combination guarantees optimal skill learning up to an approximation error and reduces necessary physical interaction with the environment, making it suitable for real-robot training. We provide extensive results showcasing how our proposed method generally outperforms current state-of-the-art and conclude by evaluating MoPAC for learning on a physical robotic hand performing valve rotation and finger gaiting–a task that requires grasping, manipulation, and then regrasping of an object.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561298","","Training;Heuristic algorithms;Reinforcement learning;Predictive models;Prediction algorithms;Valves;Data models","control engineering computing;deep learning (artificial intelligence);dexterous manipulators;predictive control;reinforcement learning;robot programming","policy learning;real-robot training;finger gaiting;robot skill acquisition;deep reinforcement learning;information theoretic model predictive control;MoPAC;optimal trajectories;model predictive actor-critic","","11","","45","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"An Application of Continuous Deep Reinforcement Learning Approach to Pursuit-Evasion Differential Game","M. Wang; L. Wang; T. Yue","School of Aeronautic Science and Engineering, Beihang University, Beijing, China; School of Aeronautic Science and Engineering, Beihang University, Beijing, China; School of Aeronautic Science and Engineering, Beihang University, Beijing, China","2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)","6 Jun 2019","2019","","","1150","1156","Pursuit-evasion differential game is a classic decision-making process in continuous domain. Most recently, the reinforcement learning (RL) technique has greatly advanced the research in decision-making field. In this paper, the dynamic model of the game is described and the optimization problem of the purser in the game is addressed. To learn the control strategy with self-learning, reinforcement learning is considered. An actor-critic based, model-free, end-to-end approach Deep Deterministic Policy Gradient (DDPG) Algorithm is applied to train the pursuer. In the first training phase the pursuer is trained only with a given evader's control strategy. In the second training phase, the pursuer and evader are trained simultaneously without any expert knowledge given in advance. The result shows that the pursuer and the evader can learn the control strategy during the training phase.","","978-1-5386-6243-4","10.1109/ITNEC.2019.8729310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8729310","Differential Game;Reinforcement Learning;DDPG;Self-Learning","Games;Reinforcement learning;Mathematical model;Approximation algorithms;Decision making;Training;Vehicle dynamics","decision making;differential games;learning (artificial intelligence)","pursuit-evasion differential game;decision-making process;deep deterministic policy gradient algorithm;continuous deep reinforcement learning approach","","11","","18","IEEE","6 Jun 2019","","","IEEE","IEEE Conferences"
"Distributional Deep Reinforcement Learning with a Mixture of Gaussians","Y. Choi; K. Lee; S. Oh","Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, Korea; Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, Korea; Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, Korea","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","9791","9797","In this paper, we propose a novel distributional reinforcement learning (RL) method which models the distribution of the sum of rewards using a mixture density network. Recently, it has been shown that modeling the randomness of the return distribution leads to better performance in Atari games and control tasks. Despite the success of the prior work, it has limitations which come from the use of a discrete distribution. First, it needs a projection step and softmax parametrization for the distribution, since it minimizes the KL divergence loss. Secondly, its performance depends on discretization hyperparameters such as the number of atoms and bounds of the support which require domain knowledge. We mitigate these problems with the proposed parameterization, a mixture of Gaussians. Furthermore, we propose a new distance metric called the Jensen-Tsallis distance, which allows the computation of the distance between two mixtures of Gaussians in a closed form. We have conducted various experiments to validate the proposed method, including Atari games and autonomous vehicle driving.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793505","","Measurement;Reinforcement learning;Games;Approximation algorithms;Neural networks;Autonomous vehicles;Stochastic processes","Gaussian processes;learning (artificial intelligence);statistical distributions","discrete distribution;softmax parametrization;KL divergence loss;discretization hyperparameters;Atari games;distributional deep reinforcement learning;mixture density network;return distribution;mixtures of Gaussians;Jensen-Tsallis distance;autonomous vehicle driving","","11","","31","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"EmoRL: Continuous Acoustic Emotion Classification Using Deep Reinforcement Learning","E. Lakomkin; M. A. Zamani; C. Weber; S. Magg; S. Wermter","Department of Informatics, Knowledge Technology Institute, Hamburg, Germany; Department of Informatics, Knowledge Technology Institute, Hamburg, Germany; Department of Informatics, Knowledge Technology Institute, Hamburg, Germany; Department of Informatics, Knowledge Technology Institute, Hamburg, Germany; Department of Informatics, Knowledge Technology Institute, Hamburg, Germany","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","4445","4450","Acoustically expressed emotions can make communication with a robot more efficient. Detecting emotions like anger could provide a clue for the robot indicating unsafe/undesired situations. Recently, several deep neural network-based models have been proposed which establish new state-of-the-art results in affective state evaluation. These models typically start processing at the end of each utterance, which not only requires a mechanism to detect the end of an utterance but also makes it difficult to use them in a real-time communication scenario, e.g. human-robot interaction. We propose the EmoRL model that triggers an emotion classification as soon as it gains enough confidence while listening to a person speaking. As a result, we minimize the need for segmenting the audio signal for classification and achieve lower latency as the audio signal is processed incrementally. The method is competitive with the accuracy of a strong baseline model, while allowing much earlier prediction.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8461058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461058","","Acoustics;Robots;Adaptation models;Logic gates;Feature extraction;Predictive models;Recurrent neural networks","acoustic signal processing;emotion recognition;human-robot interaction;learning (artificial intelligence);neural nets","EmoRL model;audio signal;continuous acoustic emotion classification;deep reinforcement learning;acoustically expressed emotions;deep neural network-based models;affective state evaluation;real-time communication scenario;human-robot interaction","","11","","25","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"BaRC: Backward Reachability Curriculum for Robotic Reinforcement Learning","B. Ivanovic; J. Harrison; A. Sharma; M. Chen; M. Pavone","Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA; Department of Mechanical Engineering, Stanford University, Stanford, CA, USA; Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA; School of Computing Science, Simon Fraser University, Burnaby, BC, Canada; Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","15","21","Model-free Reinforcement Learning (RL) offers an attractive approach to learn control policies for high dimensional systems, but its relatively poor sample complexity often necessitates training in simulated environments. Even in simulation, goal-directed tasks whose natural reward function is sparse remain intractable for state-of-the-art model-free algorithms for continuous control. The bottleneck in these tasks is the prohibitive amount of exploration required to obtain a learning signal from the initial state of the system. In this work, we leverage physical priors in the form of an approximate system dynamics model to design a curriculum for a model-free policy optimization algorithm. Our Backward Reachability Curriculum (BaRC) begins policy training from states that require a small number of actions to accomplish the task, and expands the initial state distribution backwards in a dynamically-consistent manner once the policy optimization algorithm demonstrates sufficient performance. BaRC is general, in that it can accelerate training of any model-free RL algorithm on a broad class of goal-directed continuous control MDPs. Its curriculum strategy is physically intuitive, easy-to-tune, and allows incorporating physical priors to accelerate training without hindering the performance, flexibility, and applicability of the model-free RL algorithm. We evaluate our approach on two representative dynamic robotic learning problems and find substantial performance improvement relative to previous curriculum generation techniques and naive exploration strategies.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794206","","Robots;Task analysis;Training;Computational modeling;Heuristic algorithms;Complexity theory;Approximation algorithms","learning (artificial intelligence);Markov processes;optimisation;path planning;robots","BaRC;initial state distribution backwards;model-free RL algorithm;goal-directed continuous control MDPs;curriculum strategy;representative dynamic robotic learning problems;goal-directed tasks;learning signal;model-free policy optimization algorithm;backward reachability curriculum;curriculum generation techniques;robotic reinforcement learning;model-free reinforcement learning;model-free algorithms;reward function;exploration strategies","","11","","43","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Cross-Domain Transfer in Reinforcement Learning Using Target Apprentice","G. Joshi; G. Chowdhary","Department of Agriculture and Biological Engineering and Coordinated Science Lab, University of Illinois, Urbana-Champaign, IL, USA; Department of Agriculture and Biological Engineering and Coordinated Science Lab, University of Illinois, Urbana-Champaign, IL, USA","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","7525","7532","In this paper, we present a new approach to transfer in Reinforcement Learning (RL) for cross-domain tasks. Unlike, available transfer approaches, where target task learning is accelerated through initialized learning from source, we propose to adapt and reuse the optimal source policy directly in the related domains. We show the optimal policy from a related source task can be near optimal in target domain provided an adaptive policy accounts for the model error between target and the projected source. A significant advantage of the proposed policy augmentation is in generalizing the policies across related domains without having to re-Iearn the new tasks. We demonstrate that, this architecture leads to better sample efficiency in the transfer, reducing sample complexity of target task learning to target apprentice learning.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8462977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462977","","Task analysis;Adaptation models;Automobiles;Manifolds;Bicycles;Learning (artificial intelligence);Complexity theory","generalisation (artificial intelligence);learning (artificial intelligence)","target apprentice learning;cross-domain transfer;reinforcement learning;cross-domain tasks;target task learning;target domain;policy augmentation","","11","","23","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach for Control of a Nature-Inspired Aerial Vehicle","D. Sufiyan; L. T. S. Win; S. K. H. Win; G. S. Soh; S. Foong","Engineering Product Development Pillar, Singapore University of Technology & Design (SUTD), Singapore; Engineering Product Development Pillar, Singapore University of Technology & Design (SUTD), Singapore; Engineering Product Development Pillar, Singapore University of Technology & Design (SUTD), Singapore; Engineering Product Development Pillar, Singapore University of Technology & Design (SUTD), Singapore; Engineering Product Development Pillar, Singapore University of Technology & Design (SUTD), Singapore","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","6030","6036","In this work, reinforcement learning is used to develop a position controller for an underactuated nature-inspired Unmanned Aerial Vehicle (UAV). This particular configuration of UAVs achieves lift by spinning its entire body contrary to standard multi-rotors or fixed-wing aircraft. Deep Deterministic Policy Gradients (DDPG) with Ape-X Distributed Prioritized Experience Replay was used to train neural network function approximators that were implemented as the final control policy. The reinforcement learning agent was trained in simulations and directly ported over to real-life hardware. Position control tests were performed on the learned control policy and compared to a baseline PID controller. The learned controller was found to exhibit better control over the inherent oscillations that arise from the non-linear dynamics of the platform.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794446","","Training;Aerodynamics;Neural networks;Mathematical model;Reinforcement learning;Drag;Prototypes","autonomous aerial vehicles;function approximation;gradient methods;learning (artificial intelligence);neural nets;position control;three-term control","nature-inspired aerial vehicle;position controller;UAV;fixed-wing aircraft;neural network function approximators;reinforcement learning agent;learned controller;deep deterministic policy gradients;PID controller;Ape-X distributed prioritized experience replay;multi-rotors;underactuated nature-inspired unmanned aerial vehicle;body contrary","","10","","18","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"MAMUT: Multi-Agent Reinforcement Learning for Efficient Real-Time Multi-User Video Transcoding","L. Costero; A. Iranfar; M. Zapater; F. D. Igual; K. Olcoz; D. Atienza","Departamento de Arquitectura de Computadores y Automática, Universidad Complutense de Madrid, Spain; Embedded Systems Laboratory (ESL), Swiss Federal Institute of Technology Lausanne (EPFL), Switzerland; Embedded Systems Laboratory (ESL), Swiss Federal Institute of Technology Lausanne (EPFL), Switzerland; Departamento de Arquitectura de Computadores y Automática, Universidad Complutense de Madrid, Spain; Departamento de Arquitectura de Computadores y Automática, Universidad Complutense de Madrid, Spain; Embedded Systems Laboratory (ESL), Swiss Federal Institute of Technology Lausanne (EPFL), Switzerland","2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)","16 May 2019","2019","","","558","563","Real-time video transcoding has recently raised as a valid alternative to address the ever-increasing demand for video contents in servers' infrastructures in current multi-user environments. High Efficiency Video Coding (HEVC) makes efficient online transcoding feasible as it enhances user experience by providing the adequate video configuration, reduces pressure on the network, and minimizes inefficient and costly video storage. However, the computational complexity of HEVC, together with its myriad of configuration parameters, raises challenges for power management, throughput control, and Quality of Service (QoS) satisfaction. This is particularly challenging in multi-user environments where multiple users with different resolution demands and bandwidth constraints need to be served simultaneously. In this work, we present MAMUT, a multi-agent machine learning approach to tackle these challenges. Our proposal breaks the design space composed of run-time adaptation of the transcoder and system parameters into smaller sub-spaces that can be explored in a reasonable time by individual agents. While working cooperatively, each agent is in charge of learning and applying the optimal values for internal HEVC and system-wide parameters. In particular, MAMUT dynamically tunes Quantization Parameter, selects number of threads per video, and sets the operating frequency with throughput and video quality objectives under compression and power consumption constraints. We implement MAMUT on an enterprise multicore server and compare equivalent scenarios to state-of-the-art alternative approaches. The obtained results reveal that MAMUT consistently attains up to 8× improvement in terms of FPS violations (and thus Quality of Service), 24% power reduction, as well as faster and more accurate adaptation both to the video contents and available resources.","1558-1101","978-3-9819263-2-3","10.23919/DATE.2019.8715256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715256","","Streaming media;Real-time systems;Throughput;Transcoding;Bit rate;Quality of service","computational complexity;learning (artificial intelligence);multi-agent systems;optimisation;quality of service;transcoding;video coding","individual agents;internal HEVC;system-wide parameters;MAMUT;Quantization Parameter;video quality objectives;power consumption constraints;video contents;multiagent reinforcement;multiuser video transcoding;real-time video transcoding;current multiuser environments;High Efficiency Video Coding;user experience;adequate video configuration;configuration parameters;multiple users;multiagent machine;run-time adaptation;transcoder;system parameters;video storage;resolution demands;resolution demands;Quality of Service satisfaction;Quality of Service satisfaction","","10","","19","","16 May 2019","","","IEEE","IEEE Conferences"
"Augmenting Reinforcement Learning with Behavior Primitives for Diverse Manipulation Tasks","S. Nasiriany; H. Liu; Y. Zhu",The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin,"2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","7477","7484","Realistic manipulation tasks require a robot to interact with an environment with a prolonged sequence of motor actions. While deep reinforcement learning methods have recently emerged as a promising paradigm for automating manipulation behaviors, they usually fall short in long-horizon tasks due to the exploration burden. This work introduces Manipulation Primitive-augmented reinforcement Learning (MAPLE), a learning framework that augments standard reinforcement learning algorithms with a pre-defined library of behavior primitives. These behavior primitives are robust functional modules specialized in achieving manipulation goals, such as grasping and pushing. To use these heterogeneous primitives, we develop a hierarchical policy that involves the primitives and instantiates their executions with input parameters. We demonstrate that MAPLE out-performs baseline approaches by a significant margin on a suite of simulated manipulation tasks. We also quantify the compositional structure of the learned behaviors and highlight our method's ability to transfer policies to new task variants and to physical hardware. Videos and code are available at https://ut-austin-rpl.github.io/maple","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812140","NSF(grant numbers:CNS-1955523); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812140","","Scalability;Reinforcement learning;Grasping;Libraries;Hardware;Behavioral sciences;Task analysis","learning (artificial intelligence);manipulators;mobile robots;multi-robot systems","behavior primitives;diverse Manipulation tasks;realistic manipulation tasks;deep reinforcement learning methods;automating manipulation behaviors;long-horizon tasks;Manipulation Primitive-augmented reinforcement Learning;learning framework;standard reinforcement learning algorithms;manipulation goals;heterogeneous primitives;simulated manipulation tasks;learned behaviors;task variants","","10","","71","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Sample and Feedback Efficient Hierarchical Reinforcement Learning from Human Preferences","R. Pinsler; R. Akrour; T. Osa; J. Peters; G. Neumann","Engineering Department, University of Cambridge, Cambridge, UK; Fachbereich Informatik, Technische Universität Darmstadt, Germany; Department of Complexity Science and Engineering, University of Tokyo, Tōkyō, Japan; Fachbereich Informatik, Technische Universität Darmstadt, Germany; Fachbereich Informatik, Technische Universität Darmstadt, Germany","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","596","601","While reinforcement learning has led to promising results in robotics, defining an informative reward function is challenging. Prior work considered including the human in the loop to jointly learn the reward function and the optimal policy. Generating samples from a physical robot and requesting human feedback are both taxing efforts for which efficiency is critical. We propose to learn reward functions from both the robot and the human perspectives to improve on both efficiency metrics. Learning a reward function from the human perspective increases feedback efficiency by assuming that humans rank trajectories according to a low-dimensional outcome space. Learning a reward function from the robot perspective circumvents the need for a dynamics model while retaining the sample efficiency of model-based approaches. We provide an algorithm that incorporates bi-perspective reward learning into a general hierarchical reinforcement learning framework and demonstrate the merits of our approach on a toy task and a simulated robot grasping task.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460907","","Robots;Grasping;Task analysis;Trajectory;Learning (artificial intelligence);Context modeling;Customer relationship management","human-robot interaction;learning (artificial intelligence)","human feedback;reward function;bi-perspective reward learning;simulated robot grasping task;general hierarchical reinforcement learning framework;robot perspective;feedback efficiency;physical robot;informative reward function;human preferences","","10","","33","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"FIXAR: A Fixed-Point Deep Reinforcement Learning Platform with Quantization-Aware Training and Adaptive Parallelism","J. Yang; S. Hong; J. -Y. Kim","School of Electrical Engineering, KAIST; School of Electrical Engineering, KAIST; School of Electrical Engineering, KAIST","2021 58th ACM/IEEE Design Automation Conference (DAC)","8 Nov 2021","2021","","","259","264","Deep reinforcement learning (DRL) is a powerful technology to deal with decision-making problem in various application domains such as robotics and gaming, by allowing an agent to learn its action policy in an environment to maximize a cumulative reward. Unlike supervised models which actively use data quantization, DRL still uses the single-precision floating-point for training accuracy while it suffers from computationally intensive deep neural network (DNN) computations. In this paper, we present a deep reinforcement learning acceleration platform named FIXAR, which employs fixed-point data types and arithmetic units for the first time using a SW/HW co-design approach. We propose a quantization-aware training algorithm in fixed-point, which enables to reduce the data precision by half after a certain amount of training time without losing accuracy. We also design a FPGA accelerator that employs adaptive dataflow and parallelism to handle both inference and training operations. Its processing element has configurable datapath to efficiently support the proposed quantized-aware training. We validate our FIXAR platform, where the host CPU emulates the DRL environment and the FPGA accelerates the agent’s DNN operations, by running multiple benchmarks in continuous action spaces based on a latest DRL algorithm called DDPG. Finally, the FIXAR platform achieves 25293.3 inferences per second (IPS) training throughput, which is 2.7 times higher than the CPU-GPU platform. In addition, its FPGA accelerator shows 53826.8 IPS and 2638.0 IPS/W energy efficiency, which are 5.5 times higher and 15.4 times more energy efficient than those of GPU, respectively. FIXAR also shows the best IPS throughput and energy efficiency among other state-of-the-art acceleration platforms using FPGA, even it targets one of the most complex DNN models.","0738-100X","978-1-6654-3274-0","10.1109/DAC18074.2021.9586213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586213","Reinforcement Learning;Accelerator;Platform;Quantization;Deep Neural Network;FPGA","Training;Reinforcement learning;Parallel processing;Benchmark testing;Throughput;Energy efficiency;Inference algorithms","deep learning (artificial intelligence);field programmable gate arrays;floating point arithmetic;graphics processing units;hardware accelerators;logic design","fixed-point deep reinforcement learning platform;adaptive parallelism;decision-making problem;application domains;action policy;data quantization;single-precision floating-point;training accuracy;computationally intensive deep neural network computations;fixed-point data types;quantization-aware training algorithm;data precision;training time;FPGA accelerator;adaptive dataflow;inference;training operations;quantized-aware training;FIXAR platform;DRL environment;DNN operations;latest DRL algorithm;CPU-GPU platform;energy efficiency;state-of-the-art acceleration;complex DNN models;deep reinforcement learning acceleration platform","","10","","20","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Robot Collision Avoidance With Self-State-Attention and Sensor Fusion","Y. Han; I. H. Zhan; W. Zhao; J. Pan; Z. Zhang; Y. Wang; Y. -J. Liu","BNRist, MOE-Key Laboratory of Pervasive Computing, Department of Computer Science and Technology, Tsinghua University, Beijing, China; BNRist, MOE-Key Laboratory of Pervasive Computing, Department of Computer Science and Technology, Tsinghua University, Beijing, China; BNRist, MOE-Key Laboratory of Pervasive Computing, Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science, University of Hong Kong, Hong Kong; Advanced Computing and Storage Laboratory, Huawei Technologies Company Ltd, Shenzhen, China; Advanced Computing and Storage Laboratory, Huawei Technologies Company Ltd, Shenzhen, China; BNRist, MOE-Key Laboratory of Pervasive Computing, Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Robotics and Automation Letters","7 Jun 2022","2022","7","3","6886","6893","3D LiDAR sensors can provide 3D point clouds of the environment, and are widely used in automobile navigation; while 2D LiDAR sensors can only provide point cloud in a 2D sweeping plane, and then are only used for navigating robots of small height, e.g., floor mopping robots. In this letter, we propose a simple yet effective deep reinforcement learning (DRL) method with our self-state-attention unit and give a solution that can use low-cost devices (i.e., a 2D LiDAR sensor and a monocular camera) to navigate a tall mobile robot of one meter height. The overrall pipeline is that we (1) infer the dense depth information of RGB images with the aid of the 2D LiDAR sensor data (i.e., point clouds in a plane with fixed height), (2) further filter the dense depth map into a 2D minimal depth data and fuse with 2D LiDAR data, and (3) make use of DRL module with our self-state-attention unit to a partially observable sequential decision making problem that can deal with partially accurate data. We present a novel DRL training scheme for robot navigation, proposing a concise and effective self-state-attention unit and proving that applying this unit can replace multi-stage training, achieve better results and generalization capability. Experiments on both simulated data and a real robot show that our method can perform efficient collision avoidance only using low-cost 2D LiDAR sensor and monocular camera.","2377-3766","","10.1109/LRA.2022.3178791","National Natural Science Foundation of China(grant numbers:61725204); Tsinghua University Initiative Scientific Research Program(grant numbers:20211080093); Science and Technology Department of Jiangsu Province, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9789512","Deep reinforcement learning;robot Collision avoidance;self state attention;sensor fusion","Training;Point cloud compression;Laser radar;Three-dimensional displays;Navigation;Robot vision systems;Sensor fusion","cameras;collision avoidance;decision making;image sensors;learning (artificial intelligence);mobile robots;navigation;optical radar;path planning;radar imaging;robot vision;sensor fusion","automobile navigation;point cloud;2D sweeping plane;floor mopping robots;effective deep reinforcement learning method;tall mobile robot;2D LiDAR sensor data;2D minimal depth data;2D LiDAR data;robot navigation;effective self-state-attention unit;low-cost 2D LiDAR sensor;robot collision avoidance;sensor fusion;3D LiDAR sensors","","10","","42","IEEE","7 Jun 2022","","","IEEE","IEEE Journals"
"Improving the efficiency of Bayesian inverse reinforcement learning","B. Michini; J. P. How","AeroSpace Controls Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; AeroSpace Controls Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA","2012 IEEE International Conference on Robotics and Automation","28 Jun 2012","2012","","","3651","3656","Inverse reinforcement learning (IRL) is the task of learning the reward function of a Markov Decision Process (MDP) given knowledge of the transition function and a set of expert demonstrations. While many IRL algorithms exist, Bayesian IRL [1] provides a general and principled method of reward learning by casting the problem in the Bayesian inference framework. However, the algorithm as originally presented suffers from several inefficiencies that prohibit its use for even moderate problem sizes. This paper proposes modifications to the original Bayesian IRL algorithm to improve its efficiency and tractability in situations where the state space is large and the expert demonstrations span only a small portion of it. The key insight is that the inference task should be focused on states that are similar to those encountered by the expert, as opposed to making the naive assumption that the expert demonstrations contain enough information to accurately infer the reward function over the entire state space. A modified algorithm is presented and experimental results show substantially faster convergence while maintaining the solution quality of the original method.","1050-4729","978-1-4673-1405-3","10.1109/ICRA.2012.6225241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225241","","Bayesian methods;Vectors;Kernel;Cooling;Inference algorithms;Schedules;Standards","belief networks;inference mechanisms;learning (artificial intelligence);Markov processes","Bayesian inverse reinforcement learning;Markov decision process;transition function;IRL algorithms;Bayesian inference framework;inference task;expert demonstrations;convergence","","9","","14","IEEE","28 Jun 2012","","","IEEE","IEEE Conferences"
"Hierarchical sub-task decomposition for reinforcement learning of multi-robot delivery mission","H. Kawano","NTT Communication Science Laboratories, NTT Corporation, Kanagawa, Japan","2013 IEEE International Conference on Robotics and Automation","17 Oct 2013","2013","","","828","835","In applying reinforcement learning (RL) to multi-robot control, the size of the learning state space easily explodes because the state space has a high dimension. Hierarchical reinforcement learning (HRL) is one of the most practical approaches to solve the problem; however, automatically decomposing a plain MDP state space into sub-spaces has not been studied thoroughly enough to be applied to practical robotics problems. We propose a method that automatically forms hierarchical sub-tasks for multi-robot delivery missions. The method executes sub-task decomposition and the learning process in a step-by-step manner, by widening the robot's range of movements around the load and gradually decreasing the domain of the load position. The method automatically detects the state in which cooperative motion among the robots is needed for them to accomplish the mission. The performance of the method is demonstrated by simulations.","1050-4729","978-1-4673-5643-5","10.1109/ICRA.2013.6630669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630669","","Robots;Trajectory;Learning (artificial intelligence);Boolean functions;Space missions;Aerospace electronics;Joints","learning (artificial intelligence);mobile robots;multi-robot systems","hierarchical sub-task decomposition;learning state space;hierarchical reinforcement learning;hierarchical subtasks;load position;cooperative motion;multirobot control;multirobot delivery mission","","9","","19","IEEE","17 Oct 2013","","","IEEE","IEEE Conferences"
"An Interactive Lane Change Decision Making Model With Deep Reinforcement Learning","S. Jiang; J. Chen; M. Shen","School of Engineering and Applied Sciences, Harvard University, Cambridge, USA; College of New Energy, China University of Petroleum, Qingdao, China; Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, USA","2019 7th International Conference on Control, Mechatronics and Automation (ICCMA)","10 Feb 2020","2019","","","370","376","By considering lane change maneuver as primarily a Partial Observed Markov Decision Process (POMDP) and motion planning problem, this paper presents an interactive model with a Recurrent Neural Network (RNN) approach to determine the adversarial or cooperative intention probability of following vehicle in target lane. To make proper and efficient lane change decision, Deep Q-value network (DQN) is applied to solve POMDP with expected global maximum reward. Then quintic polynomials-based motion planning algorithm is used to obtain both optimal lateral and longitudinal trajectory for autonomous vehicle to pursuit. Experimental results demonstrate the capability of the proposed model to execute lane change maneuver with comfortable and safety reference trajectory at an appropriate time instance and traffic gap in various highway traffic scenarios.","","978-1-7281-3787-2","10.1109/ICCMA46720.2019.8988750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8988750","autonomous driving;lane change;interactive;POMDP;reinforcement learning;RNN;quintic polynomials","","control engineering computing;decision making;decision theory;learning (artificial intelligence);Markov processes;optimal control;path planning;polynomials;probability;recurrent neural nets;road traffic control;road vehicles;traffic engineering computing;trajectory control","interactive lane change decision making model;deep reinforcement learning;lane change maneuver;POMDP;motion planning problem;recurrent neural network approach;adversarial intention probability;cooperative intention probability;expected global maximum reward;quintic polynomials-based motion planning algorithm;optimal lateral trajectory;partial observed Markov decision process;deep Q-value network;optimal longitudinal trajectory","","8","","20","IEEE","10 Feb 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning Impedance Control of a Robotic Prosthesis to Coordinate With Human Intact Knee Motion","R. Wu; M. Li; Z. Yao; W. Liu; J. Si; H. Huang","School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; UNC/NCSU Department of Biomedical Engineering, NC State University, Raleigh, NC, USA; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; UNC/NCSU Department of Biomedical Engineering, NC State University, Raleigh, NC, USA; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; UNC/NCSU Department of Biomedical Engineering, NC State University, Raleigh, NC, USA","IEEE Robotics and Automation Letters","13 Jun 2022","2022","7","3","7014","7020","This study aims to demonstrate reinforcement learning tracking control for automatically configuring the impedance parameters of a robotic knee prosthesis. While our previous studies involving human subjects have focused on tuning the impedance control parameters to meet a fixed, subjectively prescribed target motion profile to enable continuous walking with human-in-the-loop, in this paper we develop a new tracking control solution for a robotic knee to mimic the motion of the intact knee. As such, we replaced the prescribed target knee motion by an automatically generated profile based on the intact knee. As the profile of the intact knee varies over time due to human adaptation, we are presented with a challenging tracking control problem in the context of classical control theory. By formulating the “echo control” of the robotic knee as a reinforcement learning problem, we provide a promising new tool for real-time tracking control design without explicitly representing the underlying dynamics using a mathematical model, which can be difficult to obtain for a human-robot system. Additionally, our results may inspire future studies and new robotic prosthesis impedance control designs that can potentially coordinate between the intact and the robotic limbs toward daily use of the robotic device.","2377-3766","","10.1109/LRA.2022.3179420","National Science Foundation(grant numbers:1563921,1808752,1563454,1808898); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9786637","Reinforcement learning;prosthetics and exoskeletons;compliance and impedance control;physical human-robot interaction","Knee;Prosthetics;Impedance;Robots;Robot kinematics;Legged locomotion;Tuning","artificial limbs;biomechanics;control system synthesis;human-robot interaction;medical robotics;motion control;prosthetics;reinforcement learning;robot dynamics;tracking","human intact knee motion;impedance parameters;robotic knee prosthesis;human subjects;impedance control parameters;fixed prescribed target motion profile;subjectively prescribed target motion profile;tracking control solution;prescribed target knee motion;automatically generated profile;human adaptation;challenging tracking control problem;classical control theory;echo control;reinforcement learning problem;real-time tracking control design;human-robot system;robotic prosthesis impedance control designs;robotic limbs;robotic device","","8","","22","IEEE","1 Jun 2022","","","IEEE","IEEE Journals"
"Interaction-Aware Multi-Agent Reinforcement Learning for Mobile Agents with Individual Goals","A. Mohseni-Kabir; D. Isele; K. Fujimura","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Honda Research Institute USA; Honda Research Institute USA","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","3370","3376","In a multi-agent setting, the optimal policy of a single agent is largely dependent on the behavior of other agents. We investigate the problem of multi-agent reinforcement learning, focusing on decentralized learning in non-stationary domains for mobile robot navigation. We identify a cause for the difficulty in training non-stationary policies: mutual adaptation to sub-optimal behaviors, and we use this to motivate a curriculum-based strategy for learning interactive policies. The curriculum has two stages. First, the agent leverages policy gradient algorithms to learn a policy that is capable of achieving multiple goals. Second, the agent learns a modifier policy to learn how to interact with other agents in a multi-agent setting. We evaluated our approach on both an autonomous driving lane-change domain and a robot navigation domain.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793721","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793721","","Training;Robots;Games;Markov processes;Reinforcement learning;Navigation;Autonomous vehicles","gradient methods;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;navigation;path planning;robot programming","interaction-aware multiagent reinforcement learning;mobile agents;individual goals;optimal policy;decentralized learning;mobile robot navigation;policy gradient algorithms;nonstationary policies;curriculum-based strategy;interactive policy learning","","8","","29","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Playtesting in Match 3 Game Using Strategic Plays via Reinforcement Learning","Y. Shin; J. Kim; K. Jin; Y. B. Kim","Department of Image Science and Art, Chung-Ang University, Dongjak, South Korea; Department of Image Science and Art, Chung-Ang University, Dongjak, South Korea; Department of Image Science and Art, Chung-Ang University, Dongjak, South Korea; Department of Image Science and Art, Chung-Ang University, Dongjak, South Korea","IEEE Access","19 Mar 2020","2020","8","","51593","51600","Playtesting is a lifecycle phase in game development wherein the completeness and smooth progress of planned content are verified before release of a new game. Although studies on playtesting in Match 3 games have attempted to utilize Monte Carlo tree search (MCTS) and convolutional neural networks (CNNs), the applicability of these methods are limited because the associated training is time-consuming and data collection is difficult. To address this problem, game playtesting was performed via learning based on strategic play in Match 3 games. Five strategic plays were defined in the Match 3 game under consideration and game playtesting was performed for each situation via reinforcement learning. The proposed agent performed within a 5% margin of human performance on the most complex mission in the experiment. We demonstrate that it is possible for the level designer to measure the difficulty of the level via playtesting various missions. This study also provides level testing standards for several types of missions in Match 3 games.","2169-3536","","10.1109/ACCESS.2020.2980380","National Research Foundation of Korea(grant numbers:NRF-2018R1C1B5046461); Seoul R&BD Program(grant numbers:CY190039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9034187","Actor-critic;agent;artificial intelligence;game mission;game strategy;match 3;playtesting;reinforcement learning","Games;Learning (artificial intelligence);Color;Automation;Monte Carlo methods;Convolutional neural networks","computer games;convolutional neural nets;learning (artificial intelligence);Monte Carlo methods;software engineering;tree searching","Match 3 game;strategic play;reinforcement learning;game development;convolutional neural networks;game playtesting;CNN;lifecycle phase;Monte Carlo tree search;MCTS;level designer;level testing standards","","8","","32","CCBY","12 Mar 2020","","","IEEE","IEEE Journals"
"Knowledge-Guided Reinforcement Learning Control for Robotic Lower Limb Prosthesis","X. Gao; J. Si; Y. Wen; M. Li; H. H. Huang","Department of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; Department of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; Department of Biomedical Engineering, North Carolina State University, Raleigh, NC, USA; Department of Biomedical Engineering, North Carolina State University, Raleigh, NC, USA; Department of Biomedical Engineering, North Carolina State University, Raleigh, NC, USA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","754","760","Robotic prostheses provide new opportunities to better restore lost functions than passive prostheses for trans-femoral amputees. But controlling a prosthesis device automatically for individual users in different task environments is an unsolved problem. Reinforcement learning (RL) is a naturally promising tool. For prosthesis control with a user in the loop, it is desirable that the controlled prosthesis can adapt to different task environments as quickly and smoothly as possible. However, most RL agents learn or relearn from scratch when the environment changes. To address this issue, we propose the knowledge-guided Q-learning (KG-QL) control method as a principled way for the problem. In this report, we collected and used data from two able-bodied (AB) subjects wearing a RL controlled robotic prosthetic limb walking on level ground. Our ultimate goal is to build an efficient RL controller with reduced time and data requirements and transfer knowledge from AB subjects to amputee subjects. Toward this goal, we demonstrate its feasibility by employing OpenSim, a well-established human locomotion simulator. Our results show the OpenSim simulated amputee subject improved control tuning performance over learning from scratch by utilizing knowledge transfer from AB subjects. Also in this paper, we will explore the possibility of information transfer from AB subjects to help tuning for the amputee subjects.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196749","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196749","","Prosthetics;Task analysis;Impedance;Knee;Legged locomotion;Tuning","artificial limbs;biomechanics;gait analysis;learning (artificial intelligence);medical robotics;neurophysiology;patient rehabilitation","knowledge transfer;control tuning performance;amputee subject;AB subjects;transfer knowledge;data requirements;RL controller;robotic prosthetic limb;control method;knowledge-guided Q-learning;RL agents learn;controlled prosthesis;prosthesis control;prosthesis device;trans-femoral amputees;passive prostheses;lost functions;robotic lower limb prosthesis;guided reinforcement learning control","","7","","24","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"VPE: Variational Policy Embedding for Transfer Reinforcement Learning","I. Arnekvist; D. Kragic; J. A. Stork","Authors are with the Robotics, Perception, and Learning lab, Royal Institute of Technology, Sweden; Authors are with the Robotics, Perception, and Learning lab, Royal Institute of Technology, Sweden; Authors are with the Robotics, Perception, and Learning lab, Royal Institute of Technology, Sweden","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","36","42","Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffer from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments.We consider the problem of transferring knowledge within a family of similar Markov decision processes. We assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793556","","Optimization;Training;Task analysis;Robots;Adaptation models;Reinforcement learning;Supervised learning","learning (artificial intelligence);Markov processes;pendulums;variational techniques","variational policy embedding;transfer reinforcement Learning;complex problems;deployment conditions;data collection;simulation training;Q-function;master policy;latent variables;latent space;low-dimensional space;simulation-to-real transfer;reinforcement learning methods;Markov decision processes","","7","","40","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Framework for Underwater Locomotion of Soft Robot","G. Li; J. Shintake; M. Hayashibe","Department of Robotics, Graduate School of Engineering, Neuro-Robotics Lab, Tohoku University, Sendai, Japan; Department of Mechanical and Intelligent Systems Engineering, University of Electro-Communications, Tokyo, Japan; Department of Robotics, Graduate School of Engineering, Neuro-Robotics Lab, Tohoku University, Sendai, Japan","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","12033","12039","Soft robotics is an emerging technology with excellent application prospects. However, due to the inherent compliance of the materials used to build soft robots, it is extremely complicated to control soft robots accurately. In this paper, we introduce a data-based control framework for solving the soft robot underwater locomotion problem using deep reinforcement learning (DRL). We first built a soft robot that can swim based on the dielectric elastomer actuator (DEA). We then modeled it in a simulation for the purpose of training the neural network and tested the performance of the control framework through real experiments on the robot. The framework includes the following: a simulation method for the soft robot that can be used to collect data for training the neural network, the neural network controller of the swimming robot trained in the simulation environment, and the computer vision method to collect the observation space from the real robot using a camera. We confirmed the effectiveness of the learning method for the soft swimming robot in the simulation environment by allowing the robot to learn how to move from a random initial state to a specific direction. After obtaining the trained neural network through the simulation, we deployed it on the real robot and tested the performance of the control framework. The soft robot successfully achieved the goal of moving in a straight line in disturbed water. The experimental results suggest the potential of using deep reinforcement learning to improve the locomotion ability of mobile soft robots.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561145","","Training;Learning systems;Aquatic robots;Computational modeling;Neural networks;Robot vision systems;Reinforcement learning","computer vision;control engineering computing;intelligent robots;marine control;mobile robots;neurocontrollers;reinforcement learning;robot vision","deep reinforcement learning;soft swimming robot;mobile soft robots;soft robot underwater locomotion problem;data-based control framework;DRL;dielectric elastomer actuator;DEA;neural network controller;camera","","7","","27","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Safe Reinforcement Learning with Policy-Guided Planning for Autonomous Driving","J. Rong; N. Luan","School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China","2020 IEEE International Conference on Mechatronics and Automation (ICMA)","26 Oct 2020","2020","","","320","326","The uncertainty and complexity of autonomous driving make Deep Reinforcement Learning (DRL) appealing. DRL can optimize the expected reward by interacting with environments. However, DRL has no guarantee of safety, which is essential for autonomous driving. Besides, the reward shaping for DRL in autonomous driving is also challenging. In this work, we introduced a policy-guided trajectory planner and proposed a hierarchical structure to try to solve these problems. The high-level DRL agent’s output is policies that are considered as suggestions for the low-level policy-guided trajectory planner. To achieve the guarantee of safety, we first translated the traffic rules and policies into formal specifications. Notice that, in many cases, the policy may not be possible to be fully applied. Given certain formal specifications, we constructed a minimum-violation motion planning problem for the low-level policy-guided trajectory planner. Through this hierarchical structure, the long-term uncertainty is handled by the high-level DRL agent, and the safety is guaranteed by the low-level planner. Furthermore, the reward includes not only the violation of traffic rules but also the violation of policies. By adding the violation of policies returned by the low-level planner to the reward, the agent could explicitly learn if a policy is supported by environments and vehicle dynamics. Our method was proven to be effective through numerical experiments.","2152-744X","978-1-7281-6416-8","10.1109/ICMA49215.2020.9233522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9233522","Motion Planning;Autonomous Driving;Reinforcement Learning","Uncertainty;Trajectory planning;Reinforcement learning;Predictive models;Trajectory;Safety;Planning","formal specification;learning (artificial intelligence);mobile robots;path planning;road safety;road vehicles;vehicle dynamics","safe Reinforcement;policy-guided planning;autonomous driving;hierarchical structure;high-level DRL agent;low-level policy-guided trajectory planner;formal specifications;minimum-violation motion planning problem;low-level planner","","7","","19","IEEE","26 Oct 2020","","","IEEE","IEEE Conferences"
"User Interaction Aware Reinforcement Learning for Power and Thermal Efficiency of CPU-GPU Mobile MPSoCs","S. Dey; A. K. Singh; X. Wang; K. McDonald-Maier","Embedded and Intelligent Systems Laboratory, University of Essex; Embedded and Intelligent Systems Laboratory, University of Essex; School of Software Engineering, South China University of Technology; Embedded and Intelligent Systems Laboratory, University of Essex","2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)","15 Jun 2020","2020","","","1728","1733","Mobile user’s usage behaviour changes throughout the day and the desirable Quality of Service (QoS) could thus change for each session. In this paper, we propose a QoS aware agent to monitor mobile user’s usage behaviour to find the target frame rate, which satisfies the desired user’s QoS, and applies reinforcement learning based DVFS on a CPU-GPU MPSoC to satisfy the frame rate requirement. Experimental study on a real Exynos hardware platform shows that our proposed agent is able to achieve a maximum of 50% power saving and 29% reduction in peak temperature compared to stock Android’s power saving scheme. It also outperforms the existing state-of-the-art power and thermal management scheme by 41% and 19%, respectively.","1558-1101","978-3-9819263-4-7","10.23919/DATE48585.2020.9116294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9116294","agent system;reinforcement learning;machine learning;CPU;GPU;power optimization;thermal optimization;MPSoCs;user behaviour;user interaction;smartphone;mobile","Quality of service;Power demand;Graphics processing units;Reinforcement learning;Facebook;Performance evaluation;Thermal management","Android (operating system);graphics processing units;learning (artificial intelligence);microprocessor chips;mobile computing;power aware computing;quality of service;system-on-chip;thermal management (packaging)","quality of service;frame rate requirement;Exynos hardware platform;thermal management scheme;Android power saving scheme;QoS aware agent;mobile user;CPU-GPU mobile MPSoCs;thermal efficiency;user interaction aware reinforcement learning","","7","","26","","15 Jun 2020","","","IEEE","IEEE Conferences"
"A Data-Driven Reinforcement Learning Solution Framework for Optimal and Adaptive Personalization of a Hip Exoskeleton","X. Tu; M. Li; M. Liu; J. Si; H. H. Huang","NCSU/UNC Department of Biomedical Engineering, NC State University, Raleigh, NC; NCSU/UNC Department of Biomedical Engineering, NC State University, Raleigh, NC; NCSU/UNC Department of Biomedical Engineering, NC State University, Raleigh, NC; Department of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; NCSU/UNC Department of Biomedical Engineering, NC State University, Raleigh, NC","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","10610","10616","Robotic exoskeletons are exciting technologies for augmenting human mobility. However, designing such a device for seamless integration with the human user and to assist human movement still is a major challenge. This paper aims at developing a novel data-driven solution framework based on reinforcement learning (RL), without first modeling the human-robot dynamics, to provide optimal and adaptive personalized torque assistance for reducing human efforts during walking. Our automatic personalization solution framework includes the assistive torque profile with two control timing parameters (peak and offset timings), the least square policy iteration (LSPI) for learning the parameter tuning policy, and a cost function based on a transferred work ratio. The proposed controller was successfully validated on a healthy human subject to assist unilateral hip extension in walking. The results showed that the optimal and adaptive RL controller as a new approach was feasible for tuning assistive torque profile of the hip exoskeleton that coordinated with human actions and reduced activation level of hip extensor muscle in human.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9562062","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562062","Exoskeleton;reinforcement learning;optimal adaptive control;least square policy iteration;data driven","Legged locomotion;Torque;Robot kinematics;Exoskeletons;Reinforcement learning;Muscles;Cost function","adaptive systems;biomechanics;control engineering computing;human-robot interaction;iterative methods;medical robotics;muscle;optimal control;reinforcement learning;robot dynamics;torque control","data-driven reinforcement learning solution framework;adaptive personalization;hip exoskeleton;robotic exoskeletons;seamless integration;human user;human movement;human-robot dynamics;adaptive personalized torque assistance;human efforts;walking;automatic personalization solution framework;assistive torque profile;control timing parameters;least square policy iteration;parameter tuning policy;unilateral hip extension;optimal controller;adaptive RL controller;human actions;hip extensor muscle;human mobility augmentation;data-driven solution framework;LSPI","","7","","40","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Tactile Gym 2.0: Sim-to-Real Deep Reinforcement Learning for Comparing Low-Cost High-Resolution Robot Touch","Y. Lin; J. Lloyd; A. Church; N. F. Lepora","Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.","IEEE Robotics and Automation Letters","9 Aug 2022","2022","7","4","10754","10761","High-resolution optical tactile sensors are increasingly used in robotic learning environments due to their ability to capture large amounts of data directly relating to agent-environment interaction. However, there is a high barrier of entry to research in this area due to the high cost of tactile robot platforms, specialised simulation software, and sim-to-real methods that lack generality across different sensors. In this letter we extend the Tactile Gym simulator to include three new optical tactile sensors (TacTip, DIGIT and DigiTac) of the two most popular types, Gelsight-style (image-shading based) and TacTip-style (marker based). We demonstrate that a single sim-to-real approach can be used with these three different sensors to achieve strong real-world performance despite the significant differences between real tactile images. Additionally, we lower the barrier of entry to the proposed tasks by adapting them to an inexpensive 4-DoF robot arm, further enabling the dissemination of this benchmark. We validate the extended environment on three physically-interactive tasks requiring a sense of touch: object pushing, edge following and surface following. The results of our experimental validation highlight some differences between these sensors, which may help future researchers select and customize the physical characteristics of tactile sensors for different manipulations scenarios.","2377-3766","","10.1109/LRA.2022.3195195","China Scholarship Council; University of Bristol; EPSRC CASE; Google DeepMind; Leadership Award from the Leverhulme Trust(grant numbers:RL-2016-39); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9847020","Force and tactile sensing;Deep Learning;Dexterous Manipulation","Robot sensing systems;Robots;Sensors;Optical sensors;Tactile sensors;Optical imaging;Task analysis","control engineering computing;deep learning (artificial intelligence);haptic interfaces;manipulators;reinforcement learning;tactile sensors","Tactile Gym 2.0;sim-to-real methods;specialised simulation software;tactile robot platforms;agent-environment interaction;robotic learning environments;high-resolution optical tactile sensors;low-cost high-resolution robot touch;sim-to-real deep reinforcement learning;physically-interactive tasks;extended environment;4-DoF robot arm;tactile images;TacTip-style;image-shading;Gelsight-style;Tactile Gym simulator","","7","","34","IEEE","1 Aug 2022","","","IEEE","IEEE Journals"
"Adaptive Droplet Routing for MEDA Biochips via Deep Reinforcement Learning","M. Elfar; T. -C. Liang; K. Chakrabarty; M. Pajic","Duke University, Durham, NC, USA; Duke University, Durham, NC, USA; Duke University, Durham, NC, USA; Duke University, Durham, NC, USA","2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)","19 May 2022","2022","","","640","645","Digital microfluidic biochips (DMFBs) based on a micro-electrode-dot-array (MEDA) architecture provide fine-grained control and sensing of droplets in real-time. However, excessive actuation of microelectrodes in MEDA biochips can lead to charge trapping during bioassay execution, causing the failure of microelectrodes and erroneous bioassay outcomes. A recently proposed enhancement to MEDA allows run-time measurement of microelectrode health information, thereby enabling synthesis of adaptive routing strategies for droplets. However, existing synthesis solutions are computationally infeasible for large MEDA biochips that have been commercialized. In this paper, we propose a synthesis framework for adaptive droplet routing in MEDA biochips via deep reinforcement learning (DRL). The framework utilizes the real-time microelectrode health feedback to synthesize droplet routes that proactively minimize the likelihood of charge trapping. We show how the adaptive routing strategies can be synthesized using DRL. We implement the DRL agent, the MEDA simulation environment, and the bioassay scheduler using the OpenAI Gym environment. Our framework obtains adaptive routing policies efficiently for COVID-19 testing protocols on large arrays that reflect the sizes of commercial MEDA biochips available in the marketplace, significantly increasing probabilities of successful bioassay completion compared to existing methods.","1558-1101","978-3-9819263-6-1","10.23919/DATE54114.2022.9774737","National Science Foundation(grant numbers:ECCS-1914796); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774737","","Microelectrodes;Scalability;Biochips;Cells (biology);Reinforcement learning;Routing;Real-time systems","biomedical electronics;bioMEMS;lab-on-a-chip;learning (artificial intelligence);microelectrodes;microfluidics","commercial MEDA biochips;adaptive routing policies;MEDA simulation environment;droplet routes;real-time microelectrode health feedback;adaptive routing strategies;microelectrode health information;charge trapping;microelectrode-dot-array architecture;digital microfluidic biochips;deep reinforcement learning;adaptive droplet routing","","7","","29","","19 May 2022","","","IEEE","IEEE Conferences"
"Real-Time Trajectory Adaptation for Quadrupedal Locomotion using Deep Reinforcement Learning","S. Gangapurwala; M. Geisert; R. Orsolino; M. Fallon; I. Havoutis","Dynamic Robots Systems (DRS) Group, Oxford Robotics Institute, University of Oxford, UK; Dynamic Robots Systems (DRS) Group, Oxford Robotics Institute, University of Oxford, UK; Dynamic Robots Systems (DRS) Group, Oxford Robotics Institute, University of Oxford, UK; Dynamic Robots Systems (DRS) Group, Oxford Robotics Institute, University of Oxford, UK; Dynamic Robots Systems (DRS) Group, Oxford Robotics Institute, University of Oxford, UK","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","5973","5979","We present a control architecture for real-time adaptation and tracking of trajectories generated using a terrain-aware trajectory optimization solver. This approach enables us to circumvent the computationally exhaustive task of online trajectory optimization, and further introduces a control solution robust to systems modeled with approximated dynamics. We train a policy using deep reinforcement learning (RL) to introduce additive deviations to a reference trajectory in order to generate a feedback-based trajectory tracking system for a quadrupedal robot. We train this policy across a multitude of simulated terrains and ensure its generality by introducing training methods that avoid overfitting and convergence towards local optima. Additionally, in order to capture terrain information, we include a latent representation of the height maps in the observation space of the RL environment as a form of exteroceptive feedback. We test the performance of our trained policy by tracking the corrected set points using a model-based whole-body controller and compare it with the tracking behavior obtained without the corrective feedback in several simulation environments, and show that introducing the corrective feedback results in increase of the success rate from 72.7% to 92.4% for tracking precomputed dynamic long horizon trajectories on flat terrain and from 47.5% to 80.3% on a complex modular uneven terrain. We also show successful transfer of our training approach to the real physical system and further present cogent arguments in support of our framework.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561639","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561639","","Training;Trajectory tracking;Conferences;Computational modeling;Reinforcement learning;Computer architecture;Real-time systems","deep learning (artificial intelligence);feedback;legged locomotion;motion control;optimisation;robust control;tracking;trajectory control","time trajectory adaptation;quadrupedal locomotion;deep reinforcement learning;control architecture;real-time adaptation;terrain-aware trajectory optimization solver;computationally exhaustive task;online trajectory optimization;approximated dynamics;additive deviations;reference trajectory;feedback-based trajectory tracking system;quadrupedal robot;simulated terrains;training methods;terrain information;RL environment;exteroceptive feedback;trained policy;corrected set points;model-based whole-body controller;tracking behavior;simulation environments;corrective feedback;precomputed dynamic long horizon trajectories;flat terrain;complex modular uneven terrain","","7","","34","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Provably Safe Deep Reinforcement Learning for Robotic Manipulation in Human Environments","J. Thumm; M. Althoff","Department of Informatics, Technical Univer-sity of Munich, Garching, Germany; Department of Informatics, Technical Univer-sity of Munich, Garching, Germany","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","6344","6350","Deep reinforcement learning (RL) has shown promising results in the motion planning of manipulators. However, no method guarantees the safety of highly dynamic obstacles, such as humans, in RL-based manipulator control. This lack of formal safety assurances prevents the application of RL for manipulators in real-world human environments. Therefore, we propose a shielding mechanism that ensures ISO- verified human safety while training and deploying RL algorithms on manipulators. We utilize a fast reachability analysis of humans and manipulators to guarantee that the manipulator comes to a complete stop before a human is within its range. Our proposed method guarantees safety and significantly improves the RL performance by preventing episode-ending collisions. We demonstrate the performance of our proposed method in simulation using human motion capture data.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811698","","Training;Heuristic algorithms;Dynamics;Reinforcement learning;Motion capture;Safety;Planning","collision avoidance;learning (artificial intelligence);manipulators;mobile robots;path planning;reachability analysis;safety","safe deep reinforcement learning;robotic manipulation;highly dynamic obstacles;RL-based manipulator control;formal safety assurances;real-world human environments;human safety;method guarantees safety;RL performance;human motion capture data","","7","","32","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"VesNet-RL: Simulation-Based Reinforcement Learning for Real-World US Probe Navigation","Y. Bi; Z. Jiang; Y. Gao; T. Wendler; A. Karlas; N. Navab","Chair for Computer-Aided Medical Procedures and Augmented Reality, Technical University of Munich, Garching bei München, Germany; Chair for Computer-Aided Medical Procedures and Augmented Reality, Technical University of Munich, Garching bei München, Germany; Chair for Computer-Aided Medical Procedures and Augmented Reality, Technical University of Munich, Garching bei München, Germany; Chair for Computer-Aided Medical Procedures and Augmented Reality, Technical University of Munich, Garching bei München, Germany; Institute of Biological and Medical Imaging, Helmholtz Zentrum München, München, Germany; Chair for Computer-Aided Medical Procedures and Augmented Reality, Technical University of Munich, Garching bei München, Germany","IEEE Robotics and Automation Letters","27 May 2022","2022","7","3","6638","6645","Ultrasound (US) is one of the most common medical imaging modalities since it is radiation-free, low-cost, and real-time. In freehand US examinations, sonographers often navigate a US probe to visualize standard examination planes with rich diagnostic information. However, reproducibility and stability of the resulting images often suffer from intra- and inter-operator variation. Reinforcement learning (RL), as an interaction-based learning method, has demonstrated its effectiveness in visual navigating tasks; however, RL is limited in terms of generalization. To address this challenge, we propose a simulation-based RL framework for real-world navigation of US probes towards the standard longitudinal views of vessels. A UNet is used to provide binary masks from US images; thereby, the RL agent trained on simulated binary vessel images can be applied in real scenarios without further training. To accurately characterize actual states, a multi-modality state representation structure is introduced to facilitate the understanding of environments. Moreover, considering the characteristics of vessels, a novel standard view recognition approach based on the minimum bounding rectangle is proposed to terminate the searching process. To evaluate the effectiveness of the proposed method, the trained policy is validated virtually on 3D volumes of a volunteer’s in-vivo carotid artery, and physically on custom-designed gel phantoms using robotic US. The results demonstrate that proposed approach can effectively and accurately navigate the probe towards the longitudinal view of vessels.","2377-3766","","10.1109/LRA.2022.3176112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779963","Robotic ultrasound;reinforcement learning;medical robotics;standard plane identification","Standards;Probes;Robots;Navigation;Image segmentation;Task analysis;Training","biomedical ultrasonics;blood vessels;image segmentation;learning (artificial intelligence);medical image processing;phantoms;ultrasonic imaging","VesNet-RL;simulation-based reinforcement learning;US probe navigation;common medical imaging modalities;freehand US examinations;standard examination planes;rich diagnostic information;reproducibility;stability;inter-operator variation;interaction-based;visual navigating tasks;simulation-based RL framework;real-world navigation;binary masks;US images;RL agent;simulated binary vessel images;multimodality state representation structure;standard view recognition approach;robotic US","","7","","33","IEEE","23 May 2022","","","IEEE","IEEE Journals"
"A Game-Theoretical Approach to Cyber-Security of Critical Infrastructures Based on Multi-Agent Reinforcement Learning","M. Panfili; A. Giuseppi; A. Fiaschetti; H. B. Al-Jibreen; A. Pietrabissa; F. Delli Priscoli","Dept. of Computer, Control and Management Engineering University of Rome Sapienza, Rome, Italy; Dept. of Computer, Control and Management Engineering University of Rome Sapienza, Rome, Italy; Dept. of Computer, Control and Management Engineering University of Rome Sapienza, Rome, Italy; Dept. of Computer, Control and Management Engineering University of Rome Sapienza, Rome, Italy; Dept. of Computer, Control and Management Engineering University of Rome Sapienza, Rome, Italy; Dept. of Computer, Control and Management Engineering University of Rome Sapienza, Rome, Italy","2018 26th Mediterranean Conference on Control and Automation (MED)","23 Aug 2018","2018","","","460","465","This paper presents a control strategy for Cyber-Physical System defense developed in the framework of the European Project ATENA, that concerns Critical Infrastructure (CI) protection. The aim of the controller is to find the optimal security configuration, in terms of countermeasures to implement, in order to address the system vulnerabilities. The attack/defense problem is modeled as a multi-agent general sum game, where the aim of the defender is to prevent the most damage possible by finding an optimal trade-off between prevention actions and their costs. The problem is solved utilizing Reinforcement Learning and simulation results provide a proof of the proposed concept, showing how the defender of the protected CI is able to minimize the damage caused by his her opponents by finding the Nash equilibrium of the game in the zero-sum variant, and, in a more general scenario, by driving the attacker in the position where the damage she/he can cause to the infrastructure is lower than the cost it has to sustain to enforce her/his attack strategy.","2473-3504","978-1-5386-7890-9","10.1109/MED.2018.8442695","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8442695","Stochastic Games;Reinforcement Learning;Vulnerability Management;Critical Infrastructure Protection;Composable Security","Games;Security;Learning (artificial intelligence);Critical infrastructure;Nash equilibrium;Aerospace electronics","critical infrastructures;game theory;learning (artificial intelligence);multi-agent systems;security of data","zero-sum variant;attack strategy;game-theoretical approach;Cyber-security;control strategy;European Project ATENA;optimal security configuration;system vulnerabilities;multiagent general sum game;damage possible;optimal trade-off between prevention actions;simulation results;protected CI;critical infrastructures;multiagent reinforcement learning;cyber-physical system defense;critical infrastructure protection;attack-defense problem","","7","","24","IEEE","23 Aug 2018","","","IEEE","IEEE Conferences"
"Inverse Reinforcement Learning via Function Approximation for Clinical Motion Analysis","K. Li; M. Rath; J. W. Burdick","Department of Mechanical and Civil Engineering, California Institute of Technology, Pasadena, CA, USA; University of California Los Angeles, Los Angeles, CA, USA; Department of Mechanical and Civil Engineering, California Institute of Technology, Pasadena, CA, USA","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","610","617","This paper introduces a new method for inverse reinforcement learning in large state spaces, where the learned reward function can be used to control high-dimensional robot systems and analyze complex human movement. To avoid solving the computationally expensive reinforcement learning problems in reward learning, we propose a function approximation method to ensure that the Bellman Optimality Equation always holds, and then estimate a function to maximize the likelihood of the observed motion. The time complexity of the proposed method is linearly proportional to the cardinality of the action set, thus it can handle large state spaces efficiently. We test the proposed method in a simulated environment on reward learning, and show that it is more accurate than existing methods and significantly better in scalability. We also show that the proposed method can extend many existing methods to large state spaces. We then apply the method to evaluating the effect of rehabilitative stimulations on patients with spinal cord injuries based on the observed patient motions.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460563","","Learning (artificial intelligence);Mathematical model;Trajectory;Function approximation;Spinal cord injury;Markov processes;Estimation","function approximation;image motion analysis;injuries;learning (artificial intelligence);medical image processing;neurophysiology","Bellman Optimality Equation;reward learning;inverse reinforcement learning;learned reward function;computationally expensive reinforcement learning problems;function approximation method;clinical motion analysis","","7","","23","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"A Comparison of Different State Representations for Reinforcement Learning Based Variable Speed Limit Control","K. Kušić; E. Ivanjko; M. Gregurić","Faculty of Transport and Traffic Sciences, University of Zagreb, Zagreb, Republic of Croatia; Faculty of Transport and Traffic Sciences, University of Zagreb, Zagreb, Republic of Croatia; Faculty of Transport and Traffic Sciences, University of Zagreb, Zagreb, Republic of Croatia","2018 26th Mediterranean Conference on Control and Automation (MED)","23 Aug 2018","2018","","","1","6","Variable Speed Limit Control (VSLC) is one control method for alleviating congestions on urban motorways. Machine learning techniques, like Reinforcement Learning (RL), are a promising alternative for setting up VSLC because an optimal control policy can be achieved with a smaller computational burden in comparison with optimal control approaches. A drawback is a large number of learning iterations and the problem of the exponential expansion of the state space dimension. This can be solved with function approximation techniques. Three different approaches for feature-based state representation in RL based VSLC are compared in this paper regarding the convergence of Total Time Spent. The microscopic traffic simulator VISSIM with a representative traffic model is used to evaluate the compared approaches. Results show that function approximation methods outperform RL based VSLC formulated with a lookup table by an average improvement of 10 %, where feature extraction methods (Coarse and Tile) coding showed slightly faster learning rate.","2473-3504","978-1-5386-7890-9","10.1109/MED.2018.8442986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8442986","","Function approximation;Convergence;Encoding;Learning (artificial intelligence);Optimal control;Safety;Machine learning","feature extraction;function approximation;optimal control;road traffic control;table lookup","Variable Speed Limit Control;machine learning techniques;Reinforcement Learning;optimal control policy;learning iterations;state space dimension;function approximation techniques;feature-based state representation;RL based VSLC;variable Speed Limit Control;state representations;microscopic traffic simulator;VISSIM;lookup table;feature extraction methods","","7","","16","IEEE","23 Aug 2018","","","IEEE","IEEE Conferences"
"Soil-Adaptive Excavation Using Reinforcement Learning","P. Egli; D. Gaschen; S. Kerscher; D. Jud; M. Hutter","Robotic Systems Lab, ETH Zurich, Zurich, Switzerland; Robotic Systems Lab, ETH Zurich, Zurich, Switzerland; Robotic Systems Lab, ETH Zurich, Zurich, Switzerland; Robotic Systems Lab, ETH Zurich, Zurich, Switzerland; Robotic Systems Lab, ETH Zurich, Zurich, Switzerland","IEEE Robotics and Automation Letters","1 Aug 2022","2022","7","4","9778","9785","In this letter, we present an excavation controller for a full-sized hydraulic excavator that can adapt online to different soil characteristics. Soil properties are hard to predict and can vary even within one scoop, which requires a controller that can adapt online to the encountered soil conditions. The objective is to fill the bucket with excavation material while respecting machine limitations to prevent stalling or lifting of the machine. To this end, we train a control policy in simulation using Reinforcement Learning (RL). The soil interactions are modeled based on the Fundamental Equation of Earth-Moving (FEE) with heavily randomized soil parameters to expose the agent to a wide range of different conditions. The agent learns to output joint velocity commands, which can be directly applied to the standard proportional valves of the real machine. We test the controller on a 12-ton excavator in different types of soils. The experiments demonstrate that the controller can adapt online to changing conditions without the explicit knowledge of the soil parameters, solely from proprioceptive observations, which are easily measurable.","2377-3766","","10.1109/LRA.2022.3189834","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; National Centre of Competence in Digital Fabrication; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826363","Autonomous excavation;reinforcement learning;sim-to-real","Soil;Excavation;Trajectory;Solid modeling;Hydraulic systems;Force;Kinematics","excavators;hydraulic systems;learning (artificial intelligence);soil;valves","machine limitations;stalling;control policy;reinforcement learning;soil interactions;heavily randomized soil parameters;output joint velocity commands;12-ton excavator;soil-adaptive excavation;excavation controller;hydraulic excavator;different soil characteristics;soil properties;encountered soil conditions;excavation material","","7","","46","IEEE","11 Jul 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning Meets Hybrid Zero Dynamics: A Case Study for RABBIT","G. A. Castillo; B. Weng; A. Hereid; Z. Wang; W. Zhang","Electrical and Computer Engineering, Ohio State University, Columbus, OH, USA; Electrical and Computer Engineering, Ohio State University, Columbus, OH, USA; EECS, University of Michigan, Ann Arbor, MI, USA; Department of Mechanical Engineering, University of Hong Kong, Hong Kong; SUSTech Institute of Robotics, Southern University of Science and Technology, China","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","284","290","The design of feedback controllers for bipedal robots is challenging due to the hybrid nature of its dynamics and the complexity imposed by high-dimensional bipedal models. In this paper, we present a novel approach for the design of feedback controllers using Reinforcement Learning (RL) and Hybrid Zero Dynamics (HZD). Existing RL approaches for bipedal walking are inefficient as they do not consider the underlying physics, often requires substantial training, and the resulting controller may not be applicable to real robots. HZD is a powerful tool for bipedal control with local stability guarantees of the walking limit cycles. In this paper, we propose a non traditional RL structure that embeds the HZD framework into the policy learning. More specifically, we propose to use RL to find a control policy that maps from the robot’s reduced order states to a set of parameters that define the desired trajectories for the robot’s joints through the virtual constraints. Then, these trajectories are tracked using an adaptive PD controller. The method results in a stable and robust control policy that is able to track variable speed within a continuous interval. Robustness of the policy is evaluated by applying external forces to the torso of the robot. The proposed RL framework is implemented and demonstrated in OpenAI Gym with the MuJoCo physics engine based on the well-known RABBIT robot model.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793627","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793627","","Legged locomotion;Hip;Rabbits;Robot kinematics;Training;Computational modeling","adaptive control;control engineering computing;control system synthesis;feedback;learning (artificial intelligence);legged locomotion;mobile robots;PD control;robust control;stability","feedback controllers;bipedal robots;high-dimensional bipedal models;bipedal walking;bipedal control;walking limit cycles;HZD framework;policy learning;adaptive PD controller;stable control policy;robust control policy;RL framework;RABBIT robot model;reinforcement learning;local stability;hybrid zero dynamics;OpenAI gym;MuJoCo physics engine","","6","","37","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Self-Organised Swarm Flocking with Deep Reinforcement Learning","M. B. Bezcioglu; B. Lennox; F. Arvin","Department of Electrical and Electronic Engineering, Swarm & Computational Intelligence Lab (SwaCIL), The University of Manchester, UK; Department of Electrical and Electronic Engineering, Swarm & Computational Intelligence Lab (SwaCIL), The University of Manchester, UK; Department of Electrical and Electronic Engineering, Swarm & Computational Intelligence Lab (SwaCIL), The University of Manchester, UK","2021 7th International Conference on Automation, Robotics and Applications (ICARA)","17 Mar 2021","2021","","","226","230","Optimising a set of parameters for swarm flocking is a tedious task as it requires hand-tuning of the parameters. In this paper, we developed a self-organised flocking mechanism with a swarm of homogeneous robots. The proposed mechanism used deep reinforcement learning to teach the swarm to perform the flocking in a continuous state and action space. Collective motion was represented by a self-organising dynamic model that is based on linear spring-like forces between self-propelled particles in an active crystal. We tuned the inverse rotational and translational damping coefficients of the dynamic model for swarm populations of N ∈ {25, 100} E {25, 100} robots. We study the application of reinforcement learning in a centralised multi-agent approach, where we have a global state space matrix that is accessible by actor and critic networks. Furthermore, we showed that our method could train the system to flock regardless of the sparsity of the swarm population, which is a significant result.","","978-1-6654-0469-3","10.1109/ICARA51699.2021.9376509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376509","Swarm Robotics;Reinforcement Learning;Self-organised Flocking;Multi-agent Learning","Sociology;Dynamics;Reinforcement learning;Statistics;Task analysis;Robots;Optimization","damping;learning (artificial intelligence);matrix algebra;multi-agent systems;multi-robot systems","continuous state;action space;self-organising dynamic model;linear spring-like forces;translational damping coefficients;swarm population;global state space matrix;deep reinforcement learning;homogeneous robots;self-organised swarm flocking;centralised multiagent approach;self-propelled particles;active crystal;actor critic networks","","6","","22","IEEE","17 Mar 2021","","","IEEE","IEEE Conferences"
"Data-driven Construction of Symbolic Process Models for Reinforcement Learning","E. Derner; J. Kubalík; R. Babuška","Department of Control Engineering, Czech Technical University in Prague, Prague, Czech Republic; Czech Institute of Informatics, Czech Technical University in Prague, Prague, Czech Republic; Cognitive Robotics, Faculty of 3mE, Czech Technical University in Prague, Prague, Czech Republic","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","5105","5112","Reinforcement learning (RL) is a suitable approach for controlling systems with unknown or time-varying dynamics. RL in principle does not require a model of the system, but before it learns an acceptable policy, it needs many unsuccessful trials, which real robots usually cannot withstand. It is well known that RL can be sped up and made safer by using models learned online. In this paper, we propose to use symbolic regression to construct compact, parsimonious models described by analytic equations, which are suitable for realtime robot control. Single node genetic programming (SNGP) is employed as a tool to automatically search for equations fitting the available data. We demonstrate the approach on two benchmark examples: a simulated mobile robot and the pendulum swing-up problem; the latter both in simulations and real-time experiments. The results show that through this approach we can find accurate models even for small batches of training data. Based on the symbolic model found, RL can control the system well.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8461182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461182","Model learning for control;AI-based methods;symbolic regression;reinforcement learning;optimal control","Mathematical model;Data models;Learning (artificial intelligence);Computational modeling;Mobile robots;Genetic programming","genetic algorithms;Internet;learning (artificial intelligence);mobile robots;pendulums;time-varying systems","time varying dynamics;controlling systems;data driven construction;online;single node genetic programming;SNGP;pendulum swing up problem;training data;accurate models;real-time experiments;simulated mobile robot;realtime robot control;analytic equations;parsimonious models;symbolic regression;acceptable policy;RL;reinforcement learning;symbolic process models","","6","","34","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Deep reinforcement learning for building HVAC control","T. Wei; Yanzhi Wang; Q. Zhu","University of California, Riverside; Syracuse University; University of California, Riverside","2017 54th ACM/EDAC/IEEE Design Automation Conference (DAC)","9 Oct 2017","2017","","","1","6","Buildings account for nearly 40% of the total energy consumption in the United States, about half of which is used by the HVAC (heating, ventilation, and air conditioning) system. Intelligent scheduling of building HVAC systems has the potential to significantly reduce the energy cost. However, the traditional rule-based and model-based strategies are often inefficient in practice, due to the complexity in building thermal dynamics and heterogeneous environment disturbances. In this work, we develop a data-driven approach that leverages the deep reinforcement learning (DRL) technique, to intelligently learn the effective strategy for operating the building HVAC systems. We evaluate the performance of our DRL algorithm through simulations using the widely-adopted EnergyPlus tool. Experiments demonstrate that our DRL-based algorithm is more effective in energy cost reduction compared with the traditional rule-based approach, while maintaining the room temperature within desired range.","","978-1-4503-4927-7","10.1145/3061639.3062224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8060306","","Buildings;Mathematical model;Training;Learning (artificial intelligence);Meteorology;Aerospace electronics;Neural networks","building management systems;buildings (structures);cost reduction;energy consumption;HVAC;learning (artificial intelligence)","deep reinforcement learning;building HVAC control;data-driven approach;DRL algorithm;EnergyPlus tool;intelligent scheduling;air conditioning;United States;total energy consumption;HVAC control;energy cost reduction","","6","","27","","9 Oct 2017","","","IEEE","IEEE Conferences"
"Near-Infrared-to-Visible Vein Imaging via Convolutional Neural Networks and Reinforcement Learning","V. M. Leli; A. Rubashevskii; A. Sarachakov; O. Rogov; D. V. Dylov","Skolkovo Institute of Science and Technology, Moscow, Russia; Skolkovo Institute of Science and Technology, Moscow, Russia; Skolkovo Institute of Science and Technology, Moscow, Russia; Skolkovo Institute of Science and Technology, Moscow, Russia; Skolkovo Institute of Science and Technology, Moscow, Russia","2020 16th International Conference on Control, Automation, Robotics and Vision (ICARCV)","8 Jan 2021","2020","","","434","441","Peripheral Difficult Venous Access (PDVA) is a commonplace problem in clinical practice which results in repetitive punctures, damaged veins, and significant discomfort to the patients. Nowadays, the poor visibility of subcutaneous vasculature in the visible part of the light spectrum is overcome by near-infrared (NIR) imaging and a returned projection of the recognized vasculature back to the arm of the patient. We introduce the first “smart” engine to govern the components of such imagers in a mixed reality setting. Namely, a closed-loop hardware system that optimizes cross-talk between the virtual mask generated from the NIR measurement and the projected augmenting image is proposed. Such real-virtual image translation is accomplished in several steps. First, the NIR vein segmentation task is solved using U-Net-based network architecture and the Frangi vesselness filter. The generated mask is then transformed and translated into the visible domain by a projector that adjusts for distortions and misalignment with the true vasculature using the paradigm of Reinforcement Learning (RL). We propose a new class of mixed reality reward functions that guarantees proper alignment of the projected image regardless of angle, translation, and scale offsets between the NIR measurement and the visible projection.","","978-1-7281-7709-0","10.1109/ICARCV50220.2020.9305503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305503","","Veins;Image segmentation;Training;Cameras;Computer architecture;Task analysis;Random access memory","biomedical optical imaging;blood vessels;convolutional neural nets;image segmentation;infrared imaging;learning (artificial intelligence);medical image processing;skin","NIR measurement;visible projection;infrared-to-visible vein imaging;convolutional neural networks;reinforcement learning;clinical practice;repetitive punctures;damaged veins;subcutaneous vasculature;visible part;light spectrum;near-infrared imaging;recognized vasculature;smart engine;mixed reality setting;closed-loop hardware system;virtual mask;projected augmenting image;real-virtual image translation;NIR vein segmentation task;network architecture;generated mask;visible domain;mixed reality reward;projected image;peripheral difficult venous access","","6","","41","IEEE","8 Jan 2021","","","IEEE","IEEE Conferences"
"Human-in-the-loop reinforcement learning","H. Liang; L. Yang; H. Cheng; W. Tu; M. Xu","Center for Robotics, University of Electronic Science and Technology of China; Center for Robotics, University of Electronic Science and Technology of China; Center for Robotics, University of Electronic Science and Technology of China; Center for Robotics, University of Electronic Science and Technology of China; Center for Robotics, University of Electronic Science and Technology of China","2017 Chinese Automation Congress (CAC)","1 Jan 2018","2017","","","4511","4518","This paper focuses on presenting a human-in-the-loop reinforcement learning theory framework and foreseeing its application to driving decision making. Currently, the technologies in human-vehicle collaborative driving face great challenges, and do not consider the Human-in-the-loop learning framework and Driving Decision-Maker optimization under the complex road conditions. The main content of this paper aimed at presenting a study framework as follows: (1) the basic theory and model of the hybrid reinforcement learning; (2) hybrid reinforcement learning algorithm for human drivers; (3)hybrid reinforcement learning algorithm for autopilot; (4) Driving decision-maker verification platform. This paper aims at setting up the human-machine hybrid reinforcement learning theory framework and foreseeing its solutions to two kinds of typical difficulties about human-machine collaborative Driving Decision-Maker, which provides the basic theory and key technologies for the future of intelligent driving. The paper serves as a potential guideline for the study of human-in-the-loop reinforcement learning.","","978-1-5386-3524-7","10.1109/CAC.2017.8243575","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8243575","Human-in-the-loop Reinforcement Learning;Driving Decision-Maker;Human-Driving","Learning (artificial intelligence);Man-machine systems;Intelligent vehicles;Heuristic algorithms;Algorithm design and analysis;Robots","decision making;driver information systems;learning (artificial intelligence);man-machine systems","Human-in-the-loop learning framework;study framework;human drivers;human-machine hybrid reinforcement learning theory framework;intelligent driving;human-in-the-loop reinforcement learning;face great challenges;hybrid reinforcement learning algorithm;Driving decision-maker verification platform;human-vehicle collaborative driving;human-machine collaborative driving decision-maker","","6","","45","IEEE","1 Jan 2018","","","IEEE","IEEE Conferences"
"Active Perception in Adversarial Scenarios using Maximum Entropy Deep Reinforcement Learning","M. Shen; J. P. How","Department of Mechanical Engineering, Massachusetts Institute of Technology (MIT), 77 Massachusetts Ave., Cambridge, MA, USA; Department of Aeronautics and Astronautics, MIT, 77 Massachusetts Ave., Cambridge, MA, USA","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","3384","3390","We pose an active perception problem where an autonomous agent actively interacts with a second agent with potentially adversarial behaviors. Given the uncertainty in the intent of the other agent, the objective is to collect further evidence to help discriminate potential threats. The main technical challenges are the partial observability of the agent intent, the adversary modeling, and the corresponding uncertainty modeling. Note that an adversary agent may act to mislead the autonomous agent by using a deceptive strategy that is learned from past experiences. We propose an approach that combines belief space planning, generative adversary modeling, and maximum entropy reinforcement learning to obtain a stochastic belief space policy. By accounting for various adversarial behaviors in the simulation framework and minimizing the predictability of the autonomous agent's action, the resulting policy is more robust to unmodeled adversarial strategies. This improved robustness is empirically shown against an adversary that adapts to and exploits the autonomous agent's policy when compared with a standard Chance-Constraint Partially Observable Markov Decision Process robust approach.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794389","","Uncertainty;Games;Autonomous agents;Reinforcement learning;Nash equilibrium;Planning;Adaptation models","belief networks;entropy;learning (artificial intelligence);multi-agent systems;neural nets;planning (artificial intelligence);stochastic processes","partial observability;adversary agent;autonomous agent;belief space planning;generative adversary modeling;maximum entropy reinforcement learning;stochastic belief space policy;unmodeled adversarial strategies;maximum entropy deep reinforcement learning;active perception problem;potentially adversarial behaviors;uncertainty modeling;standard chance-constraint partially observable Markov decision","","6","","26","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Multi-Modal Mutual Information (MuMMI) Training for Robust Self-Supervised Deep Reinforcement Learning","K. Chen; Y. Lee; H. Soh","Dept. of Computer Science, National University of Singapore; Dept. of Computer Science, National University of Singapore; Dept. of Computer Science, National University of Singapore","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","4274","4280","This work focuses on learning useful and robust deep world models using multiple, possibly unreliable, sensors. We find that current methods do not sufficiently encourage a shared representation between modalities; this can cause poor performance on downstream tasks and over-reliance on specific sensors. As a solution, we contribute a new multi-modal deep latent state-space model, trained using a mutual information lower-bound. The key innovation is a specially-designed density ratio estimator that encourages consistency between the latent codes of each modality. We tasked our method to learn policies (in a self-supervised manner) on multi-modal Natural MuJoCo benchmarks and a challenging Table Wiping task. Experiments show our method significantly outperforms state-of-the-art deep reinforcement learning methods, particularly in the presence of missing observations.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561187","Science and Engineering Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561187","","Training;Technological innovation;Conferences;Reinforcement learning;Robot sensing systems;Sensors;State-space methods","deep learning (artificial intelligence);reinforcement learning;sensors","multimodal deep latent state-space model;density ratio estimator;self-supervised manner;multimodal Natural MuJoCo benchmarks;multimodal mutual information training;robust self-supervised deep reinforcement learning;robust deep world models;downstream tasks;MuMMI training;useful world model learning;table wiping task","","6","","22","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Model-Based Probabilistic Pursuit via Inverse Reinforcement Learning","F. Shkurti; N. Kakodkar; G. Dudek","Mobile Robotics Lab, School of Computer Science at McGill University in Montreal, Canada; Mobile Robotics Lab, School of Computer Science at McGill University in Montreal, Canada; Mobile Robotics Lab, School of Computer Science at McGill University in Montreal, Canada","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","7804","7811","We address the integrated prediction, planning, and control problem that enables a single follower robot (the photographer) to quickly re-establish visual contact with a moving target (the subject) that has escaped the follower's field of view. We deal with this scenario, which reactive controllers are typically ill-equipped to handle, by making plausible predictions about the long- and short-term behavior of the target, and planning pursuit paths that will maximize the chance of seeing the target again. At the core of our pursuit method is the use of predictive models of target behavior, which help narrow down the set of possible future locations of the target to a few discrete hypotheses, as well as the use of combinatorial search in physical space to check those hypotheses efficiently. We model target behavior in terms of a learned navigation reward function, using Inverse Reinforcement Learning, based on semantic terrain features of satellite maps. Our pursuit algorithm continuously predicts the latent destination of the target and its position in the future, and relies on efficient graph representation and search methods in order to navigate to locations at which the target is most likely to be seen at an anticipated time. We perform extensive evaluation of our predictive pursuit algorithm over multiple satellite maps, thousands of simulation scenarios, against state-of-the art MDP and POMDP solvers. We show that our method significantly outperforms them by exploiting domain-specific knowledge, while being able to run in real-time.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8463196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463196","","Navigation;Planning;Trajectory;Visualization;Entropy;Predictive models;Learning (artificial intelligence)","decision theory;graph theory;learning (artificial intelligence);Markov processes;mobile robots;navigation;path planning;probability;search problems","control problem;single follower robot;visual contact;moving target;plausible predictions;predictive models;discrete hypotheses;combinatorial search;physical space;model target behavior;learned navigation reward function;semantic terrain features;search methods;predictive pursuit algorithm;multiple satellite maps;simulation scenarios;inverse reinforcement learning;long term behavior;short term behavior;planning pursuit paths;locations;graph representation;latent destination;position;POMDP solvers;domain specific knowledge;model based probabilistic pursuit","","6","","21","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning","Z. Qiao; J. Schneider; J. M. Dolan","Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, USA; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","2667","2673","For autonomous vehicles, effective behavior planning is crucial to ensure safety of the ego car. In many urban scenarios, it is hard to create sufficiently general heuristic rules, especially for challenging scenarios that some new human drivers find difficult. In this work, we propose a behavior planning structure based on reinforcement learning (RL) which is capable of performing autonomous vehicle behavior planning with a hierarchical structure in simulated urban environments. Application of the hierarchical structure [1] allows the various layers of the behavior planning system to be satisfied. Our algorithms can perform better than heuristic-rule-based methods for elective decisions such as when to turn left between vehicles approaching from the opposite direction or possible lane-change when approaching an intersection due to lane blockage or delay in front of the ego car. Such behavior is hard to evaluate as correct or incorrect, but some aggressive expert human drivers handle such scenarios effectively and quickly. On the other hand, compared to traditional RL methods, our algorithm is more sample-efficient, due to the use of a hybrid reward mechanism and heuristic exploration during the training process. The results also show that the proposed method converges to an optimal policy faster than traditional RL methods.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561095","","Training;Visualization;Heuristic algorithms;Urban areas;Reinforcement learning;Planning;Safety","driver information systems;learning (artificial intelligence);road traffic;vehicles","urban intersections;hierarchical reinforcement learning;autonomous vehicles;effective behavior planning;ego car;urban scenarios;sufficiently general heuristic rules;autonomous vehicle behavior planning;hierarchical structure;simulated urban environments;heuristic-rule-based methods;aggressive expert human drivers;traditional RL methods","","6","","28","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Cascade Flight Control of Quadrotors Based on Deep Reinforcement Learning","H. Han; J. Cheng; Z. Xi; B. Yao","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Robotics and Automation Letters","25 Aug 2022","2022","7","4","11134","11141","Numerous algorithms have been proposed for quadrotor flight control. Conventional methods require massive labor of parameter adjustment. Deep reinforcement learning (DRL) methods also need enormous computation and complicated hyperparameter tuning, since most of them regard the quadrotor dynamics as a black box. To overcome the drawbacks of both conventional and learning-based methods, this letter proposes a DRL-based cascade quadrotor flight controller. Under the small-angle restriction, the quadrotor dynamics are decomposed into six subsystems, each containing only one degree of freedom (DOF) for the agent to control. Six agents are sequentially trained to fully control the corresponding DOF without any prior knowledge of quadrotor dynamic parameters. Experiments show that, with a total training time of 17 min, the proposed controller could accomplish the fixed point tracking task with an error lower than 5 mm, rise time lower than 1.5 s, and peak value lower than 1%. The controller could also track time-variant trajectories.","2377-3766","","10.1109/LRA.2022.3196455","National Natural Science Foundation of China(grant numbers:62071104); NNSFC&CAAC(grant numbers:U2133211); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9850366","Deep reinforcement learning;flight control;quadrotor","Training;Attitude control;Vehicle dynamics;Process control;Heuristic algorithms;Artificial neural networks;Rotors","aircraft control;autonomous aerial vehicles;cascade control;control engineering computing;deep learning (artificial intelligence);helicopters;position control;reinforcement learning;robot dynamics","degree of freedom;small-angle restriction;quadrotor dynamic parameters;DRL-based cascade quadrotor flight controller;quadrotor dynamics;hyperparameter tuning;deep reinforcement learning methods;parameter adjustment;time 17.0 min;size 5.0 mm;time 1.5 s","","6","","29","IEEE","4 Aug 2022","","","IEEE","IEEE Journals"
"A coordinated multi-agent reinforcement learning approach to multi-level cache co-partitioning","R. Jain; P. R. Panda; S. Subramoney","Dept. of Computer Sc. and Engg., Indian Institute of Technology, Delhi; Dept. of Computer Sc. and Engg., Indian Institute of Technology, Delhi; Microarchitecture Research Lab, Intel Technology India Pvt. Ltd., Bangalore","Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017","15 May 2017","2017","","","800","805","The widening gap between the processor and memory performance has led to the inclusion of multiple levels of caches in the modern multi-core systems. Processors with simultaneous multithreading (SMT) support multiple hardware threads on the same physical core, which results in shared private caches. Any inefficiency in the cache hierarchy can negatively impact the system performance and motivates the need to perform a co-optimization of multiple cache levels by trading off individual application throughput for better system throughput and energy-delay-product (EDP). We propose a novel coordinated multiagent reinforcement learning technique for performing Dynamic Cache Co-partitioning, called Machine Learned Caches (MLC). MLC has low implementation overhead and does not require any special hardware data profilers. We have validated our proposal with 15 8-core workloads created using Spec2006 benchmarks and found it to be an effective co-partitioning technique. MLC exhibited system throughput and EDP improvements of up to 14% (gmean:9.35%) and 19.2% (gmean: 13.5%) respectively. We believe this is the first attempt at addressing the problem of multi-level cache co-partitioning.","1558-1101","978-3-9815370-8-6","10.23919/DATE.2017.7927098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7927098","","Sensitivity;Resource management;Throughput;Learning (artificial intelligence);Hardware;Multicore processing;Computational modeling","cache storage;learning (artificial intelligence);multi-agent systems","multilevel cache co-partitioning;simultaneous multithreading;SMT;cache hierarchy;system performance;cache level co-optimization;application throughput;energy-delay-product;coordinated multiagent reinforcement learning technique;dynamic cache co-partitioning;machine learned caches;MLC;EDP improvement","","6","","19","","15 May 2017","","","IEEE","IEEE Conferences"
"Hierarchies of Planning and Reinforcement Learning for Robot Navigation","J. Wöhlke; F. Schmitt; H. van Hoof","Bosch Center for Artificial Intelligence, Renningen, Germany; Bosch Center for Artificial Intelligence, Renningen, Germany; UvA-Bosch Delta Lab, University of Amsterdam, Amsterdam, Netherlands","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","10682","10688","Solving robotic navigation tasks via reinforcement learning (RL) is challenging due to their sparse reward and long decision horizon nature. However, in many navigation tasks, high-level (HL) task representations, like a rough floor plan, are available. Previous work has demonstrated efficient learning by hierarchal approaches consisting of path planning in the HL representation and using sub-goals derived from the plan to guide the RL policy in the source task. However, these approaches usually neglect the complex dynamics and sub-optimal sub-goal-reaching capabilities of the robot during planning. This work overcomes these limitations by proposing a novel hierarchical framework that utilizes a trainable planning policy for the HL representation. Thereby robot capabilities and environment conditions can be learned utilizing collected rollout data. We specifically introduce a planning policy based on value iteration with a learned transition model (VI-RL). In simulated robotic navigation tasks, VI-RL results in consistent strong improvement over vanilla RL, is on par with vanilla hierarchal RL on single layouts but more broadly applicable to multiple layouts, and is on par with trainable HL path planning baselines except for a parking task with difficult non-holonomic dynamics where it shows marked improvements.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561151","","Navigation;Layout;Reinforcement learning;Path planning;Data models;Planning;Task analysis","learning (artificial intelligence);mobile robots;navigation;path planning","trainable HL path planning;parking task;reinforcement learning;robot navigation;sparse reward;long decision horizon nature;high-level task representations;rough floor plan;hierarchal approaches;HL representation;using sub-goals;RL policy;source task;complex dynamics;sub-optimal sub-goal-reaching;novel hierarchical framework;trainable planning policy;robot capabilities;collected rollout data;learned transition model;simulated robotic navigation tasks;VI-RL results;consistent strong improvement;vanilla RL;vanilla hierarchal RL","","5","","48","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Multi-agent reinforcement learning with adaptive mimetism","T. Yamaguchi; M. Miura; M. Yachida","Department of Systems Engineering, Faculty of Engineering Science, Osaka University, Osaka, Japan; Department of Systems Engineering, Faculty of Engineering Science, Osaka University, Osaka, Japan; Department of Systems Engineering, Faculty of Engineering Science, Osaka University, Osaka, Japan","Proceedings 1996 IEEE Conference on Emerging Technologies and Factory Automation. ETFA '96","6 Aug 2002","1996","1","","288","294 vol.1","To learn the group behavior of a multi-agent system, it is important to selectively share the learning results in order to speed up learning without homogenizing the agents' behaviors. This paper describes a new method designed to permit multiple agents in an environment to learn cooperatively. The advantage of our method is to dynamically switch the learning mode between mimetism and reinforcement learning according to the situation. Mimetism seeks stability in group behavior, while individual reinforcement learning seeks the best solution. Accordingly, selective mimetism that allows the agents to partially share learning results works to prevent homogenization among the agents.","","0-7803-3685-2","10.1109/ETFA.1996.573308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=573308","","Learning;Switches;Systems engineering and theory;Multiagent systems;Design methodology;Stability;Autonomous agents;Computational efficiency;Convergence;Communication system control","cooperative systems","reinforcement learning;multiple agent system;adaptive mimetism;cooperative systems;stability;group behavior;learning system","","5","","15","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Asynchronous Reinforcement Learning Framework for Net Order Exploration in Detailed Routing","T. Qu; Y. Lin; Z. Lu; Y. Su; Y. Wei","University of Chinese Academy of Sciences, Beijing, China; CS Department, Peking University, Beijing, China; CS Department, Peking University, Beijing, China; Guangdong Greater Bay Area Applied Research Institute of Integrated Circuit and Systems, Guangdong, China; Guangdong Greater Bay Area Applied Research Institute of Integrated Circuit and Systems, Guangdong, China","2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)","16 Jul 2021","2021","","","1815","1820","The net orders in detailed routing are crucial to routing closure, especially in most modern routers following the sequential routing manner with the rip-up and reroute scheme. In advanced technology nodes, detailed routing has to deal with complicated design rules and large problem sizes, making its performance more sensitive to the order of nets to be routed. In literature, the net orders are mostly determined by simple heuristic rules tuned for specific benchmarks. In this work, we propose an asynchronous reinforcement learning (RL) framework to search for optimal ordering strategies automatically. By asynchronous querying the router and training the RL agents, we can generate highperformance routing sequences to achieve better solution quality.","1558-1101","978-3-9819263-5-4","10.23919/DATE51398.2021.9474007","National Natural Science Foundation of China(grant numbers:61874002,62004006); Beijing Municipal Natural Science Foundation(grant numbers:2017ZX02315001); National Key Research and Development Program of China(grant numbers:2019YFB2205005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474007","","Training;Correlation;Reinforcement learning;Benchmark testing;Network architecture;Routing","learning (artificial intelligence);optimisation;query processing;telecommunication network routing","net order exploration;detailed routing;net orders;routing closure;sequential routing manner;complicated design rules;asynchronous reinforcement learning framework;optimal ordering strategies;highperformance routing sequences","","5","","22","","16 Jul 2021","","","IEEE","IEEE Conferences"
"Air combat autonomous maneuver decision for one-on-one within visual range engagement base on robust multi-agent reinforcement learning","W. KONG; D. ZHOU; K. ZHANG; Z. YANG","Northwestern Polytechnical University, Xi’an, China; Northwestern Polytechnical University, Xi’an, China; Northwestern Polytechnical University, Xi’an, China; Northwestern Polytechnical University, Xi’an, China","2020 IEEE 16th International Conference on Control & Automation (ICCA)","30 Nov 2020","2020","","","506","512","Based on a robust multi-agent reinforcement learning (MARL) algorithm framework, an autonomous maneuver decision-making algorithm for UCAV air combat in one-on-one combat in the visible range is designed and implemented. This algorithm can solve the problem that the single agent reinforcement learning algorithm cannot converge during the training process due to the unstable environment. At the same time, considering the shortcomings of the MADDPG algorithm in a strong competitive environment, it is easy to obtain a very fragile strategy, which is only targeted at a specific equilibrium strategy. In this paper, a minimax module is introduced to obtain the expected perturbation, which can locally approach the worst-case perturbation through the gradient. Through simulation tests of algorithm convergence and policy quality, the algorithm is found to be effective.","1948-3457","978-1-7281-9093-8","10.1109/ICCA51439.2020.9264567","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264567","Robust MADDPG;Air combat;Reinforcement learning;Maneuver strategy","Reinforcement learning;Markov processes;Games;Atmospheric modeling;Aircraft;Training;Libraries","aerospace computing;decision making;learning (artificial intelligence);military aircraft;military computing;minimax techniques;multi-agent systems","minimax module;MARL algorithm;MADDPG algorithm;single agent reinforcement learning algorithm;visible range;UCAV air combat;autonomous maneuver decision-making algorithm;multiagent reinforcement learning algorithm framework;robust multiagent reinforcement learning;visual range engagement base;air combat autonomous maneuver decision","","5","","19","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Emergent Control of MPSoC Operation by a Hierarchical Supervisor / Reinforcement Learning Approach","F. Maurer; B. Donyanavard; A. M. Rahmani; N. Dutt; A. Herkersdorf","Technical University, Munich, Germany; University of California, Irvine, USA; University of California, Irvine, USA; University of California, Irvine, USA; Technical University, Munich, Germany","2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)","15 Jun 2020","2020","","","1562","1567","MPSoCs increasingly depend on adaptive resource management strategies at runtime for efficient utilization of resources when executing complex application workloads. In particular, conflicting demands for adequate computation performance and power-/energy-efficiency constraints make desired application goals hard to achieve. We present a hierarchical, cross-layer hardware/software resource manager capable of adapting to changing workloads and system dynamics with zero initial knowledge. The manager uses rule-based reinforcement learning classifier tables (LCTs) with an archive-based backup policy as leaf controllers. The LCTs directly manipulate and enforce MPSoC building block operation parameters in order to explore and optimize potentially conflicting system requirements (e.g., meeting a performance target while staying within the power constraint). A supervisor translates system requirements and application goals into per-LCT objective functions (e.g., core instructions-per-second (IPS). Thus, the supervisor manages the possibly emergent behavior of the low-level LCT controllers in response to 1) switching between operation strategies (e.g., maximize performance vs. minimize power; and 2) changing application requirements. This hierarchical manager leverages the dual benefits of a software supervisor (enabling flexibility), together with hardware learners (allowing quick and efficient optimization). Experiments on an FPGA prototype confirmed the ability of our approach to identify optimized MPSoC operation parameters at runtime while strictly obeying given power constraints.","1558-1101","978-3-9819263-4-7","10.23919/DATE48585.2020.9116574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9116574","Backup-based reinforcement machine learning;MPSoC runtime management;hierarchical reflective control","Runtime;Optimization;Reinforcement learning;Linear programming;Control systems;Hardware","distributed processing;knowledge based systems;learning (artificial intelligence);resource allocation;system-on-chip","adaptive resource management;power constraint;low-level LCT controllers;software supervisor;MPSoC control;energy efficiency constraints;rule-based reinforcement learning classifier tables;hardware learners;distributed LCTs","","5","","19","","15 Jun 2020","","","IEEE","IEEE Conferences"
"A robust stability approach to robot reinforcement learning based on a parameterization of stabilizing controllers","S. R. Friedrich; M. Buss","TUM Institute for Advanced Study, Garching, Germany; TUM Institute for Advanced Study, Garching, Germany","2017 IEEE International Conference on Robotics and Automation (ICRA)","24 Jul 2017","2017","","","3365","3372","Reinforcement learning has become more and more popular in robotics for acquiring feedback controllers. Many approaches aim for learning a controller from scratch, i.e., data-driven without any modeling of the physical plant. However, stability properties of the closed loop are often not considered, or established only a-posteriori or ad hoc. We propose to employ reinforcement learning in the context of model-based control, allowing to learn in a framework of stabilizing controllers built by using only little prior model knowledge. This way, the action space is suitably structured for safe learning of a feedback controller to compensate for uncertainties due to model mismatch or external disturbances. The resulting scheme is developed around a decentralized PD feedback controller. Therefore, given such a controller, by the proposed method one can also add a learning module for performance enhancement. We demonstrate our approach both in simulation and in a hardware experiment using a two degree of freedom robot manipulator.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989382","","Learning (artificial intelligence);Adaptive control;Stability analysis;Robot control;Adaptation models;Standards","feedback;learning (artificial intelligence);manipulators;robust control","robust stability approach;robot reinforcement learning;stabilizing controllers;controller stabilization;feedback controller;model-based control;robot manipulator","","5","","21","IEEE","24 Jul 2017","","","IEEE","IEEE Conferences"
"Dynamic Thermal Management with Proactive Fan Speed Control Through Reinforcement Learning","A. Iranfar; F. Terraneo; G. Csordas; M. Zapater; W. Fornaciari; D. Atienza","Embedded Systems Laboratory (ESL), Swiss Federal Institute of Technology Lausanne (EPFL), Switzerland; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Italy; Embedded Systems Laboratory (ESL), Swiss Federal Institute of Technology Lausanne (EPFL), Switzerland; Embedded Systems Laboratory (ESL), Swiss Federal Institute of Technology Lausanne (EPFL), Switzerland; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Italy; Embedded Systems Laboratory (ESL), Swiss Federal Institute of Technology Lausanne (EPFL), Switzerland","2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)","15 Jun 2020","2020","","","418","423","Dynamic Thermal Management (DTM) has become a major challenge since it directly affects Multiprocessors Systems-on-chip (MPSoCs) performance, power consumption, and reliability. In this work, we propose a transient fan model, enabling adaptive fan speed control simulation for efficient DTM. Our model is validated through a thermal test chip achieving less than 2°C error in the worst case. With multiple fan speeds, however, the DTM design space grows significantly, which can ultimately make conventional solutions impractical. We address this challenge through a reinforcement learning-based solution to proactively determine the number of active cores, operating frequency, and fan speed. The proposed solution is able to reduce fan power by up to 40% compared to a DTM with constant fan speed with less than 1% performance degradation. Also, compared to a state-of-the-art DTM technique our solution improves the performance by up to 19% for the same fan power.","1558-1101","978-3-9819263-4-7","10.23919/DATE48585.2020.9116510","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9116510","","Fans;Mathematical model;Heat sinks;Atmospheric modeling;Velocity control;Adaptation models;Transient analysis","fans;learning (artificial intelligence);microprocessor chips;multiprocessing systems;power aware computing;power consumption;system-on-chip;temperature control;thermal management (packaging);velocity control","thermal test chip;multiple fan speeds;conventional solutions;reinforcement learning-based solution;fan power;constant fan speed;DTM technique;dynamic thermal management;proactive fan speed control;power consumption;transient fan model;adaptive fan speed control simulation;multiprocessors systems-on-chip performance;MPSoCs","","5","","32","","15 Jun 2020","","","IEEE","IEEE Conferences"
"Early Failure Detection of Deep End-to-End Control Policy by Reinforcement Learning","K. Lee; K. Saigol; E. A. Theodorou","Georgia Institute of Technology, Atlanta, GA, USA; Lyft Inc, San Francisco, CA, USA; Georgia Institute of Technology, Atlanta, GA, USA","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","8543","8549","We propose the use of Bayesian networks, which provide both a mean value and an uncertainty estimate as output, to enhance the safety of learned control policies under circumstances in which a test-time input differs significantly from the training set. Our algorithm combines reinforcement learning and end-to-end imitation learning to simultaneously learn a control policy as well as a threshold over the predictive uncertainty of the learned model, with no hand-tuning required. Corrective action, such as a return of control to the model predictive controller or human expert, is taken before the failure of tasks, when the uncertainty threshold is exceeded. We validate our method on fully-observable and vision-based partially-observable systems using cart-pole and autonomous driving simulations using deep convolutional Bayesian neural networks. We demonstrate that our method is robust to uncertainty resulting from varying system dynamics as well as from partial state observability.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794189","","Uncertainty;Bayes methods;Task analysis;Neural networks;Safety;Training;Autonomous vehicles","belief networks;control engineering computing;convolutional neural nets;learning (artificial intelligence);learning systems;observability;predictive control","learned control policies;reinforcement learning;end-to-end imitation;predictive uncertainty;model predictive controller;fully-observable vision-based partially-observable systems;deep convolutional Bayesian neural networks;deep end-to-end control policy;Bayesian networks;mean value;corrective action;partial state observability","","5","","19","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning of Robotic Prosthesis for Gait Symmetry in Trans-Femoral Amputated Patients","N. Sacchi; G. P. Incremona; A. Ferrara","Dipartimento di Informatica Bioingegneria, Robotica e Ingegneria dei Sistemi, University of Genova, Genova, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy; Dipartimento di Ingegneria Industriale e Dell’Informazione, University of Pavia, Pavia, Italy","2021 29th Mediterranean Conference on Control and Automation (MED)","15 Jul 2021","2021","","","723","728","This work proposes a novel control methodology to achieve gait symmetry in trans-femoral amputated patients with prostheses. The proposed approach allows to overcome the limits of classical model-based control strategies by introducing a Deep Reinforcement Learning (DRL) method trained ad hoc for generating the velocity control signals fed into the active lower-limb robotic prosthesis. More specifically, the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm is used to concurrently learn a Q-function and the best policy. The proposal has the advantages of being model-free and capable of adapting to different walking velocities, just requiring few measurements and without the need to online re-tune the control parameters when the human motions change. The proposed model-free approach has been tested in a realistic scenario simulated in the CoppeliaSim environment relying on gait patterns retrieved experimentally by means of markers placed on a human subject.","2473-3504","978-1-6654-2258-1","10.1109/MED51440.2021.9480224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9480224","","Legged locomotion;Training;Adaptation models;Velocity control;Reinforcement learning;Angular velocity;Velocity measurement","gait analysis;learning (artificial intelligence);medical control systems;medical robotics;patient rehabilitation;prosthetics;velocity control","Deep Reinforcement Learning;gait symmetry;trans-femoral amputated patients;novel control methodology;classical model-based control strategies;velocity control signals;lower-limb robotic prosthesis;Twin Delayed Deep Deterministic Policy Gradient algorithm;different walking velocities;control parameters;model-free approach;gait patterns","","5","","30","IEEE","15 Jul 2021","","","IEEE","IEEE Conferences"
"Skip Training for Multi-Agent Reinforcement Learning Controller for Industrial Wave Energy Converters","S. Sarkar; V. Gundecha; S. Ghorbanpour; A. Shmakov; A. R. Babu; A. Pichard; M. Cocho","Hewlett Packard Enterprise, USA; Hewlett Packard Enterprise, USA; Hewlett Packard Enterprise, USA; Hewlett Packard Enterprise, USA; Hewlett Packard Enterprise, USA; Carnegie Clean Energy, Australia; Carnegie Clean Energy, Australia","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","212","219","Recent Wave Energy Converters (WEC) are equipped with multiple legs and generators to maximize energy generation. Traditional controllers have shown limitations to capture complex wave patterns and the controllers must efficiently maximize the energy capture. This paper introduces a Multi-Agent Reinforcement Learning controller (MARL), which outperforms the traditionally used spring damper controller. Our initial studies show that the complex nature of problems makes it hard for training to converge. Hence, we propose a novel ""skip training"" approach which enables the MARL training to overcome performance saturation and converge to more optimum controllers compared to default MARL training, boosting power generation. We also present another novel hybrid training initialization (STHTI) approach, where the individual agents of the MARL controllers can be initially trained against the baseline Spring Damper (SD) controller individually and then be trained one agent at a time or all together in future iterations to accelerate convergence. We achieved double-digit gains in energy efficiency over the baseline Spring Damper controller with the proposed MARL controllers using the Asynchronous Advantage Actor-Critic (A3C) algorithm.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926561","","Training;Reinforcement learning;Computer architecture;Wind power generation;Shock absorbers;Generators;Springs","energy conservation;multi-agent systems;optimal control;power engineering computing;power generation control;reinforcement learning;springs (mechanical);vibration control;wave power generation","energy generation;complex wave patterns;skip training approach;optimum controllers;power generation;hybrid training initialization approach;MARL controllers;energy efficiency;multiagent reinforcement learning controller;industrial wave energy converters;WEC;spring damper controller;STHTI;asynchronous advantage actor-critic algorithm","","5","","32","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"Continual Model-Based Reinforcement Learning with Hypernetworks","Y. Huang; K. Xie; H. Bharadhwaj; F. Shkurti","Division of Engineering Science, University of Toronto, Canada; Department of Computer Science, University of Toronto, Canada; Department of Computer Science, University of Toronto, Canada; Department of Computer Science, University of Toronto, Canada","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","799","805","Effective planning in model-based reinforcement learning (MBRL) and model-predictive control (MPC) relies on the accuracy of the learned dynamics model. In many instances of MBRL and MPC, this model is assumed to be stationary and is periodically re-trained from scratch on state transition experience collected from the beginning of environment interactions. This implies that the time required to train the dynamics model - and the pause required between plan executions - grows linearly with the size of the collected experience. We argue that this is too slow for lifelong robot learning and propose HyperCRL, a method that continually learns the encountered dynamics in a sequence of tasks using task-conditional hypernetworks. Our method has three main attributes: first, it includes dynamics learning sessions that do not revisit training data from previous tasks, so it only needs to store the most recent fixed-size portion of the state transition experience; second, it uses fixed-capacity hypernetworks to represent non-stationary and task-aware dynamics; third, it outperforms existing continual learning alternatives that rely on fixed-capacity networks, and does competitively with baselines that remember an ever increasing coreset of past experience. We show that HyperCRL is effective in continual model-based reinforcement learning in robot locomotion and manipulation scenarios, such as tasks involving pushing and door opening. Our project website with videos is at this link http://rvl.cs.toronto.edu/blog/2020/hypercrl/","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560793","","Training;Conferences;Training data;Reinforcement learning;Switches;Robot learning;Planning","control engineering computing;manipulators;motion control;network theory (graphs);optimal control;predictive control;reinforcement learning","model-based reinforcement learning;MBRL;model-predictive control;MPC;task-conditional hypernetworks;fixed-capacity hypernetworks;task-aware dynamics;HyperCRL;robot locomotion;robot manipulation","","5","","44","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Adaptive Informative Path Planning Using Deep Reinforcement Learning for UAV-based Active Sensing","J. Rückin; L. Jin; M. Popović","Cluster of Excellence PhenoRob, Institute of Geodesy and Geoinformation, University of Bonn; Cluster of Excellence PhenoRob, Institute of Geodesy and Geoinformation, University of Bonn; Cluster of Excellence PhenoRob, Institute of Geodesy and Geoinformation, University of Bonn","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","4473","4479","Aerial robots are increasingly being utilized for environmental monitoring and exploration. However, a key challenge is efficiently planning paths to maximize the information value of acquired data as an initially unknown environment is explored. To address this, we propose a new approach for informative path planning based on deep reinforcement learning (RL). Combining recent advances in RL and robotic applications, our method combines tree search with an offline-learned neural network predicting informative sensing actions. We introduce several components making our approach applicable for robotic tasks with high-dimensional state and large action spaces. By deploying the trained network during a mission, our method enables sample-efficient online replanning on platforms with limited computational resources. Simulations show that our approach performs on par with existing methods while reducing runtime by 8-10×. We validate its performance using real-world surface temperature data.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812025","Deutsche Forschungsgemeinschaft(grant numbers:2070 - 390732324); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812025","","Temperature sensors;Temperature distribution;Runtime;Neural networks;Reinforcement learning;Robot sensing systems;Path planning","autonomous aerial vehicles;learning (artificial intelligence);mobile robots;neural nets;path planning;remotely operated vehicles;tree searching","adaptive informative path planning;deep reinforcement learning;UAV-based active sensing;aerial robots;environmental monitoring;information value;initially unknown environment;RL;robotic applications;offline-learned neural network;informative sensing actions;approach applicable;robotic tasks;high-dimensional state;large action;trained network;sample-efficient online replanning","","5","","32","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Improving Model-Based Balance Controllers Using Reinforcement Learning and Adaptive Sampling","V. C. V. Kumar; S. Ha; K. Yamane","Disney Research, Georgia Institute of Technology; Disney Research, Pdivision; Disney Research, Pdivision","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","7541","7547","Balance control to recover from a wide range of disturbances is an important skill for humanoid robots. Traditionally, researchers have often designed a balance controller by applying optimal control theory on a simplified model that abstracts the full-body dynamics. However, the resulting controller may not be able to recover from unexpected scenarios such as non-planar pushes, or fail to exploit full-body actions such as balancing with arm movements. This paper presents a learning framework for enhancing the performance of a model-based optimal controller by expanding the region of attraction (RoA). We train a control policy that generates additional control signals on top of the model-based controller using deep reinforcement learning techniques. Instead of relying on standard reinforcement learning formulations, we explicitly model the region of attraction and continuously adjust it during the training. By drawing the training disturbances at the boundary of the RoA, we can effectively expand the RoA while avoiding local minima. We test our learning framework for in-place balancing as well as balancing with stepping on a humanoid model in simulation.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8463209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463209","","Perturbation methods;Hip;Adaptation models;Robots;Training;Lips;Learning (artificial intelligence)","humanoid robots;learning (artificial intelligence);optimal control;sampling methods","control signals;adaptive sampling;model-based balance controllers;humanoid model;in-place balancing;training disturbances;standard reinforcement learning formulations;deep reinforcement learning techniques;control policy;RoA;model-based optimal controller;learning framework;full-body actions;nonplanar pushes;full-body dynamics;optimal control theory","","4","","28","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Motion Planning in Human Robot cooperative Scenarios","G. Nicola; S. Ghidoni","Department of information Engineering, University of Padova, Padova, Italy; Department of information Engineering, University of Padova, Padova, Italy","2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA )","30 Nov 2021","2021","","","01","07","In this paper we tackle motion planning in industrial human-robot cooperative scenarios modeled as a reinforcement learning problem solved in a simulated environment. The agent learns the most effective policy to reach the designated target position while avoiding collisions with a human, performing a pick and place task in the robot workspace, and with fixed obstacles. The policy acts as a feedback motion planner (or reactive motion planner), therefore at each time-step it senses the surrounding environment and computes the action to be performed. In this work a novel formulation of the action that guarantees the trajectory derivatives continuity is proposed to create smooth trajectories that are necessary for maximizing the human trust in the robot. The action is defined as the sub-trajectory the agent must follow for the duration of a time-step, therefore the complete trajectory is the concatenation of all the trajectories computed at each time-step. The proposed method does not require to infer the action the human is currently performing and/or foresee the space occupied by the human. Indeed, during the training phase in a simulated environment the agent experience how the human behaves in the specific scenario, therefore it learns the policy that best adapts to the human actions and movements. The proposed method is finally applied in a scenario of human-robot cooperative pick and place.","","978-1-7281-2989-1","10.1109/ETFA45728.2021.9613505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9613505","Human Robot Cooperation;Motion Planning;Deep Reinforcement Learning","Training;Service robots;Conferences;Reinforcement learning;Robot sensing systems;Trajectory;Planning","collision avoidance;feedback;human-robot interaction;learning (artificial intelligence);mobile robots;motion control;multi-robot systems;trajectory control","simulated environment;designated target position;pick and place task;robot workspace;fixed obstacles;feedback motion planner;smooth trajectories;human actions;deep reinforcement learning;human robot cooperative scenarios;collision avoidance","","4","","21","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Variable Speed Limit Control for Mixed Traffic Flows","F. Vrbanić; E. Ivanjko; S. Mandžuka; M. Miletić","Faculty of Transport and Traffic Sciences, University of Zagreb, Zagreb, Croatia; Faculty of Transport and Traffic Sciences, University of Zagreb, Zagreb, Croatia; Faculty of Transport and Traffic Sciences, University of Zagreb, Zagreb, Croatia; Faculty of Transport and Traffic Sciences, University of Zagreb, Zagreb, Croatia","2021 29th Mediterranean Conference on Control and Automation (MED)","15 Jul 2021","2021","","","560","565","Today's urban mobility requires results for resolving increasingly complex demands on the traffic management system. Hence, the main problem is to achieve a satisfactory level of service for urban motorways as part of the urban traffic network. In addition, with the introduction of Connected and Autonomous Vehicles (CAVs), additional challenges for modern control systems arise. This study focuses on the Variable Speed Limit (VSL) based on Q-Learning with CAVs as actuators in the control loop. The Q-Learning algorithm is combined with the two-step Temporal Difference target to increase the effectiveness of the algorithm for learning the VSL control policy for mixed traffic flows. Different CAV penetration rates are analyzed, and the results are compared with a rule-based VSL and the no control case. The obtained results show that Q-Learning based VSL can learn the control policy and improve the Total Travel Time and Mean Travel Time for different CAV penetration rates. The results are most apparent in the case of low CAV penetration rates. There is also an indication that the increase of the CAV penetration rate reduces the need for separate VSL control.","2473-3504","978-1-6654-2258-1","10.1109/MED51440.2021.9480215","European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9480215","","Actuators;Sensitivity analysis;Simulation;Microscopy;Neural networks;Reinforcement learning;Control systems","learning (artificial intelligence);road traffic control;traffic engineering computing;velocity control","connected and autonomous vehicles;q-learning;mean travel time;total travel time;VSL control policy;two-step Temporal Difference target;control loop;modern control systems;urban traffic network;urban motorways;traffic management system;urban mobility;mixed traffic flows;variable speed limit control;VSL control;CAV penetration rate;low CAV penetration rates;rule-based VSL","","4","","28","IEEE","15 Jul 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Train Driving Optimization","J. Huang; E. Zhang; J. Zhang; S. Huang; Z. Zhong","dept. School of Vehicle and Mobility, Tsinghua University, Beijing, China; dept. School of Software, Tsinghua University, Beijing, China; dept. Qian Xuesen College, Xi’an Jiaotong University, Xi’an, China; dept. School of Software, Tsinghua University, Beijing, China; dept. School of Vehicle and Mobility, Tsinghua University, Beijing, China","2019 Chinese Automation Congress (CAC)","13 Feb 2020","2019","","","2375","2381","In contemporary society, railway networks are usually considered among the most critical undertakers of transportation tasks. Thanks to the born-with preferential conditions of the railway systems, in particular the settings and circumstances in which they run, railroad trains are best positioned to become automatic, vis-vis other transportation systems, such as road traffic. Having said that, one must admit that the transformation towards intelligent man-less train driving system will not be easy, given the requirements in high dimensionality, non-linearity, multi-origin restraints, and other factors that may change over time. Challenges, including energy efficiency, on-time performance, and safety, need to be addressed. The authors of the present paper used a deep reinforcement learning (DRL) method to generate energy-optimized train driving solutions. The neural network representation is based on a Bidirectional Long-Short-Term Memory (BLSTM) and an automatic encoder, which integrate sequential properties of the time-series and inherent non-sequential attributes. It is believed that the learned model has value in assisting decision makers in some representative cases of industrial control, as experiments show that this is perfectly adaptable to meet the basic safety, energy saving, and punctuality requirements in train driving.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8996988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8996988","train driving;optimization;automatic driving;deep reinforcement learning","Gears;Optimization;Rail transportation;Energy consumption;Resistance;Machine learning;Safety","learning (artificial intelligence);railways;recurrent neural nets;time series;transportation","train driving optimization;railway networks;railway systems;railroad trains;transportation systems;energy-optimized train driving solutions;neural network representation;automatic encoder;time series;deep reinforcement learning;bidirectional long-short-term memory","","4","","17","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Approach for Multi-UAV Cooperative Searching in Unknown Environments","W. Yue; X. Guan; Y. Xi","College of Marine Electrical Engineering, Dalian Maritime University, Dalian, China; College of Marine Electrical Engineering, Dalian Maritime University, Dalian, China; College of Marine Electrical Engineering, Dalian Maritime University, Dalian, China","2019 Chinese Automation Congress (CAC)","13 Feb 2020","2019","","","2018","2023","In this paper, an important topic of cooperative search for multi-dynamic targets in unknown sea area by unmanned aerial vehicles (UAVs) is studied based on reinforcement learning (RL) algorithm. First of all, considering the model of environmental, UAVs dynamics, target dynamics and sensor detection, the multi-UAVs sea area search map is established, then, the search map is updated by using the concept of “Territory awareness information map and the original search map is expanded. Finally, according to the search efficiency function, a reward and punishment function is designed, and RL method is used to generate a multi-UAVs cooperative search path on-line. The simulation results show that, according to the proposed algorithm, UAVs can effectively perform the search task in the sea area with no prior information.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8997436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8997436","Multi-UAVs;cooperative search;reinforcement learning;dynamic target","Manganese;Search problems;Task analysis;Learning (artificial intelligence);Vehicle dynamics;Unmanned aerial vehicles;Uncertainty","autonomous aerial vehicles;control engineering computing;learning (artificial intelligence);mobile robots","multiUAV cooperative searching;reward and punishment function;sensor detection;territory awareness information map;search efficiency function;multiUAVs sea area search map;UAVs dynamics;reinforcement learning algorithm;unmanned aerial vehicles;multidynamic targets","","4","","13","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Multi-Agent Reinforcement Learning for Adaptive Routing: A Hybrid Method using Eligibility Traces","S. Zeng; X. Xu; Y. Chen","The Chinese University of Hong Kong, Shenzhen; The Chinese University of Hong Kong, Shenzhen; The Chinese University of Hong Kong, Shenzhen","2020 IEEE 16th International Conference on Control & Automation (ICCA)","30 Nov 2020","2020","","","1332","1339","Packet routing in communication networks is a natural problem for sequential decision-making. Previously, many heuristic methods are proposed based on domain knowledge. Most of them rely on the understanding of the network environment and the algorithms are sensitive to the model of the networks. However, in practice it is intractable to model the dynamics of a real network perfectly, since many features of the network such as traffic load and network topology are irregularly changing with time and the changes are hard to predict. Hence, model-free approach becomes more promising in dealing with a complex dynamic environment. In this work, we consider a model-free method by leveraging reinforcement learning techniques. We propose a multi-agent reinforcement learning framework for adaptive routing in communication networks, which takes advantage of both the real-time Q-learning and the actor-critic methods. Provided a global feedback signal, the routers (agents) act independently but are able to learn cooperative behaviors to reduce packet delivery time. Our algorithm is robust to some dynamic changes in the network and each agent learns an adaptive policy to route packets. Simulation results demonstrate that our proposed algorithm outperforms some existing benchmark algorithms.","1948-3457","978-1-7281-9093-8","10.1109/ICCA51439.2020.9264518","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264518","","Routing;Routing protocols;Load modeling;Reinforcement learning;Heuristic algorithms;Gradient methods;Delays","cooperative communication;feedback;learning (artificial intelligence);multi-agent systems;telecommunication computing;telecommunication network routing;telecommunication network topology","multiagent reinforcement learning;adaptive routing;eligibility traces;packet routing;communication networks;sequential decision-making;heuristic methods;domain knowledge;network environment;traffic load;model-free approach;complex dynamic environment;model-free method;reinforcement learning techniques;actor-critic methods;packet delivery time;dynamic changes;adaptive policy;route packets;real-time Q-learning","","4","","14","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Relative Distributed Formation and Obstacle Avoidance with Multi-agent Reinforcement Learning","Y. Yan; X. Li; X. Qiu; J. Qiu; J. Wang; Y. Wang; Y. Shen","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Shanghai AI Laboratory, Shanghai, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","1661","1667","Multi-agent formation as well as obstacle avoid-ance is one of the most actively studied topics in the field of multi-agent systems. Although some classic controllers like model predictive control (MPC) and fuzzy control achieve a certain measure of success, most of them require precise global information which is not accessible in harsh environments. On the other hand, some reinforcement learning (RL) based approaches adopt the leader-follower structure to organize different agents' behaviors, which sacrifices the collaboration between agents thus suffering from bottlenecks in maneuver-ability and robustness. In this paper, we propose a distributed formation and obstacle avoidance method based on multi-agent reinforcement learning (MARL). Agents in our system only utilize local and relative information to make decisions and control themselves distributively, and will reorganize themselves into a new topology quickly in case that any of them is dis-connected. Our method achieves better performance regarding formation error, formation convergence rate and on-par success rate of obstacle avoidance compared with baselines (both classic control methods and another RL-based method). The feasibility of our method is verified by both simulation and hardware implementation with Ackermann-steering vehicles.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812263","National Key R&D Program of China(grant numbers:2020YFC1511803); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812263","","Geometry;Adaptation models;Reinforcement learning;Hardware;Robustness;Topology;Collision avoidance","collision avoidance;fuzzy control;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;predictive control","obstacle avoidance;classic control methods;RL-based method;relative distributed formation;multiagent reinforcement learning;multiagent formation;obstacle avoid-ance;actively studied topics;multiagent systems;classic controllers;model predictive control;fuzzy control;precise global information;reinforcement learning based approaches;different agents;local information;relative information;performance regarding formation error;formation convergence rate","","4","","39","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Joint QoS Control and Bitrate Selection for Video Streaming based on Multi-agent Reinforcement Learning","H. Jin; Q. Wang; S. Li; J. Chen","University of Electronic Science and Technology of China, Chengdu; University of Electronic Science and Technology of China, Chengdu; University of Electronic Science and Technology of China, Chengdu; University of Electronic Science and Technology of China, Chengdu","2020 IEEE 16th International Conference on Control & Automation (ICCA)","30 Nov 2020","2020","","","1360","1365","Limited network resource and explosive growth in video services have made the networks resource scheduling and adaptive video streaming control even more important for the multi-users quality of experience (QoE) within the cell. The adaptive streaming technology enables the media player to dynamically select the most suitable encoding bitrate to satisfy the users QoE. To achieve the highest possible quality of service (QoS), network nodes QoS control, aka downstream bandwidth configuration, can be dynamically optimized to improve the utilization of bandwidth resources. In this paper, we propose a joint QoS control and adaptive bitrate (ABR) algorithm based on multi-agent reinforcement learning with asynchronous advantage actor-critic (MARL-A3C). The proposed method can process video streams from multiple user nodes simultaneously and adapt to different indicators of QoS and QoE. Under the predetermined QoE standard, the algorithm dynamically controls the maximum downlink rate for the video streaming delivery links, and dynamically selects the encoding bitrate of the video in an active manner for multiple users. The MARL algorithm learns the reward value function of each user state through continuous interaction with the environment and then learns the optimal strategy through these reward value functions. We evaluate the proposed MARL-A3C algorithm on a simulation platform of a fog radio access point (F-RAN) system and compare it with the state-of-art ABR algorithms. The experiment results show that the MARL-A3C outperforms the existing methods in the multi-user F-RAN scenario.","1948-3457","978-1-7281-9093-8","10.1109/ICCA51439.2020.9264312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264312","","Bit rate;Streaming media;Quality of experience;Quality of service;Servers;Bandwidth;Reinforcement learning","learning (artificial intelligence);multi-agent systems;quality of experience;quality of service;scheduling;telecommunication computing;video coding;video streaming","joint QoS control;bitrate selection;video streaming;multiagent reinforcement learning;video services;network resource scheduling;multiusers quality;adaptive bitrate algorithm;asynchronous advantage actor-critic;MARL-A3C;MARL algorithm;reward value function;ABR algorithms;multiuser F-RAN scenario;adaptive video streaming control;downstream bandwidth configuration;fog radio access point system","","4","","18","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Gaussian processes for informative exploration in reinforcement learning","J. J. Chung; N. R. J. Lawrance; S. Sukkarieh","Australian Centre for Field Robotics, School of Aerospace, Mechanical and MechatronicEngineering, University of Sydney, NSW, Australia; Australian Centre for Field Robotics, School of Aerospace, Mechanical and MechatronicEngineering, University of Sydney, NSW, Australia; Australian Centre for Field Robotics, School of Aerospace, Mechanical and MechatronicEngineering, University of Sydney, NSW, Australia","2013 IEEE International Conference on Robotics and Automation","17 Oct 2013","2013","","","2633","2639","This paper presents the iGP-SARSA(λ) algorithm for temporal difference reinforcement learning (RL) with non-myopic information gain considerations. The proposed algorithm uses a Gaussian process (GP) model to approximate the state-action value function, Q, and incorporates the variance measure from the GP into the calculation of the discounted information gain value for all future state-actions rolled out from the current state-action. The algorithm was compared against a standard SARSA(λ) algorithm on two simulated examples: a battery charge/discharge problem, and a soaring glider problem. Results show that incorporating the information gain value into the action selection encouraged exploration early on, allowing the iGP-SARSA(λ) algorithm to converge to a more profitable reward cycle, while the e-greedy exploration strategy in the SARSA(λ) algorithm failed to search beyond the local optimal solution.","1050-4729","978-1-4673-5643-5","10.1109/ICRA.2013.6630938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630938","","Function approximation;Training;Tiles;Batteries;Approximation algorithms;Discharges (electric)","Gaussian processes;learning (artificial intelligence)","Gaussian process;informative exploration;iGP-SARSA algorithm;temporal difference reinforcement learning;nonmyopic information gain;state-action value function;battery discharge problem;soaring glider problem;ε-greedy exploration strategy","","4","","15","IEEE","17 Oct 2013","","","IEEE","IEEE Conferences"
"Remote-Center-of-Motion Recommendation toward Brain Needle Intervention Using Deep Reinforcement Learning","H. Gao; X. Xiao; L. Qiu; M. Q. . -H. Meng; N. K. K. King; H. Ren","Department of Biomedical Engineering, National University of Singapore, Singapore; Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Biomedical Engineering, National University of Singapore, Singapore; Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Surgery, National University of Singapore, Singapore; Department of Electronic Engineering, The Chinese University of Hong Kong","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","8295","8301","Brain needle intervention is a specific diagnosis and therapy procedure in brain disorders, such as brain tumors and Parkinson’s disease. Preoperative needle path planning is a vital step to guarantee the patient’s safety and reduce lesions. For positioning accuracy in the CT/MRI environment, we have developed a novel needle intervention robot in our previous work. Because the robot is currently designed for the rigid needle, the task of preoperative path-planning is to search for an optimal Remote Center of Motion (RCM) for needle insertion. Therefore, this work proposes an RCM recommendation system using deep reinforcement learning. Considering the robot kinematics, this system takes the following criteria/constraints into consideration: clinical obstacle (blood vessels, tissues) avoidance (COA), mechanically inverse kinematics (MIK) and mechanically less motion (MLM) for the robot. We design a reward function to combine the above three criteria based on their corresponding importance level and utilize proximal policy optimization (PPO) as the main agent of reinforcement learning (RL). RL methods are proved to be competent in searching the RCM, which satisfies the above criteria simultaneously. On the one hand, the results present that RL agents obtain the success rate of finishing the designed task at 93%, which has reached the human level in the tests. On the other hand, the RL agents have the remarkable capability of combining more complex criteria/constraints in future work.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560747","Remote Center of Motion (RCM);recommendation system;rigid needle;path planning;intervention robot;criteria (COA;MIK;MLM);deep reinforcement learning","Robot kinematics;Medical treatment;Reinforcement learning;Kinematics;Needles;Path planning;Safety","biological tissues;biomedical MRI;blood vessels;brain;computerised tomography;diseases;learning (artificial intelligence);medical image processing;medical robotics;needles;neurophysiology;path planning;position control;robot kinematics;surgery;tumours","Remote-Center-of-Motion recommendation;brain needle intervention;deep reinforcement learning;specific diagnosis;therapy procedure;brain disorders;brain tumors;preoperative needle path planning;needle intervention robot;rigid needle;preoperative path-planning;optimal Remote Center;needle insertion;RCM recommendation system;robot kinematics;RL agents","","4","","24","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach to Directed Test Generation for Shared Memory Verification","N. Pfeifer; B. V. Zimpel; G. A. G. Andrade; L. C. V. dos Santos","Federal University of Santa Catarina, Florianópolis, Brazil; Federal University of Santa Catarina, Florianópolis, Brazil; Federal University of Santa Catarina, Florianópolis, Brazil; Federal University of Santa Catarina, Florianópolis, Brazil","2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)","15 Jun 2020","2020","","","538","543","Multicore chips are expected to rely on coherent shared memory. Albeit the coherence hardware can scale gracefully, the protocol state space grows exponentially with core count. That is why design verification requires directed test generation (DTG) for dynamic coverage control under the tight time constraints resulting from slow simulation and short verification budgets. Next generation EDA tools are expected to exploit Machine Learning for reaching high coverage in less time. We propose a technique that addresses DTG as a decision process and tries to find a decision-making policy for maximizing the cumulative coverage, as a result of successive actions taken by an agent. Instead of simply relying on learning, our technique builds upon the legacy from constrained random test generation (RTG). It casts DTG as coverage-driven RTG, and it explores distinct RTG engines subject to progressively tighter constraints. We compared three Reinforcement Learning generators with a state-of-the-art generator based on Genetic Programming. The experimental results show that the proper enforcement of constraints is more efficient for guiding learning towards higher coverage than simply letting the generator learn how to select the most promising memory events for increasing coverage. For a 3-level MESI 32-core design, the proposed approach led to the highest observed coverage (95.81%), and it was 2.4 times faster than the baseline generator to reach the latter's maximal coverage.","1558-1101","978-3-9819263-4-7","10.23919/DATE48585.2020.9116198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9116198","Multicore chips;shared memory;design verification;reinforcement learning;decision process.","Engines;Generators;Test pattern generators;Reinforcement learning;Protocols;Time factors;Instruction sets","automatic test pattern generation;genetic algorithms;learning (artificial intelligence);microprocessor chips;shared memory systems","reinforcement learning;directed test generation;shared memory verification;multicore chips;coherent shared memory;coherence hardware;protocol state space;DTG;dynamic coverage control;tight time constraints;short verification budgets;EDA tools;machine learning;decision-making policy;constrained random test generation;coverage-driven RTG;generators;memory events;3-level MESI 32-core design;baseline generator;RTG engines","","4","","22","","15 Jun 2020","","","IEEE","IEEE Conferences"
"Adaptive reinforcement learning in box-pushing robots","K. S. Hwang; J. L. Ling; W. -H. Wang","Department of Electrical Engineering, National Sun Yat-sen University, Kaoshiung, Taiwan; Department of Information Management, Shih Hsin University, Taipei, Taiwan; Precision Machinery Research & Development Center, Taiwan","2014 IEEE International Conference on Automation Science and Engineering (CASE)","30 Oct 2014","2014","","","1182","1187","In this paper, an adaptive state aggregation Q-Learning method, with the capability of multi-agent cooperation, was proposed to enhance the efficiency of reinforcement learning (RL) and applied to box-pushing tasks for humanoid robots. First, a decision tree was applied to partition the state space according to temporary differences in reinforcement learning, so that a real valued action domain could be represented by a discrete space. Furthermore, adaptive state Q-Learning, which is the modification of estimating Q-value by tabular or function approximation, was proposed to demonstrate the efficiency of reinforcement learning when a humanoid robot pushing a box was simulated. During the process of a robot pushing a box, because the box moves along with the direction the robot asserts force and the pushing point on the box, the robot needs to learn how to adjust angles, avoid obstacles, keep gravity, and push the box to the target point. From the simulation results, the proposed method shows its learning efficiency outperforms the Q-Learning without using adaptive states.","2161-8089","978-1-4799-5283-0","10.1109/CoASE.2014.6899476","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6899476","","Robot kinematics;Learning (artificial intelligence);Humanoid robots;Decision trees;Robot sensing systems;Training","collision avoidance;decision trees;function approximation;humanoid robots;learning (artificial intelligence);multi-agent systems;multi-robot systems","Q-value estimation;tabular approximation;function approximation;obstacle avoidance;real valued action domain;state space;decision tree;humanoid robots;RL;multiagent cooperation capability;adaptive state aggregation Q-Learning method;box-pushing robots;adaptive reinforcement learning","","4","","18","IEEE","30 Oct 2014","","","IEEE","IEEE Conferences"
"A Multi-phase Intersection Traffic Signal Control Strategy with Deep Reinforcement Learning","C. Li; Y. Li; G. Liu","School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; Operation No.4 Company Chongqing, Railway Transportation Group co., Ltd, Chongqing, China","2018 Chinese Automation Congress (CAC)","24 Jan 2019","2018","","","959","964","In this paper, a deep reinforcement learning (DNQ) algorithm for multi-phase intersection traffic control is proposed for improves the capacity of the urban road intersections. Here, deep learning is applied for extracting the features of traffic flow to learn the Q-function of reinforcement learning. The denoising stacked autoencoders are considered to reduce the effects of abnormal data generated during system operation. Considering the connection between the signal timing scheme and the phase sequence, the DNQ algorithm is used to adjust the sequence of the signal phase according to the dynamic traffic characteristics of the intersection while realtime self-adaptive adjustment of the signal timing. Simulations in platform consisting of VISSIM and Python are applied to test the algorithm. The performance of the proposed method is comprehensively compared with a traditional algorithm with fixed or free phase sequence under different traffic demand. Simulation results suggest that the proposed method signify-cantly reduces the delay in the intersection when compared to the alternative methods.","","978-1-7281-1312-8","10.1109/CAC.2018.8623383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8623383","intersection;deep reinforcement learning;signal timing;styling;phase sequence","Reinforcement learning;Information science;Companies;Rail transportation;Timing","control engineering computing;learning (artificial intelligence);road traffic control;traffic engineering computing","multiphase intersection traffic signal control strategy;deep reinforcement learning algorithm;multiphase intersection traffic control;urban road intersections;deep learning;traffic flow;denoising stacked autoencoders;DNQ algorithm;signal phase;dynamic traffic characteristics;signal timing;fixed phase sequence;free phase sequence;traffic demand;VISSIM;Python;feature extraction","","4","","14","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Meta Reinforcement Learning for Optimal Design of Legged Robots","Á. Belmonte-Baeza; J. Lee; G. Valsecchi; M. Hutter","Robotic Systems Lab (RSL), ETH Zürich, Zürich, Switzerland; Robotic Systems Lab (RSL), ETH Zürich, Zürich, Switzerland; Robotic Systems Lab (RSL), ETH Zürich, Zürich, Switzerland; Robotic Systems Lab (RSL), ETH Zürich, Zürich, Switzerland","IEEE Robotics and Automation Letters","13 Oct 2022","2022","7","4","12134","12141","The process of robot design is a complex task and the majority of design decisions are still based on human intuition or tedious manual tuning. A more informed way of facing this task is computational design methods where design parameters are concurrently optimized with corresponding controllers. Existing approaches, however, are strongly influenced by predefined control rules or motion templates and cannot provide end-to-end solutions. In this paper, we present a design optimization framework using model-free meta reinforcement learning, and its application to the optimizing kinematics and actuator parameters of quadrupedal robots. We use meta reinforcement learning to train a locomotion policy that can quickly adapt to different designs. This policy is used to evaluate each design instance during the design optimization. We demonstrate that the policy can control robots of different designs to track random velocity commands over various rough terrains. With controlled experiments, we show that the meta policy achieves close-to-optimal performance for each design instance after adaptation. Lastly, we compare our results against a model-based baseline and show that our approach allows higher performance while not being constrained by predefined motions or gait patterns.","2377-3766","","10.1109/LRA.2022.3211785","la Caixa Foundation(grant numbers:LCF/BQ/EU20/11810067); ESA(grant numbers:4000131516/20/NL/MH/ic); ERC(grant numbers:852044); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910025","Reinforcement Learning;Mechanism Design;Legged Robots","Robots;Design optimization;Legged locomotion;Task analysis;Optimization;Adaptation models;Training","learning (artificial intelligence);legged locomotion;mobile robots;motion control;optimisation;robot dynamics","computational design methods;design parameters;corresponding controllers;predefined control rules;end-to-end solutions;design optimization framework;model-free meta reinforcement learning;optimizing kinematics;actuator parameters;quadrupedal robots;locomotion policy;design instance;meta policy;close-to-optimal performance;model-based baseline;optimal design;legged robots;robot design;complex task;design decisions;human intuition;tedious manual tuning","","4","","29","IEEE","4 Oct 2022","","","IEEE","IEEE Journals"
"Mixed Reinforcement Learning for Efficient Policy Optimization in Stochastic Environments","Y. Mu; B. Peng; Z. Gu; S. E. Li; C. Liu; B. Nie; J. Zheng; B. Zhang","State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Tsinghua University, Beijing, China; State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Tsinghua University, Beijing, China; State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Tsinghua University, Beijing, China; State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Tsinghua University, Beijing, China; Sibley School of Mechanical and Aerospace Engineering, Cornell University, New York, USA; State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Tsinghua University, Beijing, China; Smart Transportation Division, Didi Chuxing, China; Smart Transportation Division, Didi Chuxing, China","2020 20th International Conference on Control, Automation and Systems (ICCAS)","1 Dec 2020","2020","","","1212","1219","Reinforcement learning has the potential to control stochastic nonlinear systems in optimal manners successfully. We propose a mixed reinforcement learning (mixed RL) algorithm by simultaneously using dual representations of environmental dynamics to search the optimal policy. The dual representation includes an empirical dynamic model and a set of state-action data. The former can embed the designer's knowledge and reduce the difficulty of learning, and the latter can be used to compensate the model inaccuracy since it reflects the real system dynamics accurately. Such a design has the capability of improving both learning accuracy and training speed. In the mixed RL framework, the additive uncertainty of stochastic model is compensated by using explored state-action data via iterative Bayesian estimator (IBE). The optimal policy is then computed in an iterative way by alternating between policy evaluation (PEV) and policy improvement (PIM). The effectiveness of mixed RL is demonstrated by a typical optimal control problem of stochastic non-affine nonlinear systems (i.e., double lane change task with an automated vehicle).","2642-3901","978-89-93215-20-5","10.23919/ICCAS50221.2020.9268413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268413","Reinforcement learning;Bayesian estimation;Dynamic model;State-action data","Data models;Computational modeling;Stochastic processes;Heuristic algorithms;Vehicle dynamics;Uncertainty;Bayes methods","Bayes methods;iterative methods;learning (artificial intelligence);nonlinear control systems;optimal control;search problems;stochastic systems","optimal policy;empirical dynamic model;mixed RL framework;policy improvement;optimal control problem;nonaffine nonlinear systems;policy optimization;mixed reinforcement learning;additive uncertainty;iterative Bayesian estimator;policy evaluation;stochastic nonaffine nonlinear systems","","4","","27","","1 Dec 2020","","","IEEE","IEEE Conferences"
"A New Robotic Knee Impedance Control Parameter Optimization Method Facilitated by Inverse Reinforcement Learning","W. Liu; R. Wu; J. Si; H. Huang","UNC/NCSU Department of Biomedical Engineering, North Carolina State University, Raleigh, NC, USA; School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; UNC/NCSU Department of Biomedical Engineering, North Carolina State University, Raleigh, NC, USA","IEEE Robotics and Automation Letters","22 Aug 2022","2022","7","4","10882","10889","Recent efforts in the design of intelligent controllers for configuring robotic prostheses have demonstrated new possibilities in improving mobility and restoring locomotion for individuals with lower-limb disabilities. In these efforts, personalizing the controller of the robotic device is a crucial step in order to meet individual user's needs and physical conditions. Reinforcement learning (RL) based control designs are among some of the most promising approaches to achieving real-time, optimal adaptive tuning capability. However, such designs to date rely on subjectively determining human-robot walking performance measures, commonly in a quadratic form. To further automate the RL design for robotic knee control parameter tuning and potentially improve human-robot locomotion performance, this study introduces a new bilevel optimization method to objectively specify such control design performance measures via inverse reinforcement learning (IRL), which in turn, will be used in low level (forward) RL design of the impedance control parameters. We demonstrate the effectiveness of the bilevel optimization approach with improved human-robot walking performance using systematic OpenSim simulation studies.","2377-3766","","10.1109/LRA.2022.3194326","National Science Foundation(grant numbers:1563454,1563921,1808752,1808898,1926998); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9842322","Reinforcement learning;learning from demons- tration;wearable robotics;compliance and impedance control","Knee;Impedance;Robots;Cost function;Legged locomotion;Tuning;Kinematics","biomechanics;control engineering computing;control system synthesis;handicapped aids;human-robot interaction;intelligent control;legged locomotion;medical robotics;motion control;optimisation;patient rehabilitation;prosthetics;reinforcement learning","bilevel optimization method;control design performance measures;inverse reinforcement learning;low level RL design;impedance control parameters;bilevel optimization approach;improved human-robot walking performance;intelligent controllers;robotic prostheses;mobility;restoring locomotion;lower-limb disabilities;robotic device;individual user;reinforcement learning based control designs;optimal adaptive tuning capability;human-robot walking performance measures;robotic knee control parameter tuning;human-robot locomotion performance;new robotic knee impedance control parameter optimization method","","4","","31","IEEE","27 Jul 2022","","","IEEE","IEEE Journals"
"A framework for the adaptive transfer of robot skill knowledge using reinforcement learning agents","R. J. Malak; P. K. Khosla","Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA","Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164)","9 Jul 2003","2001","2","","1994","2001 vol.2","A framework, called skill advice guided exploration (SAGE), for the adaptive transfer of robot skill knowledge using reinforcement learning (RL) agents is presented. A skill is viewed as a reactive policy which maps world states to agent actions. It may be acquired via learning or it may be hand-coded by the designer. The SAGE framework allows multiple, possibly conflicting, sources of knowledge to be incorporated simultaneously. An abstraction for knowledge in an RL system, called advice, is introduced. The advice abstraction permits the transfer of information between RL agents with differing internal representations. A SAGE-based system can learn to disregard misleading advice. The potential of this methodology is demonstrated on a set of discrete learning tasks. Results show that SAGE-based systems can benefit from relevant information and that incorrect information does not prevent learning of the task solution. The benefits, limitations, and possible extensions of this work are discussed.","1050-4729","0-7803-6576-3","10.1109/ROBOT.2001.932900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=932900","","Learning;Knowledge transfer;Knowledge engineering;Robot vision systems;Humans;Knowledge representation;Table lookup;State estimation;Stochastic processes","learning (artificial intelligence);robot programming;adaptive systems;software agents","adaptive knowledge transfer;robot skill transfer;reinforcement learning;skill advice guided exploration;software agents;misleading advice","","4","","10","IEEE","9 Jul 2003","","","IEEE","IEEE Conferences"
