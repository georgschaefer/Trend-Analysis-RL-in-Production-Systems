"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Partial Consistency for Stabilizing Undiscounted Reinforcement Learning","H. Gao; Z. Yang; T. Tan; T. Zhang; J. Ren; P. Sun; S. Guo; F. Chen","Department of Automation, Tsinghua University, Beijing 100084, China.; Department of Automation, Tsinghua University, Beijing 100084, China.; Department of Civil and Environmental Engineering, Stanford University, Stanford, CA 94305 USA.; Department of Automation, Tsinghua University, Beijing 100084, China.; Department of Automation, Tsinghua University, Beijing 100084, China.; Department of Automation, Tsinghua University, Beijing 100084, China.; Department of Automation and the Department of Precision Instrument, Tsinghua University, Beijing 100086, China; Center for Brain-Inspired Computing Research, Department of Automation, Tsinghua University, Beijing 100190, China, and also with the LSBDPA Beijing Key Laboratory, Beijing 100084, China","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","15","Undiscounted return is an important setup in reinforcement learning (RL) and characterizes many real-world problems. However, optimizing an undiscounted return often causes training instability. The causes of this instability problem have not been analyzed in-depth by existing studies. In this article, this problem is analyzed from the perspective of value estimation. The analysis result indicates that the instability originates from transient traps that are caused by inconsistently selected actions. However, selecting one consistent action in the same state limits exploration. For balancing exploration effectiveness and training stability, a novel sampling method called last-visit sampling (LVS) is proposed to ensure that a part of actions is selected consistently in the same state. The LVS method decomposes the state-action value into two parts, i.e., the last-visit (LV) value and the revisit value. The decomposition ensures that the LV value is determined by consistently selected actions. We prove that the LVS method can eliminate transient traps while preserving optimality. Also, we empirically show that the method can stabilize the training processes of five typical tasks, including vision-based navigation and manipulation tasks.","2162-2388","","10.1109/TNNLS.2022.3165941","National Natural Science Foundation of China(grant numbers:62176133,61836004); Tsinghua--Guoqiang Research Program(grant numbers:2019GQG0006); National Key Research and Development Program of China(grant numbers:2021ZD0200300); Shuimu Tsinghua Scholar Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762369","Last visit (LV);partial consistency;reinforcement learning (RL);transient trap;undiscounted return.","Transient analysis;Task analysis;Training;Optimization;Estimation;Automation;Sun","","","","","","","CCBYNCND","25 Apr 2022","","","IEEE","IEEE Early Access Articles"
"A Reinforcement Learning Approach to Dynamic Scheduling in a Product-Mix Flexibility Environment","Y. -R. Shiue; K. -C. Lee; C. -T. Su","Department of Management Engineering, Fujian Business University, Fuzhou, China; Department of Industrial Engineering and Engineering Management, National Tsing Hua University, Hsinchu, Taiwan; Department of Management Engineering, Fujian Business University, Fuzhou, China","IEEE Access","16 Jun 2020","2020","8","","106542","106553","Machine bottlenecks, resulting from shifting and unbalanced machine loads caused by resource capacity limitations, impair product-mix flexibility production systems. Thus, the knowledge base (KB) of a dynamic scheduling control system should be dynamic and include a knowledge revision mechanism for monitoring crucial changes that occur in the production system. In this paper, reinforcement learning (RL)-based dynamic scheduling and a selection mechanism for multiple dynamic scheduling rules (MDSRs) are proposed to support the operating characteristics of a flexible manufacturing system (FMS) and semiconductor wafer fabrication (FAB). The proposed RL-based dynamic scheduling MDSR selection mechanism consisted of initial MDSR KB generation and revision phases. According to various performance criteria, the presented approach yields a system performance that is superior to those of the fixed-decision scheduling approach, the machine learning classification approach, and the classical MDSR selection mechanism.","2169-3536","","10.1109/ACCESS.2020.3000781","Ministry of Science and Technology, Taiwan(grant numbers:MOST 107-2221-E-007-076-MY2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9110908","Manufacturing execution system;dynamic scheduling;machine learning;reinforcement learning;Q-learning","Dynamic scheduling;Job shop scheduling;Machine learning;Production systems;Frequency modulation","dynamic scheduling;flexible manufacturing systems;knowledge based systems;learning (artificial intelligence);production engineering computing","flexible manufacturing system;RL-based dynamic scheduling MDSR selection mechanism;semiconductor wafer fabrication;reinforcement learning-based dynamic scheduling;knowledge revision mechanism;dynamic scheduling control system;knowledge base;product-mix flexibility production systems;resource capacity limitations;machine bottlenecks","","8","","80","CCBY","8 Jun 2020","","","IEEE","IEEE Journals"
"Q-Learning Based Reinforcement Learning Controller for Concentration Control of Food Preparation","M. Supriya; K. Srilatha; S. S. P; S. Oommen; H. C; S. N","Computer Science and Engineering, Geetanjali College of Engineering and Technology; Computer Science and Engineering, Geetanjali College of Engineering and Technology; Computer Science and Engineering, Presidency University; School of Electrical and Electronics Engineering, REVA University, Bangalore; Department of Mechanical Engineering, BGS Institute of Technology, Karnataka; Department of Mechanical Engineering, BGS Institute of Technology, Karnataka","2023 5th International Conference on Inventive Research in Computing Applications (ICIRCA)","28 Aug 2023","2023","","","872","877","Food preservation is the practice of preparing food in such a way that its beneficial characteristics (sensory and nutritional) are maintained for as long as feasible after preparation. Water elimination and reduction of water content are the basis of the food preservation theory. Maintaining concentration at a particular level is crucial in the food manufacturing industry. Common processing equipment in the food industry is the Continuous Stirred Tank Reactor (CS TR). The CS TR's function is to keep the reactor's internal temperature and product concentration constant. This study aims to keep the product concentration constant using two different controllers. The first is a traditional Proportional Integral Derivative (PID) controller, and the second is a Reinforcement Learning (RL) controller based on Q-learning. A mathematical model of the CS TR is constructed, and the controllers are developed. The concentration of the product is set to the specific value and allows the controller to track it. Error measures such as Mean Squared Error (MSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE) are used to assess the performance of each controller. The RL controller shows promising results in terms of error metrics, suggesting that it may be a viable option for maintaining concentration in food industries.","","979-8-3503-2142-5","10.1109/ICIRCA57980.2023.10220591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10220591","Q-Learning;CSTR;Controller Performance;Concentration;Error Metrics","Industries;Q-learning;Measurement uncertainty;Process control;Optimization methods;Food preservation;Mathematical models","chemical reactors;control engineering computing;food preservation;food processing industry;mean square error methods;production engineering computing;reinforcement learning;three-term control","concentration control;continuous stirred tank reactor;CSTR function;error metrics;food manufacturing industry;food preparation;food preservation theory;mathematical model;mean absolute error;mean absolute percentage error;mean squared error;particular level;PID controller;processing equipment;product concentration constant;proportional integral derivative controller;Q-learning based reinforcement learning controller;reactor internal temperature;reinforcement learning controller;RL controller;water content;water elimination","","","","16","IEEE","28 Aug 2023","","","IEEE","IEEE Conferences"
"Curriculum Multi-Stage Reinforcement Learning for Automated Interlinked Production Systems on Virtual Commissioning Simulations","F. Jaensch; A. Steidle; A. Verl","ISW University of Stuttgart, Stuttgart, Germany; ISW University of Stuttgart, Stuttgart, Germany; ISW University of Stuttgart, Stuttgart, Germany","2021 Third International Conference on Transdisciplinary AI (TransAI)","18 Oct 2021","2021","","","129","136","In order to automate the software engineering process of interlinked production systems, reinforcement learning applications can be used to learn the control flow logic on the basis of virtual production systems. Since the simulation- based prototypes are available for virtual commissioning (VC) anyway, they can be used simultaneously as reinforcement learning environments. In this work, the event-discrete flow logic for the transport and assembly of a target workpiece is learned automatically by reinforcement learning on the real use case of the VC simulation of a PLC-based production system. According to the idea of curriculum learning, the system is trained separately in subsystems to support its modularity and to reduce the complexity of the overall learning process. With regard to the learning processes, subsystems, sequence errors, termination criteria and necessary action and state adjustments typical for the PLC-based plant are identified and implemented in the VC simulation. The reward functions are derived with respect to the individual subsystems. The learned controls of the subsystems are then merged back together for a complete flow of the entire system.","","978-1-6654-3412-6","10.1109/TransAI51903.2021.00031","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9565636","reinforcement learning;virtual commissioning simulation","Learning systems;Production systems;Prototypes;Process control;Reinforcement learning;Aerospace electronics;Control systems","learning (artificial intelligence);programmable controllers;virtual manufacturing","curriculum multistage reinforcement learning;automated interlinked production systems;virtual commissioning;software engineering process;reinforcement learning applications;control flow logic;virtual production systems;reinforcement learning environments;event-discrete flow logic;VC simulation;production system;curriculum learning;learning process;PLC-based plant;learned controls;complete flow","","","","19","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Convex Optimization-Based Inverse Reinforcement Learning in Design Space Exploring","Y. -F. Jin; Y. -S. Xia; X. -J. Zha","Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China","2022 IEEE 16th International Conference on Solid-State & Integrated Circuit Technology (ICSICT)","1 Dec 2022","2022","","","1","3","In recent years, the application of machine learning in Electronic Design Automation (EDA) has become a trend. Especially in deep reinforcement learning, it has proven to be an excellent algorithm to optimize the delay and area of the circuit. However, one fundamental assumption of existing reinforcement learning algorithms is that reward function, the most direct representation of the designer’s intention, needs to be provided beforehand. In practice, the reward function directly determines the efficiency of reinforcement learning (RL). This paper proposed a high-performance convex optimization-based inverse reinforcement learning (IRL) framework for writing the reward function, which directly solves the problem by giving different labels to expert trajectories and experimental trajectories to learn the reward functions, respectively. Applied to an evaluation of the EPFL integrated benchmark suite, the model optimization after changing the reward function is more efficient than state-of art methods, and with the similar Quality of Results (QoRs), it saves 36.2% of iterations.","","978-1-6654-6906-7","10.1109/ICSICT55466.2022.9963424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9963424","","Integrated circuit technology;Machine learning algorithms;Design automation;Reinforcement learning;Writing;Market research;Trajectory","convex programming;deep learning (artificial intelligence);electronic design automation;reinforcement learning","deep reinforcement learning;design space exploring;electronic design automation;EPFL;high-performance convex optimization-based inverse reinforcement learning framework;machine learning;quality of results;reward function","","","","12","IEEE","1 Dec 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Hardware Security: Opportunities, Developments, and Challenges","S. Patnaik; V. Gohil; H. Guo; J. J. Rajendran","Electrical & Computer Engineering, Texas A&M University, College Station, Texas, USA; Electrical & Computer Engineering, Texas A&M University, College Station, Texas, USA; Electrical & Computer Engineering, Texas A&M University, College Station, Texas, USA; Electrical & Computer Engineering, Texas A&M University, College Station, Texas, USA","2022 19th International SoC Design Conference (ISOCC)","7 Feb 2023","2022","","","217","218","Reinforcement learning (RL) is a machine learning paradigm where an autonomous agent learns to make an optimal sequence of decisions by interacting with the underlying environment. The promise demonstrated by RL-guided workflows in unraveling electronic design automation problems has encouraged hardware security researchers to utilize autonomous RL agents in solving domain-specific problems. From the perspective of hardware security, such autonomous agents are appealing as they can generate optimal actions in an unknown adversarial environment. On the other hand, the continued globalization of the integrated circuit supply chain has forced chip fabrication to off-shore, untrustworthy entities, leading to increased concerns about the security of the hardware. Furthermore, the unknown adversarial environment and increasing design complexity make it challenging for defenders to detect subtle modifications made by attackers (a.k.a. hardware Trojans). In this brief, we outline the development of RL agents in detecting hardware Trojans, one of the most challenging hardware security problems. Additionally, we outline potential opportunities and enlist the challenges of applying RL to solve hardware security problems.","2163-9612","978-1-6654-5971-6","10.1109/ISOCC56007.2022.10031569","National Science Foundation(grant numbers:1822848,2039610); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10031569","Reinforcement Learning;Hardware Security","Integrated circuits;Design automation;Supply chains;Globalization;Reinforcement learning;Hardware;Autonomous agents","electronic design automation;electronic engineering computing;invasive software;learning (artificial intelligence);multi-agent systems;reinforcement learning","autonomous agent;autonomous RL agents;challenging hardware security problems;domain-specific problems;hardware security researchers;increasing design complexity;integrated circuit supply chain;machine learning paradigm;optimal actions;reinforcement learning;RL-guided workflows;underlying environment;unknown adversarial environment;unraveling electronic design automation problems","","","","22","IEEE","7 Feb 2023","","","IEEE","IEEE Conferences"
"Power optimization with Reinforcement Learning in Logic Synthesis","C. Yang; Y. Xia","Faculty of Electrical Engineering and Computer Science (EECS), Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science (EECS), Ningbo University, Ningbo, China","2021 IEEE 14th International Conference on ASIC (ASICON)","1 Dec 2021","2021","","","1","3","Logic optimization is an NP-hard problem though there is a wealth of research on algorithms for logic optimization, but how these algorithms are used is usually determined by heuristics. In engineering, it often requires experienced engineers to spend a lot of time tweaking optimization commands in electronic design automation (EDA) tools to achieve improved hardware performance. In this paper, we use the Deep Reinforcement Learning algorithm Advantage Actor-Critic to train an agent. This agent is able to optimize the dynamic power consumption of the designs using the commands in the synthesis tool. Experimental results show that the dynamic power consumption optimization results of the method proposed in this paper outperform the heuristic algorithm in ABC(academic synthesis tools).","2162-755X","978-1-6654-3867-4","10.1109/ASICON52560.2021.9620356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9620356","","Power demand;Design automation;NP-hard problem;Heuristic algorithms;Conferences;Reinforcement learning;Tools","deep learning (artificial intelligence);electronic design automation;integrated circuit design;logic design;optimisation","power optimization;logic synthesis;logic optimization;NP-hard problem;optimization commands;electronic design automation tools;improved hardware performance;synthesis tool;dynamic power consumption optimization results;heuristic algorithm;Deep Reinforcement Learning algorithm Advantage Actor-Critic;academic synthesis tools","","","","14","IEEE","1 Dec 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Incentive Mechanism Design for Platoon Autonomous Driving With Social Effect","B. Li; K. Xie; X. Huang; Y. Wu; S. Xie","School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; State Key Laboratory of Internet of Things for Smart City, University of Macau, Taipa, China; School of Automation, Guangdong University of Technology, Guangzhou, China","IEEE Transactions on Vehicular Technology","15 Jul 2022","2022","71","7","7719","7729","In platoon autonomous driving, a leader vehicle (LV) leads the entire platoon moving forward by instructing the follower vehicles (FVs) to drive cooperatively. To gather sufficient environmental data as the prior knowledge, the LV employs the FVs to perform the data collection tasks. Each FV determines the participation willingness and its effort level according to an incentive mechanism presented by the LV and decisions of the social partners in the same platoon. We develop a game theoretic approach to model the strategic interactions among the LV and FVs. After that, the equilibrium solution is derived based on the backward induction method and information collection of the FVs. We further propose a deep reinforcement learning approach to effectively seek the equilibrium solution without the complete information. In our scheme, we apply a multi-agent deep deterministic policy gradient algorithm to train each game player as an agent with the continuous action space. Finally, numerical results are provided to demonstrate the effectiveness and efficiency of our scheme.","1939-9359","","10.1109/TVT.2022.3164656","National Natural Science Foundation of China(grant numbers:62001125,61973087,U1911401); State Key Laboratory of Synthetical Automation for Process Industries(grant numbers:2020-KF-21-02); Science and Technology Development Fund of Macau SAR(grant numbers:0060/2019/A1,0162/2019/A3); FDCT-MOST(grant numbers:0066/2019/AMJ); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9749953","DRL;platoon autonomous driving;social effect","Games;Data collection;Autonomous vehicles;Task analysis;Costs;Automation;Sensors","game theory;gradient methods;incentive schemes;mobile robots;multi-agent systems;reinforcement learning","LV;social effect;platoon autonomous driving;incentive mechanism design;multiagent deep deterministic policy gradient algorithm;deep reinforcement learning approach;information collection;backward induction method;equilibrium solution;game theoretic approach;social partners;data collection tasks;FVs;follower vehicles;leader vehicle","","","","30","IEEE","5 Apr 2022","","","IEEE","IEEE Journals"
"Automated Design of Complex Analog Circuits with Multiagent based Reinforcement Learning","J. Zhang; J. Bao; Z. Huang; X. Zeng; Y. Lu","State Key Lab. of ASIC & System, School of Information Science and Technology; State Key Lab. of ASIC & System, School of Information Science and Technology; Frontier Institute of Chip and System; State Key Lab. of ASIC & System, Microelectronics Department, Fudan University, Shanghai, China; State Key Lab. of ASIC & System, School of Information Science and Technology","2023 60th ACM/IEEE Design Automation Conference (DAC)","15 Sep 2023","2023","","","1","6","Despite the effort of analog circuit design automation, currently complex analog circuit design still requires extensive manual iterations, making it labor intensive and time-consuming. Recently, reinforcement learning (RL) algorithms have been demonstrated successfully for the analog circuit design optimization. However, a robust and highly efficient RL method to design analog circuits with complex design space has not been fully explored yet. In this work, inspired by multiagent planning theory as well as human expert design practice, we propose a multiagent based RL (MA-RL) framework to tackle this issue. Particularly, we (i) partition the complex analog circuits into several sub-blocks based on topology information and effectively reduce the complexity of design search space; (ii) leverage MA-RL for the circuit optimization, where each agent corresponds to a single sub-block, and the interactions between agents delicately mimic the best design tradeoffs between circuit sub-blocks by human experts; (iii) introduce the multiagent twin-delayed techniques to further boost training stability and accomplish higher performances. Experiments on two different analog circuit topologies and knowledge transfers between two technology nodes are demonstrated. It’s shown that MA-RL framework can achieve the best FoM for complex analog circuits design. This work shines the light for future large scale analog circuit system design automation.","","979-8-3503-2348-1","10.1109/DAC56929.2023.10247909","National Natural Science Foundation of China; Shanghai Municipal Education Commission; Natural Science Foundation of Shanghai; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247909","Complex analog circuits;Circuit design automation;Multiagent reinforcement learning;Twin delayed deep deterministic policy gradient","Training;Design automation;Reinforcement learning;Manuals;Analog circuits;Topology;Circuit stability","analogue circuits;circuit optimisation;electronic design automation;multi-agent systems;reinforcement learning","analog circuit design automation;analog circuit design optimization;automated design;circuit optimization;circuit subblocks;complex analog circuit design;complex design space;design search space;different analog circuit topologies;large scale analog circuit system design automation;MA-RL framework;multiagent based reinforcement learning;multiagent based RL framework;reinforcement learning algorithms;robust RL method","","","","20","IEEE","15 Sep 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-Based Multichannel Access for Industrial Wireless Networks With Dynamic Multiuser Priority","X. Liu; C. Xu; H. Yu; P. Zeng","State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China","IEEE Transactions on Industrial Informatics","13 Jul 2022","2022","18","10","7048","7058","In Industry 4.0, massive heterogeneous industrial devices generate a great deal of data with different quality of service requirements, and communicate via industrial wireless networks (IWNs). However, the limited time-frequency resources of IWNs cannot well support the high concurrent access of massive industrial devices with strict real-time and reliable communication requirements. To address this problem, a deep reinforcement learning-based dynamic priority multichannel access (DRL-DPMCA) algorithm is proposed in this article. Firstly, according to the time-sensitivity of industrial data, industrial devices are assigned with different priorities, based on which their channel access probabilities are dynamically adjusted. Then, the Markov decision process is utilized to model the dynamic priority multichannel access problem. To cope with the explosion of state space caused by the multichannel access of massive industrial devices with dynamic priorities, DRL is used to establish the mapping from states to actions. Next, the long-term cumulative reward is maximized to obtain an effective policy. Especially, with joint consideration of the access reward and priority reward, a compound reward for multichannel access and dynamic priority is designed. For breaking the time correlation of training data while accelerating the convergence of DRL-DPMCA, an experience replay with experience-weight is proposed to store and sample experiences categorically. Besides, the gated recurrent unit, dueling architecture and step-by-step $\varepsilon$-greedy method are employed to make states more comprehensive and reduce model oscillation. Extensive experiments show that, compared with slotted-Aloha and deep Q network algorithms, DRL-DPMCA converges quickly, and guarantees the highest channel access probability and the minimum queuing delay for high-priority industrial devices in the context of minimum access conflict and nearly 100% channel utilization.","1941-0050","","10.1109/TII.2021.3139349","National Key Research and Development Program of China(grant numbers:2020YFB1710900); National Natural Science Foundation of China(grant numbers:62173322,61803368,U1908212,61821005); China Postdoctoral Science Foundation(grant numbers:2019M661156); Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:2019202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9667302","Deep reinforcement learning;dynamic priority;industrial wireless networks (IWNs);multichannel access;quality of service","Delays;Reliability;Real-time systems;Informatics;Heuristic algorithms;Transmitters;Quality of service","access protocols;deep learning (artificial intelligence);greedy algorithms;Markov processes;multi-access systems;probability;production engineering computing;quality of service;queueing theory;radio networks;reinforcement learning;telecommunication network reliability;wireless channels","industrial wireless networks;dynamic multiuser priority;massive heterogeneous industrial devices;IWNs;time-frequency resources;high concurrent access;massive industrial devices;reliable communication requirements;deep reinforcement learning-based dynamic priority multichannel access algorithm;DRL-DPMCA;time-sensitivity;industrial data;channel access probabilities;dynamic priority multichannel access problem;long-term cumulative reward;access reward;priority reward;time correlation;deep Q network algorithms;minimum access conflict;high-priority industrial devices;highest channel access probability","","1","","32","IEEE","31 Dec 2021","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Minimizing Tardiness in Parallel Machine Scheduling With Sequence Dependent Family Setups","B. Paeng; I. -B. Park; J. Park","Department of Industrial Engineering, Seoul National University, Seoul, South Korea; Department of Industrial Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Industrial Engineering, Seoul National University, Seoul, South Korea","IEEE Access","26 Jul 2021","2021","9","","101390","101401","Parallel machine scheduling with sequence-dependent family setups has attracted much attention from academia and industry due to its practical applications. In a real-world manufacturing system, however, solving the scheduling problem becomes challenging since it is required to address urgent and frequent changes in demand and due-dates of products. To minimize the total tardiness of the scheduling problem, we propose a deep reinforcement learning (RL) based scheduling framework in which trained neural networks (NNs) are able to solve unseen scheduling problems without re-training even when such changes occur. Specifically, we propose state and action representations whose dimensions are independent of production requirements and due-dates of jobs while accommodating family setups. At the same time, an NN architecture with parameter sharing was utilized to improve the training efficiency. Extensive experiments demonstrate that the proposed method outperforms the recent metaheuristics, rule-based, and other RL-based methods in terms of total tardiness. Moreover, the computation time for obtaining a schedule by our framework is shorter than those of the metaheuristics and other RL-based methods.","2169-3536","","10.1109/ACCESS.2021.3097254","National Research Foundation of Korea (NRF); Ministry of Science and ICT (MSIT)(grant numbers:NRF-2015R1D1A1A01057496); Institute of Engineering Research, Seoul National University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9486959","Deep reinforcement learning;unrelated parallel machine scheduling;sequence-dependent family setups;total tardiness objective;deep Q-network","Job shop scheduling;Production;Processor scheduling;Manufacturing systems;Scheduling;Training;Markov processes","deep learning (artificial intelligence);manufacturing systems;minimisation;neural net architecture;parallel machines;production engineering computing;scheduling","deep reinforcement learning;parallel machine scheduling;sequence-dependent family setups;real-world manufacturing system;total tardiness;neural network training;unseen scheduling problems;action representations;production requirements;RL-based methods;NN architecture;tardiness minimization","","10","","62","CCBY","15 Jul 2021","","","IEEE","IEEE Journals"
"Fault-Tolerant Controller Design for a Class of Nonlinear MIMO Discrete-Time Systems via Online Reinforcement Learning Algorithm","Z. Wang; L. Liu; H. Zhang; G. Xiao","College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","20 May 2017","2016","46","5","611","622","This paper concentrates on the reinforcement learning (RL)-based fault-tolerant control (FTC) problem for a class of multiple-input-multiple-output (MIMO) nonlinear discrete-time systems. Both incipient faults and abrupt faults are taken into account. Based on the approximation ability of neural networks (NNs), an RL algorithm is incorporated into the FTC strategy, in which an action network is developed to generate the optimal control signal and a critic network is used to approximate the novel cost function, respectively. Compared with the existing results, a novel fault tolerant controller is proposed based on an RL method to reduce a long-term performance index after a fault occurs. The meaning of minimizing the performance index after a fault occurs in an MIMO system is that waste will be decreased and energy will be saved. Note that the weights of NNs are adjusted online rather than offline. Then, it is proven that the adaptive parameters, tracking errors, and optimal control signals are uniformly bounded even in the presence of the unknown fault dynamics. Finally, a numerical simulation is provided to show the effectiveness of the proposed FTC approach.","2168-2232","","10.1109/TSMC.2015.2478885","National Natural Science Foundation of China(grant numbers:61473070,61433004); Fundamental Research Funds for the Central Universities(grant numbers:N130504002,N140406001,N130104001); State Key Laboratory of Synthetical Automation for Process Industries(grant numbers:2013ZCX01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293675","Adaptive control;fault-tolerant control (FTC);multiple-input-multiple-output (MIMO) discrete-time systems;neural networks (NNs);reinforcement learning (RL);Adaptive control;fault-tolerant control (FTC);multiple-input-multiple-output (MIMO) discrete-time systems;neural networks (NNs);reinforcement learning (RL)","MIMO;Artificial neural networks;Discrete-time systems;Cost function;Fault tolerance;Fault tolerant systems;Optimal control","approximation theory;control system synthesis;discrete time systems;fault tolerant control;learning (artificial intelligence);MIMO systems;neurocontrollers;nonlinear control systems;optimal control;performance index","fault-tolerant controller design;nonlinear MIMO discrete-time systems;online reinforcement learning algorithm;RL-based fault-tolerant control problem;RL-based FTC problem;multiple-input-multiple-output nonlinear discrete-time systems;incipient faults;abrupt faults;neural network approximation ability;NN approximation ability;action network;critic network;cost function approximation;performance index reduction;online NN weight adjustment;adaptive parameters;tracking errors;uniformly bounded optimal control signals;unknown fault dynamics","","76","","42","IEEE","7 Oct 2015","","","IEEE","IEEE Journals"
"Reinforcement Learning of Material Flow Control Logic Using Hardware-in-the-Loop Simulation","F. Jaensch; A. Csiszar; A. Kienzlen; A. Verl","Institute of control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, Stuttgart, Germany; Institute of control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, Stuttgart, Germany; Institute of control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, Stuttgart, Germany; Institute of control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, Stuttgart, Germany","2018 First International Conference on Artificial Intelligence for Industries (AI4I)","14 Mar 2019","2018","","","77","80","In this paper the concept of reinforcement learning agent is presented, which can deduce the correct control policy of a plant by acting in its digital twin (the HiL simulation). This way the agent substitutes a real control system. By using reinforcement learning methods, a proof of concept application is presented for a simplistic material flow system, with the same type of access to the digital twin which a PLC controller-hardware would have. With the presented approach the agent is able to find the correct control policy.","","978-1-5386-9209-7","10.1109/AI4I.2018.8665712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8665712","","Reinforcement learning;Belts;Hardware-in-the loop simulation;Control systems;Process control;Software;Training","control engineering computing;hardware-in-the loop simulation;learning (artificial intelligence);process control;programmable controllers","PLC controller-hardware;material flow control logic;hardware-in-the-loop simulation;reinforcement learning agent;control system;process control logic","","9","1","13","IEEE","14 Mar 2019","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Based Approach for AGVs Path Planning","X. Guo; Z. Ren; Z. Wu; J. Lai; D. Zeng; S. Xie","School of Automation Guangdong Key Laboratory of IoT Information Technology, Guangdong University of Technology, Guangzhou, China; School of Automation Guangdong Key Laboratory of IoT Information Technology, Guangdong University of Technology, Guangzhou, China; School of Automation Guangdong Key Laboratory of IoT Information Technology, Guangdong University of Technology, Guangzhou, China; School of Automation Guangdong Key Laboratory of IoT Information Technology, Guangdong University of Technology, Guangzhou, China; School of Automation Guangdong Key Laboratory of IoT Information Technology, Guangdong University of Technology, Guangzhou, China; School of Automation Guangdong-HongKong-Macao Joint Laboratory for Smart Discrete Manufacturing, Guangdong University of Technology, Guangzhou, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","6833","6838","Autonomous path planning of automated guided vehicles (AGVs) is an important part of the intelligent logistics systems (ILS), which can greatly improve the abilities of intelligence and automation. Traditional AGVs navigation, electromagnetic navigation, and tape navigation, only navigate on metal belts or magnetic tapes, and the freedom of navigation is greatly reduced. This paper models the AGVs path planning problem as a reinforcement learning model to improve the capabilities of autonomous path planning and freedom of navigation. The path planning method based on the Dueling Double Deep Q Network with Prioritized Experience Reply (Dueling DDQN-PER) is used to learn the AGVs control in a simulation environment of an ILS. Using multi-modal sensory environment information, the AGV's ability to approach the target location and avoid obstacles has been significantly improved. Experimental results show that the AGVs can autonomously plan the path in the environment with obstacles, and has a high success rate and obstacle avoidance ability.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327532","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327532","deep reinforcement learning;path planning;AGV","Path planning;Reinforcement learning;Neural networks;Automation;Laboratories;Logistics;Information technology","automatic guided vehicles;collision avoidance;learning (artificial intelligence);logistics;mobile robots;navigation","intelligent logistics systems;AGV navigation;electromagnetic navigation;tape navigation;magnetic tapes;reinforcement learning model;autonomous path planning;Dueling Double Deep Q Network;AGV control;multimodal sensory environment information;deep reinforcement;automated guided vehicles;AGV path planning problem","","5","","21","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"An Effective Macro Placement Algorithm Based On Curiosity-Driven Reinforcement Learning","J. Gu; H. Gu; K. Liu; Z. Zhu","National ASIC System Engineering Center, Southeast University, Nanjing, China; National ASIC System Engineering Center, Southeast University, Nanjing, China; National ASIC System Engineering Center, Southeast University, Nanjing, China; National ASIC System Engineering Center, Southeast University, Nanjing, China","2023 International Symposium of Electronics Design Automation (ISEDA)","25 Aug 2023","2023","","","364","368","In mixed-size circuit designs, due to the numerous connections with standard cells and huge sizes of macros, the positions of macros greatly affect the placement of standard cells and then the layout quality. In this work, we use an on-policy reinforcement learning method to train an agent from scratch and make the agent learn macro placement. To satisfy the non-overlap constraint of macro placement, we design a fast non-overlapping algorithm. To get better feature extraction of global and local state information, we adopt a convolutional neural network (CNN) and an edge-based graph neural network (Edge-GNN). In addition, we add intrinsic reward as the curiosity in the reward design to encourage the agent to explore, helping the agent better understand how to place macros effectively. Experiment results show that our proposed framework is more effective in optimizing the wirelength of the placement compared with a simulated annealing method.","","979-8-3503-0451-0","10.1109/ISEDA59274.2023.10218370","National Key Research and Development Program of China(grant numbers:2022YFB4400500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10218370","Physical design;Placement;Electronic design automation;Reinforcement learning;Graph neural network","Measurement;Design automation;Costs;Layout;Reinforcement learning;Simulated annealing;Feature extraction","convolutional neural nets;feature extraction;graph neural networks;graph theory;learning (artificial intelligence);neural nets;reinforcement learning;simulated annealing","convolutional neural network;curiosity-driven reinforcement learning;edge-based graph neural network;effective macro placement algorithm;global state information;huge sizes;layout quality;local state information;macros;mixed-size circuit designs;nonoverlap constraint;nonoverlapping algorithm;numerous connections;on-policy reinforcement;reward design;standard cells","","","","10","IEEE","25 Aug 2023","","","IEEE","IEEE Conferences"
"An Extension of Genetic Network Programming with Reinforcement Learning Using Actor-Critic","H. Hatakeyama; S. Mabu; K. Hirasawa; Jinglu Hu","Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan","2006 IEEE International Conference on Evolutionary Computation","11 Sep 2006","2006","","","1537","1543","A new graph-based evolutionary algorithm named "" Genetic Network Programming, GNP"" has been already proposed. GNP represents its solutions as graph structures, which can improve the expression ability and performance. In addition, GNP with Reinforcement Learning (GNP-RL) was proposed a few years ago. Since GNP-RL can do reinforcement learning during task execution in addition to evolution after task execution, it can search for solutions efficiently. In this paper, GNP with Actor-Critic (GNP-AC) which is a new type of GNP-RL is proposed. Originally, GNP deals with discrete information, but GNP-AC aims to deal with continuous information. The proposed method is applied to the controller of the Khepera simulator and its performance is evaluated.","1941-0026","0-7803-9487-9","10.1109/CEC.2006.1688491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688491","","Economic indicators;Genetic programming;Evolutionary computation;Production systems;Learning systems;Wheels;Libraries","genetic algorithms;graph theory;learning (artificial intelligence)","genetic network programming;reinforcement learning;actor-critic;graph-based evolutionary algorithm;expression ability;task execution;discrete information;Khepera simulator;performance evaluation","","1","","7","IEEE","11 Sep 2006","","","IEEE","IEEE Conferences"
"View-based programming with reinforcement learning for robotic manipulation","Y. Maeda; T. Watanabe; Y. Moriyama","Division of Systems Research, Faculty of Engineering, Yokohama National University, Yokohama, Japan; Seiko Epson Corporation, Japan; Department of Mechanical Engineering, Division of Systems Integration, Graduate School of Engineering, Yokohama National University, Japan","2011 IEEE International Symposium on Assembly and Manufacturing (ISAM)","7 Jul 2011","2011","","","1","6","In this paper, we study a method of robot programming with view-based image processing. It can achieve more robustness against changes of task conditions than conventional teaching/playback without losing its general versatility. In order to reduce human demonstrations required for the view-based robot programming, we integrate reinforcement learning with the method. First we construct an initial neural network as a mapping from images to appropriate robot motions using human demonstration data. Next we train the neural network with actor-critic reinforcement learning so that it can work well even in task conditions that are not identical to those in the demonstrations. Our proposed method is successfully applied to pushing and pick-and-place tasks in a virtual environment.","","978-1-61284-343-8","10.1109/ISAM.2011.5942329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5942329","","Learning;Artificial neural networks;Humans;Education;Robot motion;Virtual environment","learning (artificial intelligence);neural nets;robot programming;robot vision","view-based programming;reinforcement learning;robotic manipulation;robot programming;view-based image processing;human demonstrations;view-based robot programming;neural network;robot motions","","2","","10","IEEE","7 Jul 2011","","","IEEE","IEEE Conferences"
"On Scheduling a Photolithograhy Toolset Based on a Deep Reinforcement Learning Approach with Action Filter","T. Kim; H. Kim; T. -e. Lee; J. R. Morrison; E. Kim","Department of Industrial and Systems Engineering, Korea Advanced Institute of Science and Technology, Daejeon, SOUTH KOREA; Department of Industrial and Systems Engineering, Korea Advanced Institute of Science and Technology, Daejeon, SOUTH KOREA; Department of Industrial and Systems Engineering, Korea Advanced Institute of Science and Technology, Daejeon, SOUTH KOREA; School of Engineering Technology, Central Michigan University, Mt Pleasant, MI, USA; Smart IT Team, Samsung Display Company, Asan-si, Chungcheongnam-Do, SOUTH KOREA","2021 Winter Simulation Conference (WSC)","23 Feb 2022","2021","","","1","10","Production scheduling of semiconductor manufacturing tools is a challenging problem due to the complexity of the equipment and systems in modern wafer fabs. In our study, we focus on the photolithography toolset and consider it as a non-identical parallel machine scheduling problem with random lot arrivals and auxiliary resource constraints. The proposed methodology strives to learn a near optimal scheduling policy by incorporating WIP, masks, and the tardiness of jobs. An Action Filter (AF) is proposed as a methodology to eliminate illogical actions and speed the learning process of agents. The proposed model was evaluated in a simulation environment inspired by practical photolithography scheduling problems across various settings with reticle and qualification constraints. Our experiments demonstrated improved performance compared to typical rule-based strategies. Relative to our learning methods, weighted shortest processing time (WSPT) and apparent tardiness cost with setups (ATCS) rules perform 28% and 32% worse for weighted tardiness, respectively.","1558-4305","978-1-6654-3311-2","10.1109/WSC52266.2021.9715450","Samsung Display Co., Ltd; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9715450","","Schedules;Job shop scheduling;Lithography;Optimal scheduling;Production;Reinforcement learning;Filtering algorithms","deep learning (artificial intelligence);integrated circuit manufacture;photolithography;production engineering computing;reinforcement learning;semiconductor device manufacture;single machine scheduling","photolithography toolset;nonidentical parallel machine scheduling problem;random lot arrivals;auxiliary resource constraints;methodology strives;optimal scheduling policy;illogical actions;practical photolithography scheduling problems;reticle;qualification constraints;typical rule-based strategies;learning methods;tardiness cost;photolithograhy toolset;deep reinforcement;production scheduling;semiconductor manufacturing tools;modern wafer fabs;action filter","","3","","22","IEEE","23 Feb 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Supplier-Agents for Electricity Markets","A. Rahimi-Kian; H. Tabarraei; B. Sadeghi","Control & Intelligent Processing Center of Excellence, Department of ECE, University of Tehran, Tehran, Iran; Control & Intelligent Processing Center of Excellence, Department of ECE, University of Tehran, Tehran, Iran; Control & Intelligent Processing Center of Excellence, Department of ECE, University of Tehran, Tehran, Iran","Proceedings of the 2005 IEEE International Symposium on, Mediterrean Conference on Control and Automation Intelligent Control, 2005.","13 Mar 2006","2005","","","1405","1410","Bidding strategies play important roles in maximizing the profits of power suppliers in competitive electricity markets. Therefore, it will be an advantage for a supplier to search for optimal bidding strategies in the market. In this paper the problem of designing fuzzy reinforcement learning (FRL) supplier-agents that compete in forward electricity markets (e.g. Day-Ahead energy market) to maximize their revenues is studied. An IEEE 30-bus power system with 6 generators (supplier-agents) and three demand areas with stochastic loads are used for our simulation studies. This model is applicable to different types of commodity markets with numerous supply and demand agents","2158-9879","0-7803-8936-0","10.1109/.2005.1467220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467220","","Learning;Power markets;Electricity supply industry;Pricing;Power supplies;Power system modeling;Power system simulation;Power generation;Stochastic systems;Supply and demand","electricity supply industry;financial management;learning (artificial intelligence);multi-agent systems;power markets;supply and demand","supplier agents;electricity market;profit maximization;power suppliers;optimal bidding strategy;fuzzy reinforcement learning;forward electricity markets;day-ahead energy market;revenue maximization;IEEE 30-bus power system;power generators;stochastic load;commodity market;supply and demand agents","","4","","11","IEEE","13 Mar 2006","","","IEEE","IEEE Conferences"
"Genetic Network Programming with Reinforcement Learning Using Sarsa Algorithm","S. Mabu; H. Hatakeyama; K. Hirasawa; Jinglu Hu","Advanced Research Institute for Science and Engineering, Waseda University, Kitakyushu, Fukuoka, Japan; School of Information, Production and Systems, Waseda University, Kitakyushu, Fukuoka, Japan; School of Information, Production and Systems, Waseda University, Kitakyushu, Fukuoka, Japan; School of Information, Production and Systems, Waseda University, Kitakyushu, Fukuoka, Japan","2006 IEEE International Conference on Evolutionary Computation","11 Sep 2006","2006","","","463","469","A new graph-based evolutionary algorithm called Genetic Network Programming (GNP) has been proposed. The solutions of GNP are represented as graph structures, which can improve the expression ability and performance. In addition, GNP with Reinforcement Learning (GNP-RL) has been proposed to search for solutions efficiently. GNP-RL can use current information and change its programs during task execution, i. e., online learning. Thus, it has an advantage over evolution-based algorithms in case much information can be obtained during task execution. GNP-RL has a special state-action space and it contributes to reducing the size of the Q-table and learning efficiently. The proposed method is applied to the controller of Khepera simulator and its performance is evaluated.","1941-0026","0-7803-9487-9","10.1109/CEC.2006.1688346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688346","","Learning;Economic indicators;Evolutionary computation;Genetic programming;Diversity reception;Continuous production;Production systems;Neural networks","genetic algorithms;graph theory;learning (artificial intelligence)","genetic network programming;reinforcement learning;Sarsa algorithm;graph-based evolutionary algorithm;online learning;Khepera simulator;performance evaluation","","2","","8","IEEE","11 Sep 2006","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Variable Impedance Control for High Precision Human-robot Collaboration Tasks","Y. Meng; J. Su; J. Wu","State Key Laboratory for Management and Control of Complex System, Institute of Automation, Chinese Academy of Science, Beijing, China; State Key Laboratory for Management and Control of Complex System, Institute of Automation, Chinese Academy of Science, Beijing, China; State Key Laboratory for Management and Control of Complex System, Institute of Automation, Chinese Academy of Science, Beijing, China","2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM)","15 Sep 2021","2021","","","560","565","Human-robot collaboration is an important area with great potential in intelligent manufacturing. Due to the diversity of collaboration tasks, robot collaboration skills should have the ability to adapt to different skills. However, problems such as skill expression and generalization are challenging. Meanwhile, the differences in the skills of various operators bring difficulties to collaborative robots. This work develops a variable impedance learning method for human-robot collaboration assembly. Unlike most previous work that mainly dis-cussed a special human collaborator with the fixed impedance parameters, this work learns a robot impedance by reinforcement learning. We aim to make the inertia, damping, and stiffness parameters adaptive by Proximal Policy Optimization (PPO) algorithm. Hence, we can let the robot collaborate with various human collaborators to accomplish a high-precision assembly task. Two experiment results illustrate the validity of the proposed method. The detailed experimental videos are available at https://youtu.be/AJyjW2NwA74.","","978-1-6654-3909-1","10.1109/ICARM52023.2021.9536100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9536100","","Service robots;Force;Collaboration;Reinforcement learning;Robot sensing systems;Manipulators;Trajectory","human-robot interaction;learning (artificial intelligence);optimisation;robotic assembly;robots","high precision human-robot collaboration tasks;intelligent manufacturing;collaborative robots;variable impedance learning method;human-robot collaboration assembly;robot impedance;reinforcement learning;human collaborators;high-precision assembly task;variable impedance control","","4","","18","IEEE","15 Sep 2021","","","IEEE","IEEE Conferences"
"Visual learning framework based on reinforcement learning","Fang Liu; Jianbo Su","Department of Automation, Shanghai Jiaotong University, Shanghai, China; Department of Automation, Shanghai Jiaotong University, Shanghai, China","Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788)","18 Oct 2004","2004","6","","4865","4868 Vol.6","This paper proposes a novel visual learning framework for attention control in active computer vision. The general hierarchical framework is constructed by using reinforcement learning to organize the image processing procedures and find optimal control strategy so as to efficiently reduce the computational cost. This framework allows the interactions between information in different levels and integration of visual modules with other machine learning algorithms, which make it possible to fulfill the specific task quickly by only processing relatively small quantities of data. The experiments of the selective attention on robot are provided to verify the effectiveness of the proposed framework.","","0-7803-8273-0","10.1109/WCICA.2004.1343635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1343635","","Optimal control;Computer vision;Image processing;Machine learning algorithms;Computational efficiency;Layout;Humans;Psychology;Image sampling;Cognitive robotics","learning (artificial intelligence);computer vision;optimal control;robots","visual learning framework;reinforcement learning;attention control;active computer vision;image processing;optimal control strategy;visual modules;machine learning algorithms","","","","15","IEEE","18 Oct 2004","","","IEEE","IEEE Conferences"
"Action Masked Deep Reinforcement learning for Controlling Industrial Assembly Lines","A. M. Ali; L. Tirel","Dept. of Mechanical and Aerospace Engineering, Carleton University, Ottawa, Canada; Dept. of Computer, Control, and Management Engineering, University of Rome ""La Sapienza"", Rome, Italy","2023 IEEE World AI IoT Congress (AIIoT)","13 Jul 2023","2023","","","0797","0803","The scalability of control algorithms controlling industrial assembly lines either model-based or model-free is still a big barrier from the point of view of implementation. In this paper, various Deep Reinforcement learning (DRL) algorithms will be tested in a virtual environment of a generic industrial assembly line in terms of optimally with respect to optimal exact solutions, and computational time. The agents will take bigger rewards when they minimize the finishing time while maintaining constraints, such as time precedence between tasks. The control actions will be modeled as a task assignment matrix that assigns the tasks to be completed to specific workstations. Another control action is the resource allocation. Due to the big number of constraints, many actions will be unfeasible leading to a longer time of training of the agent to learn not to apply unfeasible actions, therefore action masking will be introduced to the DRL algorithms to reduce the action space only to the feasible actions, thereby significantly reducing training time. Proximal Policy Optimization (PPO) agent shows faster and more stable convergence to the optimal solution over the Deep-Q Network (DQN) family, including the double DQN and Dueling one.","","979-8-3503-3761-7","10.1109/AIIoT58121.2023.10174426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174426","Industry 4.0;Deep Reinforcement learning;Action Masking;Assembly line Balancing Problem","Training;Deep learning;Computational modeling;Scalability;Virtual environments;Reinforcement learning;Workstations","deep learning (artificial intelligence);learning (artificial intelligence);optimisation;reinforcement learning;resource allocation","action masked Deep Reinforcement;action masking;action space;big barrier;computational time;control action;control algorithms;controlling industrial assembly lines;Deep Reinforcement learning algorithms;Deep-Q Network family;DRL algorithms;feasible actions;finishing time;generic industrial assembly line;maintaining constraints;optimal exact solutions;optimal solution;Proximal Policy Optimization agent shows;task assignment matrix;time precedence;training time;unfeasible actions;virtual environment","","","","20","IEEE","13 Jul 2023","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Framework for Train Rescheduling","Q. Shi; D. Cui; S. Yu; Z. Yuan; L. Cheng; P. Yue","The State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University Shenyang, China; The State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University Shenyang, China; The State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University Shenyang, China; China Academy of Railway Sciences, Beijing, China; Purification Equipment Research Institute of CSIC, Handan, China; The State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University Shenyang, China","2022 27th International Conference on Automation and Computing (ICAC)","10 Oct 2022","2022","","","1","6","With the development of computing intelligence, reinforcement learning has recently proposed to solve the problem of train timetable rescheduling (TTR). However, the traditional Q-learning is confronted with challegnes of high memory overhead and low convergence speed due to the large state and action space in TTR. Q-learning also suffers from limited capability of generalization, which requires the Q table has to be re-trained when dealing a new delay scenario. In order to accelerate the convergence speed and improve the generalization capability, this paper proposes a deep reinforcement learning (DQN) method to solve the TTR problem. First, a multi-stage Markov sequential decision model is established to represent the TTR problem. Secondly, to address the slow convergence speed due to the large action search space, an adaptive action generation method is adopted. In the proposed DQN, an objective network and an empirical replay mechanism are used, and the adoption of neural network improves the generalization of the reinforcement learning. The performance of the proposed DQN is evaluated by the simulation of a high-speed railway line in China. Compared with the Firt Come First Serve (FCFS) benchamark method, the rescheduled time table given by the proposed DQN has less total delays. The training iterations required for the DQN to converge is 50% less than that of a traditional Q-learning and has the capability to deal with various delay scenarios without retraining.","","978-1-6654-9807-4","10.1109/ICAC55051.2022.9911095","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9911095","deep reinforcement learning;train scheduling model;BP network","Training;Adaptation models;Q-learning;Scheduling algorithms;Neural networks;Search problems;Scheduling","deep learning (artificial intelligence);iterative methods;Markov processes;railways;reinforcement learning;scheduling;search problems","first come first serve benchamark method;high memory overhead;train timetable rescheduling;computing intelligence;traditional Q-learning;training iterations;high-speed railway line;adaptive action generation method;action search space;slow convergence speed;multistage Markov sequential decision model;TTR problem;DQN;deep reinforcement learning method;generalization capability;delay scenario","","","","10","IEEE","10 Oct 2022","","","IEEE","IEEE Conferences"
"Automatic Generation of Rescheduling Knowledge in Socio-technical Manufacturing Systems using Deep Reinforcement Learning","J. A. Palombarini; E. C. Martínez","Depto. Ing. en Sistemas de Información, UTN FRVM, Villa María, Argentina; Instituto de Desarrollo y Diseño (INGAR), CONICET-UTN, Santa Fe, Argentina","2018 IEEE Biennial Congress of Argentina (ARGENCON)","21 Feb 2019","2018","","","1","8","The generation of rescheduling knowledge for handling unforeseen events has become a key element of any real-time disruption management strategy, to ensuring a highly efficient production in increasing dynamic conditions without sacrificing cost effectiveness, product quality and on-time delivery, which are key competences in modern socio-technical manufacturing systems characterised by a diminishing predictability of environmental conditions at the shop-floor. In this work, a real-time rescheduling task is modelled and solved resorting to the integration of a schedule state simulator with an artificial agent that can learn successful schedule repairing policies directly from high-dimensional sensory inputs. The rescheduling knowledge is stored in a deep Q-network, which can be used reactively to select repair actions in order to make progress toward a goal schedule state. The network is trained using deep Q-learning with experience replay over a variety of simulated transitions between schedule states using the schedule visual images and negligible prior knowledge as input. Finally, an industrial example is discussed showing that the approach enables learning successful rescheduling policies and encoding task-specific knowledge that can be understood by human experts.","","978-1-5386-5032-5","10.1109/ARGENCON.2018.8646172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8646172","","Schedules;Task analysis;Reinforcement learning;Uncertainty;Manufacturing systems;Real-time systems;Maintenance engineering","knowledge management;learning (artificial intelligence);manufacturing systems;neural nets;production engineering computing;real-time systems;scheduling","rescheduling knowledge;deep reinforcement learning;product quality;on-time delivery;real-time rescheduling task;deep Q-network;deep Q-learning;real-time disruption management;socio-technical manufacturing systems","","7","","23","IEEE","21 Feb 2019","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Carrier-borne Aircraft Support Operation Scheduling","H. Feng; W. Zeng","Artificial Intelligence and Automation School, Huazhong University of Science and Technology, Wuhan, China; Artificial Intelligence and Automation School, Huazhong University of Science and Technology, Wuhan, China","2021 International Conference on Intelligent Computing, Automation and Applications (ICAA)","30 Dec 2021","2021","","","929","935","The makespan of support operations of carrier-borne aircraft is a key factor affecting the sortie generation rate. The support operation process involves multiple support resources and operational tasks should satisfy serial and parallel constraint relationships. The effective coordination of these processes can be considered as a multi-resource constrained multi-project scheduling problem (MRCMPSP), which is a complex NP-hard problem. In this paper, a deep reinforcement learning (RL) method is designed to solve the problem, including the image representation of the state, the definition of action mapping, and reward function. Deep convolution neural network and advantage actor-critic algorithm (A2C) are utilized to provide a new solution to the scheduling problem, and experimental results show that the effectiveness of the proposed algorithm.","","978-1-6654-3730-1","10.1109/ICAA53760.2021.00169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9653624","","Automation;Processor scheduling;NP-hard problem;Convolution;Neural networks;Reinforcement learning;Image representation","aerospace computing;aircraft;approximation theory;convolutional neural nets;deep learning (artificial intelligence);image representation;reinforcement learning;scheduling","carrier-borne aircraft support operation scheduling;sortie generation rate;support operation process;multiple support resources;serial constraint relationships;parallel constraint relationships;multiproject scheduling problem;NP-hard problem;deep reinforcement learning method;deep convolution neural network;advantage actor-critic algorithm","","","","13","IEEE","30 Dec 2021","","","IEEE","IEEE Conferences"
"A Distributed Path Planning Algorithm via Reinforcement Learning","K. Cai; G. Chen","College of Automation, Chongqing University, Chongqing, China; College of Automation, Chongqing University, Chongqing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","3365","3370","Recently, Q-Learning has gained increasing attention in path planning, due to its model-free feature and adaptability to dynamic environment. However, it requires vast storage space and suffers from slow convergence speed as the scale of planning space increases. In order to solve these problems, a kind of cooperation scheme is introduced. We divide the planning space into several sections and each section is equipped with a robot. Each robot is responsible for exploring the local space and sharing a small amount of local information with its neighboring robots. Asynchronous communication is considered in algorithm design, which facilitates the information interaction between robots. Additionally, a table is designed to record the most recent action taken in each state and the corresponding next state, which is used to update the Q-table. Finally, the efficacy of the proposed algorithm is validated in a maze.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055825","National Natural Science Foundation of China; Natural Science Foundation of Chongqing; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055825","Path planning;Q-learning;Value iteration;Distributed algorithms;Multiagent System","Asynchronous communication;Q-learning;Automation;Simulation;Path planning;Space exploration;Planning","learning (artificial intelligence);mobile robots;multi-robot systems;path planning;reinforcement learning","adaptability;algorithm design;cooperation scheme;distributed path planning algorithm;dynamic environment;information interaction;local information;local space;model-free feature;neighboring robots;planning space;Q-Learning;recent action;reinforcement Learning;slow convergence speed;space increases;suffers;vast storage space","","","","16","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"USV Target Interception Control With Reinforcement Learning and Motion Prediction Method","Y. Liu; Y. Wang; L. Dong","School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Cyber Science and Engineering, Southeast University, Nanjing, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","1050","1054","In this paper, an unmanned surface vehicle (USV) target interception problem is studied with reinforcement learning (RL)-based method. In the proposed new structure, the proximal policy optimization (PPO) and proportional derivative (PD) are combined. First, the PD controller is used to predict the interception position. Then, the PPO algorithm is trained to control the USV, so that it can move quickly to the predicted position. By comparing with the traditional PPO algorithm, the simulation results verify that the proposed algorithm spends less time solving the problem of the USV interception of a moving target.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023694","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023694","Reinforcement learning;Intercepting target;Motion prediction","Missiles;Automation;Simulation;Reinforcement learning;Prediction methods;Prediction algorithms;Trajectory","learning (artificial intelligence);mobile robots;multi-robot systems;optimisation;PD control;position control;remotely operated vehicles","interception position;interception problem;motion prediction method;moving target;PD controller;predicted position;reinforcement learning-based method;traditional PPO algorithm;unmanned surface vehicle;USV interception;USV target interception control","","","","9","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Heurestics and reinforcement learning in manufacturing control: Optimization by phases","A. Nassima; B. Bouziane; M. Amine","LIO University of Oran, Oran, Algeria; LIO University of Oran, Oran, Algeria; LIO University of Oran, Oran, Algeria","2013 International Conference on Control, Decision and Information Technologies (CoDIT)","23 Dec 2013","2013","","","798","803","Manufacturing control still continues to attract the attention of researchers for 3 decades already. But what concerns them most is to find a compromise between optimizing either for cost, time, ect. and responsiveness to cope with competition increasingly growing. In this paper we aim to develop a bi-phases manufacturing control, the first phase is assignment which is allocating resource to task, we use here an heuristic based on iterative optimization. Than the second phase sequencing which define a beginning and end date for each task, we use for that holon negotiation and reactive learning. we applied the developed approach on flexibal job shop (FJS).","","978-1-4673-5549-0","10.1109/CoDIT.2013.6689645","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6689645","Manufacturing control;Holon system;heterarchical structure;heurestics;Reinforcement learning","Job shop scheduling;Learning (artificial intelligence);Manufacturing;Sequential analysis;Control systems;Dynamic scheduling","distributed control;flexible manufacturing systems;industrial control;iterative methods;job shop scheduling;learning systems;optimisation;resource allocation","distributed dynamic control;FJS;flexible job shop;reactive learning;holon negotiation;second phase sequencing;iterative optimization;resource allocation;biphases manufacturing control;reinforcement learning","","","","22","IEEE","23 Dec 2013","","","IEEE","IEEE Conferences"
"Scheduling Semiconductor Testing Facility by Using Cuckoo Search Algorithm With Reinforcement Learning and Surrogate Modeling","Z. Cao; C. Lin; M. Zhou; R. Huang","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; Helen and John C. Hartmann Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China","IEEE Transactions on Automation Science and Engineering","4 Apr 2019","2019","16","2","825","837","A semiconductor final testing scheduling problem with multiresource constraints is considered in this paper, which is proved to be NP-hard. To minimize the makespan for this scheduling problem, a cuckoo search algorithm with reinforcement learning (RL) and surrogate modeling is presented. A parameter control scheme is proposed to ensure the desired diversification and intensification of population on the basis of RL, which uses the proportion of beneficial mutation as feedback information according to Rechenberg's 1/5 criterion. To reduce computational complexity, a surrogate model is employed to evaluate the relative ranking of solutions. A heuristic approach based on the relative ranking of encoding value and a modular function is proposed to convert continuous solutions obtained from Lévy flight into discrete ones. The computational complexity and convergence analysis results are presented. The proposed algorithm is validated with benchmark and randomly generated cases. Various simulation experiments and comparison between the proposed algorithm and several popular methods are performed to validate its effectiveness.","1558-3783","","10.1109/TASE.2018.2862380","National Natural Science Foundation of China(grant numbers:51375038,61403018); Natural Science Foundation of Beijing Municipality(grant numbers:4162046); Open Project Program of the State Key Laboratory of Synthetical Automation for Process Industries(grant numbers:PAL-N201804); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462749","Cuckoo search (CS) algorithm;reinforcement learning (RL);scheduling;semiconductor;surrogate modeling","Job shop scheduling;Computational modeling;Testing;Learning (artificial intelligence);Automation;Complexity theory","computational complexity;evolutionary computation;learning (artificial intelligence);optimisation;production engineering computing;scheduling;search problems;semiconductor device manufacture;semiconductor device testing","scheduling semiconductor testing facility;cuckoo search algorithm;reinforcement learning;semiconductor final testing scheduling problem;multiresource constraints;NP-hard;surrogate modeling;parameter control scheme;desired diversification;beneficial mutation;computational complexity;surrogate model;relative ranking;Rechenberg 1-5 criterion;Lévy flight;convergence analysis results","","101","","47","IEEE","12 Sep 2018","","","IEEE","IEEE Journals"
"Mastering the Complex Assembly Task With a Dual-Arm Robot: A Novel Reinforcement Learning Method","D. Jiang; H. Wang; Y. Lu","School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China; School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China; School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China","IEEE Robotics & Automation Magazine","14 Jun 2023","2023","30","2","57","66","Deep reinforcement learning (DRL) has achieved great success across multiple fields; however, in the field of robot control, the acquisition of large amounts of motion data from real robots is challenging. In this work, an algorithm is proposed to train a neural network model with a large amount of data in a simulated environment and then transfer the model to the real environment. Proximal policy optimization (PPO) is used to train the agent in simulator, and generative adversarial imitation learning (GAIL) is specified to transfer the model to the real world. The algorithm can guide the two-armed robot to complete the task well in the face of complex assembly tasks. A total of three tasks of different difficulties were set to test the performance of the algorithm. In a large experimental study, the proposed algorithm outperforms other algorithms, and the real robot arm completes the assembly task significantly faster than script and keyboard operations.","1558-223X","","10.1109/MRA.2023.3262461","The National Key R&D Program of China(grant numbers:2021YFF0306405); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10113133","","Robots;Task analysis;Manipulators;Neural networks;Data models;Reinforcement learning;Service robots;Deep learning;Generative adversarial networks","control engineering computing;deep learning (artificial intelligence);mobile robots;reinforcement learning","complex assembly task;deep reinforcement learning;DRL;dual-arm robot;GAIL;generative adversarial imitation learning;motion data;neural network model;PPO;proximal policy optimization;real robot arm;reinforcement learning method;robot arm;robot control;simulated environment;two-armed robot","","1","","19","IEEE","1 May 2023","","","IEEE","IEEE Magazines"
"Research on Intelligent Warehouse Path Planning Based on Multi-Objective Reinforcement Learning","J. Yang; W. Yu; H. Wang","School of Software, Southeast University, Nanjing, China; Frontiers Science Center for Mobile Information Communication and Security, School of Mathematics, Southeast University, Nanjing, China; School of Mathematics, Southeast University, Nanjing, China","2023 IEEE 13th International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","28 Sep 2023","2023","","","1143","1148","In the context of intelligent warehousing, path planning is a crucial means for the efficient and safe handling of materials. Traditional path planning methods usually only consider the shortest path and fail to balance multiple objectives, such as maximizing cargo load and avoiding congestion. To address this issue, we propose a novel multi-policy multi-objective reinforcement learning approach for intelligent warehouse path planning, focusing on the path planning of Automated Guided Vehicles (AGVs) within the warehouse environment. Our method simultaneously optimizes multiple objectives, using our algorithm to achieve trade-offs among them and obtain multiple policies. Experimental results in simulated warehouse environments show that our method outperforms other state-of-the-art approaches in terms of multiple objectives. The proposed approach offers a promising solution for intelligent path planning in modern warehouses.","2642-6633","979-8-3503-1519-6","10.1109/CYBER59472.2023.10256597","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256597","","Uncertainty;Remotely guided vehicles;Warehousing;Decision making;Focusing;Reinforcement learning;Path planning","automatic guided vehicles;learning (artificial intelligence);mobile robots;optimisation;path planning;reinforcement learning;warehouse automation;warehousing","avoiding congestion;cargo load;intelligent path;intelligent warehouse path;intelligent warehousing;modern warehouses;multiple policies;novel multipolicy multiobjective reinforcement learning approach;path planning;shortest path;simulated warehouse environments;traditional path;warehouse environment","","","","12","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Multi mobile robot navigation using distributed value function reinforcement learning","S. Babvey; O. Momtahan; M. R. Meybodi","Amirkabir University of Technology슠, Tehran, Iran; Georgia Institute of Technology, Atlanta, USA; Amirkabir University of Technology슠, Tehran, Iran","2003 IEEE International Conference on Robotics and Automation (Cat. No.03CH37422)","10 Nov 2003","2003","1","","957","962 vol.1","In this paper we propose a new fuzzy-based navigation system for two intelligent mobile robots using distributed value function reinforcement learning. The robots use their sensors to provide information about their workspace. A fuzzy controller uses this information to select a proper action for the currently sensed state. The parameters of the input and output fuzzy membership functions are determined by a learning automation at each time step based on the sparseness of the obstacles. Therefore the robots learn to control their velocity, and attention range regarding the density of the obstacles in the workspace. The distributed approach enables the robots to learn more than one simple behavior concurrently. So, in contrast to the existing methods no behavior blending is needed. This approach also enables the robots to learn a value function, which is an estimate of future rewards for both of them. In other words cooperation is maintained and each robot learns to execute the actions that are good for both of them. The proposed controller has a very simple architecture and clear logic. The time and computation cost are low and it can adapt well to environment changes. Computer simulations are used to investigate the effectiveness of the controller.","1050-4729","0-7803-7736-2","10.1109/ROBOT.2003.1241716","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1241716","","Mobile robots;Navigation;Learning;Robotics and automation;Intelligent sensors;Robot sensing systems;Automatic control;Intelligent systems;Intelligent robots;Fuzzy control","multi-robot systems;mobile robots;intelligent sensors;intelligent robots;fuzzy control;velocity control;adaptive control;learning (artificial intelligence);learning automata;fuzzy logic;control engineering computing","multimobile robot navigation;distributed value function reinforcement learning;fuzzy-based navigation system;intelligent mobile robots;robot sensors;adaptive fuzzy controller;learning automation;velocity control;attention range control;logic controller;control architecture;computation cost","","3","","11","IEEE","10 Nov 2003","","","IEEE","IEEE Conferences"
"Fabricatio-Rl: A Reinforcement Learning Simulation Framework For Production Scheduling","A. Rinciog; A. Meyer","Department of Mechanical Engineering, Leonhard-Euler-Straße 5, TU Dortmund University, Dortmund, GERMANY; Department of Mechanical Engineering, Leonhard-Euler-Straße 5, TU Dortmund University, Dortmund, GERMANY","2021 Winter Simulation Conference (WSC)","23 Feb 2022","2021","","","1","12","Production scheduling is the task of assigning job operations to processing resources such that a target goal is optimized. constraints on job structure and resource capabilities, including stochastic influences, e.g. job arrivals, define individual problems. Reinforcement learning (RL) solvers are adaptive and potentially robust in highly stochastic settings. However, benchmarking RL solutions for stochastic problems is challenging, requiring the simulation of complex production settings while guaranteeing reproducible stochasticity. No such simulation is currently available. To cover this gap, we introduce FabricatioRL, an RL compatible, customizable and extensible benchmarking simulation framework. Our contribution is twofold: We first derive requirements to ensure that generic production setups can be covered, the simulation framework can interface with both traditional approaches and RL, and experiments are reproducible. Then, we detail the FabricatioRL design and implementation satisfying the obtained requirements in terms of framework input, core simulation process, and the interface with different scheduling systems.","1558-4305","978-1-6654-3311-2","10.1109/WSC52266.2021.9715366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9715366","","Adaptation models;Runtime;Systematics;Digital twin;Production;Reinforcement learning;Benchmark testing","computer simulation;production control;reinforcement learning;scheduling;stochastic processes","production scheduling;job operations;job structure;resource capabilities;reinforcement learning solvers;stochastic problems;complex production settings;reproducible stochasticity;generic production setups;core simulation process;benchmarking simulation;Fabricatio-RL;reinforcement learning simulation","","3","","43","IEEE","23 Feb 2022","","","IEEE","IEEE Conferences"
"Reinforcement learning congestion controller for multimedia surveillance system","Ming-Chang Hsiao; Kao-Shing Hwang; Shun-Wen Tan; Cheng-Shong Wu","Department of Electrical Engineering, National Chung Cheng University, Chiayi, Taiwan; Department of Electrical Engineering, National Chung Cheng University, Chiayi, Taiwan; Department of Electrical Engineering, National Chung Cheng University, Chiayi, Taiwan; Department of Electrical Engineering, National Chung Cheng University, Chiayi, Taiwan","2003 IEEE International Conference on Robotics and Automation (Cat. No.03CH37422)","10 Nov 2003","2003","3","","4403","4407 vol.3","The use of reinforcement learning scheme for congestion control in factory surveillance network is presented in this paper. Traditional methods perform congestion control by means of monitoring the queue length. When the queue length is greater than a predefined threshold, the source rate is decreased at a fixed rate. However, the determination of the congested threshold and sending rate is difficult for these methods. We adopted a simple reinforcement learning method, called Adaptive Heuristic Critic (AHC), to solve the problem. The AHC controller maintains an expectation of reward and takes the best policy to control source flow. By way of learning and then taking right actions, simulation results have shown that the approach can promote the system utilization and decrease packet loss.","1050-4729","0-7803-7736-2","10.1109/ROBOT.2003.1242282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1242282","","Learning;Control systems;Multimedia systems;Surveillance;Neural networks;Automatic control;Multiplexing;High-speed networks;Communication system traffic control;Traffic control","telecommunication congestion control;multimedia communication;adaptive control;learning systems;surveillance;learning (artificial intelligence);factory automation","reinforcement learning method;congestion controller;multimedia surveillance system;queue length monitoring;factory surveillance network;sending rate;adaptive heuristic critic controller;packet loss;factory automation","","2","","7","IEEE","10 Nov 2003","","","IEEE","IEEE Conferences"
"Embedding Reinforcement Learning in Simulation","A. AboElHassan; S. Yacout","Mathematical and Industrial Engineering Polytechnique Montréal, Montréal, Canada; Mathematical and Industrial Engineering Polytechnique Montréal, Montréal, Canada","2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA )","30 Nov 2021","2021","","","1","6","Reinforcement learning (RL) usage in the industrial domain is on the rise since the shift towards Industry4.0 systems. The need for faster adaptive systems is encouraging manufacturers to invest more in artificial intelligence (AI) technologies. Our aim is to add better intelligence into simulation tools by embedding RL capabilities. Discrete event simulations have been used for decision support in manufacturing systems for decades. New simulation tools such as AnyLogic have improved significantly in the past few years. In this paper, we built a RL library for AnyLogic simulation models. The RL library is developed for model designers, who may not be experts in the field of RL. We applied our RL library on a real-world use case model for truck dispatching problem. The results show the benefits of using RL in real-world problems to find better dispatching policies. Additionally, the visualization capabilities of AnyLogic enabled us to explain the RL agent's reaction to changes in the system.","","978-1-7281-2989-1","10.1109/ETFA45728.2021.9613611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9613611","Discrete Event Simulation;Agent-Based Modeling;Decision-Making;Dispatching;Reinforcement Learning","Visualization;Adaptation models;Reinforcement learning;Tools;Programming;Libraries;Dispatching","artificial intelligence;decision support systems;discrete event simulation;dispatching;learning (artificial intelligence);production engineering computing","artificial intelligence technologies;discrete event simulations;anylogic simulation models;real-world use case model;embedding reinforcement learning usage;Industry 4.0;dispatching problem;visualization capability","","","","16","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Algorithm to Sustain Temperature in Pharma Supply Chain Management","V. Ramasamy; P. Pravinkumar","Department of Electronics and Communication Engineering, School of Electrical and Electronics Engineering, SASTRA Deemed University, Thanjavur, INDIA; Department of Electronics and Communication Engineering, School of Electrical and Electronics Engineering, SASTRA Deemed University, Thanjavur, INDIA","2023 International Conference on Computer Communication and Informatics (ICCCI)","24 May 2023","2023","","","1","3","The cold supply chain in the pharmaceutical industry is a technique designed to transport and store temperature-sensitive vaccines to sustain their effectiveness and to uphold safety measures as they are directly involved with human life. The reinforcement learning algorithm is a feedback mechanism using machine learning where the agent, environment, action, state and rewards are correlated. This paper is to sustain a cold supply chain for vaccines like Lupride and Degapride in Sun pharmaceuticals; a reinforcement learning algorithm has been devised to measure, monitor and correct the temperature of the cold storage unit. Temperature-sensitive vaccines need to maintain their chemical and biological traits during their transit from packaging to delivery.","2473-7577","979-8-3503-4821-7","10.1109/ICCCI56745.2023.10128458","Science and Engineering Research Board; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10128458","component; Reinforcement learning;Temperature;cold supply chain;Pharmaceuticals","Temperature measurement;Temperature sensors;Machine learning algorithms;Supply chain management;Supply chains;Reinforcement learning;Vaccines","blockchains;cold storage;learning (artificial intelligence);pharmaceutical industry;production engineering computing;supply chain management;supply chains","cold storage unit;cold supply chain;human life;machine learning;pharma supply chain management;pharmaceutical industry;reinforcement learning algorithm;safety measures;Sun pharmaceuticals;sustain temperature;temperature-sensitive vaccines","","","","8","IEEE","24 May 2023","","","IEEE","IEEE Conferences"
"A General Approach for the Automation of Hydraulic Excavator Arms Using Reinforcement Learning","P. Egli; M. Hutter","Robotic Systems Lab, ETH Zurich, Zürich, Switzerland; Robotic Systems Lab, ETH Zurich, Zürich, Switzerland","IEEE Robotics and Automation Letters","28 Mar 2022","2022","7","2","5679","5686","This article presents a general approach to derive an end effector trajectory tracking controller for highly nonlinear hydraulic excavator arms. Rather than requiring an analytical model of the system, we use a neural network model that is trained based on measurements collected during operation of the machine. The data-driven model effectively represents the actuator dynamics including the cylinder-to-joint-space conversion. Requiring only the distances between the individual joints, a simulation is set up to train a control policy using reinforcement learning (RL). The policy outputs pilot stage control commands that can be directly applied to the machine without further fine-tuning. The proposed approach is implemented on a Menzi Muck M545, a 12 $\mathrm{t}$ hydraulic excavator, and tested in different task space trajectory tracking scenarios, with and without soil interaction. Compared to a commercial grading controller, which requires laborious hand-tuning by expert engineers, the learned controller shows higher tracking accuracy, indicating that the achieved performance is sufficient for the practical application on construction sites and that the proposed approach opens a new avenue for future machine automation.","2377-3766","","10.1109/LRA.2022.3152865","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; National Centre of Competence in Digital Fabrication; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9743573","Robotics and automation in construction;reinforcement learning;autonomous excavator;sim-to-real","Analytical models;Automation;Trajectory tracking;Neural networks;Hydraulic systems;Reinforcement learning;Arms","adaptive control;end effectors;excavators;learning (artificial intelligence);manipulator dynamics;motion control;neural nets;neurocontrollers;position control;tracking;trajectory control","12 t hydraulic excavator;actuator dynamics;analytical model;commercial grading controller;control policy;cylinder-to-joint-space conversion;data-driven model;different task space trajectory tracking scenarios;end effector trajectory;fine-tuning;future machine automation;higher tracking accuracy;highly nonlinear hydraulic excavator arms;individual joints;laborious hand-tuning;learned controller;Menzi Muck M545;neural network model;pilot stage control commands;reinforcement learning","","9","","17","IEEE","28 Mar 2022","","","IEEE","IEEE Journals"
"Mapless navigation based on continuous deep reinforcement learning","X. Chen; L. Su; H. Dai","School of Electrical Engineering and Automation, Xiamen University of Technology, Xiamen, China; School of Electrical Engineering and Automation, Xiamen University of Technology, Xiamen, China; Quanzhou Institute of Equipment Manufacturing Haixi Institutes, Chinese Academy of Sciences, Quanzhou, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","6758","6763","This paper proposes a map-free navigation scheme based on continuous deep reinforcement learning to solve the problem that robots cannot flexibly avoid obstacles and navigate in a dynamic environment. The reinforcement learning algorithm used in this article is near-end strategy optimization (proximal strategy optimization, PPO), and the benchmark algorithm is the discrete deep reinforcement learning algorithm Deep Q network algorithm (Deep Q network, DQN).Experiments in the Gazebo simulation environment prove that the training efficiency and success rate of the PPO algorithm are much higher than that of the DQN algorithm. In this paper, the trained strategy model in the simulation environment is directly transplanted to the actual robot. The experimental results verify that the physical robot can have good navigation and obstacle avoidance capabilities without training again. The tested single-target navigation success rate is 80%, and the multi-target navigation success rate is 70%.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727354","Reinforcement learning;Proximal policy optimization;Policy migration;Deep Q network","Training;Navigation;Heuristic algorithms;Reinforcement learning;Radar;Games;Stability analysis","collision avoidance;control engineering computing;learning (artificial intelligence);mobile robots;navigation;path planning;robot vision","mapless navigation;continuous deep reinforcement learning;map-free navigation scheme;near-end strategy optimization;proximal strategy optimization;benchmark algorithm;discrete deep reinforcement learning algorithm Deep Q network;Gazebo simulation environment;PPO algorithm;DQN algorithm;trained strategy model;good navigation;obstacle avoidance capabilities;single-target navigation success rate;multitarget navigation success rate","","","","20","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Research and Application of Reinforcement Learning Based on Constraint MDP in Coal Mine","Z. Xiao-hu; Z. Ke-ke; W. Qing-qing; M. Fang-qing","College of information and electrics, China University of Mining and Technology, Xuzhou, Jiangsu, China; College of information and electrics, China University of Mining and Technology, Xuzhou, Jiangsu, China; College of information and electrics, China University of Mining and Technology, Xuzhou, Jiangsu, China; College of information and electrics, China University of Mining and Technology, Xuzhou, Jiangsu, China","2009 WRI World Congress on Computer Science and Information Engineering","24 Jul 2009","2009","4","","687","691","Reinforcement learning is an algorithm without model which is learning what to do--how to map situations to actions-so as to maximize a numerical reward signal. Reinforcement learning provides an available method to the systems, which are very difficult to build up accurate models around complex environment. But now many practical problems demand a maximum reward with not much cost (expense). For example, the production of coal mine is closely correlated with security in that it increases production in the limited range of security situation. On the base of Markov decision process (MDP) and reinforcement learning, the paper introduced constraint Markov decision process into reinforcement learning. The paper improved Q-learning algorithm with adding cost factor and gave a new Q-learning algorithm based on constraint MDP. Finally, according to the constraint between production and safety in coal mine, the paper made the simulation investigation about the action control of coal shearer in coal mine working face. The simulation result had verified the validity of the method.","","978-0-7695-3507-4","10.1109/CSIE.2009.587","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5171084","constraint MDP;reinforcement learning;Q-learning;cost;coal shearer","Learning;Security;Production;Cost function;Guidelines;Computer science;Application software;Educational institutions;Product safety;Mining industry","coal;learning (artificial intelligence);Markov processes;mining;safety systems","reinforcement learning;constraint MDP;coal mine safety;Markov decision process;Q-learning algorithm;cost factor;coal production;numerical reward signal;security situation","","","","6","IEEE","24 Jul 2009","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning of Cooperative Control with Four Robotic Agents by MADDPG","Z. Wang; R. Wan; X. Gui; G. Zhou","School of Electronic and Electrical Engineering, Wuhan Textile University, Wuhan, China; School of Electronic and Electrical Engineering, Wuhan Textile University, Wuhan, China; School of Electronic and Electrical Engineering, Wuhan Textile University, Wuhan, China; Institute of Engineering and Technology, Fragrant City Institute of Intelligent Electromechanical Industry and Technology of Hubei Province, Hubei University of Science and Technology, Xianning, China","2020 International Conference on Computer Engineering and Intelligent Control (ICCEIC)","1 Mar 2021","2020","","","287","290","Due to the nature of complexity, inflexibility and non-robustness of classical cooperative control algorithms, the deep reinforcement learning has been widely researched and applied in collective and continuous behaviour control. Especially for multi-agents in real world, acquiring a full view world with a quick learning is still a great challenge. Inspired by Policy Gradient (PG) and its successors, a toy model with multi-agents by four two-dimensional manipulators environment is built based on physics engine-based MuJoCo. With a modified deep deterministic policy gradient algorithm and different credit strategies for individual agent, the cooperation and competition behaviour to target location between agents are studied. The experimental results show that each robot can complete the task with a negligible convergence effect, indicating that the MADDPG algorithm has a good performance in a complex environment, and successfully learn the strategy of multi-agent collaboration. However, with the instability of the environment caused by the increase in the number of agents, deep reinforcement learning has certain difficulties in the joint action space.","","978-1-7281-8573-6","10.1109/ICCEIC51584.2020.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361848","deep reinforcement learning;MADDPG algorithm;multi-agent collaboration","Training;Robot kinematics;Toy manufacturing industry;Collaboration;Reinforcement learning;Aerospace electronics;Task analysis","control engineering computing;deep learning (artificial intelligence);gradient methods;intelligent robots;manipulators;multi-agent systems;multi-robot systems","deep deterministic policy gradient algorithm;MADDPG;multiagent collaboration;deep reinforcement learning;robotic agents;collective behaviour control;continuous behaviour control;multiagents;physics engine-based MuJoCo;cooperative control;two-dimensional manipulators environment;credit strategies;cooperation behaviour;competition behaviour;convergence effect","","3","","6","IEEE","1 Mar 2021","","","IEEE","IEEE Conferences"
"Effective strategies for complex skill real-time learning using reinforcement learning","Wei Ying-zi; Zhao Ming-yang","Graduate School of Chinese Academy of Sciences, Beijing, China; Robotics Laboratory, Shenyang Institute ofAutomation, Chinese Academy and Sciences, Shenyang, China","IEEE International Conference on Robotics, Intelligent Systems and Signal Processing, 2003. Proceedings. 2003","19 Apr 2004","2003","1","","388","392 vol.1","Following the principle of human skill learning, robot acquiring skill is a process similar to human skill learning. Reinforcement learning is on-line actor critic method for robot to develop its skill. The reinforcement Junction has become the critical component for its effect of evaluating the action and guiding the learning process. A difference form of augmented reward function is considered carefully. In this paper we present a strategy for the task of complex skill learning. Automatic robot shaping policy is to dissolve the complex skill into a hierarchical learning process. Variable resolution discretization of input space is introduced to improve the generalization capability of CMAC-based RL. Conventional e -greedy policy has the shortage of unnecessary randomization. Boltzmann distribution selection is also introduced to the balance of exploration and exploitation. We describe our ideas of reinforcement learning methods and also illustrate with an example the utility of method for learning skilled robot control on line.","","0-7803-7925-X","10.1109/RISSP.2003.1285605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285605","","Learning;Humans;Robotics and automation;Robot vision systems;Computer science;Laboratories;Orbital robotics;Boltzmann distribution;Robot control;Manufacturing","learning (artificial intelligence);Boltzmann equation;robots;Markov processes;cerebellar model arithmetic computers","complex skill real-time learning;reinforcement learning method;human skill learning;online actor critic method;augmented reward function;complex skill learning;automatic robot shaping policy;hierarchal learning process;variable resolution discretization;CMAC-based RL;/spl epsi/-greedy policy;unnecessary randomization;Boltzmann distribution selection;online skilled robot control","","1","1","15","IEEE","19 Apr 2004","","","IEEE","IEEE Conferences"
"Development of simulator for efficient aquaculture of Sillago japonica using reinforcement learning","H. Kuroki; H. Ikeoka; K. Isawa","Faculty of Engineering Department of Computer Science, Fukuyama University, Fukuyama, Japan; Faculty of Engineering Department of Computer Science, Fukuyama University, Fukuyama, Japan; Faculty of Engineering Department of Architecture, Fukuyama University, Fukuyama, Japan","2020 International Conference on Image Processing and Robotics (ICIP)","5 Mar 2021","2020","","","1","4","Recently, the situation in the Japanese fishing industry has become critical, resulting in one of the most significant food issues in Japan. Aquaculture technology is expected to be a solution to this problem. Sillago japonica is a fish that inhabits shallow waters in parts of Asia, and large Sillago japonica is very expensive. Therefore, we believe that aquaculture of this fish would help revitalize the fishing industry in Japan. However, aquaculture requires considerable manual labor. Hence, we need to introduce new technologies for the aquaculture of Sillago japonica. Specifically, we have been developing two systems to improve efficiency and reduce costs of this aquaculture: an environment control system and an automatic feeding system. The former is to maintain favorable environment conditions for the fish in the aquaculture tank. The latter is for optimal feeding of the fish. In this paper, we describe the development of an automatic feeding system using artificial intelligence (AI). This system includes four processes: image input, image recognition, feeder control, and feeding action. We have adopted AI technologies to assist in the second and third processes. Although these two processes can be implemented together, it is easier for the AI to learn them as two separate processes. In particular, the second (image recognition) process uses supervised learning, and the third (feeder control) process uses reinforcement learning. However, it is impractical to train the AI in the third process in a real-world aquaculture environment that sustains many failures. Therefore, we have developed an aquaculture simulator to facilitate AI learning of the feeder control process. Additionally, we performed an experiment to validate our simulator using the number of feeders and the number of fish as a parameter.","","978-1-7281-6541-7","10.1109/ICIP48927.2020.9367369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9367369","artificial intelligence;simulation;reinforcement learning;aquaculture","Industries;Image recognition;Process control;Reinforcement learning;Fish;Artificial intelligence;Aquaculture","aquaculture;fishing industry;image recognition;learning (artificial intelligence)","aquaculture tank;environment control system;Japan;significant food issues;Japanese fishing industry;Sillago japonica;efficient aquaculture;feeder control process;AI learning;aquaculture simulator;real-world aquaculture environment;reinforcement learning;automatic feeding system;fish","","3","","12","IEEE","5 Mar 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Method for Autonomous Navigation of Mobile Robots in Unknown Environments","R. Van Hoa; T. D. Chuyen; N. T. Lam; T. N. Son; N. D. Dien; V. T. T. Linh","Faculty of Electrical Engineering, University of Economics - Technology for Industries, Ha Noi, Viet Nam; Department of Industrial Automation, School of Electrical Engineering, Hanoi University of Science and Technology, Ha Noi, Viet Nam; Faculty of Electrical Engineering, University of Economics - Technology for Industries, Ha Noi, Viet Nam; Faculty of Electrical Engineering, University of Economics - Technology for Industries, Ha Noi, Viet Nam; Faculty of Electrical Engineering, University of Economics - Technology for Industries, Ha Noi, Viet Nam; Faculty of Electrical Engineering, University of Economics - Technology for Industries, Ha Noi, Viet Nam","2020 International Conference on Advanced Mechatronic Systems (ICAMechS)","6 Jan 2021","2020","","","266","269","The Reinforcement Learning is a subset of machine learning that deals with learning decisions from rewards given by the environment. The model classic reinforcement learning (RL) algorithms are usually applied to small sets of states and an action. However, in real applications, the state spaces are of a large scale and this will bring the problems in the generalization and the curse of dimensionality. In this research, authors integrate neural networks into reinforcement learning methods to generalize the value of all the states. The simulation results on the Gazebo software framework show the feasibility of the model proposed method algorithm. The robot can safely navigate an unprotected work environment and becomes a truly intelligent system with the ability to learn and adapt itself to the model.","2325-0690","978-1-7281-6530-1","10.1109/ICAMechS49982.2020.9310129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9310129","Artificial Intelligence;Reinforcement Learning;Neural Network;Autonomous Navigation;Mobile Robots","Robots;Reinforcement learning;Robot sensing systems;Navigation;Service robots;Electrical engineering;Autonomous robots","learning (artificial intelligence);mobile robots;navigation;neural nets;path planning","unprotected work environment;autonomous navigation;mobile robots;machine learning;reinforcement learning algorithms;unknown environments;Gazebo software framework;neural networks","","","","23","IEEE","6 Jan 2021","","","IEEE","IEEE Conferences"
"A study of reinforcement learning with knowledge sharing for distributed autonomous system","K. Ito; A. Gofuku; Y. Imoto; M. Takeshita","Department of Syst.ems Engineering, Faculty of Engineering, Ohyama University, Japan; Department of Syst.ems Engineering, Faculty of Engineering, Ohyama University, Japan; Department of Syst.ems Engineering, Faculty of Engineering, Ohyama University, Japan; Department of Syst.ems Engineering, Faculty of Engineering, Ohyama University, Japan","Proceedings 2003 IEEE International Symposium on Computational Intelligence in Robotics and Automation. Computational Intelligence in Robotics and Automation for the New Millennium (Cat. No.03EX694)","18 Aug 2003","2003","3","","1120","1125 vol.3","Reinforcement learning is one of effective controller for autonomous robots. Because it does not need priori knowledge and behaviors to complete given tasks are obtained automatically be repeating trial and error. However a large number of trials are required to realize complex tasks. So the task that can be obtained using the real robot is restricted to simple ones. Considering these points, various methods that prove the learning cost of reinforcement learning have been proposed. In the method that uses priori knowledge, the methods lose the autonomy that is most important feature of reinforcement learning in applying it to the robots. In the Dyna-Q, that is one of simple and effective reinforcement learning architecture integrating online planning, a model of environment is learned from real experience and by utilizing the model to learn, the learning time is decreased. In this architecture, the autonomy is held, however the model depends on the task, so acquired knowledge of environment cannot be reused to other tasks. In the real world, human beings can learn various behaviors to complete complex tasks without priori knowledge of the tasks. We can try to realize the task in our image without moving our body. After the training in the image, by trying to the real environment, we save time to learn. It means that we have model of environment and we utilize the model to learn. We consider that the key ability that makes the learning process faster is construction of environment model and utilization of it. In this paper, we have proposed a method to obtain an environment model that is independent of the task. And by utilizing the model we have decreased learning time. We consider distributed autonomous agents, and we show that the environment model is constructed quickly by sharing the experience of each agent, even when each agent has own independent task. To demonstrate the effectiveness of the proposed method, we have applied the method to the Q-learning and simulations of a puddle world are carried out. As a result effective behaviors have been obtained quickly.","","0-7803-7866-0","10.1109/CIRA.2003.1222154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1222154","","Learning;Knowledge engineering;Robotics and automation;Systems engineering and theory;Costs;Automatic control;Robot control;Humans;Indium tin oxide;Control systems","robots;knowledge based systems;learning (artificial intelligence);planning (artificial intelligence)","reinforcement learning;knowledge sharing;distributed autonomous system;autonomous robots;priori knowledge;Dyna-Q;online planning;complex task;learning process;Q-learning;puddle world","","5","","18","IEEE","18 Aug 2003","","","IEEE","IEEE Conferences"
"VGN: Value Decomposition With Graph Attention Networks for Multiagent Reinforcement Learning","Q. Wei; Y. Li; J. Zhang; F. -Y. Wang","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China, also with the School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China, and also with the Institute of Systems Engineering, Macau University of Science and Technology, Macau 999078, China.; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China.; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China.; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China, also with the Institute of Systems Engineering, Macau University of Science and Technology, Macau 999078, China, and also with the Qingdao Academy of Intelligent Industries, Qingdao 266109, China.","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","14","Although value decomposition networks and the follow on value-based studies factorizes the joint reward function to individual reward functions for a kind of cooperative multiagent reinforcement problem, in which each agent has its local observation and shares a joint reward signal, most of the previous efforts, however, ignored the graphical information between agents. In this article, a new value decomposition with graph attention network (VGN) method is developed to solve the value functions by introducing the dynamical relationships between agents. It is pointed out that the decomposition factor of an agent in our approach can be influenced by the reward signals of all the related agents and two graphical neural network-based algorithms (VGN-Linear and VGN-Nonlinear) are designed to solve the value functions of each agent. It can be proved theoretically that the present methods satisfy the factorizable condition in the centralized training process. The performance of the present methods is evaluated on the StarCraft Multiagent Challenge (SMAC) benchmark. Experiment results show that our method outperforms the state-of-the-art value-based multiagent reinforcement algorithms, especially when the tasks are with very hard level and challenging for existing methods.","2162-2388","","10.1109/TNNLS.2022.3172572","National Key Research and Development Program of China(grant numbers:2021YFE0206100); National Natural Science Foundation of China(grant numbers:62073321); National Defense Basic Scientific Research Program(grant numbers:JCKY2019203C029); Science and Technology Development Fund Macau(grant numbers:0015/2020/AMJ); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777847","Deep learning;graph attention networks (GATs);multiagent systems;reinforcement learning.","Mathematical models;Task analysis;Games;Q-learning;Neural networks;Behavioral sciences;Training","","","","2","","","IEEE","18 May 2022","","","IEEE","IEEE Early Access Articles"
"Path Planning Method With Improved Artificial Potential Field—A Reinforcement Learning Perspective","Q. Yao; Z. Zheng; L. Qi; H. Yuan; X. Guo; M. Zhao; Z. Liu; T. Yang","Department of Digital Factory, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Department of Digital Factory, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, USA; College of Computer and Communication Engineering, Liaoning Shihua University, Fushun, China; Department of Digital Factory, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Department of Digital Factory, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Department of Digital Factory, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China","IEEE Access","31 Jul 2020","2020","8","","135513","135523","The artificial potential field approach is an efficient path planning method. However, to deal with the local-stable-point problem in complex environments, it needs to modify the potential field and increases the complexity of the algorithm. This study combines improved black-hole potential field and reinforcement learning to solve the problems which are scenarios of local-stable-points. The black-hole potential field is used as the environment in a reinforcement learning algorithm. Agents automatically adapt to the environment and learn how to utilize basic environmental information to find targets. Moreover, trained agents adopt variable environments with the curriculum learning method. Meanwhile, the visualization of the avoidance process demonstrates how agents avoid obstacles and reach the target. Our method is evaluated under static and dynamic experiments. The results show that agents automatically learn how to jump out of local stability points without prior knowledge.","2169-3536","","10.1109/ACCESS.2020.3011211","National Basic Research Program of China (973 Program)(grant numbers:2018YFF0214704); Liaoning Revitalization Talents Program(grant numbers:XLYC1907166); Liaoning Province Department of Education Foundation of China(grant numbers:L2019027); Liaoning Province Dr. Research Foundation of China(grant numbers:20170520135); National Natural Science Foundation of China(grant numbers:61903229,61973180,61802015); Natural Science Foundation of Shandong Province(grant numbers:ZR2019BF004,ZR2019BF041); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9146273","Reinforcement learning;neural network;potential field;path planning","Path planning;Learning (artificial intelligence);Gravity;Potential energy;Mobile agents;Real-time systems","collision avoidance;learning (artificial intelligence);mobile robots;multi-robot systems;stability","improved artificial potential field;local stability points;curriculum learning method;variable environments;reinforcement learning algorithm;local-stable-points;black-hole potential field;complex environments;local-stable-point problem;efficient path planning method","","71","","43","CCBY","22 Jul 2020","","","IEEE","IEEE Journals"
"Research on the Problem of 3D Bin Packing under Incomplete Information Based on Deep Reinforcement Learning","Y. Wu; L. Yao","School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China","2021 International Conference on E-Commerce and E-Management (ICECEM)","17 Dec 2021","2021","","","38","42","The Bin Packing Problem (BPP) in the logistics industry is a classic NP-hard problem. In practical applications, often only the size information of the current box can be obtained whereas getting the information of the subsequent boxes almost impossible. In consequence, an algorithm is very important for giving the packing position in the case of incomplete information. This paper used Deep Reinforcement Learning (DRL) algorithm and Monte Carlo Tree Search (MCTS), formed the state input shape for this problem to establish a model to solve the 3D bin packing problem under incomplete information. This model can achieve an average space utilization of 65%. The study's results proved that the model can solve the packing problem under incomplete information and has certain practical benefits.","","978-1-6654-1025-0","10.1109/ICECEM54757.2021.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636905","bin packing problem;deep Q-learning;Monte Carlo tree search","Industries;Solid modeling;Three-dimensional displays;Monte Carlo methods;Shape;NP-hard problem;Reinforcement learning","bin packing;computational complexity;deep learning (artificial intelligence);logistics;Monte Carlo methods;production engineering computing;tree searching","packing position;deep reinforcement learning algorithm;classic NP-hard problem;3D bin packing problem;incomplete information;logistics industry;DRL algorithm;Monte Carlo tree search;MCTS;current box size information","","","","23","IEEE","17 Dec 2021","","","IEEE","IEEE Conferences"
"Globally Perceived Obstacle Avoidance for Robots Based on Virtual Twin and Deep Reinforcement Learning","R. Jiang; F. Ying; G. Zhang; Y. Xing; H. Liu","College of Information Science and Technology, Donghua University, China; College of Information Science and Technology, Donghua University, China; College of Information Science and Technology, Donghua University, China; College of Information Science and Technology, Donghua University, China; College of Information Science and Technology, Donghua University, China","2023 7th International Conference on Robotics, Control and Automation (ICRCA)","5 Apr 2023","2023","","","45","49","Investigations on obstacle-avoidable robotic trajectory generation is of great significance to the secure production of ordinary machinery factories, which allows robots to work in complex environments. However, conventional collision-free trajectory generation is highly dependent on manual analysis of the environment, making the trajectory generation extremely dedicated. To solve this problem, a more intelligent obstacle-avoidable trajectory generation method based on deep reinforcement learning that can globally perceive obstacle's information and automatically generate trajectories without inverse kinematics is proposed in this paper, where a virtual system corresponding to the physical robot platform is constructed for policy learning motivated by the concept of virtual twin, and an obstacle-avoidable reward with a global perception capability is proposed. Experimental results have verified the superior performance of the proposed method.","","979-8-3503-4578-0","10.1109/ICRCA57894.2023.10087870","Natural Science Foundation of Shanghai(grant numbers:21ZR1401100); Fundamental Research Funds for the Central Universities(grant numbers:2232022G-09); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10087870","deep reinforcement learning;robot;obstacle avoid-ance;trajectory generation","Deep learning;Training;Torque;Reinforcement learning;Production facilities;Trajectory;Safety","collision avoidance;deep learning (artificial intelligence);mobile robots;reinforcement learning;trajectory control","complex environments;conventional collision-free trajectory generation;deep reinforcement learning;global perception capability;globally perceived obstacle avoidance;intelligent obstacle-avoidable trajectory generation method;obstacle-avoidable reward;obstacle-avoidable robotic trajectory generation;ordinary machinery factories;physical robot platform;policy learning;secure production;virtual system;virtual twin","","","","14","IEEE","5 Apr 2023","","","IEEE","IEEE Conferences"
"Learning to Optimize: Reference Vector Reinforcement Learning Adaption to Constrained Many-Objective Optimization of Industrial Copper Burdening System","L. Ma; N. Li; Y. Guo; X. Wang; S. Yang; M. Huang; H. Zhang","College of Software, Northeastern University, Shenyang, China; College of Software, Northeastern University, Shenyang, China; School of Information Science and Engineering, China University of Mining and Technology (Beijing), Beijing, China; College of Computer Science, Northeastern University, Shenyang, China; School of Computer Science and Informatics, De Montfort University, Leicester, U.K.; College of Information Science and Engineering, State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; Chinese Academy of Sciences, Shenyang Institute of Automation, Shenyang, China","IEEE Transactions on Cybernetics","18 Nov 2022","2022","52","12","12698","12711","The performance of decomposition-based algorithms is sensitive to the Pareto front shapes since their reference vectors preset in advance are not always adaptable to various problem characteristics with no a priori knowledge. For this issue, this article proposes an adaptive reference vector reinforcement learning (RVRL) approach to decomposition-based algorithms for industrial copper burdening optimization. The proposed approach involves two main operations, that is: 1) a reinforcement learning (RL) operation and 2) a reference point sampling operation. Given the fact that the states of reference vectors interact with the landscape environment (quite often), the RL operation treats the reference vector adaption process as an RL task, where each reference vector learns from the environmental feedback and selects optimal actions for gradually fitting the problem characteristics. Accordingly, the reference point sampling operation uses estimation-of-distribution learning models to sample new reference points. Finally, the resultant algorithm is applied to handle the proposed industrial copper burdening problem. For this problem, an adaptive penalty function and a soft constraint-based relaxing approach are used to handle complex constraints. Experimental results on both benchmark problems and real-world instances verify the competitiveness and effectiveness of the proposed algorithm.","2168-2275","","10.1109/TCYB.2021.3086501","National Natural Science Foundation of China(grant numbers:61773103,61973305,61872073,71620107003,61673331,62032013); Liaoning Revitalization Talents Program(grant numbers:XLYC1902010,XLYC1802115); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9484680","Copper burdening optimization;many-objective optimization;reference vector reinforcement learning (RVRL)","Optimization;Adaptation models;Copper;Reinforcement learning","copper;evolutionary computation;learning (artificial intelligence);metallurgical industries;optimisation;production engineering computing;surface treatment;vectors","adaptive penalty function;adaptive reference vector reinforcement learning approach;decomposition-based algorithms;estimation-of-distribution learning models;industrial copper burdening optimization;industrial copper burdening problem;industrial copper burdening system;main operations;many-objective optimization;Pareto front shapes;problem characteristics;reference point sampling operation;reference points;reference vector adaption process;reference vector reinforcement learning adaption;reference vectors preset;reinforcement learning operation;resultant algorithm;RL operation;selects optimal actions;soft constraint-based relaxing approach","","67","","54","IEEE","14 Jul 2021","","","IEEE","IEEE Journals"
"Learning to Box: Reinforcement Learning using Heuristic Three-step Curriculum Learning","H. Rho; Y. Yu; K. Lee","School of Integrated Technology (SIT), Gwangju Institute of Science and Technology (GIST) Gwangju, Korea; School of Integrated Technology (SIT), Gwangju Institute of Science and Technology (GIST) Gwangju, Korea; School of Integrated Technology (SIT), Gwangju Institute of Science and Technology (GIST) Gwangju, Korea","2022 22nd International Conference on Control, Automation and Systems (ICCAS)","9 Jan 2023","2022","","","227","231","The reinforcement learning paradigm is a widely used approach to solving sequential problems. In this paper, we utilized reinforcement learning with curriculum learning to train the agent to play boxing without any human demonstration. We presented as a curriculum three steps for a single agent to learn boxing: standing, walking and punching. The agent who learned through the proposed curriculum successfully threw a jab into a punching bag, but the agent who trained to punch from the beginning did not. Not only did we show the effectiveness of curriculum learning in single-agent boxing, but we also showed that the agent could play boxing without human demonstrations.","2642-3901","978-89-93215-24-3","10.23919/ICCAS55662.2022.10003778","Korea Institute for Advancement of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003778","Reinforcement Learning;Transfer Learning;Curriculum Learning;Human Locomotion","Legged locomotion;Training;Automation;Punching;Humanoid robots;Reinforcement learning;Control systems","computer aided instruction;multi-agent systems;reinforcement learning","heuristic three-step curriculum learning;human demonstration;punching bag;reinforcement learning paradigm;sequential problems;single-agent boxing;standing punching;walking;walking punching","","","","20","","9 Jan 2023","","","IEEE","IEEE Conferences"
"Off-Policy Reinforcement Learning for Tracking in Continuous-Time Systems on Two Time Scales","W. Xue; J. Fan; V. G. Lopez; Y. Jiang; T. Chai; F. L. Lewis","State Key Laboratory of Synthetical Automation for Process Industries and International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries and International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; UTA Research Institute, The University of Texas at Arlington, Arlington, TX, USA; State Key Laboratory of Synthetical Automation for Process Industries and International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries and International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; UTA Research Institute, The University of Texas at Arlington, Arlington, TX, USA","IEEE Transactions on Neural Networks and Learning Systems","5 Oct 2021","2021","32","10","4334","4346","This article applies a singular perturbation theory to solve an optimal linear quadratic tracker problem for a continuous-time two-time-scale process. Previously, singular perturbation was applied for system regulation. It is shown that the two-time-scale tracking problem can be separated into a linear–quadratic tracker (LQT) problem for the slow system and a linear–quadratic regulator (LQR) problem for the fast system. We prove that the solutions to these two reduced-order control problems can approximate the LQT solution of the original control problem. The reduced-order slow LQT and fast LQR control problems are solved by off-policy integral reinforcement learning (IRL) using only measured data from the system. To test the effectiveness of the proposed method, we use an industrial thickening process as a simulation example and compare our method to a method with the known system model and a method without time-scale separation.","2162-2388","","10.1109/TNNLS.2020.3017461","NSFC(grant numbers:61991400,61991404,61991403,61533015); Higher Education Discipline Innovation Project(grant numbers:B08015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9189789","Data-driven optimization;off-policy integral reinforcement learning (IRL);singular perturbation;two-time-scale system","Perturbation methods;Optimal control;Learning (artificial intelligence);Indexes;Process control;Regulators","continuous time systems;control system synthesis;learning (artificial intelligence);linear quadratic control;nonlinear control systems;perturbation theory;reduced order systems;singularly perturbed systems","reduced-order control problems;LQT solution;original control problem;off-policy integral reinforcement learning;known system model;time-scale separation;policy reinforcement;continuous-time systems;time scales;singular perturbation theory;optimal linear quadratic tracker problem;continuous-time two-time-scale process;system regulation;two-time-scale tracking problem;linear-quadratic tracker problem;slow system;linear-quadratic regulator problem;fast system","","16","","43","IEEE","9 Sep 2020","","","IEEE","IEEE Journals"
"Quantitative Measurement Method of Tourism Contribution to Regional Economic Development based on Reinforcement Learning: from the Perspective of SVM","X. Xie","Guilin Tourism University, Guilin, China","2022 3rd International Conference on Electronics and Sustainable Communication Systems (ICESC)","19 Sep 2022","2022","","","1333","1336","Based on the reinforcement learning and SVM, this paper studies the quantitative measurement method of tourism’s contribution to regional development. It regards tourism as a ""detached"" economic activity in the current industrial sectors and closely related to it. Based on tourism economics, applied statistics, and investment output Analysis and other theories and methods give a quantitative measurement method of the contribution of tourism to regional economic development. Firstly, it quantitatively analyzes the spillover effect of the regional tourism economic growth from the overall perspective, and then specifically combines the characteristics of the tourism industry to explore the tourism economy.","","978-1-6654-7971-4","10.1109/ICESC54411.2022.9885481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9885481","Quantitative Measurement;Regional Economic;Reinforcement Learning;SVM","Economics;Support vector machines;Communication systems;Current measurement;Tourism industry;Reinforcement learning;Investment","economics;investment;support vector machines;travel industry","reinforcement learning;SVM;quantitative measurement method;regional development;detached economic activity;tourism economics;regional economic development;regional tourism economic growth;tourism industry;tourism economy;tourism contribution","","","","24","IEEE","19 Sep 2022","","","IEEE","IEEE Conferences"
"An End-to-end Hierarchical Reinforcement Learning Framework for Large-scale Dynamic Flexible Job-shop Scheduling Problem","K. Lei; P. Guo; Y. Wang; J. Xiong; W. Zhao","School of Mechanical Engineering, Southwest Jiaotong University, Chengdu, China; School of Mechanical Engineering, Southwest Jiaotong University, Chengdu, China; Department of Mathematics, Auburn University at Montgomery, Montgomery, AL, USA; School of Mechanical Engineering, Southwest Jiaotong University, Chengdu, China; School of Mechanical Engineering, Southwest Jiaotong University, Chengdu, China","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","1","8","The dynamic flexible job shop scheduling problem (DFJSP) is frequently encountered in the modern manufacturing industry. As the intelligent manufacturing paradigm evolves, it is urgent to design a dynamic scheduling framework for handling the uncertainty and complexity in the real-time production line control. DFJSP aims to dynamically address new job random arrival so that the desired objective could be optimized, such as the minimization of makespan. The intractability of this problem can be directly reflected by the following two points. 1) the arrival times of new jobs are unknown in advance, so the scheduling framework needs to schedule them in real-time optimization; 2) Two optimization tasks, including job operation selection and machine assignment, have to be handled, which means multiple actions must be controlled simultaneously. This paper proposes a novel end-to-end hierarchical reinforcement learning framework to cope with the large-scale DFJSP. For generality, a higher-level layer is designed to automatically divide the DFJSP into a series of sub-problems with different scales, i.e., static FJSPs, which aims to achieve global optimization. Moreover, two lower-level layers are constructed to efficiently solve the sub-problems generated from the higher-level layer. One layer's policy based on the graph neural network is trained to schedule a job operation, and another policy based on the multi-layer perceptron is trained to assign a machine to process the job operation. Especially a Markov decision process (MDP), including state, action, and reward function, is designed for each layer of the hierarchy. Numerical experiments, including offline training and online testing, are conducted on several large-scale instances with diverse production configurations. The results verify the effectiveness of the proposed framework compared with the existing dynamic scheduling methods such as well-known dispatching rules and the existing heuristics.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892005","National Key Research and Development Plan(grant numbers:2020YFB1712200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892005","hierarchical reinforcement learning;dynamic flexible job shop scheduling problem;graph neural network;Markov decision process;real-time optimization","Schedules;Job shop scheduling;Reinforcement learning;Production;Dynamic scheduling;Real-time systems;Dispatching","dynamic scheduling;flexible manufacturing systems;graph theory;job shop scheduling;learning (artificial intelligence);Markov processes;multilayer perceptrons;neural nets;scheduling","end-to-end hierarchical reinforcement learning framework;large-scale dynamic flexible job-shop scheduling problem;dynamic flexible job shop scheduling problem;modern manufacturing industry;intelligent manufacturing paradigm;dynamic scheduling framework;real-time production line control;job random arrival;arrival times;real-time optimization;job operation selection;large-scale DFJSP;higher-level layer;global optimization;lower-level layers;multilayer perceptron;existing dynamic scheduling methods","","1","","21","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach for Solving the Fragment Assembly Problem","M. -I. Bocicor; G. Czibula; I. -G. Czibula","Department of Computer Science, Babeş-Bolyai University, Cluj-Napoca, Romania; Department of Computer Science, Babeş-Bolyai University, Cluj-Napoca, Romania; Department of Computer Science, Babeş-Bolyai University, Cluj-Napoca, Romania","2011 13th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing","15 Mar 2012","2011","","","191","198","The DNA fragment assembly is a very complex optimization problem important within many fields including bioinformatics and computational biology. The problem is NP-hard, that is why many computational techniques including computational intelligence algorithms were designed for finding good solutions for this problem. Since DNA fragment assembly is a crucial part of any sequencing project, researchers are still focusing on developing better assemblers. In this paper we aim at proposing a new reinforcement learning based model for solving the fragment assembly problem. We are particularly focusing on the DNA fragment assembly problem. Our model is based on a Q-learning agent-based approach. The experimental evaluation confirms a good performance of the proposed model and indicates the potential of our proposal.","","978-1-4673-0207-4","10.1109/SYNASC.2011.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6169520","bioinformatics;reinforcement learning;DNA fragment assembly","DNA;Assembly;Learning;Training;Biological cells;Bioinformatics;Layout","bioinformatics;computational complexity;DNA;learning (artificial intelligence)","reinforcement learning;complex optimization problem;bioinformatics;computational biology;NP-hard problem;computational intelligence;sequencing project;DNA fragment assembly problem;Q-learning agent-based approach","","14","","25","IEEE","15 Mar 2012","","","IEEE","IEEE Conferences"
"Research on robot motion control based on local weighted kNN-TD reinforcement learning","F. Han; L. Jin; Y. Yang; Z. Cao; T. Zhang","College of Information Engineering, Yangzhou University, Yangzhou, China; College of Information Engineering, Yangzhou University, Yangzhou, China; College of Information Engineering, Yangzhou University, Yangzhou, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; College of Information Engineering, Yangzhou University, Yangzhou, China","Proceedings of the 10th World Congress on Intelligent Control and Automation","24 Nov 2012","2012","","","3648","3651","Learning is an important capability for an individual robot, which provides an effective way for understanding, planning, and decision-making in a complex environment. For robot motion control, a local weighted k-nearest neighbors states selection method based on environment information and task information is presented. Based on this method, TD reinforcement learning algorithm is combined to reduce the misclassified probability of kNN-TD method, which is finally verified by the simulations.","","978-1-4673-1398-8","10.1109/WCICA.2012.6359080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6359080","k-nearest neighbors;reinforcement learning;motion control","Learning;Automation;Robot motion;Educational institutions;Markov processes;Computer science","decision making;learning (artificial intelligence);mobile robots;motion control;probability","robot motion control;local weighted kNN-TD reinforcement learning;decision making;local weighted k-nearest neighbor states selection method;task information;environment information;misclassified probability reduction","","2","","7","IEEE","24 Nov 2012","","","IEEE","IEEE Conferences"
"Reinforcement learning for procurement agents of factory of the future","B. Simsek; S. Albayrak; A. Korth","Sekretariat GORI-1, Berlin, Germany; Sekretariat GORI-1, Berlin, Germany; Sekretariat GORI-1, Berlin, Germany","Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753)","3 Sep 2004","2004","2","","1331","1337 Vol.2","Factory of the future is emerging with the existence of new modeling and application tools that can both simulate and manage the whole production process in an autonomous, intelligent and interactive manner. Holonic modeling and its software correspondence agent oriented technology provides us with these tools. Especially the use of learning algorithms trying to optimize the behaviors of software agents within a dynamic environment is the key factor in reaching the required properties. In this paper, we use the well known Q learning algorithm of reinforcement learning (RL) in evaluating production orders within a supply chain management (SCM) framework and making decisions with respect to these evaluations. We introduce our SCM model and show that RL performs better than traditional tools for dynamic problem solving in daily business. We also show cases where RL fails to perform efficiently.","","0-7803-8515-2","10.1109/CEC.2004.1331051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1331051","","Learning;Procurement;Production facilities;Application software;Intelligent agent;Software tools;Optimized production technology;Software algorithms;Software agents;Supply chain management","learning (artificial intelligence);software agents;supply chain management;decision making;factory automation","reinforcement learning;procurement agents;holonic modeling;learning algorithms;software agents;Q learning algorithm;supply chain management;agent oriented technology","","2","","24","IEEE","3 Sep 2004","","","IEEE","IEEE Conferences"
"Graph-QMIX: Addressing the Partial Observation Issues via Graph Neural Network in Multi-Agent Reinforcement Learning","D. Pan; D. An; R. Zhang","School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China; SKLMSE lab, MOE Key Lab for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, China; Nanjing Agricultural University, Nanjing, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","1275","1280","In recent years, with the development of multiagent reinforcement learning, more and more complex tasks have been solved. However, today’s multi-agent reinforcement learning faces two challenges: 1) the global state is always used to train the neural network, which is hard to obtain in the real-world; 2) compared to the global state, concatenating local observations decreases the performance of multi-agent reinforcement learning algorithms. These challenges make it difficult to apply multi-agent reinforcement learning algorithms in real-world scenarios. To solve these challenges, we proposed the Graph-QMIX algorithm, where all agents are seen as a graph, and the graph convolutional neural network is used to integrate the local observations of the agents. We evaluate our method in map 2s vs lsc and map 10m vs 11m of SMAC environment. Empirically simulation results show that our method reaches a strong performance as much as QMIX using the global state, and is much stronger than QMIX using the concatenating local observations.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023781","","Automation;Simulation;Reinforcement learning;Graph neural networks;Convolutional neural networks;Task analysis;Faces","convolutional neural nets;graph neural networks;learning (artificial intelligence);multi-agent systems","graph convolutional neural network;Graph-QMIX algorithm;local observation concatenation;multiagent reinforcement learning algorithm;neural network training;partial observation;SMAC environment","","","","19","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Game Confrontation of 5v5 Multi-Agent Based on MAPPO Reinforcement Learning Algorithm","Y. Liu; Q. Wang","Suzhou United Graduate School, Southeast University, Suzhou, China; Automation college, Southeast University, Nanjing, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","1395","1398","In the 5v5 air intelligence game, this is an environment in which the data of the agent platform can be observed. We will first consider using rules to build agents, but rules can not deal with all kinds of combat situations, which makes us think of the method of Multi-Agent Reinforcement Learning. This paper combines rules and reinforcement learning methods to construct an agent to try to solve the problem of air intelligent game.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023568","Multi agent system;reinforcement learning;rule agent","Automation;Reinforcement learning;Games;Multi-agent systems","computer games;knowledge based systems;military computing;multi-agent systems;reinforcement learning","5v5 air intelligence game;5v5 multiagent reinforcement learning;air intelligent game;game confrontation;MAPPO reinforcement learning algorithm;military field;rule agent","","","","4","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Applying Reinforcement Learning Method for Real-time Energy Management","A. B. Dayani; H. Fazlollahtabar; R. Ahmadiahangar; A. Rosin; M. S. Naderi; M. Bagheri","Department of Engineering, Mazandaran University of Science and Technology, Babol, Iran; Department of Industrial Engineering, School of Engineering, Damghan University, Damghan, Iran; Department of Electrical Power Engineering and Mechatronics, Tallinn University of Technology, Tallinn, Estonia; Department of Electrical Power Engineering and Mechatronics, Tallinn University of Technology, Tallinn, Estonia; Department of Electrical and Computer Engineering, North Tehran Branch Islamic Azad University, Tehran, Iran; Department of Electrical and Computer Engineering, Nazarbayev University, Astana, Kazakestan","2019 IEEE International Conference on Environment and Electrical Engineering and 2019 IEEE Industrial and Commercial Power Systems Europe (EEEIC / I&CPS Europe)","1 Aug 2019","2019","","","1","5","Today energy management and optimization is a key factor to control the whole production cycle, distribution and energy consumption. Electrical energy consumption optimization involves proper modeling and prediction. Preservation of energy-efficient resources and proper consumption management is one of the most important challenges in all countries of the world. In this study, we present a decision support system for managing energy by reinforcement learning. First, a set of different energy uncertain consumption data and adopted decisions were considered in the form of fuzzy. Then, the prediction of consumption was done by the Q-learning algorithm, which is a solution to the Markov decision problem. Then the rules are presented to describe what the system implies. The proposed method is capable of working in real-time approach and handle the consumption fluctuations in learning and predicting process.","","978-1-7281-0653-3","10.1109/EEEIC.2019.8783766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8783766","Energy management;decision support system;reinforcement learning;Demand;Q-learning algorithm","Energy management;Optimization;Energy consumption;Reinforcement learning;Production;Prediction algorithms;Industries","decision support systems;energy conservation;energy consumption;energy management systems;learning (artificial intelligence);Markov processes;optimisation;power engineering computing","production cycle;electrical energy consumption optimization;energy-efficient resources;decision support system;Q-learning algorithm;Markov decision problem;consumption fluctuations;reinforcement learning method;real-time energy management;energy uncertain consumption data","","9","","22","IEEE","1 Aug 2019","","","IEEE","IEEE Conferences"
"Dynamic Pricing using Reinforcement Learning in Hospitality Industry","I. Singh",Thoughtworks,"2022 IEEE Bombay Section Signature Conference (IBSSC)","14 Feb 2023","2022","","","1","6","Hotel room pricing is a very common use case in the hospitality industry. Such use cases take dynamic pricing strategies for setting optimum prices wherein prices are dynamically adjusted based on user engagement. However, it is challenging to design an approach that makes pricing dynamic with respect to complex market change. In this paper, we suggest a reinforcement learning based solution for this problem. The approach employs a Deep Q-Network (DQN) agent trained to recommend/suggest optimum pricing strategies which maximizes the total profits for a day. In addition, the pricing strategy is optimized in such a way that empty rooms remain minimal. A real-life hotel-bookings data set is being used for testing this approach. The data is aggregated and preprocessed before being used for the task. The pricing strategy is influenced by the hotel-demand, type of rooms, number of nights and other variables. The hotel-demand is derived from a Random-forest model trained on the processed data to simulate original demand distribution of processed data. Using the DQN based dynamic pricing strategy, a potential 15–20 percentage higher reward(profits) were obtained compared to fixed pricing, and rule-based pricing strategy. At the same time the empty rooms left were significantly lower for the DQN based approach.","","978-1-6654-9291-1","10.1109/IBSSC56953.2022.10037523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10037523","Dynamic Pricing;Deep Reinforcement Learning;Hospitality Industry;Deep Q-network;Revenue management","Industries;Training;Deep learning;IEEE Sections;Pricing;Reinforcement learning;Predictive models","hotel industry;learning (artificial intelligence);pricing;profitability;random forests;reinforcement learning","DQN based approach;dynamic pricing strategy;fixed pricing;hospitality industry;hotel room pricing;hotel-demand;optimum prices;optimum pricing strategies;processed data;real-life hotel-bookings data set;reinforcement learning","","","","25","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Headland Turn Automation Concept for Tractor-Trailer System with Deep Reinforcement Learning","E. Olcay; X. Rui; R. Wang","Fraunhofer Institute for Transportation and Infrastructure Systems IVI - Application Center »Connected Mobility and Infrastructure«, Inzolstadt, Germany; Technical University of Munich (TUM), Germany; Technical University of Munich (TUM), Germany","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","7","Navigation of agricultural vehicles along predefined paths using control and path planning strategies has been extensively studied, but some cases require challenging maneuvers that cannot be manually predefined. An example of such cases in agriculture is the headland turning process, which heavily depends on the field geometry, tractor, and implement. Therefore, automating this process can improve time efficiency, optimize land use, and reduce the cognitive burden on semi-autonomous tractor drivers. Reinforcement Learning (RL) is a powerful framework for learning complex policies in high-dimensional environments, making it widely used in autonomous driving. In this paper, we investigate the effectiveness of recent model-free RL algorithms, specifically policy optimization and Q-learning types, for headland turning of a tractor-trailer combination. The findings show that reinforcement learning-based approaches are effective for learning a headland turn maneuver and provide insights into which state-of-the-art method has a superior performance in learning a headland turn maneuver.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260531","Agricultural automation;neural network;deep reinforcement learning","Geometry;Deep learning;Automation;Q-learning;Navigation;Agricultural machinery;Turning","agricultural machinery;agriculture;deep learning (artificial intelligence);mobile robots;navigation;optimisation;path planning;reinforcement learning","agricultural vehicles;agriculture;autonomous driving;challenging maneuvers;cognitive burden;complex policies;deep reinforcement learning;field geometry;headland turn automation concept;headland turn maneuver;headland turning process;high-dimensional environments;land use;model-free RL algorithms;path planning strategies;policy optimization;Q-learning types;reinforcement learning-based approaches;semiautonomous tractor drivers;time efficiency;tractor-trailer combination;tractor-trailer system","","","","22","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Reinforcement learning based throttle and brake control for autonomous vehicle following","Q. Zhu; Z. Huang; Z. Sun; D. Liu; B. Dai","College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China; College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China; College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China; College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China; College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China","2017 Chinese Automation Congress (CAC)","1 Jan 2018","2017","","","6657","6662","In this paper, we focus on the basic form of autonomous follow driving problem with one leader and one follower. A reinforcement learning based throttle and brake control approach is developed for the follower vehicle. Near optimal control law is directly learned by “trial and error” with the neural dynamic programming algorithm. According to the timely updated following state, the learned control policy can deliver appropriate throttle and brake control commands for the follower. Simulation tests to illustrate the effectiveness of the presented method are carried out with the highly recognized vehicle dynamic simulator CarSim.","","978-1-5386-3524-7","10.1109/CAC.2017.8243976","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8243976","autonomous driving;autonomous following;throttle and brake control;neural dynamic programming;reinforcement learning","Brakes;Learning (artificial intelligence);Autonomous vehicles;Dynamic programming;Aerospace electronics;Markov processes;Heuristic algorithms","braking;control engineering computing;dynamic programming;learning (artificial intelligence);mobile robots;neurocontrollers;optimal control;position control;road vehicles;vehicle dynamics;velocity control","follower vehicle;optimal control law;neural dynamic programming algorithm;learned control policy;reinforcement learning;autonomous vehicle following;trial and error;throttle and brake control approach;leader vehicle;throttle and brake control commands;vehicle dynamic simulator","","5","","22","IEEE","1 Jan 2018","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Method for Intermediate Point Enthalpy Control in Super-critical Power Unit","Y. Huang; R. Yao; X. Liu; S. Lin; W. Zhang","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2018 Chinese Automation Congress (CAC)","24 Jan 2019","2018","","","651","654","The intermediate point enthalpy is a significant indicator for the steam temperature in thermal power unit, which has a great impact on the safety and economy in power unit. The intermediate point enthalpy control becomes more difficult in supercritical power unit than in subcritical unit because of controlled plant's characteristics of non-linearity, large inertia and coupling between different input. In this paper, a reinforcement learning method using proximal policy optimization algorithm is proposed to operate the feed water following control scheme for coordinated control system in supercritical unit. Experiment indicates that the proposed method can achieve satisfactory control performance compared with feed-forward decoupling control.","","978-1-7281-1312-8","10.1109/CAC.2018.8623467","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8623467","supercritical power units;coordinated control system;intermediate point enthalpy control;reinforcement learning;Markov decision process;proximal policy optimization","Enthalpy;Feeds;Fuels;Process control;Temperature control;Markov processes","control engineering computing;enthalpy;feedforward;learning (artificial intelligence);optimisation;power engineering computing;power generation control;steam power stations;temperature control","reinforcement learning method;intermediate point enthalpy control;super-critical power unit;thermal power unit;supercritical power unit;coordinated control system;steam temperature;proximal policy optimization algorithm;feed water following control scheme","","","","14","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Research on Path Planning of Mobile Robot Based on Reinforcement Learning","M. Ji; J. Li; S. Li; Z. Liu; J. Song; L. Liu","Fujian Industrial Automation Technology Research and Development Center, Fujian Institute of Engineering, Fuzhou, China; Fujian Industrial Automation Technology Research and Development Center, Fujian Institute of Engineering, Fuzhou, China; Department of Computer Science and Mathematics, Fujian Institute of Engineering, Fuzhou, China; Fujian Industrial Automation Technology Research and Development Center, Fujian Institute of Engineering, Fuzhou, China; Fujian Industrial Automation Technology Research and Development Center, Fujian Institute of Engineering, Fuzhou, China; Fujian Industrial Automation Technology Research and Development Center, Fujian Institute of Engineering, Fuzhou, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","748","751","With the continuous change of artificial intelligence technology, the study of path planning for mobile robots is no longer limited to traditional path algorithms. Reinforcement learning, as an artificial intelligence algorithm with excellent performance in the field of path planning, has also gradually become the object of research on path planning. In order to find a fast path from the starting point to the end point of a mobile robot in a complex environment, reinforcement learning is used to find a valid path by sensing the environment and continuously receiving rewarding feedback through a method of trial and error learning like humans. Therefore, this paper selects two algorithms to verify their effectiveness on the basis of building a two-dimensional grid map. The final experimental results show that both reinforcement learning algorithms can eventually avoid obstacles and plan a valid path in a complex environment through continuous learning iterations. Q-Learning also reduces the path distance by 10% and the number of convergence iterations by 82% compared to Sarsa.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055343","Reinforcement learning;Mobile robot;Path planning;Navigation experiments;Award feedback","Training;Q-learning;Simulation;Buildings;Robot sensing systems;Path planning;Sensors","artificial intelligence;collision avoidance;learning (artificial intelligence);mobile robots;path planning;reinforcement learning","artificial intelligence algorithm;artificial intelligence technology;complex environment;continuous learning iterations;error learning;fast path;mobile robot;path distance;path planning;reinforcement learning algorithms;traditional path algorithms;valid path","","","","9","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Path Planning for Mobile Robots Using Time-Sensitive Reward","Z. Ruqing; L. Xin; L. Shubin; Z. Jihuai; L. Fusheng","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China","2022 19th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)","19 Jan 2023","2022","","","1","4","In the mobile robots’ field, the global path planning task in known map scenarios is an urgent problem to be solved. Deep Reinforcement Learning (DRL), an efficient decision-making method, has been widely used to solve path-planning problems. Nonetheless, as the map size increases, the existing DRL algorithms are prone to the problem of sparse rewards. The above drawback makes the mobile robot converge slowly on the map. Even in extreme cases such as trap maps, the robot obtains the optimal convergent solution differently. For this purpose, this paper encodes the various node information in the map separately as a state description of the environment. To efficiently perform path-planning tasks for mobile robots in various complex scenarios, a time-sensitive reward function based on DRL is presented. The simulation experiments on a variety of complex environmental maps are conducted. The experimental results demonstrate the effectiveness of the proposed method. Our method ensures that the DRL algorithm is able to converges to a feasible solution quickly.","2576-8964","978-1-6654-9389-5","10.1109/ICCWAMTIP56608.2022.10016608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10016608","Path planning;Collision avoidance;Deep learning in robotics and automation;Deep reinforcement learning (DRL);Time-sensitive reward","Deep learning;Decision making;Reinforcement learning;Information processing;Media;Path planning;Mobile robots","decision making;learning (artificial intelligence);mobile robots;optimisation;path planning","complex environmental maps;deep Reinforcement;Deep Reinforcement;DRL algorithm;efficient decision-making method;existing DRL algorithms;global path planning task;known map scenarios;map size increases;mobile robot converge;mobile robots;optimal convergent solution;path-planning problems;path-planning tasks;sparse rewards;time-sensitive reward function;trap maps","","","","8","IEEE","19 Jan 2023","","","IEEE","IEEE Conferences"
"Dynamic Process Planning using Digital Twins and Reinforcement Learning","Z. Müller-Zhang; P. O. Antonino; T. Kuhn","Embedded Software Engineering, Fraunhofer IESE, Kaiserslautern, Germany; Embedded Software Engineering, Fraunhofer IESE, Kaiserslautern, Germany; Division Embedded Systems, Fraunhofer IESE, Kaiserslautern, Germany","2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)","5 Oct 2020","2020","1","","1757","1764","In order to enable changeable production of Industry 4.0 applications, a production system should respond to unpredictable changes quickly and adequately. This requires process planning to be performed based on the real time operating conditions and dynamic changes to be handled with cognitive skills. To meet this demand, we present a process planning approach using digital twins and reinforcement learning to derive near-optimal process plans. The digital twins enable access to real-time information about the production system. They also constitute the environment for training the agent of the reinforcement learning method. The environment works as a virtual plant, containing the attributes of the product and resources, and uses simulation models of the resources to calculate the reward for an action in terms of reinforcement learning. Reinforcement learning enables our approach to derive process plans via trial and error. Besides the virtual plant, our approach has a planner, which plays the role of the agent to derive near-optimal plans by trying different actions in the virtual plant, and observes the rewards. We apply the Q-learning algorithm to derive near optimal process plans. The evaluation results show that our approach is able to derive near-optimal process plans for different problem sizes. The evaluation also demonstrated the planner’s ability to identify by itself which action to take in which situation. Consequently, no modeling of the preconditions and effects of the actions is necessary.","1946-0759","978-1-7281-8956-7","10.1109/ETFA46521.2020.9211946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211946","Process Planning;Reinforcement Learning;Digital Twin;Discrete-Event Simulation","Training;Production systems;Q-learning;Digital twin;Conferences;Process planning;Real-time systems","learning (artificial intelligence);manufacturing systems;process planning;production engineering computing","reinforcement learning method;dynamic changes;real-time operating conditions;production system;digital twins;dynamic process planning;near-optimal process plans;Q-learning algorithm;virtual plant;near-optimal plans","","3","","24","IEEE","5 Oct 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning with Reward Shaping and Hybrid Exploration in Sparse Reward Scenes","Y. Yang; W. Cao; L. Guo; C. Gan; M. Wu","School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China","2023 IEEE 6th International Conference on Industrial Cyber-Physical Systems (ICPS)","24 May 2023","2023","","","1","6","High precision modeling in industrial systems is difficult and costly. Model-free intelligent control methods, represented by reinforcement learning, have been applied in industrial systems broadly. The hard evaluated of production states and the low value density of processing data causes sparse rewards, which lead to an insufficient performance of reinforcement learning. To overcome the difficulty of reinforcement learning in sparse reward scenes, a reinforcement learning method with reward shaping and hybrid exploration is proposed. By perfecting the rewards distribution in the state space of environment, the reward shaping can make the state-value estimation of reinforcement learning more accurate. By improving the rewards distribution in time dimension, the hybrid exploration can make the iteration of reinforcement learning more efficient and more stable. Finally, the effectiveness of the proposed method is verified by simulations.","2769-3899","979-8-3503-1125-9","10.1109/ICPS58381.2023.10128012","National Natural Science Foundation of China(grant numbers:61773354); Natural Science Foundation of Hubei Province(grant numbers:2020CFA031); 111 Project(grant numbers:B17040); China University of Geosciences (Wuhan)(grant numbers:CUGDCJJ202210); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10128012","reinforcement learning;sparse reward;reward shaping;hybrid exploration","Estimation;Reinforcement learning;Production;Cyber-physical systems;Stability analysis;Intelligent control;Convergence","intelligent control;production engineering computing;reinforcement learning","high precision modeling;hybrid exploration;industrial systems;model-free intelligent control methods;reinforcement learning method;reward shaping;rewards distribution;sparse reward scenes;state-value estimation","","","","19","IEEE","24 May 2023","","","IEEE","IEEE Conferences"
"Commodity and Forex trade automation using Deep Reinforcement Learning","B. A. Usha; T. N. Manjunath; T. Mudunuri","Department of ISE, BMSIT&M, Bengaluru, INDIA; Department of ISE, BMSIT&M, Bengaluru, INDIA; Department of ISE, BMSIT&M, Bengaluru, INDIA","2019 1st International Conference on Advanced Technologies in Intelligent Control, Environment, Computing & Communication Engineering (ICATIECE)","13 Apr 2020","2019","","","27","31","Machine learning is an application of artificial intelligence based on the theory that machines can learn from data, discern patterns and make decisions with negligible human intervention. With today's world being inundated by data, machine learning is very relevant due to the amount of learning potential. Machine learning caters to a variety of applications including image recognition, speech recognition, weather prediction, portfolio optimization and so on. The Forex Exchange is a market that allows traders and investors to buy, sell and exchange currencies of various nations. It is regarded as the largest financial market with over 5 trillion American dollars in daily trades, which is larger than the equity and futures markets combined. The Commodity market is a market that allows buying, selling and exchanging of raw materials or primary products. Using the concept of machine learning, this project aims to develop and introduce an agent to automate the trade of a given commodity or currency in a simulated market with the objectives of maximizing returns and minimizing losses for the trader. The model learns from trends in historical market data and is capable of buying, selling or holding a trade at a given instance. The model is validated by running the agent on unseen market data of a later period and the returns generated are analyzed.","","978-1-7281-0418-8","10.1109/ICATIECE45860.2019.9063807","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9063807","machine learning;deep learning;reinforcement learning;commodity trading;forex trading","Machine learning;Investment;Currencies;Oils;Learning (artificial intelligence);Gold;Supervised learning","foreign exchange trading;investment;learning (artificial intelligence);neural nets","artificial intelligence;commodity trade automation;forex trade automation;forex exchange;commodity market;futures markets;financial market;machine learning;deep reinforcement learning;historical market data;simulated market","","4","","7","IEEE","13 Apr 2020","","","IEEE","IEEE Conferences"
"Deep sparse representation via deep dictionary learning for reinforcement learning","J. Tang; Z. Li; S. Xie; S. Ding; S. Zheng; X. Chen","School of Automation, Guangdong University of Technology, Guangzhou, P. R. China; School of Automation, Guangdong University of Technology, Guangzhou, P. R. China; School of Automation, Guangdong University of Technology, Guangzhou, P. R. China; School of Artificial Intelligence, Guilin University of Electronic Technology, Guilin, P. R. China; School of Automation, Guangdong University of Technology, Guangzhou, P. R. China; School of Automation, Guangdong University of Technology, Guangzhou, P. R. China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","2398","2403","Reinforcement Learning (RL) currently has been widely applied for various fields of decision-making, which relies on the function approximation for tackling complicated tasks. However, most RL models including Deep Reinforcement Learning (DRL) suffer from the catastrophic interference, resulting in poor control performance. Recently, the sparse representation has been proven effective for avoiding the interference and is easy to train, but shallow sparse representations have limited representation capability. In this paper, we proposed to employ the deep dictionary learning (DDL) model for reinforcement learning as a joint optimization problem over the deep sparse representation and the function approximation, which considers a multi-layer dictionary for obtaining a deeper latent sparse representation with better data representation capability. Firstly, to address the difficulties in optimization of DDL, we employ the decomposition method that we can decompose the multi-layer problem into a set of single-layer subproblems. Secondly, to solve the joint optimization problem efficiently, we apply the alternating update scheme for updating the sparse representation, multi-layer dictionary, and weight for value prediction alternately. Thirdly, we can directly obtain the closed-form solution of the sparse representation by the Proximal Operator (PO), multi-layer dictionary and weight are trained by the gradient descent method. Finally, the agent utilizes the pre-trained multi-layer dictionary and weight for RL control, and the weight is updated dynamically at each step. The experimental results on two benchmark environments demonstrate that the performance of the RL control training based on the DDL model can be significantly improved.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902583","National Natural Science Foundation of China(grant numbers:62076077,61903090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902583","Reinforcement Learning;Deep sparse representation;Deep dictionary learning;Proximal operator","Training;Dictionaries;Closed-form solutions;Decision making;Reinforcement learning;Interference;Benchmark testing","function approximation;gradient methods;image representation;learning (artificial intelligence)","data representation capability;deep dictionary learning model;Deep Reinforcement;deep sparse representation;deeper latent sparse representation;function approximation;joint optimization problem;multilayer problem;pre-trained multilayer dictionary;reinforcement learning;RL control training;RL models;shallow sparse representations","","1","","23","","11 Oct 2022","","","IEEE","IEEE Conferences"
"A reinforcement learning method for multi-AGV scheduling in manufacturing","T. Xue; P. Zeng; H. Yu","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","2018 IEEE International Conference on Industrial Technology (ICIT)","30 Apr 2018","2018","","","1557","1561","This paper addresses a multi-AGV flow-shop scheduling problem with a reinforcement learning method. Each AGV equipped with a robotic manipulator, operates on the fixed tracks, transporting semi-finished products between successive machines. The objectives dealt with here is to obtain a AGV schedule that minimize the average job delay and total makespan. After formulating such schedule problem as a Markov problem by defining state features, actions space and reward function, a new scheduling method is proposed, based on reinforcement learning. In this new method AGVs share full information on each machine's instant state and job being executed, making decisions thorough understanding of the entire flow shop. Simulation results demonstrate that this new method learns optimal or near-optimal solution from the past experience and provides better performance than multi-agent scheduling method in a dynamic environment.","","978-1-5090-5949-2","10.1109/ICIT.2018.8352413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8352413","multi-AGV;flow-shop;reinforcement learning;Markov problem;optimal solution","Job shop scheduling;Learning (artificial intelligence);Task analysis;Manufacturing;Robots;Schedules","automatic guided vehicles;decision making;flow shop scheduling;job shop scheduling;learning (artificial intelligence);Markov processes;minimisation;multi-agent systems","reinforcement learning method;multiAGV flow-shop scheduling problem;robotic manipulator;semifinished products;AGV schedule;Markov problem;actions space;reward function;multiagent scheduling method;job delay minimization;makespan minimization;decisions making","","27","","13","IEEE","30 Apr 2018","","","IEEE","IEEE Conferences"
"Optimizing reserve size in genetic algorithms with reserve selection using reinforcement learning","Yang Chen; Jinglu Hu; Kotaro Hirasawa; Songnian Yu","Graduate School of Information, Production and Systems, Waseda University, Kitakyushu, Japan; Graduate School of Information, Production and Systems, Waseda University, Kitakyushu, Japan; Graduate School of Information, Production and Systems, Waseda University, Kitakyushu, Japan; School of Computer Engineering and Science, Shanghai University, Shanghai, China","SICE Annual Conference 2007","7 Jan 2008","2007","","","1341","1347","Recently, an improved genetic algorithm with a reserve selection mechanism (GARS) has been proposed to prevent premature convergence, where a parameter called reserve size plays an important role in optimization performance. In this paper, we propose an approach to the learning of an optimal reserve size in GARS based on the technique of reinforcement learning, where the learning model and algorithm are presented respectively. The experimental results demonstrate the effectiveness of learning algorithm in discovering the optimal reserve size accurately and efficiently.","","978-4-907764-27-2","10.1109/SICE.2007.4421191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4421191","Genetic algorithms;global optimization;population diversity;premature convergence;reinforcement learning;reserve selection","Genetic algorithms;Learning;Convergence;Production systems;Genetic engineering;Large-scale systems;Testing;Noise reduction;Computational efficiency;Feedback","genetic algorithms;learning (artificial intelligence)","genetic algorithms;reserve selection;reinforcement learning;premature convergence","","2","","9","","7 Jan 2008","","","IEEE","IEEE Conferences"
"Research on improvement of model-free average reward reinforcement learning and its simulation experiment","Wei Chen; Zhenkun Zhai; Xiong Li; Jing Guo; Jie Wang","Faculty of Automation, Guangdong University of Technology, Guangzhou, China; Faculty of Automation, Guangdong University of Technology, Guangzhou, China; Faculty of Automation, Guangdong University of Technology, Guangzhou, China; Faculty of Automation, Guangdong University of Technology, Guangzhou, China; Faculty of Automation, Guangdong University of Technology, Guangzhou, China","2009 Chinese Control and Decision Conference","7 Aug 2009","2009","","","4933","4936","Traditional reinforcement learning always emphasizes the independent learning of a single agent. In Multi-Agent System (MAS), considering the relationship between independent learning and group learning, this paper presents a hybrid algorithm based on average reward reinforcement learning. In learning process of the modified algorithm, it still pays attention to the independent learning. In order to select an action which can reflect the multi-agent environmental information, we add the observed information and the prediction of other agent's actions when the learning agent chooses his action according to the current environmental state. The advantage of this design is that not only the agent will learn the optimal policy through autonomous study, but also as one member of MAS, the learning process can be integrated into the whole multi-agent environment. Robocup simulation league (2D) is a typical multi-agent system. By applying the new method to the training of the player, we prove the feasibility and validity of this algorithm.","1948-9447","978-1-4244-2722-2","10.1109/CCDC.2009.5194915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5194915","Multi-agent system;Reinforcement learning;R-learning;Robocup","Learning;Multiagent systems;Automation;Artificial intelligence;Autonomous agents;State-space methods;Stochastic systems;Robots","control engineering computing;learning (artificial intelligence);multi-agent systems","reinforcement learning;multi-agent system;hybrid algorithm;robocup simulation league","","","","8","IEEE","7 Aug 2009","","","IEEE","IEEE Conferences"
"Mapless Navigation Based on VDAS-PPO Deep Reinforcement Learning","J. Wu; W. Chen; J. Ji; X. Chen; L. Su; H. Dai","School Of Electrical Engineering And Automation, Xiamen University Of Technology, Xiamen, China; School Of Electrical Engineering And Automation, Xiamen University Of Technology, Xiamen, China; School Of Electrical Engineering And Automation, Xiamen University Of Technology, Nanchang, China; School Of Electrical Engineering And Automation, Xiamen University Of Technology, Xiamen, China; School Of Electrical Engineering And Automation, Xiamen University Of Technology, Xiamen, China; Quanzhou Institute of Equipment Manufacturing Haixi Institutes, Chinese Academy of Sciences, Quanzhou, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","1945","1950","Low sample utilization is a problem for the continuous action space’s Proximal Policy Optimization (PPO) algorithm. This paper proposes a Proximal Policy Optimization (VDAS-PPO) based on Velocity-Directed Action Selection. Firstly, the multi-information fusion reinforcement learning network is designed for mapless navigation to well utilize sample experience and information representation in the current state by fusing target location information, velocity value information, and lidar observation value information. Moreover, during the decision-making process, a novel reward function is proposed to optimize the action decision by designing a speed variation factor. The sample utilization rate is significantly increased by the VDAS-PPO algorithm, which also guarantees that the agent can learn quickly and effectively when training the network model. Three selected scenarios are built up on the simulation platform Gazebo to test the effectiveness of the proposed VDAS-PPO algorithm. The training episodes of the algorithm in this paper are decreased by 5–6 times compared to the PPO and Deep Q Network (DQN) algorithms, and the SPL value is also enhanced.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055939","Reinforcement learning;Mapless navigation;VDAS-PPO;Reward function","Training;Deep learning;Laser radar;Automation;Navigation;Decision making;Reinforcement learning","collision avoidance;computer simulation;control engineering computing;data fusion;decision making;deep learning (artificial intelligence);mobile robots;navigation;optimisation;reinforcement learning","action decision;continuous action space;decision-making process;deep reinforcement learning;Gazebo simulation platform;information representation;lidar observation value information;mapless navigation;mobile robot navigation;multiinformation fusion reinforcement learning network;obstacle avoidance;proximal policy optimization;reward function;sample utilization rate;speed variation factor;target location information;VDAS-PPO;velocity value information;velocity-directed action selection","","","","20","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Reinforcement-Learning-Based Competitive Opinion Maximization Approach in Signed Social Networks","Q. He; X. Wang; Y. Zhao; B. Yi; X. Lu; M. Yang; M. Huang","College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, College of Computer Science and Engineering, Northeastern University, Shenyang, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China; College of Software Engineering, Northeastern University, Shenyang, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, College of Information Science and Engineering, Northeastern University, Shenyang, China","IEEE Transactions on Computational Social Systems","30 Sep 2022","2022","9","5","1505","1514","Competitive opinion maximization (COM) in signed social networks targets at selecting a subset of influential individuals (i.e., seed nodes), spreading the desired opinions of the product to their neighbors against its opponents, and eventually achieving the maximum opinion propagation. Current studies mainly focus on competitive influence maximization and opinion maximization. However, COM in signed social networks has not been studied in depth. In this article, we study the COM in signed social networks and propose a novel reinforcement-learning-based opinion maximization framework (RLOM) to solve the COM problem. The proposed RLOM is composed of two phases: the activated dynamic opinion model and the reinforcement-learning-based seeding process. We theoretically prove the COM problem to be NP-hard. To model the opinion propagation process, we propose the activated dynamic opinion model based on a stateless Q-learning approach. Moreover, we propose the reinforcement-learning-based seeding scheme, which is leveraged in an unknown opponent strategy. Experiment results verify the effectiveness of our method in terms of effective opinions on three signed datasets.","2329-924X","","10.1109/TCSS.2021.3120421","National Natural Science Foundation of China(grant numbers:61872073); Key Project of Natural Science Foundation of China(grant numbers:62032013); NFSC through the Major International (Regional) Joint Research Project(grant numbers:71620107003); LiaoNing Revitalization Talents Program(grant numbers:XLYC1902010); Fundamental Research Funds for the Central Universities(grant numbers:N2119004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9611781","Activated dynamic opinion model;opinion maximization (OM);Q-learning theory;signed social network","Social networking (online);Integrated circuit modeling;Companies;Greedy algorithms;Reinforcement learning;Ions;Industries","computational complexity;optimisation;reinforcement learning;social aspects of automation;social network theory;social networking (online)","opinion propagation process;activated dynamic opinion model;signed social networks;maximum opinion propagation;competitive influence maximization;reinforcement-learning-based competitive opinion maximization;stateless Q-learning approach;influential individuals;seed nodes;RLOM;reinforcement-learning-based seeding process;NP-hard problem;opponent strategy","","10","","42","IEEE","11 Nov 2021","","","IEEE","IEEE Journals"
"A Hybrid Reinforcement Learning Approach With a Spiking Actor Network for Efficient Robotic Arm Target Reaching","K. M. Oikonomou; I. Kansizoglou; A. Gasteratos","Department of Production and Management Engineering, Laboratory of Robotics and Automation, Democritus University of Thrace, Xanthi, Greece; Department of Production and Management Engineering, Laboratory of Robotics and Automation, Democritus University of Thrace, Xanthi, Greece; Department of Production and Management Engineering, Laboratory of Robotics and Automation, Democritus University of Thrace, Xanthi, Greece","IEEE Robotics and Automation Letters","12 Apr 2023","2023","8","5","3007","3014","The increasing demand for applications in competitive fields, such as assisted living and aerial robots, drives contemporary research into the development, implementation and integration of power-constrained solutions. Although, deep neural networks (DNNs) have achieved remarkable performances in many robotics applications, energy consumption remains a major limitation. The letter at hand proposes a hybrid variation of the well-established deep deterministic policy gradient (DDPG) reinforcement learning approach to train a 6 $^{\circ }$ of freedom robotic arm in the target-reach task available at: In particular, we introduce a spiking neural network (SNN) for the actor model and a DNN for the critic one, aiming to find an optimal set of actions for the robot. The deep critic network is employed only during training and discarded afterwards, allowing the deployment of the SNN in neuromorphic hardware for inference. The agent is supported by a combination of RGB and laser scan data exploited for collision avoidance and object detection. We compare the hybrid-DDPG model against a classic DDPG one, demonstrating the superiority of our approach.","2377-3766","","10.1109/LRA.2023.3264836","Autonomous Robotic Unmanned Aerial Vehicle System for Navigation in Inaccessible Interior Spaces and Human Detection - MIDRES; Study, Design, Development and Implementation of a Holistic System for Upgrading the Quality of Life and Activity of the Elderly; European Commission; Greek National funds(grant numbers:T2EDK-00592); Support for Regional Excellence(grant numbers:MIS5047294); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10092938","Bioinspired robot learning;reinforcement learning;spiking neural networks;robotic arm target reach;deep deterministic policy gradient","Robots;Task analysis;Neurons;End effectors;Biological neural networks;6-DOF;Training","collision avoidance;deep learning (artificial intelligence);energy consumption;gradient methods;learning (artificial intelligence);neural nets;object detection;reinforcement learning;telecommunication computing","actor model;aerial robots;assisted living;competitive fields;deep critic network;deep deterministic policy gradient reinforcement learning approach;deep neural networks;drives contemporary research;efficient robotic arm target reaching;energy consumption;freedom robotic arm;hybrid reinforcement learning approach;hybrid variation;hybrid-DDPG model;power-constrained solutions;remarkable performances;robotics applications;SNN;spiking actor network;spiking neural network;target-reach task","","1","","34","IEEE","5 Apr 2023","","","IEEE","IEEE Journals"
"A Reinforcement Learning Driven Cooperative Meta-Heuristic Algorithm for Energy-Efficient Distributed No-Wait Flow-Shop Scheduling With Sequence-Dependent Setup Time","F. Zhao; T. Jiang; L. Wang","School of Computer and Communication Technology, Lanzhou University of Technology, Lanzhou, China; School of Computer and Communication Technology, Lanzhou University of Technology, Lanzhou, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Industrial Informatics","21 Jun 2023","2023","19","7","8427","8440","Green manufacturing has attracted increasing attention under the background of carbon peaking and carbon neutrality. Distributed production has widely existed in various manufacturing industries with the development of globalization. This article investigates an energy-efficient distributed no-wait flow-shop scheduling problem with sequence-dependent setup time (DNWFSP-SDST) to minimization of makespan and total energy consumption (TEC). A mixed-integer linear programming model of energy-efficient DNWFSP-SDST is constructed and a cooperative meta-heuristic algorithm based on Q-learning (CMAQ) is proposed to address energy-efficient DNWFSP-SDST in this article. In CMAQ, a heuristic named RNRa is proposed to generate initial solutions. A bipopulation cooperative framework based on double Q-learning is designed to further optimize the solutions. According to the properties of energy-efficient DNWFSP-SDST, an energy-saving strategy based on knowledge is proposed to improve makespan and TEC. The results of experiments show that the performance of CMAQ is superior to certain state-of-the-art comparison algorithms in solving energy-efficient DNWFSP-SDST.","1941-0050","","10.1109/TII.2022.3218645","National Natural Science Foundation of China(grant numbers:62063021,62273193); Key Talent Project of Gansu Province(grant numbers:ZZ2021G50700016); Key Research Programs of Science and Technology Commission Foundation of Gansu Province(grant numbers:21YF5WA086); Lanzhou Science Bureau project(grant numbers:2018-rc-98); Natural Science Foundation of Gansu Province(grant numbers:21JR7RA204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933863","Cooperative meta-heuristic algorithm;energy-efficient distributed scheduling;knowledge;no-wait flow-shop;Q-learning;sequence-dependent setup time","Energy efficiency;Production facilities;Job shop scheduling;Q-learning;Metaheuristics;Energy consumption;Steel","carbon;energy consumption;flow shop scheduling;globalisation;heuristic programming;integer programming;linear programming;production engineering computing;reinforcement learning","bipopulation cooperative framework;C/el;carbon neutrality;carbon peaking;CMAQ;double Q-learning;energy saving strategy;energy-efficient distributed no-wait flow-shop scheduling-sequence-dependent setup time;energy-efficient DNWFSP-SDST;flow-shop scheduling problem;globalization development;green manufacturing;manufacturing industries;meta-heuristic algorithm;mixed-integer linear programming model;reinforcement learning;sequence-dependent setup time;TEC;total energy consumption","","9","","33","IEEE","1 Nov 2022","","","IEEE","IEEE Journals"
"Trust-Region Method with Deep Reinforcement Learning in Analog Design Space Exploration","K. -E. Yang; C. -Y. Tsai; H. -H. Shen; C. -F. Chiang; F. -M. Tsai; C. -A. Wang; Y. Ting; C. -S. Yeh; C. -T. Lai","EECS, National Tsing Hua University, Hsinchu, Taiwan; MediaTek Inc., Hsinchu, Taiwan; MediaTek Inc., Hsinchu, Taiwan; MediaTek Inc., Hsinchu, Taiwan; MediaTek Inc., Hsinchu, Taiwan; MediaTek Inc., Hsinchu, Taiwan; MediaTek Inc., Hsinchu, Taiwan; MediaTek Inc., Hsinchu, Taiwan; MediaTek Inc., Hsinchu, Taiwan","2021 58th ACM/IEEE Design Automation Conference (DAC)","8 Nov 2021","2021","","","1225","1230","This paper introduces new perspectives on analog design space search. To minimize the time-to-market, this endeavor better cast as constraint satisfaction problem than global optimization defined in prior arts. We incorporate model based agents, contrasted with model-free learning, to implement a trust-region strategy. As such, simple feed-forward networks can be trained with supervised learning, where the convergence is relatively trivial. Experiment results demonstrate orders of magnitude improvement on search iterations. Additionally, the unprecedented consideration of PVT conditions are accommodated. On circuits with TSMC 5/6nm process, our method achieve performance surpassing human designers. Furthermore, this framework is in production in industrial settings.","0738-100X","978-1-6654-3274-0","10.1109/DAC18074.2021.9586087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586087","transistor sizing;artificial intelligence;reinforcement learning;electronic design automation","Employee welfare;Design automation;Supervised learning;Reinforcement learning;Production;Search problems;Space exploration","constraint satisfaction problems;deep learning (artificial intelligence);electronic design automation;feedforward neural nets;iterative methods;multi-agent systems;optimisation;supervised learning","performance surpassing human designers;trust-region method;deep reinforcement learning;analog design space exploration;analog design space search;time-to-market;constraint satisfaction problem;global optimization;model-free learning;simple feed-forward networks;supervised learning;search iterations;model based agents;PVT condition;TSMC process","","4","","24","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Modeling and Dynamic Analysis of Three-Degree-of-Freedom Spherical Actuator under Deep Reinforcement Learning Control","H. Fusayasu; A. Heya; K. Hirata","Manufacturing Innovation Division, Panasonic Connect Co., Ltd., Kadoma, Osaka, Japan; Materials and Manufacturing Science, Graduate School of Engineering, Osaka University, Suita, Osaka, Japan; Materials and Manufacturing Science, Graduate School of Engineering, Osaka University, Suita, Osaka, Japan","2022 23rd International Conference on the Computation of Electromagnetic Fields (COMPUMAG)","21 Jul 2022","2022","","","1","4","Multi-degree-of-freedom (multi-DOF) spherical actuators have been developed for the fields of robotics and industrial machinery. The input current for each coil is calculated using a torque generating equation based on a torque map. If there is a difference between the analyzed and measured torque maps, this modeling error will reduce positioning accuracy. Permanent magnet type actuators can generate unexpected cogging torque due to various manufacturing errors. This manufacturing variation causes a torque map modeling error and significantly deteriorates the performance of conventional control systems using proportional-integral-differential (PID) controllers. Therefore, we propose a method to improve the positioning accuracy by introducing a compensator using reinforcement learning for the torque map modeling error pre-calculated by a 3-D finite element method (3-D FEM). We designed a simulation that assumes manufacturing variations in cogging torque and applied this current compensator to test its performance.","","978-1-6654-9863-0","10.1109/COMPUMAG55718.2022.9826990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826990","multi-degree-of-freedom;spherical actuator;deep reinforcement learning;industrial robots;torque constant map","Actuators;Solid modeling;Forging;Analytical models;Torque;Reinforcement learning;Mathematical models","actuators;automotive components;electric actuators;finite element analysis;permanent magnet motors;permanent magnets;position control;steering systems;synchronous motors;three-term control;torque","three-degree-of-freedom spherical actuator;deep reinforcement learning control;multidegree-of-freedom;spherical actuators;industrial machinery;input current;analyzed measured torque maps;positioning accuracy;permanent magnet type actuators;unexpected cogging torque;manufacturing errors;manufacturing variation;conventional control systems;proportional-integral-differential controllers;torque map modeling error;current compensator","","","","9","IEEE","21 Jul 2022","","","IEEE","IEEE Conferences"
"Behavior control of multi-robot using the prior-knowledge based reinforcement learning","Meiping Song; Guochang Gu; Rubo Zhang","College of computer Science and Technology, Harbin Engineering of Technology, Harbin, Heilongjiang, China; College of computer Science and Technology, Harbin Engineering of Technology, Harbin, Heilongjiang, China; College of computer Science and Technology, Harbin Engineering of Technology, Harbin, Heilongjiang, China","Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788)","18 Oct 2004","2004","6","","5027","5030 Vol.6","In the partially known environment, it was hard to control the robot's behavior exactly and flexibly. The rule-based method can't cover all the possible conditions, and the traditional reinforcement learning method also has the problem of convergence. The prior-knowledge based reinforcement learning prompted here combines the advantages of these two methods and avoids the above disadvantages. It takes the determinately known rules as prior-knowledge to train the learner, so as to guarantee the direction and convergence of learning and speed up the learning. At the same time, the adaptive quality of learner makes it automatically exploit the unknown environment. This makes up the shortcoming of the incompletely known rules. When this method is applied to the action integration of robot's behavior control in the pursuit-evasion game, it overcomes the toothed problem of rule-based control and the unexpected cases of traditional reinforcement learning. The robot is proved experimentally to circumambulate the obstacles smoothly, and collide with the obstacle rarely.","","0-7803-8273-0","10.1109/WCICA.2004.1343673","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1343673","","Learning;Robot control;Collision avoidance;Robotics and automation;Automatic control;Orbital robotics;Robot sensing systems;Educational institutions;Knowledge engineering;Convergence","multi-robot systems;learning (artificial intelligence);convergence;collision avoidance;mobile robots;intelligent control","multirobot system;reinforcement learning;behavior control;convergence;rule control;pursuit-evasion game;obstacle circumambulation","","3","","4","IEEE","18 Oct 2004","","","IEEE","IEEE Conferences"
"Reinforcement Learning with the Classical Q-Learning Algorithm for Optimizing Single Intersection Performance","M. Rosyidi; S. Bismantoko; T. Widodo","Center Of Technology For System And Infrastructure Of Transportation, Agency For The Assessment And Application Of Technology, Tangerang Selatan, Indonesia; Center Of Technology For System And Infrastructure Of Transportation, Agency For The Assessment And Application Of Technology, Tangerang Selatan, Indonesia; Center Of Technology For System And Infrastructure Of Transportation, Agency For The Assessment And Application Of Technology, Tangerang Selatan, Indonesia","2020 International Conference on Data Analytics for Business and Industry: Way Towards a Sustainable Economy (ICDABI)","20 Jan 2021","2020","","","1","5","Many factors causing the issue of the land transportation like intersections problems . In cities where the volume of vehicles passing through an intersection is so large that sometimes a vehicle can pass through an intersection after two or three red lights. The length of the queue at each leg of the intersection can cause the average delay time to be large. In this research, optimizing the performance of a single intersection is an attempt to minimize the average delay time at a single intersection using the classical Q-Learning algorithm. The data used in this study were obtained from the local government of Pekalongan City. The results of this study indicate that minimizing the average delay time will also affect the traffic light time cycle setting, which causes the queue length at the intersection to decrease as well, in this study the parameter value used for the learning rate is 0.5, and the average delay time decreased by 2.96 seconds.","","978-1-7281-9675-6","10.1109/ICDABI51230.2020.9325657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9325657","Q-Learning;Single Intersection;Optimizing;Delay Time","Delays;Reinforcement learning;Urban areas;Transportation;Legged locomotion;Industries;Data analysis","learning (artificial intelligence);road traffic;road vehicles;traffic engineering computing;transportation","optimizing single intersection performance;intersections problems;average delay time;classical Q-learning algorithm;traffic light time cycle setting;learning rate;land transportation;local government;Pekalongan City","","2","","10","IEEE","20 Jan 2021","","","IEEE","IEEE Conferences"
"An Online Reinforcement Learning Approach for Solving the Dynamic Flexible Job-Shop Scheduling Problem for Multiple Products and Constraints","N. E. -D. Ali Said; Y. Samaha; E. Azab; L. A. Shihata; M. Mashaly","Mechatronics Department, German University in Cairo, Cairo, Egypt; Networks Department, German University in Cairo, Cairo, Egypt; Electronics Departement, German University in Cairo, Cairo, Egypt; Production Departement, German University in Cairo, Cairo, Egypt; Networks Department, German University in Cairo, Cairo, Egypt","2021 International Conference on Computational Science and Computational Intelligence (CSCI)","22 Jun 2022","2021","","","134","139","In the manufacturing industries, the most challenging problems are mostly related to time efficiency and customer satisfaction. This is mainly translated to how efficient is the frequent task of scheduling jobs to alternative routes on a number of machines. Although scheduling has been studied for decades, there is a shortage to a generalized approach for the production scheduling that can adapt to changes occurring continuously at any production environment. This research work addresses the dynamic production scheduling problem and the optimization techniques that could be applied to the production schedule to increase its efficiency. An algorithm is proposed to apply the Q-learning optimization technique on a dynamic flexible job-shop scheduling problem of a real case study of a pharmaceutical factory with 18 machines and 22 products. Proposed algorithm is shown to be able to achieve an efficient schedule with short make-span in minimal time duration and without requiring any learning process from previous schedules, thus increasing the factory's overall efficiency. In addition, the proposed algorithm operates online as any change occurring in the production environment is signaled automatically to it where it responds be regenerating the most optimal updated production schedule.","","978-1-6654-5841-2","10.1109/CSCI54926.2021.00095","Information Technology Industry Development Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9799136","Artificial Intelligence;Job-Shop;Optimization;Production Scheduling;Reinforcement Learning","Schedules;Job shop scheduling;Q-learning;Scientific computing;Processor scheduling;Heuristic algorithms;Dynamic scheduling","customer satisfaction;job shop scheduling;learning (artificial intelligence);scheduling","production environment;dynamic production scheduling problem;Q-learning optimization technique;dynamic flexible job-shop scheduling problem;efficient schedule;previous schedules;optimal updated production schedule;online reinforcement learning approach;time efficiency;scheduling jobs","","","","21","IEEE","22 Jun 2022","","","IEEE","IEEE Conferences"
"Sim-to-real: Six-legged Robot Control with Deep Reinforcement Learning and Curriculum Learning","B. Qin; Y. Gao; Y. Bai","Dept. automation, Shanghai Jiao Tong University, Shang hai, China; MoE Key Lab of Articial Intelligence dept. of Automation, Shanghai Jiao Tong University, Shang hai, China; Dept. automation, Shanghai Jiao Tong University, Shang hai, China","2019 4th International Conference on Robotics and Automation Engineering (ICRAE)","23 Mar 2020","2019","","","1","5","Six-Iegged robots have higher stability and balance, which helps them face more complex terrain conditions, such as sand, swamp, mine and so forth. Therefore, it is necessary to study the gait planning of six-legged robot to adapt to complex terrain. In order to control six-legged robots to adapt to different terrains, we adopt the method of deep reinforcement learning (DRL) to plan the gait of six-legged robots. The main idea is training the robot through Actor-Critic network with proximal policy optimization (PPO), in which outputs are step length, step height and orientation of the robot. This is an end-to-end approach, which tries to make the robot learn by itself and finally achieve its safe arrival to the target point through complex terrains. In order to train a good model for our robots, simplified environment is adopted to accelerate the training process. We also use curriculum learning to speed up and optimize the training. Then, we verify the reliability of the method in simulation platform and finally transfer the learned model to real robot. Our experiment shows the effectiveness of deep reinforcement learning for locomotion of six-legged robots, the acceleration of the training process by means of curriculum learning, and the improvement of the training effect.","","978-1-7281-4740-6","10.1109/ICRAE48301.2019.9043822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9043822","Six-Iegged robot;deep reinforcement learning;curriculum learning and sim-to-real","Legged locomotion;Training;Robot kinematics;Reinforcement learning;Orbits","learning (artificial intelligence);legged locomotion;optimisation;path planning;robot programming;stability","six-Iegged robots;complex terrain conditions;deep reinforcement learning;training process;curriculum learning;robot control;stability;gait planning;DRL;actor-critic network;proximal policy optimization;robot learning;reliability;simulation platform","","9","","22","IEEE","23 Mar 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning System of UAV for Antenna Beam Localization","S. Omi; H. -S. Shin; A. Tsourdos; J. Espeland; A. Buchi","School of Aerospace, Transport and Manufacturing, Cranfield University, Bedford, UK; School of Aerospace, Transport and Manufacturing, Cranfield University, Bedford, UK; School of Aerospace, Transport and Manufacturing, Cranfield University, Bedford, UK; QuadSAT, Odense, Denmark; QuadSAT, Odense, Denmark","2021 IEEE Conference on Antenna Measurements & Applications (CAMA)","11 Feb 2022","2021","","","61","65","Along with the growth of satellite communication industry, the demands and benefits to perform satellite terminal antenna evaluation are increasing. UAV based in-situ measurement can increase the efficiency of the measurement procedure. Main beam localization is a necessary procedure to execute the antenna evaluation test. To accelerate the process of finding the antenna beam centre, this paper develop a meta-reinforcement learning based algorithm. The developed algorithm is compared with other methods and it showed the best performance in terms of accuracy, robustness and travelling efficiency not only in the simulated radiation pattern environment but also in the empirically obtained radiation pattern.","2643-6795","978-1-7281-9697-8","10.1109/CAMA49227.2021.9703640","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9703640","antenna measurement;UAV measurement;beam localization;meta-reinforcement learning","Antenna measurements;Location awareness;Industries;Satellite antennas;Particle beams;Satellites;Reinforcement learning","antenna radiation patterns;antenna testing;autonomous aerial vehicles;reinforcement learning;satellite antennas;telecommunication computing","main beam localization;antenna evaluation test;antenna beam centre;travelling efficiency;reinforcement learning system;antenna beam localization;satellite terminal antenna evaluation;UAV based in-situ measurement;metareinforcement learning based algorithm;simulated radiation pattern environment;empirically obtained radiation pattern","","2","","15","IEEE","11 Feb 2022","","","IEEE","IEEE Conferences"
"Environment-Adaptable Printed-Circuit Board Positioning Using Deep Reinforcement Learning","C. Solorzano; D. -M. Tsai","Industrial Engineering and Management, Yuan-Ze University, Taoyuan, Taiwan; Industrial Engineering and Management, Yuan-Ze University, Taoyuan, Taiwan","IEEE Transactions on Components, Packaging and Manufacturing Technology","22 Feb 2022","2022","12","2","382","390","Vision-based object positioning is very important in the electronic industry for assembly and inspection tasks. Many methods have been proposed to tackle the problem, either by traditional machine vision or by deep learning (DL) techniques. The traditional methods rely on template matching or feature point correspondence. They are computationally intensive and are easily affected by illumination changes and noise. DL models such as convolutional neural networks (CNNs) are computationally very efficient but are also sensitive against environmental changes. In this article, a deep reinforcement learning (DRL) model based on the Actor-Critic style Proximal Policy Optimization algorithm(s) (AC-PPO) is proposed. The proposed method is applied for the positioning of printed circuit boards (PCBs). The model uses as the current environment the sensed image and the reference template as a guide. It requires only a single manually marked template in the reference image. All possible training images are automatically and randomly generated during the neural network training without human intervention. The proposed reinforcement learning (RL) model is shown to be adaptive to environmental changes, including illumination, noise, de-focusing, and template occlusion, compared with the CNN regressor. Experimental results indicate that the proposed model on average can achieve estimation errors less than 1 pixel in translation and 1° in orientation, with fast evaluation for the real-time PCB positioning task.","2156-3985","","10.1109/TCPMT.2022.3142033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678960","Printed circuit board (PCB) positioning;proximal policy optimization;reinforcement learning (RL);vision-based measurement","Lighting;Training;Convolutional neural networks;Task analysis;Inspection;Reinforcement learning;Visualization","automatic optical inspection;computer vision;convolutional neural nets;deep learning (artificial intelligence);printed circuit manufacture;production engineering computing","environment-adaptable printed-circuit board positioning;vision-based object positioning;electronic industry;assembly;inspection tasks;machine vision;template matching;illumination changes;convolutional neural networks;deep reinforcement learning model;AC-PPO;printed circuit boards;reference image;neural network training;real-time PCB positioning task;actor-critic style proximal policy optimization algorithm","","4","","38","IEEE","12 Jan 2022","","","IEEE","IEEE Journals"
"Transfer learning via linear multi-variable mapping under reinforcement learning framework","Q. Cheng; X. Wang; L. Shen","College of Mechatronics and Automation, National University of Defense Technology, Changsha, Hunan Province, P. R. China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, Hunan Province, P. R. China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, Hunan Province, P. R. China","2017 36th Chinese Control Conference (CCC)","11 Sep 2017","2017","","","8795","8799","Though popular in many agent learning tasks, reinforcement learning still faces problems, such as long learning time in complex environment. Transfer learning could shorten the learning time and improve the performance in reinforcement learning by reusing the knowledge acquired from different but related source task. Due to the difference in state space and/or action space of the target and source task, transfer via inter-task mapping is a popular method. The design of the inter-task mapping is very critical to this transfer learning method. In this paper, we propose a linear multi-variable mapping (LMVM) for the transfer learning to make a better use of the knowledge learned from the source task. Unlike the inter-task mapping used before, the LMVM is not a one-to-one mapping but a one-to-many mapping, which is based on the idea that the element in target task is related with several similar elements from source task. We test transfer learning via our new mapping on the Keepaway platform. The experimental results show that our method could make the reinforcement learning agents learn much faster than those without transfer and those transfer with simpler mappings.","1934-1768","978-988-15639-3-4","10.23919/ChiCC.2017.8028754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8028754","Transfer Learning;Reinforcement Learning;Linear Multi-variable Mapping;Inter-task Mapping;Keepaway","Learning (artificial intelligence);Transfer functions;Learning systems;Mechatronics;Automation;Electronic mail;Design methodology","learning (artificial intelligence);multi-agent systems","linear multivariable mapping;reinforcement learning framework;agent learning tasks;transfer learning method;reinforcement learning agents;inter-task mapping method;LMVM;Keepaway platform;one-to-one mapping;one-to-many mapping","","3","","9","","11 Sep 2017","","","IEEE","IEEE Conferences"
"Continuous reinforcement learning algorithm for skills learning in an autonomous mobile robot","M. J. L. Boada; V. Egido; R. Barber; M. A. Salichs","Mechanical Department, Carlos III Technical University of Madrid, Madrid, Spain; System Engineering and Automation Department, Carlos III Technical University of Madrid, Madrid, Spain; System Engineering and Automation Department, Carlos III Technical University of Madrid, Madrid, Spain; System Engineering and Automation Department, Carlos III Technical University of Madrid, Madrid, Spain","IEEE 2002 28th Annual Conference of the Industrial Electronics Society. IECON 02","28 Feb 2003","2002","4","","2611","2616 vol.4","Learning endows a mobile robot with a higher flexibility and allows it to adapt to changes occurring in the environment or in its internal state in order to improve its results. Based on this idea, this paper presents a reinforcement learning algorithm which allows the robot to learn simple skills such as go to goal and contour following. In the proposed learning algorithm the robot receives a real continuous reinforcement signal. Thus, it is not necessary to estimate an expected reward. Most of the robotic applications work with continuous variables such as velocity, position, sensors readings etc. The presented reinforcement learning algorithm is able to manage continuous input and output spaces. Finally, the robot is capable of performing the complex skill called go to go avoiding obstacles from the sequencing of previously learnt skills.","","0-7803-7474-6","10.1109/IECON.2002.1182805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1182805","","Mobile robots;Robot sensing systems;Orbital robotics;Systems engineering and theory;Robotics and automation;Intelligent sensors;Neural networks;Legged locomotion;Supervised learning;Intelligent robots","mobile robots;robot vision;learning (artificial intelligence);collision avoidance","RWI-1321 mobile robot;vision system;laser sensor;continuous reinforcement learning algorithm;skills learning;autonomous mobile robot;go to goal;contour following;continuous reinforcement signal;continuous input space management;continuous output space management;go to go avoiding obstacles","","","","18","IEEE","28 Feb 2003","","","IEEE","IEEE Conferences"
"MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer","Q. Yang; J. A. Stork; T. Stoyanov","Autonomous Mobile Manipulation lab at the Center for Applied Autonomous Sensor Systems (AASS), Örebro University, Örebro, Sweden; Autonomous Mobile Manipulation lab at the Center for Applied Autonomous Sensor Systems (AASS), Örebro University, Örebro, Sweden; Autonomous Mobile Manipulation lab at the Center for Applied Autonomous Sensor Systems (AASS), Örebro University, Örebro, Sweden","IEEE Robotics and Automation Letters","29 Jun 2022","2022","7","3","7652","7659","In manufacturing, assembly tasks have been a challenge for learning algorithms due to variant dynamics of different environments. Reinforcement learning (RL) is a promising framework to automatically learn these tasks, yet it is still not easy to apply a learned policy or skill, that is the ability of solving a task, to a similar environment even if the deployment conditions are only slightly different. In this letter, we address the challenge of transferring knowledge within a family of similar tasks by leveraging multiple skill priors. We propose to learn prior distribution over the specific skill required to accomplish each task and compose the family of skill priors to guide learning the policy for a new task by comparing the similarity between the target task and the prior ones. Our method learns a latent action space representing the skill embedding from demonstrated trajectories for each prior task. We have evaluated our method on a task in simulation and a set of peg-in-hole insertion tasks and demonstrate better generalization to new tasks that have never been encountered during training. Our Multi-Prior Regularized RL (MPR-RL) method is deployed directly on a real world Franka Panda arm, requiring only a set of demonstrated trajectories from similar, but crucially not identical, problem instances.","2377-3766","","10.1109/LRA.2022.3184805","Wallenberg AI; Autonomous Systems and Software Program; Knut och Alice Wallenbergs Stiftelse; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9803274","Machine learning for robot control;reinforcement learning;transfer learning","Task analysis;Robots;Reinforcement learning;Aerospace electronics;Trajectory;Training;Entropy","control engineering computing;generalisation (artificial intelligence);industrial manipulators;intelligent robots;production engineering computing;reinforcement learning;robotic assembly","demonstrated trajectories;peg-in-hole insertion tasks;MPR-RL;reinforcement learning;knowledge transfer;assembly tasks;variant dynamics;learned policy;multiple skill priors;multiprior regularized RL method;manufacturing;generalization;Franka Panda arm","","4","","31","CCBY","22 Jun 2022","","","IEEE","IEEE Journals"
"An Implementation of Reinforcement Learning in Assembly Path Planning based on 3D Point Clouds","W. -C. Chang; D. P. Andini; V. -T. Pham","Department of Electrical Engineering, National Taipei University of Technology, Taipei, Taiwan, R.O.C.; International Graduate Program in Electrical Engineering and Computer Science, National Taipei University of Technology, Taipei, Taiwan, R.O.C.; Department of Electrical Engineering, National Taipei University of Technology, Taipei, Taiwan, R.O.C.","2018 International Automatic Control Conference (CACS)","10 Jan 2019","2018","","","1","6","3D point clouds consisting of a lot of informatively geometric data have been playing critical roles in many applications such as 3D segmentation, polyline annotation for lane tracking, and especially in manufacturing industry. In particular, this paper proposes to apply Reinforcement Learning (RL) to resolve an automated assembly task based on 3D point cloud data. To address this task, the proposed structure is separated into 2 stages including registration stage and assembly path planning stage. Firstly, in the registration stage, one of the objects is matched to an assembled model to determine the transformation between two 3D point clouds by using RANdom Sample Consensus (RANSAC) and Iterative Closet Point (ICP). Secondly, we employ Q-learning method to train a model to make optimal decisions in assemble path planning task. The entire optimized assembly path planning task has been successfully accomplished for typical objects. Finally, the performance of the approach developed in this paper has been validated by experiments.","","978-1-5386-6278-6","10.1109/CACS.2018.8606737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606737","","Three-dimensional displays;Training;Path planning;Task analysis;Solid modeling;Reinforcement learning;Machine learning algorithms","assembling;computer graphics;image registration;iterative methods;learning (artificial intelligence);path planning;production engineering computing","assemble path planning task;informatively geometric data;automated assembly task;3D point cloud data;iterative closet point;reinforcement learning;registration stage;assembly path planning stage;random sample consensus;RANSAC;ICP;Q-learning method","","2","","23","IEEE","10 Jan 2019","","","IEEE","IEEE Conferences"
"Calibration of Agent-Based Model Using Reinforcement Learning","B. Song; G. Xiong; S. Yu; P. Ye; X. Dong; Y. Lv","The State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Fraunhofer Institute for Systems and Innovation Research, Breslauer, Germany; The State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI)","22 Sep 2021","2021","","","278","281","In the research and application of Agent-Based Models (ABM), parameter calibration is an important content. Based on the existing state transfer equations that link the micro-parameters and macro-states of the multi-agent system, this paper further proposes to introduce Reinforcement Learning when calibrating the parameters. The state transfer of the agent after learning is used to calibrate the micro-parameters of ABM, and the interaction between each agent and multiple other agents is expressed as the parameters of the agent. The application case study of population migration demonstrates that our method can achieve high accuracy and low computational complexity.","","978-1-6654-3337-2","10.1109/DTPI52967.2021.9540180","National Key R&D Program of China(grant numbers:2018YFB1702700); National Natural Science Foundation of China(grant numbers:U1909204,61773381,U1811463,U19B2029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9540180","Agent-Based Model;Reinforcement Learning;calibration","Learning systems;Microscopy;Digital twin;Sociology;Reinforcement learning;Mathematical models;Calibration","learning (artificial intelligence);multi-agent systems","ABM;reinforcement learning;agent based models;parameter calibration;multiagent system;state transfer equations","","2","","19","IEEE","22 Sep 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-Based QoS Optimization for Software-Defined Factory Heterogeneous Networks","D. Xia; J. Wan; P. Xu; J. Tan","Guangdong Provincial Key Laboratory of Technique and Equipment for Macromolecular Advanced Manufacturing, School of Mechanical and Automotive Engineering, South China University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Technique and Equipment for Macromolecular Advanced Manufacturing, School of Mechanical and Automotive Engineering, South China University of Technology, Guangzhou, China; Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Technique and Equipment for Macromolecular Advanced Manufacturing, School of Mechanical and Automotive Engineering, South China University of Technology, Guangzhou, China","IEEE Transactions on Network and Service Management","1 Feb 2023","2022","19","4","4058","4068","The routing algorithm based on a single routing metric parameter is difficult to meet the Quality of Services (QoS) requirements of multi-source data flows in the software-defined factory heterogeneous network, resulting in link congestion and waste of network resources. To solve the problem, this paper proposes a QoS optimization method for software-defined factory heterogeneous networks based on the double deep Q network (DDQN). First, a QoS optimization architecture is proposed for software-defined factory heterogeneous networks, and the optimization function for network latency and load balancing is established. Then, the DDQN algorithm is used to obtain the optimal paths of multi-source data flows, and the optimal path is uniformly issued by the software-defined network controller. Experiments showed that the proposed algorithm has good convergence and generalizability. Compared with the existing algorithms, it outperforms in network latency, network jitter, and network throughput, improving network load balancing and routing efficiency. This research is able to provide a solution to the QoS optimization and dynamic traffic scheduling in smart heterogeneous networks and provide a technical reference point for realizing customized smart factories.","1932-4537","","10.1109/TNSM.2022.3208342","Guangdong Province Key Areas R & D Program(grant numbers:2019B010150002); Joint Fund of the National Natural Science Foundation of China and Guangdong Province(grant numbers:U1801264); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9896990","Software-defined network;QoS optimization;traffic scheduling;double deep Q network;smart factory","Quality of service;Routing;Optimization;Heuristic algorithms;Production facilities;Delays;Smart manufacturing","deep learning (artificial intelligence);optimisation;production facilities;quality of service;reinforcement learning;resource allocation;software defined networking;telecommunication computing;telecommunication network routing;telecommunication traffic","customized smart factories;deep reinforcement learning;double deep Q network;dynamic traffic scheduling;link congestion;multisource data flows;network jitter;network latency;network load balancing;network resources;network throughput;optimal path;optimization function;QoS optimization architecture;QoS optimization method;quality of services;routing algorithm;single routing metric parameter;smart heterogeneous networks;software-defined factory heterogeneous network;software-defined network controller","","3","","32","IEEE","21 Sep 2022","","","IEEE","IEEE Journals"
"Enabling Rewards for Reinforcement Learning in Laser Beam Welding processes through Deep Learning","M. Schmitz; F. Pinsker; A. Ruhri; B. Jiang; G. Safronov","Data Analytics in Production / BMW Group Chair of IT-Management, University of Erlangen-Nurmeberg, Munich, Germany; PU Digital Solutions T-Systems, International GmbH, Munich, Germany; Overall Vehicle Integration, BMW Group, Munich, Germany; PU Digital Solutions T-Systems, International GmbH, Munich, Germany; Innovation Management / BMW Group Institute of Materials Engineering of Additive Manufacturing, Technical University of Munich, Munich, Germany","2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA)","23 Feb 2021","2020","","","1424","1431","Self-optimizing robots and machines in future factories are an exciting next step towards an ever more efficient industry. To achieve this goal, robots used in production must gain an understanding of the quality of their behavior. Machine learning can help us move closer to this goal. In this paper, we provide insights on the feasibility of self-optimized laser welding robots and show how accurate quality analysis based on deep learning and smart computer vision algorithms provide a reliable input for quality evaluation and ultimately a scoring function. Furthermore, the suggested scoring function can capture the defining properties of a weld. In turn, the score can be used as feedback to define a reward for a reinforcement learning agent's action, which then optimizes the robot's behavior accordingly. Our experiments show that we can achieve very good accuracy and consistency when evaluating the quality of the weld with deep learning and statistical modeling. Finally, we provide a production-oriented learning architecture that considers the scoring component in a reinforcement learning pipeline.","","978-1-7281-8470-8","10.1109/ICMLA51294.2020.00221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9356189","deep learning;reinforcement learning;quality inspection;laser welding","Deep learning;Service robots;Welding;Reinforcement learning;Production facilities;Laser beams;Reliability","computer vision;control engineering computing;deep learning (artificial intelligence);laser beam welding;product quality;robotic welding;statistical analysis","enabling rewards;laser beam welding processes;deep learning;self-optimizing robots;machine learning;self-optimized laser welding robots;quality analysis;smart computer vision algorithms;quality evaluation;scoring function;reinforcement learning pipeline;robot behavior;statistical modeling;production-oriented learning architecture","","2","","30","IEEE","23 Feb 2021","","","IEEE","IEEE Conferences"
"Multiple Environment Integral Reinforcement Learning-Based Fault-Tolerant Control for Affine Nonlinear Systems","H. -J. Ma; L. -X. Xu; G. -H. Yang","College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China","IEEE Transactions on Cybernetics","17 Mar 2021","2021","51","4","1913","1928","This paper studies the fault-tolerant control (FTC) problem for unknown affine nonlinear systems with actuators faults. The considered types of faults are stuck (lock-in-place), loss-in-effectiveness (LIE), and bias, under which a part of the actuators is disabled. The objective is to find the remaining (not fully LIE) actuators, and manipulate them to obtain the best achievable performance in real time. First, considering that the best achievable performance is determined by the remaining actuators, a set of basic policies is predesigned with multiple levels of performances for different groups of activated actuators. Second, an identifier is designed based on history data to find the remaining actuators and, thus, the suitable predesigned basic policy. Third, to further accommodate the partial LIEs and biases, a compensator works together with the selected basic policy, to build the predesigned performance. In addressing the FTC problem, several techniques are developed: adjustable mechanisms are novelly integrated to deal with the state-dependent nonlinearities in neural network (NN) approximation, disturbances, and mismatch errors; history data are newly applied to estimate the faulty parameters; and a compensator is specially designed to deal with LIEs and biases in different input channels. Also in theory, the convergences of algorithms and the stability of closed-loop systems are proved, by formally giving the invariant sets of the initial state and the NN weights. Unlike the existing FTC methods dealing with LIE and bias based on model information to optimize the tracking error, this result can handle stuck faults without knowing system dynamics and satisfy different levels of performances described by Hamilton-Jacobi-Bellman equations. Finally, a simulation example of quadrotor unmanned aerial vehicle is given to verify the effectiveness of the proposed FTC scheme.","2168-2275","","10.1109/TCYB.2018.2889679","National Natural Science Foundation of China(grant numbers:61873306); Fundamental Research Funds for the Central Universities(grant numbers:N170404016,N170308028); State Key Laboratory of Synthetical Automation for Process Industries(grant numbers:2018ZCX19); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8616791","Actuator faults;affine nonlinear systems;fault-tolerant control (FTC);IRL","Actuators;Nonlinear systems;Mathematical model;Fault tolerance;Fault tolerant systems;History;Artificial neural networks","actuators;adaptive control;autonomous aerial vehicles;closed loop systems;control system synthesis;fault diagnosis;fault tolerance;fault tolerant control;neurocontrollers;nonlinear control systems;stability","fault-tolerant control problem;affine nonlinear systems;actuators faults;lock-in-place;loss-in-effectiveness;LIE;activated actuators;history data;suitable predesigned basic policy;partial LIEs;state-dependent nonlinearities;neural network approximation;system dynamics;FTC scheme;multiple environment integral reinforcement learning;Hamilton-Jacobi-Bellman equations;quadrotor unmanned aerial vehicle","","38","","37","IEEE","17 Jan 2019","","","IEEE","IEEE Journals"
"Environment Exploration for Mapless Navigation based on Deep Reinforcement Learning","N. D. Toan; K. Gon-Woo","Department of Control and Robot Engineering, Chungbuk National University, Chungbuk, Korea; Department of Intelligent Systems and Robotics, Chungbuk National University, Chungbuk, Korea","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","17","20","In recent years, reinforcement learning has attracted researchers' attention with the AlphaGo event. Especially in autonomous mobile robots, the reinforcement learning approach can be applied to the mapless navigation problem. The Robot can complete the set tasks well and works well in different environments without maps and ready-made path plans. However, for reinforcement learning in general and mapless navigation based on reinforcement learning in particular, exploitation and exploration balance are issues that need to be carefully considered. Specifically, the fact that the agent (Robot) can discover and execute actions in a particular working environment plays a significant role in improving the performance of the reinforcement learning problem. By creating some noise during the convolutional neural network training, the above problem can be solved by some popular approaches today. With outstanding advantages compared to other approaches, the Boltzmann policy approach has been used in our problem. It helps the Robot explore more thoroughly in complex environments, and the policy is also more optimized.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649893","Ministry of Trade, Industry, and Energy(MOTIE); Korea Institute for Advancement of Technology(KIAT)(grant numbers:P0004631); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649893","reinforcement learning;mapless navigation;exploitation;exploration balancing;Boltzmann policy","Training;Automation;Navigation;Reinforcement learning;Control systems;Mobile robots;Convolutional neural networks","Boltzmann machines;control engineering computing;convolutional neural nets;deep learning (artificial intelligence);intelligent robots;mobile robots;navigation;path planning;reinforcement learning;robot vision","mapless navigation problem;exploration balance;environment exploration;deep reinforcement learning;autonomous mobile robots;AlphaGo event;convolutional neural network training;Boltzmann policy approach","","","","9","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Top-Down Human-Cyber-Physical Data Fusion Based on Reinforcement Learning","S. Chen; J. Wang; H. Li; Z. Wang; F. Liu; S. Li","Computer Integrated Manufacturing System Research Center, College of Electronics and Information Engineering, Tongji University, Shanghai, China; Computer Integrated Manufacturing System Research Center, College of Electronics and Information Engineering, Tongji University, Shanghai, China; Computer Integrated Manufacturing System Research Center, College of Electronics and Information Engineering, Tongji University, Shanghai, China; Computer Integrated Manufacturing System Research Center, College of Electronics and Information Engineering, Tongji University, Shanghai, China; Computer Integrated Manufacturing System Research Center, College of Electronics and Information Engineering, Tongji University, Shanghai, China; Computer Integrated Manufacturing System Research Center, College of Electronics and Information Engineering, Tongji University, Shanghai, China","IEEE Access","28 Jul 2020","2020","8","","134233","134245","With the development of industrial Internet and artificial intelligence, data fusion in cross-domains and cross-layers have become an inevitable trend. Most of the data fusion involved in the production process of hot rolling are concentrated on the level of sensors, Internet of Things (IoT) and the Internet; but human data are not well integrated. In order to avoid the human factor from becoming the bottleneck of the entire production schedule, this paper proposes a ternary data fusion model based on reinforcement learning algorithm. The related data source from human-cyber-physical space includes: social network, Internet and IoT. By merging the ternary data, a variety of data (including humans’) can be quickly calculated to obtain better and faster decisions. In order to achieve automated fusion from ternary data, this paper proposes a method based on reinforcement learning: firstly, the domain ontology used for associating ternary data is reduced and tessellated (dimension reduction), and then the reinforcement learning model is used to form “the new ontology”. Compared with resource-intensive global calculations (which may cost a few days), the new method can complete the calculations in minutes. This means that the new method optimizes the data source required for decision-making and improves the efficiency. Finally, the production scheduling of hot rolled steel is used as an example to verify the feasibility of the proposed method.","2169-3536","","10.1109/ACCESS.2020.3011254","National Science and Technology Innovation 2030 Next-Generation Artificial Intelligence Major Project, Data-Driven Tripartite Collaborative Decision-Making and Optimization(grant numbers:2018AAA0101801); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9146164","Human-cyber-physical data fusion;ternary data fusion;cyber-physical system;domain ontology;reinforcement learning","Data integration;Ontologies;Data models;Manufacturing;Semantics;Learning (artificial intelligence);Internet","Internet of Things;learning (artificial intelligence);ontologies (artificial intelligence);sensor fusion","related data source;human-cyber-physical space;IoT;reinforcement learning model;human-cyber-physical data fusion;cross-domains;cross-layers;human data;human factor;entire production schedule;ternary data fusion model;reinforcement learning algorithm","","6","","51","CCBY","22 Jul 2020","","","IEEE","IEEE Journals"
"Development of a real-time learning scheduler using reinforcement learning concepts","L. C. Rabelo; A. Jones; Y. Yih","Department of Industrial and Systems Engineering, Ohio University, Athens, OH, USA; National Institute for Standards and Technology, Gaithersburg, MD, USA; Grisson Hall, School of Industrial Engineering, Purdue University, West Lafayette, IN, USA","Proceedings of 1994 9th IEEE International Symposium on Intelligent Control","6 Aug 2002","1994","","","291","296","A scheme for the scheduling of flexible manufacturing systems (FMS) has been developed which divides the scheduling function (built upon a generic controller architecture) into four different steps: candidate rule selection, transient phenomena analysis, multicriteria compromise analysis, and learning. This scheme is based on a hybrid architecture which utilizes neural networks, simulation, genetic algorithms, and induction mechanism. This paper investigates the candidate rule selection process, which selects a small list of scheduling rules from a larger list of such rules. This candidate rule selector is developed by using the integration of dynamic programming and neural networks. The system achieves real-time learning using this approach. In addition, since an expert scheduler is not available, it utilizes reinforcement signals from the environment (a measure of how desirable the achieved state is as measured by the resulting performance criteria). The approach is discussed and further research issues are presented.<>","2158-9860","0-7803-1990-7","10.1109/ISIC.1994.367802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=367802","","Learning;Job shop scheduling;Flexible manufacturing systems;Monitoring;Neural networks;Control systems;Transient analysis;Genetic algorithms;Dynamic programming;Constraint optimization","learning (artificial intelligence);flexible manufacturing systems;genetic algorithms;neural nets;dynamic programming;production control","real-time learning scheduler;reinforcement learning concepts;flexible manufacturing systems;generic controller architecture;candidate rule selection;transient phenomena analysis;multicriteria compromise analysis;hybrid architecture;neural networks;simulation;genetic algorithms;induction mechanism;dynamic programming;real-time learning","","4","2","28","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Optimizing Dynamic Timing Analysis and Reinforcement Learning","J. Obert; A. Shia",Sandia National Labs; Sandia National Labs,"2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)","15 Mar 2022","2021","","","105","111","There are multiple factors involved in successfully manufacturing ASIC/VLSI chips, and ensuring operational specifications are maintained throughout the design and manufacturing process is often challenging. Dynamic timing analysis (DTA) is the principal method used to validate that a manufactured chip complies to its design specifications. In DTA functionality of both synchronous and asynchronous designs are verified by applying input signals and checking for correct output signals. In complex designs where the number of input signal permutations is extremely large, the computing resources required to properly verify the functionality of a chip is prohibitive. In this paper, a strategy using reinforcement learning (RL) for reducing DTA time and resources in such cases is discussed. RL assisted DTA holds much promise in ensuring that VLSI chip design and functionality are fully and optimally verified.","","978-1-6654-2174-4","10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00030","Sandia National Laboratories; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9730371","ASIC Design Verification;Reinforcment Learning;Dynamic Timing Analysis;ASIC Design","Manufacturing processes;Reinforcement learning;Very large scale integration;Big Data;Timing;Manufacturing;Chip scale packaging","application specific integrated circuits;electronic engineering computing;integrated circuit design;logic design;reinforcement learning;VLSI","dynamic timing analysis;reinforcement learning;operational specifications;manufacturing process;principal method;manufactured chip;design specifications;DTA functionality;synchronous designs;asynchronous designs;input signals;correct output signals;complex designs;input signal permutations;VLSI chip design;ASIC-VLSI chip","","","","16","IEEE","15 Mar 2022","","","IEEE","IEEE Conferences"
"DuAK: Reinforcement Learning-Based Knowledge Graph Reasoning for Steel Surface Defect Detection","Y. Zhang; H. Wang; W. Shen; G. Peng","ZJU-UIUC Institute, Zhejiang University, Haining, China; ZJU-UIUC Institute, Zhejiang University, Haining, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Advanced Rolling and Intelligent Manufacturing, University of Science and Technology Beijing, Beijing, China","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","13","Surface defect is a crucial factor affecting the product quality of steel products. Current studies mainly focus on defect recognition and classification using machine vision-based algorithms, which lack the trace of potential causes and the reuse of experiential knowledge. To address this issue, we construct a knowledge graph for steel surface defects by fusing the multi-source and heterogeneous industrial data, including process parameters, chemical compositions, defect images, operation logs and empirical knowledge. A policy-based reinforcement learning approach is developed to solve the path reasoning problem over the industrial knowledge graph in defect detection and diagnosis. The approach employs two agents to explore the path efficiently from opposite directions, utilizes an integrated reward function that comprehensively considers the path direction, path length and entity distance to perform action selection, and adopts the path sharing mechanism and the prior knowledge to update selection policy. Experimental comparisons with the state-of-the-art knowledge reasoning algorithms on two benchmark datasets, NELL-995 and FB15K-237, validate the performance and merits of the proposed method. The effectiveness of the proposed method is also evaluated on a practical steel surface defect dataset, and the results show that our approach performs well in knowledge reasoning on the surface defect graph. Note to Practitioners—The surface quality of products has become a widely concerned focus in manufacturing industries. With the development of industrial IoT and Cyber-physical system technologies, more and more industrial data has been collected, and machine learning-based algorithms have been developed and applied to the recognition of detect defects. However, the algorithms do not take full advantage of the multi-source and heterogeneous defect-related data. On the other hand, it is also difficult to accumulate, inherit and reuse the experts’ knowledge of solving historical cases in the long-term production process. In order to deal with the above obstacles, we apply the knowledge graph for steel surface defect detection. In the proposed approach, a policy-based reinforcement learning algorithm is developed to solve the path reasoning problem over the industrial knowledge graph. To further improve the performance of our algorithm, we employ two agents to explore the path efficiently from opposite directions, utilize an integrated reward function which comprehensively considers the path direction, path length and entity distance to perform action selection, adopt the path sharing mechanism and updated selection policy to reuse the prior knowledge. As a result, our algorithm obtains high precision in knowledge reasoning tasks on two benchmark datasets and a practical steel surface defect dataset compared with some existing algorithms. Hence, it can be readily applied to real surface defect detection problems and facilitates intelligent manufacturing in steel production.","1558-3783","","10.1109/TASE.2023.3307588","National Key Research and Development Program of China(grant numbers:2020YFB1707803); National Natural Science Foundation of China(grant numbers:62273032,61903031); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LDT23F02023F02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247201","Steel surface defect;knowledge graph;reinforcement learning;prior knowledge","Steel;Knowledge graphs;Cognition;Surface treatment;Production;Feature extraction;Classification algorithms","","","","","","","IEEE","11 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Batch reinforcement learning based dynamic optimization for polyethylene grade transitions","Y. Wei; Z. Xiong; Y. Jiang; D. Huang","Department of Automation, Tsinghua University, Beijing, Beijing, CN; Department of Automation, Tsinghua University, Beijing, Beijing, CN; Department of Automation, Tsinghua University, Beijing, Beijing, CN; Department of Automation, Tsinghua University, Beijing, Beijing, CN","Proceedings of the 33rd Chinese Control Conference","15 Sep 2014","2014","","","7612","7616","It is necessary for polyethylene grade transitions to establish dynamic optimization in order to preserve competitiveness in the global polymer market. Although the typical technology of iterative learning control has been performed to track the reference trajectories, it is usually difficult to obtain the optimal reference trajectories. A transition from one specific grade to another specific grade can be considered as one batch, therefore, we propose a feasible scheme for dynamic optimization of polyethylene grade transitions based on the batch reinforcement learning, which can derive a best possible policy after a few batches. The scheme aims to integrate the offline reference optimization and online implementations, and explore a better control policy instead of tracking the predetermined trajectories. This designed scheme has three distinct phases, collecting observations from reference and the historical trajectories, learning a policy, and executions. The proposed method is verified by the simulated polyethylene grade transitions, and good performance has been obtained.","1934-1768","978-9-8815-6387-3","10.1109/ChiCC.2014.6896268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6896268","Dynamic optimization;batch reinforcement learning;observations;polyethylene grade transitions","Trajectory;Optimization;Approximation methods;Approximation algorithms;Decision trees;Polyethylene","batch processing (industrial);dynamic programming;learning (artificial intelligence);polymers;production engineering computing","batch reinforcement learning;dynamic optimization;polyethylene grade transitions;polymer market;iterative learning control;reference trajectories;control policy","","","","10","","15 Sep 2014","","","IEEE","IEEE Conferences"
"Impedance Control in Uncertain Environment using Reinforcement Learning","W. Wei; H. Peng; J. Pang; Q. Yu","CYBER Technology in Automation, Control, and Intelligent Systems, Suzhou, China; CYBER Technology in Automation, Control, and Intelligent Systems, Suzhou, China; CYBER Technology in Automation, Control, and Intelligent Systems, Suzhou, China; CYBER Technology in Automation, Control, and Intelligent Systems, Suzhou, China","2019 IEEE 9th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","16 Apr 2020","2019","","","1203","1208","Impedance control is a very classic control method in robot and human interaction. Humans use their ability to adaptively adjust arm impedance parameters to perform well in a variety of tasks. This ability allows us to successfully complete interactive tasks even in uncertain disturbance environments. By analyzing the results of many force field experiments, We got a conclusion that humans usually use two methods to change their impedance in external perturbations: 1) In the situation of unpredictable perturbations, humans adapt their impedance through muscle contraction; 2) In the situation of predictable perturbations, humans adds a constant term to offset the known perturbations. In this paper, We show how 3-DOFs simulated robot use the reinforcement learning algorithm to perform similar behavior in both situations. We apply our model-free reinforcement learning algorithm PI2 (policy improvement with path integrals) to the robot to learn the end-effector variable impedance schedules and trajectories. Our findings provide a approach to automatically learn the impedance empirically without requiring to build a physical or environmental dynamics model.","2379-7711","978-1-7281-0770-7","10.1109/CYBER46603.2019.9066697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066697","","Robots;Impedance;Force;Trajectory;Task analysis;Perturbation methods;Adaptation models","end effectors;learning (artificial intelligence);learning systems;spatial variables control","impedance control;uncertain environment;classic control method;human interaction;arm impedance parameters;interactive tasks;uncertain disturbance environments;force field experiments;external perturbations;unpredictable perturbations;predictable perturbations;model-free reinforcement learning algorithm;end-effector variable impedance schedules;3-DOFs simulated robot;policy improvement with path integrals;PI2 algorithm;environmental dynamics model","","","","19","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
