@inproceedings{10.1145/3459955.3460601,
author = {Iiyama, Tomoshi and Kitakoshi, Daisuke and Suzuki, Masato},
title = {An Approach for Creation of Logistics Management System for Food Banks Based on Reinforcement Learning},
year = {2021},
isbn = {9781450389136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459955.3460601},
doi = {10.1145/3459955.3460601},
abstract = {Food loss has become a serious problem in recent years, especially in developed countries including Japan. Food welfare organizations called Food Banks which distribute still-edible food to needy people have been active in Japan. Here, we sought to develop a logistics management system for the food banks to improve the efficiency of their food delivery. We propose a method to optimize the food delivery schedule of food banks by means of a reinforcement learning algorithm. In our proposed algorithm, an agent aims to learn the policy for delivering food at the lowest cost. We performed computer simulations to evaluate the validity of the proposed method, and the results demonstrated that the agent could acquire the optimal policy in a small virtual environment in many cases.},
booktitle = {Proceedings of the 4th International Conference on Information Science and Systems},
pages = {60–67},
numpages = {8},
keywords = {Reinforcement learning, Food delivery schedule optimization, Food Bank},
location = {Edinburgh, United Kingdom},
series = {ICISS '21}
}

@inproceedings{10.5555/3463952.3464055,
author = {Malladi, Tejasvi and Murugappan, Karpagam and Sudarsanam, Depak and Suriyanarayanan, Ramasubramanian and Vasan, Arunchandar},
title = {To Hold or Not to Hold? - Reducing Passenger Missed Connections in Airlines Using Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Missed connections at transit airports are a source of both poor customer experience and reduced airline operational efficiency. Airlines typically handle missed connections by rebooking customers. Recently, airlines have started holding departing flights for some time in a rule-based manner to avoid missed connections. However, rule-based heuristics typically use information local to a flight and do not learn in a globally informed way across the entire network.We complement existing approaches by learning a policy for holding a flight to avoid misconnections, using reinforcement learning (RL). The state presented to the RL agent uses forecasted flight-specific context; and measured network-wide context. The reward uses components that trade off the decrease in on-time performance due to the hold decisions, for a decrease in missed connections. We attribute the global rewards to individual local hold actions through a novel delay tree that approximates the network interactions. Multiple flights are handled through the same instance of the agent handling them in sequence with varying state information.We evaluate our approach for two different airlines with training and testing over a microsimulator that uses real-world data for calibration. Across different algorithms (DQN, AC, A2C, DDPG), we find that the best performing RL-based agent is able to reduce significantly more (up to 50\%) missed connections for a minimal decrease (~5\%) in on-time performance; when compared with a current rule-based heuristic. Further, the approach is tunable and able to transfer learn across different airlines.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {862–870},
numpages = {9},
keywords = {missed connections, reinforcement learning, airline operations},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@article{10.5555/3586589.3586767,
author = {Basei, Matteo and Guo, Xin and Hu, Anran and Zhang, Yufei},
title = {Logarithmic Regret for Episodic Continuous-Time Linear-Quadratic Reinforcement Learning over a Finite-Time Horizon},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {We study finite-time horizon continuous-time linear-quadratic reinforcement learning problems in an episodic setting, where both the state and control coefficients are unknown to the controller. We first propose a least-squares algorithm based on continuous-time observations and controls, and establish a logarithmic regret bound of magnitude O((ln M)(ln ln M)), with M being the number of learning episodes. The analysis consists of two components: perturbation analysis, which exploits the regularity and robustness of the associated Riccati differential equation; and parameter estimation error, which relies on subexponential properties of continuous-time least-squares estimators. We further propose a practically implementable least-squares algorithm based on discrete-time observations and piecewise constant controls, which achieves similar logarithmic regret with an additional term depending explicitly on the time stepsizes used in the algorithm.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {178},
numpages = {34},
keywords = {continuous-time, episodic reinforcement learning, stochastic control, regret analysis, linear-quadratic}
}

@inproceedings{10.1145/3605801.3605828,
author = {Zhou, Zixiang and Li, Mingchu},
title = {Deep Reinforcement Learning Based Edge-Enabled Vehicle to Everything Service Placement for 5G Millimeter Wave},
year = {2023},
isbn = {9798400700620},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605801.3605828},
doi = {10.1145/3605801.3605828},
abstract = {Vehicle to Everything (V2X) communication and services are the foundation for realizing the future of Intelligent Transportation Systems (ITSs), as the potential number of users and device connections in the future is huge, in the billions, attracting many stakeholders. However, many such services have strict performance requirements, especially in terms of latency and bandwidth, and the introduction of the fifth-generation millimeter wave (5G mmWave) multi-access edge computing (MEC) can further reduce latency while increasing bandwidth. However, due to the characteristics of millimeter-wave, factors such as rain and obstacles can degrade system performance, and the available computing resources at edge nodes are limited, so it is worthwhile to study the problem of optimal V2X service placement on suitable computing nodes to meet service quality. To this end, this paper designs a model for the dynamic environment of 5G millimeter-wave communication for the problem of optimal V2X service placement (DVSP). Previous work has not considered the placement of V2X services in adverse conditions such as rain and obstacles when millimeter-wave communication is introduced. To address this problem, a deep reinforcement learning-based V2X service placement algorithm (DQN-VSPA) is developed. Simulation results show that the DVSP model successfully guarantees and maintains the QoS requirements of all different V2X services. In addition, the proposed DQN-VSPA algorithm has the lowest latency compared to the heuristic algorithm.},
booktitle = {Proceedings of the 2023 2nd International Conference on Networks, Communications and Information Technology},
pages = {138–142},
numpages = {5},
keywords = {multi-access edge computing (MEC), deep reinforcement learning, V2X service placement, 5G mmWave},
location = {Qinghai, China},
series = {CNCIT '23}
}

@inproceedings{10.1145/3579371.3589081,
author = {Kong, Xiangyu and Huang, Yi and Zhu, Jianfeng and Man, Xingchen and Liu, Yang and Feng, Chunyang and Gou, Pengfei and Tang, Minggui and Wei, Shaojun and Liu, Leibo},
title = {MapZero: Mapping for Coarse-Grained Reconfigurable Architectures with Reinforcement Learning and Monte-Carlo Tree Search},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589081},
doi = {10.1145/3579371.3589081},
abstract = {Coarse-grained reconfigurable architecture (CGRA) has become a promising candidate for data-intensive computing due to its flexibility and high energy efficiency. CGRA compilers map data flow graphs (DFGs) extracted from applications onto CGRAs, playing a fundamental role in fully exploiting hardware resources for acceleration. Yet the existing compilers are time-demanding and cannot guarantee optimal results due to the traversal search of enormous search spaces brought about by the spatio-temporal flexibility of CGRA structures and the complexity of DFGs. Inspired by the amazing progress in reinforcement learning (RL) and Monte-Carlo tree search (MCTS) for real-world problems, we consider constructing a compiler that can learn from past experiences and comprehensively understand the target DFG and CGRA.In this paper, we propose an architecture-aware compiler for CGRAs based on RL and MCTS, called MapZero - a framework to automatically extract the characteristics of DFG and CGRA hardware and map operations onto varied CGRA fabrics. We apply Graph Attention Network to generate an adaptive embedding for DFGs and also model the functionality and interconnection status of the CGRA, aiming at training an RL agent to perform placement and routing intelligently.Experimental results show that MapZero can generate superior-quality mappings and reduce compilation time hundreds of times compared to state-of-the-art methods. MapZero can find high-quality mappings very quickly when the feasible solution space is rather small and all other compilers fail. We also demonstrate the scalability and broad applicability of our framework.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {46},
numpages = {14},
keywords = {reinforcement learning, compiler, graph neural network, coarse-grained reconfigurable architecture},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3461353.3461382,
author = {Ge, JINGYI and WANG, YI and LI, JIAYI and BAI, HUIWEN and LIU, LINSHENG and WANG, SHENGFA and XUE, XINWEI and LI, FENGQI},
title = {A Reinforcement Learning-Based Path Planning Method for Complex Thin-Walled Structures in 3D Printing},
year = {2021},
isbn = {9781450388634},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461353.3461382},
doi = {10.1145/3461353.3461382},
abstract = {Path planning is an important part of the 3D printing process. The optimized path planning method can improve not only effect of the molding but also the efficiency of printing process. However, traditional path planning methods are not satisfactory in 3D printing, especially when printing the entities with complex thin-wall structures. We propose an intelligent path planning method named Q-Path, based on reinforcement learning for complex thin-walled structures. We first convert the path planning task to a full-path traversing problem. Then we use the Q-learning algorithm to find the optimal solution with the constraints of 3D printing, such as the minimum number of lifts and turns of the print head. Experimental results show that the proposed methods are superior to the traditional methods in printing complex thin-walled structures.},
booktitle = {Proceedings of the 2021 5th International Conference on Innovation in Artificial Intelligence},
pages = {234–240},
numpages = {7},
keywords = {reinforcement learning, path planning, 3D printing},
location = {Xia men, China},
series = {ICIAI '21}
}

@inproceedings{10.1145/3575813.3597343,
author = {Cao, Zhiwei and Wang, Ruihang and Zhou, Xin and Wen, Yonggang},
title = {Toward Model-Assisted Safe Reinforcement Learning for Data Center Cooling Control: A Lyapunov-Based Approach},
year = {2023},
isbn = {9798400700323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575813.3597343},
doi = {10.1145/3575813.3597343},
abstract = {This paper considers intelligent data center cooling control via the Deep Reinforcement Learning (DRL) approach to improve data center sustainability. Existing DRL-based controllers are trained with a simplified data hall thermodynamic model which assumes uniform room temperature distribution. This assumption is not valid for a real-world data center with highly nonuniform temperature distribution. Furthermore, most of them cannot guarantee thermal safety during the DRL learning process. To bridge these gaps, we propose LyaSafe, a model-assisted safe DRL approach for data center cooling control. To address the safety evaluation issue, we develop a coupled model that combines a differentiable surrogate data hall thermodynamics model with the energy model. It can simulate both data hall temperature distribution and the facility energy consumption. To address safe learning, we introduce a novel constrained Markov Decision Process (CMDP) formulation for data center cooling control by considering the Rack Cooling Index (RCI), the best-practice metric for evaluating compliance with ASHRAE data center thermal guidelines. The objective is to minimize data center carbon footprints while regulating the RCI within a threshold. We first derive the safety set based on the concept of the virtual queue and Lyapunov stability theory. Next, we rectify unsafe actions from the DRL agent by projecting them to the safety set. We evaluate LyaSafe in a data center hosting 20 racks and 299 servers. Evaluation results show that LyaSafe can ensure strict safety during the DRL learning while achieving up to 50 metric tons of annual carbon emission savings using Singapore’s statistics. Moreover, we conduct root cause analysis for the savings, revealing the importance of joint control of the data hall and the chiller plant.},
booktitle = {Proceedings of the 14th ACM International Conference on Future Energy Systems},
pages = {333–346},
numpages = {14},
keywords = {cooling control, Lyapunov stability theory, Data center, safe reinforcement learning},
location = {Orlando, FL, USA},
series = {e-Energy '23}
}

@inproceedings{10.1145/3418094.3418137,
author = {Jiang, Hongwei and Wang, William Yu Chung and Goh, Tiong-Thye and Zhu, Jie},
title = {Enhancing Decision Making with Deep Reinforcement Learning in a Context of Novel Coronavirus Outbreak: An Example in Emergency Department},
year = {2020},
isbn = {9781450377768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3418094.3418137},
doi = {10.1145/3418094.3418137},
abstract = {Physicians in hospitals are expected to improve treatment outcome and reduce health care costs. Information systems have been widely adopted in hospitals but not been properly integrated to provide information for decision support. The objective of this research is trying to validate the feasibility of enhancing hospital resource planning system in decision support by utilizing data stored in multiple systems in the hospital with a deep reinforcement learning approach to assist medical practitioner making a more accurate and efficient decision. Following the Design Science Research Method, this research is going to build an artefact to utilize data from electronic health record (EHR) and hospital resource planning (HRP) to provide medical decision support in the emergency department setting.},
booktitle = {Proceedings of the 4th International Conference on Medical and Health Informatics},
pages = {113–117},
numpages = {5},
keywords = {Emergency Department, Hospital Information System, Deep Reinforcement Learning, Electronic Health Record, Decision Making, Hospital Resource Planning},
location = {Kamakura City, Japan},
series = {ICMHI '20}
}

@inproceedings{10.1145/1921249.1921254,
author = {Feng, Chaochao and Lu, Zhonghai and Jantsch, Axel and Li, Jinwen and Zhang, Minxuan},
title = {A Reconfigurable Fault-Tolerant Deflection Routing Algorithm Based on Reinforcement Learning for Network-on-Chip},
year = {2010},
isbn = {9781450303972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1921249.1921254},
doi = {10.1145/1921249.1921254},
abstract = {We propose a reconfigurable fault-tolerant deflection routing algorithm (FTDR) based on reinforcement learning for NoC. The algorithm reconfigures the routing table through a kind of reinforcement learning---Q-learning using 2-hop fault information. It is topology-agnostic and insensitive to the shape of the fault region. In order to reduce the routing table size, we also propose a hierarchical Q-learning based deflection routing algorithm (FTDR-H) with area reduction up to 27\% for a switch in an 8 x 8 mesh compared to the original FTDR. Experimental results show that in the presence of faults, FTDR and FTDR-H are better than other fault-tolerant deflection routing algorithms and a turn model based fault-tolerant routing algorithm.},
booktitle = {Proceedings of the Third International Workshop on Network on Chip Architectures},
pages = {11–16},
numpages = {6},
keywords = {fault-tolerant, deflection routing, NoC, reinforcement learning},
location = {Atlanta, Georgia, USA},
series = {NoCArc '10}
}

@inproceedings{10.1145/3605573.3605612,
author = {Zhang, Jingrun and Yu, Guangba and He, Zilong and Ai, Liang and Chen, Pengfei},
title = {DeepPower: Deep Reinforcement Learning Based Power Management for Latency Critical Applications in Multi-Core Systems},
year = {2023},
isbn = {9798400708435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605573.3605612},
doi = {10.1145/3605573.3605612},
abstract = {Latency-critical (LC) applications are widely deployed in modern datacenters. Effective power management for LC applications can yield significant cost savings. However, it poses a significant challenge in maintaining the desired Service Level Aggrement (SLA) levels. Prior researches have mainly emphasized predicting the service time of request and utilize heuristic algorithms for CPU frequency adjustment. Unfortunately, the control granularity is limited to the request level and manual feature selection is needed. This paper proposes DeepPower, a deep reinforcement learning (DRL) based power management solution for LC applications. DeepPower comprises two key components, a DRL agent for monitoring the system load changes and a thread controller for CPU frequency adjustment. Considering the high overhead of the neural network and the short service time of requests, it is infeasible to employ DRL for direct adjustment of CPU frequency at the request level. Instead, DeepPower proposes a hierarchical control mechanism. That means the DRL agent adjusts the parameter of thread controller with longer intervals, and thread controller adjusts the CPU frequency with shorter intervals. This control mechanism enables DeepPower to adapt to dynamic workloads and achieves fine-grained frequency adjustments. We evaluate DeepPower with some common LC applications under dynamic workload. The experimental results show that DeepPower saves up to 28.4\% power compared with state-of-the-art methods and reduces the percentage of request timeout.},
booktitle = {Proceedings of the 52nd International Conference on Parallel Processing},
pages = {327–336},
numpages = {10},
keywords = {power management, hierarchical control, latency-critical system, deep reinforcement learning},
location = {Salt Lake City, UT, USA},
series = {ICPP '23}
}

@inproceedings{10.1145/3461598.3461608,
author = {Paul, Ananya and Mitra, Sulata},
title = {Management of Traffic Signals Using Deep Reinforcement Learning in Bidirectional Recurrent Neural Network in ITS},
year = {2021},
isbn = {9781450389679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461598.3461608},
doi = {10.1145/3461598.3461608},
abstract = {The traffic flow management is primarily done through traffic signals, whose inefficient control causes numerous problems, such as long waiting time and huge waste of energy. To improve traffic flow efficiency, obtaining real-time traffic information as an input and dynamically adjusting the traffic signal duration accordingly is essential. Among the existing methods, Deep Reinforcement Learning (DRL) has shown to be the most effective solution. In this paper, a dynamic mechanism to control the traffic signal of a large scale road network is proposed using policy gradient method. A single agent is trained with spatio–temporal data of the multiple intersections of the network to alleviate congestion. The proposed system is implemented in two different types of deep bidirectional Recurrent Neural Network (RNN) - Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU). The simulation experiments demonstrate that the proposed system could reduce traffic congestion in terms of different simulation metrics during high density traffic flows.},
booktitle = {Proceedings of the 2021 5th International Conference on Intelligent Systems, Metaheuristics \&amp; Swarm Intelligence},
pages = {60–64},
numpages = {5},
keywords = {Traffic signal control, GRU, DRL, LSTM, Policy gradient},
location = {Victoria, Seychelles},
series = {ISMSI '21}
}

@inproceedings{10.1145/3242969.3242976,
author = {Weber, Klaus and Ritschel, Hannes and Aslan, Ilhan and Lingenfelser, Florian and Andr\'{e}, Elisabeth},
title = {How to Shape the Humor of a Robot - Social Behavior Adaptation Based on Reinforcement Learning},
year = {2018},
isbn = {9781450356923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242969.3242976},
doi = {10.1145/3242969.3242976},
abstract = {A shared sense of humor can result in positive feelings associated with amusement, laughter, and moments of bonding. If robotic companions could acquire their human counterparts' sense of humor in an unobtrusive manner, they could improve their skills of engagement. In order to explore this assumption, we have developed a dynamic user modeling approach based on Reinforcement Learning, which allows a robot to analyze a person's reaction while it tells jokes and continuously adapts its sense of humor. We evaluated our approach in a test scenario with a Reeti robot acting as an entertainer and telling different types of jokes. The exemplary adaptation process is accomplished only by using the audience's vocal laughs and visual smiles, but no other form of explicit feedback. We report on results of a user study with 24 participants, comparing our approach to a baseline condition (with a non-learning version of the robot) and conclude by providing limitations and implications of our approach in detail.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimodal Interaction},
pages = {154–162},
numpages = {9},
keywords = {human-robot-interaction, social adaptation, socially-aware agents},
location = {Boulder, CO, USA},
series = {ICMI '18}
}

@inproceedings{10.1145/3132340.3132355,
author = {He, Ying and Yu, F. Richard and Zhao, Nan and Yin, Hongxi and Boukerche, Azzedine},
title = {Deep Reinforcement Learning (DRL)-Based Resource Management in Software-Defined and Virtualized Vehicular Ad Hoc Networks},
year = {2017},
isbn = {9781450351645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132340.3132355},
doi = {10.1145/3132340.3132355},
abstract = {Vehicular ad hoc networks (VANETs) have attracted great interests from both industry and academia. The developments of VANETs are heavily influenced by information and communications technologies, which have fueled a plethora of innovations in various areas, including networking, caching and computing. Nevertheless, these important enabling technologies have traditionally been studied separately in the existing works on vehicular networks. In this paper, we propose an integrated framework that can enable dynamic orchestration of networking, caching and computing resources to improve the performance of next generation vehicular networks. We formulate the resource allocation strategy in this framework as a joint optimization problem, where the gains of not only networking but also caching and computing are taken into consideration in the proposed framework. The complexity of the system is very high when we jointly consider these three technologies. Therefore, we propose a novel deep reinforcement learning approach in this paper. Simulation results with different system parameters are presented to show the effectiveness of the proposed scheme.},
booktitle = {Proceedings of the 6th ACM Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications},
pages = {47–54},
numpages = {8},
keywords = {vehicular ad hoc networks, trust management, security, software-defined networking},
location = {Miami, Florida, USA},
series = {DIVANet '17}
}

@inproceedings{10.1145/3528416.3530239,
author = {Lin, Na and Qin, Hongzhi and Shi, Junling and Zhao, Liang},
title = {Deep Reinforcement Learning Empowered Multiple UAVs-Assisted Caching and Offloading Optimization in D2D Wireless Networks},
year = {2022},
isbn = {9781450393386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528416.3530239},
doi = {10.1145/3528416.3530239},
abstract = {Device-to-device (D2D) content caching is a promising technology to mitigate the backhaul pressure, and reduce the contents transmission delay. In this paper, to improve the content hit rate (CHR) and the utilization efficiency of the limited caching capacity, we put forward a caching content placement strategy by predicting the user preference and the content popularity, where unmanned aerial vehicles (UAVs) are introduced into the D2D networks to provide computation offloading services to the users. A <u>d</u>ynamic <u>r</u>esource <u>a</u>llocation <u>o</u>ptimization <u>a</u>lgorithm (DRAOA) is proposed to deploy UAVs and plan UAVs trajectory adaptively according to the users' task requirements. Simulation results show that the proposed caching content placement policy outperforms the existing baselines. Additionally, the DRAOA can effectively improve the network capacity and mitigate the computation delay compared to the other two DRL algorithms.},
booktitle = {Proceedings of the 19th ACM International Conference on Computing Frontiers},
pages = {150–158},
numpages = {9},
keywords = {unmanned aerial vehicles (UAVs), deep reinforcement learning, task offloading, D2D caching networks},
location = {Turin, Italy},
series = {CF '22}
}

@inproceedings{10.1145/3583131.3590446,
author = {An, Guangyan and Wu, Ziyu and Shen, Zhilong and Shang, Ke and Ishibuchi, Hisao},
title = {Evolutionary Multi-Objective Deep Reinforcement Learning for Autonomous UAV Navigation in Large-Scale Complex Environments},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590446},
doi = {10.1145/3583131.3590446},
abstract = {Autonomous navigation of Unmanned Aerial Vehicles (UAVs) in large-scale complex environments presents a significant challenge in modern aerospace engineering, as it requires effective decision-making in an environment with limited sensing capacity, dynamic changes, and dense obstacles. Reinforcement Learning (RL) has been applied in sequential control problems, but the manual setting of hyperparameters, including reward functions, often results in suboptimal solutions and inadequate training. To address these limitations, we propose a framework that combines Multi-Objective Evolutionary Algorithms (MOEAs) with RL algorithms. The proposed framework generates a set of non-dominating parameters for the reward function using MOEAs, leading to diverse decision-making preferences, efficient convergence, and improved performance. The framework was tested on the autonomous navigation of UAVs and demonstrated significant improvement compared to traditional RL methods. This work offers a novel perspective on the problem of autonomous UAV navigation in large-scale complex environments and highlights the potential for further improvement through the integration of RL and MOEAs.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {633–641},
numpages = {9},
location = {Lisbon, Portugal},
series = {GECCO '23}
}

@article{10.1145/3589973,
author = {Sam, Tyler and Chen, Yudong and Yu, Christina Lee},
title = {Overcoming the Long Horizon Barrier for Sample-Efficient Reinforcement Learning with Latent Low-Rank Structure},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/3589973},
doi = {10.1145/3589973},
abstract = {The practicality of reinforcement learning algorithms has been limited due to poor scaling with respect to the problem size, as the sample complexity of learning an ε-optimal policy is Ω(|S||A|H/ ε2) over worst case instances of an MDP with state space S, action space A, and horizon H. We consider a class of MDPs for which the associated optimal Q* function is low rank, where the latent features are unknown. While one would hope to achieve linear sample complexity in |S| and |A| due to the low rank structure, we show that without imposing further assumptions beyond low rank of Q*, if one is constrained to estimate the Q function using only observations from a subset of entries, there is a worst case instance in which one must incur a sample complexity exponential in the horizon H to learn a near optimal policy. We subsequently show that under stronger low rank structural assumptions, given access to a generative model, Low Rank Monte Carlo Policy Iteration (LR-MCPI) and Low Rank Empirical Value Iteration (LR-EVI) achieve the desired sample complexity of \~{O}((|S|+|A|)poly (d,H)/ε2) for a rank d setting, which is minimax optimal with respect to the scaling of |S|, |A|, and ε. In contrast to literature on linear and low-rank MDPs, we do not require a known feature mapping, our algorithm is computationally simple, and our results hold for long time horizons. Our results provide insights on the minimal low-rank structural assumptions required on the MDP with respect to the transition kernel versus the optimal action-value function.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {may},
articleno = {29},
numpages = {60},
keywords = {low-rank matrix estimation, reinforcement learning}
}

@inproceedings{10.1145/3461353.3461373,
author = {Gong, Peng and Shi, Dianxi and Xue, Chao and Chen, Xucan},
title = {A Domain Data Pattern Randomization Based Deep Reinforcement Learning Method for Sim-to-Real Transfer},
year = {2021},
isbn = {9781450388634},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461353.3461373},
doi = {10.1145/3461353.3461373},
abstract = {Transferring reinforcement learning policies trained in a physical simulator to the real world is a highly challenging problem, because the gap between the simulation and reality, usually causes the transferred model to perform poorly in the real world. Many algorithms including domain randomization, have been proposed to try to bridge the gap between simulation and reality. However, most of them are to change the value of the corresponding data by superimposing gaussian noise on robot dynamics parameters or environmental data. Such policies often fail to solve the problem of long-term/intermittent missing data patterns caused by sensor failures in the actual operation of the robot. Faced with this problem, we proposed a memory-enhanced domain data pattern randomization method. This method achieves data enhancement by randomizing the distribution pattern of data connection, at the same time, the memory mechanism based on recurrent neural network is introduced into the decision model, to alleviate the jitter of environmental distribution caused by data pattern changes, so as to improve the decision-making ability of the robot in some observable scenes triggered by the change of data pattern.},
booktitle = {Proceedings of the 2021 5th International Conference on Innovation in Artificial Intelligence},
pages = {1–7},
numpages = {7},
keywords = {Deep Reinforcement Learning, Sim to Real, Domain Randomization, Data Pattern, Recurrent Neural Network},
location = {Xia men, China},
series = {ICIAI '21}
}

@inproceedings{10.1145/3417188.3417194,
author = {Gonzalez-Garcia, Alejandro and Barragan-Alcantar, David and Collado-Gonzalez, Ivana and Garrido, Leonardo},
title = {Control of an Unmanned Surface Vehicle Based on Adaptive Dynamic Programming and Deep Reinforcement Learning},
year = {2020},
isbn = {9781450375481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417188.3417194},
doi = {10.1145/3417188.3417194},
abstract = {This paper presents a low-level controller for an unmanned surface vehicle based on Adaptive Dynamic Programming (ADP) and deep reinforcement learning (DRL). The model-based algorithm Back-propagation Through Time and a simulation of the mathematical model of the vessel are implemented to train a deep neural network to drive the surge speed and yaw dynamics. The controller presents successful simulation results validating the feasibility of the proposed strategy and contributes to the diversity of validated applications of ADP and DRL control strategies.},
booktitle = {Proceedings of the 2020 4th International Conference on Deep Learning Technologies},
pages = {118–122},
numpages = {5},
keywords = {back-propagation through time, marine robotics, adaptive dynamic programming, higher education, deep reinforcement learning, unmanned surface vehicle},
location = {Beijing, China},
series = {ICDLT '20}
}

@inproceedings{10.1145/3583788.3583802,
author = {Wei, Xueyu and Xue, Wei and Zhao, Wei and Shen, Yuanxia and Yu, Gaohang},
title = {Joint Action Representation and Prioritized Experience Replay for Reinforcement Learning in Large Discrete Action Spaces},
year = {2023},
isbn = {9781450398633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583788.3583802},
doi = {10.1145/3583788.3583802},
abstract = {In dealing with the large discrete action spaces, a joint action representation and prioritized experience replay method is proposed in this paper, which consists of three modules. In the first module, we use the k-nearest neighbor method to reduce the dimensionality of the original action space, generating a compact action space, and then the critic network is introduced to further evaluate and filter this compact space to obtain the optimal action. Note that the optimal action may have inconsistency with the actual desired action. Then in the second module, we introduce a multi-step update technique to reduce the training variance when storing data in the replay buffer. In the third module, considering the existence of correlation between samples when sampling data, we assign the corresponding weight to the sample experience by calculating the absolute value of temporal difference error and use such a non-uniform sampling method to prioritize the samples for sampling. Experimental results on four benchmark environments demonstrate the effectiveness and efficiency of the proposed method in dealing with the large discrete action spaces.},
booktitle = {Proceedings of the 2023 7th International Conference on Machine Learning and Soft Computing},
pages = {96–101},
numpages = {6},
keywords = {Large discrete action spaces, Prioritized experience replay., Reinforcement learning, Action representation},
location = {Chongqing, China},
series = {ICMLSC '23}
}

@inproceedings{10.5555/3398761.3398825,
author = {Hostallero, David Earl and Kim, Daewoo and Moon, Sangwoo and Son, Kyunghwan and Kang, Wan Ju and Yi, Yung},
title = {Inducing Cooperation through Reward Reshaping Based on Peer Evaluations in Deep Multi-Agent Reinforcement Learning},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We propose a deep reinforcement learning algorithm for semi-cooperative multi-agent tasks, where agents are equipped with their separate reward functions, yet with some willingness to cooperate. It is intuitive that defining and directly maximizing a global reward function leads to cooperation because there is no concept of selfishness among agents. However, it may not be the best way of inducing such cooperation due to problems that arise from training multiple agents with a single reward (e.g., credit assignment). In addition, agents may intentionally be given separate reward functions to induce task prioritization whereas a global reward function may be difficult to define without diluting the effect of different tasks and causing their reward factors to be disregarded. Our algorithm, called Peer Evaluation-based Dual DQN (PED-DQN), proposes to give peer evaluation signals to observed agents, which quantify how they strategically value a certain transition. This exchange of peer evaluation among agents over time turns out to render agents to gradually reshape their reward functions so that their action choices from the myopic best response tend to result in a more cooperative joint action.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {520–528},
numpages = {9},
keywords = {multi-agent systems, multi-agent reinforcement learning, reinforcement learning, machine learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3557915.3561018,
author = {Muthugama, Lakmal and Xie, Hairuo and Tanin, Egemen and Karunasekera, Shanika and Gunarathna, Udesh},
title = {Concurrent Optimization of Safety and Traffic Flow Using Deep Reinforcement Learning for Autonomous Intersection Management},
year = {2022},
isbn = {9781450395298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3557915.3561018},
doi = {10.1145/3557915.3561018},
abstract = {With increasing connectivity and autonomy in traffic eco-systems, Autonomous Intersection Management (AIM) has attracted strong attention from the research community. AIM helps optimize traffic by coordinating the trajectory of connected vehicles around intersections. Most of the existing AIM solutions are developed for single-objective optimization problems that are focused on improving traffic flow. A complete AIM solution needs to perform bi-objective optimization that considers both traffic flow and safety. However, the computational complexity for achieving both objectives is significantly high with the existing solutions, especially when traffic demand is stochastic. We address the limitations of the existing solutions using deep reinforcement learning (deep RL) that helps solve complex problems efficiently. Our solution uses two types of RL agents. The first type is intersection-level agents, which generate theoretically sound trajectory plans for individual vehicles approaching intersections. The second type is vehicle-level agents that control vehicles' actual trajectories around the intersections based on the plans. Both agents incorporate traffic flow and safety constraints into their decision making. Our experimental results show that our solution achieves a high safety level with a minimum impact on travel time.},
booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
articleno = {82},
numpages = {12},
keywords = {spatio-temporal data management, autonomous intersection management, deep reinforcement learning, multi-objective optimization},
location = {Seattle, Washington},
series = {SIGSPATIAL '22}
}

@inproceedings{10.1145/3544793.3560410,
author = {Azam, Mohib and Chen, Xinlei and Xu, Susu},
title = {Incentivizing Mobility of Multi-Agent Vehicle Swarms with Deep Reinforcement Learning for Sensing Coverage Optimization},
year = {2023},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544793.3560410},
doi = {10.1145/3544793.3560410},
abstract = {Mobile Crowdsensing Systems (MCS) often utilize non-dedicated mobile agents (e.g., ridesharing cars) equipped with sensors to collect urban data as they move around a city. Utilizing these non-dedicated agents provides a cheap means for mobile crowdsensing. However, driver agents often focus on traveling to areas where they can pick up more passengers, which can cause the distribution of uploaded data to mismatch our intended distribution. In this work, we propose a passive incentivization scheme in which incentives are applied to different map locations for agents who travel to that location at that time and upload data. We design a simulated environment for multi-agent mobile crowdsensing systems based on real-world taxi data and model large amount of agents with heterogeneous utility functions and routing behaviors, namely myopic, semi-myopic, and farsighted. With this environment, we propose using reinforcement learning to search for an ideal incentivization policy. Finally, we show through experiments that a Proximal Policy Optimization-based method is capable of learning such a policy in spite of the complexities and high dimensionality of the environment and action spaces.},
booktitle = {Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
pages = {397–402},
numpages = {6},
keywords = {Reinforcement Learning, Sensing Quality Optimization, Combinatorial Optimization, Mobile Crowd Sensing},
location = {Cambridge, United Kingdom},
series = {UbiComp/ISWC '22 Adjunct}
}

@inproceedings{10.5555/3398761.3398968,
author = {Alshehri, Mona and Reyes, Napoleon and Barczak, Andre},
title = {Evolving Meta-Level Reasoning with Reinforcement Learning and A* for Coordinated Multi-Agent Path-Planning},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This work presents an extension to a graph-based evolutionary algorithm, called Genetic Network Programming with Reinforcement Learning (GNP-RL) to make it more amenable for solving coordinated multi-agent path-planning tasks in dynamic environments. We improve the algorithm's ability to evolve meta-level reasoning strategies in three aspects: genetic composition, search and learning strategies, using optimal search algorithm, constraint conformance and task prioritization techniques.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1744–1746},
numpages = {3},
keywords = {reinforcement learning, optimal search, multi-agent learning, learning agent capabilities, evolutionary algorithms},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3576842.3582325,
author = {Taherisadr, Mojtaba and Stavroulakis, Stelios Andrew and Elmalaki, Salma},
title = {AdaPARL: Adaptive Privacy-Aware Reinforcement Learning for Sequential Decision Making Human-in-the-Loop Systems},
year = {2023},
isbn = {9798400700378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576842.3582325},
doi = {10.1145/3576842.3582325},
abstract = {Reinforcement learning (RL) presents numerous benefits compared to rule-based approaches in various applications. Privacy concerns have grown with the widespread use of RL trained with privacy-sensitive data in IoT devices, especially for human-in-the-loop systems. On the one hand, RL methods enhance the user experience by trying to adapt to the highly dynamic nature of humans. On the other hand, trained policies can leak the user’s private information. Recent attention has been drawn to designing privacy-aware RL algorithms while maintaining an acceptable system utility. A central challenge in designing privacy-aware RL, especially for human-in-the-loop systems, is that humans have intrinsic variability, and their preferences and behavior evolve. The effect of one privacy leak mitigation can differ for the same human or across different humans over time. Hence, we can not design one fixed model for privacy-aware RL that fits all. To that end, we propose adaPARL, an adaptive approach for privacy-aware RL, especially for human-in-the-loop IoT systems. adaPARL provides a personalized privacy-utility trade-off depending on human behavior and preference. We validate the proposed adaPARL on two IoT applications, namely (i) Human-in-the-Loop Smart Home and (ii) Human-in-the-Loop Virtual Reality (VR) Smart Classroom. Results obtained on these two applications validate the generality of adaPARL and its ability to provide a personalized privacy-utility trade-off. On average, adaPARL improves the utility by while reducing the privacy leak by on average.},
booktitle = {Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation},
pages = {262–274},
numpages = {13},
keywords = {Reinforcement Learning, IoT, Human-in-the-Loop, Privacy},
location = {San Antonio, TX, USA},
series = {IoTDI '23}
}

@inproceedings{10.1145/3205651.3208231,
author = {Antipov, Denis and Buzdalova, Arina and Stankevich, Andrew},
title = {Runtime Analysis of a Population-Based Evolutionary Algorithm with Auxiliary Objectives Selected by Reinforcement Learning},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208231},
doi = {10.1145/3205651.3208231},
abstract = {We propose the method of selection of auxiliary objectives (2 + 2λ)-EA+RL which is the population-based modification of the EA+RL method. We analyse the efficiency of this method on the problem XdivK that is considered to be a hard problem for random search heuristics due to multiple plateaus. We prove that in the case of presence of a helping auxiliary objective this method can find the optimum in 0(n2) fitness evaluations in expectation, while the initial EA+RL, which is not population-based, yields at least Ω (nk−1) fitness evaluations, where k is the plateau size. We also prove that in the case of presence of an obstructive auxiliary objective the expected runtime increases only by a constant factor.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1886–1889},
numpages = {4},
keywords = {multiobjectivization, runtime analysis, reinforcement learning},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@inproceedings{10.5555/3581644.3581672,
author = {Hara, Takanori and Sasabe, Masahiro},
title = {Deep Reinforcement Learning with Graph Neural Networks for Capacitated Shortest Path Tour Based Service Chaining},
year = {2023},
isbn = {9783903176515},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {Network functions virtualization (NFV) realizes diverse and flexible network services by executing network functions on generic hardware as virtual network functions (VNFs). A certain network service is regarded as a sequence of VNFs, called service chain. The service chaining (SC) problem aims at finding an appropriate service path from an origin node to a destination node while executing the VNFs at the intermediate nodes in the required order under resource constraints on nodes and links. The SC problem belongs to the complexity class NP-hard. In our previous work, we modeled the SC problem as an integer linear program (ILP) based on the capacitated shortest path tour problem (CSPTP) where the CSPTP is an extended version of the SPTP with the node and link capacity constraints. We also developed the Lagrangian heuristics to achieve the balance between optimality and computational complexity. In this paper, we further propose a deep reinforcement learning (DRL) framework with the graph neural network (GNN) to realize the CSPTP-based SC adaptive to changes in service demand and/or network topology. Numerical results show that (1) the proposed framework achieves almost the same optimality as the ILP for the CSPTP-based SC and (2) it also works well without retraining even when the service demand changes or the network is partly damaged.},
booktitle = {Proceedings of the 18th International Conference on Network and Service Management},
articleno = {23},
numpages = {9},
keywords = {capacitated shortest path tour problem (CSPTP), deep reinforcement learning (DRL), network functions virtualization (NFV), graph neural networks (GNNs), service chaining},
location = {Thessaloniki, Greece},
series = {CNSM '22}
}

@inproceedings{10.1145/1390156.1390251,
author = {Parr, Ronald and Li, Lihong and Taylor, Gavin and Painter-Wakefield, Christopher and Littman, Michael L.},
title = {An Analysis of Linear Models, Linear Value-Function Approximation, and Feature Selection for Reinforcement Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390251},
doi = {10.1145/1390156.1390251},
abstract = {We show that linear value-function approximation is equivalent to a form of linear model approximation. We then derive a relationship between the model-approximation error and the Bellman error, and show how this relationship can guide feature selection for model improvement and/or value-function improvement. We also show how these results give insight into the behavior of existing feature-selection algorithms.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {752–759},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.5555/3463952.3464246,
author = {Haeri, Hossein},
title = {Reward-Sharing Relational Networks in Multi-Agent Reinforcement Learning as a Framework for Emergent Behavior},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Most prior works on MARL seek to implement intra-agent complex interactions by explicitly communicating agent actions. However, there have only been a few efforts that examine emergence as arising from complex 'social' interactions or relations based on individual objectives and reward functions. This study is about integrating a user-defined relational network into the MARL setup and evaluating the effects of agent-agent relations on the generation of emergent behaviors. Specifically, we propose a framework that uses the notion of Reward-Sharing Relational Networks (RSRN) to determine the relationship between agents where edge weights determine how much one agent is invested in the success of (or 'cares about') another. The preliminary results indicate that reward-sharing relational networks can effectively influence the learned behaviors towards the imposed relational network.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1808–1810},
numpages = {3},
keywords = {reinforcement learning, social simulation, multi-agent systems},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.5555/3398761.3398819,
author = {Goecks, Vinicius G. and Gremillion, Gregory M. and Lawhern, Vernon J. and Valasek, John and Waytowich, Nicholas R.},
title = {Integrating Behavior Cloning and Reinforcement Learning for Improved Performance in Dense and Sparse Reward Environments},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper investigates how to efficiently transition and update policies, trained initially with demonstrations, using off-policy actor-critic reinforcement learning. It is well-known that techniques based on Learning from Demonstrations, for example behavior cloning, can lead to proficient policies given limited data. However, it is currently unclear how to efficiently update that policy using reinforcement learning as these approaches are inherently optimizing different objective functions. Previous works have used loss functions, which combine behavior cloning losses with reinforcement learning losses to enable this update. However, the components of these loss functions are often set anecdotally, and their individual contributions are not well understood. In this work, we propose the Cycle-of-Learning (CoL) framework that uses an actor-critic architecture with a loss function that combines behavior cloning and 1-step Q-learning losses with an off-policy pre-training step from human demonstrations. This enables transition from behavior cloning to reinforcement learning without performance degradation and improves reinforcement learning in terms of overall performance and training time. Additionally, we carefully study the composition of these combined losses and their impact on overall policy learning. We show that our approach outperforms state-of-the-art techniques for combining behavior cloning and reinforcement learning for both dense and sparse reward scenarios. Our results also suggest that directly including the behavior cloning loss on demonstration data helps to ensure stable learning and ground future policy updates.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {465–473},
numpages = {9},
keywords = {reinforcement learning, machine learning for robotics, human-robot/agent interaction, agent-based analysis of human interaction},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3503047.3503057,
author = {Zhao, Xin-ye and Yang, Mei and Peng, Cui and Wang, Chao},
title = {Research on Intelligent Operational Assisted Decision-Making of Naval Battlefield Based on Deep Reinforcement Learning},
year = {2022},
isbn = {9781450385862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503047.3503057},
doi = {10.1145/3503047.3503057},
booktitle = {Proceedings of the 3rd International Conference on Advanced Information Science and System},
articleno = {10},
numpages = {7},
keywords = {War Complex System, Deep Reinforcemant Learning, Multi-Schema Decision, Operational Decision Support, Operational Simulation},
location = {Sanya, China},
series = {AISS '21}
}

@inproceedings{10.1145/3449639.3459348,
author = {Ianta, Alexandru and Amaral, Ryan and Bayer, Caleidgh and Smith, Robert J. and Heywood, Malcolm I.},
title = {On the Impact of Tangled Program Graph Marking Schemes under the Atari Reinforcement Learning Benchmark},
year = {2021},
isbn = {9781450383509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449639.3459348},
doi = {10.1145/3449639.3459348},
abstract = {Tangled program graphs (TPG) support emergent modularity by first identifying subsets of programs that can usefully coexist (a team/ graph node) and then identifying the circumstance under which to reference other teams (arc adaptation). Variation operators manipulate the content of teams and arcs. This introduces cycles into the TPG structures. Previously, this effect was eradicated at run time by marking nodes while evaluating TPG individuals. In this work, a new marking heuristic is introduced, that of arc (learner) marking. This means that nodes can be revisited, but not the same arcs. We investigate the impact of this through 18 titles from the Arcade Learning Environment. The performance and complexity of the policies appear to be similar, but with specific tasks (game titles) resulting in preferences for one scheme over another.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {111–119},
numpages = {9},
keywords = {tangled program graphs, visual reinforcement learning},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{10.5555/3463952.3463970,
author = {Alegre, Lucas N. and Bazzan, Ana L. C. and da Silva, Bruno C.},
title = {Minimum-Delay Adaptation in Non-Stationary Reinforcement Learning via Online High-Confidence Change-Point Detection},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Non-stationary environments are challenging for reinforcement learning algorithms. If the state transition and/or reward functions change based on latent factors, the agent is effectively tasked with optimizing a behavior that maximizes performance over a possibly infinite random sequence of Markov Decision Processes (MDPs), each of which drawn from some unknown distribution. We call each such MDP acontext. Most related works make strong assumptions such as knowledge about the distribution over contexts, the existence of pre-training phases, ora priori knowledge about the number, sequence, or boundaries between contexts. We introduce an algorithm that efficiently learns policies in non-stationary environments. It analyzes a possibly infinite stream of data and computes, in real-time, high-confidence change-point detection statistics that reflect whether novel, specialized policies need to be created and deployed to tackle novel contexts, or whether previously-optimized ones might be reused. We show that(i) this algorithm minimizes the delay until unforeseen changes to a context are detected, thereby allowing for rapid responses; and(ii) it bounds the rate of false alarm, which is important in order to minimize regret. Our method constructs a mixture model composed of a (possibly infinite) ensemble of probabilistic dynamics predictors that model the different modes of the distribution over underlying latent MDPs. We evaluate our algorithm on high-dimensional continuous reinforcement learning problems and show that it outperforms state-of-the-art (model-free and model-based) RL algorithms, as well as state-of-the-art meta-learning methods specially designed to deal with non-stationarity.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {97–105},
numpages = {9},
keywords = {model-based RL, change-point detection, reinforcement learning, non-stationarity},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.5555/3535850.3535995,
author = {Vasco, Miguel and Yin, Hang and Melo, Francisco S. and Paiva, Ana},
title = {How to Sense the World: Leveraging Hierarchy in Multimodal Perception for Robust Reinforcement Learning Agents},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This work addresses the problem of sensing the world: how to learn a multimodal representation of a reinforcement learning agent's environment that allows the execution of tasks under incomplete perceptual conditions. To address such problem, we argue for hierarchy in the design of representation models and contribute with a novel multimodal representation model, MUSE. The proposed model learns a hierarchy of representations: low-level modality-specific representations, encoded from raw observation data, and a high-level multimodal representation, encoding joint-modality information to allow robust state estimation. We employ MUSE as the perceptual model of deep reinforcement learning agents provided with multimodal observations in Atari games. We perform a comparative study over different designs of reinforcement learning agents, showing that MUSE allows agents to perform tasks under incomplete perceptual experience with minimal performance loss. Finally, we also evaluate the generative performance of MUSE in literature-standard multimodal scenarios with higher number and more complex modalities, showing that it outperforms state-of-the-art multimodal variational autoencoders in single and cross-modality generation.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1301–1309},
numpages = {9},
keywords = {reinforcement learning, unsupervised learning, multimodal representation learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3563357.3566168,
author = {Kurte, Kuldeep and Amasyali, Kadir and Munk, Jeffrey and Zandi, Helia},
title = {Deep Reinforcement Learning with Online Data Augmentation to Improve Sample Efficiency for Intelligent HVAC Control},
year = {2022},
isbn = {9781450398909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563357.3566168},
doi = {10.1145/3563357.3566168},
abstract = {Deep Reinforcement Learning (DRL) has started showing success in real-world applications such as building energy optimization. Much of the research in this space utilized simulated environments to train RL-agent in an offline mode. Very few research have used DRL-based control in real-world systems due to two main reasons: 1) sample efficiency challenge---DRL approaches need to perform a lot of interactions with the environment to collect sufficient experiences to learn from, which is difficult in real systems, and 2) comfort or safety related constraints---user's comfort must never or at least rarely be violated. In this work, we propose a novel deep Reinforcement Learning framework with online Data Augmentation (RLDA) to address the sample efficiency challenge of real-world RL. We used a time series Generative Adversarial Network (TimeGAN) architecture as a data generator. We further evaluated the proposed RLDA framework using a case study of an intelligent HVAC control. With a ≈28\% improvement in the sample efficiency, RLDA framework lays the way towards increased adoption of DRL-based intelligent control in real-world building energy management systems.},
booktitle = {Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {479–483},
numpages = {5},
keywords = {deep reinforcement learning, intelligent HVAC control, building energy, demand response, data augmentation},
location = {Boston, Massachusetts},
series = {BuildSys '22}
}

@inproceedings{10.1145/3508297.3508318,
author = {Xue, Fei and Li, Hongqiang and Wang, Jili and Qiu, Gao and Liu, Junyong and Liu, Youbo and Liu, Tingjian and Wang, Tianxiang},
title = {A Deep Reinforcement Learning-Based Real-Time Control for Transfer Limits of Critical Inter-Corridors},
year = {2022},
isbn = {9781450385169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508297.3508318},
doi = {10.1145/3508297.3508318},
abstract = {Controlling transfer power flow below transfer limits of inter-corridors is crucial for power system security. Traditional way that empirically decides pessimistic limits incurs low utilization of inter-corridors. To improve operational flexibility, a real-time intelligent controller that enables precise tracking for transfer limits is carried out. The concerned problem is firstly modelled based on the limit qualification by total transfer capability (TTC). To allow real-time controller, which is reinforcement learning (RL), to learn control law from the model, a physics and data co-driven interactive environment is built, where computational intractable TTC-induced security criterion is substituted by pre-trained supervised learners. Numerical studies on the modified IEEE 39-bus system manifest the reliability and impressive efficiency of the proposed method on transfer limits control.},
booktitle = {2021 4th International Conference on Electronics and Electrical Engineering Technology},
pages = {122–127},
numpages = {6},
keywords = {Transient stability, Total transfer capability, Reinforcement learning, Electric power systems},
location = {Nanjing, China},
series = {EEET 2021}
}

@inproceedings{10.1145/3592473.3592564,
author = {Li, Yueheng and Zheng, Qianyuan and Zhang, Zicheng and Chen, Hao and Ma, Zhan},
title = {Improving ABR Performance for Short Video Streaming Using Multi-Agent Reinforcement Learning with Expert Guidance},
year = {2023},
isbn = {9798400701849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592473.3592564},
doi = {10.1145/3592473.3592564},
abstract = {In the realm of short video streaming, popular adaptive bitrate (ABR) algorithms developed for classical long video applications suffer from catastrophic failures because they are tuned to solely adapt bitrates. Instead, short video adaptive bitrate (SABR) algorithms have to properly determine which video at which bitrate level together for content prefetching, without sacrificing the users' quality of experience (QoE) and yielding noticeable bandwidth wastage jointly. Unfortunately, existing SABR methods are inevitably entangled with slow convergence and poor generalization. Thus, in this paper, we propose Incendio, a novel SABR framework that applies Multi-Agent Reinforcement Learning (MARL) with Expert Guidance to separate the decision of video ID and video bitrate in respective buffer management and bitrate adaptation agents to maximize the system-level utilized score modeled as a compound function of QoE and bandwidth wastage metrics. To train Incendio, it is first initialized by imitating the hand-crafted expert rules and then fine-tuned through the use of MARL. Results from extensive experiments indicate that Incendio outperforms the current state-of-the-art SABR algorithm with a 53.2\% improvement measured by the utility score while maintaining low training complexity and inference time.},
booktitle = {Proceedings of the 33rd Workshop on Network and Operating System Support for Digital Audio and Video},
pages = {58–64},
numpages = {7},
keywords = {reinforcement learning, adaptive bitrate, short video streaming},
location = {Vancouver, BC, Canada},
series = {NOSSDAV '23}
}

@inproceedings{10.1145/3319619.3321956,
author = {Jackson, Ethan C. and Daley, Mark},
title = {Novelty Search for Deep Reinforcement Learning Policy Network Weights by Action Sequence Edit Metric Distance},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3321956},
doi = {10.1145/3319619.3321956},
abstract = {Reinforcement learning (RL) problems often feature deceptive local optima, and methods that optimize purely for reward often fail to learn strategies for overcoming them [2]. Deep neuroevolution and novelty search have been proposed as effective alternatives to gradient-based methods for learning RL policies directly from pixels. We introduce and evaluate the use of novelty search over agent action sequences by Levenshtein distance as a means for promoting innovation. We also introduce a method for stagnation detection and population regeneration inspired by recent developments in the RL community [5], [1] that is derived from novelty search. Our methods extend a state-of-the-art method for deep neuroevolution using a simple genetic algorithm (GA) designed to efficiently learn deep RL policy network weights [6]. Results provide further evidence that GAs are competitive with gradient-based algorithms for deep RL in the Atari 2600 benchmark. Results also demonstrate that novelty search over agent action sequences can be effectively used as a secondary source of evolutionary selection pressure.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {173–174},
numpages = {2},
keywords = {novelty search, deep reinforcement learning, genetic algorithms},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@article{10.1109/TNET.2021.3053771,
author = {Wu, Qiong and Chen, Xu and Zhou, Zhi and Chen, Liang and Zhang, Junshan},
title = {Deep Reinforcement Learning With Spatio-Temporal Traffic Forecasting for Data-Driven Base Station Sleep Control},
year = {2021},
issue_date = {April 2021},
publisher = {IEEE Press},
volume = {29},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3053771},
doi = {10.1109/TNET.2021.3053771},
abstract = {To meet the ever increasing mobile traffic demand in 5G era, base stations (BSs) have been densely deployed in radio access networks (RANs) to increase the network coverage and capacity. However, as the high density of BSs is designed to accommodate peak traffic, it would consume an unnecessarily large amount of energy if BSs are on during off-peak time. To save the energy consumption of cellular networks, an effective way is to deactivate some idle base stations that do not serve any traffic demand. In this paper, we develop a traffic-aware dynamic BS sleep control framework, named DeepBSC, which presents a novel data-driven learning approach to determine the BS active/sleep modes while meeting lower energy consumption and satisfactory Quality of Service (QoS) requirements. Specifically, the traffic demands are predicted by the proposed GS-STN model, which leverages the geographical and semantic spatial-temporal correlations of mobile traffic. With accurate mobile traffic forecasting, the BS sleep control problem is cast as a Markov Decision Process that is solved by Actor-Critic reinforcement learning methods. To reduce the variance of cost estimation in the dynamic environment, we propose a benchmark transformation method that provides robust performance indicator for policy update. To expedite the training process, we adopt a Deep Deterministic Policy Gradient (DDPG) approach, together with an explorer network, which can strengthen the exploration further. Extensive experiments with a real-world dataset corroborate that our proposed framework significantly outperforms the existing methods.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {935–948},
numpages = {14}
}

@article{10.1613/jair.1.12270,
author = {Cao, Yongcan and Zhan, Huixin},
title = {Efficient Multi-Objective Reinforcement Learning via Multiple-Gradient Descent with Iteratively Discovered Weight-Vector Sets},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12270},
doi = {10.1613/jair.1.12270},
abstract = {Solving multi-objective optimization problems is important in various applications where users are interested in obtaining optimal policies subject to multiple (yet often conflicting) objectives. A typical approach to obtain the optimal policies is to first construct a loss function based on the scalarization of individual objectives and then derive optimal policies that minimize the scalarized loss function. Albeit simple and efficient, the typical approach provides no insights/mechanisms on the optimization of multiple objectives due to the lack of ability to quantify the inter-objective relationship. To address the issue, we propose to develop a new efficient gradient-based multi-objective reinforcement learning approach that seeks to iteratively uncover the quantitative inter-objective relationship via finding a minimum-norm point in the convex hull of the set of multiple policy gradients when the impact of one objective on others is unknown a priori. In particular, we first propose a new PAOLS algorithm that integrates pruning and approximate optimistic linear support algorithm to efficiently discover the weight-vector sets of multiple gradients that quantify the inter-objective relationship. Then we construct an actor and a multi-objective critic that can co-learn the policy and the multi-objective vector value function. Finally, the weight discovery process and the policy and vector value function learning process can be iteratively executed to yield stable weight-vector sets and policies. To validate the effectiveness of the proposed approach, we present a quantitative evaluation of the approach based on three case studies.},
journal = {J. Artif. Int. Res.},
month = {may},
pages = {319–349},
numpages = {31}
}

@inproceedings{10.1145/3427773.3427871,
author = {Chen, Bingqing and Jin, Ming and Wang, Zhe and Hong, Tianzhen and Berg\'{e}s, Mario},
title = {Towards Off-Policy Evaluation as a Prerequisite for Real-World Reinforcement Learning in Building Control},
year = {2020},
isbn = {9781450381932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427773.3427871},
doi = {10.1145/3427773.3427871},
abstract = {We present an initial study of off-policy evaluation (OPE), a problem prerequisite to real-world reinforcement learning (RL), in the context of building control. OPE is the problem of estimating a policy's performance without running it on the actual system, using historical data from the existing controller. It enables the control engineers to ensure a new, pretrained policy satisfies the performance requirements and safety constraints of a real-world system, prior to interacting with it. While many methods have been developed for OPE, no study has evaluated which ones are suitable for building operational data, which are generated by deterministic policies and have limited coverage of the state-action space. After reviewing existing works and their assumptions, we adopted the approximate model (AM) method. Furthermore, we used bootstrapping to quantify uncertainty and correct for bias. In a simulation study, we evaluated the proposed approach on 10 policies pretrained with imitation learning. On average, the AM method estimated the energy and comfort costs with 1.84\% and 14.1\% error, respectively.},
booktitle = {Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings \&amp; Cities},
pages = {52–56},
numpages = {5},
keywords = {Off-policy Evaluation, Building Control, Reinforcement Learning},
location = {Virtual Event, Japan},
series = {RLEM'20}
}

@article{10.1145/3522596,
author = {Wang, Chaoyang and Guo, Zhiqiang and Li, Jianjun and Li, Guohui and Pan, Peng},
title = {A Text-Based Deep Reinforcement Learning Framework Using Self-Supervised Graph Representation for Interactive Recommendation},
year = {2022},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2691-1922},
url = {https://doi.org/10.1145/3522596},
doi = {10.1145/3522596},
abstract = {Due to its nature of learning from dynamic interactions and planning for long-run performance, Reinforcement Learning (RL) has attracted much attention in Interactive Recommender Systems (IRSs). However, most of the existing RL-based IRSs usually face large discrete action space problem, which severely limits their efficiency. Moreover, data sparsity is another problem that most IRSs are confronted with. The utilization of recommendation-related textual knowledge can tackle this problem to some extent, but existing RL-based recommendation methods either neglect to combine textual information or are not suitable for incorporating it. To address these two problems, in this article, we propose a Text-based deep Reinforcement learning framework using self-supervised Graph representation for Interactive Recommendation (TRGIR). Specifically, we leverage textual information to map items and users into a same feature space by a self-supervised embedding method based on the graph convolutional network, which greatly alleviates data sparsity problem. Moreover, we design an effective method to construct an action candidate set, which reduces the scale of the action space directly. Two types of representative reinforcement learning algorithms have been applied to implement TRGIR. Since the action space of IRS is discrete, it is natural to implement TRGIR with Deep Q-learning Network (DQN). In the TRGIR implementation with Deep Deterministic Policy Gradient (DDPG), denoted as TRGIR-DDPG, we design a policy vector, which can represent user’s preferences, to generate discrete actions from the candidate set. Through extensive experiments on three public datasets, we demonstrate that TRGIR-DDPG achieves state-of-the-art performance over several baselines in a time-efficient manner.},
journal = {ACM/IMS Trans. Data Sci.},
month = {may},
articleno = {44},
numpages = {25},
keywords = {textual information, graph convolutional network, Reinforcement Learning, representation learning, Recommender system}
}

@article{10.1145/3551388,
author = {Hossain, Tahera and Shen, Wanggang and Antar, Anindya and Prabhudesai, Snehal and Inoue, Sozo and Huan, Xun and Banovic, Nikola},
title = {A Bayesian Approach for Quantifying Data Scarcity When Modeling Human Behavior via Inverse Reinforcement Learning},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3551388},
doi = {10.1145/3551388},
abstract = {Computational models that formalize complex human behaviors enable study and understanding of such behaviors. However, collecting behavior data required to estimate the parameters of such models is often tedious and resource intensive. Thus, estimating dataset size as part of data collection planning (also known as Sample Size Determination) is important to reduce the time and effort of behavior data collection while maintaining an accurate estimate of model parameters. In this article, we present a sample size determination method based on Uncertainty Quantification (UQ) for a specific Inverse Reinforcement Learning (IRL) model of human behavior, in two cases: (1) pre-hoc experiment design—conducted in the planning stage before any data is collected, to guide the estimation of how many samples to collect; and (2) post-hoc dataset analysis—performed after data is collected, to decide if the existing dataset has sufficient samples and whether more data is needed. We validate our approach in experiments with a realistic model of behaviors of people with Multiple Sclerosis (MS) and illustrate how to pick a reasonable sample size target. Our work enables model designers to perform a deeper, principled investigation of the effects of dataset size on IRL model parameters.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {mar},
articleno = {8},
numpages = {27},
keywords = {inverse reinforcement learning, behavior modeling, Sample size determination, bayesian inference}
}

@inproceedings{10.5555/3320516.3320651,
author = {Li, Maojia Patrick and Sankaran, Prashant and Kuhl, Michael E. and Ganguly, Amlan and Kwasinski, Andres and Ptucha, Raymond},
title = {Simulation Analysis of a Deep Reinforcement Learning Approach for Task Selection by Autonomous Material Handling Vehicles},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {The use of autonomous vehicles is a growing trend in the material handling and warehousing. Some challenges that face material handling include the navigation within a warehouse, precision localization and movement, and task selection decisions. In this paper, we address the issue of task selection. In particular, we develop a deep reinforcement learning methodology to enable a vehicle to select from among multiple tasks and move to the closest task in the context of material handling in a warehouse. To evaluate the deep reinforcement learning methodology, we conduct a simulation-based experiment to generate scenarios to first train and then test the capabilities of the method. The results of the experiment show that the method performs well under the given conditions.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {1073–1083},
numpages = {11},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.5555/3466184.3466382,
author = {Afridi, Muhammad Tariq and Nieto-Isaza, Santiago and Ehm, Hans and Ponsignon, Thomas and Hamed, Abdelgafar},
title = {A Deep Reinforcement Learning Approach for Optimal Replenishment Policy in a Vendor Managed Inventory Setting for Semiconductors},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Vendor Managed Inventory (VMI) is a mainstream supply chain collaboration model. Measurement approaches defining minimum and maximum inventory levels for avoiding product shortages and overstocking are rampant. No approach undertakes the responsibility aspect concerning inventory level status, especially in semiconductor industry which is confronted with short product life cycles, long process times, and volatile demand patterns. In this work, a root-cause enabling VMI performance measurement approach to assign responsibilities for poor performance is undertaken. Additionally, a solution methodology based on reinforcement learning is proposed for determining optimal replenishment policy in a VMI setting. Using a simulation model, different demand scenarios are generated based on real data from Infineon Technologies AG and compared on the basis of key performance indicators. Results obtained by the proposed method show improved performance than the current replenishment decisions of the company.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1753–1764},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.5555/3400397.3400412,
author = {Canonico, Lorenzo Barberis and McNeese, Nathan},
title = {Flash Crashes in Multi-Agent Systems Using Minority Games and Reinforcement Learning to Test AI Safety},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {As AI advances and becomes more complicated, it becomes necessary to study the safety implications of its behavior. This paper expands upon prior AI-safety research to create a model to study the harmful outcomes of multi-agent systems. In this paper, we outline previous work that has highlighted multiple aspects of AI-safety research and focus on AI-safety systems in multi-agent systems. After overviewing previous literature, we present a model focused on flash crashes, a concept often found in economics. The model was constructed using an interdisciplinary approach that includes game theory, machine learning, cognitive science and systems theory to study flash crashes in complex human-AI systems. We use the model to study a complex interaction between AI-agents, and our results indicate the multi-agent system in question is prone to cause flash crashes.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {193–204},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1109/DS-RT52167.2021.9576130,
author = {Campoverde, Luis Miguel Samaniego and Tropea, Mauro and De Rango, Floriano},
title = {An IoT Based Smart Irrigation Management System Using Reinforcement Learning Modeled through a Markov Decision Process},
year = {2022},
isbn = {9781665433266},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DS-RT52167.2021.9576130},
doi = {10.1109/DS-RT52167.2021.9576130},
abstract = {In this paper, the proposal of an irrigation system exploiting IoT is provided. The proposed system, based on IoT sensors and smart platforms such as Raspberry PI and Arduino, is able to manage farm operations in term of irrigation. The control of water pumps for irrigation is driven by two important parameters: soil moisture and evapotranspiration. The system uses a Reinforcement Learning approach based on Markov Decision Process in order to learn the right water amount needed by the plants. This approach allows to reduce both water and energy consumption. The proposal has been compared with a conventional irrigation system that normally is based on a soil humidity threshold and takes its decision considering the threshold setting. Conducted experiments show the water and energy saving by the use of the proposed Smart Irrigation system.},
booktitle = {Proceedings of the 2021 IEEE/ACM 25th International Symposium on Distributed Simulation and Real Time Applications},
articleno = {16},
numpages = {4},
keywords = {internet of things, reinforcement learning, markovian decision process, smart irrigation},
location = {Valencia, Spain},
series = {DS-RT '21}
}

@inproceedings{10.5555/3306127.3331763,
author = {Georgila, Kallirroi and Core, Mark G. and Nye, Benjamin D. and Karumbaiah, Shamya and Auerbach, Daniel and Ram, Maya},
title = {Using Reinforcement Learning to Optimize the Policies of an Intelligent Tutoring System for Interpersonal Skills Training},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Reinforcement Learning (RL) has been applied successfully to Intelligent Tutoring Systems (ITSs) in a limited set of well-defined domains such as mathematics and physics. This work is unique in using a large state space and for applying RL to tutoring interpersonal skills. Interpersonal skills are increasingly recognized as critical to both social and economic development. In particular, this work enhances an ITS designed to teach basic counseling skills that can be applied to challenging issues such as sexual harassment and workplace conflict. An initial data collection was used to train RL policies for the ITS, and an evaluation with human participants compared a hand-crafted ITS which had been used for years with students (control) versus the new ITS guided by RL policies. The RL condition differed from the control condition most notably in the strikingly large quantity of guidance it provided to learners. Both systems were effective and there was an overall significant increase from pre- to post-test scores. Although learning gains did not differ significantly between conditions, learners had a significantly higher self-rating of confidence in the RL condition. Confidence and learning gains were both part of the reward function used to train the RL policies, and it could be the case that there was the most room for improvement in confidence, an important learner emotion. Thus, RL was successful in improving an ITS for teaching interpersonal skills without the need to prune the state space (as previously done).},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {737–745},
numpages = {9},
keywords = {reinforcement learning, interpersonal skills training, intelligent tutoring systems, social agents},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3523227.3547370,
author = {Bergman, Christy D. and Hakhamaneshi, Kourosh},
title = {Hands-on Reinforcement Learning for Recommender Systems - From Bandits to SlateQ to Offline RL with Ray RLlib},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523227.3547370},
doi = {10.1145/3523227.3547370},
abstract = {Reinforcement learning (RL) is gaining traction as a complementary approach to supervised learning for RecSys due to its ability to solve sequential decision-making processes for delayed rewards. Recent advances in offline reinforcement learning, off-policy evaluation, and more scalable, performant system design with the ability to run code in parallel, have made RL more tractable for the RecSys real time use cases. This tutorial introduces RLlib [9], a comprehensive open-source Python RL framework built for production workloads. RLlib is built on top of open-source Ray [8], an easy-to-use, distributed computing framework for Python that can handle complex, heterogeneous applications. Ray and RLlib run on compute clusters on any cloud without vendor lock. Using Colab notebooks, you will leave this tutorial with a complete, working example of parallelized Python RL code using RLlib for RecSys on a github repo.},
booktitle = {Proceedings of the 16th ACM Conference on Recommender Systems},
pages = {700–701},
numpages = {2},
keywords = {Reinforcement learning, RLlib, Recommender systems},
location = {Seattle, WA, USA},
series = {RecSys '22}
}

@inproceedings{10.1145/3580305.3599358,
author = {Geng, Haoyu and Wang, Runzhong and Wu, Fei and Yan, Junchi},
title = {GAL-VNE: Solving the VNE Problem with Global Reinforcement Learning and Local One-Shot Neural Prediction},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599358},
doi = {10.1145/3580305.3599358},
abstract = {The NP-hard combinatorial Virtual Network Embedding (VNE) Problem refers to finding the node and edge mapping between a virtual net (request) and the physical net (resource). Learning-based methods are recently devised beyond traditional heuristic solvers. However, the efficiency and scalability hinder its applicability as reinforcement learning (RL) is often adopted in an auto-regressive node-by-node mapping manner to handle complex mapping constraints, for each coming request for mapping. Moreover, existing learning-based works often independently consider each online request, limiting the long-term online service performance. In this paper, we present a synergistic Global-And-Local learning approach for the VNE problem (GAL-VNE). At the global level across requests, RL is employed to capture the cross-request relation for better global resource accommodation to improve overall performance. At the local level within each request, we aim to replace the sequential decision-making procedure which relies much on the network size, with a more efficient one-shot solution generation scheme. The main challenge for such a one-shot model is how to encode the constraints under an end-to-end learning and inference paradigm. Accordingly, within the "rank-then-search" paradigm, we propose to first pretrain a graph neural network (GNN)-based node ranker with imitation supervision from an off-the-shelf solver (moderately expensive yet high quality), which is meanwhile regularized by a neighboring smooth prior. Then RL is used to finetune the GNN ranker whose supervision directly refers to the final (undifferentiable) business objectives concerning revenue and cost, etc. Experiments on benchmarks show that our method outperforms classic and learning-based methods in both efficacy and efficiency.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {531–543},
numpages = {13},
keywords = {reinforcement learning, virtual network embedding, combinatorial optimization},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3615587.3615988,
author = {Xu, Tingli and Wang, Ran and Hao, Jie},
title = {Deep Reinforcement Learning-Based Multi-Objective Optimization for Mobile Charging Services in Internet of Electric Vehicles},
year = {2023},
isbn = {9798400703416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615587.3615988},
doi = {10.1145/3615587.3615988},
abstract = {With increasing environmental awareness, electric vehicles (EVs) are becoming the preferred choice for future transportation. However, the inconvenience of charging has been a significant obstacle to the development of EVs. Mobile charging has emerged as a promising solution, providing additional charging capacity for EVs. This paper investigates the scheduling problem of a mobile charging vehicle (MCV) that aims to meet charging service requests from a group of randomly distributed EVs within their expected time windows. The objective is to minimize the total distance traveled by the MCV and the time penalty cost while ensuring that all EVs' energy demands are met. To address this multi-objective problem, we propose a deep reinforcement learning approach that decomposes the problem into subproblems using weighted grouping techniques. Each subproblem is then modeled and solved using a deep neural network based on the transformer attention mechanism. To train this model, we utilize a reinforcement learning algorithm combined with a neighbor parameter transfer training method. Extensive simulation results demonstrate that our approach outperforms traditional methods such as NSGA-II and MOEA/D in terms of online solving speed, optimization performance and generalization abilities.},
booktitle = {Proceedings of the 18th Workshop on Mobility in the Evolving Internet Architecture},
pages = {1–6},
numpages = {6},
keywords = {transformer, deep reinforcement learning, multi-objective optimization, mobile charging service},
location = {Madrid, Spain},
series = {MobiArch '23}
}

