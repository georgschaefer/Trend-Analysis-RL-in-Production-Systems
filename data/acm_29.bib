@inproceedings{10.1145/3490354.3494413,
author = {Li, Zechu and Liu, Xiao-Yang and Zheng, Jiahao and Wang, Zhaoran and Walid, Anwar and Guo, Jian},
title = {FinRL-Podracer: High Performance and Scalable Deep Reinforcement Learning for Quantitative Finance},
year = {2022},
isbn = {9781450391481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490354.3494413},
doi = {10.1145/3490354.3494413},
abstract = {Machine learning techniques are playing more and more important roles in finance market investment. However, finance quantitative modeling with conventional supervised learning approaches has a number of limitations, including the difficulty in defining appropriate labels, lack of consistency in modeling and trading execution, and lack of modeling the dynamic nature of the finance market. The development of deep reinforcement learning techniques is partially addressing these issues. Unfortunately, the steep learning curve and the difficulty in quick modeling and agile development are impeding finance researchers from using deep reinforcement learning in quantitative trading. In this paper, we propose an RLOps in finance paradigm and present a FinRL-Podracer framework to accelerate the development pipeline of deep reinforcement learning (DRL)-driven trading strategy and to improve both trading performance and training efficiency. FinRL-Podracer is a cloud solution that features high performance and high scalability and promises continuous training, continuous integration, and continuous delivery of DRL-driven trading strategies, facilitating a rapid transformation from algorithmic innovations into a profitable trading strategy. First, we propose a generational evolution mechanism with an ensemble strategy to improve the trading performance of a DRL agent, and schedule the training of a DRL algorithm onto a GPU cloud via multi-level mapping. Then, we carry out the training of DRL components with high-performance optimizations on GPUs. Finally, we evaluate the FinRL-Podracer framework for a stock trend prediction task on an NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three popular DRL libraries Ray RLlib, Stable Baseline 3 and FinRL, i.e., 12\% ~ 35\% improvements in annual return, 0.1 ~ 0.6 improvements in Sharpe ratio and 3\texttimes{} ~ 7\texttimes{} speed-up in training time. We show the high scalability by training a trading agent in 10 minutes with 80 A100 GPUs, on NASDAQ-100 constituent stocks with minute-level data over 10 years.},
booktitle = {Proceedings of the Second ACM International Conference on AI in Finance},
articleno = {48},
numpages = {9},
keywords = {deep reinforcement learning, scalability, GPU cloud, RLOps in finance, stock trend prediction},
location = {Virtual Event},
series = {ICAIF '21}
}

@inproceedings{10.1145/3297280.3297413,
author = {Molderez, Tim and Oeyen, Bjarno and De Roover, Coen and De Meuter, Wolfgang},
title = {Marlon: A Domain-Specific Language for Multi-Agent Reinforcement Learning on Networks},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297413},
doi = {10.1145/3297280.3297413},
abstract = {Distributed systems can consist of thousands of network nodes interacting with each other. Given their size, managing these systems to perform optimally is a task that should be automated. Multi-agent reinforcement learning (MARL) is a suitable technique for tackling such problems. However, the application of MARL in a distributed system is not trivial. To bridge the gap between these two domains, we introduce the Marlon language. It enables MARL experts to focus on solving machine learning problems, rather than the complexities of distributed computing. We evaluate Marlon by comparing the implementation of a load balancing use case in Marlon with an ad-hoc implementation.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1322–1329},
numpages = {8},
keywords = {domain-specific languages, distributed systems, multi-agent reinforcement learning},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3357384.3357808,
author = {Zhang, Yusi and Yang, Zhi and Wang, Liang and He, Li},
title = {Autor3: Automated Real-Time Ranking with Reinforcement Learning in E-Commerce Sponsored Search Advertising},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357808},
doi = {10.1145/3357384.3357808},
abstract = {Sponsored search platforms rank the advertisements (ads) by a ranking function to determine the impression allocation and the charging price for the advertisers. To place ads optimally, it is highly desirable but remain challenging to adapt ranking function to ad traffic at both large-scale and fine granularity. In this paper, we propose an automatic adaptive auction system called Autor 3. Our system leverages the variability and correlation of ad traffic in a search session and models ranking ads in a session as a multi-step decision-making problem. With effective yet lightweight abstractions of auction states and ranking actions, Autor3 builds a reinforcement learning (RL) framework to learn the ranking decision at the fine granularity of page views (i.e., impressions) over the large-scale auction volume. Our offline experiments show that our method considering sequential decision are superior to those that do not. We deployed Autor3 to process the billion-scale impressions per day in Taobao, the largest e-commerce platform in China. Using online A/B test and a subsequent full-scale deployment, we show that both the Revenue-Per-Mille (RPM) and Click-Through-Rates (CTRs) are improved comparing to the previous keyword-level approach used in Taobao's live production environment.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2499–2507},
numpages = {9},
keywords = {reinforcement learning, adaptive auction, sponsored search},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3488932.3497768,
author = {Song, Wei and Li, Xuezixiang and Afroz, Sadia and Garg, Deepali and Kuznetsov, Dmitry and Yin, Heng},
title = {MAB-Malware: A Reinforcement Learning Framework for Blackbox Generation of Adversarial Malware},
year = {2022},
isbn = {9781450391405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488932.3497768},
doi = {10.1145/3488932.3497768},
abstract = {Modern commercial antivirus systems increasingly rely on machine learning (ML) to keep up with the rampant inflation of new malware. However, it is well-known that machine learning models are vulnerable to adversarial examples (AEs). Previous works have shown that ML malware classifiers are fragile to the white-box adversarial attacks. However, ML models used in commercial antivirus (AV) products are usually not available to attackers and only return hard classification labels. Therefore, it is more practical to evaluate the robustness of ML models and real-world AVs in a pure black-box manner. We propose a black-box Reinforcement Learning (RL) based framework to generate AEs for PE malware classifiers and AV engines. It regards the adversarial attack problem as a multi-armed bandit problem, which finds an optimal balance between exploiting the successful patterns and exploring more varieties. Compared to other frameworks, our improvements lie in three points: 1) limiting the exploration space by modeling the generation process as a stateless process to avoid combination explosions, 2) reusing the successful payload in modeling; and 3) minimizing the changes on AE samples to correctly assign the rewards in RL learning (which also helps identify the root cause of evasions). As a result, our framework has much higher evasion rates than other off-the-shelf frameworks. Results show it has over 74\%--97\% evasion rate for two state-of-the-art ML detectors and over 32\%--48\% evasion rate for commercial AVs in a pure black-box setting. We also demonstrate that the transferability of adversarial attacks among ML-based classifiers is higher than that between ML-based classifiers and commercial AVs.},
booktitle = {Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security},
pages = {990–1003},
numpages = {14},
keywords = {adversarial learning, reinforcement learning, malware classification},
location = {Nagasaki, Japan},
series = {ASIA CCS '22}
}

@inproceedings{10.1145/3373087.3375359,
author = {Rajat, Rachit and Meng, Yuan and Kuppannagari, Sanmukh and Srivastava, Ajitesh and Prasanna, Viktor and Kannan, Rajgopal},
title = {QTAccel: A Generic FPGA Based Design for Q-Table Based Reinforcement Learning Accelerators},
year = {2020},
isbn = {9781450370998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373087.3375359},
doi = {10.1145/3373087.3375359},
abstract = {Q-Table based Reinforcement Learning (QRL) is a class of widely used algorithms in AI that work by successively improving the estimates of Q values -- quality of state-action pairs, stored in a table. They significantly outperform Neural Network based techniques when the state space is tractable. Fast learning for AI applications in several domains (e.g. robotics), with tractable 'mid-sized' Q-tables, still necessitates performing substantial rapid updates. State-of-the-art FPGA implementations of QRL do not scale with the increasing Q-Table state space, thus are not efficient for such applications. In this work, we develop a novel FPGA implementation of QRL, scalable to large state spaces and facilitating a large class of AI applications. Our pipelined architecture provides higher throughput while using significantly fewer on-chip resources and thereby supports a variety of action selection policies that covers Q-Learning and variations of bandit algorithms. Possible dependencies caused by consecutive Q value updates are handled, allowing the design to process one Q-sample every clock cycle. Additionally, we provide the first known FPGA implementation of the SARSA (State-Action-Reward-State-Action) algorithm. We evaluate our architecture for Q-Learning and SARSA algorithms and show that our designs achieve a high throughput of up to 180 million Q samples per second.},
booktitle = {Proceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {323},
numpages = {1},
keywords = {artificial intelligence, reinforcement learning accelerator, fpga acceleration, q learning},
location = {Seaside, CA, USA},
series = {FPGA '20}
}

@article{10.5555/2789272.2886799,
author = {Geramifard, Alborz and Dann, Christoph and Klein, Robert H. and Dabney, William and How, Jonathan P.},
title = {RLPy: A Value-Function-Based Reinforcement Learning Framework for Education and Research},
year = {2015},
issue_date = {January 2015},
publisher = {JMLR.org},
volume = {16},
number = {1},
issn = {1532-4435},
abstract = {RLPy is an object-oriented reinforcement learning software package with a focus on value-function-based methods using linear function approximation and discrete actions. The framework was designed for both educational and research purposes. It provides a rich library of fine-grained, easily exchangeable components for learning agents (e.g., policies or representations of value functions), facilitating recently increased specialization in reinforcement learning. RLPy is written in Python to allow fast prototyping, but is also suitable for large-scale experiments through its built-in support for optimized numerical libraries and parallelization. Code profiling, domain visualizations, and data analysis are integrated in a self-contained package available under the Modified BSD License at http://github.com/rlpy/rlpy. All of these properties allow users to compare various reinforcement learning algorithms with little effort.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {1573–1578},
numpages = {6},
keywords = {value-function, open source, reinforcement learning, empirical evaluation}
}

@inproceedings{10.1145/3431379.3460650,
author = {Kang, Yao and Wang, Xin and Lan, Zhiling},
title = {Q-Adaptive: A Multi-Agent Reinforcement Learning Based Routing on Dragonfly Network},
year = {2021},
isbn = {9781450382175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3431379.3460650},
doi = {10.1145/3431379.3460650},
abstract = {High-radix interconnects such as Dragonfly and its variants rely on adaptive routing to balance network traffic for optimum performance. Ideally, adaptive routing attempts to forward packets between minimal and non-minimal paths with the least congestion. In practice, current adaptive routing algorithms estimate routing path congestion based on local information such as output queue occupancy. Using local information to estimate global path congestion is inevitably inaccurate because a router has no precise knowledge of link states a few hops away. This inaccuracy could lead to interconnect congestion. In this study, we present Q-adaptive routing, a multi-agent reinforcement learning routing scheme for Dragonfly systems. Q-adaptive routing enables routers to learn to route autonomously by leveraging advanced reinforcement learning technology. The proposed Q-adaptive routing is highly scalable thanks to its fully distributed nature without using any shared information between routers. Furthermore, a new two-level Q-table is designed for Q-adaptive to make it computational lightly and saves 50\% of router memory usage compared with the previous Q-routing. We implement the proposed Q-adaptive routing in SST/Merlin simulator. Our evaluation results show that Q-adaptive routing achieves up to 10.5\% system throughput improvement and 5.2x average packet latency reduction compared with adaptive routing algorithms. Remarkably, Q-adaptive can even outperform the optimal VALn non-minimal routing under the ADV+1 adversarial traffic pattern with up to 3\% system throughput improvement and 75\% average packet latency reduction.},
booktitle = {Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {189–200},
numpages = {12},
keywords = {dragonfly, hpc, interconnect network, multi-agent reinforcement learning, routing},
location = {Virtual Event, Sweden},
series = {HPDC '21}
}

@inproceedings{10.5555/3306127.3331799,
author = {Shi, Longxiang and Li, Shijian and Cao, Longbing and Yang, Long and Pan, Gang},
title = {TBQ(σ): Improving Efficiency of Trace Utilization for Off-Policy Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Off-policy reinforcement learning with eligibility traces faces is challenging because of the discrepancy between target policy and behavior policy. One common approach is to measure the difference between two policies in a probabilistic way, such as importance sampling and tree-backup. However, existing off-policy learning methods based on probabilistic policy measurement are inefficient when utilizing traces under a greedy target policy, which is ineffective for control problems. The traces are cut immediately when a non-greedy action is taken, which may lose the advantage of eligibility traces and slow down the learning process. Alternatively, some non-probabilistic measurement methods such as General Q($\l{}ambda$) and Naive Q($\l{}ambda$) never cut traces, but face convergence problems in practice. To address the above issues, this paper introduces a new method named TBQ(σ), which effectively unifies the tree-backup algorithm and Naive Q($\l{}ambda$). By introducing a new parameter σ to illustrate the degree of utilizing traces, TBQ(σ) creates an effective integration of TB($\l{}ambda$) and Naive Q($\l{}ambda$) and continuous role shift between them. The contraction property of TB(σ) is theoretically analyzed for both policy evaluation and control settings. We also derive the online version of TBQ(σ) and give the convergence proof. We empirically show that, for εin(0,1]$ in ε-greedy policies, there exists some degree of utilizing traces for $\l{}ambdain[0,1]$, which can improve the efficiency in trace utilization for off-policy reinforcement learning, to both accelerate the learning process and improve the performance.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1025–1032},
numpages = {8},
keywords = {deep learning, eligibility traces, reinforcement learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3396851.3402364,
author = {Park, June Young and Nagy, Zoltan},
title = {HVACLearn: A Reinforcement Learning Based Occupant-Centric Control for Thermostat Set-Points},
year = {2020},
isbn = {9781450380096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396851.3402364},
doi = {10.1145/3396851.3402364},
abstract = {In this paper, we present a Reinforcement Learning (RL) based Occupant-Centric Controller (OCC) for thermostats, HVACLearn. Monitoring indoor air temperature, occupancy, and thermal vote, the agent learns the unique occupant behavior and indoor environments and calculates adaptive thermostat set-points to balance between occupant comfort and energy efficiency. We simulated HVACLearn performance in a single occupant office with occupant behavior models from the literature (i.e., occupancy and thermal vote). Compared to a reference controller, HVACLearn reduced the number of button presses (too hot) significantly, while consuming same or less cooling energy. For the heating, HVACLearn resulted in almost same number of button presses (too cold) with slightly less heating energy consumption.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Future Energy Systems},
pages = {434–437},
numpages = {4},
keywords = {Learning Thermostat, Reinforcement Learning, Occupant-Centric Control},
location = {Virtual Event, Australia},
series = {e-Energy '20}
}

@inproceedings{10.1145/3427773.3427864,
author = {Murugesan, S. and Jiang, Z. and Risbeck, M. J. and Amores, J. and Zhang, C. and Ramamurti, V. and Drees, K. H. and Lee, Y. M.},
title = {Less is More: Simplified State-Action Space for Deep Reinforcement Learning Based HVAC Control},
year = {2020},
isbn = {9781450381932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427773.3427864},
doi = {10.1145/3427773.3427864},
abstract = {How do we optimize heating, ventilation and air-conditioning (HVAC) systems for energy cost and occupant comfort? How do we accomplish this in an automated fashion that adapts with time with minimal human intervention? Answers to these questions have tremendous impact on building occupant comfort, building operating costs and, importantly, environmental footprint. Understandably, this topic has received considerable attention from experts both in the industry and the academia. Among these works, deep learning, specifically deep reinforcement learning (DRL) is emerging as a data-driven control strategy without requiring an explicit dynamic model of the system. Another advantage of DRL, when successfully developed, is that it can continue to learn and adapt as the building/HVAC characteristics change with time. DRL agents, however, are challenging to train. On one hand, they may need months or years of training data (sample inefficiency), potentially inconveniencing building occupants and incurring high energy costs for a long time. On the other hand, they may converge to local optima or simply do not converge. This paper highlights one strategy to mitigate some of these challenges. We show that the choice of state and action space is as important as the choice of DRL architectures and neural network training techniques. Specifically, we formulate the HVAC control problem as a Partially Observable Markov Decision Process (POMDP), build a DRL agent using Deep Q Networks (DQN) on a building simulator, and quantify gains over a widely adopted baseline heuristic method. Subsequently, we reformulate the original problem as a restricted POMDP by severely restricting the observation (state space) and action space, and build a DRL agent for the restricted POMDP. The performance gains from this DRL agent is double that of the original agent, implying that complex state and action spaces, while information rich, can lead to complex loss functions that could not be maneuvered well by a DRL agent. Our larger message is this: 'less' (state-action space) can be 'more', in the context of DRL training.},
booktitle = {Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings \&amp; Cities},
pages = {20–23},
numpages = {4},
keywords = {DQN, energy efficiency, energy cost, state-action space, Deep reinforcement learning, partially observable Markov decision processes},
location = {Virtual Event, Japan},
series = {RLEM'20}
}

@inproceedings{10.1145/3335484.3335513,
author = {Li, Fengcun and Hu, Bo},
title = {DeepJS: Job Scheduling Based on Deep Reinforcement Learning in Cloud Data Center},
year = {2019},
isbn = {9781450362788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335484.3335513},
doi = {10.1145/3335484.3335513},
abstract = {Job scheduling is a key building block of a cloud data center. Hand-crafted heuristics cannot automatically adapt to the change of the environment and optimize for specific workloads. We present the DeepJS, a job scheduling algorithm based on deep reinforcement learning under the framework of the bin packing problem. DeepJS can automatically obtain a fitness calculation method which will minimize the makespan (maximize the throughput) of a set of jobs directly from experience. Through a trace-driven simulation, we demonstrate the convergence and generalization of DeepJS and the essence of DeepJS learning. The results prove that DeepJS outperforms the heuristic-based job scheduling algorithms.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Computing},
pages = {48–53},
numpages = {6},
keywords = {Job scheduling, deep reinforcement learning, cloud data center, bin packing problem},
location = {Guangzhou, China},
series = {ICBDC '19}
}

@inproceedings{10.1145/3538637.3538866,
author = {Findeis, Arduin and Kazhamiaka, Fiodar and Jeen, Scott and Keshav, Srinivasan},
title = {Beobench: A Toolkit for Unified Access to Building Simulations for Reinforcement Learning},
year = {2022},
isbn = {9781450393973},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538637.3538866},
doi = {10.1145/3538637.3538866},
abstract = {Reinforcement learning (RL) is often considered a promising approach for controlling complex building operations. In this context, RL algorithms are typically evaluated using a testing framework that simulates building operations. To make general claims and avoid overfitting, an RL method should be evaluated on a large and diverse set of buildings. Unfortunately, due to the complexity of creating building simulations, none of the existing frameworks provide more than a handful of simulated buildings. Moreover, each framework has its own particularities, which makes it difficult to evaluate the same algorithm on multiple frameworks. To address this, we present Beobench: a Python toolkit1 that provides unified access to building simulations from multiple frameworks using a container-based approach. We demonstrate the power of our approach with an example showing how Beobench can launch RL experiments in any supported framework with a single command.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Future Energy Systems},
pages = {374–382},
numpages = {9},
keywords = {building control, building simulation, building energy optimisation, reinforcement learning},
location = {Virtual Event},
series = {e-Energy '22}
}

@inproceedings{10.1145/3384419.3430734,
author = {Fraternali, Francesco and Balaji, Bharathan and Sengupta, Dhiman and Hong, Dezhi and Gupta, Rajesh K.},
title = {Ember: Energy Management of Batteryless Event Detection Sensors with Deep Reinforcement Learning},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430734},
doi = {10.1145/3384419.3430734},
abstract = {Energy management can extend the lifetime of batteryless, energy-harvesting systems by judiciously utilizing the energy available. Duty cycling of such systems is especially challenging for event detection, as events arrive sporadically and energy availability is uncertain. If the node sleeps too much, it may miss important events; if it depletes energy too quickly, it will stop operating in low energy conditions and miss events. Thus, accurate event prediction is important in making this tradeoff. We propose Ember, an energy management system based on deep reinforcement learning to duty cycle event-driven sensors in low energy conditions. We train a policy using historical real-world data traces of motion, temperature, humidity, pressure, and light events. The resulting policy can learn to capture up to 95\% of the events without depleting the node. Without historical data for training when deploying a node at a new location, we propose a self-supervised mechanism to collect ground-truth data while learning from the data at the same time. Ember learns to capture the majority of events within a week without any historical data and matches the performance of the policies trained with historical data in a few weeks. We deployed 40 nodes running Ember for indoor sensing and demonstrate that the learned policies generalize to real-world settings as well as outperform state-of-the-art techniques.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {503–516},
numpages = {14},
keywords = {deep reinforcement learning, event-driven sensing, batteryless},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@inproceedings{10.1145/1551609.1551639,
author = {van Reeuwijk, C.},
title = {Maestro: A Self-Organizing Peer-to-Peer Dataflow Framework Using Reinforcement Learning},
year = {2009},
isbn = {9781605585871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1551609.1551639},
doi = {10.1145/1551609.1551639},
abstract = {In this paper we describe Maestro, a dataflow computation framework for Ibis, our Java-based grid middleware. The novelty of Maestro is that it is a self-organizing peer-to-peer system, meaning that it distributes the tasks in a flow over the available nodes based on local decisions on each node, without any central coordination. As a result, the computations are more scalable, more resilient against failing nodes, and less sensitive to communication latencies.Maestro uses a task distribution approach based on reinforcement learning, a learning mechanism where the positive outcome of a choice makes it more likely that the same choice repeated in the future. Maestro selects the most efficient node for each stage in the computation based on the observed computation and communication times. To ensure agility, the selection decisions are made as late as possible without letting the nodes fall idle. Using this task distribution algorithm, the nodes can be used efficiently, even in a heterogeneous system with failure-prone nodes communicating through high-latency connections.},
booktitle = {Proceedings of the 18th ACM International Symposium on High Performance Distributed Computing},
pages = {187–196},
numpages = {10},
keywords = {reinforcement learning, self organizing, peer to peer},
location = {Garching, Germany},
series = {HPDC '09}
}

@inproceedings{10.1145/3511808.3557412,
author = {Duan, Zhongjie and Chen, Cen and Cheng, Dawei and Liang, Yuqi and Qian, Weining},
title = {Optimal Action Space Search: An Effective Deep Reinforcement Learning Method for Algorithmic Trading},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557412},
doi = {10.1145/3511808.3557412},
abstract = {Algorithmic trading is a crucial yet challenging task in the financial domain, where trading decisions are made sequentially from milliseconds to days based on the historical price movements and trading frequency. To model such a sequential decision making process in the dynamic financial markets, Deep Reinforcement Learning (DRL) based methods have been applied and demonstrated their success in finding trading strategies that achieve profitable returns. However, the financial markets are complex imperfect information games with high-level of noise and uncertainties which usually make the exploration policy of DRL less effective. In this paper, we propose an end-to-end DRL method that explores solutions on the whole graph via a probabilistic dynamic programming algorithm. Specifically, we separate the state into environment state and position state, and model the position state transition as a directed acyclic graph. To obtain reliable gradients for model training, we adopt a probabilistic dynamic programming algorithm to explore solutions over the whole graph instead of sampling a path. By avoiding the sampling procedure, we propose an efficient training algorithm and overcome the efficiency problem in most existing DRL methods. Furthermore, our method is compatible with most recurrent neural network architecture, which makes our method easy to implement and very effective in practice. Extensive experiments have been conducted on two real-world stock datasets. Experimental results demonstrate that our method can generate stable trading strategies for both high-frequency and low-frequency trading, significantly outperforming the baseline DRL methods on annualized return and Sharpe ratio.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {406–415},
numpages = {10},
keywords = {reinforcement learning, algorithmic trading},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3466772.3467043,
author = {Moon, Sangwoo and Ahn, Sumyeong and Son, Kyunghwan and Park, Jinwoo and Yi, Yung},
title = {Neuro-DCF: Design of Wireless MAC via Multi-Agent Reinforcement Learning Approach},
year = {2021},
isbn = {9781450385589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466772.3467043},
doi = {10.1145/3466772.3467043},
abstract = {The carrier sense multiple access (CSMA) algorithm has been used in the wireless medium access control (MAC) under standard 802.11 implementation due to its simplicity and generality. An extensive body of research on CSMA has long been made not only in the context of practical protocols, but also in a distributed way of optimal MAC scheduling. However, the current state-of-the-art CSMA (or its extensions) still suffers from poor performance, especially in multi-hop scenarios, and often requires patch-based solutions rather than a universal solution. In this paper, we propose an algorithm which adopts an experience-driven approach and train CSMA-based wireless MAC by using deep reinforcement learning. We name our protocol, Neuro-DCF. Two key challenges are: (i) a stable training method for distributed execution and (ii) a unified training method for embracing various interference patterns and configurations. For (i), we adopt a multi-agent reinforcement learning framework, and for (ii) we introduce a novel graph neural network (GNN) based training structure. We provide extensive simulation results which demonstrate that our protocol, Neuro-DCF, significantly outperforms 802.11 DCF and O-DCF, a recent theory-based MAC protocol, especially in terms of improving delay performance while preserving optimal utility. We believe our multi-agent reinforcement learning based approach would get broad interest from other learning-based network controllers in different layers that require distributed operation.},
booktitle = {Proceedings of the Twenty-Second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {141–150},
numpages = {10},
keywords = {Optimal CSMA, Wireless MAC, Multi-agent RL},
location = {Shanghai, China},
series = {MobiHoc '21}
}

@inproceedings{10.5555/3398761.3399080,
author = {Zhang, Yi and Qian, Yu and Yao, Yichen and Hu, Haoyuan and Xu, Yinghui},
title = {Learning to Cooperate: Application of Deep Reinforcement Learning for Online AGV Path Finding},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent path finding (MAPF), naturally exists in applications like picking-up and dropping-off parcels by automated guided vehicles (AGVs) in the warehouse. Existing algorithms, like conflict-based search (CBS), windowed hierarchical cooperative A* (WHCA), and other A* variants, are widely used to find the shortest paths in different manners. However, in real-world environments, MAPF cases are dynamically generated and need to be solved in real time. In this work, a decentralized multi-agent reinforcement learning (MARL) framework with multi-step ahead tree search (MATS) strategy is proposed to make efficient decisions. Through performing experiments on a 30*30 grid world and a real-world warehouse case, our proposed MARL policy is proved to be capable of: 1) scaling to a large number of agents in real-world environment with online response time within acceptable levels; 2) outperforming existing algorithms with shorter path length and solution time, as the number of agents increases.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2077–2079},
numpages = {3},
keywords = {multi-agent reinforcement learning, multi-agent path finding},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3583781.3590312,
author = {Gandhi, Upma and Aghaeekiasaraee, Erfan and Bustany, Ismail S. K. and Mousavi, Payam and E. Taylor, Matthew and Behjat, Laleh},
title = {RL-Ripper: A Framework for Global Routing Using Reinforcement Learning and Smart Net Ripping Techniques},
year = {2023},
isbn = {9798400701252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583781.3590312},
doi = {10.1145/3583781.3590312},
abstract = {Physical designers have been using heuristics to solve challenging problems in routing. However, these heuristic solutions are not adaptable to the ever-changing fabrication demands and their effectiveness is limited by the experience and creativity of the designer. Reinforcement learning is an effective method to tackle sequential optimization problems due to its ability to adapt and learn through trial and error, creating policies that can handle complex tasks. This study presents an RL framework for global routing that incorporates a self-learning model called RL-Ripper. The primary function of RL-Ripper is to identify the best nets to rip to decrease the number of total short violations. In this work, the final global routing results are evaluated against CUGR, a state-of-the-art global router, using the ISPD 2018 benchmarks. The proposed RL-Ripper framework's approach can reduce the short violations compared to CUGR. Moreover, the RL-Ripper reduced the total number of short violations after the first iteration of detailed routing over the baseline while being on par with the wirelength, VIA, and runtime. The major impact of the proposed framework is to provide a novel learning-based approach to global routing that can be replicated for newer technologies.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2023},
pages = {197–201},
numpages = {5},
keywords = {global routing, rip-up and re-route, reinforcement learning, zmq},
location = {Knoxville, TN, USA},
series = {GLSVLSI '23}
}

@inproceedings{10.1145/1890784.1890786,
author = {Shah, Kunal and Kumar, Mohan},
title = {DReL: A Middleware for Wireless Sensor Networks Management Using Reinforcement Learning Techniques},
year = {2010},
isbn = {9781450304542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1890784.1890786},
doi = {10.1145/1890784.1890786},
abstract = {Support for autonomous and adaptive management is essential for many wireless sensor network (WSN) applications expected to be functioning over long periods of time. WSN management is further complicated by heterogeneity in terms of resources as well as applications deployed on those resources. In this paper, we present, Distributed Reinforcement Learning (DReL), middleware that provides adaptive WSN management by applying techniques from reinforcement learning and utility theory. DReL exploits a two-tier learning scheme consisting of: a) micro-learner-managing node's local tasks and resources by learning utilities of performing various tasks in different states; and b) macro-learner- managing macroscopic view and actions of a node by ensuring system as a whole achieves application's goal. Novel contributions of DReL include design and development of utility based mechanisms and associated data-structures for task, data and reward distribution by adopting concepts from directed diffusion. Our work demonstrates that individual node level as well as global level learning can help in designing a generic scheme to optimize the system while maintaining robust, localized and distributed sensing as provided by directed diffusion. Through preliminary performance analysis it is shown that DReL results in substantial increase in system lifetime compared to traditional directed diffusion.},
booktitle = {Proceedings of the 5th International Workshop on Middleware Tools, Services and Run-Time Support for Sensor Networks},
pages = {1–7},
numpages = {7},
keywords = {data-centric communication, reinforcement learning, adaptive framework, distributed sensor management},
location = {Bangalore, India},
series = {MidSens '10}
}

@inproceedings{10.1145/3412382.3458262,
author = {Billah, Md Fazlay Rabbi Masum and Saoda, Nurani and Gao, Jiechao and Campbell, Bradford},
title = {BLE Can See: A Reinforcement Learning Approach for RF-Based Indoor Occupancy Detection},
year = {2021},
isbn = {9781450380980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412382.3458262},
doi = {10.1145/3412382.3458262},
abstract = {The emergence of radio frequency (RF) dependent device-free indoor occupancy detection has seen slow acceptance due to its high fragility. Experimentation shows that an RF-dependent occupancy detector initially performs well in the room to be sensed. However, once the physical arrangement of objects changes in the room, the performance of the classifier degrades significantly. To address this issue, we propose BLECS, a Bluetooth-dependent indoor occupancy detection system which can adapt itself in the dynamic environment. BLECS uses a reinforcement learning approach to predict the occupancy of an indoor environment and updates its decision policy by interacting with existing IoT devices and sensors in the room. We tested this system in five different rooms for 520 hours in total, involving four occupants. Results show that, BLECS achieves 21.4\% performance improvement in a dynamic environment compared to the state-of-the-art supervised learning algorithm with an average F1 score of 86.52\%. This system can also predict occupancy with a maximum 89.23\% F1 score in a completely unknown environment with no initial trained model.},
booktitle = {Proceedings of the 20th International Conference on Information Processing in Sensor Networks (Co-Located with CPS-IoT Week 2021)},
pages = {132–147},
numpages = {16},
keywords = {BLE, Occupancy detection, DQN, Reinforcement learning},
location = {Nashville, TN, USA},
series = {IPSN '21}
}

@inproceedings{10.1145/3378184.3378194,
author = {Kl\"{o}ckner, Robin and Klose, Patrick},
title = {Deep-MARLIN: Using Deep Multi-Agent Reinforcement Learning for Adaptive Traffic Light Control},
year = {2020},
isbn = {9781450376303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378184.3378194},
doi = {10.1145/3378184.3378194},
abstract = {Almost every major city in the world is facing a significant economic loss caused by traffic congestions. In this context, it already has been shown that Adaptive Traffic Light Control (ATLC) can be an effective solution to improve a diversity of different traffic-related metrics. The problem of ATLC can be modeled in various ways with reinforcement learning being one of the most promising frameworks. Especially the application of Multi-Agent Reinforcement Learning (MARL) can be a suitable approach for learning to adaptively control the traffic of realistic road networks. Among the set of MARL algorithms, Multi-Agent Reinforcement Learning for Integrated Network (MARLIN) stands out and is shown to be particularly suited to the problem of ATLC with producing remarkable results. MARLIN models the multi-agent framework as a stochastic game providing an explicit coordination mechanism for the agents. However, in MARLIN, the possible size of the state and action space is limited and the features for the state representation need to be hand-crafted. Therefore, in this study, the algorithm is combined with function approximation by using artificial neural networks to overcome these limitations. deep-MARLIN is explained and bench-marked in a large and realistic traffic environment using Simulation of Urban MObility (SUMO). The results indicate that when compared to MARLIN, deep-MARLIN converges faster to a policy that is producing lower average vehicle delay.},
booktitle = {Proceedings of the 3rd International Conference on Applications of Intelligent Systems},
articleno = {10},
numpages = {6},
keywords = {Artificial Intelligence, Reinforcement Learning, Multi-Agent, Adaptive Traffic Light Control, Stochastic Game, Game Theory, Neural Networks, Machine Learning},
location = {Las Palmas de Gran Canaria, Spain},
series = {APPIS 2020}
}

@inproceedings{10.1145/3341216.3342210,
author = {Dethise, Arnaud and Canini, Marco and Kandula, Srikanth},
title = {Cracking Open the Black Box: What Observations Can Tell Us About Reinforcement Learning Agents},
year = {2019},
isbn = {9781450368728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341216.3342210},
doi = {10.1145/3341216.3342210},
abstract = {Machine learning (ML) solutions to challenging networking problems, while promising, are hard to interpret; the uncertainty about how they would behave in untested scenarios has hindered adoption. Using a case study of an ML-based video rate adaptation model, we show that carefully applying interpretability tools and systematically exploring the model inputs can identify unwanted or anomalous behaviors of the model; hinting at a potential path towards increasing trust in ML-based solutions.},
booktitle = {Proceedings of the 2019 Workshop on Network Meets AI \&amp; ML},
pages = {29–36},
numpages = {8},
keywords = {post-hoc explanations, feature analysis, neural adaptive video streaming, explainable machine learning},
location = {Beijing, China},
series = {NetAI'19}
}

@inproceedings{10.1145/3486611.3486649,
author = {Manoharan, Praveen and Venkat, Malini Pooni and Nagarathinam, Srinarayana and Vasan, Arunchandar},
title = {Learn to Chill: Intelligent Chiller Scheduling Using Meta-Learning and Deep Reinforcement Learning},
year = {2021},
isbn = {9781450391146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486611.3486649},
doi = {10.1145/3486611.3486649},
abstract = {Centralized chiller plants with multiple chillers are typically over-provisioned. Therefore, intelligent scheduling is required for the supply (operating chillers) to efficiently meet the demand (actual cooling load of buildings). Traditional cooling-load based control (CLC) may result in poor part-loaded efficiency. Recent data-driven approaches to chiller control either unrealistically assume perfect knowledge of individual chiller power at various leaving chilled water temperatures (LWTs) or control all chillers with same LWT.We complement existing work with iChill, an end-to-end learning-based intelligent chiller power prediction and scheduling strategy. First, given a dataset of chillers of varying capacities, each of which operates at a fixed LWT and varying loads, iChill meta-learns a model for power prediction. Specifically, for an unseen target chiller, the meta-learned model is re-trained with known LWT to predict power at unseen LWT. Second, given the configuration of a chiller plant and a cooling load profile, iChill learns to schedule individual chillers by jointly deciding the ON/OFF status and LWT; using deep reinforcement learning (DRL).We train and evaluate iChill in a simulated environment with real-world data from a chiller plant of 22 chillers. Specifically, we compare iChill's (1) meta-learned power model with regular transfer learning; and (2) DRL scheduling with multiple baselines including CLC and an oracle model-based predictive control (MPC) strategy with perfect knowledge. We find that iChill's (1) meta-learning improves over transfer learning by up to 15.5\%; and (2) DRL scheduling saves 11.5\% energy over CLC and is comparable with oracle MPC (12\% over CLC). Finally, off-line pre-training of iChill's DRL on the meta-learned chiller models reduces the need for real-world training experimentation by 11x from 3 years to 96 days.},
booktitle = {Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {21–30},
numpages = {10},
keywords = {control, chiller plant, simulation, deep reinforcement learning, meta-learning},
location = {Coimbra, Portugal},
series = {BuildSys '21}
}

@inproceedings{10.1145/3568294.3580172,
author = {Sarmonov, Shamil and Shakerimov, Aidar and Aimysheva, Arna and Amirova, Aida and Oralbayeva, Nurziya and Zhanatkyzy, Aida and Telisheva, Zhansaule and Sandygulova, Anara},
title = {Robot-Assisted First Language Learning in a New Latin Alphabet: The Reinforcement Learning-Based QWriter System},
year = {2023},
isbn = {9781450399708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568294.3580172},
doi = {10.1145/3568294.3580172},
abstract = {This works addresses the lately initiated Cyrillic-to-Latin alphabet shift in Kazakhstan that may bring challenges for early literacy development and acquisition; both public and scientific communities agree on possible resistance to acquiring and using a new alphabet. To support the acquisition of the new Kazakh Latin alphabet and its handwriting, this study proposes a reinforcement-learning (RL) system named QWriter. It comprises a humanoid robot NAO, a tablet with a stylus, and an RL agent that learns from a child's mistakes and progresses to maximize alphabet learning in the shortest amount of time by altering the order of practice words in response to a child's mistakes. We conducted a five-sessions experiment using a between-subject design with 69 Kazakh children ages 7 to 10 and compared their learning performance with a human tutor to assess the effectiveness of the QWriter system. Overall results show no significant differences in learning gains between the two conditions. Our study foregrounds the promising potential of the RL-based social robot in teaching foundational letter acquisition and writing over time.},
booktitle = {Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {677–681},
numpages = {5},
keywords = {reinforcement learning, child-robot interaction, alphabet learning},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@inproceedings{10.5555/3398761.3398907,
author = {Silva, Rui and Vasco, Miguel and Melo, Francisco S. and Paiva, Ana and Veloso, Manuela},
title = {Playing Games in the Dark: An Approach for Cross-Modality Transfer in Reinforcement Learning},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this work we explore the use of latent representations obtained from multiple input sensory modalities (such as images or sounds) in allowing an agent to learn and exploit policies over different subsets of input modalities. We propose a three-stage architecture that allows a reinforcement learning agent trained over a given sensory modality, to execute its task on a different sensory modality---for example, learning a visual policy over image inputs, and then execute such policy when only sound inputs are available. We show that the generalized policies achieve better out-of-the-box performance when compared to different baselines. Moreover, we show this holds in different OpenAI gym and video game environments, even when using different multimodal generative models and reinforcement learning algorithms.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1260–1268},
numpages = {9},
keywords = {multi-task learning, deep reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.5555/3398761.3398888,
author = {Ramesh, Divya and Liu, Anthony Z. and Echeverria, Andres J. and Song, Jean Y. and Waytowich, Nicholas R. and Lasecki, Walter S.},
title = {Yesterday's Reward is Today's Punishment: Contrast Effects in Human Feedback to Reinforcement Learning Agents},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Autonomous agents promise users of a personalized future, allowing them to direct their attention to tasks most meaningful to them. However, the demands of personalization stand unfulfilled by current agent training paradigms such as machine learning, which require many orders of data to train agents on a single task. In sequential decision making domains, Reinforcement Learning (RL) enables this need, when a priori training of desired behaviors is intractable. Prior work has leveraged user input to train agents by mapping them to numerical reward signals. However, recent approaches have identified inconsistent human feedback as a bottleneck to achieving best-case performance. In this work, we present empirical evidence to show that human perception affected by contrast effects distorts their feedback to Reinforcement Learning agents. Through a set of studies involving 900 participants from Amazon Mechanical Turk who were asked to give feedback to RL agents, we show that participants significantly underrate an agent's actions after being exposed to an agent of higher competence on the same task. To understand the significance of this effect on agent performance during training, we then simulate trainers that underrate actions of an agent based on past performance - creating a systematically skewed feedback signal - integrated into an actor-critic framework. Our results show that agent performance is reduced by up to 98\% in the presence of systematic skews in human feedback in Atari environments. Our work provides a conceptual understanding of a source of inconsistency in human feedback, thus informing the design of human-agent interactions.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1090–1097},
numpages = {8},
keywords = {human-in-the-loop, reinforcement learning, contrast effects, human-agent interaction},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@article{10.5555/3455716.3455818,
author = {Bargiacchi, Eugenio and Roijers, Diederik M. and Now\'{e}, Ann},
title = {AI-Toolbox: A C++ Library for Reinforcement Learning and Planning (with Python Bindings)},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {This paper describes AI-Toolbox, a C++ software library that contains reinforcement learning and planning algorithms, and supports both single and multi agent problems, as well as partial observability. It is designed for simplicity and clarity, and contains extensive documentation of its API and code. It supports Python to enable users not comfortable with C++ to take advantage of the library's speed and functionality. AI-Toolbox is free software, and is hosted online at https://github.com/Svalorzen/AI-Toolbox.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {102},
numpages = {12},
keywords = {open-source, reinforcement learning, multiagent, POMDP, software, MDP}
}

@article{10.1145/3523230.3523232,
author = {Gouel, Matthieu and Vermeulen, Kevin and Mouchet, Maxime and Rohrer, Justin P. and Fourmaux, Olivier and Friedman, Timur},
title = {Zeph \&amp; Iris Map the Internet: A Resilient Reinforcement Learning Approach to Distributed IP Route Tracing},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0146-4833},
url = {https://doi.org/10.1145/3523230.3523232},
doi = {10.1145/3523230.3523232},
abstract = {We describe a new system for distributed tracing at the IP level of the routes that packets take through the IPv4 internet. Our Zeph algorithm coordinates route tracing efforts across agents at multiple vantage points, assigning to each agent a number of /24 destination prefixes in proportion to its probing budget and chosen according to a reinforcement learning heuristic that aims to maximize the number of multipath links discovered. Zeph runs on top of Iris, our fault tolerant system for orchestrating internet measurements across distributed agents of heterogeneous probing capacities. Iris is built around third party free open source software and modern containerization technology, thereby presenting a new model for assembling a resilient and maintainable internet measurement architecture. We show that carefully choosing the destinations to probe from which vantage point matters to optimize topology discovery and that a system can learn which assignment will maximize the overall discovery based on previous measurements. After 10 cycles of probing, Zeph is capable of discovering 2.4M nodes and 10M links in a cycle of 6 hours, when deployed on 5 Iris agents. This is at least 2 times more nodes and 5 times more links than other production systems for the same number of prefixes probed.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {mar},
pages = {2–9},
numpages = {8},
keywords = {internet topology, active internet measurements}
}

@inproceedings{10.5555/3237383.3238121,
author = {Nguyen, Hung The and Garratt, Matthew and Bui, Lam Thu and Abbass, Hussein},
title = {Apprenticeship Bootstrapping: Inverse Reinforcement Learning in a Multi-Skill UAV-UGV Coordination Task},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Apprenticeship learning enables learning from human demonstrations performed on tasks. However, acquiring demonstrations in complex tasks where a human expert is not available can be a challenge. In this paper, we propose a new learning algorithm, called Apprenticeship bootstrapping via Inverse Reinforcement Learning using Deep Q-learning (ABS via IRL-DQN), to learn a complex task through using demonstrations performed on primitive sub-tasks. The algorithm is evaluated on an aerial and ground coordination scenario, where an Unmanned Aerial Vehicle (UAV) is required to maintain three Unmanned Ground Vehicles (UGVs) within a field of view of the UAV 's camera (FoV). The results show that performance of our proposed algorithm is comparable to that of a human, and competitive to the original IRL using expert demonstrations performed on the composite task.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2204–2206},
numpages = {3},
keywords = {ground-air interaction, ugvs, deep q-learning, uavs, apprenticeship learning, inverse reinforcement learning},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/3488560.3498471,
author = {Stamenkovic, Dusan and Karatzoglou, Alexandros and Arapakis, Ioannis and Xin, Xin and Katevas, Kleomenis},
title = {Choosing the Best of Both Worlds: Diverse and Novel Recommendations through Multi-Objective Reinforcement Learning},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498471},
doi = {10.1145/3488560.3498471},
abstract = {Since the inception of Recommender Systems (RS), the accuracy of the recommendations in terms of relevance has been the golden criterion for evaluating the quality of RS algorithms. However, by focusing on item relevance, one pays a significant price in terms of other important metrics: users get stuck in a "filter bubble" and their array of options is significantly reduced, hence degrading the quality of the user experience and leading to churn. Recommendation, and in particular session-based/sequential recommendation, is a complex task with multiple - and often conflicting objectives - that existing state-of-the-art approaches fail to address. In this work, we take on the aforementioned challenge and introduce Scalarized Multi-Objective Reinforcement Learning (SMORL) for the RS setting, a novel Reinforcement Learning (RL) framework that can effectively address multi-objective recommendation tasks. The proposed SMORL agent augments standard recommendation models with additional RL layers that enforce it to simultaneously satisfy three principal objectives: accuracy, diversity, and novelty of recommendations. We integrate this framework with four state-of-the-art session-based recommendation models and compare it with a single-objective RL agent that only focuses on accuracy. Our experimental results on two real-world datasets reveal a substantial increase in aggregate diversity, a moderate increase in accuracy, reduced repetitiveness of recommendations, and demonstrate the importance of reinforcing diversity and novelty as complementary objectives.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {957–965},
numpages = {9},
keywords = {recommendation, multi-objective reinforcement learning, novelty, diversity, reinforcement learning},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3366486.3366533,
author = {Rahmati, Mehdi and Nadeem, Mohammad and Sadhu, Vidyasagar and Pompili, Dario},
title = {UW-MARL: Multi-Agent Reinforcement Learning for Underwater Adaptive Sampling Using Autonomous Vehicles},
year = {2020},
isbn = {9781450377409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366486.3366533},
doi = {10.1145/3366486.3366533},
abstract = {Near-real-time water-quality monitoring in uncertain environments such as rivers, lakes, and water reservoirs of different variables is critical to protect the aquatic life and to prevent further propagation of the potential pollution in the water. In order to measure the physical values in a region of interest, adaptive sampling is helpful as an energy- and time-efficient technique since an exhaustive search of an area is not feasible with a single vehicle. We propose an adaptive sampling algorithm using multiple autonomous vehicles, which are well-trained, as agents, in a Multi-Agent Reinforcement Learning (MARL) framework to make efficient sequence of decisions on the adaptive sampling procedure. The proposed solution is evaluated using experimental data, which is fed into a simulation framework. Experiments were conducted in the Raritan River, Somerset and in Carnegie Lake, Princeton, NJ during July 2019.},
booktitle = {Proceedings of the 14th International Conference on Underwater Networks \&amp; Systems},
articleno = {32},
numpages = {5},
keywords = {field experiments, autonomous underwater vehicles, Multi-agent reinforcement learning, underwater adaptive sampling},
location = {Atlanta, GA, USA},
series = {WUWNet '19}
}

@article{10.5555/3586589.3586905,
author = {Lan, Tian and Srinivasa, Sunil and Wang, Huan and Zheng, Stephan},
title = {WarpDrive: Fast End-to-End Deep Multi-Agent Reinforcement Learning on a GPU},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {WarpDrive is a flexible, lightweight, and easy-to-use open-source framework for end-to-end deep multi-agent reinforcement learning (MARL) on a Graphics Processing Unit (GPU), available at https://github.com/salesforce/warp-drive. It addresses key system bottlenecks when applying MARL to complex environments with high-dimensional state, observation, or action spaces. For example, WarpDrive eliminates data copying between the CPU and GPU and runs thousands of simulations and agents in parallel. It also enables distributed training on multiple GPUs and scales to millions of agents. In all, WarpDrive enables orders-of-magnitude faster MARL compared to common CPU-GPU implementations. For example, WarpDrive yields 2.9 million environment steps/second with 2000 environments and 1000 agents (at least 100\texttimes{} faster than a CPU version) in a 2d-Tag simulation. It is user-friendly: e.g., it provides a lightweight, extendable Python interface and flexible environment wrappers. It is also compatible with PyTorch. In all, WarpDrive offers a platform to significantly accelerate reinforcement learning research and development.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {316},
numpages = {6},
keywords = {GPU acceleration, multi-agent systems, deep reinforcement learning}
}

@inproceedings{10.1145/3459637.3482283,
author = {Xu, Zhi and Liu, Shuncheng and Wu, Ziniu and Chen, Xu and Zeng, Kai and Zheng, Kai and Su, Han},
title = {PATROL: A Velocity Control Framework for Autonomous Vehicle via Spatial-Temporal Reinforcement Learning},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482283},
doi = {10.1145/3459637.3482283},
abstract = {The largest portion of urban congestion is caused by 'phantom' traffic jams, causing significant delay travel time, fuel waste, and air pollution. It frequently occurs in high-density traffics without any obvious signs of accidents or roadworks. The root cause of 'phantom' traffic jams in one-lane traffics is the sudden change in velocity of some vehicles (i.e. harsh driving behavior (HDB)), which may generate a chain reaction with accumulated impact throughout the vehicles along the lane. This paper makes the first attempt to address this notorious problem in a one-lane traffic environment through velocity control of autonomous vehicles. Specifically, we propose a velocity control framework, called PATROL (sPAtial-temporal ReinfOrcement Learning). First, we design a spatial-temporal graph inside the reinforcement learning model to process and extract the information (e.g. velocity and distance difference) of multiple vehicles ahead across several historical time steps in the interactive environment. Then, we propose an attention mechanism to characterize the vehicle interactions and an LSTM structure to understand the vehicles' driving patterns through time. At last, we modify the reward function used in previous velocity control works to enable the autonomous driving agent to predict the HDB of preceding vehicles and smoothly adjust its velocity, which could alleviate the chain reaction caused by HDB. We conduct extensive experiments to demonstrate the effectiveness and superiority of PATROL in alleviating the 'phantom' traffic jam in simulation environments. Further, on the real-world velocity control dataset, our method significantly outperforms the existing methods in terms of driving safety, comfortability, and efficiency.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {2271–2280},
numpages = {10},
keywords = {autonomous vehicle, velocity control, reinforcement learning},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3511808.3557283,
author = {Sun, Shuo and Xue, Wanqi and Wang, Rundong and He, Xu and Zhu, Junlei and Li, Jian and An, Bo},
title = {DeepScalper: A Risk-Aware Reinforcement Learning Framework to Capture Fleeting Intraday Trading Opportunities},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557283},
doi = {10.1145/3511808.3557283},
abstract = {Reinforcement learning (RL) techniques have shown great success in many challenging quantitative trading tasks, such as portfolio management and algorithmic trading. Especially, intraday trading is one of the most profitable and risky tasks because of the intraday behaviors of the financial market that reflect billions of rapidly fluctuating capitals. However, a vast majority of existing RL methods focus on the relatively low frequency trading scenarios (e.g., day-level) and fail to capture the fleeting intraday investment opportunities due to two major challenges: 1) how to effectively train profitable RL agents for intraday investment decision-making, which involves high-dimensional fine-grained action space; 2) how to learn meaningful multi-modality market representation to understand the intraday behaviors of the financial market at tick-level.Motivated by the efficient workflow of professional human intraday traders, we propose DeepScalper, a deep reinforcement learning framework for intraday trading to tackle the above challenges. Specifically, DeepScalper includes four components: 1) a dueling Q-network with action branching to deal with the large action space of intraday trading for efficient RL optimization; 2) a novel reward function with a hindsight bonus to encourage RL agents making trading decisions with a long-term horizon of the entire trading day; 3) an encoder-decoder architecture to learn multi-modality temporal market embedding, which incorporates both macro-level and micro-level market information; 4) a risk-aware auxiliary task to maintain a striking balance between maximizing profit and minimizing risk. Through extensive experiments on real-world market data spanning over three years on six financial futures (2 stock index and 4 treasury bond), we demonstrate that DeepScalper significantly outperforms many state-of-the-art baselines in terms of four financial criteria. Furthermore, we conduct a series of exploratory and ablative studies to analyze the contributions of each component in DeepScalper.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {1858–1867},
numpages = {10},
keywords = {reinforcement learning, quantitative investment},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1109/ASE51524.2021.9678832,
author = {Zhang, Shaohua and Liu, Shuang and Sun, Jun and Chen, Yuqi and Huang, Wenzhi and Liu, Jinyi and Liu, Jian and Hao, Jianye},
title = {FIGCPS: Effective Failure-Inducing Input Generation for Cyber-Physical Systems with Deep Reinforcement Learning},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678832},
doi = {10.1109/ASE51524.2021.9678832},
abstract = {Cyber-Physical Systems (CPSs) are composed of computational control logic and physical processes, which intertwine with each other. CPSs are widely used in various domains of daily life, including those safety-critical systems and infrastructures, such as medical monitoring, autonomous vehicles, and water treatment systems. It is thus critical to effectively test them. However, it is not easy to obtain test cases which can fail the CPS. In this work, we propose a failure-inducing input generation approach FIGCPS, which requires no knowledge of the CPS under test or any history logs of the CPS which are usually hard to obtain. Our approach adopts deep reinforcement learning techniques to interact with the CPS under test and effectively searches for failure-inducing input guided by rewards. Our approach adaptively collects information from the CPS, which reduces the training time and is also able to explore different states. Moreover, our approach is the first attempt to generate failure-inducing input for CPSs with both continuous action space and high-dimensional discrete action space, which are common for some classes of CPSs. The evaluation results show that FIGCPS not only achieves a higher success rate than the state-of-the-art approaches but also finds two new attacks in a well-tested CPS.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {555–567},
numpages = {13},
keywords = {test case generation, CPS, deep reinforcement learning},
location = {Melbourne, Australia},
series = {ASE '21}
}

@article{10.1145/2034832.2034861,
author = {Romano, Paolo and Leonetti, Matteo},
title = {Poster: Selftuning Batching in Total Order Broadcast via Analytical Modelling and Reinforcement Learning},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5999},
url = {https://doi.org/10.1145/2034832.2034861},
doi = {10.1145/2034832.2034861},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {sep},
pages = {77},
numpages = {1}
}

@inproceedings{10.1145/3212721.3212839,
author = {Hantrakul, Lamtharn and Kondak, Zachary and Weinberg, Gil},
title = {Practice Makes Perfect: Towards Learned Path Planning for Robotic Musicians Using Deep Reinforcement Learning},
year = {2018},
isbn = {9781450365048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3212721.3212839},
doi = {10.1145/3212721.3212839},
abstract = {When a pianist effortlessly glides across the keyboard during an improvised solo, the musician is executing a series of movements informed by years of practice ingrained with musical knowledge. This paper proposes an analogous approach that enables Robotic Musicians to learn about its degrees of freedom and physical constraints through "practice" in the form of Deep Reinforcement Learning. We use a Deep Q Network (DQN) to train a virtual agent representing a real 4-armed robotic musician, to motion-plan the optimal sequence of movements given a musical sequence through a learned strategy instead of a search strategy. Early results from our proof-of-concept system demonstrate that DRL can achieve optimal control of a musical agent, learning a form of bi-manual coordination in the process.},
booktitle = {Proceedings of the 5th International Conference on Movement and Computing},
articleno = {25},
numpages = {4},
keywords = {Datasets of motion recordings, Embodied cognition and movement, Expressive movement synthesis, Machine learning for movement},
location = {Genoa, Italy},
series = {MOCO '18}
}

@inproceedings{10.1145/3387168.3387199,
author = {Dankwa, Stephen and Zheng, Wenfeng},
title = {Twin-Delayed DDPG: A Deep Reinforcement Learning Technique to Model a Continuous Movement of an Intelligent Robot Agent},
year = {2020},
isbn = {9781450376259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387168.3387199},
doi = {10.1145/3387168.3387199},
abstract = {In this current research, Twin-Delayed DDPG (TD3) algorithm has been used to solve the most challenging virtual Artificial Intelligence application by training a 4-ant-legged robot as an Intelligent Agent to run across a field. Twin-Delayed DDPG (TD3) is an incredibly smart AI model of a Deep Reinforcement Learning which combines the state-of-the-art methods in Artificial Intelligence. These includes Policy gradient, Actor-Critics, and continuous Double Deep Q-Learning. These Deep Reinforcement Learning approaches trained an Intelligent agent to interact with an environment with automatic feature engineering, that is, necessitating minimal domain knowledge. For the implementation of the TD3, we used a two-layer feedforward neural network of 400 and 300 hidden nodes respectively, with Rectified Linear Units (ReLU) as an activation function between each layer for both the Actor and Critics. We, then added a final tanh unit after the output of the Actor. The Critic receives both the state and action as input to the first layer. Both the network parameters were updated using Adam optimizer. The idea behind the Twin-Delayed DDPG (TD3) is to reduce overestimation bias in Deep Q-Learning with discrete actions which are ineffective in an Actor-Critic domain setting. Based on the Maximum Average Reward over the evaluation time-step, our model achieved an approximate maximum of 2364. Therefore, we can truly say that, TD3 has obviously improved on both the learning speed and performance of the Deep Deterministic Policy Gradient (DDPG) in a challenging environment in a continuous control domain.},
booktitle = {Proceedings of the 3rd International Conference on Vision, Image and Signal Processing},
articleno = {66},
numpages = {5},
keywords = {Actor-Critic, Deep Reinforcement Learning, Artificial Intelligence, Twin-Delayed Deep Deterministic Policy Gradient},
location = {Vancouver, BC, Canada},
series = {ICVISP 2019}
}

@inproceedings{10.1145/3458648.3460008,
author = {Ahadi-Sarkani, Armand and Elmalaki, Salma},
title = {ADAS-RL: Adaptive Vector Scaling Reinforcement Learning For Human-in-the-Loop Lane Departure Warning},
year = {2021},
isbn = {9781450384407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458648.3460008},
doi = {10.1145/3458648.3460008},
abstract = {Multiple sensory modalities are fast becoming a key instrument in the future of the automotive industry. Collision avoidance, lane departure warning, and self-parking are examples of Advanced Driver Assistance Systems (ADAS) that are becoming possible with the adoption of more sensors. Moreover, thanks to the recent advances in mobile computing and wearable devices, the driver is now equipped with advanced sensory systems. This rich sensory environment paves the way to integrate the human factor into the loop of computation of ADAS to provide a personalized experience. In this paper, we introduce ADAS-RL, a Reinforcement Learning based algorithm that integrates the behavior and reactions of the driver with the vehicle context to continuously adapt and tune the warning interventions of Lane Departure Warning System (LDW). We validated ADAS-RL against human drivers using CARLA simulator. Our evaluation shows a significant enhancement in the driver experience compared to the standard LDW systems. ADAS-RL shows the ability to track the changes in driving behavior and adapt the frequency of warnings allowing drivers to stay within a reasonable distance (around 1.75m) from lane markings with a significant decrease in the false warnings.},
booktitle = {Proceedings of the First International Workshop on Cyber-Physical-Human System Design and Implementation},
pages = {13–18},
numpages = {6},
keywords = {Lane Departure Warning, Human-in-the-Loop, Q-learning, Reinforcement Learning, Advanced Driver Assistance Systems (ADAS)},
location = {Nashville, TN, USA},
series = {CPHS21}
}

@inproceedings{10.1145/3323679.3326523,
author = {Bhattacharyya, Rajarshi and Bura, Archana and Rengarajan, Desik and Rumuly, Mason and Shakkottai, Srinivas and Kalathil, Dileep and Mok, Ricky K. P. and Dhamdhere, Amogh},
title = {QFlow: A Reinforcement Learning Approach to High QoE Video Streaming over Wireless Networks},
year = {2019},
isbn = {9781450367646},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323679.3326523},
doi = {10.1145/3323679.3326523},
abstract = {Wireless Internet access has brought legions of heterogeneous applications all sharing the same resources. However, current wireless edge networks that cater to worst or average case performance lack the agility to best serve these diverse sessions. Simultaneously, software reconfigurable infrastructure has become increasingly mainstream to the point that dynamic per packet and per flow decisions are possible at multiple layers of the communications stack. Exploiting such reconfigurability requires the design of a system that can enable a configuration, measure the impact on the application performance (Quality of Experience), and adaptively select a new configuration. Effectively, this feedback loop is a Markov Decision Process whose parameters are unknown. The goal of this work is to design, develop and demonstrate QFlow that instantiates this feedback loop as an application of reinforcement learning (RL). Our context is that of reconfigurable (priority) queueing, and we use the popular application of video streaming as our use case. We develop both model-free and model-based RL approaches that are tailored to the problem of determining which clients should be assigned to which queue at each decision period. Through experimental validation, we show how the RL-based control policies on QFlow are able to schedule the right clients for prioritization in a high-load scenario to outperform the status quo, as well as the best known solutions with over 25\% improvement in QoE, and a perfect QoE score of 5 over 85\% of the time.},
booktitle = {Proceedings of the Twentieth ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {251–260},
numpages = {10},
keywords = {Reinforcement Learning, Video Streaming, Reconfigurable queueing},
location = {Catania, Italy},
series = {Mobihoc '19}
}

@inproceedings{10.5555/3320516.3321065,
author = {Shin, Kyohong and Lee, Taesik},
title = {Spartan: A Meta-Algorithm for Reinforcement Learning Using State Partitioning and Action Network},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {Targeting finite-horizon Markov Decision Process problems, we propose a novel approach with an aim to significantly enhance the scalability of reinforcement learning (RL) algorithms. Our approach, which we call a State Partitioning and Action Network, SPartAN in short, is a meta-algorithm that offers a framework an RL algorithm can be incorporated into. Key ideas in SPartAN are threefold: reducing the size of an original RL problem by partitioning the state space into smaller compartments, using a simulation model to directly obtain values of the terminal states of the upstream compartment, and constructing a quality heuristic policy in the downstream compartment by an action network to use in the simulation. Using temporal difference learning as an example RL algorithm, we show that SPartAN is able to reliably derive a high quality policy solution. Through empirical analysis, we also find that a smaller downstream state subspace in SPartAN yields higher performance.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {4182–4183},
numpages = {2},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1145/3557915.3560993,
author = {Feng, Tao and Wang, Huandong and Fan, Xiaochen and Xia, Tong and Li, Yong},
title = {Reviving the Economy While Saving Lives: A Deep Reinforcement Learning Approach for Smart POI Reopening},
year = {2022},
isbn = {9781450395298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3557915.3560993},
doi = {10.1145/3557915.3560993},
abstract = {With the gradual improvements in COVID-19 metrics and the accelerated immunization progress, countries around the world have began to focus on reviving the economy while continuously strengthening epidemic control. POInt-of-Interest (POI) reopening, as a necessity for restoring human mobilities, has become a crucial step to recouple economic recovery and public health management. In contrast to the lock-down policy, POI reopening demands a dynamic trade-off between epidemic interventions and economic costs. In the urban scenario, there exist three key challenges in developing effective POI reopening strategies as follows. (1) During the POI reopening process, there are multiple urban factors affecting the epidemic transmission, which are difficult to simultaneously incorporate and balance in a single reopening strategy; (2) the effects of POI reopening on both economic recovery and epidemic control are long-term, which are hard to capture by static models; and (3) the dual objectives of minimizing infections and maintaining POIs' visits are conflicting, making it difficult to achieve a flexible and scalable trade-off. To tackle the above challenges, we propose Reopener, a deep reinforcement learning (RL) framework for smart POI reopening. First, we utilize a bipartite graph neural network to automatically encode all urban factors that would affect the epidemic prevention and POI visit restriction. Second, we employ a RL-based deep policy network to enable flexible updates in restrictions on POIs along with the trend of epidemic. Third, we design a novel reward function to guide the RL agent to learn smartly, thus comprehensively trading off infections and visit sustainability of POIs. Extensive experimental results demonstrate that Reopener outperforms all baseline methods with remarkable improvements, by reducing the overall economic cost by at least 6.42\%. Reopener can effectively suppress infections and support a phase-based POI reopening process, which provides valuable insights for strategy design in post-COVID-19 economic recovery.},
booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
articleno = {58},
numpages = {12},
keywords = {COVID-19 pandemic, POI reopening, epidemic control, bipartite graph neural network, deep reinforcement learning},
location = {Seattle, Washington},
series = {SIGSPATIAL '22}
}

@article{10.1145/3379345,
author = {Fu, Xiaoqin and Cai, Haipeng and Li, Wen and Li, Li},
title = {SEADS: Scalable and Cost-Effective Dynamic Dependence Analysis of Distributed Systems via Reinforcement Learning},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3379345},
doi = {10.1145/3379345},
abstract = {Distributed software systems are increasingly developed and deployed today. Many of these systems are supposed to run continuously. Given their critical roles in our society and daily lives, assuring the quality of distributed systems is crucial. Analyzing runtime program dependencies has long been a fundamental technique underlying numerous tool support for software quality assurance. Yet conventional approaches to dynamic dependence analysis face severe scalability barriers when they are applied to real-world distributed systems, due to the unbounded executions to be analyzed in addition to common efficiency challenges suffered by dynamic analysis in general.In this article, we present SEADS, a distributed, online, and cost-effective dynamic dependence analysis framework that aims at scaling the analysis to real-world distributed systems. The analysis itself is distributed to exploit the distributed computing resources (e.g., a cluster) of the system under analysis; it works online to overcome the problem with unbounded execution traces while running continuously with the system being analyzed to provide timely querying of analysis results (i.e., runtime dependence set of any given query). Most importantly, given a user-specified time budget, the analysis automatically adjusts itself to better cost-effectiveness tradeoffs (than otherwise) while respecting the budget by changing various analysis parameters according to the time being spent by the dependence analysis. At the core of the automatic adjustment is our application of a reinforcement learning method for the decision making—deciding which configuration to adjust to according to the current configuration and its associated analysis cost with respect to the user budget. We have implemented SEADS for Java and applied it to eight real-world distributed systems with continuous executions. Our empirical results revealed the efficiency and scalability advantages of our framework over a conventional dynamic analysis, at least for dynamic dependence computation at method level. While we demonstrate it in the context of dynamic dependence analysis in this article, the methodology for achieving and maintaining scalability and greater cost-effectiveness against continuously running systems is more broadly applicable to other dynamic analyses.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {10},
numpages = {45},
keywords = {cost-effectiveness, dynamic analysis, reinforcement learning, scalability, Distributed systems}
}

@inproceedings{10.1145/3383313.3412252,
author = {Huang, Jin and Oosterhuis, Harrie and de Rijke, Maarten and van Hoof, Herke},
title = {Keeping Dataset Biases out of the Simulation: A Debiased Simulator for Reinforcement Learning Based Recommender Systems},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3412252},
doi = {10.1145/3383313.3412252},
abstract = {Reinforcement learning for recommendation (RL4Rec) methods are increasingly receiving attention as an effective way to improve long-term user engagement. However, applying RL4Rec online comes with risks: exploration may lead to periods of detrimental user experience. Moreover, few researchers have access to real-world recommender systems. Simulations have been put forward as a solution where user feedback is simulated based on logged historical user data, thus enabling optimization and evaluation without being run online. While simulators do not risk the user experience and are widely accessible, we identify an important limitation of existing simulation methods. They ignore the interaction biases present in logged user data, and consequently, these biases affect the resulting simulation. As a solution to this issue, we introduce a debiasing step in the simulation pipeline, which corrects for the biases present in the logged data before it is used to simulate user behavior. To evaluate the effects of bias on RL4Rec simulations, we propose a novel evaluation approach for simulators that considers the performance of policies optimized with the simulator. Our results reveal that the biases from logged data negatively impact the resulting policies, unless corrected for with our debiasing method. While our debiasing methods can be applied to any simulator, we make our complete pipeline publicly available as the Simulator for OFfline leArning and evaluation (SOFA): the first simulator that accounts for interaction biases prior to optimization and evaluation.},
booktitle = {Proceedings of the 14th ACM Conference on Recommender Systems},
pages = {190–199},
numpages = {10},
keywords = {Reinforcement learning, Interaction bias, Simulation, Recommender systems},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}

@inproceedings{10.1145/3229543.3229547,
author = {Zhang, Yi and Bai, Bo and Xu, Kuai and Lei, Kai},
title = {IFS-RL: An Intelligent Forwarding Strategy Based on Reinforcement Learning in Named-Data Networking},
year = {2018},
isbn = {9781450359115},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229543.3229547},
doi = {10.1145/3229543.3229547},
abstract = {Named-Data Networking (NDN) is a new communication paradigm where network primitives are based on named-data rather than host identifiers. Compared with IP, NDN has a unique feature that forwarding plane enables each router to select the next forwarding hop independently without relying on routing. Therefore, forwarding strategies play a significant role for adaptive and efficient data transmission in NDN. Most of the existing forwarding strategies use fixed control rules based on simplified or inaccurate models of the deployment environment. As a result, existing schemes inevitably fail to achieve optimal performance across a broad set of network conditions and application demands. In this paper, We propose IFS-RL, an intelligent forwarding strategy based on reinforcement learning. IFS-RL trains a neural network model which chooses appropriate interfaces for the forwarding of Interest based on observations collected by routing node. Not relying on pre-programmed models, IFS-RL learns to make decisions solely through observations of the resulting performance of past decisions. Therefore, IFS-RL can implement intelligent forwrarding which adapt to a wide range of network conditions. Besides, we also researches the learning granularity and the enhancement for network topology change. We compare IFS-RL to state-of-the-art forwarding strategies in ndnSIM. Experimental results show that IFS-RL can achieve higher throughput and lower packet drop rates.},
booktitle = {Proceedings of the 2018 Workshop on Network Meets AI \&amp; ML},
pages = {54–59},
numpages = {6},
keywords = {forwarding strategy, reinforcement learning, Named-Data Networking, learning granularity, network topology},
location = {Budapest, Hungary},
series = {NetAI'18}
}

@article{10.5555/3455716.3455919,
author = {Engelhardt, Dalit},
title = {Dynamic Control of Stochastic Evolution: A Deep Reinforcement Learning Approach to Adaptively Targeting Emergent Drug Resistance},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {The challenge in controlling stochastic systems in which low-probability events can set the system on catastrophic trajectories is to develop a robust ability to respond to such events without significantly compromising the optimality of the baseline control policy. This paper presents CelluDose, a stochastic simulation-trained deep reinforcement learning adaptive feedback control prototype for automated precision drug dosing targeting stochastic and heterogeneous cell proliferation. Drug resistance can emerge from random and variable mutations in targeted cell populations; in the absence of an appropriate dosing policy, emergent resistant subpopulations can proliferate and lead to treatment failure. Dynamic feedback dosage control holds promise in combatting this phenomenon, but the application of traditional control approaches to such systems is fraught with challenges due to the complexity of cell dynamics, uncertainty in model parameters, and the need in medical applications for a robust controller that can be trusted to properly handle unexpected outcomes. Here, training on a sample biological scenario identified single-drug and combination therapy policies that exhibit a 100\% success rate at suppressing cell proliferation and responding to diverse system perturbations while establishing low-dose no-event baselines. These policies were found to be highly robust to variations in a key model parameter subject to significant uncertainty and unpredictable dynamical changes.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {203},
numpages = {30},
keywords = {reinforcement learning, control, drug resistance, deep learning, adaptive dosing}
}

@inproceedings{10.5555/3539845.3539924,
author = {Zhang, Wen and Zhang, Jeff and Xie, Mimi and Liu, Tao and Wang, Wenlu and Pan, Chen},
title = {M2M-Routing: Environmental Adaptive Multi-Agent Reinforcement Learning Based Multi-Hop Routing Policy for Self-Powered IoT Systems},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Energy harvesting (EH) technologies facilitate the trending proliferation of IoT devices with sustainable power supplies. However, the intrinsic weak and unstable nature of EH results in frequent and unpredictable power interruptions in EH IoT devices, which further causes unpleasant packet loss or reconnection failures in IoT network. Therefore, conventional routing and energy allocation methods are inefficient in the EH environments. The complexity of the EH environment caused a stumbling block to an intelligent routing policy and energy allocation. To address the problems, this work proposes an environment adaptive Deep Reinforcement Learning (DRL)-based multi-hop routing policy, M2M-Routing, to jointly optimize energy allocation and routing policy and mitigate these challenges through leveraging the offline computation resources. We prepare multi-models for the complex energy harvesting environment offline. By searching a historically similar power trace to identify the model ID, the prepared DRL model is selected to manage energy allocation and routing policy on the query power traces. Simulation results indicate that M2M-Routing improves the amount of data delivery by ~ 3X to ~ 4X compared with baselines.},
booktitle = {Proceedings of the 2022 Conference \&amp; Exhibition on Design, Automation \&amp; Test in Europe},
pages = {316–321},
numpages = {6},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.5555/3581644.3581664,
author = {Khalil, Alvi Ataur and Rahman, Mohammad Ashiqur},
title = {FeD-UP: Federated Deep Reinforcement Learning-Based UAV Path Planning against Hostile Defense System},
year = {2023},
isbn = {9783903176515},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {In military operations, unmanned aerial vehicles (UAVs) have been heavily utilized in recent years. However, due to the antenna installment regulation, UAVs cannot be controlled by human operators in a restricted area. Hence, artificial intelligence (AI)-driven UAVs are the practical solution to this out-of-coverage problem. With the increased use of autonomous UAVs in military applications, defense systems are deployed to target and shoot down the enemy UAVs in operation. Thus, UAVs are needed to be trained, not only to achieve goals but also to avoid static and dynamic hostile defense systems. In this work, we propose FeD-UP, a federated deep reinforcement learning (DRL)-based UAV path planning framework, that enables UAVs to carry out missions in a hostile environment with a dynamic defense system. The federated learning (FL) based training accelerates the reinforcement learning process and improves model performance. We additionally introduce significant reply memory buffer (SRMB) to quicken the training process more, by selecting the crucial experiences during the training period. The experimental results validate the efficiency of the proposed model in controlling UAVs in dynamic, hostile environments.},
booktitle = {Proceedings of the 18th International Conference on Network and Service Management},
articleno = {16},
numpages = {7},
keywords = {path planning, unmanned aerial vehicles, deep reinforcement learning, federated learning},
location = {Thessaloniki, Greece},
series = {CNSM '22}
}

@article{10.1145/3414840,
author = {Ren, Hongshuai and Wang, Yang and Xu, Chengzhong and Chen, Xi},
title = {SMig-RL: An Evolutionary Migration Framework for Cloud Services Based on Deep Reinforcement Learning},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3414840},
doi = {10.1145/3414840},
abstract = {Service migration is an often-used approach in cloud computing to minimize the access cost by moving the service close to most users. Although it is effective in a certain sense, the service migration in existing research still suffers from some deficiencies in its evolutionary abilities in scalability, sensitivity, and adaptability to effectively react to the dynamically changing environments. This article proposes an evolutionary framework based on deep reinforcement learning for virtual service migration in large-scale mobile cloud centers. To enhance the spatio-temporal sensitivity of the algorithm, we design a scalable reward function for virtual service migration, redefine the input state, and add a Recurrent Neural Network (RNN) to the learning framework. Additionally, in order to enhance the adaptability of the algorithm, we also decompose the action space and exploit the network cost to adjust the number of virtual machine (VMs). The experimental results show that, compared with the existing results, the migration strategy generated by the algorithm can not only significantly reduce the total service cost and achieve the load balancing at the same time, but also address the burst situations with low cost in dynamic environments.},
journal = {ACM Trans. Internet Technol.},
month = {oct},
articleno = {43},
numpages = {18},
keywords = {Q-learning, dynamic service migration, deep reinforcement learning, Cloud computing, RNN, mobile access}
}

@inproceedings{10.1145/3307772.3328281,
author = {Subramanian, Easwar and Bichpuriya, Yogesh and Achar, Avinash and Bhat, Sanjay and Singh, Abhay Pratap and Sarangan, Venkatesh and Natarajan, Akshaya},
title = {LEarn: A Reinforcement Learning Based Bidding Strategy for Generators in Single Sided Energy Markets},
year = {2019},
isbn = {9781450366717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307772.3328281},
doi = {10.1145/3307772.3328281},
abstract = {We aim to increase the profit of a given generator participating in a single-sided wholesale energy market. We model the market clearing mechanism and the behavior of other generators competing in the market. We utilize interesting structures in the data to classify generators and then build novel supervised competition models for each class of generators. We leverage these models to build an interactive system through which we discover better bidding strategies for the given generator using reinforcement learning (RL). We relax several assumptions made in existing works in order to make the problem more relevant to real life. Our MDP formulation enables us to tackle action space explosion in an efficient way. Further, our state formulation enables us to compute optimal actions across all time-steps of the day in parallel. We compare the performance of the proposed RL based bidding agent with the historical real world performance of a generator in a wholesale energy market.},
booktitle = {Proceedings of the Tenth ACM International Conference on Future Energy Systems},
pages = {121–127},
numpages = {7},
keywords = {Energy markets, Bidding, Reinforcement learning},
location = {Phoenix, AZ, USA},
series = {e-Energy '19}
}

