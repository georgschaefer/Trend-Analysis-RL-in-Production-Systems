@inproceedings{10.3115/1117562.1117566,
author = {Litman, Diane and Singh, Satinder and Kearns, Michael and Walker, Marilyn},
title = {NJFun: A Reinforcement Learning Spoken Dialogue System},
year = {2000},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1117562.1117566},
doi = {10.3115/1117562.1117566},
abstract = {This paper describes NJFun, a real-time spoken dialogue system that provides users with information about things to do in New Jersey. NJFun automatically optimizes its dialogue strategy over time, by using a methodology for applying reinforcement learning to a working dialogue system with human users.},
booktitle = {Proceedings of the 2000 ANLP/NAACL Workshop on Conversational Systems - Volume 3},
pages = {17–20},
numpages = {4},
location = {Seattle, Washington},
series = {ANLP/NAACL-ConvSyst '00}
}

@inproceedings{10.1145/3508352.3549437,
author = {Chen, Hanning and Issa, Mariam and Ni, Yang and Imani, Mohsen},
title = {DARL: Distributed Reconfigurable Accelerator for Hyperdimensional Reinforcement Learning},
year = {2022},
isbn = {9781450392174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508352.3549437},
doi = {10.1145/3508352.3549437},
abstract = {Reinforcement Learning (RL) is a powerful technology to solve decisionmaking problems such as robotics control. Modern RL algorithms, i.e., Deep Q-Learning, are based on costly and resource hungry deep neural networks. This motivates us to deploy alternative models for powering RL agents on edge devices. Recently, brain-inspired Hyper-Dimensional Computing (HDC) has been introduced as a promising solution for lightweight and efficient machine learning, particularly for classification.In this work, we develop a novel platform capable of real-time hyperdimensional reinforcement learning. Our heterogeneous CPU-FPGA platform, called DARL, maximizes FPGA's computing capabilities by applying hardware optimizations to hyperdimensional computing's critical operations, including hardware-friendly encoder IP, the hypervector chunk fragmentation, and the delayed model update. Aside from hardware innovation, we also extend the platform to basic single-agent RL to support multi-agents distributed learning. We evaluate the effectiveness of our approach on OpenAI Gym tasks. Our results show that the FPGA platform provides on average 20\texttimes{} speedup compared to current state-of-the-art hyperdimensional RL methods running on Intel Xeon 6226 CPU. In addition, DARL provides around 4.8\texttimes{} faster and 4.2\texttimes{} higher energy efficiency compared to the state-of-the-art RL accelerator while ensuring a better or comparable quality of learning.},
booktitle = {Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
articleno = {84},
numpages = {9},
location = {San Diego, California},
series = {ICCAD '22}
}

@inproceedings{10.1145/3489517.3530668,
author = {Ni, Yang and Issa, Mariam and Abraham, Danny and Imani, Mahdi and Yin, Xunzhao and Imani, Mohsen},
title = {HDPG: Hyperdimensional Policy-Based Reinforcement Learning for Continuous Control},
year = {2022},
isbn = {9781450391429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489517.3530668},
doi = {10.1145/3489517.3530668},
abstract = {Traditional robot control or more general continuous control tasks often rely on carefully hand-crafted classic control methods. These models often lack the self-learning adaptability and intelligence to achieve human-level control. On the other hand, recent advancements in Reinforcement Learning (RL) present algorithms that have the capability of human-like learning. The integration of Deep Neural Networks (DNN) and RL thereby enables autonomous learning in robot control tasks. However, DNN-based RL brings both high-quality learning and high computation cost, which is no longer ideal for currently fast-growing edge computing scenarios.In this paper, we introduce HDPG, a highly-efficient policy-based RL algorithm using Hyperdimensional Computing. Hyperdimensional computing is a lightweight brain-inspired learning methodology; its holistic representation of information leads to a well-defined set of hardware-friendly high-dimensional operations. Our HDPG fully exploits the efficient HDC for high-quality state value approximation and policy gradient update. In our experiments, we use HDPG for robotics tasks with continuous action space and achieve significantly higher rewards than DNN-based RL. Our evaluation also shows that HDPG achieves 4.7\texttimes{} faster and 5.3\texttimes{} higher energy efficiency than DNN-based RL running on embedded FPGA.},
booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference},
pages = {1141–1146},
numpages = {6},
location = {San Francisco, California},
series = {DAC '22}
}

@inproceedings{10.1145/3490354.3494366,
author = {Liu, Xiao-Yang and Yang, Hongyang and Gao, Jiechao and Wang, Christina Dan},
title = {FinRL: Deep Reinforcement Learning Framework to Automate Trading in Quantitative Finance},
year = {2022},
isbn = {9781450391481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490354.3494366},
doi = {10.1145/3490354.3494366},
abstract = {Deep reinforcement learning (DRL) has been envisioned to have a competitive edge in quantitative finance. However, there is a steep development curve for quantitative traders to obtain an agent that automatically positions to win in the market, namely to decide where to trade, at what price and what quantity, due to the error-prone programming and arduous debugging. In this paper, we present the first open-source framework FinRL as a full pipeline to help quantitative traders overcome the steep learning curve. FinRL is featured with simplicity, applicability and extensibility under the key principles, full-stack framework, customization, reproducibility and hands-on tutoring.Embodied as a three-layer architecture with modular structures, FinRL implements fine-tuned state-of-the-art DRL algorithms and common reward functions, while alleviating the debugging workloads. Thus, we help users pipeline the strategy design at a high turnover rate. At multiple levels of time granularity, FinRL simulates various markets as training environments using historical data and live trading APIs. Being highly extensible, FinRL reserves a set of user-import interfaces and incorporates trading constraints such as market friction, market liquidity and investor's risk-aversion. Moreover, serving as practitioners' stepping stones, typical trading tasks are provided as step-by-step tutorials, e.g., stock trading, portfolio allocation, cryptocurrency trading, etc.},
booktitle = {Proceedings of the Second ACM International Conference on AI in Finance},
articleno = {1},
numpages = {9},
keywords = {deep reinforcement learning, portfolio allocation, automated trading, markov decision process, quantitative finance},
location = {Virtual Event},
series = {ICAIF '21}
}

@inproceedings{10.1145/3542929.3563501,
author = {Zhang, Kuo and Wang, Peijian and Gu, Ning and Nguyen, Thu D.},
title = {GreenDRL: Managing Green Datacenters Using Deep Reinforcement Learning},
year = {2022},
isbn = {9781450394147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3542929.3563501},
doi = {10.1145/3542929.3563501},
abstract = {Managing datacenters to maximize efficiency and sustain-ability is a complex and challenging problem. In this work, we explore the use of deep reinforcement learning (RL) to manage "green" datacenters, bringing a robust approach for designing efficient management systems that account for specific workload, datacenter, and environmental characteristics. We design and evaluate GreenDRL, a system that combines a deep RL agent with simple heuristics to manage workload, energy consumption, and cooling in the presence of onsite generation of renewable energy to minimize brown energy consumption and cost. Our design addresses several important challenges, including adaptability, robustness, and effective learning in an environment comprising an enormous state/action space and multiple stochastic processes. Evaluation results (using simulation) show that GreenDRL is able to learn important principles such as delaying deferrable jobs to leverage variable generation of renewable (solar) energy, and avoiding the use of power-intensive cooling settings even at the expense of leaving some renewable energy unused. In an environment where a fraction of the workload is deferrable by up to 12 hours, GreenDRL can reduce grid electricity consumption for days with different solar energy generation and temperature characteristics by 32--54\% compared to a FIFO baseline approach. GreenDRL also matches or outperforms a management approach that uses linear programming together with oracular future knowledge to manage workload and server energy consumption, but leaves the management of the cooling system to a separate (and independent) controller. Overall, our work shows that deep RL is a promising technique for building efficient management systems for green datacenters.},
booktitle = {Proceedings of the 13th Symposium on Cloud Computing},
pages = {445–460},
numpages = {16},
keywords = {deep reinforcement learning, power management, datacenter, scheduling, green datacenter},
location = {San Francisco, California},
series = {SoCC '22}
}

@article{10.1145/3272127.3275014,
author = {Peng, Xue Bin and Kanazawa, Angjoo and Malik, Jitendra and Abbeel, Pieter and Levine, Sergey},
title = {SFV: Reinforcement Learning of Physical Skills from Videos},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3272127.3275014},
doi = {10.1145/3272127.3275014},
abstract = {Data-driven character animation based on motion capture can produce highly naturalistic behaviors and, when combined with physics simulation, can provide for natural procedural responses to physical perturbations, environmental changes, and morphological discrepancies. Motion capture remains the most popular source of motion data, but collecting mocap data typically requires heavily instrumented environments and actors. In this paper, we propose a method that enables physically simulated characters to learn skills from videos (SFV). Our approach, based on deep pose estimation and deep reinforcement learning, allows data-driven animation to leverage the abundance of publicly available video clips from the web, such as those from YouTube. This has the potential to enable fast and easy design of character controllers simply by querying for video recordings of the desired behavior. The resulting controllers are robust to perturbations, can be adapted to new settings, can perform basic object interactions, and can be retargeted to new morphologies via reinforcement learning. We further demonstrate that our method can predict potential human motions from still images, by forward simulation of learned controllers initialized from the observed pose. Our framework is able to learn a broad range of dynamic skills, including locomotion, acrobatics, and martial arts. (Video1)},
journal = {ACM Trans. Graph.},
month = {dec},
articleno = {178},
numpages = {14},
keywords = {reinforcement learning, video imitation, motion reconstruction, physics-based character animation, computer vision}
}

@article{10.1613/jair.1.12440,
author = {Toro Icarte, Rodrigo and Klassen, Toryn Q. and Valenzano, Richard and McIlraith, Sheila A.},
title = {Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning},
year = {2022},
issue_date = {May 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {73},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12440},
doi = {10.1613/jair.1.12440},
abstract = {Reinforcement learning (RL) methods usually treat reward functions as black boxes. As such, these methods must extensively interact with the environment in order to discover rewards and optimal policies. In most RL applications, however, users have to program the reward function and, hence, there is the opportunity to make the reward function visible – to show the reward function’s code to the RL agent so it can exploit the function’s internal structure to learn optimal policies in a more sample efficient manner. In this paper, we show how to accomplish this idea in two steps. First, we propose reward machines, a type of finite state machine that supports the specification of reward functions while exposing reward function structure. We then describe different methodologies to exploit this structure to support learning, including automated reward shaping, task decomposition, and counterfactual reasoning with off-policy learning. Experiments on tabular and continuous domains, across different tasks and RL agents, show the benefits of exploiting reward structure with respect to sample efficiency and the quality of resultant policies. Finally, by virtue of being a form of finite state machine, reward machines have the expressive power of a regular language and as such support loops, sequences and conditionals, as well as the expression of temporally extended properties typical of linear temporal logic and non-Markovian reward specification.},
journal = {J. Artif. Int. Res.},
month = {may},
numpages = {36},
keywords = {reinforcement learning, automated reasoning, causality, knowledge representation}
}

@inproceedings{10.1145/3301275.3302285,
author = {Leino, Katri and Oulasvirta, Antti and Kurimo, Mikko},
title = {RL-KLM: Automating Keystroke-Level Modeling with Reinforcement Learning},
year = {2019},
isbn = {9781450362726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301275.3302285},
doi = {10.1145/3301275.3302285},
abstract = {The Keystroke-Level Model (KLM) is a popular model for predicting users' task completion times with graphical user interfaces. KLM predicts task completion times as a linear function of elementary operators. However, the policy, or the assumed sequence of the operators that the user executes, needs to be prespeciffed by the analyst. This paper investigates Reinforcement Learning (RL) as an algorithmic method to obtain the policy automatically. We define the KLM as an Markov Decision Process, and show that when solved with RL methods, this approach yields user-like policies in simple but realistic interaction tasks. RL-KLM offers a quick way to obtain a global upper bound for user performance. It opens up new possibilities to use KLM in computational interaction. However, scalability and validity remain open issues.},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
pages = {476–480},
numpages = {5},
keywords = {reinforcement learning, keystroke-level modelling, computational evaluation, computational design},
location = {Marina del Ray, California},
series = {IUI '19}
}

@article{10.5555/3586589.3586856,
author = {Weng, Jiayi and Chen, Huayu and Yan, Dong and You, Kaichao and Duburcq, Alexis and Zhang, Minghao and Su, Yi and Su, Hang and Zhu, Jun},
title = {Tianshou: A Highly Modularized Deep Reinforcement Learning Library},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {In this paper, we present Tianshou, a highly modularized Python library for deep reinforcement learning (DRL) that uses PyTorch as its backend. Tianshou intends to be research-friendly by providing a flexible and reliable infrastructure of DRL algorithms. It supports online and offline training with more than 20 classic algorithms through a unified interface. To facilitate related research and prove Tianshou's reliability, we have released Tianshou's benchmark of MuJoCo environments, covering eight classic algorithms with state-of-the-art performance.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {267},
numpages = {6},
keywords = {modularized, library, benchmark, PyTorch, deep reinforcement learning}
}

@inproceedings{10.1145/3178876.3185994,
author = {Zheng, Guanjie and Zhang, Fuzheng and Zheng, Zihan and Xiang, Yang and Yuan, Nicholas Jing and Xie, Xing and Li, Zhenhui},
title = {DRN: A Deep Reinforcement Learning Framework for News Recommendation},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3185994},
doi = {10.1145/3178876.3185994},
abstract = {In this paper, we propose a novel Deep Reinforcement Learning framework for news recommendation. Online personalized news recommendation is a highly challenging problem due to the dynamic nature of news features and user preferences. Although some online recommendation models have been proposed to address the dynamic nature of news recommendation, these methods have three major issues. First, they only try to model current reward (e.g., Click Through Rate). Second, very few studies consider to use user feedback other than click / no click labels (e.g., how frequent user returns) to help improve recommendation. Third, these methods tend to keep recommending similar news to users, which may cause users to get bored. Therefore, to address the aforementioned challenges, we propose a Deep Q-Learning based recommendation framework, which can model future reward explicitly. We further consider user return pattern as a supplement to click / no click label in order to capture more user feedback information. In addition, an effective exploration strategy is incorporated to find new attractive news for users. Extensive experiments are conducted on the offline dataset and online production environment of a commercial news recommendation application and have shown the superior performance of our methods.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {167–176},
numpages = {10},
keywords = {news recommendation, reinforcement learning, deep Q-Learning},
location = {Lyon, France},
series = {WWW '18}
}

@article{10.14778/3229863.3236263,
author = {Trummer, Immanuel and Moseley, Samuel and Maram, Deepak and Jo, Saehan and Antonakakis, Joseph},
title = {SkinnerDB: Regret-Bounded Query Evaluation via Reinforcement Learning},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3236263},
doi = {10.14778/3229863.3236263},
abstract = {Robust query optimization becomes illusory in the presence of correlated predicates or user-defined functions. Occasionally, the query optimizer will choose join orders whose execution time is by many orders of magnitude higher than necessary. We present SkinnerDB, a novel database management system that is designed from the ground up for reliable optimization and robust performance.SkinnerDB implements several adaptive query processing strategies based on reinforcement learning. We divide the execution of a query into small time periods in which different join orders are executed. Thereby, we converge to optimal join orders with regret bounds, meaning that the expected difference between actual execution time and time for an optimal join order is bounded. To the best of our knowledge, our execution strategies are the first to provide comparable formal guarantees. SkinnerDB can be used as a layer on top of any existing database management system. We use optimizer hints to force existing systems to try out different join orders, carefully restricting execution time per join order and data batch via timeouts. We choose timeouts according to an iterative scheme that balances execution time over different timeouts to guarantee bounded regret. Alternatively, SkinnerDB can be used as a standalone, featuring an execution engine that is tailored to the requirements of join order learning. In particular, we use a specialized multi-way join algorithm and a concise tuple representation to facilitate fast switches between join orders. In our demonstration, we let participants experiment with different query types and databases. We visualize the learning process and compare against baselines.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2074–2077},
numpages = {4}
}

@inproceedings{10.1145/3488560.3498438,
author = {Luo, Ziyan and Miao, Congcong},
title = {RLMob: Deep Reinforcement Learning for Successive Mobility Prediction},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498438},
doi = {10.1145/3488560.3498438},
abstract = {Human mobility prediction is an important task in the field of spatiotemporal sequential data mining and urban computing. Despite the extensive work on mining human mobility behavior, little attention was paid to the problem of successive mobility prediction. The state-of-the-art methods of human mobility prediction are mainly based on supervised learning. To achieve higher predictability and adapt well to the successive mobility prediction, there are four key challenges: 1) disability to the circumstance that the optimizing target is discrete-continuous hybrid and non-differentiable. In our work, we assume that the user's demands are always multi-targeted and can be modeled as a discrete-continuous hybrid function; 2) difficulty to alter the recommendation strategy flexibly according to the changes in user needs in real scenarios; 3) error propagation and exposure bias issues when predicting multiple points in successive mobility prediction; 4) cannot interactively explore user's potential interest that does not appear in the history. While previous methods met these difficulties, reinforcement learning (RL) is an intuitive answer for this task to settle these issues. We innovatively introduce RL to the successive prediction task. In this paper, we formulate this problem as a Markov Decision Process. We further propose a framework - RLMob to solve our problem. A simulated environment is carefully designed. An actor-critic framework with an instance of Proximal Policy Optimization (PPO) is applied to adapt to our scene with a large state space. Experiments show that on the task, the performance of our approach is consistently superior to that of the compared approaches.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {648–656},
numpages = {9},
keywords = {human mobility prediction, successive mobility prediction, deep reinforcement learning, sequential data mining},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3459637.3481917,
author = {Xie, Ruobing and Zhang, Shaoliang and Wang, Rui and Xia, Feng and Lin, Leyu},
title = {Explore, Filter and Distill: Distilled Reinforcement Learning in Recommendation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481917},
doi = {10.1145/3459637.3481917},
abstract = {Reinforcement learning (RL) has been verified in real-world list-wise recommendation. However, RL-based recommendation suffers from huge memory and computation costs due to its large-scale models. Knowledge distillation (KD) is an effective approach for model compression widely used in practice. However, RL-based models strongly rely on sufficient explorations on the enormous user-item space due to the data sparsity issue, which multiplies the challenges of KD with RL models. What the teacher should teach and how much the student should learn from each lesson need to be carefully designed. In this work, we propose a novel Distilled reinforcement learning framework for recommendation (DRL-Rec), which aims to improve both effectiveness and efficiency in list-wise recommendation. Specifically, we propose an Exploring and filtering module before the distillation, which decides what lessons the teacher should teach from both teachers' and students' aspects. We also conduct a Confidence-guided distillation at both output and intermediate levels with a list-wise KL divergence loss and a Hint loss, which aims to understand how much the student should learn for each lesson. We achieve significant improvements on both offline and online evaluations in a well-known recommendation system. DRL-Rec has been deployed on WeChat Top Stories for more than six months, affecting millions of users. The source codes are released in https://github.com/modriczhang/DRL-Rec.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {4243–4252},
numpages = {10},
keywords = {knowledge distillation, list-wise recommendation, reinforcement learning, recommendation},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3298689.3346981,
author = {Shi, Bichen and Ozsoy, Makbule Gulcin and Hurley, Neil and Smyth, Barry and Tragos, Elias Z. and Geraci, James and Lawlor, Aonghus},
title = {PyRecGym: A Reinforcement Learning Gym for Recommender Systems},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3346981},
doi = {10.1145/3298689.3346981},
abstract = {Recommender systems (RS) share many features and objectives with reinforcement learning (RL) systems. The former aim to maximise user satisfaction by recommending the right items to the right users at the right time, the latter maximise future rewards by selecting state-changing actions in some environment. The concept of an RL gym has become increasingly important when it comes to supporting the development of RL models. A gym provides a simulation environment in which to test and develop RL agents, providing a state model, actions, rewards/penalties etc. In this paper we describe and demonstrate the PyRecGym gym, which is specifically designed for the needs of recommender systems research, by supporting standard test datasets (MovieLens, Yelp etc.), common input types (text, numeric etc.), and thereby offering researchers a reproducible research environment to accelerate experimentation and development of RL in RS.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {491–495},
numpages = {5},
keywords = {reinforcement learning, reinforcement learning gym, recommender systems, click-through-rate},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@inproceedings{10.1145/3299869.3300088,
author = {Trummer, Immanuel and Wang, Junxiong and Maram, Deepak and Moseley, Samuel and Jo, Saehan and Antonakakis, Joseph},
title = {SkinnerDB: Regret-Bounded Query Evaluation via Reinforcement Learning},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3300088},
doi = {10.1145/3299869.3300088},
abstract = {SkinnerDB is designed from the ground up for reliable join ordering. It maintains no data statistics and uses no cost or cardinality models. Instead, it uses reinforcement learning to learn optimal join orders on the fly, during the execution of the current query. To that purpose, we divide the execution of a query into many small time slices. Different join orders are tried in different time slices. We merge result tuples generated according to different join orders until a complete result is obtained. By measuring execution progress per time slice, we identify promising join orders as execution proceeds. Along with SkinnerDB, we introduce a new quality criterion for query execution strategies. We compare expected execution cost against execution cost for an optimal join order. SkinnerDB features multiple execution strategies that are optimized for that criterion. Some of them can be executed on top of existing database systems. For maximal performance, we introduce a customized execution engine, facilitating fast join order switching via specialized multi-way join algorithms and tuple representations. We experimentally compare SkinnerDB's performance against various baselines, including MonetDB, Postgres, and adaptive processing methods. We consider various benchmarks, including the join order benchmark and TPC-H variants with user-defined functions. Overall, the overheads of reliable join ordering are negligible compared to the performance impact of the occasional, catastrophic join order choice.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1153–1170},
numpages = {18},
keywords = {regret-bounded query evaluation, query optimization, reinforcement learning},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.5555/3437539.3437740,
author = {Wang, Hanrui and Wang, Kuan and Yang, Jiacheng and Shen, Linxiao and Sun, Nan and Lee, Hae-Seung and Han, Song},
title = {GCN-RL Circuit Designer: Transferable Transistor Sizing with Graph Neural Networks and Reinforcement Learning},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
abstract = {Automatic transistor sizing is a challenging problem in circuit design due to the large design space, complex performance trade-offs, and fast technology advancements. Although there have been plenty of work on transistor sizing targeting on one circuit, limited research has been done on transferring the knowledge from one circuit to another to reduce the re-design overhead. In this paper, we present GCN-RL Circuit Designer, leveraging reinforcement learning (RL) to transfer the knowledge between different technology nodes and topologies. Moreover, inspired by the simple fact that circuit is a graph, we learn on the circuit topology representation with graph convolutional neural networks (GCN). The GCN-RL agent extracts features of the topology graph whose vertices are transistors, edges are wires. Our learning-based optimization consistently achieves the highest Figures of Merit (FoM) on four different circuits compared with conventional black box optimization methods (Bayesian Optimization, Evolutionary Algorithms), random search and human expert designs. Experiments on transfer learning between five technology nodes and two circuit topologies demonstrate that RL with transfer learning can achieve much higher FoMs than methods without knowledge transfer. Our transferable optimization method makes transistor sizing and design porting more effective and efficient.},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {201},
numpages = {6},
keywords = {circuit design automation, graph neural network, transistor sizing, reinforcement learning, transfer learning},
location = {Virtual Event, USA},
series = {DAC '20}
}

@article{10.1145/2623372,
author = {Wang, Xinxi and Wang, Yi and Hsu, David and Wang, Ye},
title = {Exploration in Interactive Personalized Music Recommendation: A Reinforcement Learning Approach},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/2623372},
doi = {10.1145/2623372},
abstract = {Current music recommender systems typically act in a greedy manner by recommending songs with the highest user ratings. Greedy recommendation, however, is suboptimal over the long term: it does not actively gather information on user preferences and fails to recommend novel songs that are potentially interesting. A successful recommender system must balance the needs to explore user preferences and to exploit this information for recommendation. This article presents a new approach to music recommendation by formulating this exploration-exploitation trade-off as a reinforcement learning task. To learn user preferences, it uses a Bayesian model that accounts for both audio content and the novelty of recommendations. A piecewise-linear approximation to the model and a variational inference algorithm help to speed up Bayesian inference. One additional benefit of our approach is a single unified model for both music recommendation and playlist generation. We demonstrate the strong potential of the proposed approach with simulation results and a user study.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {sep},
articleno = {7},
numpages = {22},
keywords = {model, music, Recommender systems, machine learning, application}
}

@inproceedings{10.5555/3433701.3433742,
author = {Zhang, Di and Dai, Dong and He, Youbiao and Bao, Forrest Sheng and Xie, Bing},
title = {RLScheduler: An Automated HPC Batch Job Scheduler Using Reinforcement Learning},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {Today's high-performance computing (HPC) platforms are still dominated by batch jobs. Accordingly, effective batch job scheduling is crucial to obtain high system efficiency. Existing HPC batch job schedulers typically leverage heuristic priority functions to prioritize and schedule jobs. But, once configured and deployed by the experts, such priority functions can hardly adapt to the changes of job loads, optimization goals, or system settings, potentially leading to degraded system efficiency when changes occur. To address this fundamental issue, we present RLScheduler, an automated HPC batch job scheduler built on reinforcement learning. RLScheduler relies on minimal manual interventions or expert knowledge, but can learn high-quality scheduling policies via its own continuous 'trial and error'. We introduce a new kernel-based neural network structure and trajectory filtering mechanism in RLScheduler to improve and stabilize the learning process. Through extensive evaluations, we confirm that RLScheduler can learn high-quality scheduling policies towards various workloads and various optimization goals with relatively low computation cost. Moreover, we show that the learned models perform stably even when applied to unseen workloads, making them practical for production use.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {31},
numpages = {15},
location = {Atlanta, Georgia},
series = {SC '20}
}

@inproceedings{10.1145/3360322.3360857,
author = {Ding, Xianzhong and Du, Wan and Cerpa, Alberto},
title = {OCTOPUS: Deep Reinforcement Learning for Holistic Smart Building Control},
year = {2019},
isbn = {9781450370059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360322.3360857},
doi = {10.1145/3360322.3360857},
abstract = {Recently, significant efforts have been done to improve quality of comfort for commercial buildings' users while also trying to reduce energy use and costs. Most of these efforts have concentrated in energy efficient control of the HVAC (Heating, Ventilation, and Air conditioning) system, which is usually the core system in charge of controlling buildings' conditioning and ventilation. However, in practice, HVAC systems alone cannot control every aspect of conditioning and comfort that affects buildings' occupants. Modern lighting, blind and window systems, usually considered as independent systems, when present, can significantly affect building energy use, and perhaps more importantly, user comfort in terms of thermal, air quality and illumination conditions. For example, it has been shown that a blind system can provide 12\%~35\% reduction in cooling load in summer while also improving visual comfort.In this paper, we take a holistic approach to deal with the trade-offs between energy use and comfort in commercial buildings. We developed a system called OCTOPUS, which employs a novel deep reinforcement learning (DRL) framework that uses a data-driven approach to find the optimal control sequences of all building's subsystems, including HVAC, lighting, blind and window systems. The DRL architecture includes a novel reward function that allows the framework to explore the trade-offs between energy use and users' comfort, while at the same time enable the solution of the high-dimensional control problem due to the interactions of four different building subsystems. In order to cope with OCTOPUS's data training requirements, we argue that calibrated simulations that match the target building operational points are the vehicle to generate enough data to be able to train our DRL framework to find the control solution for the target building. In our work, we trained OCTOPUS with 10-year weather data and a building model that is implemented in the EnergyPlus building simulator, which was calibrated using data from a real production building. Through extensive simulations we demonstrate that OCTOPUS can achieve 14.26\% and 8.1\% energy savings compared with the state-of-the art rule-based method in a LEED Gold Certified building and the latest DRL-based method available in the literature respectively, while maintaining human comfort within a desired range.},
booktitle = {Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {326–335},
numpages = {10},
keywords = {energy efficiency, deep reinforcement learning, HVAC control},
location = {New York, NY, USA},
series = {BuildSys '19}
}

@inproceedings{10.1145/3304112.3325607,
author = {Zhang, Rui-Xiao and Huang, Tianchi and Ma, Ming and Pang, Haitian and Yao, Xin and Wu, Chenglei and Sun, Lifeng},
title = {Enhancing the Crowdsourced Live Streaming: A Deep Reinforcement Learning Approach},
year = {2019},
isbn = {9781450362986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3304112.3325607},
doi = {10.1145/3304112.3325607},
abstract = {With the growing demand for crowdsourced live streaming (CLS), how to schedule the large-scale dynamic viewers effectively among different Content Delivery Network (CDN) providers has become one of the most significant challenges for CLS platforms. Although abundant algorithms have been proposed in recent years, they suffer from a critical limitation: due to their inaccurate feature engineering or naive rules, they cannot optimally schedule viewers. To address this concern, we propose LTS (Learn to schedule), a deep reinforcement learning (DRL) based scheduling approach that can dynamically adapt to the variation of both viewer traffics and CDN performance. After the extensive evaluation the real data from a leading CLS platform in China, we demonstrate that LTS improves the average quality of experience (QoE) over state-of-the-art approach by 8.71\%-15.63\%.},
booktitle = {Proceedings of the 29th ACM Workshop on Network and Operating Systems Support for Digital Audio and Video},
pages = {55–60},
numpages = {6},
keywords = {reinforcement learning, crowdsourced live streaming, scheduling},
location = {Amherst, Massachusetts},
series = {NOSSDAV '19}
}

@inproceedings{10.1109/ASE.2019.00077,
author = {Zheng, Yan and Xie, Xiaofei and Su, Ting and Ma, Lei and Hao, Jianye and Meng, Zhaopeng and Liu, Yang and Shen, Ruimin and Chen, Yingfeng and Fan, Changjie},
title = {Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00077},
doi = {10.1109/ASE.2019.00077},
abstract = {Game testing has been long recognized as a notoriously challenging task, which mainly relies on manual playing and scripting based testing in game industry. Even until recently, automated game testing still remains to be largely untouched niche. A key challenge is that game testing often requires to play the game as a sequential decision process. A bug may only be triggered until completing certain difficult intermediate tasks, which requires a certain level of intelligence. The recent success of deep reinforcement learning (DRL) sheds light on advancing automated game testing, without human competitive intelligent support. However, the existing DRLs mostly focus on winning the game rather than game testing. To bridge the gap, in this paper, we first perform an in-depth analysis of 1349 real bugs from four real-world commercial game products. Based on this, we propose four oracles to support automated game testing, and further propose Wuji, an on-the-fly game testing framework, which leverages evolutionary algorithms, DRL and multi-objective optimization to perform automatic game testing. Wuji balances between winning the game and exploring the space of the game. Winning the game allows the agent to make progress in the game, while space exploration increases the possibility of discovering bugs. We conduct a large-scale evaluation on a simple game and two popular commercial games. The results demonstrate the effectiveness of Wuji in exploring space and detecting bugs. Moreover, Wuji found 3 previously unknown bugs1, which have been confirmed by the developers, in the commercial games.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {772–784},
numpages = {13},
keywords = {evolutionary multi-objective optimization, game testing, artificial intelligence, deep reinforcement learning},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1109/TNET.2019.2924471,
author = {Sciancalepore, Vincenzo and Costa-Perez, Xavier and Banchs, Albert},
title = {RL-NSB: Reinforcement Learning-Based 5G Network Slice Broker},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2924471},
doi = {10.1109/TNET.2019.2924471},
abstract = {Network slicing is considered one of the main pillars of the upcoming 5G networks. Indeed, the ability to slice a mobile network and tailor each slice to the needs of the corresponding tenant is envisioned as a key enabler for the design of future networks. However, this novel paradigm opens up to new challenges, such as isolation between network slices, the allocation of resources across them, and the admission of resource requests by network slice tenants. In this paper, we address this problem by designing the following building blocks for supporting network slicing: i traffic and user mobility analysis, ii a learning and forecasting scheme per slice, iii optimal admission control decisions based on spatial and traffic information, and iv a reinforcement process to drive the system towards optimal states. In our framework, namely RL-NSB, infrastructure providers perform admission control considering the service level agreements SLA of the different tenants as well as their traffic usage and user distribution, and enhance the overall process by the means of learning and the reinforcement techniques that consider heterogeneous mobility and traffic models among diverse slices. Our results show that by relying on appropriately tuned forecasting schemes, our approach provides very substantial potential gains in terms of system utilization while meeting the tenants’ SLAs.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {1543–1557},
numpages = {15}
}

@inproceedings{10.1145/3274895.3274963,
author = {Schmoll, Sebastian and Schubert, Matthias},
title = {Vision Paper: Reinforcement Learning in Smart Spatio-Temporal Environments},
year = {2018},
isbn = {9781450358897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274895.3274963},
doi = {10.1145/3274895.3274963},
abstract = {Smart cities offer more and more real-time information provided by sensor networks and traffic cameras. This information can be very valuable for transportation planing. For instance, knowing which parking spots are currently available close to my destination is very valuable in order to reduce the travel time and thus, maximize the resource usage and minimize the traffic load. The future development of this information is usually uncertain. However, algorithms for routing applications should consider that new information will become available during travelling along the computed path. In order to exploit the provided information to a full extend, it is not sufficient to compute a static route or travel plan because the optimality of the plan might degrade as the state of the environment might consistently change. We argue that in order to plan transportation and understand observed trajectories in smart environments, it is necessary to compute action policies (i.e. the most promising action for all situations and in particular, the encountered situations) instead of static routes. Analogously, it makes sense to understand human behaviour based on the sequence of decisions in the encountered situations. To compute and analyse policies, the field of reinforcement learning already provides a rich set of tools. We describe existing approaches of reinforcement learning in spatial tasks and highlight which new challenges arise from developing reinforcement learning techniques for smart environments.},
booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {81–84},
numpages = {4},
keywords = {reinforcement learning, smart cities, real-time information, artificial intelligence, Markov decision process, routing},
location = {Seattle, Washington},
series = {SIGSPATIAL '18}
}

@inproceedings{10.5555/2343576.2343630,
author = {Rao, Karun and Whiteson, Shimon},
title = {V-MAX: Tempered Optimism for Better PAC Reinforcement Learning},
year = {2012},
isbn = {0981738117},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recent advances in reinforcement learning have yielded several PAC-MDP algorithms that, using the principle of optimism in the face of uncertainty, are guaranteed to act near-optimally with high probability on all but a polynomial number of samples. Unfortunately, many of these algorithms, such as R-MAX, perform poorly in practice because their initial exploration in each state, before the associated model parameters have been learned with confidence, is random. Others, such as Model-Based Interval Estimation (MBIE) have weaker sample complexity bounds and require careful parameter tuning. This paper proposes a new PAC-MDP algorithm called V-MAX designed to address these problems. By restricting its optimism to future visits, V-MAX can exploit its experience early in learning and thus obtain more cumulative reward than R-MAX. Furthermore, doing so does not compromise the quality of exploration, as we prove bounds on the sample complexity of V-MAX that are identical to those of R-MAX. Finally, we present empirical results in two domains demonstrating that V-MAX can substantially outperform R-MAX and match or outperform MBIE while being easier to tune, as its performance is invariant to conservative choices of its primary parameter.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {375–382},
numpages = {8},
keywords = {sample complexity, reinforcement learning},
location = {Valencia, Spain},
series = {AAMAS '12}
}

@inproceedings{10.5555/2484920.2485086,
author = {Torrey, Lisa and Taylor, Matthew},
title = {Teaching on a Budget: Agents Advising Agents in Reinforcement Learning},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper introduces a teacher-student framework for reinforcement learning. In this framework, a teacher agent instructs a student agent by suggesting actions the student should take as it learns. However, the teacher may only give such advice a limited number of times. We present several novel algorithms that teachers can use to budget their advice effectively, and we evaluate them in two experimental domains: Mountain Car and Pac-Man. Our results show that the same amount of advice, given at different moments, can have different effects on student learning, and that teachers can significantly affect student learning even when students use different learning methods and state representations.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1053–1060},
numpages = {8},
keywords = {reinforcement learning, agent teaching, advice taking},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@article{10.1145/3386569.3392433,
author = {Luo, Ying-Sheng and Soeseno, Jonathan Hans and Chen, Trista Pei-Chun and Chen, Wei-Chao},
title = {CARL: Controllable Agent with Reinforcement Learning for Quadruped Locomotion},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3386569.3392433},
doi = {10.1145/3386569.3392433},
abstract = {Motion synthesis in a dynamic environment has been a long-standing problem for character animation. Methods using motion capture data tend to scale poorly in complex environments because of their larger capturing and labeling requirement. Physics-based controllers are effective in this regard, albeit less controllable. In this paper, we present CARL, a quadruped agent that can be controlled with high-level directives and react naturally to dynamic environments. Starting with an agent that can imitate individual animation clips, we use Generative Adversarial Networks to adapt high-level controls, such as speed and heading, to action distributions that correspond to the original animations. Further fine-tuning through the deep reinforcement learning enables the agent to recover from unseen external perturbations while producing smooth transitions. It then becomes straightforward to create autonomous agents in dynamic environments by adding navigation modules over the entire process. We evaluate our approach by measuring the agent's ability to follow user control and provide a visual analysis of the generated motion to show its effectiveness.},
journal = {ACM Trans. Graph.},
month = {aug},
articleno = {38},
numpages = {10},
keywords = {deep reinforcement learning (DRL), quadruped, locomotion, generative adversarial network (GAN), motion synthesis}
}

@inproceedings{10.1145/3502181.3531470,
author = {Zhang, Di and Dai, Dong and Xie, Bing},
title = {SchedInspector: A Batch Job Scheduling Inspector Using Reinforcement Learning},
year = {2022},
isbn = {9781450391993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502181.3531470},
doi = {10.1145/3502181.3531470},
abstract = {Improving the performance of job executions is an important goal of HPC batch job schedulers, such as minimizing job waiting time, slowdown, or completion time. Such a goal is often accomplished using carefully designed heuristics based on job features, such as job size and job duration. However, these heuristics overlook important runtime factors (e.g., cluster availability and waiting job patterns), which may vary across time and make a previously sound scheduling decision not hold any longer. In this study, we propose a new approach to incorporate runtime factors into batch job scheduling for better job execution performance. The key idea is to add a scheduling inspector on top of the base job scheduler to scrutinize its scheduling decisions. The inspector will take the runtime factors into consideration and accordingly determine the fitness of the scheduled job. It then either accepts the scheduled job or rejects it and asks the base schedulers to try again later. We realize such an inspector, namely SchedInspector, by leveraging the intelligence of reinforcement learning. Through extensive experiments, we show SchedInspector can intelligently integrate the runtime factors into various batch job scheduling policies, including the state-of-the-art one, to gain better job execution performance, such as smaller average bounded job slowdown (up to 69\% better) or average job waiting time (up to 52\% better), across various real-world workloads. We also show that although rejecting scheduling decisions may leave the resources idle hence affect the system utilization, SchedInspector is able to achieve the job execution performance improvement with marginal impact on the system utilization (typically less than 1\%). We consider one key advantage of SchedInspector is it automatically learns to work with and improve existing job scheduling policies without changing them, which makes it promising to serve as a generic enhancer for various batch job scheduling policies.},
booktitle = {Proceedings of the 31st International Symposium on High-Performance Parallel and Distributed Computing},
pages = {97–109},
numpages = {13},
keywords = {high performance computing (hpc), reinforcement learning, batch job scheduling},
location = {Minneapolis, MN, USA},
series = {HPDC '22}
}

@inproceedings{10.1145/3447555.3466581,
author = {Heimerson, Albin and Br\"{a}nnvall, Rickard and Sj\"{o}lund, Johannes and Eker, Johan and Gustafsson, Jonas},
title = {Towards a Holistic Controller: Reinforcement Learning for Data Center Control},
year = {2021},
isbn = {9781450383332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447555.3466581},
doi = {10.1145/3447555.3466581},
abstract = {The increased use of cloud and other large scale datacenter IT services and the associated power usage has put the spotlight on more energy-efficient datacenter management. In this paper, a simple model was developed to represent the heat rejection system and energy usage in a small DC setup. The model was then controlled by a reinforcement learning agent that handles both the load balancing of the IT workload, as well as cooling system setpoints. The main contribution is the holistic approach to datacenter control where both facility metrics, IT hardware metric and cloud service logs are used as inputs. The application of reinforcement learning in the proposed holistic setup is feasible and achieves results that outperform standard algorithms. The paper presents both the simplified DC model and the reinforcement learning agent in detail and discusses how this work can be extended towards a richer datacenter model.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Future Energy Systems},
pages = {424–429},
numpages = {6},
keywords = {reinforcement learning, power optimization, load balancing, datacenter},
location = {Virtual Event, Italy},
series = {e-Energy '21}
}

@inproceedings{10.1145/3383455.3422571,
author = {Bisi, Lorenzo and Liotet, Pierre and Sabbioni, Luca and Reho, Gianmarco and Montali, Nico and Restelli, Marcello and Corno, Cristiana},
title = {Foreign Exchange Trading: A Risk-Averse Batch Reinforcement Learning Approach},
year = {2021},
isbn = {9781450375849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383455.3422571},
doi = {10.1145/3383455.3422571},
abstract = {Automated Trading Systems' impact on financial markets is ever growing, particularly on the intraday Foreign Exchange market. Historically, the FX trading systems are based on advanced statistical methods and technical analysis able to extract trading signals from financial data. In this work, we explore how to find a trading strategy via Reinforcement Learning by means of a state-of-the-art batch algorithm, Fitted Q-Iteration. Furthermore, we include a Multi-Objective formulation of the problem to keep the risk of noisy profits under control. We show that the algorithm is able to detect favorable temporal patterns, which are used by the agent to maximize the return. Finally, we show that as risk aversion increases, the resulting policies become smoother, as the portfolio positions are held for longer periods.},
booktitle = {Proceedings of the First ACM International Conference on AI in Finance},
articleno = {26},
numpages = {8},
keywords = {fitted Q iteration, extra-trees, FX-trading, risk-aversion, reinforcement learning, multi objective},
location = {New York, New York},
series = {ICAIF '20}
}

@inproceedings{10.1145/3383668.3419938,
author = {Arzate Cruz, Christian and Igarashi, Takeo},
title = {MarioMix: Creating Aligned Playstyles for Bots with Interactive Reinforcement Learning},
year = {2020},
isbn = {9781450375870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383668.3419938},
doi = {10.1145/3383668.3419938},
abstract = {In this paper, we propose a generic framework that enables game developers without knowledge of machine learning to create bot behaviors with playstyles that align with their preferences. Our framework is based on interactive reinforcement learning (RL), and we used it to create a behavior authoring tool called MarioMix. This tool enables non-experts to create bots with varied playstyles for the game titled Super Mario Bros. The main interaction procedure of MarioMix consists of presenting short clips of gameplay displaying precomputed bots with different playstyles to end-users. Then, end-users can select the bot with the playstyle that behaves as intended. We evaluated MarioMix by incorporating input from game designers working in the industry.},
booktitle = {Extended Abstracts of the 2020 Annual Symposium on Computer-Human Interaction in Play},
pages = {134–139},
numpages = {6},
keywords = {interactive reinforcement learning, human-agent interaction, interactive machine learning},
location = {Virtual Event, Canada},
series = {CHI PLAY '20}
}

@inproceedings{10.5555/3306127.3332066,
author = {Tomar, Manan and Sathuluri, Akhil and Ravindran, Balaraman},
title = {MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Shaping in humans and animals has been shown to be a powerful tool for learning complex tasks as compared to learning in a randomized fashion. This makes the problem less complex and enables one to solve the easier sub task at hand first. Generating a curriculum for such guided learning involves subjecting the agent to easier goals first, and then gradually increasing their difficulty. This paper takes a similar direction and proposes a dual curriculum scheme for solving robotic manipulation tasks with sparse rewards, called MaMiC. It includes a macro curriculum scheme which divides the task into multiple sub-tasks followed by a micro curriculum scheme which enables the agent to learn between such discovered sub-tasks. We show how combining macro and micro curriculum strategies help in overcoming major exploratory constraints considered in robot manipulation tasks without having to engineer any complex rewards. The performance of such a dual curriculum scheme is analyzed on the Fetch environments.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2226–2228},
numpages = {3},
keywords = {curriculum learning, reinforcement learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{10.1145/3072959.3073602,
author = {Peng, Xue Bin and Berseth, Glen and Yin, Kangkang and Van De Panne, Michiel},
title = {DeepLoco: Dynamic Locomotion Skills Using Hierarchical Deep Reinforcement Learning},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3072959.3073602},
doi = {10.1145/3072959.3073602},
abstract = {Learning physics-based locomotion skills is a difficult problem, leading to solutions that typically exploit prior knowledge of various forms. In this paper we aim to learn a variety of environment-aware locomotion skills with a limited amount of prior knowledge. We adopt a two-level hierarchical control framework. First, low-level controllers are learned that operate at a fine timescale and which achieve robust walking gaits that satisfy stepping-target and style objectives. Second, high-level controllers are then learned which plan at the timescale of steps by invoking desired step targets for the low-level controller. The high-level controller makes decisions directly based on high-dimensional inputs, including terrain maps or other suitable representations of the surroundings. Both levels of the control policy are trained using deep reinforcement learning. Results are demonstrated on a simulated 3D biped. Low-level controllers are learned for a variety of motion styles and demonstrate robustness with respect to force-based disturbances, terrain variations, and style interpolation. High-level controllers are demonstrated that are capable of following trails through terrains, dribbling a soccer ball towards a target location, and navigating through static or dynamic obstacles.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {41},
numpages = {13},
keywords = {physics-based character animation, motion control, locomotion skills}
}

@article{10.5555/3586589.3586844,
author = {Dong, Shi and Van Roy, Benjamin and Zhou, Zhengyuan},
title = {Simple Agent, Complex Environment: Efficient Reinforcement Learning with Agent States},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {We design a simple reinforcement learning (RL) agent that implements an optimistic version of Q-learning and establish through regret analysis that this agent can operate with some level of competence in any environment. While we leverage concepts from the literature on provably efficient RL, we consider a general agent-environment interface and provide a novel agent design and analysis. This level of generality positions our results to inform the design of future agents for operation in complex real environments. We establish that, as time progresses, our agent performs competitively relative to policies that require longer times to evaluate. The time it takes to approach asymptotic performance is polynomial in the complexity of the agent's state representation and the time required to evaluate the best policy that the agent can represent. Notably, there is no dependence on the complexity of the environment. The ultimate per-period performance loss of the agent is bounded by a constant multiple of a measure of distortion introduced by the agent's state representation. This work is the first to establish that an algorithm approaches this asymptotic condition within a tractable time frame.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {255},
numpages = {54},
keywords = {Q-learning, dynamic programming, reinforcement learning, regret analysis, agent design}
}

@inproceedings{10.5555/3539845.3539947,
author = {Wan, Zishen and Anwar, Aqeel and Mahmoud, Abdulrahman and Jia, Tianyu and Hsiao, Yu-Shun and Reddi, Vijay Janapa and Raychowdhury, Arijit},
title = {FRL-FI: Transient Fault Analysis for Federated Reinforcement Learning-Based Navigation Systems},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Swarm intelligence is being increasingly deployed in autonomous systems, such as drones and unmanned vehicles. Federated reinforcement learning (FRL), a key swarm intelligence paradigm where agents interact with their own environments and cooperatively learn a consensus policy while preserving privacy, has recently shown potential advantages and gained popularity. However, transient faults are increasing in the hardware system with continuous technology node scaling and can pose threats to FRL systems. Meanwhile, conventional redundancy-based protection methods are challenging to deploy on resource-constrained edge applications. In this paper, we experimentally evaluate the fault tolerance of FRL navigation systems at various scales with respect to fault models, fault locations, learning algorithms, layer types, communication intervals, and data types at both training and inference stages. We further propose two cost-effective fault detection and recovery techniques that can achieve up to 3.3x improvement in resilience with &lt;2.7\% overhead in FRL systems.},
booktitle = {Proceedings of the 2022 Conference \&amp; Exhibition on Design, Automation \&amp; Test in Europe},
pages = {430–435},
numpages = {6},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.1145/3372224.3419186,
author = {Zhang, Huanhuan and Zhou, Anfu and Lu, Jiamin and Ma, Ruoxuan and Hu, Yuhan and Li, Cong and Zhang, Xinyu and Ma, Huadong and Chen, Xiaojiang},
title = {OnRL: Improving Mobile Video Telephony via Online Reinforcement Learning},
year = {2020},
isbn = {9781450370851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372224.3419186},
doi = {10.1145/3372224.3419186},
abstract = {Machine learning models, particularly reinforcement learning (RL), have demonstrated great potential in optimizing video streaming applications. However, the state-of-the-art solutions are limited to an "offline learning" paradigm, i.e., the RL models are trained in simulators and then are operated in real networks. As a result, they inevitably suffer from the simulation-to-reality gap, showing far less satisfactory performance under real conditions compared with simulated environment. In this work, we close the gap by proposing OnRL, an online RL framework for real-time mobile video telephony. OnRL puts many individual RL agents directly into the video telephony system, which make video bitrate decisions in real-time and evolve their models over time. OnRL then aggregates these agents to form a high-level RL model that can help each individual to react to unseen network conditions. Moreover, OnRL incorporates novel mechanisms to handle the adverse impacts of inherent video traffic dynamics, and to eliminate risks of quality degradation caused by the RL model's exploration attempts. We implement OnRL on a mainstream operational video telephony system, Alibaba Taobao-live. In a month-long evaluation with 543 hours of video sessions from 151 real-world mobile users, OnRL outperforms the prior algorithms significantly, reducing video stalling rate by 14.22\% while maintaining similar video quality.},
booktitle = {Proceedings of the 26th Annual International Conference on Mobile Computing and Networking},
articleno = {29},
numpages = {14},
keywords = {video telephony, online learning, reinforcement learning},
location = {London, United Kingdom},
series = {MobiCom '20}
}

@inproceedings{10.1145/1329125.1329351,
author = {Ahmadi, Mazda and Taylor, Matthew E. and Stone, Peter},
title = {IFSA: Incremental Feature-Set Augmentation for Reinforcement Learning Tasks},
year = {2007},
isbn = {9788190426275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1329125.1329351},
doi = {10.1145/1329125.1329351},
abstract = {Reinforcement learning is a popular and successful framework for many agent-related problems because only limited environmental feedback is necessary for learning. While many algorithms exist to learn effective policies in such problems, learning is often used to solve real world problems, which typically have large state spaces, and therefore suffer from the "curse of dimensionality." One effective method for speeding-up reinforcement learning algorithms is to leverage expert knowledge. In this paper, we propose a method for dynamically augmenting the agent's feature set in order to speed up value-function-based reinforcement learning. The domain expert divides the feature set into a series of subsets such that a novel problem concept can be learned from each successive subset. Domain knowledge is also used to order the feature subsets in order of their importance for learning. Our algorithm uses the ordered feature subsets to learn tasks significantly faster than if the entire feature set is used from the start. Incremental Feature-Set Augmentation (IFSA) is fully implemented and tested in three different domains: Gridworld, Blackjack and RoboCup Soccer Keepaway. All experiments show that IFSA can significantly speed up learning and motivates the applicability of this novel RL method.},
booktitle = {Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems},
articleno = {186},
numpages = {8},
keywords = {reinforcement learning},
location = {Honolulu, Hawaii},
series = {AAMAS '07}
}

@inproceedings{10.1145/1449764.1449811,
author = {Simpkins, Christopher and Bhat, Sooraj and Isbell, Charles and Mateas, Michael},
title = {Towards Adaptive Programming: Integrating Reinforcement Learning into a Programming Language},
year = {2008},
isbn = {9781605582153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1449764.1449811},
doi = {10.1145/1449764.1449811},
abstract = {Current programming languages and software engineering paradigms are proving insufficient for building intelligent multi-agent systems--such as interactive games and narratives--where developers are called upon to write increasingly complex behavior for agents in dynamic environments. A promising solution is to build adaptive systems; that is, to develop software written specifically to adapt to its environment by changing its behavior in response to what it observes in the world. In this paper we describe a new programming language, An Adaptive Behavior Language (A2BL), that implements adaptive programming primitives to support partial programming, a paradigm in which a programmer need only specify the details of behavior known at code-writing time, leaving the run-time system to learn the rest. Partial programming enables programmers to more easily encode software agents that are difficult to write in existing languages that do not offer language-level support for adaptivity. We motivate the use of partial programming with an example agent coded in a cutting-edge, but non-adaptive agent programming language (ABL), and show how A2BL can encode the same agent much more naturally.},
booktitle = {Proceedings of the 23rd ACM SIGPLAN Conference on Object-Oriented Programming Systems Languages and Applications},
pages = {603–614},
numpages = {12},
keywords = {reinforcement learning, adaptive programming, partial programming, object-oriented programming},
location = {Nashville, TN, USA},
series = {OOPSLA '08}
}

@article{10.1145/1449955.1449811,
author = {Simpkins, Christopher and Bhat, Sooraj and Isbell, Charles and Mateas, Michael},
title = {Towards Adaptive Programming: Integrating Reinforcement Learning into a Programming Language},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/1449955.1449811},
doi = {10.1145/1449955.1449811},
abstract = {Current programming languages and software engineering paradigms are proving insufficient for building intelligent multi-agent systems--such as interactive games and narratives--where developers are called upon to write increasingly complex behavior for agents in dynamic environments. A promising solution is to build adaptive systems; that is, to develop software written specifically to adapt to its environment by changing its behavior in response to what it observes in the world. In this paper we describe a new programming language, An Adaptive Behavior Language (A2BL), that implements adaptive programming primitives to support partial programming, a paradigm in which a programmer need only specify the details of behavior known at code-writing time, leaving the run-time system to learn the rest. Partial programming enables programmers to more easily encode software agents that are difficult to write in existing languages that do not offer language-level support for adaptivity. We motivate the use of partial programming with an example agent coded in a cutting-edge, but non-adaptive agent programming language (ABL), and show how A2BL can encode the same agent much more naturally.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {603–614},
numpages = {12},
keywords = {reinforcement learning, partial programming, adaptive programming, object-oriented programming}
}

@inproceedings{10.5555/2772879.2772954,
author = {Liebman, Elad and Saar-Tsechansky, Maytal and Stone, Peter},
title = {DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In recent years, there has been growing focus on the study of automated recommender systems. Music recommendation systems serve as a prominent domain for such works, both from an academic and a commercial perspective. A fundamental aspect of music perception is that music is experienced in temporal context and in sequence. In this work we present DJ-MC, a novel reinforcement-learning framework for music recommendation that does not recommend songs individually but rather song sequences, or playlists, based on a model of preferences for both songs and song transitions. The model is learned online and is uniquely adapted for each listener. To reduce exploration time, DJ-MC exploits user feedback to initialize a model, which it subsequently updates by reinforcement. We evaluate our framework with human participants using both real song and playlist data. Our results indicate that DJ-MC's ability to recommend sequences of songs provides a significant improvement over more straightforward approaches, which do not take transitions into account.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {591–599},
numpages = {9},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.5555/3237383.3238074,
author = {Saunders, William and Sastry, Girish and Stuhlm\"{u}ller, Andreas and Evans, Owain},
title = {Trial without Error: Towards Safe Reinforcement Learning via Human Intervention},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {During training, model-free reinforcement learning (RL) systems can explore actions that lead to harmful or costly consequences. Having a human "in the loop'' and ready to intervene at all times can prevent these mistakes, but is prohibitively expensive for current algorithms. We explore how human oversight can be combined with a supervised learning system to prevent catastrophic events during training. We demonstrate this scheme on Atari games, with a Deep RL agent being overseen by a human for four hours. When the class of catastrophes is simple, we are able to prevent all catastrophes without affecting the agent's learning (whereas an RL baseline fails due to catastrophic forgetting).},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2067–2069},
numpages = {3},
keywords = {human-in-the-loop planning/learning, deep learning, safe exploration, reinforcement learning},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/3404835.3462905,
author = {Li, Yuanchun and Riva, Oriana},
title = {Glider: A Reinforcement Learning Approach to Extract UI Scripts from Websites},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462905},
doi = {10.1145/3404835.3462905},
abstract = {Web automation scripts (tasklets) are used by personal AI assistants to carry out human tasks such as reserving a car or buying movie tickets. Generating tasklets today is a tedious job which requires much manual effort. We propose Glider, an automated and scalable approach to generate tasklets from a natural language task query and a website URL. A major advantage of Glider is that it does not require any pre-training. Glider models tasklet extraction as a state space search, where agents can explore a website's UI and get rewarded when making progress towards task completion. The reward is computed based on the agent's navigating pattern and the similarity between its trajectory and the task query. A hierarchical reinforcement learning policy is used to efficiently find the action sequences that maximize the reward. To evaluate Glider, we used it to extract tasklets for tasks in various categories (shopping, real-estate, flights, etc.); in 79\% of cases a correct tasklet was generated.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1420–1430},
numpages = {11},
keywords = {web interfaces, ui script, task completion, reinforcement learning},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@article{10.1162/evco.2007.15.3.369,
author = {Mabu, Shingo and Hirasawa, Kotaro and Hu, Jinglu},
title = {A Graph-Based Evolutionary Algorithm: Genetic Network Programming (GNP) and Its Extension Using Reinforcement Learning},
year = {2007},
issue_date = {Fall 2007},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {15},
number = {3},
issn = {1063-6560},
url = {https://doi.org/10.1162/evco.2007.15.3.369},
doi = {10.1162/evco.2007.15.3.369},
abstract = {This paper proposes a graph-based evolutionary algorithm called Genetic Network Programming (GNP). Our goal is to develop GNP, which can deal with dynamic environments efficiently and effectively, based on the distinguished expression ability of the graph (network) structure. The characteristics of GNP are as follows. 1) GNP programs are composed of a number of nodes which execute simple judgment/processing, and these nodes are connected by directed links to each other. 2) The graph structure enables GNP to re-use nodes, thus the structure can be very compact. 3) The node transition of GNP is executed according to its node connections without any terminal nodes, thus the past history of the node transition affects the current node to be used and this characteristic works as an implicit memory function. These structural characteristics are useful for dealing with dynamic environments. Furthermore, we propose an extended algorithm, “GNP with Reinforcement Learning (GNPRL)” which combines evolution and reinforcement learning in order to create effective graph structures and obtain better results in dynamic environments. In this paper, we applied GNP to the problem of determining agents' behavior to evaluate its effectiveness. Tileworld was used as the simulation environment. The results show some advantages for GNP over conventional methods.},
journal = {Evol. Comput.},
month = {sep},
pages = {369–398},
numpages = {30},
keywords = {tileworld, reinforcement learning, Evolutionary computation, graph structure, agent}
}

@inproceedings{10.5555/3306127.3331761,
author = {Krening, Samantha and Feigh, Karen M.},
title = {Newtonian Action Advice: Integrating Human Verbal Instruction with Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {A goal of Interactive Machine Learning is to enable people without specialized training to teach agents how to perform tasks. Many of the existing algorithms that learn from human instructions are evaluated using simulated feedback and focus on how quickly the agent learns. While this is valuable information, it ignores important aspects of the human-agent interaction such as frustration. To correct this, we propose a method for the design and verification of interactive algorithms that includes a human-subject study that measures the human's experience working with the agent. In this paper, we present Newtonian Action Advice, a method of incorporating human verbal action advice with Reinforcement Learning in a way that improves the human-agent interaction. In addition to simulations, we validated the Newtonian Action Advice algorithm by conducting a human-subject experiment. The results show that Newtonian Action Advice can perform better than Policy Shaping, a state-of-the-art IML algorithm, both in terms of RL metrics like cumulative reward and human factors metrics like frustration.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {720–727},
numpages = {8},
keywords = {natural language interface, learning from human teachers, interactive machine learning, verification, reinforcement learning, human-subject experiment},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3336191.3371801,
author = {Zou, Lixin and Xia, Long and Du, Pan and Zhang, Zhuo and Bai, Ting and Liu, Weidong and Nie, Jian-Yun and Yin, Dawei},
title = {Pseudo Dyna-Q: A Reinforcement Learning Framework for Interactive Recommendation},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371801},
doi = {10.1145/3336191.3371801},
abstract = {Applying reinforcement learning (RL) in recommender systems is attractive but costly due to the constraint of the interaction with real customers, where performing online policy learning through interacting with real customers usually harms customer experiences. A practical alternative is to build a recommender agent offline from logged data, whereas directly using logged data offline leads to the problem of selection bias between logging policy and the recommendation policy. The existing direct offline learning algorithms, such as Monte Carlo methods and temporal difference methods are either computationally expensive or unstable on convergence. To address these issues, we propose Pseudo Dyna-Q (PDQ). In PDQ, instead of interacting with real customers, we resort to a customer simulator, referred to as the World Model, which is designed to simulate the environment and handle the selection bias of logged data. During policy improvement, the World Model is constantly updated and optimized adaptively, according to the current recommendation policy. This way, the proposed PDQ not only avoids the instability of convergence and high computation cost of existing approaches but also provides unlimited interactions without involving real customers. Moreover, a proved upper bound of empirical error of reward function guarantees that the learned offline policy has lower bias and variance. Extensive experiments demonstrated the advantages of PDQ on two real-world datasets against state-of-the-arts methods.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {816–824},
numpages = {9},
keywords = {pseudo dyna-q, customer simulator, recommender systems, model-based reinforcement learning, offline policy learning},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{10.1145/1555228.1555263,
author = {Rao, Jia and Bu, Xiangping and Xu, Cheng-Zhong and Wang, Leyi and Yin, George},
title = {VCONF: A Reinforcement Learning Approach to Virtual Machines Auto-Configuration},
year = {2009},
isbn = {9781605585642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1555228.1555263},
doi = {10.1145/1555228.1555263},
abstract = {Virtual machine (VM) technology enables multiple VMs to share resources on the same host. Resources allocated to the VMs should be re-configured dynamically in response to the change of application demands or resource supply. Because VM execution involves privileged domain and VM monitor, this causes uncertainties in VMs' resource to performance mapping and poses challenges in online determination of appropriate VM configurations. In this paper, we propose a reinforcement learning (RL) based approach, namely VCONF, to automate the VM configuration process. VCONF employs model-based RL algorithms to address the scalability and adaptability issues in applying RL in systems management. Experimental results on both controlled environments and a testbed of clouds with Xen VMs and representative server workloads demonstrate the effectiveness of VCONF. The approach is able to find optimal (near optimal) configurations in small scale systems and shows good adaptability and scalability.},
booktitle = {Proceedings of the 6th International Conference on Autonomic Computing},
pages = {137–146},
numpages = {10},
keywords = {virtual machines, autonomic computing, cloud computing, reinforcement learning},
location = {Barcelona, Spain},
series = {ICAC '09}
}

@inproceedings{10.1145/2449396.2449413,
author = {Glowacka, Dorota and Ruotsalo, Tuukka and Konuyshkova, Ksenia and Athukorala, kumaripaba and Kaski, Samuel and Jacucci, Giulio},
title = {Directing Exploratory Search: Reinforcement Learning from User Interactions with Keywords},
year = {2013},
isbn = {9781450319652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2449396.2449413},
doi = {10.1145/2449396.2449413},
abstract = {Techniques for both exploratory and known item search tend to direct only to more specific subtopics or individual documents, as opposed to allowing directing the exploration of the information space. We present an interactive information retrieval system that combines Reinforcement Learning techniques along with a novel user interface design to allow active engagement of users in directing the search. Users can directly manipulate document features (keywords) to indicate their interests and Reinforcement Learning is used to model the user by allowing the system to trade off between exploration and exploitation. This gives users the opportunity to more effectively direct their search nearer, further and following a direction. A task-based user study conducted with 20 participants comparing our system to a traditional query-based baseline indicates that our system significantly improves the effectiveness of information retrieval by providing access to more relevant and novel information without having to spend more time acquiring the information.},
booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
pages = {117–128},
numpages = {12},
keywords = {recommender systems, adaptive interfaces, data mining, machine learning, information filtering},
location = {Santa Monica, California, USA},
series = {IUI '13}
}

@inproceedings{10.1145/3408308.3427976,
author = {Pedasingu, Bala Suraj and Subramanian, Easwar and Bichpuriya, Yogesh and Sarangan, Venkatesh and Mahilong, Nidhisha},
title = {Bidding Strategy for Two-Sided Electricity Markets: A Reinforcement Learning Based Framework},
year = {2020},
isbn = {9781450380614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408308.3427976},
doi = {10.1145/3408308.3427976},
abstract = {We aim to increase the revenue or reduce the purchase cost of a given market participant in a double-sided, day-ahead, wholesale electricity market serving a smart city. Using an operations research based market clearing mechanism and attention based time series forecaster as sub-modules, we build a holistic interactive system. Through this system, we discover better bidding strategies for a market participant using reinforcement learning (RL). We relax several assumptions made in existing literature in order to make the problem setting more relevant to real life. Our Markov Decision Process (MDP) formulation enables us to tackle action space explosion and also compute optimal actions across time-steps in parallel. Our RL framework is generic enough to be used by either a generator or a consumer participating in the electricity market.We study the efficacy of the proposed RL based bidding framework from the perspective of a generator as well as a buyer on real world day-ahead electricity market data obtained from the European Power Exchange (EPEX). We compare the performance of our RL based bidding framework against three baselines: (a) an ideal but un-realizable bidding strategy; (b) a realizable approximate version of the ideal strategy; and (c) historical performance as found from the logs. Under both perspectives, we find that our RL based framework is more closer to the ideal strategy than other baselines. Further, the RL based framework improves the average daily revenue of the generator by nearly €7,200 (€2.64 M per year) and €9,000 (€3.28 M per year) over the realizable ideal and historical strategies respectively. When used on behalf of a buyer, it reduces average daily procurement cost by nearly €2,700 (€0.97 M per year) and €57,200 (€52.63 M per year) over the realizable ideal and historical strategies respectively. We also observe that our RL based framework automatically adapts its actions to changes in the market power of the participant.},
booktitle = {Proceedings of the 7th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {110–119},
numpages = {10},
keywords = {Reinforcement Learning, Electricity Markets, Optimization, Bidding, Forecasting},
location = {Virtual Event, Japan},
series = {BuildSys '20}
}

@inproceedings{10.1145/3297280.3297368,
author = {Talamini, Jacopo and Medvet, Eric and Bartoli, Alberto},
title = {Communication-Based Cooperative Tasks: How the Language Expressiveness Affects Reinforcement Learning},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297368},
doi = {10.1145/3297280.3297368},
abstract = {We consider a cooperative multi-agent system in which cooperation may be enforced by communication between agents but in which agents must learn to communicate. The system consists of a game in which agents may move in a 2D world and are given the task of reaching specified targets. Each agent knows the target of another agent but not its own, thus the only way to solve the task is for the agents to guide one another using communication and, in particular, by learning how to communicate. We cast this game in terms of a partially observed Markov game and show that agents may learn policies for moving and communicating in the form of a neural network by means of reinforcement learning. We investigate in depth the impact on the learning quality of the expressiveness of the language, which is a function of vocabulary size, number of agents and number of targets.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {898–905},
numpages = {8},
keywords = {neural networks, reinforcement learning, language expressiveness, multi-agent systems, agent communication},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.5555/2615731.2617491,
author = {Bhatia, Taranjeet Singh and Khan, Saad Ahmad and B\"{o}l\"{o}ni, Ladislau},
title = {The Education of a Crook: Reinforcement Learning in Social-Cultural Settings},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The ability to manipulate social and cultural values in order to achieve one's own goals is a hard-to-teach but profitable skill. In this paper we represent a complex social scenario, the Spanish Steps flower selling scam, using a social calculus framework based on culture sanctioned social metrics (CSSMs) and concrete beliefs (CBs). Then, we show how a crooked seller can learn a profitable strategy through reinforcement learning. Although the search space defined by the social calculus is large, we found that function approximation based Q-learning allows us to successfully learn efficient strategies in a relatively small number of runs. The learned strategy allows the seller to manipulate an unprepared tourist's social values of politeness and dignity, as well as his perception of the peers and crowds opinion. This allows the seller to manipulate some of his opponents to act against their own interests by purchasing an overpriced flower while well-knowing that they are being cheated.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1397–1398},
numpages = {2},
keywords = {social-cultural, multi-agents, simulation},
location = {Paris, France},
series = {AAMAS '14}
}

@inproceedings{10.5555/3306127.3331701,
author = {Bakker, Jasper and Hammond, Aron and Bloembergen, Daan and Baarslag, Tim},
title = {RLBOA: A Modular Reinforcement Learning Framework for Autonomous Negotiating Agents},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Negotiation is a complex problem, in which the variety of settings and opponents that may be encountered prohibits the use of a single predefined negotiation strategy. Hence the agent should be able to learn such a strategy autonomously. To this end we propose RLBOA, a modular framework that facilitates the creation of autonomous negotiation agents using reinforcement learning. The framework allows for the creation of agents that are capable of negotiating effectively in many different scenarios. To be able to cope with the large size of the state and action spaces and diversity of settings, we leverage the modular BOA-framework. This decouples the negotiation strategy into a Bidding strategy, an Opponent model and an Acceptance condition. Furthermore, we map the multidimensional contract space onto the utility axis which enables a compact and generic state and action description. We demonstrate the value of the RLBOA framework by implementing an agent that uses tabular Q-learning on the compressed state and action space to learn a bidding strategy. We show that the resulting agent is able to learn well-performing bidding strategies in a range of negotiation settings and is able to generalize across opponents and domains.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {260–268},
numpages = {9},
keywords = {learning agent-to-agent interactions (negotiation, trust, coordination), reinforcement learning, bargaining and negotiation},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3340531.3417422,
author = {Wang, Hongzhi and Qi, Zhixin and Zheng, Lei and Feng, Yun and Ouyang, Junfei and Zhang, Haoqi and Zhang, Xiangxi and Shen, Ziming and Liu, Shirong},
title = {April: An Automatic Graph Data Management System Based on Reinforcement Learning},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3417422},
doi = {10.1145/3340531.3417422},
abstract = {The great amount and complex structure of graph data bring a big challenge to graph data management. However, traditional management approaches cannot tackle the challenge. Fortunately, reinforcement learning provides a new approach to solve this problem due to its automation and adaptivity in decision making. Motivated by this, we develop April, an automatic graph data management system, which performs storage structure selection, index selection, and query optimization based on reinforcement learning. The system selects storage structure, indices effectively and automatically, and optimizes the SPARQL queries efficiently. April also offers a friendly interface for users, which allows users to interact with the system in a customized mode. We demonstrate the effectiveness and efficiency of April with two graph data benchmarks.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management},
pages = {3465–3468},
numpages = {4},
keywords = {index selection, storage structure selection, reinforcement learning, graph data management, query optimization},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

