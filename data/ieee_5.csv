"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Influence Graph based Task Decomposition and State Abstraction in Reinforcement Learning","L. Yu; F. Hong; P. Wang; Y. Xu; Y. Liu","School of Information Science and Engineering, Central South University, Changsha, China; School of Information Science and Engineering, Central South University, Changsha, China; School of Information Science and Engineering, Central South University, Changsha, China; School of Information Science and Engineering, Central South University, Changsha, China; School of Information Science and Engineering, Central South University, Changsha, China","2008 The 9th International Conference for Young Computer Scientists","12 Dec 2008","2008","","","136","141","Task decomposition and state abstraction are crucial parts in reinforcement learning. It allows an agent to ignore aspects of its current states that are irrelevant to its current decision, and therefore speeds up dynamic programming and learning. This paper presents the SVI algorithm that uses a dynamic Bayesian network model to construct an influence graph that indicates relationships between state variables. SVI performs state abstraction for each subtask by ignoring irrelevant state variables and lower level subtasks. Experiment results show that the decomposition of tasks introduced by SVI can significantly accelerate constructing a near-optimal policy. This general framework can be applied to a broad spectrum of complex real world problems such as robotics, industrial manufacturing, games and others.","","978-0-7695-3398-8","10.1109/ICYCS.2008.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4708962","reinforcement learning;dynamic Bayesian network;influence graph;task decomposition;SVI algorithm","Learning;Mice;Bayesian methods;Function approximation;Stochastic processes;Information science;Dynamic programming;Acceleration;Service robots;Manufacturing industries","Bayes methods;dynamic programming;graph theory;learning (artificial intelligence)","influence graph;task decomposition;state abstraction;reinforcement learning;dynamic programming;state variable influence algorithm;dynamic Bayesian network model","","1","","11","IEEE","12 Dec 2008","","","IEEE","IEEE Conferences"
"Reward Shaping-Based Actor–Critic Deep Reinforcement Learning for Residential Energy Management","R. Lu; Z. Jiang; H. Wu; Y. Ding; D. Wang; H. -T. Zhang","Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; Center for Applied Mathematics, Tianjin University, Tianjin, China; Department of Electrical and Electronic Engineering, University of Navarra, San Sebastian, Spain; Key Laboratory of Intelligent Control and Optimization for Industrial Equipment of Ministry of Education, Dalian University of Technology, Dalian, China; Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Industrial Informatics","8 Mar 2023","2023","19","3","2662","2673","Residential energy consumption continues to climb steadily, requiring intelligent energy management strategies to reduce power system pressures and residential electricity bills. However, it is challenging to design such strategies due to the random nature of electricity pricing, appliance demand, and user behavior. This article presents a novel reward shaping (RS)-based actor–critic deep reinforcement learning (ACDRL) algorithm to manage the residential energy consumption profile with limited information about the uncertain factors. Specifically, the interaction between the energy management center and various residential loads is modeled as a Markov decision process that provides a fundamental mathematical framework to represent the decision-making in situations where outcomes are partially random and partially influenced by the decision-maker control signals, in which the key elements containing the agent, environment, state, action, and reward are carefully designed, and the electricity price is considered as a stochastic variable. An RS-ACDRL algorithm is then developed, incorporating both the actor and critic network and an RS mechanism, to learn the optimal energy consumption schedules. Several case studies involving real-world data are conducted to evaluate the performance of the proposed algorithm. Numerical results demonstrate that the proposed algorithm outperforms state-of-the-art RL methods in terms of learning speed, solution optimality, and cost reduction.","1941-0050","","10.1109/TII.2022.3183802","National Key R&D Program of China(grant numbers:2021ZD0201300); National Natural Science Foundation of China(grant numbers:62003143,U2141235); Science and Technology Project of State Grid Corporation of China(grant numbers:5100-202199557A-0-5-ZN); Fundamental Research Funds for the Central Universities(grant numbers:HUST 2020kfyXJJS084,2022SMECP03); State Key Laboratory of Alternate Electrical Power System with Renewable Energy Sources(grant numbers:LAPS21006); Key Laboratory of Industrial Internet of Things and Networked Control(grant numbers:2020FF02); Open Fund of Hubei Key Laboratory of Mechanical Transmission and Manufacturing Engineering at Wuhan University of Science and Technology(grant numbers:MTMEOF2021B04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797851","Deep deterministic policy gradient;deep reinforcement learning;demand response;residential energy management;reward shaping (RS)","Energy management;Home appliances;Energy consumption;Costs;Schedules;Informatics;Load modeling","5G mobile communication;cost reduction;decision making;deep learning (artificial intelligence);demand side management;domestic appliances;energy consumption;energy management systems;learning (artificial intelligence);Markov processes;multi-access systems;optimisation;power engineering computing;pricing;reinforcement learning;resource allocation;telecommunication computing","appliance demand;critic network;decision-maker control signals;decision-making;electricity price;electricity pricing;energy management center;fundamental mathematical framework;intelligent energy management strategies;Markov decision process;optimal energy consumption schedules;power system pressures;residential electricity bills;residential energy consumption profile;residential energy management;residential loads;reward shaping-based actor-critic deep reinforcement;RS mechanism;RS-ACDRL algorithm;user behavior","","7","","36","IEEE","16 Jun 2022","","","IEEE","IEEE Journals"
"A New Reinforcement Learning Based Learning Rate Scheduler for Convolutional Neural Network in Fault Classification","L. Wen; X. Li; L. Gao","School of Mechanical Engineering and Electronic Information, China University of Geosciences, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Industrial Electronics","1 Sep 2021","2021","68","12","12890","12900","Convolutional neural network (CNN) has gained increasing attention in fault classification. However, the performance of CNN is sensitive to its learning rate. Some previous works have been done to tune the learning rate, including the “trial and error” and manual search, which heavily depend on the experts' experiences and should be conducted repeatedly on every dataset. Because of the variety of the fault data, it is time-consuming and labor intensive to use these traditional tuning methods for fault classification. To overcome this problem, in this article, we develop a novel learning rate scheduler based on the reinforcement learning (RL) for convolutional neural network (RL-CNN) in fault classification, which can schedule the learning rate efficiently and automatically. First, a new RL agent is designed to learn the policies about the learning rate adjustment during the training process. Second, a new structure of RL-CNN is developed to balance the exploration and exploitation of the agent. Third, the bagging ensemble version of RL-CNN (RL-CNN-Ens) is presented. Three bearing datasets are used to test the performance of RL-CNN-Ens. The results show that RL-CNN-Ens outperforms the traditional DLs and machine learning methods. Meanwhile, RL-CNN-Ens can find the state-of-the-art learning rate schedulers as human designed, showing its potential in fault classification.","1557-9948","","10.1109/TIE.2020.3044808","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2019YFB1704600); Natural Science Foundation of China(grant numbers:51775216); State Key Laboratory of Digital Manufacturing Equipment and Technology of Huazhong University of Science and Technology(grant numbers:DMETKF2020029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301217","Convolutional neural network (CNN);fault classification;learning rate;reinforcement learning (RL)","Tuning;Training;Reinforcement learning;Mathematical model;Convolutional neural networks;Manuals;Flowcharts","convolutional neural nets;fault diagnosis;learning (artificial intelligence);production engineering computing;scheduling","RL-CNN-Ens;machine learning methods;fault classification;convolutional neural network;fault data;learning rate adjustment;reinforcement learning based learning rate scheduler;RL-CNN bagging ensemble version","","53","","33","IEEE","21 Dec 2020","","","IEEE","IEEE Journals"
"Fast Simulation Method with Reinforcement Learning for Automated Optimization of Electronic Systems","",,"2023 IEEE Region 10 Symposium (TENSYMP)","28 Aug 2023","2023","","","1","6","Design automation of electronic systems is challenging due to the growing design space, high performance tradeoffs, and rapid technological advances. To solve this problem, this paper presents an automated optimization framework that combines Fast Simulation with deep reinforcement learning for automatic circuit design. Fast Simulation can quickly and accurately evaluate circuit performance by neural networks. Deep reinforcement learning is used to find optimal parameters in the design space. Compared with existing reinforcement learning methods, the proposed method can automatically generate labels for the optimization results of the reinforcement learning agent by the simulator to retrain the neural network. To this end, the proposed optimization method performs better designs and reduces the required number of simulations.","2642-6102","978-1-6654-8258-5","10.1109/TENSYMP55890.2023.10223650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10223650","","Deep learning;Training;Circuit optimization;Design methodology;Neural networks;Transfer learning;Reinforcement learning","deep learning (artificial intelligence);electronic design automation;optimisation;reinforcement learning","automated optimization framework;automatic circuit design;circuit performance;deep reinforcement learning;design automation;electronic systems;fast simulation method;neural network;reinforcement learning agent","","","","23","IEEE","28 Aug 2023","","","IEEE","IEEE Conferences"
"Demand Charge Control for Energy-intensive Enterprises based on Deep Reinforcement Learning","Q. Wang; F. Gao; K. Liu; F. Ming; Z. Xu; J. Wu","State Key Laboratory of Mechanical Manufacturing System Engineering, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory of Mechanical Manufacturing System Engineering, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory of Mechanical Manufacturing System Engineering, Xi’an Jiaotong University, Xi’an, China; The Moe Klinns Laboratory, Xi’an Jiaotong University, Xi’an, China; The Moe Klinns Laboratory, Xi’an Jiaotong University, Xi’an, China; The Moe Klinns Laboratory, Xi’an Jiaotong University, Xi’an, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","6791","6796","With the advancement of the dual-carbon goal and energy consumption revolution, demand-side management relying on smart grids has gradually become the focus of related research. Due to the obvious surging and high uncertainty of the power load of energy-intensive enterprises, it is difficult to obtain the best control strategy for demand charge. In this paper, we solve the problem of multi-equipment demand charge control for multi-batch tasks with discontinuous production by controlling the load of controllable equipment to reduce electricity costs. To solve the problems of long-time sequence with time coupled and complex systems with difficult modeling, we established a Markov decision process (MDP) model for real-time demand charge control innovatively. To avoid the curse of dimensionality caused by the increasing state space, we introduced the deep Q learning (DQN) algorithm, which successfully solves MDP problems with large state space. Moreover, we introduced constrained deep Q-learning (CDQN) aiming at a large number of action constraints in the problem, which selects the optimal action from the feasible action zone instead of the whole action space to improve the training efficiency and data utilization. Finally, we conducted experiments on simulation case experiments. Under the basic day-ahead production scheduling plan, real-time demand charge control can reduce costs by 10.4% compared with uncontrolled, indicating that this method has achieved excellent performance in obtaining demand charge control strategies.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728428","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728428","ultra-short-term scheduling;deep reinforcement learning;demand charge control","Training;Costs;Q-learning;Uncertainty;Production;Aerospace electronics;Real-time systems","decision theory;deep learning (artificial intelligence);energy management systems;Markov processes;power engineering computing;power markets;reinforcement learning;scheduling;smart power grids;stochastic programming","energy-intensive enterprises;deep reinforcement learning;dual-carbon goal;energy consumption revolution;demand-side management;control strategy;multiequipment demand charge;multibatch tasks;controllable equipment;long-time sequence;Markov decision process model;real-time demand charge control;deep Q learning algorithm;MDP problems","","","","11","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Approach for Capacitated Supply Chain optimization under Demand Uncertainty","Z. Peng; Y. Zhang; Y. Feng; T. Zhang; Z. Wu; H. Su","Institute of Cyber-Systems and Control Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control Zhejiang University, Hangzhou, China","2019 Chinese Automation Congress (CAC)","13 Feb 2020","2019","","","3512","3517","With the global trade competition becoming further intensified, Supply Chain Management (SCM) technology has become critical to maintain competitive advantages for enterprises. However, the economic integration and increased market uncertainty have brought great challenges to SCM. In this paper, two Deep Reinforcement Learning (DRL) based methods are proposed to solve multi-period capacitated supply chain optimization problem under demand uncertainty. The capacity constraints are satisfied from both modelling perspective and DRL algorithm perspective. Both continuous action space and discrete action space are considered. The performance of the methods is analyzed through the simulation of three different cases. Compared to the baseline of (r, Q) policy, the proposed methods show promising results for the supply chain optimization problem.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8997498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8997498","supply chain optimization;deep reinforcement learning;demand uncertainty;vanilla policy gradient","Supply chains;Optimization;Mathematical model;Uncertainty;Machine learning;Dynamic scheduling","globalisation;industrial economics;international trade;learning (artificial intelligence);optimisation;production engineering computing;supply chain management;supply chains","supply chain management technology;DRL algorithm;multiperiod capacitated supply chain optimization problem;market uncertainty;economic integration;SCM;global trade competition;demand uncertainty;deep reinforcement learning approach","","12","","19","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"AGV Path Planning Model based on Reinforcement Learning","X. Liao; Y. Wang; Y. Xuan; D. Wu","College of Information Sciences and Technology, Donghua University Engineering Research Center of Digitized Textile & Fashion Technology, Ministry of Education, Donghua University, Shanghai, China; College of Information Sciences and Technology, Donghua University, Shanghai, China; College of Information Sciences and Technology, Donghua University, Shanghai, China; College of Information Sciences and Technology, Donghua University, Shanghai, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","6722","6726","With the rapid growth of logistics transportation, automated guided vehicle (AGV) technology has developed speedily. Path planning is one of the key research topics of AGV. It is difficult to plan an optimal path from starting position to target position for AGV in the complex environment. In this paper, reinforcement learning technology is introduced to solve the problem that it is difficult to model AGV path planning due to complex and unknown environment. The Sarsa algorithm based on simulated annealing strategy can effectively guide AGV to plan the optimal path in the grid graph, and improve the success rate. Aiming at the problem that the traditional reinforcement learning algorithm processes data insufficiently in case of large-scale state space, the potential field method combined with deep q-network algorithm is proposed for AGV path planning. The algorithm can effectively guide AGV to carry out optimal path planning, and solve the problem that the traditional reinforcement learning algorithm can not deal with complex space. Finally, these algorithms are applied to the AGV path planning system to simulate the motion state of a single AGV from the loading point to the unloading point. It verifies that our algorithms can effectively implement the AGV intelligent path planning and improve the efficiency of warehousing logistics.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9326742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326742","AGV;path planning;reinforcement learning;Sarsa algorithm;deep Q network algorithm","Path planning;Reinforcement learning;Planning;Logistics;Automobiles;Neural networks;Training","automatic guided vehicles;graph theory;learning (artificial intelligence);logistics;mobile robots;path planning;production engineering computing;simulated annealing;warehousing","deep q-network algorithm;optimal path planning;AGV intelligent path planning;reinforcement learning technology;complex environment;unknown environment;simulated annealing strategy;Sarsa algorithm;warehousing logistics;logistics transportation;automated guided vehicle technology;grid graph","","6","","12","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Genetic reinforcement learning for scheduling heterogeneous machines","G. H. Kim; C. S. G. Lee","School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA","Proceedings of IEEE International Conference on Robotics and Automation","6 Aug 2002","1996","3","","2798","2803 vol.3","Concerns the development of a learning-based heuristic for scheduling heterogeneous machines. List scheduling methods are flexible enough to be used for a large class of problems, including the heterogeneous machine problem. However, designing a priority rule requires insight into the characteristics of the problem. We propose the iterative list scheduling, which refines priority rules while generating a number of schedules. We also show that the iterative list scheduling can be formulated as a reinforcement learning problem, defining states and actions. Due to the large number of possible states, reinforcement learning algorithms which use value functions in constructing an optimal policy may not be suitable for scheduling problems. Encoding the policies of reinforcement learning into genetic algorithms leads to the genetic reinforcement learning (GRL), which directly works with the policies rather than the values of states. A GRL-based scheduler, EVIS (Evolutionary Intracell Scheduler), has been applied to problems such as the heterogeneous machine scheduling, the job-shop scheduling, the flow-shop scheduling, and the open-shop scheduling problems. The proposed model of EVIS, which has the linear order of population-fitness convergence, was verified with computer experiments. Even without fine tuning of EVIS, the quality of solutions found by EVIS was comparable to that of problem-tailored heuristics for most of the problem instances.","1050-4729","0-7803-2988-0","10.1109/ROBOT.1996.506586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506586","","Learning;Job shop scheduling;Processor scheduling;Genetic algorithms;Iterative algorithms;Space exploration;Genetic engineering;Scheduling algorithm;Encoding;Biological cells","genetic algorithms;learning (artificial intelligence);heuristic programming;iterative methods;production control","genetic reinforcement learning;heterogeneous machine scheduling;learning-based heuristic;iterative list scheduling;reinforcement learning problem;optimal policy;EVIS;Evolutionary Intracell Scheduler;linear-order population-fitness convergence","","5","","16","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning toward Robust Multi-echelon Supply Chain Inventory Optimization","I. E. Shar; W. Sun; H. Wang; C. Gupta","University of Pittsburgh, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Hitachi America, Ltd, R&D, Industrial AI Lab; Hitachi America, Ltd, R&D, Industrial AI Lab","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","1385","1391","Multi-echelon supply chains (SC) are highly complex systems with many inherent uncertainties, including customer demands and transportation time from one location to another. In addition to the complex SC structures, inventory decisions made at different stages affect each other. Maintaining global-level optimal inventory levels along the entire supply chain that is robust to changing business needs remains challenging. In this work, we have developed a simulation environment, GymSC, for reinforcement learning of multi-echelon SC inventory policies. Using a series of model-free deep reinforcement learning algorithms, we trained dynamic SC policies that have significantly improved performance when compared with popularly used heuristics. The robustness of the learned policy is demonstrated by its adaptability to a previously unseen environment with non-stationary customer demands. The presented work showcases the effectiveness and robustness of deep reinforcement learning in solving important practical SC optimization problems. In addition, we present this configurable simulation environment as a platform for testing existing and new algorithms to develop robust inventory policies with a goal to encourage further integration of deep reinforcement learning into the SC management problems as a promising alternative for addressing inventory optimization with uncertainties.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926659","","Adaptation models;Uncertainty;Heuristic algorithms;Supply chains;Decision making;Transportation;Reinforcement learning","customer satisfaction;optimisation;production engineering computing;reinforcement learning;stock control;supply chain management;supply chains","robust multiechelon supply chain inventory optimization;model-free deep reinforcement learning algorithms;nonstationary customer demands;robust inventory policies;transportation time;GymSC","","1","","24","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"A new thermal power generation control in reinforcement learning","L. Zou; Z. Zhuang; Y. Cheng; Y. Huang; W. Zhang","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2018 Chinese Automation Congress (CAC)","24 Jan 2019","2018","","","1734","1739","In this paper, a novel framework applying the reinforcement learning algorithms to thermal power generation control is proposed instead of traditional PID controllers. In order to implement a time series response by the neural network, we discrete the control model and embed it into the environment. By defining an episode as the adjustment process of the steady state to the setpoint, the agent can learn the whole embedding model instead of only accepting scalar error signals. The experiments have proven that proximal policy optimization framework shows the same robustness and fast response as the traditional PID control in the single input single output(SISO) system and performs better in the multiple input multiple output(MIMO) system.","","978-1-7281-1312-8","10.1109/CAC.2018.8623337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8623337","reinforcement learning;thermal power;traditional control;PID","Coal;Mathematical model;Load modeling;Power generation;Reinforcement learning;Control systems;Automation","control engineering computing;learning (artificial intelligence);MIMO systems;neurocontrollers;optimisation;power generation control;power system control;single-input single-output systems (control);three-term control;time series","MIMO system;multiple input multiple output system;SISO system;single input single output system;neural network;PID controllers;time series response;reinforcement learning;thermal power generation control;proximal policy optimization framework;control model","","1","","14","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Robot Motion Skills Acquisition Method Based on GU-ProMPs and Reinforcement Learning","J. Fu; C. Wang; J. Du; F. Luo; C. Yang","School of Automation, Wuhan University of Technology, WuHan, China; School of Automation, Wuhan University of Technology, WuHan, China; School of Automation, Wuhan University of Technology, WuHan, China; School of Automation, Wuhan University of Technology, WuHan, China; College of Automation Science and Engineering, South China University of Technology, Guangzhou, China","2019 WRC Symposium on Advanced Robotics and Automation (WRC SARA)","16 Dec 2019","2019","","","302","308","This Aiming at robot motor skill acquisition problem, this paper will propose a GP-UKF probabilistic movement primitives (GU-ProMPs) learning frarnework, which combines with the learning from demonstration (LfD) and the policy improvements with path integrals (PI2). Specifically, when the parameterized model of ProMPs cannot be expressed linearly, we will replace the classical ProMPs linear representation with GU-ProMPs to acquire a nonlinear policy, as a result, the representation of imitation learning and the robustness of system are enhanced. Moreover, when the strategy combined with PI2, the tasks of adding additional constraints to the index set can be acquired automatically and completed with high quality. Based on the NAO and UR5 robot, the experimental results of classical proves the effectiveness and feasibility of the method.","","978-1-7281-5552-4","10.1109/WRC-SARA.2019.8931921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8931921","Learning from Demonstration;Reinforcement Learning;ProMPs;Path Integrals;UKF document","Automation","humanoid robots;human-robot interaction;learning (artificial intelligence);learning systems;mobile robots;motion control;path planning;probability;student experiments","path integrals;probabilistic movement primitives;UR5 robot;NAO;system robustness;LfD;learning from demonstration;ProMPs linear representation;imitation learning;PI2;policy improvements;movement primitives learning frarnework;GP-UKF;reinforcement learning;GU-ProMPs;robot motion skills acquisition method","","","","18","IEEE","16 Dec 2019","","","IEEE","IEEE Conferences"
"Traffic Signal Control Using Offline Reinforcement Learning","X. Dai; C. Zhao; X. Li; X. Wang; F. -Y. Wang","School of Artificial Intelligence, University of Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences; The State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","8090","8095","The problem of traffic signal control is essential but remains unsolved. Some researchers use online reinforcement learning, including the off-policy one, to derive an optimal control policy through interaction between agents and environments in simulation. However, it is difficult to deploy the policy in real transportation systems due to the gap between simulated and real traffic data. In this paper, we consider an offline reinforcement learning method to tackle the problem. First, we construct a realistic traffic environment and obtain offline data based on a classic actuated traffic signal controller. Then, we use an offline reinforcement learning algorithm, namely conservative Q-learning, to learn an efficient control policy via offline datasets. We conduct experiments on a typical road intersection and compare the conservative Q-learning policy with the actuated policy and two data-driven policies based on off-policy reinforcement learning and imitation learning. Empirical results indicate that in the offline-learning setting the conservative Q-learning policy performs significantly better than other baselines, including the actuated policy, but the other two data-driven policies perform poorly in test scenarios.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728551","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728551","Traffic signal control;offline reinforcement learning;off-policy reinforcement learning","Q-learning;Automation;Roads;Transportation;Optimal control;Cloning;Data models","reinforcement learning;road traffic control","traffic data;offline reinforcement learning method;realistic traffic environment;offline datasets;conservative Q-learning policy;actuated policy;data-driven policies;off-policy reinforcement learning;imitation learning;online reinforcement learning;optimal control policy;actuated traffic signal controller","","","","17","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Learning Task-independent Joint Control for Robotic Manipulators with Reinforcement Learning and Curriculum Learning","L. Væhrens; D. D. Álvarez; U. Berger; S. Bøgh","Dept. of Materials and Production, Aalborg University, Aalborg, Denmark; Mercedes-Benz AG Daimler AG, Stuttgart, Germany; Chair of Automation Technology, Brandenburg University of Technology, Cottbus, Germany; Dept. of Materials and Production, Aalborg University, Aalborg, Denmark","2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)","23 Mar 2023","2022","","","1250","1257","We present a deep reinforcement learning-based approach to control robotic manipulators and construct task-independent trajectories for point-to-point motions. The research objective in this work is to learn control in the joint action space, which can be generalized to various industrial manipulators. The approach necessitates that the neural network learns a mapping from joint movements to the reward landscape determined by the distance to the goal and nearby obstacles. In addition, curriculum learning is embedded in this approach to facilitate learning by reducing the complexity of the environment. Conducted experiments demonstrate how the reinforcement learning-based approach can be applied to three different industrial manipulators in simulation with minimal configuration changes. The results of our contribution demonstrate that a model can be trained in a simulation environment, transferred to the real world, and used in complex environments. Furthermore, the Sim2Real transfer, augmented by curriculum learning, highlights that the robots behave in the same way in the real world as in the simulation and that the operations in the real world are safe from a control and trajectory point-of-view.","","978-1-6654-6283-9","10.1109/ICMLA55696.2022.00201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10068864","Deep Reinforcement Learning;Industrial Manipulators;Path Planning","Training;Point cloud compression;Service robots;Robot vision systems;Neural networks;Reinforcement learning;Manipulators","control engineering computing;deep learning (artificial intelligence);industrial manipulators;manipulators;reinforcement learning","curriculum learning;deep reinforcement learning-based approach;industrial manipulators;joint action space;joint movements;point-to-point motions;robotic manipulator control;Sim2Real transfer;task-independent joint control learning;task-independent trajectories","","","","34","IEEE","23 Mar 2023","","","IEEE","IEEE Conferences"
"Real-world Robot Reaching Skill Learning Based on Deep Reinforcement Learning","N. Liu; T. Lu; Y. Cai; R. Wang; S. Wang","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China","2020 Chinese Control And Decision Conference (CCDC)","11 Aug 2020","2020","","","4780","4784","Traditional programming method can achieve certain manipulation tasks with the assumption that robot environment is known and structured. However, with robots gradually applied in more domains, robots often encounter working scenes which are complicated, unpredictable, and unstructured. To overcome the limitation of traditional programming method, in this paper, we apply deep reinforcement learning (DRL) method to train robot agent to obtain skill policy. As policy trained with DRL on real-world robot is time-consuming and costly, we propose a novel and simple learning paradigm with the aim of training physical robot efficiently. Firstly, our method train a virtual agent in an simulated environment to reach random target position from random initial position. Secondly, virtual agent trajectory sequence obtained with the trained policy, is transformed to real-world robot command with coordinate transformation to control robot performing reaching tasks. Experiments show that the proposed method can obtain self-adaptive reaching policy with low training cost, which is of great benefits for developing intelligent and robust robot manipulation skill system.","1948-9447","978-1-7281-5855-6","10.1109/CCDC49329.2020.9164770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9164770","Robot;reaching skill learning;deep reinforcement learning","Robot kinematics;Training;Task analysis;Machine learning;Trajectory;Neural networks","control engineering computing;intelligent robots;learning (artificial intelligence);manipulators;neural nets","physical robot training;real-world robot reaching skill learning;robust robot manipulation skill system;intelligent robot manipulation skill system;self-adaptive reaching policy;real-world robot command;virtual agent trajectory sequence;robot agent;DRL;deep reinforcement learning method;robot environment","","2","","19","IEEE","11 Aug 2020","","","IEEE","IEEE Conferences"
"Evaluation on the robustness of Genetic Network Programming with reinforcement learning","S. Mabu; A. Tjahjadi; S. Sendari; K. Hirasawa","Graduate School of Information Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information Production and Systems, Waseda University, Fukuoka, Japan","2010 IEEE International Conference on Systems, Man and Cybernetics","22 Nov 2010","2010","","","1659","1664","Genetic Network Programming (GNP) has been proposed as one of the evolutionary algorithms and extended with reinforcement learning (GNP-RL). The combination of evolution and learning can efficiently evolve programs and the fitness improvement has been confirmed in the simulations of tileworld problems, elevator group supervisory control systems, stock trading models and wall following behavior of Khepera robot. However, its robustness in testing environments has not been analyzed in detail yet. In this paper, the learning mechanism in the testing environment is introduced and it is confirmed that GNP-RL can show the robustness using a robot simulator WEBOTS, especially when unexperienced sensor troubles suddenly occur. The simulation results show that GNP-RL works well in the testing even if wrong sensor information is given because GNP-RL has a function to change programs using alternative actions automatically. In addition, the analysis on the effects of the parameters of GNP-RL is carried out in both training and testing simulations.","1062-922X","978-1-4244-6588-0","10.1109/ICSMC.2010.5642323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5642323","evolutionary computation;genetic network programming;reinforcement learning;robustness;Khepera robot","Variable speed drives;Economic indicators;Wheels","genetic algorithms;learning (artificial intelligence);mobile robots;robust control;sensors","genetic network programming;reinforcement learning;evolutionary algorithm;tile world problem;elevator group supervisory control system;stock trading model;wall following behavior;Khepera robot;testing environment;GNP-RL;robot simulator;WEBOTS;unexperienced sensor;sensor information","","4","","10","IEEE","22 Nov 2010","","","IEEE","IEEE Conferences"
"Flexible Job Shop Scheduling via Dual Attention Network-Based Reinforcement Learning","R. Wang; G. Wang; J. Sun; F. Deng; J. Chen","School of Automation, National Key Laboratory of Autonomous Intelligent Unmanned Systems, Beijing Institute of Technology, Beijing, China; School of Automation, National Key Laboratory of Autonomous Intelligent Unmanned Systems, Beijing Institute of Technology, Beijing, China; School of Automation, National Key Laboratory of Autonomous Intelligent Unmanned Systems, Beijing Institute of Technology, Beijing, China; School of Automation, National Key Laboratory of Autonomous Intelligent Unmanned Systems, Beijing Institute of Technology, Beijing, China; Department of Control Science and Engineering, Tongji University, Shanghai, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","12","Flexible manufacturing has given rise to complex scheduling problems such as the flexible job shop scheduling problem (FJSP). In FJSP, operations can be processed on multiple machines, leading to intricate relationships between operations and machines. Recent works have employed deep reinforcement learning (DRL) to learn priority dispatching rules (PDRs) for solving FJSP. However, the quality of solutions still has room for improvement relative to that by the exact methods such as OR-Tools. To address this issue, this article presents a novel end-to-end learning framework that weds the merits of self-attention models for deep feature extraction and DRL for scalable decision-making. The complex relationships between operations and machines are represented precisely and concisely, for which a dual-attention network (DAN) comprising several interconnected operation message attention blocks and machine message attention blocks is proposed. The DAN exploits the complicated relationships to construct production-adaptive operation and machine features to support high-quality decision-making. Experimental results using synthetic data as well as public benchmarks corroborate that the proposed approach outperforms both traditional PDRs and the state-of-the-art DRL method. Moreover, it achieves results comparable to exact methods in certain cases and demonstrates favorable generalization ability to large-scale and real-world unseen FJSP tasks.","2162-2388","","10.1109/TNNLS.2023.3306421","National Key Research and Development Program of China(grant numbers:2021YFB1714800); National Natural Science Foundation of China(grant numbers:62173034,61925303,62025301,62088101); CAAI-Huawei MindSpore Open Fund; Chongqing Natural Science Foundation(grant numbers:2021ZX4100027); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10246328","Deep reinforcement learning (DRL);flexible job-shop scheduling;graph attention networks (GATs);self-attention mechanism","Production;Feature extraction;Decision making;Job shop scheduling;Manufacturing;Task analysis;Reinforcement learning","","","","","","","IEEE","11 Sep 2023","","","IEEE","IEEE Early Access Articles"
"RIDRL: A Deep Reinforcement Learning Based on Multiple Dispatching Rules and IGA Algorithm for JSP","S. Jin; L. Shubin; L. Xin; W. You; L. Fusheng","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu","2022 19th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)","19 Jan 2023","2022","","","1","4","Job shop scheduling problem (JSP) is a very general and complex scheduling problem in the manufacturing industry. The traditional priority dispatching rule (PDR) can get the approximate solution for some specific problems. Nevertheless, for the complex and changing realistic factory floor, the quality of the existing solution fluctuates significantly. To solve the problem, this paper fuse multiple dispatching rules and the Insertion Greedy Algorithm (IGA) to deep reinforcement learning (DRL), namely RIDRL, to solve the job shop scheduling problem. In this method, we manually choose five generalizable state features as the states of the workshop environment. Employing 18 scheduling rules as the action space in the agent while designing a quick converge reward function. Additionally, we use a Proximal Policy Optimization Algorithm (PPO) to train the DRL agent with minimizing makespan as the optimization objective. Several simulation experiments on many standard instances indicate that the proposed method obtains competitive solutions for problems of different sizes.","2576-8964","978-1-6654-9389-5","10.1109/ICCWAMTIP56608.2022.10016480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10016480","Reinforcement Learning;Job Shop Scheduling Problem;PPO;PDR","Greedy algorithms;Deep learning;Training;Job shop scheduling;Processor scheduling;Reinforcement learning;Dispatching","dispatching;genetic algorithms;greedy algorithms;job shop scheduling;learning (artificial intelligence);optimisation;scheduling","approximate solution;complex scheduling problem;deep reinforcement learning;employing 18 scheduling rules;existing solution fluctuates;general scheduling problem;IGA;Insertion Greedy Algorithm;job shop scheduling problem;JSP;paper fuse multiple dispatching rules;Proximal Policy Optimization Algorithm;realistic factory floor;RIDRL;traditional priority dispatching rule","","","","6","IEEE","19 Jan 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning - Based Control Systems For Networked Power Infrastructures","N. Saha; S. Rezapour; M. H. Amini","Knight Foundation School of Computing and Information Sciences, Florida International University, Miami, FL, USA; Enterprise and Logistics Engineering, Knight Foundation School of Computing and Information Sciences, Florida International University, Miami, FL, USA; Knight Foundation School of Computing and Information Sciences, Florida International University, Miami, FL, USA","2021 International Conference on Computational Science and Computational Intelligence (CSCI)","22 Jun 2022","2021","","","1113","1116","Coordination in today’s large power systems is critical to satisfy demand fluctuations and reduce the required generation capacity. In this paper, we employ Reinforcement Learning (RL) techniques to design a control system to enhance coordination among facilities and minimize the power production cost. The controlling agent of the RL observes the the system’s state (e.g., demand volumes), perform actions (e.g., production volumes), and get rewards or punishments as the effect of their actions. Therefore, agent gets trained in the environment, find an optimal policy towards the system’s goal, and establish a smart energy system. The developed control mechanism is tested for different loads (residential, commercial and industrial) and compared against non-smart approaches that exist in the literature.","","978-1-6654-5841-2","10.1109/CSCI54926.2021.00235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9798956","multi-agent reinforcement learning;smart energy system;control system;infrastructure","Q-learning;Fluctuations;Scientific computing;Heuristic algorithms;Decision making;Production;Optimal scheduling","demand side management;design engineering;power engineering computing;power generation economics;power system state estimation;reinforcement learning","industrial load;commercial load;residential load;nonsmart approaches;smart energy system;production volumes;controlling agent;power production cost;generation capacity;demand fluctuations;power systems;networked power infrastructures;control system design;reinforcement learning","","","","9","IEEE","22 Jun 2022","","","IEEE","IEEE Conferences"
"Dynamic Maintenance for a Large Scale Identical Parallel Manufacturing Systems Using Reinforcement Learning","M. Salmani; F. Azizi; H. Rasay; F. Naderkhani",Concordia University; Concordia University; Concordia University; Concordia University,"2023 Annual Reliability and Maintainability Symposium (RAMS)","5 Apr 2023","2023","","","1","8","In this paper, we propose a Machin Learning (ML)-based framework for maintenance decision making for multi-unit system. More specifically, we propose Reinforcement Learning (RL) approach for dynamic maintenance model for multi- component parallel system subject to stochastic degradation and random failures. Deterioration of each unit occurs independently according to a three-state homogenous Markov process such that each unit has three states, namely, healthy, unhealthy, and failure state. The interaction among system states are modeled based on Birth/Birth-Death process. The overall system state is defined based on different combination of individual component state. The optimal maintenance policy for the system is obtained by modeling the problem as Markov Decision Process (MDP) and Q-learning algorithm with focus on cost minimization is applied as a solution methodology. In comparison to tradition MDP approaches, proposed RL solution is more effective and practical in terms of time and cost savings. Specifically, when the state-space of the problem is large, traditional MDP is note capable to converge to the optimal policy in a timely fashion. Therefore, there is a is the decisive need for development of RL-based solution for maintenance decision making. A numerical example is provided which demonstrates how the RL can be used to find the optimal maintenance policy for the system under study.","2577-0993","978-1-6654-6053-8","10.1109/RAMS51473.2023.10088200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10088200","Dynamic Maintenance;Manufacturing Systems;Reinforcement Learning","Costs;Q-learning;Decision making;Random access memory;Maintenance engineering;Markov processes;Systems engineering and theory","decision making;learning (artificial intelligence);maintenance engineering;manufacturing systems;Markov processes;reinforcement learning","dynamic maintenance model;healthy failure state;individual component state;Machin Learning-based framework;maintenance decision making;multi component parallel system;multiunit system;optimal maintenance policy;proposed RL solution;random failures;Reinforcement Learning;RL-based solution;scale identical parallel manufacturing systems;state-space;system state;three-state homogenous Markov process;unhealthy, failure state","","","","17","IEEE","5 Apr 2023","","","IEEE","IEEE Conferences"
"A reinforcement learning approach for optimal heating curve adaption","C. Huang; S. Seidel; F. Paschke; J. Bräunig","Division Engineering of Adaptive Systems EAS, Fraunhofer Institute of Integrated Circuits IIS, Dresden, Germany; Division Engineering of Adaptive Systems EAS, Fraunhofer Institute of Integrated Circuits IIS, Dresden, Germany; Division Engineering of Adaptive Systems EAS, Fraunhofer Institute of Integrated Circuits IIS, Dresden, Germany; Division Engineering of Adaptive Systems EAS, Fraunhofer Institute of Integrated Circuits IIS, Dresden, Germany","2022 IEEE 27th International Conference on Emerging Technologies and Factory Automation (ETFA)","25 Oct 2022","2022","","","1","4","In this work in progress paper we analyse the potential of intelligent supply temperature control for a heating network of an office building. As the building is equipped with a low temperature floor heating system and large window areas, the major goal of optimisation is to avoid overheating of the building during days with significant amount of solar energy but cool nights. A reinforcement learning based approach is applied and compared with first the standard heating curve and second with a simple rule based supply temperature control. The first results show that reinforcement learning based approach can effectively reduce overheating while still keep the desired level of comfort.","","978-1-6654-9996-5","10.1109/ETFA52439.2022.9921461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9921461","Reinforcement Learning;q-learning;supply temperature control;Building energy system;Building automation;Machine learning","Heating systems;Temperature;Buildings;Reinforcement learning;Solar energy;Windows;Standards","building management systems;control engineering computing;energy conservation;floors;power engineering computing;reinforcement learning;space heating;temperature control","reinforcement learning approach;optimal heating curve adaption;intelligent supply temperature control;heating network;office building;low temperature floor heating system;window areas;standard heating curve;rule based supply temperature control","","","","16","IEEE","25 Oct 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning methods for Automation Forex Trading","T. Chau; M. -T. Nguyen; D. -V. Ngo; A. -D. T. Nguyen; T. -H. Do","Faculty of Information Science and Engineering, University of Information Technology, Ho Chi Minh City, Vietnam; Faculty of Information Science and Engineering, University of Information Technology, Ho Chi Minh City, Vietnam; Faculty of Information Science and Engineering, University of Information Technology, Ho Chi Minh City, Vietnam; Faculty of Information Science and Engineering, University of Information Technology, Ho Chi Minh City, Vietnam; Faculty of Information Science and Engineering, University of Information Technology, Ho Chi Minh City, Vietnam","2022 RIVF International Conference on Computing and Communication Technologies (RIVF)","18 Jan 2023","2022","","","671","676","In Forex market, designing effective strategies are a critical role in investment. However, it is a challenging task due to its inherent characteristics, which include high volatility, trend, noise, and market shocks. In this paper, we propose four actor-critic-based algorithms: Proximal Policy Optimization (PPO), Actor-Critic using Kronecker-Factored Trust Region (ACKTR), Deep Deterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3). It employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. Besides, the ensemble trading strategy was combined with four algorithms to find the best method. We use our algorithms to test the 30 forex currencies.","2162-786X","978-1-6654-6166-5","10.1109/RIVF55975.2022.10013861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10013861","Forex Trading;Deep Reinforcement Learning;TD3;PPO;DDPG;Automation Trading;ACKTR;Actor-Critic","Deep learning;Automation;Electric shock;Reinforcement learning;Market research;Communications technology;Task analysis","deep learning (artificial intelligence);foreign exchange trading;investment;optimisation;reinforcement learning;stock markets","ACKTR;actor-critic using Kronecker-factored trust region;automation Forex trading;deep deterministic policy gradient;deep reinforcement learning;ensemble trading strategy;forex currencies;investment return;market shocks;PPO;proximal policy optimization;stock trading strategy;TD3;twin delayed DDPG","","","","20","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"Reinforcement-Learning-Based Local Search Approach to Integrated Order Batching: Driving Growth for Logistics and Retail","L. Zhou; C. Lin; Z. Cao","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China","IEEE Robotics & Automation Magazine","14 Jun 2023","2023","30","2","34","45","As an important part of Industry 4.0, a smart warehouse can offer smart tips and operational constraints for users. Improving its work efficiency is a promising growth driver for logistics companies and retailers. Therefore, a reinforcement-learning-based adaptive iterated local search (RAILS) approach is proposed to improve order-picking efficiency for a smart warehouse. A batching algorithm is proposed to deal with fluctuating orders efficiently and quickly obtain a high-quality initial solution. It can speed up the search for near-optimal solutions by extracting and using the features of the orders. Then, a perturbation mechanism is designed based on reinforcement learning that can adaptively select the perturbation type and determine the perturbation strength instead of a random way. Experimental results demonstrate that the proposed approach outperforms several existing ones, and its superiority becomes more significant as problems scale up.","1558-223X","","10.1109/MRA.2023.3265515","Beijing Leading Talents Program(grant numbers:Z191100006119031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10106464","","Smart manufacturing;Perturbation methods;Heuristic algorithms;Feature extraction;Schedules;Mathematical models;Search problems;Reinforcement learning","iterative methods;learning (artificial intelligence);logistics;production engineering computing;reinforcement learning;search problems;warehouse automation","batching algorithm;fluctuating orders;high-quality initial solution;integrated order batching;logistics companies;operational constraints;order-picking efficiency;promising growth driver;reinforcement learning;reinforcement-learning-based adaptive iterated local search approach;reinforcement-learning-based local search approach;retailers;smart tips;smart warehouse;work efficiency","","","","35","IEEE","21 Apr 2023","","","IEEE","IEEE Magazines"
"Composite rules selection using reinforcement learning for dynamic job-shop scheduling","Yingzi Wei; Mingyang Zhao","Shenyang Institute of Automation, Chinese Academy and Sciences, Shenyang, China; Shenyang Institute of Automation, Chinese Academy and Sciences, Shenyang, China","IEEE Conference on Robotics, Automation and Mechatronics, 2004.","13 Jun 2005","2004","2","","1083","1088 vol.2","Dispatching rules are usually applied dynamically to schedule the job in the dynamic job-shop. Existing scheduling approaches seldom address the machine selection in the scheduling process. Following the principles of traditional dispatching rules, composite rules, considering both the machine selection and job selection, were proposed in this paper. Reinforcement learning (IRL) is an on-line actor critic method. The dynamic system is trained to enhance its learning and adaptive capability by a RL algorithm. We define the conception of pressure for describing the system feature and determining the state sequence of search space. Designing a reward function should be guided based on the scheduling goal. We present the conception of jobs' estimated mean lateness (EMLT) that is used to determine the amount of reward or penalty. The scheduling system is trained by Q-learning algorithm through the learning stage and then it successively schedules the operations. Competitive results with the RL-agent approach suggest that it can be used as real-time optimal scheduling technology.","","0-7803-8645-0","10.1109/RAMECH.2004.1438070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1438070","","Learning;Dynamic scheduling;Optimal scheduling;Job shop scheduling;Intelligent agent;Automation;Dispatching;Scheduling algorithm;Computer science;Manufacturing","learning (artificial intelligence);dynamic scheduling;job shop scheduling","composite rules selection;dynamic job shop scheduling;dispatching rules;reinforcement learning;estimated mean lateness;real-time optimal scheduling technology;Q-learning algorithm","","3","","9","IEEE","13 Jun 2005","","","IEEE","IEEE Conferences"
"Internal Environment Controlling Algorithm of DMF Distillation Column based on Reinforcement Learning","H. Wang; Z. Ge","School of Engineering and Automation, Hefei University of Technology, Hefei; School of Engineering and Automation, Hefei University of Technology, Hefei","2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","3 Feb 2021","2020","9","","1479","1483","Most of the existing DMF distillation column internal environment control is achieved by controlling the flow rate of heating oil, but in practical engineering, due to the high temperature of heating oil, the control is difficult and the control cost is extremely high. In view of this situation, this paper proposes a control scheme based on Q-Learning. The control scheme realizes the control of liquid level, pressure, and temperature in the DMF distillation column by controlling the flow rate of liquid in and out and the wind speed of exhaust air, so as to solve the above problems effectively. The algorithm also set up inner and outer loop control, to maintain a large control range while ensuring the control accuracy and avoiding the exponential increase of calculation. Finally, the simulation results on MATLAB show that the scheme is feasible.","2693-2865","978-1-7281-5244-8","10.1109/ITAIC49862.2020.9338759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9338759","Reinforcement learning;Q-Learning;DMF;process control","Distillation equipment;Oils;Wind speed;Heat engines;Reinforcement learning;Mathematical model;Matlab","closed loop systems;control engineering computing;control system synthesis;distillation;distillation equipment;humidity control;learning (artificial intelligence);pressure control;process control;production engineering computing;temperature control","reinforcement learning;flow rate;heating oil;control cost;control scheme;inner loop control;outer loop control;control range;control accuracy;internal environment controlling algorithm;DMF distillation column internal environment control;MATLAB;Q-learning","","1","","10","IEEE","3 Feb 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Maintenance Scheduling for a Two-Machine Flow Line with Deteriorating Quality States","M. Wei; C. Qi","Institute of Systems Engineering, Key Laboratory of Education Ministry for Image Processing and Intelligent Control, Huazhong University of Science and Technology, Wuhan, China; Institute of Systems Engineering, Key Laboratory of Education Ministry for Image Processing and Intelligent Control, Huazhong University of Science and Technology, Wuhan, China","2012 Third Global Congress on Intelligent Systems","7 Feb 2013","2012","","","176","179","This paper investigates the maintenance policy of Two-Machine-One-Buffer flow line two machines with an intermediate buffer and analyzes effects of parameters on maintenance policy and system performance. The system produces good products with gradually decreasing probability when no maintenance is executed. A maintenance policy of control limit form having been testified in many literatures is adopted and a discounted reward Q-P-learning algorithm based on policy iteration is used to find out the optimal maintenance triggering states and also verify the model.","2155-6091","978-1-4673-3072-5","10.1109/GCIS.2012.82","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449511","maintenance scheduling;quality state;Q-P-learning;flow line;control limit","Production systems;Heuristic algorithms;Preventive maintenance;Markov processes;Numerical models","learning (artificial intelligence);maintenance engineering;probability;product quality;production engineering computing;scheduling","reinforcement learning-based maintenance scheduling;deteriorating quality states;maintenance policy;two-machine-one-buffer flow line maintenance policy;system performance;probability;control limit;discounted reward Q-P-learning algorithm;policy iteration;optimal maintenance triggering states;model verification","","","","8","IEEE","7 Feb 2013","","","IEEE","IEEE Conferences"
"Multilayered reinforcement learning for complicated collision avoidance problems","T. Fujii; Y. Arai; H. Asama; I. Endo","RIKEN Institute of Physical and Chemical Research, Wako, Saitama, Japan; RIKEN Institute of Physical and Chemical Research, Wako, Saitama, Japan; RIKEN Institute of Physical and Chemical Research, Wako, Saitama, Japan; RIKEN Institute of Physical and Chemical Research, Wako, Saitama, Japan","Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146)","6 Aug 2002","1998","3","","2186","2191 vol.3","We have proposed the collision avoidance methods in a multirobot system based on the information exchanged by the ""LOCISS: Locally Communicable Infrared Sensory System"", which is developed by the authors. One of the problems in the LOCISS based methods is that the number of situations which should be considered increases very much when the number of the robots and stationary obstacles in the working environment increases. In order to reduce the required computational power and memory capacity for such a large number of situations, we propose, in this paper, a multilayered reinforcement learning scheme to acquire appropriate collision avoidance behaviors. The feasibility and the performance of the proposed scheme is examined through the experiment using actual mobile robots.","1050-4729","0-7803-4300-X","10.1109/ROBOT.1998.680648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=680648","","Collision avoidance;Mobile robots;Robotics and automation;Robot motion;Humans;Signal detection;Random processes;Multirobot systems;Learning systems;Explosions","mobile robots;learning (artificial intelligence);optical communication;cooperative systems","multilayered reinforcement learning;complicated collision avoidance problems;multirobot system;LOCISS;Locally Communicable Infrared Sensory System;stationary obstacles;computational power;memory capacity;collision avoidance behaviors;mobile robots","","25","","5","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Poincaré-Map-Based Reinforcement Learning For Biped Walking","Jun Morimoto; Jun Nakanishi; Gen Endo; G. Cheng; C. G. Atkeson; G. Zeglin","Computational Brain Project, ICORP, Japan Science and Technology Corporation, Seika, Kyoto, Japan; Computational Brain Project, ICORP, Japan Science and Technology Corporation, Seika, Kyoto, Japan; Sony Intelligence Dynamics Laboratories, Inc., Seika, Kyoto, Japan; Computational Brain Project, ICORP, Japan Science and Technology Corporation, Seika, Kyoto, Japan; The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA","Proceedings of the 2005 IEEE International Conference on Robotics and Automation","10 Jan 2006","2005","","","2381","2386","We propose a model-based reinforcement learning algorithm for biped walking in which the robot learns to appropriately modulate an observed walking pattern. Via-points are detected from the observed walking trajectories using the minimum jerk criterion. The learning algorithm modulates the via-points as control actions to improve walking trajectories. This decision is based on a learned model of the Poincaré map of the periodic walking pattern. The model maps from a state in the single support phase and the control actions to a state in the next single support phase. We applied this approach to both a simulated robot model and an actual biped robot. We show that successful walking policies are acquired.","1050-4729","0-7803-8914-X","10.1109/ROBOT.2005.1570469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1570469","Biped Walking;Reinforcement Learning;Poincaré map","Learning;Legged locomotion;Hip;Robotics and automation;Humans;Trajectory;Foot;Leg;Robot control;Torso","","Biped Walking;Reinforcement Learning;Poincaré map","","24","","22","IEEE","10 Jan 2006","","","IEEE","IEEE Conferences"
"Interaction Models for Multiagent Reinforcement Learning","R. Ribeiro; A. P. Borges; F. Enembreck","University of Contestado, Santa Catarina, Brazil; Computer Science, PPGIa PUCPR, Curitiba, Parana, Brazil; Computer Science, PPGIa PUCPR, Curitiba, Parana, Brazil","2008 International Conference on Computational Intelligence for Modelling Control & Automation","24 Jul 2009","2008","","","464","469","This article proposes and compares different interaction models for reinforcement learning based on multi-agent system. The cooperation during the learning process is crucial to guarantee the convergence to a good policy. The exchange of rewards among the agents during the interaction is a complex task and if it is inadequate it may cause delays in learning or generate unexpected transitions, making the cooperation inefficient and con-verging to a non-satisfactory policy. In order to allow the interactive discovery of high quality policies we have developed several cooperation models based on the ex-change of action policies between the agents. Experimental results have shown that the proposed cooperation models are able to speed up the convergence of the agents while achieving optimal action policies even in high-dimensional environments (e.g. traffic), outperforming the standard Q-learning algorithm.","","978-0-7695-3514-2","10.1109/CIMCA.2008.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172670","Multiagent Systems;Cooperative Reinforcement Learning and Cooperation Models","Learning;Convergence;Multiagent systems;Traffic control;Computer science;Delay;Measurement;Proposals;Environmental management;Automation","learning (artificial intelligence);multi-agent systems","interaction models;multiagent reinforcement learning;multiagent system;learning process;high quality policies;interactive discovery;cooperation models","","12","","15","IEEE","24 Jul 2009","","","IEEE","IEEE Conferences"
"A Pursuit-Evasion Algorithm Based on Hierarchical Reinforcement Learning","J. Liu; S. Liu; H. Wu; Y. Zhang","Computer Science School, Northeast Normal University, Changchun, China; Computer Science School, Northeast Normal University, Changchun, China; Computer Science School, Northeast Normal University, Changchun, China; Computer Science School, Northeast Normal University, Changchun, China","2009 International Conference on Measuring Technology and Mechatronics Automation","18 Aug 2009","2009","2","","482","486","This paper proposed a pursuit-evasion algorithm based on the Option method from hierarchical reinforcement learning and applied it into multi-robot pursuit-evasion game in 2D-Dynamic environment. The algorithm efficiency is studied by comparing it with Q-learning. We decompose the complex task with option method, and divide the learning process into two parts: High-level learning and Low-level learning, then design a new mechanism in order to make the learning process perform parallel. The simulation result shows the Option algorithm can efficiently reduce the complexity of pursuit-evasion task, avoid traditional reinforcement learning curse of dimensionality, and improve the learning result.","2157-1481","978-0-7695-3583-8","10.1109/ICMTMA.2009.213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5203477","Pursuit Evasion Problem;Reinforcement Learning","Pursuit algorithms;Learning;Collaboration;Paper technology;Mechatronics;Automation;Computer science;Genetic programming;Computational geometry;Dynamic programming","game theory;intelligent robots;learning (artificial intelligence);multi-robot systems","hierarchical reinforcement learning;option method;multirobot pursuit-evasion game;2D-dynamic environment;Q-learning;high-level learning;low-level learning","","11","","21","IEEE","18 Aug 2009","","","IEEE","IEEE Conferences"
"Robotic Arm Motion Planning Based on Residual Reinforcement Learning","D. Zhou; R. Jia; H. Yao; M. Xie","School of Mechanical Electronic & Information Engineering, China University of Mining and Technology (Beijing), Beijing, China; School of Mechanical Electronic & Information Engineering, China University of Mining and Technology (Beijing), Beijing, China; School of Mechanical Electronic & Information Engineering, China University of Mining and Technology (Beijing), Beijing, China; School of Mechanical Electronic & Information Engineering, China University of Mining and Technology (Beijing), Beijing, China","2021 13th International Conference on Computer and Automation Engineering (ICCAE)","10 May 2021","2021","","","89","94","The application of reinforcement learning algorithms to motion planning is a research hotspot in robotics in recent years. However, training reinforcement learning agents from scratch has low training efficiency and difficulty in convergence. In this paper, a robot motion planning method based on residual reinforcement learning is proposed. This method divides the agent's policy of motion planning into initial policy and residual policy. The initial policy is composed of a neural network motion planner responsible for guiding the training direction of residual policy. The residual policy is composed of Proximal Policy Optimization (PPO) algorithm in reinforcement learning. A motion planning experiment is carried out in a simulation environment, and the result shows that the method can successfully perform motion planning. The comparison experiment between PPO and the proposed algorithm demonstrates that the proposed algorithm has better motion planning performance.","","978-1-6654-1295-7","10.1109/ICCAE51876.2021.9426160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9426160","robotic arm;motion planning;PPO algorithm;residual policy","Training;Robot motion;Automation;Neural networks;Virtual environments;Reinforcement learning;Manipulators","learning (artificial intelligence);mobile robots;motion control;path planning","robotic arm motion planning;residual reinforcement learning;robotics;training reinforcement;low training efficiency;robot motion;agent;initial policy;residual policy;neural network motion planner;training direction;Proximal Policy Optimization algorithm;motion planning experiment;motion planning performance","","6","","20","IEEE","10 May 2021","","","IEEE","IEEE Conferences"
"A new multi-agent reinforcement learning approach","J. Li; Q. Pan; B. Hong","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, Heilongjiang, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, Heilongjiang, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, Heilongjiang, China","The 2010 IEEE International Conference on Information and Automation","19 Jul 2010","2010","","","1667","1671","A new multi-agent reinforcement learning approach is proposed to learn the optimal behaviors among cooperative agent teams. The approach combines advantages of the integer programming, single agent learning and repeated game in a multi-agent framework. The integer programming is used to build cooperative teams in order to prevent the curse of dimensionality. Every cooperative team learns independently, whose members take the best response actions in the light of other agents actions in the same condition, after many repeated games, the aim root could be found. Because of other agents influence, the process of learning is supervised periodically, then through changing the learning rate to gain the right learning results. Simulation results on pursuit problem show that the proposed learning approach overcomes the divergence and improves learning speed obviously.","","978-1-4244-5704-5","10.1109/ICINFA.2010.5512238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5512238","multi-agent;system(MAS);reinforcement learning;Q-learning;pursuit problem","Learning;Multiagent systems;Linear programming;Collaborative work;Automation;Computer science;State-space methods;Pursuit algorithms;Intestines;Artificial intelligence","integer programming;learning (artificial intelligence);multi-agent systems","multi-agent reinforcement learning approach;cooperative agent team;integer programming","","6","","14","IEEE","19 Jul 2010","","","IEEE","IEEE Conferences"
"A stable Lyapunov constrained reinforcement learning based neural controller for non linear systems","A. Kumar; R. Sharma","ICE Division, NSIT, Dwarka, New Delhi, India; ICE Division, NSIT, Dwarka, New Delhi, India","International Conference on Computing, Communication & Automation","6 Jul 2015","2015","","","185","189","This paper proposes a Lyapunov constrained neural network based reinforcement learning (RL) controller with guaranteed stability for non linear systems. Neural networks have been used as universal function approximators to deal with one of the core problem faced in RL commonly known as `The Curse of Dimensionality'. We propose to constrain controller action set to the one dictated by the Lyapunov stability theory to produce a controller with guaranteed stability. We prove that when the controller action set is constrained the cost function turns out to be a Lyapunov candidate function thereby guaranteeing stability of the controller. Proposed methodology has been applied to the benchmark inverted pendulum (IP) balancing problem to validate its effectiveness. Simulation results and comparison against baseline neural Q learning control brings out the effectiveness and viability of the proposed control scheme.","","978-1-4799-8890-7","10.1109/CCAA.2015.7148402","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7148402","Reinforcement Learning;Neural Networks;Lyapunov Theory;Inverted Pendulum","Learning (artificial intelligence);Cost function;Linear systems;Artificial neural networks;Torque;Automation","function approximation;learning (artificial intelligence);Lyapunov methods;neurocontrollers;nonlinear control systems;stability","stable Lyapunov constrained reinforcement learning based neural controller;nonlinear systems;universal function approximators;RL controller;curse-of-dimensionality;controller action set;Lyapunov stability theory;guaranteed stability;cost function;Lyapunov function;inverted pendulum balancing problem;IP balancing problem","","4","","22","IEEE","6 Jul 2015","","","IEEE","IEEE Conferences"
"Biped walking on rough terfrain using reinforcement learning","Y. Zhang; Q. Huang; S. Bi; H. Min; Q. Zheng; Y. Luo","School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China","2015 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)","5 Oct 2015","2015","","","2061","2066","In this paper, we propose a novel reinforcement learning method to stabilize biped walking on rough terrain. For the state space and the action space of the biped walking problem is continuous, the neural network is used in our method, which is based on actor-critic learning, to approximate the policy function of actor and the value function of critic. The neural network learns on-line through the process. The proposed method is examined in simulation. The simulation results show that the robot can learn to improve the stability of walking on rough terrain by using the proposed method.","","978-1-4799-8730-6","10.1109/CYBER.2015.7288266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7288266","Humanoid;Biped Walking;Reinforcement Learning","Conferences;Automation;Control systems;Intelligent systems","humanoid robots;learning (artificial intelligence);legged locomotion;neurocontrollers;stability;state-space methods","biped walking;rough terrain;reinforcement learning;state space;neural network;actor-critic learning;policy function;value function;stability","","4","","9","IEEE","5 Oct 2015","","","IEEE","IEEE Conferences"
"Late Breaking Results: Reinforcement Learning for Scalable Logic Optimization with Graph Neural Networks","X. Timoneda; L. Cavigelli","Zurich Research Center, Huawei Technologies, Switzerland; Zurich Research Center, Huawei Technologies, Switzerland","2021 58th ACM/IEEE Design Automation Conference (DAC)","8 Nov 2021","2021","","","1378","1379","Logic optimization is an NP-hard problem commonly approached through hand-engineered heuristics. We propose to combine graph convolutional networks with reinforcement learning and a novel, scalable node embedding method to learn which local transforms should be applied to the logic graph. We show that this method achieves a similar size reduction as ABC on smaller circuits and outperforms it by $1. 5 - 1. 75 \times $ on larger random graphs.","0738-100X","978-1-6654-3274-0","10.1109/DAC18074.2021.9586206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586206","","Design automation;NP-hard problem;Reinforcement learning;Transforms;Benchmark testing;Graph neural networks;Optimization","computational complexity;graph theory;neural nets;optimisation;reinforcement learning;transforms","local transforms;larger random graphs;reinforcement learning;scalable logic optimization;graph neural networks;NP-hard problem;hand-engineered heuristics;graph convolutional networks;scalable node embedding method","","3","","7","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Adaptive Controller for Swash Mass Helicopter based on Reinforcement Learning Algorithm","T. B. Rouch; D. Allahverdy; A. Fakharian","Faculty of Electrical, Biomedical and Mechatronics Engineering, Qazvin Branch, Islamic Azad University, Qazvin, Iran; Science and Research Branch, Islamic Azad University, Tehran, Iran; Faculty of Electrical, Biomedical and Mechatronics Engineering, Qazvin Branch, Islamic Azad University, Qazvin, Iran","2022 8th International Conference on Control, Instrumentation and Automation (ICCIA)","23 Mar 2022","2022","","","1","6","In this paper, a dynamical model of unmanned aerial vehicles (UAVs) known as a swash mass helicopter (SMH) is described. Since the SMH is a highly nonlinear system, an adaptive control framework is required to stabilize this type of vehicle. First, a feedback linearization control approach is applied to the rotational and translational of the SMH subsystems. Then, a learning algorithm named reinforcement learning algorithm has been employed to train the proposed controller. Finally, the simulation results have been illustrated to assess the suggested controller's performance.","","978-1-6654-9569-1","10.1109/ICCIA54998.2022.9737165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9737165","Reinforcement learning;Swash mass helicopter;Fitted value iteration;Feedback linearization;train","Machine learning algorithms;Automation;Heuristic algorithms;Simulation;Instruments;Helicopters;Reinforcement learning","adaptive control;aircraft control;autonomous aerial vehicles;feedback;helicopters;linearisation techniques;nonlinear control systems;reinforcement learning","adaptive controller;swash mass helicopter;reinforcement learning;dynamical model;unmanned aerial vehicles;feedback linearization control;SMH subsystems;nonlinear system;UAV","","3","","12","IEEE","23 Mar 2022","","","IEEE","IEEE Conferences"
"Application of a Deep Reinforcement Learning Method in Financial Market Trading","L. Ma; Y. Liu","Department of Computer Science, Dalian Neusoft University of Information, Dalian, China; Department of Computer Science, Dalian Neusoft University of Information, Dalian, China","2019 11th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA)","7 Oct 2019","2019","","","421","425","The analysis and design of financial trading systems (FTSs) is a topic of great interest both for the academic institutions and the professional one regarding to the promises by machine learning methodologies. In the paper, research is focus on the optimization of placement of limit order within a given time horizon and how to transpose the process into an end-to-end learning pipeline within the context of reinforcement learning. First, features were factored out and constructed from market data that related to movements of the Bitcoin/USD trading, which were used by deep reinforcement learning agents to learn a limit order placement policy then. A reinforcement learning environment was developed to emulate a local broker as well. Finally, an evaluation procedure is defined to determine the capabilities and limitations of the policies learned by the reinforcement learning agents.","2157-1481","978-1-7281-2165-9","10.1109/ICMTMA.2019.00099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858758","Reinforcement Learning, Finantial market, Machine learning, Agents","Conferences;Q measurement;Mechatronics;Automation;Three-dimensional displays","financial data processing;learning (artificial intelligence);stock markets","reinforcement learning agents;deep reinforcement learning method;financial market trading;FTSs;academic institutions;machine learning methodologies;time horizon;end-to-end learning pipeline;market data;limit order placement policy;Bitcoin;USD trading;evaluation procedure","","3","","4","IEEE","7 Oct 2019","","","IEEE","IEEE Conferences"
"A Novel Algorithmic Trading Approach Based on Reinforcement Learning","L. Xucheng; P. Zhihao","School of Computer and Software, Dalian Neusoft University of Information, Dalian, China; School of Computer and Software, Dalian Neusoft University of Information, Dalian, China","2019 11th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA)","7 Oct 2019","2019","","","394","398","Algorithmic trading has gained great popularity due to the rapidly increasing computing power of modern day computers. In order to reduce trading latency, market participants and academic researchers are constantly looking for better novel and successful approaches to help them to achieve greater success. In this paper, a novel reinforcement learning approach is proposed which defines the algorithmic trading problem under the framework of the classic reinforcement learning problem, aiming to optimize the agent's performance in an unknown environment. By using state-of-the-art techniques based on least-squares temporal difference learning, an algorithmic trading system is built to support the reinforcement learning process. Evaluation of the approach is done with data from the foreign exchange market and results shows that it is profitable, easy to be expanded in the future.","2157-1481","978-1-7281-2165-9","10.1109/ICMTMA.2019.00093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858672","Reinforcement Learning;Algorithmic Trading;Stock Exchange","Conferences;Q measurement;Mechatronics;Automation","commerce;learning (artificial intelligence)","algorithmic trading problem;classic reinforcement learning problem;least-squares temporal difference learning;algorithmic trading system;reinforcement learning process;foreign exchange market;modern day computers;market participants;reinforcement learning approach;computing power;algorithmic trading approach","","3","","4","IEEE","7 Oct 2019","","","IEEE","IEEE Conferences"
"Application of reinforcement learning in dynamic pricing algorithms","W. Jintian; Z. Lei","Department of Computer Science and Technology, Hefei University of Technology, Hefei, Anhui, China; Department of Computer Science and Technology, Hefei University of Technology, Hefei, Anhui, China","2009 IEEE International Conference on Automation and Logistics","25 Sep 2009","2009","","","419","423","This paper is concerned with the dynamic pricing problems of a duopoly case in electronic retail markets. Combined with the concept of performance potential, the simulated annealing Q-learning (SA-Q) and the win-or-learn-fast policy hill climbing algorithm (WoLF-PHC) are used to solve the learning problems of multi-agent systems with either average- or discounted-reward criteria, under the case that only partial information about the opponent is known. The simulation results show that the WoLF-PHC algorithm performs well in adapting environment's change and in deriving better learning values than the SA-Q algorithm.","2161-816X","978-1-4244-4794-7","10.1109/ICAL.2009.5262885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5262885","multi-agent;performance potential;WoLF-PHC;simulated annealing Q-learning","Learning;Pricing;Heuristic algorithms;Consumer electronics;Computational modeling;Computer science;Simulated annealing;Automation;Logistics;Application software","learning (artificial intelligence);multi-agent systems;pricing;retailing;simulated annealing","reinforcement learning;dynamic pricing algorithm;duopoly;electronic retail market;performance potential;simulated annealing Q-learning;win-or-learn-fast policy hill climbing algorithm;WoLF-PHC algorithm;multiagent system;average-reward criteria;discounted-reward criteria","","2","","16","IEEE","25 Sep 2009","","","IEEE","IEEE Conferences"
"Multi-agent reinforcement learning design of load-frequency Control with frequency bias estimation","F. Daneshfar; F. Mansoori; H. Bevrani",NA; NA; NA,"The 2nd International Conference on Control, Instrumentation and Automation","24 Nov 2012","2011","","","310","314","Conventional load-frequency control (LFC) systems use proportional-integral (PI) controllers. These controllers are designed based on a linear model and the nonlinearities of the system are not accounted for. Then they are incapable to gain good dynamical performance for a wide range of operating conditions. A control strategy for solving this problem in a multi-area power system is presented by using a multi-agent reinforcement learning (MARL) approach based on the frequency bias (β) estimation that genetic algorithm (GA) optimization is used to tune its parameters. This approach contains two agents in each control area, estimator agent and controller agent that communicate with each other. The proposed method does not depend on any knowledge of the system and finding area control error (ACE) signal based on the frequency biased estimation, improves the LFC performance. To demonstrate the capability of the proposed control structure, a three-control area power system simulation with two different scenarios is presented.","","978-1-4673-1690-3","10.1109/ICCIAutom.2011.6356675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6356675","Multi-agent reinforcement learning;Load-frequency control;estimation","Instruments;Automation;IP networks","control engineering computing;control nonlinearities;control system synthesis;frequency control;frequency estimation;genetic algorithms;learning (artificial intelligence);linear systems;load regulation;multi-agent systems;PI control;power engineering computing;power system control;power system simulation","multiagent reinforcement learning design;load-frequency control;frequency bias estimation;LFC system;proportional-integral controller;PI controller;controller design;linear model;system nonlinearities;dynamical performance;multiarea power system;MARL;genetic algorithm;GA optimization;parameter tuning;estimator agent;controller agent;area control error signal;ACE signal;LFC performance;control structure;three-control area power system simulation","","2","","19","IEEE","24 Nov 2012","","","IEEE","IEEE Conferences"
"End-to-End control of USV swarm using graph centric Multi-Agent Reinforcement Learning","K. Lee; K. Ahn; J. Park","Department of Industrial and Systems Engineering, KAIST, Korea; Department of Industrial and Systems Engineering, KAIST, Korea; Department of Industrial and Systems Engineering, KAIST, Korea","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","925","929","The Unmanned Surface Vehicles (USVs), which operate without a person at the surface, are used in various naval defense missions. Various missions can be conducted efficiently when a swarm of USVs are operated at the same time. However, it is challenging to establish a decentralised control strategy for all USVs. In addition, the strategy must consider various external factors, such as the ocean topography and the number of enemy forces. These difficulties necessitate a scalable and transferable decision-making module. This study proposes an algorithm to derive the decentralised and cooperative control strategy for the USV swarm using graph centric multi-agent reinforcement learning (MARL). The model first expresses the mission situation using a graph considering the various sensor ranges. Each USV agent encodes observed information into localized embedding and then derives coordinated action through communication with the surrounding agent. To derive a cooperative policy, we trained each agent's policy to maximize the team reward. Using the modified prey-predator environment of OpenAI gym, we have analyzed the effect of each component of the proposed model (state embedding, communication, and team reward). The ablation study shows that the proposed model could derive a scalable and transferable control policy of USVs, consistently achieving the highest win ratio.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649839","Defense Acquisition Program Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649839","USV swarm;Multi-agent reinforcement learning;Graph Neural Network","Sea surface;Analytical models;Automation;Decision making;Decentralized control;Neural networks;Process control","decentralised control;decision making;evolutionary computation;graph theory;learning (artificial intelligence);multi-agent systems;remotely operated vehicles","end-to-end control;USV swarm;Unmanned Surface Vehicles;USVs;naval defense missions;decentralised control strategy;scalable decision-making module;transferable decision-making module;cooperative control strategy;graph centric multiagent reinforcement learning;mission situation;USV agent;surrounding agent;scalable control policy;transferable control policy","","2","","8","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Applying hierarchical reinforcement learning to computer games","D. Xiaoqin; L. Qinghua; H. Jianjun","College of Computer Science, Wuhan University of Science and Engineering, Wuhan, Hubei Province, China; College of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei Province, China; College of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei Province, China","2009 IEEE International Conference on Automation and Logistics","25 Sep 2009","2009","","","929","932","Hierarchical finite state machine (HFSM) has proven to be a powerful tool for controlling non-player characters (NPCs) in computer games due to its flexibility and modularity. For most implementations, however, it is often the case that the control details at all levels are hand-coded. As a result, the development process is often time intensive and error prone. In this paper, we explore the use of a hierarchical reinforcement learning approach, based on hierarchies of abstract machines (HAMs), to help overcome some of these limitations. We analyse in detail both HAMs and its use for designing HFSM, propose two HAMs-related machines, and make a preliminary experiment: applying HAMs to design NPCs' behavior and implementing it in Quake2. The result shows that this method can satisfy the need for controlling NPC and has faster convergence speed than flat reinforcement learning.","2161-816X","978-1-4244-4794-7","10.1109/ICAL.2009.5262787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5262787","Hierarchical Reinforcement Learning;Hierarchical Finite State Machine;Game Design","Automata;Artificial intelligence;Power engineering computing;Educational institutions;Computer science;Machine learning;Job design;Automation;Logistics;Power engineering and energy","computer games;finite automata;finite state machines;learning (artificial intelligence)","computer games;hierarchical reinforcement learning;hierarchical finite state machine;nonplayer characters;abstract machines;Quake2","","2","","14","IEEE","25 Sep 2009","","","IEEE","IEEE Conferences"
"Reinforcement-learning-based path planning for UAVs in intensive obstacle environment","M. Guo; T. Long; H. Li; J. Sun","School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China; School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China; School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China; School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","6451","6455","In intensive obstacle environment, the available flying space is narrow, which makes it difficult to generate feasible path for UAVs within limited runtime. In this paper, a Q-learning-based planning algorithm is presented to improve the efficiency of single UAV path planning in intensive obstacle environment. By constructing the space-action state offline learning planning architecture, the proposed method realizes the rapid path planning of UAV, and solves the high time-consuming problem of reinforcement learning online path planning. Considering the time-consuming problem of Q-table re-training, a probabilistic local update mechanism is proposed by updating the Q-value of the states to reduce the high time-consuming of Q-table re-raining and realize the rapid update of Q-table. The probability of Q-value updating is up to the distance to the new obstacle. The closer the state is to the new obstacle, the higher its probability of re-training. Therefore, the flight trajectory can be quickly re-planned when the environment changes. Simulation results show that the proposed Q-learning-based planning algorithm can generate path for UAV from random start position and avoid the obstacles. Compared with the classical A* algorithm, the path planning time based on the trained Q table can be reduced from second to millisecond, which significantly improves the efficiency of path planning.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727746","UAV;path planning;reinforcement learning;Q-learning;offline training;probabilistic local update mechanism","Training;Runtime;Q-learning;Automation;Simulation;Probabilistic logic;Path planning","autonomous aerial vehicles;collision avoidance;learning (artificial intelligence);mobile robots;multi-robot systems;path planning;probability","reinforcement-learning-based path planning;intensive obstacle environment;available flying space;single UAV path;space-action state offline;rapid path planning;high time-consuming problem;reinforcement learning;Q-table;probabilistic local update mechanism;rapid update;Q-value updating;environment changes","","2","","12","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Generating Multi-agent Patrol Areas by Reinforcement Learning","B. Park; C. Kang; J. Choi","Graduate School of AI, KAIST, Daejeon, Korea; Graduate School of AI, KAIST, Daejeon, Korea; Graduate School of AI, KAIST, Daejeon, Korea","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","104","107","In this paper, we designed reinforcement learning environment for distributed patrolling agents. In the partially observable environment, the agents take actions for each one's interest and the non-stationary problem in multi-agent setting encourages the agents not to invade other agent's region. In our environment, the patrolling routes for the agents are generated implicitly. We suggested different types of the environments and evaluated with different initial positions of the agents. We also show how the reinforcement learning algorithm changes the distribution of agents as training time goes.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9650047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650047","Multi-agent system;Reinforcement learning;Patrol Task","Training;Automation;Reinforcement learning;Control systems;Entropy;Task analysis","multi-agent systems;reinforcement learning","multiagent patrol areas;reinforcement learning;distributed patrolling agents;partially observable environment","","1","","14","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Different forms of the games in multiagent reinforcement learning: alternating vs. simultanous movements","A. Akramizadeh; A. Afshar; M. -B. Menhaj","Computational intelligence and large scale system Research Laboratory EE Department, Polytechnic University, Tehran, Iran; Computational intelligence and large scale system Research Laboratory EE Department, Polytechnic University, Tehran, Iran; Computational intelligence and large scale system Research Laboratory EE Department, Polytechnic University, Tehran, Iran","2009 17th Mediterranean Conference on Control and Automation","14 Jul 2009","2009","","","1289","1294","Multiagent systems are one of the most promising solutions in most of real life applications in which some kinds of social interactions or conventions are involved. Agent oriented applications are broadly explored among which learning in unknown environment is well developed based on Markov Decision Process (MDP). On the other hand, learning in multiagent systems has been recently introduced, basically in conjunction with game theory which is the science of investigating multiple interactive agents. During learning, self-interested agents are attempting to find the equilibrium policy based on the structure of the game, mostly considered as normal form games. In this paper, we focus on bringing into discussion game structures, addressed as normal form games and extensive form games, in learning process. This includes also some modifications and refinements in initially introduced concepts as well as a proposed approach in extensive form games.","","978-1-4244-4684-1","10.1109/MED.2009.5164724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5164724","Multiagent reinforcement learning;extensive form game;normal form game;Nash equilibrium points;subgame perfect equilibrium points","Learning;Game theory;Multiagent systems;Decision making;Convergence;Automatic control;Automation;Computational intelligence;Large-scale systems;Nash equilibrium","decision theory;game theory;learning (artificial intelligence);Markov processes;multi-agent systems","multiagent reinforcement learning;multiagent system;agent oriented application;Markov decision process;game theory;multiple interactive agent","","1","","17","IEEE","14 Jul 2009","","","IEEE","IEEE Conferences"
"Mixed Reinforcement Learning for Partially Observable Markov Decision Process","Le Tien Dung; T. Komeda; M. Takagi","Shibaura Institute of Technology, Japan; Shibaura Institute of Technology, Japan; Shibaura Institute of Technology, Japan","2007 International Symposium on Computational Intelligence in Robotics and Automation","16 Jul 2007","2007","","","7","12","Reinforcement Learning has been widely used to solve problems with a little feedback from environment. Q learning can solve full observable Markov Decision Processes quite well. For Partially Observable Markov Decision Processes (POMDPs), a Recurrent Neural Network (RNN) can be used to approximate Q values. However, learning time for these problems is typically very long. In this paper, Mixed Reinforcement Learning is presented to find an optimal policy for POMDPs in a shorter learning time. This method uses both a Q value table and a RNN. Q value table stores Q values for full observable states and the RNN approximates Q values for hidden states. An observable degree is calculated for each state while the agent explores the environment. If the observable degree is less than a threshold, the state is considered as a hidden state. Results of experiment in lighting grid world problem show that the proposed method enables an agent to acquire a policy, as good as the policy acquired by using only a RNN, with better learning performance.","","1-4244-0789-3","10.1109/CIRA.2007.382910","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4269910","","Learning;Recurrent neural networks;Table lookup;Computational intelligence;Robotics and automation;USA Councils;Neurofeedback;History;Neural networks;State-space methods","learning (artificial intelligence);Markov processes;recurrent neural nets","mixed reinforcement learning;partially observable markov decision process;Q learning;recurrent neural network","","1","1","24","IEEE","16 Jul 2007","","","IEEE","IEEE Conferences"
"Using reinforcement learning for agent-based network fault diagnosis system","Jingang Cao","Department of Computer, North China Electric Power University, Baoding, Hebei, China","2011 IEEE International Conference on Information and Automation","11 Jul 2011","2011","","","750","754","In the network, it is important that faults can be diagnosed at early stage before they result in serious fault. However, the situation is not optimistic, which depends on what network management software is used. Aiming to this problem, a mobile agent-based network fault diagnosis model is proposed. In the model, agent can learn by reinforcement learning (RL), which can improve fault diagnosis performance. The structure and function of model, especially the architecture and learning algorithm of diagnostic agent, is depicted. At last, compared the system performance through simulation and experiment, and results show that the model has greater advantage.","","978-1-4577-0270-9","10.1109/ICINFA.2011.5949093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949093","fault diagnosis;network management;mobile agent;reinforcement learning","Automation;Conferences","computer network management;computer network performance evaluation;fault diagnosis;learning (artificial intelligence);mobile agents;multi-agent systems","reinforcement learning;network management software;mobile agent based network fault diagnosis model;learning algorithm","","","","9","IEEE","11 Jul 2011","","","IEEE","IEEE Conferences"
"Multi-agent Reinforcement Learning with Multi-head Attention","K. Ni; J. Chen; J. Wang; B. Liu; T. Lei","School of Optics and Photonics, Beijing Institute of Technology, Beijing, China; School of Optics and Photonics, Beijing Institute of Technology, Beijing, China; School of Optics and Photonics, Beijing Institute of Technology, Beijing, China; School of Optics and Photonics, Beijing Institute of Technology, Beijing, China; School of Optics and Photonics, Beijing Institute of Technology, Beijing, China","2023 IEEE 6th Information Technology,Networking,Electronic and Automation Control Conference (ITNEC)","30 Mar 2023","2023","6","","1311","1315","Multi-agent reinforcement learning(MARL) methods have become an important approach to solving the decision making problems of agents. As the environment’s complexity increases, the attention model can effectively solve the problem of information redundancy. However, the introduction of attention models in reinforcement learning may also lead to over-focusing and neglecting other potentially useful information. Moreover, the presence of attention would slow the convergence in the early stages of training. To address the above problem, we propose a divided attention reinforcement learning approach: (i) the involvement of an attention regularization term to make agents more divergent in their focus on different directions; (ii) the use of a layer normalization network structure and the use of a Pre-Layer Normalization(Pre-LN) network structure for the attention optimization in the initialization phase of training. It allows the agents to have a more stable and smooth gradient descent in the early stages of learning. Our approach has been tested in several multi-agent environment tasks. Compared to other related multi-agent methods, our method obtains higher final rewards and training efficiency.","2693-3128","978-1-6654-6004-0","10.1109/ITNEC56291.2023.10082248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10082248","multi-agent;reinforcement learning;attention;layer normalization","Training;Automation;Redundancy;Decision making;Reinforcement learning;Complexity theory;Task analysis","decision making;gradient methods;multi-agent systems;neural nets;reinforcement learning","attention optimization;attention regularization term;decision making problems;divided attention reinforcement learning;gradient descent;information redundancy;layer normalization network structure;MARL;multiagent environment tasks;multiagent reinforcement learning;multihead attention;pre-layer normalization network structure;pre-LN network structure","","","","17","IEEE","30 Mar 2023","","","IEEE","IEEE Conferences"
"Scene Exploration Method of Coin Collection Game Based on Deep Reinforcement Learning Algorithm","H. Wang; J. Niu","College of Information Science and Technology, North China University of Technology, Beijing, China; College of Information Science and Technology, North China University of Technology, Beijing, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","4135","4139","In game AI control, sparse-reward environment exploration has been a hotspot for agent control. At present, agents based on traditional deep reinforcement learning are unable to learn appropriate policies when faced with complex continuous action space under sparse-reward. Recently, intrinsic reward stimulation has become a promising direction to solve the sparse-reward exploration problem. But in the complex environment, the agent will still get the local optimal policy. To solve the above problems, this paper designed a coin collection game environment. Based on the Proximal Policy Optimization with Random Network Distillation, paper proposed a method for agents to explore sparse-reward environment. In this paper, custom reward function gives extrinsic rewards to the agent. 3D-sensing rays allow agent to observe environment states. By using Long Short-Term Memory (LSTM) network, previous states and current state are fused to estimate the state value. Through comparative experiments, the results show that the proposed method significantly improves the maximum reward convergence rate, and agent obtains the global optimal policy. The agent can explore the scene, and finish the goal more effectively.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728347","Deep Reinforcement Learning;Sparse-Reward Environment;Game AI Control;LSTM","Automation;Games;Reinforcement learning;Aerospace electronics;Path planning;Sensors;Artificial intelligence","computer games;control engineering computing;deep learning (artificial intelligence);multi-agent systems;optimisation;recurrent neural nets;reinforcement learning","global optimal policy;scene exploration method;deep reinforcement learning algorithm;game AI control;sparse-reward environment exploration;agent control;complex continuous action space;intrinsic reward stimulation;sparse-reward exploration problem;local optimal policy;coin collection game environment;proximal policy optimization;random network distillation;custom reward function;extrinsic rewards;environment states;long short-term memory network;maximum reward convergence rate;3D-sensing rays;LSTM","","","","9","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Path Tracking With Application of Quadruped Robot","Y. Ouyang; H. Bai; Y. Shan","School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China; School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China; School of Artificial Intelligence, Sun Yat-sen University, Guangzhou, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","7069","7074","Path tracking is a hot issue in the control of robots, especially quadruped robots for their complexity in foot structure and walking gait. Pure-pursuit is widely used in vehicle path tracking, but most of the improvements on pure-pursuit only concern about tracking error. In this letter, we introduce a reinforcement learning-based method using Proximal Policy Optimization(PPO) to adaptively control the preview distance in pure-pursuit and the step length of quadruped robot according to the curvature of the road. Experiments show that we reached a balance in decreasing tracking error as traditional pure-pursuit while maintaining the running speed of quadruped robot.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728351","legged-robot;path tracking;reinforcement learning;pure-pursuit","Legged locomotion;Learning systems;Automation;Roads;Complexity theory;Quadrupedal robots;Foot","adaptive control;gait analysis;learning (artificial intelligence);legged locomotion;mobile robots;path planning;position control;robot dynamics;tracking","reinforcement learning-based path tracking;quadruped robot;foot structure;walking gait;vehicle path tracking;tracking error;reinforcement learning-based method;traditional pure-pursuit","","","","14","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Integrated RRT Algorithm for Path Planning","H. Liu; Y. Gu; X. Li; X. Xiao","College of Information Science and Technology, Donghua University; College of Information Science and Technology, Donghua University; College of Information Science and Technology, Donghua University; College of Information Science and Technology, Donghua University","2023 WRC Symposium on Advanced Robotics and Automation (WRC SARA)","27 Sep 2023","2023","","","239","244","Rapidly-exploring random tree (RRT) algorithm, featured with strong exploration capability, is widely used in path planning tasks. However, it is difficult for RRT to find the optimal path due to its inherent characteristics of random sampling. In this paper, we propose an improved RRT algorithm integrated with deep reinforcement learning (IRRT-DRL), which can effectively search for feasible paths and exploit previous experience for optimization. It includes a unique reward function and a dynamic waypoint selection mechanism that automatically adjusts the interval between adjacent waypoints to help the agent bypass obstacles. Experimental results have verified the feasibility and superiority of the proposed approach.","2835-3358","979-8-3503-0732-0","10.1109/WRCSARA60131.2023.10261849","National Natural Science Foundation of China; Natural Science Foundation of Shanghai; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261849","Deep reinforcement learning (DRL);rapidly-exploring random tree (RRT);path planning;robot","Deep learning;Automation;Heuristic algorithms;Reinforcement learning;Path planning;Task analysis;Robots","collision avoidance;deep learning (artificial intelligence);mobile robots;path planning;reinforcement learning;trees (mathematics)","deep reinforcement learning;dynamic waypoint selection mechanism;feasible paths;improved RRT algorithm;integrated RRT algorithm;IRRT-DRL;optimal path;path planning tasks;random sampling;random tree algorithm;strong exploration capability;unique reward function","","","","19","IEEE","27 Sep 2023","","","IEEE","IEEE Conferences"
"Late Breaking Results: Reinforcement Learning-based Power Management Policy for Mobile Device Systems","E. Kwon; S. Han; Y. Park; Y. H. Kim; S. Kang","EE Department, Pohang University of Science and Technology, Pohang, Korea; EE Department, Pohang University of Science and Technology, Pohang, Korea; EE Department, Pohang University of Science and Technology, Pohang, Korea; EE Department, Pohang University of Science and Technology, Pohang, Korea; EE Department, Pohang University of Science and Technology, Pohang, Korea","2020 57th ACM/IEEE Design Automation Conference (DAC)","9 Oct 2020","2020","","","1","2","This paper presents a power management policy that exploits reinforcement learning to increase power efficiency of mobile device systems. Our Q-learning-based policy predicts a system’s characteristics and learns power management controls to adapt to the system’s variations. Therefore, we can flexibly manage the system power regardless of the application scenario and can achieve lower energy per QoS compared to previous dynamic voltage/frequency scaling governors. To minimize the process overhead, we implemented our power management policy as hardware; the hardware-implemented policy reduced the average latency up to 40× compared to the software-implemented policy.","0738-100X","978-1-7281-1085-1","10.1109/DAC18072.2020.9218716","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9218716","","Q-learning;Power demand;Design automation;Power system management;Quality of service;Mobile handsets;Hardware","learning (artificial intelligence);mobile computing;quality of service","reinforcement learning-based power management policy;mobile device systems;power efficiency;Q-learning-based policy;power management controls;system power;hardware-implemented policy;software-implemented policy;QoS","","","","8","IEEE","9 Oct 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning and Nonlinear Control of a X33 Vehicle Model ⋆","B. A. Costa; F. L. Parente; J. M. Lemos","INESC-ID/IST/University of Lisbon, Lisbon, Portugal; INESC-ID, Lisbon, Portugal; INESC-ID/IST/University of Lisbon, Lisbon, Portugal","2022 International Conference on Control, Automation and Diagnosis (ICCAD)","18 Aug 2022","2022","","","1","6","This paper explores the application of nonlinear control and reinforcement learning to control a model of X33 reentry vehicle. The control problem is formulated considering the gliding phase of the X33 spacecraft model. During this phase, no thrust is applied and wind disturbances may change the path of the spacecraft from the reference path. Several difficulties were present when using the reinforcement learning controller. The starting of the controller, the convergence of the controller gains and their relation to the excitation noise, and the available time to learn.","2767-9896","978-1-6654-9794-7","10.1109/ICCAD55197.2022.9853874","Fundação para a Ciência e a Tecnologia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9853874","Nonlinear Control;Reinforcement Learning;Spacecraft Reentry","Space vehicles;Automation;Perturbation methods;Process control;Reinforcement learning;Convergence","adaptive control;control system synthesis;learning (artificial intelligence);learning systems;nonlinear control systems;space vehicles","X33 Vehicle Model;nonlinear control;X33 reentry vehicle;control problem;gliding phase;X33 spacecraft model;reference path;reinforcement learning controller;controller gains","","","","7","IEEE","18 Aug 2022","","","IEEE","IEEE Conferences"
"Late Breaking Results: RL-LPO: Reinforcement Learning Based Leakage Power Optimization Framework with Graph Neural Network","P. Cao; J. Wang","National ASIC System Engineering Technology Research Center, Southeast University, Nanjing, China; National ASIC System Engineering Technology Research Center, Southeast University, Nanjing, China","2023 60th ACM/IEEE Design Automation Conference (DAC)","15 Sep 2023","2023","","","1","2","Leakage power optimization based on threshold voltage (Vth) assignment poses great challenge in circuit design due to its tremendous solution space. In this paper, a Reinforcement Learning-based Leakage Power Optimization framework (RL-LPO) is first-ever proposed to formulate Vth assignment as a reinforcement learning (RL) process by learning timing and physical characteristics of each circuit instance with Graph Neural Networks (GNN). The proposed RL-LPO was validated by the IWLS2005 and Opencores benchmark circuits with TSMC 28nm technology and experimental results demonstrate that our work achieves better leakage power optimization by additional 3% reduction on average than the commercial tool PrimeTime with 6.7× speed up when being transferred to unseen circuits with negligible timing degradation.","","979-8-3503-2348-1","10.1109/DAC56929.2023.10247707","National Key Research and Development Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247707","leakage power;threshold voltage;reinforcement learning","Degradation;Design automation;Reinforcement learning;Benchmark testing;Graph neural networks;Threshold voltage;Timing","graph neural networks;integrated circuit design;low-power electronics;reinforcement learning","circuit design;circuit instance;graph neural networks;IWLS2005;Opencores benchmark circuit;Opencores benchmark circuits;reinforcement learning process;reinforcement learning-based leakage power optimization framework;RL-LPO;threshold voltage assignment;Vthassignment","","","","6","IEEE","15 Sep 2023","","","IEEE","IEEE Conferences"
"Formulations for Data-Driven Control Design and Reinforcement Learning","D. Lee; D. W. Kim","Department of Electrical Engineering, KAIST, Daejeon, South Korea; Department of Electrical Engineering, Hanbat National University, Daejeon, South Korea","2022 IEEE 17th International Conference on Control & Automation (ICCA)","25 Jul 2022","2022","","","207","212","The goal of this paper is to investigate model-free data-driven control design strategies for unknown systems. In particular, we report new data-driven linear matrix inequalities (LMIs) and dynamic programming (DP) methods. Both continuous-time and discrete-time systems are considered. We consider data transition equations that include complete information on the system model using state-input trajectories. Instead of computing explicit system model, the data transition equations are used to construct data-dependent LMI and DP formulations. The proposed formulations provide additional insights in data-driven control designs. In addition, we regard the proposed methods as a complement rather than replacement of existing methods.","1948-3457","978-1-6654-9572-1","10.1109/ICCA54724.2022.9831901","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9831901","","Discrete-time systems;Automation;Control design;Computational modeling;Reinforcement learning;Mathematical models;Data models","continuous time systems;control system synthesis;discrete time systems;dynamic programming;learning (artificial intelligence);linear matrix inequalities;time-varying systems;uncertain systems","model-free data-driven control design strategies;unknown systems;dynamic programming methods;discrete-time systems;data transition equations;explicit system model","","","","27","IEEE","25 Jul 2022","","","IEEE","IEEE Conferences"
"Research on Actor-Critic Reinforcement Learning in RoboCup","He Guo; Tianyang Liu; Yuxin Wang; Feng Chen; Jianming Fan","Department of Computer Science and Engineering, Dalian University of Technology, Dalian, China; Department of Computer Science and Engineering, Dalian University of Technology, Dalian, China; Department of Computer Science and Engineering, Dalian University of Technology, Dalian, China; Department of Computer Science and Engineering, Dalian University of Technology, Dalian, China; Department of Computer Science and Engineering, Dalian University of Technology, Dalian, China","2006 6th World Congress on Intelligent Control and Automation","23 Oct 2006","2006","2","","9212","9216","Actor-critic method combines the fast convergence of value-based (critic) and directivity on search of policy gradient (actor). It is suitable for solving the problems with large state space. In this paper, the actor-critic method with tile-coding linear function approximation is analysed and applied to a RoboCup simulation subtask named ""Soccer Keepaway"". The experiments on Soccer Keepaway show that the policy learned by actor-critic method is better than policies from value-based Sarsa(lambda) and benchmarks","","1-4244-0332-4","10.1109/WCICA.2006.1713783","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1713783","Reinforcement Learning;MAS;Actor-Critic;RoboCup;Function Approximation","Learning;Function approximation;Helium;Computer science;Space technology;Electronic mail;State-space methods;Analytical models;Intelligent control;Automation","function approximation;gradient methods;learning (artificial intelligence);mobile robots;multi-robot systems","actor-critic reinforcement learning;RoboCup;policy gradient searching;tile-coding linear function approximation;Soccer Keepaway;value-based Sarsa(lambda)","","","","","IEEE","23 Oct 2006","","","IEEE","IEEE Conferences"
"Data-driven Path Following of Unmanned Surface Vehicles based on Model-Based Reinforcement Learning and Model Predictive Path Integral Control","E. Liu; D. Wang; Z. Peng; L. Liu; N. Gu","Marine Electrical Engineering, Dalian Maritime University, Dalian, China; Marine Electrical Engineering, Dalian Maritime University, Dalian, China; Marine Electrical Engineering, Dalian Maritime University, Dalian, China; Marine Electrical Engineering, Dalian Maritime University, Dalian, China; School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Melbourne, VIC, Australia","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","1045","1049","This paper considers path following problem of an under-actuated unmanned surface vehicle (USV) being lack of model information. A fully data-driven path following algorithm is presented for the USV based on model-based reinforcement learning (RL) and model predictive path integral (MPPI) control. Specifically, a fully-connected neural network is trained to approximate the state transition model of the USV based on model-based RL. Next, based on the learned model, a sampling-based MPPI control is used to obtain the optimal actions. Simulation results demonstrate the performance of the proposed data-driven path following algorithm for the USV without using any model information.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023766","National Natural Science Foundation of China; Liaoning Revitalization Talents Program; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023766","","Automation;Simulation;Neural networks;Reinforcement learning;Predictive models;Prediction algorithms;Approximation algorithms","learning systems;mobile robots;neurocontrollers;optimal control;path planning;predictive control;reinforcement learning;state estimation;unmanned surface vehicles","approximate the state transition model;fully data-driven path following algorithm;fully-connected neural network training;model information;model predictive path integral control;model-based reinforcement learning;optimal actions;sampling-based MPPI control;state transition model;underactuated unmanned surface vehicle;unmanned surface vehicles;USV","","","","11","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"RL-MUL: Multiplier Design Optimization with Deep Reinforcement Learning","D. Zuo; Y. Ouyang; Y. Ma",HKUST(GZ); HKUST(GZ); HKUST(GZ),"2023 60th ACM/IEEE Design Automation Conference (DAC)","15 Sep 2023","2023","","","1","6","Multiplication is a fundamental operation in many applications, and multipliers are widely adopted in various circuits. However, optimizing multipliers is challenging and non-trivial due to the huge design space. In this paper, we propose RL-MUL, a multiplier design optimization framework based on reinforcement learning. Specifically, we utilize matrix and tensor representations for the compressor tree of a multiplier, based on which the convolutional neural networks can be seamlessly incorporated as the agent network. The agent can learn to adjust the multiplier structure based on a Pareto-driven reward which is customized to accommodate the trade-off between area and delay. Experiments are conducted on different bit widths of multipliers. The results demonstrate that the multipliers produced by RL-MUL dominate all baseline designs in terms of both area and delay. The performance gain of RL-MUL is further validated by comparing the area and delay of processing element arrays using multipliers from RL-MUL and baseline approaches.","","979-8-3503-2348-1","10.1109/DAC56929.2023.10247941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247941","","Deep learning;Tensors;Design automation;Reinforcement learning;Performance gain;Parallel processing;Delays","deep learning (artificial intelligence);multiplying circuits;optimisation;reinforcement learning;tensors","deep reinforcement learning;matrix representation;multiplier design optimization framework;multiplier structure;Pareto-driven reward;RL-MUL;tensor representation","","","","21","IEEE","15 Sep 2023","","","IEEE","IEEE Conferences"
"Growing Robot Navigation Based on Deep Reinforcement Learning","A. Ataka; A. P. Sandiwan","Department of Electrical and Information Engineering, Universitas Gadjah Mada, Yogyakarta, Indonesia; Department of Electrical and Information Engineering, Universitas Gadjah Mada, Yogyakarta, Indonesia","2023 9th International Conference on Control, Automation and Robotics (ICCAR)","21 Jun 2023","2023","","","115","120","The recent progress in materials and structures has kick-started the development of soft eversion robot with the ability to grow in size. However, despite its promising capability to navigate challenging terrains, this type of robot still lacks a navigation strategy due to the robot's complexity courtesy of its increasing degrees of freedom as it grows. In this paper, we develop a growing robot navigation strategy based on deep reinforcement learning. The reinforcement learning was specifically designed to work with growing robot even as its degrees of freedom increase. The algorithm was shown to work in navigating growing robot in a planar environment towards a random target. The results show that the reinforcement learning is a promising candidate to be used for growing robot navigation.","2251-2454","979-8-3503-2251-4","10.1109/ICCAR57134.2023.10151740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10151740","Soft Robotics;AI in Robots;Growing Robots;Robot Control","Deep learning;Jacobian matrices;Three-dimensional displays;Automation;Navigation;Reinforcement learning;Complexity theory","deep learning (artificial intelligence);mobile robots;navigation;path planning;reinforcement learning","deep reinforcement learning;freedom increase;growing robot navigation strategy;increasing degrees;kick-started;navigating growing robot;promising capability;soft eversion robot","","","","22","IEEE","21 Jun 2023","","","IEEE","IEEE Conferences"
"Sequential Viewpoint Selection and Grasping with Partial Observability Reinforcement Learning","W. Chen; Y. Hua; B. Jin; J. Zhu; Q. Ge; X. Wang","East China Normal University, Shanghai, China; East China Normal University, Shanghai, China; East China Normal University, Shanghai, China; East China Normal University, Shanghai, China; Nanjing University of Information Science and Technology, Nanjing, China; East China Normal University, Shanghai, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","1125","1129","Despite the success of vision-based object grasping due to deep learning development, fixed-view object grasping methods still face information loss with limited performance. Recently some rule-based or heuristic-based methods have begun to sequentially consider multiple views to improve the perceptibility of the environment, which shows better performance. However, their sequence lengths are too short, or their viewpoint selection is myopic and ignores the long-term effect. This paper models sequential viewpoints selection as a Markov Decision Process. The Sequential Decided Multi-View Grasping (SDMVG) method is proposed based on reinforcement learning, and an RNN-based policy is introduced. Considering long-term return, SDMVG can generate viewpoints sequence which achieves most information gain. Numerical experiments show SDMVG can achieve ~ 10% accuracy improvement compared with rule-or heuristic-based baselines on Multi-View GraspNet Benchmark. Moreover, SDMVG approaches the global optimum with only ~ 1/40 wall time compared with the brute-force method.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023914","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023914","Object Grasping;Reinforcement;Robotic","Deep learning;Automation;Grasping;Reinforcement learning;Markov processes;Benchmark testing;Numerical models","dexterous manipulators;learning (artificial intelligence);Markov processes;recurrent neural nets","10% accuracy improvement;brute-force method;deep learning development;fixed-view;information gain;information loss;long-term effect;long-term return;Markov Decision Process;MultiView GraspNet Benchmark;partial observability reinforcement learning;RNN-based policy;SDMVG;sequence lengths;Sequential Decided MultiView Grasping method;Sequential viewpoint selection;viewpoints selection;viewpoints sequence;vision-based object grasping","","","","37","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Modifying Neural Networks in Adversarial Agents of Multi-agent Reinforcement Learning Systems","N. E. Fard; R. R. Selmic","Department of Electrical and Computer Engineering, Concordia University, Montreal, Canada; Department of Electrical and Computer Engineering, Concordia University, Montreal, Canada","2023 31st Mediterranean Conference on Control and Automation (MED)","25 Jul 2023","2023","","","824","829","This paper proposes a method to reduce the malicious agent’s negative effects on a multi-agent reinforcement learning (MARL) system, including actor-critic architecture. The method achieves the overall goal of the MARL system, which is to increase the cumulative reward of all individual agents and reduce the malicious agents’ harmful effects on the entire MARL system. Assuming that the adverse agent is detectable, we propose to change the malicious agent’s neural network (NN) structure. By leveraging a comparative methodology, we have demonstrated that a specific NN architecture using a linear activation function surpasses another utilizing a sigmoid activation function in minimizing loss. Our analysis indicates that this performance differential is attributable to the utilization of distinct activation functions within the models. This approach involves calculating the gradient of the loss function with respect to the activation function. The claims have been proven theoretically, and the simulation confirms theoretical findings.","2473-3504","979-8-3503-1543-1","10.1109/MED59994.2023.10185760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10185760","","Analytical models;Automation;Artificial neural networks;Reinforcement learning;Convergence","multi-agent systems;neural nets;reinforcement learning;transfer functions","actor-critic architecture;adversarial agents;adverse agent;linear activation function;loss function;malicious agent;MARL system;multiagent reinforcement learning system;neural network modification;NN architecture;sigmoid activation function","","","","23","IEEE","25 Jul 2023","","","IEEE","IEEE Conferences"
"Heterogeneous Graph Convolutional Network for Visual Reinforcement Learning of Action Detection","L. Wang; C. Huang; X. Chen","School of Computer Science and Artificial Intelligence, Wuhan University of Technology, Wuhan, China; Fujian(Quanzhou)-HIT Research Institute of Engineering and Technology, Quanzhou, China; Fujian Provincial Key Laboratory of Information Processing and Intelligent Control, Minjiang University, Fuzhou, China","2023 8th International Conference on Automation, Control and Robotics Engineering (CACRE)","8 Aug 2023","2023","","","52","56","Existing action detection approaches do not take spatio-temporal structural relationships of action clips into account, which leads to a low applicability in real-world scenarios and can benefit detecting if exploited. To this end, this paper proposes to formulate the action detection problem as a reinforcement learning process which is rewarded by observing both the clip sampling and classification results via adjusting the detection schemes. In particular, our framework consists of a heterogeneous graph convolutional network to represent the spatio-temporal features capturing the inherent relation, a policy network which determines the probabilities of a predefined action sampling spaces, and a classification network for action clip recognition. We accomplish the network joint learning by considering the temporal intersection over union and Euclidean distance between detected clips and ground-truth. Experiments on ActivityNet v1.3 and THUMOS14 demonstrate our method.","","979-8-3503-0277-6","10.1109/CACRE58689.2023.10208414","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10208414","Action detection;reinforcement learning;graph convolutional network","Deep learning;Visualization;Automation;Reinforcement learning;Euclidean distance;Convolutional neural networks;Robots","convolutional neural nets;feature extraction;graph theory;image classification;image motion analysis;learning (artificial intelligence);object detection;probability;reinforcement learning;supervised learning;video signal processing","action clip recognition;action clips;action detection approaches;action detection problem;classification network;clip sampling;detected clips;detection schemes;heterogeneous graph convolutional network;low applicability;network joint learning;policy network;predefined action;reinforcement learning process;spatio-temporal features;spatio-temporal structural relationships;temporal intersection;visual reinforcement learning","","","","20","IEEE","8 Aug 2023","","","IEEE","IEEE Conferences"
"Prototype Reinforcement for Few-Shot Learning","L. Xu; Q. Xie; B. Jiang; J. Zhang","School of Computer and Information Engineering, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","4912","4916","Few-shot learning requires to recognize novel classes with scarce labeled data. The effectiveness of Prototypical Networks has been recognized in existing studies, however, training on the narrow-size distribution of scarce data usually tends to get biased prototypes. In this paper, we figure out two influencing factors of the process: the feature redundancy and the feature monotony. We propose a simple but effective method to reinforce the prototype. In our method, feature transformation and feature drift are used to reduce the feature redundancy of the prototype. Besides, the pseudo-label strategy is used to recalculate the prototype, enrich the features of the prototype, and alleviate the problem of feature monotony. Effectiveness is shown on two few-shot benchmarks, mini- ImageNet, and CUB. The experimental results show that compared with the ordinary mean prototype, the reinforcement prototype can effectively improve the classification accuracy.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9326820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326820","few-shot learning;meta-learning;protonet;prototype","Training;Automation;Redundancy;Prototypes;Benchmark testing","image classification;learning (artificial intelligence)","feature monotony;few-shot benchmarks;ordinary mean prototype;reinforcement prototype;prototype reinforcement;few-shot learning;scarce labeled data;prototypical networks;narrow-size distribution;biased prototypes;feature redundancy;feature drift;pseudolabel strategy;miniImageNet;CUB","","","","23","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Multi-Robot Exploration in Unknown Environments via Multi-Agent Deep Reinforcement Learning","L. Deng; W. Gong; L. Li","Department of Control Science and Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, Tongji University, Shanghai, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","6898","6902","Mobile robots have been widely used in hazardous environments to obtain information about surroundings for humans. The volume and efficiency of sensing data can be significantly increased if multiple mobile robots collaboration are exploited. In recent years, deep learning and reinforcement learning techniques have been applied to the field of robotics, which perform well in many tasks including exploration in unknown environments. In this paper, to address the multi-robot exploration problem, a multi-agent deep reinforcement learning (MADRL) based method with the centralized training and decentralized execution (CTDE) architecture was proposed. Extensive experimental results show that our method significantly improves the multi-robot exploration performance in unknown environments. On average, the proposed method uses 12.9% less travel distance and reduces 5.8% overlapping areas when compared with traditional methods.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055585","Technology Development; Chinese Academy of Engineering; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055585","multi-robot system;deep reinforcement learning;unknown environment exploration","Deep learning;Training;Automation;Collaboration;Reinforcement learning;Robot sensing systems;Trajectory","deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;reinforcement learning","decentralized execution architecture;deep learning;hazardous environments;multiagent deep reinforcement learning based method;multiple mobile robots collaboration;multirobot exploration performance;multirobot exploration problem;reinforcement learning techniques;sensing data;tasks including exploration","","","","15","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Energy Management Control Strategy of Hybrid Electric Vehicles","F. Chen; P. Mei; H. Xie; S. Yang; B. Xu; C. Huang","School of Transportation Science and Engineering Beihang University, Beijing, China; School of Transportation Science and Engineering Beihang University, Beijing, China; School of Transportation Science and Engineering Beihang University, Beijing, China; School of Transportation Science and Engineering Beihang University, Beijing, China; School of Transportation Science and Engineering Beihang University, Beijing, China; School of Transportation and Civil Engineering Nantong University, Nantong, China","2022 8th International Conference on Control, Automation and Robotics (ICCAR)","31 May 2022","2022","","","248","252","This article is aimed at developing a control strategy based on the Q-learning algorithm for HEVs. The Q-learning algorithm deals with high-dimensional state space problems, and the agent will have a “dimension disaster” problem during the training process. Then a control strategy based on the Deep Q Network (DQN) algorithm is introduced. Since DQN can only output discrete actions, in order to achieve continuous action control, an optimized control strategy based on the Deep Deterministic Policy Gradient (DDPG) algorithm is proposed. Simulation results show that compared with Q-learning and DQN algorithms, the DDPG algorithm converges faster, and the training process is more robust. Besides, the energy optimization control strategy based on the DDPG algorithm can better control the energy of HEVs.","2251-2454","978-1-6654-8116-8","10.1109/ICCAR55106.2022.9782662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782662","Hybrid electric vehicles;energy management strategy;Q-learning algorithm;Deep Q Network algorithm;Deep Deterministic Policy Gradient algorithm","Training;Q-learning;Automation;Simulation;Aerospace electronics;Fuels;Hybrid electric vehicles","energy management systems;hybrid electric vehicles;learning (artificial intelligence);optimal control","reinforcement learning-based energy management control strategy;hybrid electric;Q-learning algorithm deals;high-dimensional state space problems;dimension disaster problem;training process;Deep Q Network algorithm;output discrete actions;continuous action control;optimized control strategy;Deep Deterministic Policy Gradient algorithm;DQN algorithms;DDPG algorithm converges;energy optimization control strategy","","","","16","IEEE","31 May 2022","","","IEEE","IEEE Conferences"
"Expert-guided Triple-memory Deep Reinforcement Learning for Robotic Motion Planning","H. Liu; G. Zhang; X. Li; X. Xiao","College of Information Science and Technology, Donghua University; College of Information Science and Technology, Donghua University; College of Information Science and Technology, Donghua University; College of Information Science and Technology, Donghua University","2023 WRC Symposium on Advanced Robotics and Automation (WRC SARA)","27 Sep 2023","2023","","","245","249","Off-policy deep reinforcement learning (DRL) mostly use memory to improve the processes of data sampling. However, it is difficult for DRL to obtain positive feedback in the early stage of the training. Although an expert memory can solve the problem of sparse rewards in the early stage of training, it can not ensure to obtain the optima, and even leads to overfitting, which affects the performance of the agent in the later stage. In this paper, an expert-guided triple-memory (ETM) framework is proposed to be applied in off-policy DRL, so as to enhance the convergence performance and enhance the stability of the algorithm. Experimental results show that, the proposed approach is feasible and superior in the obstacle-avoidance motion planning tasks for robots.","2835-3358","979-8-3503-0732-0","10.1109/WRCSARA60131.2023.10261862","National Natural Science Foundation of China; Natural Science Foundation of Shanghai; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261862","","Training;Deep learning;Robot motion;Automation;Reinforcement learning;Manipulators;Planning","collision avoidance;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;path planning;reinforcement learning","data sampling;expert memory;expert-guided triple-memory deep reinforcement;expert-guided triple-memory framework;obstacle-avoidance motion planning tasks;off-policy deep reinforcement learning;off-policy DRL;positive feedback;robotic motion planning;sparse rewards","","","","15","IEEE","27 Sep 2023","","","IEEE","IEEE Conferences"
"Selective Data Augmentation for Improving the Performance of Offline Reinforcement Learning","J. Han; J. Kim","Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea","2022 22nd International Conference on Control, Automation and Systems (ICCAS)","9 Jan 2023","2022","","","222","226","This study proposes a new data augmentation technique for offline reinforcement learning (RL). Rather than randomly choosing data points to carry out the data augmentation, our methodology selectively chooses data from sparse subspaces of the dataset to effectively augment the data region that is insufficient in the original dataset. For the augmentation, the subspaces of the dataset would be represented in the latent space created by the variational autoencoder (VAE). Data is then sampled from the latent space and converted back to the original space by using the decoder of the VAE so that the augmented data can be added to the original dataset. By using the VAE, virtual data that does not severely deviate from the original data could be generated because the VAE creates new data points by using the latent space that captures the original data distribution. We evaluate the performance of our methodology using several offline RL datasets generated from OpenAI Gym benchmark control simulations which mainly use state-based inputs.","2642-3901","978-89-93215-24-3","10.23919/ICCAS55662.2022.10003747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003747","Offline Reinforcement Learning;Data Augmentation;Variational Auto Encoder","Automation;Training data;Reinforcement learning;Benchmark testing;Aerospace electronics;Control systems;Decoding","data augmentation;neural nets;reinforcement learning","data augmentation technique;data points;data region;offline reinforcement learning;offline RL datasets;OpenAI Gym benchmark control;original data distribution;sparse subspaces;VAE;variational autoencoder;virtual data","","","","15","","9 Jan 2023","","","IEEE","IEEE Conferences"
"Guiding Deep Reinforcement Learning by Modelling Expert’s Planning Behavior","R. Ma; J. Oyekan","Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, UK; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, UK","2021 7th International Conference on Control, Automation and Robotics (ICCAR)","25 Jun 2021","2021","","","321","325","Deep Reinforcement Learning algorithm has shown its strength in solving complex robot control problems. Nevertheless, the existing challenge is that random and unnecessary exploration may lead to slow convergence, which causes difficulty in the training process. In this paper, we propose a novel framework of using a small amount of demonstration to guide the exploration. Unlike previous works, we model the demonstration as a function between direction preferences and goal-state distance using Neural Network. To evaluate our work, we designed several tasks in the robot simulation platform VREP. The results indicate that our algorithm can guide the exploration and hence improve the performance of Deep Deterministic Policy Gradient(DDPG).","2251-2454","978-1-6654-4986-1","10.1109/ICCAR52225.2021.9463325","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463325","deep reinforcement learning;robot control;demonstration modeling","Training;Automation;Robot control;Neural networks;Reinforcement learning;Planning;Task analysis","gradient methods;learning (artificial intelligence);mobile robots;neural nets","unnecessary exploration;training process;direction preferences;goal-state distance;robot simulation platform VREP;modelling expert;deep reinforcement learning algorithm;complex robot control problems;random exploration;DDPG;deep deterministic policy gradient","","","","14","IEEE","25 Jun 2021","","","IEEE","IEEE Conferences"
"Automatic Data Augmentation by Upper Confidence Bounds for Deep Reinforcement Learning","Y. Gil; J. Baek; J. Park; S. Han","Department of Convergence IT Engineering, Pohang, Korea; Department of Convergence IT Engineering, Pohang, Korea; Department of Convergence IT Engineering, Pohang, Korea; Department of Convergence IT Engineering, Pohang, Korea","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","1199","1203","In visual reinforcement learning (RL), various approaches succeeded to improve data efficiency. However, the approaches fail to show generalization capabilities if different colors or backgrounds are applied to its environment. The lack of generalization capabilities can hinder the use of RL in real-world environment, which contains lot of distractions and noises. In this paper, a novel automatic data augmentation method that can improve generalization capabilities of an RL agent. In the experiments, the proposed method shows better generalization capabilities than other approaches. These results provide a simple automatic data augmentation method for RL that can improve generalization capabilities.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649771","Reinforcement learning;Automatic data augmentation;Upper confidence bounds;Continuous control","Visualization;Automation;Image color analysis;Reinforcement learning;Control systems;Colored noise","data analysis;deep learning (artificial intelligence);image colour analysis;reinforcement learning","generalization capabilities;upper confidence bounds;deep reinforcement learning;visual reinforcement learning;RL agent;simple automatic data augmentation method","","","","22","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Portfolio Optimization Under the Framework of Reinforcement Learning","L. Xucheng; P. Zhihao","School of Computer and Software, Dalian Neusoft University of Information, Dalian, China; School of Computer and Software, Dalian Neusoft University of Information, Dalian, China","2019 11th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA)","7 Oct 2019","2019","","","799","802","Portfolio management is an art and science of making decisions about the investment mix and policy, that matching investments to objectives, assets allocation for the individuals and institutions, balancing risk against performance. In this paper, a neural network is used to train and analysis the history of the assets as well as their portfolio weights for each asset and later evaluate the potential growth for the immediate future under the reinforcement learning(RL) framework, which is based on the recent reinforcement learning(RL) developments, testing is carried with the historical data from cryptocurrency exchange market, camparison with other framework shows that the framework based on RL behaved far better than most other optimization framework in the test period.","2157-1481","978-1-7281-2165-9","10.1109/ICMTMA.2019.00180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858649","Neural networks;Reinforcement learning;Cryptocurrencies trading;Portfolio","Q measurement;Mechatronics;Automation","investment;learning (artificial intelligence);neural nets;optimisation","portfolio optimization;portfolio management;investment mix;investments;assets allocation;neural network;portfolio weights;potential growth;immediate future;cryptocurrency exchange market;optimization framework","","","","6","IEEE","7 Oct 2019","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Autonomous Traffic Flow Capacity Management","S. Barzegar; M. Ruiz; L. Velasco","Optical Communications Group (GCO), Universitat Politècnica de Catalunya (UPC), Barcelona, Spain; Optical Communications Group (GCO), Universitat Politècnica de Catalunya (UPC), Barcelona, Spain; Optical Communications Group (GCO), Universitat Politècnica de Catalunya (UPC), Barcelona, Spain","2023 23rd International Conference on Transparent Optical Networks (ICTON)","8 Aug 2023","2023","","","1","4","As the dynamicity of the traffic increases, the need for self-network operation becomes more evident. One of the solutions that might bring cost savings to network operators, is that of the dynamic capacity management of large packet flows, especially in the context of packet over optical networks. Machine Learning, and particularly Reinforcement Learning (RL), seem to be an enabler for autonomicity, as a result of its inherent capacity to learn from experience. In this tutorial, we introduce RL and review its application for autonomous capacity management of traffic flows.","2161-2064","979-8-3503-0303-2","10.1109/ICTON59386.2023.10207531","Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10207531","reinforcement learning;network automation","Image motion analysis;Computer vision;Q-learning;Costs;Automation;Quality of service;Tutorials","optical fibre networks;reinforcement learning;telecommunication computing;telecommunication traffic","autonomous traffic flow capacity management;cost savings;dynamic capacity management;large packet flows;machine learning;network operators;optical networks;reinforcement learning;self-network operation","","","","14","IEEE","8 Aug 2023","","","IEEE","IEEE Conferences"
"Off-Policy Reinforcement Learning for Synchronization in Multiagent Graphical Games","J. Li; H. Modares; T. Chai; F. L. Lewis; L. Xie","State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; Department of Electrical and Computer Engineering, Missouri University of Science and Technology, Rolla, MO, USA; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; Northeastern University, Shenyang, China; School of Electrical and Electronic Engineering, College of Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Neural Networks and Learning Systems","15 Sep 2017","2017","28","10","2434","2445","This paper develops an off-policy reinforcement learning (RL) algorithm to solve optimal synchronization of multiagent systems. This is accomplished by using the framework of graphical games. In contrast to traditional control protocols, which require complete knowledge of agent dynamics, the proposed off-policy RL algorithm is a model-free approach, in that it solves the optimal synchronization problem without knowing any knowledge of the agent dynamics. A prescribed control policy, called behavior policy, is applied to each agent to generate and collect data for learning. An off-policy Bellman equation is derived for each agent to learn the value function for the policy under evaluation, called target policy, and find an improved policy, simultaneously. Actor and critic neural networks along with least-square approach are employed to approximate target control policies and value functions using the data generated by applying prescribed behavior policies. Finally, an off-policy RL algorithm is presented that is implemented in real time and gives the approximate optimal control policy for each agent using only measured data. It is shown that the optimal distributed policies found by the proposed algorithm satisfy the global Nash equilibrium and synchronize all agents to the leader. Simulation results illustrate the effectiveness of the proposed method.","2162-2388","","10.1109/TNNLS.2016.2609500","National Natural Science Foundation of China(grant numbers:61673280,61104093,61525302,61333012,61304028,61590922,61503257); Open Project of State Key Laboratory of Synthetical Automation for Process Industries(grant numbers:PAL-N201603); Project of Liaoning Province(grant numbers:LJQ2015088,2015020164,2014020138); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7902130","Graphical game;multiagent systems (MAS);neural network (NN);reinforcement learning (RL);synchronization","Synchronization;Games;Nash equilibrium;Protocols;Optimal control;Heuristic algorithms;Mathematical model","approximation theory;dynamic programming;game theory;learning (artificial intelligence);mobile robots;multi-agent systems;optimal control;synchronisation","global Nash equilibrium;optimal distributed policies;approximate optimal control policy;critic neural networks;actor neural networks;value function;off-policy Bellman equation;data collection;data generation;behavior policy;prescribed control policy;model-free approach;agent dynamics;optimal multiagent system synchronization;multiagent graphical games;off-policy RL algorithm;off-policy reinforcement learning algorithm","","134","","31","IEEE","17 Apr 2017","","","IEEE","IEEE Journals"
"A Reinforcement Learning Empowered Cooperative Control Approach for IIoT-Based Virtually Coupled Train Sets","H. Wang; Q. Zhao; S. Lin; D. Cui; C. Luo; L. Zhu; X. Wang; T. Tang","National Research Center of Railway Safety Assessment, Beijing Jiaotong University, Beijing, China; National Research Center of Railway Safety Assessment, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; National Research Center of Railway Safety Assessment, Beijing Jiaotong University, Beijing, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China","IEEE Transactions on Industrial Informatics","5 Apr 2021","2021","17","7","4935","4945","Virtually coupled train sets (VCTS) have been proposed to increase the transportation capacity and the flexibility of railway organization. Due to the lack of reliable wireless communications and accurate perceptual information, the promotion of VCTS was challenged. With the development of industrial Internet of Things (IIoT), an IIoT-based VCTS is built in the article based on the popular communication-based train control architecture. Considering the dynamic and complex operation environment, it is difficult to achieve the efficient cooperative control of VCTS. The reason is that the traditional method is frequently trapped into a local optimization. To resolve the problem, we apply reinforcement learning (RL) to obtain an optimal policy for the IIoT-based VCTS, where the traditional artificial potential field (APF) is taken to develop the reward function. RL can thus search the global optimal policy, whereas APF can help RL to reduce the computation complexity. This can substantially increase the efficiency of the proposed approach. Simulation results confirmed that the proposed RL-based cooperative control approach would bring excellent performance in the IIoT-based VCTS.","1941-0050","","10.1109/TII.2020.3024946","National Natural Science Foundation of China(grant numbers:U18341211,61925302,61971030,61973026); Beijing Natural Science Foundation, and TCT Technology(grant numbers:L181004); Traffic Control Technology(grant numbers:9907006509); State Key Laboratory of Synthetical Automation for Process Industries; Development Program of China Railway Corporation(grant numbers:K2019X010); China Academy of Railway Sciences(grant numbers:2018YJ059); Center of National Railway Intelligent Transportation System Engineering, and Technology(grant numbers:RITS2019KF03,61803019,61803020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200541","Artificial potential field (APF);cooperative control;industrial Internet of Things (IIoT);reinforcement learning (RL);train control system;virtually coupled train sets (VCTS)","Sensors;Rail transportation;Wireless sensor networks;Wireless communication;Rails;Safety;Monitoring","computer network reliability;Internet of Things;optimisation;rail traffic control;railway communication;railway safety;telecommunication control;traffic engineering computing","IIoT-based VCTS;efficient cooperative control;reinforcement learning;RL-based;train sets;reliable wireless communications;communication-based train control architecture","","12","","23","IEEE","18 Sep 2020","","","IEEE","IEEE Journals"
"Fuzzy Control Based on Reinforcement Learning and Subsystem Error Derivatives for Strict-Feedback Systems With an Observer","D. Li; J. Dong","College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China","IEEE Transactions on Fuzzy Systems","4 Aug 2023","2023","31","8","2509","2521","In this article, a novel optimized fuzzy adaptive control method based on tracking error derivatives of subsystems is proposed for strict-feedback systems with unmeasurable states. A cost function based on the tracking error derivative is used. It not only solves the problem that the traditional input quadratic cost function at the infinite time is unbounded, but also solves the problem that the optimal control input derived from the cost function with exponential discount factor cannot make the error asymptotically stable. Considering the case where the states are unmeasurable, a fuzzy state observer is designed that removes the restriction of the Hurwitz equation for the gain parameters. Based on reinforcement learning, the observer, and error derivative cost function, an improved optimized backstepping control method is given. Using observed information and actor-critic structure to train fuzzy logic systems online, the control inputs are obtained to achieve approximate optimal control. Finally, all closed-loop signals are proved to be bounded by the Lyapunov method, and the effectiveness and advantages of the proposed algorithm are verified through two examples.","1941-0034","","10.1109/TFUZZ.2022.3227993","National Natural Science Foundation of China(grant numbers:62273079,61420106016); Fundamental Research Funds for the Central Universities in China(grant numbers:N2004002,N2104005,N182608004); Research Fund of State Key Laboratory of Synthetical Automation for Process Industries in China(grant numbers:2013ZCX01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9979784","Adaptive dynamic programming (ADP);fuzzy adaptive control;fuzzy logic systems (FLSs);fuzzy state observer;optimized backstepping control (OBC);reinforcement learning (RL)","Cost function;Optimal control;Observers;Fuzzy logic;Costs;Backstepping;Steady-state","adaptive control;closed loop systems;control nonlinearities;control system synthesis;feedback;fuzzy control;fuzzy logic;Lyapunov methods;nonlinear control systems;observers;optimal control","actor-critic structure;approximate optimal control;control inputs;error derivative cost function;exponential discount factor;fuzzy control;fuzzy logic systems;fuzzy state observer;improved optimized backstepping control method;infinite time;Lyapunov method;observed information;optimal control input;optimized fuzzy adaptive control method;reinforcement learning;strict-feedback systems;subsystem error derivatives;tracking error derivative;traditional input quadratic cost function;unmeasurable states","","5","","44","IEEE","9 Dec 2022","","","IEEE","IEEE Journals"
"Fuzzy Weight-Based Reinforcement Learning for Event-Triggered Optimal Backstepping Control of Fractional-Order Nonlinear Systems","D. Li; J. Dong","College of Information Science and Engineering, the State Key Laboratory of Synthetical Automation of Process Industries, and the Key Laboratory of Vibration and Control of Aero-Propulsion Systems Ministry of Education, Northeastern University, Shenyang, China; College of Information Science and Engineering, the State Key Laboratory of Synthetical Automation of Process Industries, and the Key Laboratory of Vibration and Control of Aero-Propulsion Systems Ministry of Education, Northeastern University, Shenyang, China","IEEE Transactions on Fuzzy Systems","","2023","PP","99","1","12","Fractional-order nonlinear systems have been widely studied, and their optimal control problems have been difficult to be solved. In this paper, an event-triggered fuzzy reinforcement learning method is proposed to achieve the optimal control of fractional-order nonlinear systems. Firstly, the fractional Hamiltonian-Jacobi-Bellman equation is derived by constructing an equivalent integer-order auxiliary system and the fractional optimal solution under a global performance index is derived. Then, two fuzzy logic systems are used to form an actor-critic structure to approximate the cost function and the optimal controller, respectively. By constructing the Lyapunov function of the optimal fuzzy weight error, the fractional weight learning laws are designed to ensure that the fuzzy weights converge to the optimum. With this approach, it is not necessary to train the fuzzy weights by gradient descent, and the persistent excitation condition is relaxed. Considering reducing the computational burden, an event-triggered mechanism is used to avoid Zeno behavior. Finally, the effectiveness of the algorithm is verified by stability analysis and simulation.","1941-0034","","10.1109/TFUZZ.2023.3294928","National Natural Science Foundation of China(grant numbers:62273079,61420106016); Fundamental Research Funds for the Central Universities in China(grant numbers:N2004002,N2104005,N182608004); Research Fund of State Key Laboratory of Synthetical Automation for Process Industries in China(grant numbers:2013ZCX01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10184190","fractional-order nonlinear systems (FONSs);optimal backstepping control;reinforcement learning (RL);fuzzy weight training;event-triggered;fuzzy logic systems (FLSs)","Optimal control;Mathematical models;Fuzzy logic;Calculus;Nonlinear systems;Performance analysis;Lyapunov methods","","","","1","","","IEEE","14 Jul 2023","","","IEEE","IEEE Early Access Articles"
"Applications of Reinforcement Learning in Frequency Regulation Control of New Power Systems","T. Zhou; Y. Wang; Y. Xu; Q. Wang; Z. Zhu","School of Automation Nanjing University of Science and Technology, Nanjing, China; School of Automation Nanjing University of Science and Technology, Nanjing, China; Product Technology Research and Development Center Jiangsu Frontier Electric Technology Co.Ltd, Nanjing, China; Department of Production Safety Huaneng Nanjing Gas Turbine Power Generation Co., Ltd, Nanjing, China; State Grid Jiangsu Electric Power Engineering Consulting Co., Ltd, Nanjing, China","2022 International Conference on Cyber-Physical Social Intelligence (ICCSI)","16 Dec 2022","2022","","","501","506","With the high-proportion access of new energy, the complexity and uncertainty of new power system are increasing. The frequency stability problem becomes more and more prominent, which brings huge challenges to the operation and control of gird. Reinforcement learning (RL) is one of the most suitable methods for power system optimization and control in artificial intelligence (AI). In order to better grasp and more effectively improve RL frequency regulation control technologies, this paper reviews the research progress of RL algorithm in the field of frequency regulation control of new power systems. Firstly, the basic principle and research branch of RL are introduced. Then the applications of RL in frequency regulation control are investigated in detail for single agent RL and multi-agent RL (MARL). Finally, the future developments for applications of reinforcement learning in frequency regulation control field are summarized and prospected.","","978-1-6654-9835-7","10.1109/ICCSI55536.2022.9970560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9970560","new power system;frequency regulation control;reinforcement learning;multi-agent;AGC","Uncertainty;Reinforcement learning;Power system stability;Control systems;Regulation;Social intelligence;Complexity theory","control engineering computing;multi-agent systems;optimisation;power engineering computing;power generation control;power grids;reinforcement learning","AI;artificial intelligence;frequency regulation control field;frequency stability problem;high-proportion access;multiagent RL;power system optimization;reinforcement learning;RL algorithm;RL frequency regulation control technologies;single agent RL","","","","44","IEEE","16 Dec 2022","","","IEEE","IEEE Conferences"
"A multi-grid reinforcement learning method for energy conservation and comfort of HVAC in buildings","B. Li; L. Xia","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","2015 IEEE International Conference on Automation Science and Engineering (CASE)","8 Oct 2015","2015","","","444","449","Online reinforcement learning often suffers from slow convergence and faults on early stages. In this paper, we propose a multi-grid method of Q-learning to handle these problems. It adopts a coarse model to fast converge to a good policy on early stages and then adopts a fine model to further improve the optimization result. This approach is applied to an optimal control problem of energy conservation and comfort of HVAC in buildings. The optimization algorithm is implemented in simulation using Matlab and EnergyPlus. The results show an improvement of our approach both in the convergence and the performance on early stages, comparing with the traditional Q-learning method for HVAC in the literature. By defining some modified Pareto domination indices, we demonstrate the superiority of multiple performances comprising the competing energy conservation and the comfort.","2161-8089","978-1-4673-8183-3","10.1109/CoASE.2015.7294119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7294119","","Convergence;Q-factor;Buildings;Mathematical model;Learning (artificial intelligence);Energy conservation;Function approximation","building management systems;buildings (structures);control engineering computing;convergence;energy conservation;HVAC;learning (artificial intelligence);optimal control;Pareto optimisation","multigrid reinforcement learning method;energy conservation;HVAC;building;online reinforcement learning;convergence;multigrid method;q-learning;optimal control problem;optimization algorithm;Matlab;EnergyPlus;Pareto domination index","","31","","19","IEEE","8 Oct 2015","","","IEEE","IEEE Conferences"
"Asynchronous deep reinforcement learning for the mobile robot navigation with supervised auxiliary tasks","T. Tongloy; S. Chuwongin; K. Jaksukam; C. Chousangsuntorn; S. Boonsang","Center of Industrial Robots and Automation (CiRA), King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand; Center of Industrial Robots and Automation (CiRA), King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand; King Mongkut's Institute of Technology Ladkrabang, Bangkok, TH; Department of Electrical Engineering, King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand; Department of Electrical Engineering, King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand","2017 2nd International Conference on Robotics and Automation Engineering (ICRAE)","15 Feb 2018","2017","","","68","72","In this paper, we present the method based on asynchronous deep reinforcement learning adapted for the mobile robot navigation with supervised auxiliary tasks. We apply the hybrid Asynchronous Advantage Actor-Critic (A3C) algorithm CPU/GPU based on TensorFlow. The mobile robot is simulated as the navigation tasks on the OpenAI-Gym-Gazebo-based environment with the collaboration with ROS Multimaster. The supervised auxiliary tasks include the depth predictions and the robot position estimation. The simulated mobile robot shows the capability to learn to navigate only the input from raw RGB-image and also perform recognition of the place on the map. We also show that the combination of all possible auxiliary tasks leads to the different learning rate.","","978-1-5386-1306-1","10.1109/ICRAE.2017.8291355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8291355","component;asynchronous reinforcement learning;GA3C;ROS;supervised auxiliary tasks;mobile robot navigation","Task analysis;Navigation;Mobile robots;Computational modeling;Adaptation models;Machine learning","control engineering computing;learning (artificial intelligence);mobile robots;path planning","learning rate;hybrid asynchronous advantage actor;OpenAI-Gym-Gazebo-based environment;ROS multimaster;TensorFlow;depth prediction;navigation tasks;supervised auxiliary tasks;mobile robot navigation;asynchronous deep reinforcement learning;simulated mobile robot;robot position estimation","","12","","12","IEEE","15 Feb 2018","","","IEEE","IEEE Conferences"
"The control of two-wheeled self-balancing vehicle based on reinforcement learning in a continuous domain","P. Xia; Y. Li","School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, Guangdong Province, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, Guangdong Province, China","2017 32nd Youth Academic Annual Conference of Chinese Association of Automation (YAC)","3 Jul 2017","2017","","","1084","1089","The control of a two-wheeled self-balancing vehicle is a complex nonlinear issue in the classical control theory. Applications of reinforcement learning in practical control problems have been proved feasible. In this paper, we present a method that derives from common Actor-Critic algorithm, which is made up of adaptive search network (ASN) and adaptive critic network (ACN). Each network is realized by a BP artificial neural network, and ASN implements the estimation of value function while ACN makes the decision to act. Besides, the TD-error is used in the learning process. In this way, we can handle the whole control task in a continuous domain. The algorithm is finally tested on an appropriate simulation model and a desirable result is achieved.","","978-1-5386-2901-7","10.1109/YAC.2017.7967572","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7967572","Actor-Critic;Continuous states and actions;BP network;TD-error","Mathematical model;Learning (artificial intelligence);Wheels;Algorithm design and analysis;Turning;Machine learning algorithms","adaptive control;backpropagation;control engineering computing;large-scale systems;neural nets;nonlinear control systems;vehicles","two-wheeled self-balancing vehicle;reinforcement learning;complex nonlinear issue;classical control theory;actor-critic algorithm;adaptive search network;ASN;adaptive critic network;ACN;BP artificial neural network;TD-error","","3","","16","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"Load Scheduling for an Electric Water Heater With Forecasted Price Using Deep Reinforcement Learning","J. Cao; L. Dong; L. Xue","School of Automation, Southeast University, Nanjing, China; College of Electronics and Information Engineering, Tongji University, Shanghai, China; School of Automation, Southeast University, Nanjing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","2500","2505","Electric water heaters have time-accumulating effect due to the ability to store energy, which makes them attract much attention in the field of smart homes. The load scheduling problem of an electric water heater is challenging due to the uncertainty of future electricity prices and the applicability of different types of electric water heaters. This paper formulates the problem as a Markov decision process (MDP) and uses deep reinforcement learning (DRL) to obtain the optimal scheduling policy. Specifically, Long Short-Term Memory (LSTM) is applied to forecast the future prices. Then adopt Deep Q Network (DQN) to determine the optimal strategy for this problem. LSTM can deal with the uncertainty of future prices. This model-free approach can handle the applicability of different types of electric water heaters. Simulation results indicate that the proposed algorithm can minimize the cost of electricity and dissatisfaction. In addition, the electricity cost and user dissatisfaction can be balanced by introducing a balance parameter according to user preferences.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9326475","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326475","Load scheduling;incentive-based demand responce;electric water heater;long short-term memory;deep reinforcement learning","Water heating;Resistance heating;Scheduling;Optimal scheduling;Reinforcement learning;Logic gates;Computer architecture","electric heating;learning (artificial intelligence);load forecasting;Markov processes;neural nets;optimisation;power engineering computing;power system economics;pricing;scheduling","electricity cost minimization;DQN;deep Q network;LSTM;long short-term memory;optimal scheduling policy;DRL;MDP;Markov decision process;smart homes;future electricity price forecasting;load scheduling problem;deep reinforcement learning;electric water heater","","2","","20","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Gas Source Localization using Improved Multi-Agent Reinforcement Learning","Z. -P. Wang; H. -N. Wu","The Science and Technology on Aircraft Control Laboratory School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; The Science and Technology on Aircraft Control Laboratory School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","6696","6701","In this paper, an improved multi-agent reinforcement learning (MARL) algorithm is proposed to solve the localization problem of gas source with disturbance sources. Firstly, based on Gaussian dispersion model, multi-point sources are estimated at the initial position of sensor network. Secondly, the multi-agent system is pre-trained in the synthetic environment derived from dispersion model and the estimated source term. Then, the improved MARL algorithm is used to guide the mobile sensors to localize the actual target source. Finally, numerical simulations are given to verify the efficiency of this method.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327850","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327850","Gas source localization (GSL);multi-agent reinforcement learning (MARL);source term estimation (STE);multi-point sources","Sensors;Dispersion;Estimation;Cost function;Location awareness;Genetic algorithms;Reinforcement learning","learning (artificial intelligence);multi-agent systems;natural gas technology;wireless sensor networks","estimated source term;MARL algorithm;gas source localization;multiagent reinforcement learning algorithm;localization problem;disturbance sources;Gaussian dispersion model;multipoint sources;sensor network;mobile sensors","","1","","16","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning with Evolutionary Computation to Policy Search for Autonomous Navigation","C. Zhang; L. Dong; C. Sun","School of Automation Southeast University, Nanjing, China; School of Automation Southeast University, Nanjing, China; School of Electronics and Information Engineering Tongji University, Shanghai, China","2020 35th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","5 Feb 2021","2020","","","288","292","Reinforcement learning has good applications for autonomous navigation in unknown and complex environments. Traditional reinforcement learning methods with the actor-critic framework sometimes will fall into a local optimum because of the complexity of the loss function. Meanwhile, evolutionary computation(EC) is a type of black box optimization algorithm, which has good robustness in policy search but lower sampling efficiency. In order to address the challenge, we introduce an algorithm that combines evolutionary computation with reinforcement learning into navigation intuitively. The parameters of actor neural network are listed as individual characteristics. Each individual represents a policy network. At the end of each episode, individuals with higher fitness function value are selected to the next generation. Other individuals update a certain number of steps through the critic network with shared replay buffer and then move into the next generation. Simulation results demonstrate the effectiveness and feasibility of this algorithm on navigation.","","978-1-7281-7684-0","10.1109/YAC51587.2020.9337605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9337605","Reinforcement learning;Navigation;Evolutionary computation;Fitness function","Navigation;Simulation;Reinforcement learning;Evolutionary computation;Task analysis;Next generation networking;Autonomous robots","control engineering computing;evolutionary computation;learning (artificial intelligence);mobile robots;neural nets;optimisation;path planning;search problems","critic network;fitness function value;policy network;actor neural network;black box optimization algorithm;autonomous navigation;policy search;evolutionary computation;reinforcement learning","","1","","19","IEEE","5 Feb 2021","","","IEEE","IEEE Conferences"
"Impedance and Trajectory Adaptation for Contact Robots Using Integral Reinforcement Learning","G. Peng; C. Yang; Y. Li; C. L. Philip Chen","School of Automation Nanjing, University of Information Science and Technology, Nanjing, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Engineering and Informatics, University of Sussex, Brighton, UK; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","1542","1547","In this paper, we develop a learning controller that adapts and tracks the impedance and trajectory for robots interacting with unknown environments. Impedance adaptation is used to compensate for contacting with the environment, while the reference trajectory learning is to maintain a prescribed interaction force. The tracking performance is ensured by an adaptive learning controller with Integral Reinforcement learning (IRL) for partially unknown system dynamics. The contact dynamics are analysed via Lyapunov theory and the effectiveness of the proposed control method is verified through simulations.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023727","Nature; Natural Science Foundation of Jiangsu Province; Startup Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023727","Robot-environment interaction;adaptive control;force control","Adaptation models;Trajectory tracking;System dynamics;Force;Reinforcement learning;Control systems;Mathematical models","adaptive control;control engineering computing;force control;Lyapunov methods;mechanical contact;mobile robots;reinforcement learning;robot dynamics","adaptive learning controller;contact dynamics;contact robots;impedance adaptation;integral reinforcement learning;interaction force;IRL;Lyapunov theory;partially unknown system dynamics;reference trajectory learning;robot interaction;tracking performance;trajectory adaptation","","","","20","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Adaptive Flexible Switching Mode Control of the Aircraft Skin Inspection Robot Using Integral Reinforcement Learning","X. Wu; C. Wang","College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","2220","2225","A flexible switching mode control scheme is introduced for the two adsorption systems of the aircraft skin inspection robot (ASIR) with completely unknown system dynamics. This flexible switching mode control issue is converted into a two-subsystem tracking control issue. The original adsorption systems and their desired trajectories are reformed as the new two-subsystem augmented systems. An augmented algebraic Riccati equation (ARE) is derived based on the performance function with the two-subsystem augmented systems. The adaptive tracking control policy using the integral reinforcement learning (IRL) is developed to overcome the optimal tracking problem of the augmented systems. The simulation results show the effectiveness of the proposed method.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327735","aircraft skin inspection robot;flexible switching mode;optimal tracking problem;integral reinforcement learning","Switches;Trajectory;Aircraft;Adsorption;Skin;Robots;Aerospace control","adaptive control;aircraft;flexible structures;inspection;learning (artificial intelligence);learning systems;mobile robots;optimal control;Riccati equations;switching systems (control);trajectory control","optimal tracking problem;ASIR;two-subsystem tracking control;augmented algebraic Riccati equation;adsorption system;completely unknown system dynamics;aircraft skin inspection robot;adaptive flexible switching mode control;integral reinforcement learning;adaptive tracking control policy;two-subsystem augmented systems","","","","22","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Learning of communication codes in multi-agent reinforcement learning problem","T. Kasai; H. Tenmoto; A. Kamiya","Advanced Course of Electronic and Information Systems Engineering, Kushiro National College of Technology, Kushiro, Hokkaido, Japan; Department of Information Engineering, Kushiro National College of Technology, Kushiro, Hokkaido, Japan; Department of Information Engineering, Kushiro National College of Technology, Kushiro, Hokkaido, Japan","2008 IEEE Conference on Soft Computing in Industrial Applications","29 May 2009","2008","","","1","6","Realization of cooperative behavior in multi-agent system is important for improving problem solving ability. Reinforcement learning is one of the learning methods for such cooperative behavior of agents. In this paper, we consider pursuit problem for multi-agent reinforcement learning with communication between the agents. In our study, the agents obtain communication codes through learning. Here, the codes are rules for communicating appropriate information under various situations. We call the learning of communication codes signal learning. The signal is expressed by bit sequence, and its length is set to be variable. We carried out experiment for performance comparison with varying the signal length from 0 to 4 bits. As a result, it has been shown that, in learning precision, the case of 1 bit or more bits communication outperformed the case of no communication. It also has been shown that 4 bits communication produced the best result among the five cases, while learning with longer signals required much more iterations.","","978-1-4244-3782-5","10.1109/SMCIA.2008.5045926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5045926","multi-agent systems;reinforcement learning;agents communication","Educational institutions;Systems engineering and theory;Telephony;Multiagent systems;Problem-solving;Learning systems;Computer applications;Communication industry;Computer industry;Electronics industry","codes;learning (artificial intelligence);multi-agent systems","multiagent reinforcement learning problem;communication code learning;cooperative behavior;appropriate information communication;signal learning;bit sequence","","13","","15","IEEE","29 May 2009","","","IEEE","IEEE Conferences"
"Human-Following and -guiding in Crowded Environments using Semantic Deep-Reinforcement-Learning for Mobile Service Robots","L. Kästner; B. Fatloun; Z. Shen; D. Gawrisch; J. Lambrecht","Faculty of Electrical Engineering, and Computer Science, Chair Industry Grade Networks and Clouds, Berlin Institute of Technology, Berlin, Germany; Faculty of Electrical Engineering, and Computer Science, Chair Industry Grade Networks and Clouds, Berlin Institute of Technology, Berlin, Germany; Faculty of Electrical Engineering, and Computer Science, Chair Industry Grade Networks and Clouds, Berlin Institute of Technology, Berlin, Germany; Faculty of Electrical Engineering, and Computer Science, Chair Industry Grade Networks and Clouds, Berlin Institute of Technology, Berlin, Germany; Faculty of Electrical Engineering, and Computer Science, Chair Industry Grade Networks and Clouds, Berlin Institute of Technology, Berlin, Germany","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","833","839","Assistance robots have gained widespread attention in various industries such as logistics and human assistance. The tasks of guiding or following a human in a crowded environment such as airports or train stations to carry weight or goods is still an open problem. In these use cases, the robot is not only required to intelligently interact with humans, but also to navigate safely among crowds. Thus, especially highly dynamic environments pose a grand challenge due to the volatile behavior patterns and unpredictable movements of humans. In this paper, we propose a Deep-Reinforcement-Learning-based agent for human-guiding and -following tasks in crowded environments. Therefore, we incorporate semantic information to provide the agent with high-level information like the social states of humans, safety models, and class types. We evaluate our proposed approach against a benchmark approach without semantic information and demonstrated enhanced navigational safety and robustness. Moreover, we demonstrate that the agent could learn to adapt its behavior to humans, which improves the human-robot interaction significantly.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812111","","Visualization;Adaptation models;Navigation;Service robots;Semantics;Robustness;Behavioral sciences","deep learning (artificial intelligence);human-robot interaction;mobile robots;reinforcement learning;semantic networks;service robots","crowded environment;mobile service robots;assistance robots;human assistance;dynamic environments;semantic information;high-level information;human-robot interaction;human-guiding;enhanced navigational safety;semantic deep-reinforcement-learning-based agent","","3","","25","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"A New Reinforcement Learning for Group-Based Marshaling Plan Considering Desired Layout of Containers in Port Terminals","Y. Hirashima; N. Ishikawa; K. Takeda","Department of Systems Engineering, Okayama University, Okayama, Japan; Okayama Ricoh Company Limited, Okayama, Japan; Hiroshima Research & Development Center, Mitsubishi Heavy Industries Limited, Hiroshima, Japan","2006 IEEE International Conference on Networking, Sensing and Control","14 Aug 2006","2006","","","670","675","In container yard terminals, containers brought by trucks in the random order. Containers have to be loaded into the ship in a certain order, since each container has its own shipping destination and it cannot be rearranged after loading. Therefore, containers have to be rearranged from the initial arrangement into the desired arrangement before shipping. In the problem, the number of container-arrangements increases by the exponential rate with increase of total count of containers and the rearrangement process occupies large part of total run time of material handling operation at the terminal. Moreover, conventional methods require enormous time and cost to derive an admissible result for rearrangement process. In this paper, a Q-Learning algorithm considering the desired position of containers for a marshaling in the container yard terminal is proposed. In the proposed method, the learning process consists of two parts: rearrangement plan assuring explicit transfer of container to the desired position and removal plan for preparing the rearrange operation. Moreover in the proposed method, each container has several desired positions, so that the learning performance of the method can be improved as compared to the conventional method. In order to show effectiveness of the proposed method, computer simulations for several examples are conducted.","","1-4244-0065-1","10.1109/ICNSC.2006.1673226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1673226","Container marshaling;Block stacking problem;Q-learning;Reinforcement learning;Binary tree","Learning;Containers;Marine vehicles;Loading;Materials handling;Costs;Cranes;Computer simulation;Stacking;Binary trees","containers;learning (artificial intelligence);loading;production engineering computing","Container marshaling;Block stacking problem;Q-learning;Reinforcement learning;Binary tree","","3","","8","IEEE","14 Aug 2006","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Carbon Nanotube Growth Automation","A. Pandey; R. Surya; M. Maschmann; P. Calyam","Dept. of Electrical Engineering and Computer Science, University of Missouri-Columbia, USA; Dept. of Mechanical and Aerospace Engineering, University of Missouri-Columbia, USA; Dept. of Mechanical and Aerospace Engineering, University of Missouri-Columbia, USA; Dept. of Electrical Engineering and Computer Science, University of Missouri-Columbia, USA","2021 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)","26 Apr 2022","2021","","","1","10","Experimental research such as cell cultures and carbon nanotube (CNT) growth are largely governed by following predefined execution protocols with fine-tuned control of parameters. There are promising opportunities to apply reinforcement learning (RL), an established learning technique in the area of artificial intelligence, in order to automate CNT growth process and accelerate related scientific breakthroughs in material discovery. Although there are benefits in RL-based exploration and exploitation methodologies, there are also challenges in developing relevant learning policies in experimental settings relating to CNT growth. In this paper, we present a novel data-driven RL approach for assisting experimental CNT growth. Our approach focuses on developing an RL model to learn from simulation-based images and characteristics of temporal CNT growth, considering various growth parameters. Our RL model learns from CNT growth variation in a simulation-based environment where critical control parameters, such as density, growth rate, tube radius, tube stiffness, and Van der Waals forces, are used. Our RL model enables CNT growth automation in order to explore of a wider range of growth conditions, and improve reproducibility. The ultimate goal of our RL model is to achieve desired CNT growth by dynamically controlling growth parameters throughout a sequence of experiments. We evaluate the effectiveness of our RL approach by measuring the improvement in the maximum compressive strength of a carbon nanotube ‘with’ and ‘without’ the RL model. Our results show the effectiveness of course correction recommended by our RL approach when controlling the parameters of angular deviation and rate of growth of carbon nanotubes, when compared against non-regulated CNT growth.","2332-5615","978-1-6654-2471-4","10.1109/AIPR52630.2021.9762145","National Science Foundation(grant numbers:CMMI-2026847); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762145","Carbon Nanotube Growth;Reinforcement Learning;Material Properties","Training;Automation;Protocols;Reinforcement learning;Carbon nanotubes;Mechanical factors;Reproducibility of results","carbon nanotubes;compressive strength;physics computing;reinforcement learning;rigidity;van der Waals forces","data-driven RL approach;simulation-based images;temporal CNT growth;critical control parameters;CNT growth automation;nonregulated CNT growth;reinforcement learning based carbon nanotube growth automation;tube radius;tube stiffness;van der Waals forces;maximum compressive strength;C","","1","","18","IEEE","26 Apr 2022","","","IEEE","IEEE Conferences"
"Robust Output Regulation and Reinforcement Learning-Based Output Tracking Design for Unknown Linear Discrete-Time Systems","C. Chen; L. Xie; Y. Jiang; K. Xie; S. Xie","School of Automation, Guangdong Key Laboratory of IoT Information Technology, Guangdong University of Technology, Guangzhou, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Biomedical Engineering, City University of Hong Kong, Hong Kong, China; School of Automation, 111 Center for Intelligent Batch Manufacturing Based on IoT Technology, Guangdong University of Technology, Guangzhou, China; Key Laboratory of Intelligent Information Processing and System Integration of IoT, Ministry of Education, Guangdong-HongKong-Macao Joint Laboratory for Smart Discrete Manufacturing, Guangzhou, China","IEEE Transactions on Automatic Control","28 Mar 2023","2023","68","4","2391","2398","In this article, we investigate the optimal output tracking problem for linear discrete-time systems with unknown dynamics using reinforcement learning (RL) and robust output regulation theory. This output tracking problem only allows to utilize the outputs of the reference system and the controlled system, rather than their states, and differs from most existing works that depend on the state of the system. The optimal tracking problem is formulated into a linear quadratic regulation problem by proposing a family of dynamic discrete-time controllers. Then, it is shown that solving the output tracking problem is equivalent to solving output regulation equations, whose solution, however, requires the knowledge of the complete and accurate system dynamics. To remove such a requirement, an off-policy RL algorithm is proposed using only the measured output data along the trajectory of the system and the reference output. By introducing reexpression error and analyzing the rank condition of the parameterization matrix, we ensure the uniqueness of the proposed RL-based optimal control via output feedback.","1558-2523","","10.1109/TAC.2022.3172590","National Natural Science Foundation of China(grant numbers:61973087,U1911401); State Key Laboratory of Synthetical Automation for Process Industries(grant numbers:2020-KF-21-02); Wallenberg-NTU Presidential Postdoctoral Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9769938","Adaptive optimal control;output tracking;reinforcement learning (RL);robust output regulation","Regulation;Optimal control;System dynamics;Standards;Trajectory;Process control;Output feedback","control system synthesis;discrete time systems;feedback;linear quadratic control;linear systems;reinforcement learning;robust control","dynamic discrete-time controllers;linear quadratic regulation problem;off-policy RL algorithm;optimal output tracking problem;optimal tracking problem;output feedback;output regulation equations;parameterization matrix;reexpression error;reinforcement learning-based output tracking design;RL-based optimal control;robust output regulation theory;unknown dynamics;unknown linear discrete-time systems","","6","","38","IEEE","5 May 2022","","","IEEE","IEEE Journals"
"A multi-agent coordination of a supply chain ordering management with multiple members using Reinforcement Learning","R. Sun; G. Zhao; Chunhua Yin","School of Information Management, Beijing Information Science and Technology University, Beijing, China; School of Information Management, Beijing Information Science and Technology University, Beijing, China; School of Information Management, Beijing Information Science and Technology University, Beijing, China","2010 8th IEEE International Conference on Industrial Informatics","16 Aug 2010","2010","","","612","616","Improving decision-making practices in a supply chain is a major source of competitive advantage in today's uncertain business environments. There is strong evidence of success in the supply chain performance in cases with high coordination among echelons. The bullwhip effect is an important phenomenon in a supply chain, in which the order variability increases as orders move up in a supply chain. Reinforcement Learning (RL) is successfully applied to some dynamical and unpredictable domains. By surveying some efficient multi-agent RL models, this paper proposes a multi-agent coordination mechanism for a supply chain ordering management system with multiple members by the method of the RL. As the improvement to previous works using RL in the supply chain ordering management domain, the method proposed in this paper can be utilized to deal with multiple members in each echelon. As a result, the RL agent derives the maximal profit using the RL technique in the stochastic supply chain with multiple echelons and multiple members in each echelon.","2378-363X","978-1-4244-7300-7","10.1109/INDIN.2010.5549671","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5549671","","Supply chains;Supply chain management;Learning;Costs;Demand forecasting;Production;Fluctuations;Manufacturing;Decision making;Technology management","decision making;learning (artificial intelligence);multi-agent systems;stochastic processes;supply chain management;supply chains","multi-agent system;supply chain ordering management;reinforcement learning;decision making;stochastic supply chain;multiple echelon","","2","","24","IEEE","16 Aug 2010","","","IEEE","IEEE Conferences"
"Reinforcement Learning Neural Network to the Problem of Autonomous Mobile Robot Obstacle Avoidance","Bing-Qiang Huang; Guang-Yi Cao; Min Guo","Department of Automation, Shanghai Jiaotong University, Shanghai, China; Department of Automation, Shanghai Jiaotong University, Shanghai, China; Department of Net Building, Jiaxing Telecom, Jiaxing, China","2005 International Conference on Machine Learning and Cybernetics","7 Nov 2005","2005","1","","85","89","An approach to the problem of autonomous mobile robot obstacle avoidance using reinforcement learning neural network is proposed in this paper. Q-learning is one kind of reinforcement learning method that is similar to dynamic programming and the neural network has a powerful ability to store the values. We integrate these two methods with the aim to ensure autonomous robot behavior in complicated unpredictable environment. The simulation results show that the simulated robot using the reinforcement learning neural network can enhance its learning ability obviously and can finish the given task in a complex environment.","2160-1348","0-7803-9091-1","10.1109/ICMLC.2005.1526924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1526924","Reinforcement learning;Reinforcement learning neural network;Obstacle avoidance","Neural networks;Mobile robots;Robotics and automation;Learning systems;Intelligent robots;Intelligent structures;Telecommunications;Electronic mail;Dynamic programming;Intelligent systems","","Reinforcement learning;Reinforcement learning neural network;Obstacle avoidance","","28","1","12","IEEE","7 Nov 2005","","","IEEE","IEEE Conferences"
"Reinforcement learning methods for finding equilibria and tracking evolution paths in conflicts","D. Li; J. Jiang; H. Xu; K. W. Hipel","College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Systems Design Engineering, University of Waterloo, Waterloo, ONT, Canada; Department of Systems Design Engineering, University of Waterloo, Waterloo, ONT, Canada","2008 IEEE International Conference on Systems, Man and Cybernetics","7 Apr 2009","2008","","","3292","3297","The search for the equilibrium states of a given conflict is a major issue in conflict analyses. There are several traditional methodologies to determine the equilibrium states of a strategic conflict, such as logical definitions within the graph model for conflict resolution and the matrix representation for conflict resolution. However, these methods depend on a graphical or mathematical representation and need analytical expressions to calculate the equilibrium states. Reinforcement learning (RL) is a type of machine learning method that can search for the equilibrium states by trial-and-error without the need for a precise mathematical model of the conflict problem. This paper proposes a novel multiple RL technique that deals with conflict resolution problems for the case of two decision makers. Moreover, the proposed method cannot only find equilibria, but also track all paths from any status quo to the equilibria in conflicts when these paths exist. This method is evaluated using two well-known conflict analysis examples. The experimental results show that the proposed method can quickly, correctly, and efficiently find the equilibria and track the evolution paths.","1062-922X","978-1-4244-2383-5","10.1109/ICSMC.2008.4811804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4811804","","Stability analysis;Game theory;Design engineering;Educational institutions;Design automation;Systems engineering and theory;Learning systems;Mathematical model;Delta modulation;Symmetric matrices","graph theory;learning (artificial intelligence);matrix algebra","reinforcement learning;evolution paths tracking;equilibrium states;conflict analyses;strategic conflict;matrix representation;conflict resolution;graphical representation;mathematical representation;machine learning;trial-and-error","","2","","21","IEEE","7 Apr 2009","","","IEEE","IEEE Conferences"
"Energy Trading in Smart Grid: A Deep Reinforcement Learning-based Approach","F. Zhang; Q. Yang","School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China; SKLMSE Lab, MOE Key Laboratory for Intelligent Networks and Network Security, School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China","2020 Chinese Control And Decision Conference (CCDC)","11 Aug 2020","2020","","","3677","3682","To achieve the efficient operation of the smart grid, appropriate energy trading strategy plays an important role in reducing multi-agent costs in the trading process as well as alleviating grid pressure. However, with the increase of the number of participants in smart grid, energy trading has been greatly challenged in terms of stable and effective operation. In this paper, we propose a deep reinforcement learning-based energy double auction trading strategy. Through the deep reinforcement learning algorithm, buyers and sellers can gradually learn the environment by treating the three elements: total supply, total demand and their own supply and demand as states, in addition, regarding both bidding price and quantity as bidding strategy. Results from simulation indicate that as the learning continues and reaches the convergence, both the cost which buyers pay in the auction has decreased significantly, and the profit which sellers earn in the auction will increase.","1948-9447","978-1-7281-5855-6","10.1109/CCDC49329.2020.9164350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9164350","Smart Grid;Energy Trading;Double Auction;Deep Reinforcement Learning","Smart grids;Machine learning;Resource management;Neural networks;Supply and demand;Optimization;Automation","electronic commerce;learning (artificial intelligence);multi-agent systems;neural nets;power engineering computing;power markets;pricing;smart power grids","smart grid;bidding strategy;energy trading;multiagent costs;trading process;deep reinforcement learning;energy double auction trading strategy;total supply;total demand;auction","","1","","12","IEEE","11 Aug 2020","","","IEEE","IEEE Conferences"
"Reinforcement learning based on human-computer interaction","Fang Liu; Jian-Bo Su","Department of Automation, Research Center of Intelligent Robotics, Shanghai JiaoTong University, Shanghai, China; Department of Automation, Research Center of Intelligent Robotics, Shanghai JiaoTong University, Shanghai, China","Proceedings. International Conference on Machine Learning and Cybernetics","19 Feb 2003","2002","2","","623","627 vol.2","A novel interactive learning structure integrated with a reinforcement learning algorithm and human-computer interaction (HCI) is proposed. This interactive learning system can benefit from measurements of the distance between the current state and goal state via an operator's professional knowledge. Thus, the learning procedure is expected to be more efficient. A guess-number task is explored to evaluate the proposed learning system. Experimental results show that the learning efficiency and convergence rate are both increased compared with the normal reinforcement learning method.","","0-7803-7508-4","10.1109/ICMLC.2002.1174410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1174410","","Human computer interaction;Learning systems;Robotics and automation;Intelligent robots;State-space methods;Artificial intelligence;Feedback;Supervised learning;Intelligent structures;Human robot interaction","learning (artificial intelligence);man-machine systems;interactive systems;convergence","reinforcement learning;human-computer interaction;interactive learning structure;current state goal state distance;operator professional knowledge;guess-number task;learning efficiency;convergence rate;HCI-based Q-learning","","","","17","IEEE","19 Feb 2003","","","IEEE","IEEE Conferences"
"A simulation of ant formation and foraging using fuzzy logic and Reinforcement Learning","S. Afshar; M. J. Mahjoob","Center for Mechatronics and Automation, School of Mechanical Engineering, University of Tehran, Tehran, Iran; Center for Mechatronics and Automation, School of Mechanical Engineering, University of Tehran, Iran","2008 IEEE Conference on Cybernetics and Intelligent Systems","11 Nov 2008","2008","","","806","811","Pheromone trails laid by foraging ants serve as a positive feedback mechanism in the ant colonies to share information (in search of food sources). The simulation conducted here of this swarm intelligence can help to realize the process and implement it further for artificial swarms. With available instrumentation we may easily record the agentspsila position in each step. Pheromone trails are then generated and each agent learns to follow the trail. Fuzzy logic is used to approximate pheromone value at each point. Agents learn to behave like ants using a reinforcement learning (RL) process. The algorithm has appropriate parameters that may be set according to the search space dimension. Simulation results are in good agreement with the observations of antspsila behavior.","2326-8239","978-1-4244-1673-8","10.1109/ICCIS.2008.4670940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670940","ant formation;foraging;reinforcement learning","Fuzzy logic;Learning;Insects;Robot kinematics;Chemicals;Mechatronics;Automation;Mechanical engineering;Particle swarm optimization;Mobile robots","control engineering computing;fuzzy logic;learning (artificial intelligence);mobile robots;multi-robot systems","fuzzy logic;reinforcement learning;ant formation simulation;foraging;swarm intelligence","","","","14","IEEE","11 Nov 2008","","","IEEE","IEEE Conferences"
"Predictive Hebbian learning of representation in a fast reinforcement controller","S. R. Schultz; M. A. Jabri","Systems Engineering and Design Automation Laboratory, Sydney University Electrical Engineering, Australia; Systems Engineering and Design Automation Laboratory, Sydney University Electrical Engineering, Australia","Proceedings of ANZIIS '94 - Australian New Zealnd Intelligent Information Systems Conference","6 Aug 2002","1994","","","56","60","A particular version of the cart-pole problem has recently been solved trivially by Moody and Tresp (1994). We present a reinforcement learning pole balancer which learns a solution to the problem nearly as quickly. Our controller, however, does not make use of a predefined representation of the input space, but instead learns an appropriate representation using a multilayer evaluation network. The hidden (representation) layer of the evaluation network is adapted using a predictive Hebbian learning algorithm.<>","","0-7803-2404-8","10.1109/ANZIIS.1994.396950","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=396950","","Hebbian theory;Learning;Automatic control;Control systems;Prediction algorithms;Neurons;Design engineering;Systems engineering and theory;Design automation;Laboratories","multivariable control systems;nonlinear control systems;Hebbian learning;multilayer perceptrons;feedforward neural nets;predictive control;adaptive control","predictive Hebbian learning algorithm;representation learning;fast reinforcement controller;cart-pole problem;reinforcement learning;pole balancer;multilayer evaluation network;hidden layer adaptation","","","","13","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Multi-agent reinforcement learning: an approach based on agents' cooperation for a common goal","Guo-quan Wang; Hai-bin Yu","Shenyang Institute of Automation, Chinese Academy and Sciences, Shenyang, China; Shenyang Institute of Automation, Chinese Academy and Sciences, Shenyang, China","8th International Conference on Computer Supported Cooperative Work in Design","8 Nov 2004","2004","1","","336","339 Vol.1","This paper is devoted to the problem of reinforcement learning in multi-agent systems. Multi-agent systems form a particular type of distributed artificial intelligence system. This work presents an approach based on agents' cooperation for a common goal. By using other agents' experiences and knowledge, an agent may learn faster, make fewer mistakes, and create some rules for unseen situations. But the information communion among agents is deficient and limited. In this paper, we assume that every agent can only observe its neighbors' current positions and can see whether or not they reach the goal after the actions have been taken. Experimental results show the effectiveness of the proposed approach.","","0-7803-7941-1","10.1109/CACWD.2004.1349042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1349042","","Learning;Multiagent systems;Equations;Artificial intelligence;Control systems;Automation;Distributed control;Cognitive science;Contracts;Random variables","learning (artificial intelligence);multi-agent systems;groupware","multi-agent reinforcement learning;agent cooperation;multi-agent system;distributed artificial intelligence system;information communion","","","","7","IEEE","8 Nov 2004","","","IEEE","IEEE Conferences"
"Reinforcement learning for Order Acceptance on a shared resource","M. M. Hing; A. van Harten; P. Schuur","Dept. of Technology and Management, University of Twente, The Netherlands; Dept. of Technology and Management, University of Twente, The Netherlands; Dept. of Technology and Management, University of Twente, The Netherlands","Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.","5 Jun 2003","2002","3","","1454","1458 vol.3","Order acceptance (OA) is one of the main functions in business control. Basically, OA involves for each order a reject/accept decision. Always accepting an order when capacity is available could disable the system to accept more convenient orders in the future with opportunity losses as a consequence. Another important aspect is the availability of information to the decision-maker. We use the stochastic modeling approach, Markov decision theory and learning methods from artificial intelligence to find decision policies, even under uncertain information. Reinforcement learning (RL) is a quite new approach in OA. It is capable of learning both the decision policy and incomplete information, simultaneously. It is shown here that RL works well compared with heuristics. Finding good heuristics in a complex situation is a delicate art. It is demonstrated that a RL trained agent can be used to support the detection of good heuristics.","","981-04-7524-1","10.1109/ICONIP.2002.1202861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1202861","","Learning;Uncertainty;Technology management;Stochastic processes;Decision theory;Artificial intelligence;Art;Job production systems;Process planning;Production planning","learning (artificial intelligence);Markov processes;decision theory;order processing;resource allocation;optimisation","reinforcement learning;order acceptance;shared resource;stochastic modeling;Markov decision theory;artificial intelligence;heuristics","","","1","3","","5 Jun 2003","","","IEEE","IEEE Conferences"
"Dynamic Content Update for Wireless Edge Caching via Deep Reinforcement Learning","P. Wu; J. Li; L. Shi; M. Ding; K. Cai; F. Yang","School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; Science and Math Cluster, Singapore University of Technology and Design, Singapore; Data61, CSIRO, Sydney, Australia; Science and Math Cluster, Singapore University of Technology and Design, Singapore; China Unicom Jiangsu Branch, Nanjing, China","IEEE Communications Letters","14 Oct 2019","2019","23","10","1773","1777","This letter studies a basic wireless caching network, where a source server is connected to a cache-enabled base station (BS) that serves multiple requesting users. A critical problem is how to improve cache hit rate under dynamic content popularity. To solve this problem, the primary contribution of this letter is to develop a novel dynamic content update strategy with the aid of deep reinforcement learning. Considering that the BS is unaware of content popularities, the proposed strategy dynamically updates the BS cache according to the time-varying requests and the BS cached contents. Toward this end, we model the problem of cache update as a Markov decision process and put forth an efficient algorithm that builds upon the long short-term memory network and external memory to enhance the decision making ability of the BS. Simulation results show that the proposed algorithm can achieve not only a higher average reward than deep Q-network but also a higher cache hit rate than the existing replacement policies, such as the least recently used, first-in first-out, and deep Q-network-based algorithms.","1558-2558","","10.1109/LCOMM.2019.2931688","National Key R&D Program(grant numbers:2018YFB1004800); National Natural Science Foundation of China(grant numbers:61872184,61727802); Ministry of Education - Singapore(grant numbers:MOE2016-T2-2-054,SUTD-ZJU Grant ZJURP1500102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778670","Content update;Markov decision process;deep reinforcement learning;cache hit rate;long-term reward","Servers;Wireless communication;Reinforcement learning;Markov processes;Heuristic algorithms;Decision making;Toy manufacturing industry","cache storage;decision making;learning (artificial intelligence);Markov processes;recurrent neural nets;telecommunication computing","dynamic content update strategy;cache-enabled base station;wireless caching network;reinforcement learning;wireless edge caching;deep Q-network-based algorithms;short-term memory network;Markov decision process","","45","","12","IEEE","29 Jul 2019","","","IEEE","IEEE Journals"
"Automatic core design using reinforcement learning","Y. Kobayashi; E. Aiyoshi","TEPCO SYSTEMS CORPORATION, Tokyo, Japan; Keio University, Yokohama Kanagawa, Japan","Proceedings of the 2004 American Control Conference","2 May 2005","2004","6","","5784","5789 vol.6","This paper deals with the application of multi-agents algorithm to the core design tool in a nuclear industry. We develop an original solution algorithm for the automatic core design of boiling water reactor using multi-agents and reinforcement learning. The characteristics of this algorithm are that the coupling structure and the coupling operation suitable for the assigned problem are assumed, and an optimal solution is obtained by mutual interference in multi-state transitions using multi-agents. We have already proposed an integrated optimization algorithm using a two-stage genetic algorithm for the automatic core design. The objective of this approach is to improve the convergence performance of the optimization in the automatic core design. We compared the results of the proposed technique using multi-agents algorithm with the two-stage genetic algorithm that had been proposed before. The proposed technique is shown to be effective in reducing the iteration numbers in the search process.","0743-1619","0-7803-8335-4","10.23919/ACC.2004.1384779","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1384779","","Learning;Design optimization;Fuels;Algorithm design and analysis;Inductors;Assembly;Genetic algorithms;Mutual coupling;Coolants;Convergence","nuclear engineering computing;nuclear power stations;boilers;multi-agent systems;learning (artificial intelligence);genetic algorithms;convergence","automatic core design tool;reinforcement learning;multiagents algorithm;nuclear industry;boiling water reactor;coupling structure;mutual interference;multistate transitions;integrated optimization algorithm;two stage genetic algorithm;convergence","","2","","4","","2 May 2005","","","IEEE","IEEE Conferences"
"Estimating Virtual Fixture Parameters in Digital Twin Environments for Robot Manipulation Tasks using Reinforcement Learning","D. F. Prado; E. Steinbach","Technical University of Munich / School of Computation, Information and Technology / Chair of Media Technology, and Munich Institute of Robotics and Machine Intelligence (MIRMI), Munich, Germany; Technical University of Munich / School of Computation, Information and Technology / Chair of Media Technology, and Munich Institute of Robotics and Machine Intelligence (MIRMI), Munich, Germany","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","6","In recent years, the idea of teleoperating robots to perform manipulation tasks has gained popularity. Being able to take remote control of a robotic system performing an assembly task provides exciting possibilities for industry, not only operating the robot in hazardous environments, but also facilitating the accessibility of human workers, requesting help from a remote expert, or even teaching skills. With the rise of teleoperated manipulators, it is expected that tools that facilitate the interaction of humans with the remote environment will be more prevalent in industry, leading to a greater availability of them. This is the case with Virtual Fixtures (VFs), which are collections of abstract sensory information overlaid on top of reflected sensory feedback from a remote environment. Other ever more prevalent tools are Digital Twins (DTs), virtual representations of systems that facilitate bidirectional communication between the real and the virtual worlds. Being more likely than ever that factories have both an implemented Digital Twin of a system and VFs for a particular manipulation system, we propose a method to leverage both tools. In this paper, methods to integrate already existing VFs in a Reinforcement Learning (RL) pipeline and to use RL to construct an optimal VF to aid the human user in a telemanipulation task are proposed, tackling the cumbersome problems of reward function and VF design. The results show that this approach is able to correctly estimate the right parameters of predefined VFs and open the possibility to future work in this topic.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260408","","Industries;Training;Shape;Service robots;Fixtures;Estimation;Reinforcement learning","control engineering computing;digital twins;feedback;human-robot interaction;industrial manipulators;manipulators;production engineering computing;reinforcement learning;telerobotics","abstract sensory information;assembly task;bidirectional communication;digital twin environments;hazardous environments;reflected sensory feedback;reinforcement learning;remote control;reward function;RL;robot manipulation tasks;robotic system;telemanipulation task;teleoperated manipulators;VF;virtual fixture parameter estimation;virtual worlds","","","","21","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
