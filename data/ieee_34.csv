"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Reinforcement Learning-Based Physical Cross-Layer Security and Privacy in 6G","X. Lu; L. Xiao; P. Li; X. Ji; C. Xu; S. Yu; W. Zhuang","Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Automation, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Peking University, Beijing, China; Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Communications Surveys & Tutorials","23 Feb 2023","2023","25","1","425","466","Sixth-generation (6G) cellular systems will have an inherent vulnerability to physical (PHY)-layer attacks and privacy leakage, due to the large-scale heterogeneous networks with booming time-sensitive applications. Important wireless techniques including non-orthogonal multiple access, mobile edge computing, millimeter-wave, massive multiple-input and multiple-output, visible light communication, terahertz, and intelligent reflecting surface can improve the spectrum efficiency and quality-of-service but will raise challenges for the 6G PHY and cross-layer security and privacy protection. Existing optimization based PHY and cross-layer security and privacy protection schemes such as the convex optimization method have to rely on accurate attack patterns and strategies and thus suffer from performance degradation in 6G systems that have shorter communication latency, more devices and higher spectrum efficiency than 5G. Reinforcement learning (RL) algorithms help wireless devices optimize their security policies to enhance the security performance in dynamic networks against smart attacks without depending on the attack model. Therefore, this article provides a comprehensive survey on the RL based 6G PHY cross-layer security and privacy protection. In this article, we investigate the potential attacks in 6G systems and discuss the PHY cross-layer security solutions. A brief overview of reinforcement learning algorithms is provided. Afterward, we review the RL based PHY-layer security and privacy protection and discuss how to apply RL algorithms in 6G security scenarios, especially focusing on the game with jammers, eavesdroppers, spoofers and inference attackers. The RL based security solutions for unmanned aerial vehicles (UAVs) and cross-layer scenarios are also reviewed. The future research directions are identified and the corresponding RL based potential solutions are discussed for 6G.","1553-877X","","10.1109/COMST.2022.3224279","Natural Science Foundation of China(grant numbers:U21A20444,62202222,61971366,U22B2062); Natural Science Foundation of Jiangsu Province(grant numbers:BK20220880); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9961877","6G;PHY-layer security;privacy;reinforcement learning;secure communications;UAVs;cross-layer security","Security;Jamming;6G mobile communication;Cross layer design;NOMA;Millimeter wave communication;Visible light communication","6G mobile communication;autonomous aerial vehicles;cellular radio;data privacy;jamming;reinforcement learning;telecommunication security","6G cellular systems;higher spectrum efficiency;inference attackers;physical cross-layer security;privacy protection;reinforcement learning algorithms;security performance;security policies;sixth-generation cellular systems;smart attacks;UAV;unmanned aerial vehicles","","8","","220","IEEE","23 Nov 2022","","","IEEE","IEEE Journals"
"Coordinated Complex-Valued Encoding Dragonfly Algorithm and Artificial Emotional Reinforcement Learning for Coordinated Secondary Voltage Control and Automatic Voltage Regulation in Multi-Generator Power Systems","L. Yin; S. Luo; Y. Wang; F. Gao; J. Yu","College of Electrical Engineering, Guangxi University, Nanning, China; College of Electrical Engineering, Guangxi University, Nanning, China; Institute of Intelligent Machines, Chinese Academy of Sciences, Hefei, China; College of Electrical Engineering, Guangxi University, Nanning, China; Department of Automation, University of Science and Technology of China, Hefei, China","IEEE Access","9 Oct 2020","2020","8","","180520","180533","This article proposes a coordinated optimization and control algorithm for coordinated secondary voltage control (CSVC) in multi-generator power systems. Firstly, to obtain a smaller voltage deviation and avoid the curse of dimensionality simultaneously, an artificial emotional reinforcement learning (AERL) is applied to automatic voltage regulation (AVR). Secondly, to obtain a smaller fitness value with lesser random for the decentralized independent variables optimization problem of the CSVC, a complex-valued encoding dragonfly algorithm (CDA) is proposed. Thirdly, the CDA and the AERL are coordinated for the CSVC and the AVR in multi-generator power systems. To verify the control performance of the AERL and the convergence of the proposed CDA, three simulation cases, i.e., IEEE 57-bus, 118-bus and 300-bus systems, are considered. The simulation results show that the CDA-AERL effectively obtains the smallest control objectives and the convergence for the CSVC in multi-generator power systems.","2169-3536","","10.1109/ACCESS.2020.3028064","Guangxi Natural Science Foundation(grant numbers:AD19245001,2020GXNSFBA159025); National Natural Science Foundation of China(grant numbers:61773359,61720106009); National Natural Science Foundation of China(grant numbers:U1736123); University of Science and Technology of China (USTC) Research Funds of the Double First-Class Initiative(grant numbers:YD2350002001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9210555","Coordinated secondary voltage control;artificial emotional reinforcement learning;complex-valued encoding dragonfly algorithm;automatic voltage regulation;multi-generator power systems","Voltage control;Optimization;Power system stability;Learning (artificial intelligence);Static VAr compensators;Heuristic algorithms","control engineering computing;learning (artificial intelligence);optimisation;power engineering computing;power generation control;voltage control;voltage regulators","coordinated complex-valued encoding dragonfly algorithm;artificial emotional reinforcement learning;coordinated secondary voltage control;automatic voltage regulation;multigenerator power systems;CSVC;AERL;decentralized independent variables optimization problem;CDA;AVR;IEEE 57-bus system;IEEE 118-bus system;IEEE 300-bus system","","8","","58","CCBY","1 Oct 2020","","","IEEE","IEEE Journals"
"A Basal Ganglia Network Centric Reinforcement Learning Model and Its Application in Unmanned Aerial Vehicle","Y. Zeng; G. Wang; B. Xu","Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Shanghai, China; Institute of Automation, Chinese Academy of Science, Beijing, China; Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Shanghai, China","IEEE Transactions on Cognitive and Developmental Systems","8 Jun 2018","2018","10","2","290","303","Reinforcement learning brings flexibility and generality for machine learning, while most of them are mathematical optimization driven approaches, and lack of cognitive and neural evidence. In order to provide a more cognitive and neural mechanisms driven foundation and validate its applicability in complex task, we develop a basal ganglia (BG) network centric reinforcement learning model. Compared to existing work on modeling BG, this paper is unique from the following perspectives: 1) the orbitofrontal cortex (OFC) is taken into consideration. OFC is critical in decision making because of its responsibility for reward representation and is critical in controlling the learning process, while most of the BG centric models do not include OFC; 2) to compensate the inaccurate memory of numeric values, precise encoding is proposed to enable working memory system remember important values during the learning process. The method combines vector convolution and the idea of storage by digit bit and is efficient for accurate value storage; and 3) for information coding, the Hodgkin-Huxley model is used to obtain a more biological plausible description of action potential with plenty of ionic activities. To validate the effectiveness of the proposed model, we apply the model to the unmanned aerial vehicle (UAV) autonomous learning process in a 3-D environment. Experimental results show that our model is able to give the UAV the ability of free exploration in the environment and has comparable learning speed as the Q learning algorithm, while the major advances for our model is that it is with solid cognitive and neural basis.","2379-8939","","10.1109/TCDS.2017.2649564","Strategic Priority Research Program of the Chinese Academy of Sciences(grant numbers:XDB02060007); Beijing Municipal Commission of Science and Technology(grant numbers:Z151100000915070,Z161100000216124); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7809052","Basal ganglia (BG) network;brain-inspired intelligence;precise encoding;reinforcement learning model;unmanned aerial vehicle (UAV) autonomous learning","Brain modeling;Biological system modeling;Computational modeling;Reinforcement learning;Task analysis;Unmanned aerial vehicles;Solid modeling","autonomous aerial vehicles;cognition;decision making;learning (artificial intelligence);neural nets","neural mechanisms;basal ganglia network centric reinforcement learning model;OFC;BG centric models;working memory system;accurate value storage;Hodgkin-Huxley model;unmanned aerial vehicle autonomous learning process;Q learning algorithm;machine learning;mathematical optimization driven approaches;neural evidence;cognitive mechanisms;learning speed;vector convolution;information coding;3-D environment;neural basis;solid cognitive basis","","8","","45","IEEE","6 Jan 2017","","","IEEE","IEEE Journals"
"Reinforcement Learning Based Underwater Wireless Optical Communication Alignment for Autonomous Underwater Vehicles","Y. Weng; J. Pajarinen; R. Akrour; T. Matsuda; J. Peters; T. Maki","Institute of Industrial Science, The University of Tokyo, Tokyo, Japan; Intelligent Autonomous Systems Laboratory, Technische Universität Darmstadt, Darmstadt, Germany; Intelligent Autonomous Systems Laboratory, Technische Universität Darmstadt, Darmstadt, Germany; Institute of Industrial Science, The University of Tokyo, Tokyo, Japan; Intelligent Autonomous Systems Laboratory, Technische Universität Darmstadt, Darmstadt, Germany; Institute of Industrial Science, The University of Tokyo, Tokyo, Japan","IEEE Journal of Oceanic Engineering","12 Oct 2022","2022","47","4","1231","1245","With the developments in underwater wireless optical communication (UWOC) technology, UWOC can be used in conjunction with autonomous underwater vehicles (AUVs) for high-speed data sharing among the vehicle formation during underwater exploration. A beam alignment problem arises during communication due to the transmission range, external disturbances and noise, and uncertainties in the AUV dynamic model. In this article, we propose an acoustic navigation method to guide the alignment process without requiring beam directors, light intensity sensors, and/or scanning algorithms as used in previous research. The AUVs need stably maintain a specific relative position and orientation for establishing an optical link. We model the alignment problem as a partially observable Markov decision process (POMDP) that takes manipulation, navigation, and energy consumption of underwater vehicles into account. However, finding an efficient policy for the POMDP under high partial observability and environmental variability is challenging. Therefore, for successful policy optimization, we utilize the soft actor–critic reinforcement learning algorithm together with AUV-specific belief updates and reward shaping based curriculum learning. Our approach outperformed baseline approaches in a simulation environment and successfully performed the beam alignment process from one AUV to another on the real AUV Tri-TON 2.","1558-1691","","10.1109/JOE.2022.3165805","Institute of Industrial Science; University of Tokyo; Technische Universität Darmstadt; Continental Automotive Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9832941","Autonomous underwater vehicle (AUV);partially observable Markov decision process (POMDP);reinforcement learning (RL);soft actor–critic (SAC);underwater wireless optical communication (UWOC)","Optical beams;Reinforcement learning;Underwater communication;Autonomous underwater vehicles;Markov processes;Wireless communication","autonomous underwater vehicles;control engineering computing;decision theory;energy consumption;Markov processes;mobile robots;navigation;optical links;reinforcement learning;robot dynamics;telecommunication computing;underwater acoustic communication;underwater optical wireless communication","acoustic navigation method;autonomous underwater vehicles;AUV dynamic model;AUV Tri-TON;AUV-specific belief updates;beam alignment problem;beam directors;energy consumption;external disturbances;high-speed data sharing;optical link;partial observability;partially observable Markov decision process;soft actor-critic reinforcement learning;specific relative position;underwater exploration;underwater wireless optical communication alignment;underwater wireless optical communication technology;UWOC;vehicle formation","","7","","53","IEEE","19 Jul 2022","","","IEEE","IEEE Journals"
"Network Selection Based on Evolutionary Game and Deep Reinforcement Learning in Space-Air-Ground Integrated Network","K. Fan; B. Feng; X. Zhang; Q. Zhang","Communication Engineering Research Centre, Harbin Institute of Technology (Shenzhen), Shenzhen, Guangdong, China; Communication Engineering Research Centre, Harbin Institute of Technology (Shenzhen), Shenzhen, Guangdong, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; Communication Engineering Research Centre, Harbin Institute of Technology (Shenzhen), Shenzhen, Guangdong, China","IEEE Transactions on Network Science and Engineering","24 May 2022","2022","9","3","1802","1812","In next generation communication system, space-air-ground integrated network (SAGIN) would be utilized to provide ubiquitous and unlimited wireless connectivity with large coverage, high throughput, and strong resilience. In this integrated network, there are multiple heterogeneous network options to satisfy service requirements, where an efficient network selection strategy is required to improve resource utilization and achieve load balance. In this paper, we propose a system model of network selection in SAGIN and formulate a corresponding evolutionary game. A network selection algorithm based on evolutionary game is proposed to study the autonomous decision-making process of network selection as a supplement. We also propose a deep deterministic policy gradient (DDPG)-based network selection algorithm to handle continuous and high-dimensional action spaces. A particular case is studied for further simulation and analysis. The evolutionary game obtains the selection strategy with the highest payoff at the evolutionary equilibrium point, and the stability of evolutionary equilibrium is proved by varying relative factors. The DDPG-based network selection algorithm obtains the same strategy with the highest reward at convergence at a slower speed. In comprehensive comparison, our proposed algorithms perform better than the repeated stochastic game approach and proximal policy optimization (PPO) algorithm.","2327-4697","","10.1109/TNSE.2022.3153480","National Natural Science Foundation of China(grant numbers:61831008,62027802); Guangdong Science and Technology Planning(grant numbers:2018B030322004,2021A1515110071); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9720158","Deep reinforcement learning;evolutionary game;network selection;space-air-ground integrated network","Games;Satellites;Heuristic algorithms;Throughput;Game theory;Statistics;Sociology","decision making;deep learning (artificial intelligence);evolutionary computation;gradient methods;next generation networks;reinforcement learning;resource allocation;space communication links;stochastic games","space-air-ground integrated network;multiple heterogeneous network options;efficient network selection strategy;corresponding evolutionary game;deep deterministic policy gradient-based network selection algorithm;evolutionary equilibrium point;DDPG-based network selection algorithm;next generation communication system;stochastic game approach;proximal policy optimization algorithm","","7","","28","IEEE","23 Feb 2022","","","IEEE","IEEE Journals"
"Timesharing-tracking framework for decentralized reinforcement learning in fully cooperative multi-agent system","X. Chen; B. Fu; Y. He; M. Wu","School of Automation, China University of Geo-sciences, Wuhan 430074, China; School of Information Science and Engineering, Central South University, Changsha 410083, China; Central South University, Changsha, Hunan, CN; China University of Geosciences, Wuhan, Hubei, CN; China University of Geosciences, Wuhan, Hubei, CN","IEEE/CAA Journal of Automatica Sinica","15 Jan 2015","2014","1","2","127","133","Dimension-reduced and decentralized learning is always viewed as an efficient way to solve multi-agent cooperative learning in high dimension. However, the dynamic environment brought by the concurrent learning makes the decentralized learning hard to converge and bad in performance. To tackle this problem, a timesharing-tracking framework (TTF), stemming from the idea that alternative learning in microscopic view results in concurrent learning in macroscopic view, is proposed in this paper, in which the joint-state best-response Q-learning (BRQ-learning) serves as the primary algorithm to adapt to the companions' policies. With the properly defined switching principle, TTF makes all agents learn the best responses to others at different joint states. Thus from the view of the whole joint-state space, agents learn the optimal cooperative policy simultaneously. The simulation results illustrate that the proposed algorithm can learn the optimal joint behavior with less computation and faster speed compared with other two classical learning algorithms.","2329-9274","","10.1109/JAS.2014.7004541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004541","Cooperative multi-agent system;reinforcement learning;immediate individual reward;timesharing tracking","Multi-agent systems;Learning (artificial intelligence);Switches;Reinforcement learning;Optimization;Robots;Games","learning (artificial intelligence);multi-agent systems;multi-robot systems","timesharing-tracking framework;decentralized reinforcement learning;fully cooperative multiagent system;TTF;alternative learning;concurrent learning;joint-state best-response Q-learning;BRQ-learning;switching principle;optimal cooperative policy learning","","7","","","","15 Jan 2015","","","IEEE","IEEE Journals"
"Data-Efficient Reinforcement Learning for Energy Optimization of Power-Assisted Wheelchairs","G. Feng; L. Buşoniu; T. -M. Guerra; S. Mohammad","LAMIH UMR CNRS 8201, Université Polytechnique Hauts-de-France, Valenciennes, France; Department of Automation, Technical University of Cluj-Napoca, Cluj-Napoca, Romania; LAMIH UMR CNRS 8201, Université Polytechnique Hauts-de-France, Valenciennes, France; Autonomad Mobility, Valenciennes, France","IEEE Transactions on Industrial Electronics","1 Aug 2019","2019","66","12","9734","9744","The objective of this paper is to develop a method for assisting users to push power-assisted wheelchairs (PAWs) in such a way that the electrical energy consumption over a predefined distance-to-go is optimal, while at the same time bringing users to a desired fatigue level. This assistive task is formulated as an optimal control problem and solved by Feng et al. using the model-free approach gradient of partially observable Markov decision processes. To increase the data efficiency of the model-free framework, we here propose to use policy learning by weighting exploration with the returns (PoWER) with 25 control parameters. Moreover, we provide a new near-optimality analysis of the finite-horizon fuzzy Q-iteration, which derives a model-based baseline solution to verify numerically the near-optimality of the presented model-free approaches. Simulation results show that the PoWER algorithm with the new parameterization converges to a near-optimal solution within 200 trials and possesses the adaptability to cope with changes of the human fatigue dynamics. Finally, 24 experimental trials are carried out on the PAW system, with fatigue feedback provided by the user via a joystick. The performance tends to increase gradually after learning. The results obtained demonstrate the effectiveness and the feasibility of PoWER in our application.","1557-9948","","10.1109/TIE.2019.2903751","French Ministry of Research; Romanian Ministry of Research and Innovation; CNCS-UEFISCDI(grant numbers:PN-III-P1-1.1-TE-2016-0670); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667013","Assistive control;disabled persons;power-assisted wheelchairs;reinforcement learning","Fatigue;Wheelchairs;Numerical models;Adaptation models;Heuristic algorithms;Force;Optimal control","decision theory;handicapped aids;learning (artificial intelligence);learning systems;Markov processes;optimal control;optimisation;wheelchairs","data-efficient reinforcement learning;energy optimization;power-assisted wheelchairs;electrical energy consumption;predefined distance-to-;desired fatigue level;assistive task;optimal control problem;partially observable Markov decision processes;data efficiency;model-free framework;policy learning;near-optimality analysis;finite-horizon fuzzy Q-iteration;model-based baseline solution;PoWER algorithm;near-optimal solution;control parameters","","7","","25","IEEE","13 Mar 2019","","","IEEE","IEEE Journals"
"WD3: Taming the Estimation Bias in Deep Reinforcement Learning","Q. He; X. Hou","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China","2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)","24 Dec 2020","2020","","","391","398","The overestimation phenomenon caused by function approximation is a well-known issue in value-based reinforcement learning algorithms such as deep Q-networks and DDPG, which could lead to suboptimal policies. To address this issue, TD3 takes the minimum value between a pair of critics, which introduces underestimation bias. By unifying these two opposites, we propose a novel Weighted Delayed Deep Deterministic Policy Gradient algorithm, which can reduce the estimation error and further improve the performance by weighting a pair of critics. We compare the learning process of value function between DDPG, TD3, and our proposed algorithm, which verifies that our algorithm could indeed eliminate the estimation error of value function. We evaluate our algorithm in the OpenAI Gym continuous control tasks, outperforming the state-of-the-art algorithms on every environment tested.","2375-0197","978-1-7281-9228-4","10.1109/ICTAI50040.2020.00068","National Key R&D Program of China(grant numbers:2017YFC1200601); National Natural Science Foundation of China(grant numbers:31672325); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288293","Deep reinforcement learning;estimation bias;neural networks","Estimation error;Conferences;Neural networks;Reinforcement learning;Tools;Function approximation;Task analysis","function approximation;gradient methods;learning (artificial intelligence)","DDPG;suboptimal policies;TD3;novel Weighted Delayed Deep Deterministic Policy Gradient algorithm;estimation error;learning process;value function;WD3;estimation bias;Deep reinforcement learning;overestimation phenomenon;function approximation;value-based reinforcement learning algorithms;deep Q-networks","","7","","35","IEEE","24 Dec 2020","","","IEEE","IEEE Conferences"
"Dynamic Event-Sampled Control of Interconnected Nonlinear Systems Using Reinforcement Learning","X. Yang; M. Xu; Q. Wei","School of Electrical and Information Engineering, Tianjin University, Tianjin 300072, China.; School of Electrical and Information Engineering, Tianjin University, Tianjin 300072, China.; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China, and also with the School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China.","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","15","We develop a decentralized dynamic event-based control strategy for nonlinear systems subject to matched interconnections. To begin with, we introduce a dynamic event-based sampling mechanism, which relies on the system's states and the variables generated by time-based differential equations. Then, we prove that the decentralized event-based controller for the whole system is composed of all the optimal event-based control policies of nominal subsystems. To derive these optimal event-based control policies, we design a critic-only architecture to solve the related event-based Hamilton-Jacobi-Bellman equations in the reinforcement learning framework. The implementation of such an architecture uses only critic neural networks (NNs) with their weight vectors being updated through the gradient descent method together with concurrent learning. After that, we demonstrate that the asymptotic stability of closed-loop nominal subsystems and the uniformly ultimate boundedness stability of critic NNs' weight estimation errors are guaranteed by using Lyapunov's approach. Finally, we provide simulations of a matched nonlinear-interconnected plant to validate the present theoretical claims.","2162-2388","","10.1109/TNNLS.2022.3178017","National Natural Science Foundation of China(grant numbers:61973228); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9789118","Adaptive dynamic programming (ADP);decentralized control;event-based control;interconnected system;reinforcement learning (RL).","Asymptotic stability;Interconnected systems;Decentralized control;Closed loop systems;Artificial neural networks;Optimal control;Nonlinear dynamical systems","","","","6","","","IEEE","6 Jun 2022","","","IEEE","IEEE Early Access Articles"
"Combining Reinforcement Learning and Rule-based Method to Manipulate Objects in Clutter","Y. Chen; Z. Ju; C. Yang","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Computing, University of Portsmouth, Portsmouth, UK; Bristol Robotics Laboratory, University of the West of England, Bristol, UK","2020 International Joint Conference on Neural Networks (IJCNN)","28 Sep 2020","2020","","","1","6","Picking up the clustered objects is always a challenging task in robot research field. And reinforcement learning enables robot to adapt to different tasks through plenty of attempts. To reduce the complexity of strategy learning, we propose a framework for robots to pick up the objects in clutter on table based on deep reinforcement learning and rule-based method. To manipulate the objects on table, we mainly divide the robot actions into two categories: one is pushing that uses the reinforcement learning method, while the other one is grasping that is inferred by image morphological processing. The pushing action can separate the stacking objects, create a robust grasp point for the following grasp. The grasp detect algorithm determines if there is a suitable grasp point. Judging on the result of pushing, the grasp detect algorithm will return a reward for pushing learning. Taking images as input, our framework can keep a high grasp rate with low computational complexity, which makes it achieve clutter clearing quickly.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9207153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207153","reinforcement learning;grasp detection;robot;clutter clearing","Reinforcement learning;Grasping;Robot kinematics;Task analysis;Clutter;Grippers","computational complexity;intelligent robots;learning (artificial intelligence);manipulators;neurocontrollers;robot vision","rule-based method;clustered objects;strategy learning;deep reinforcement learning;robot actions;pushing action;object stacking;robust grasp point;pushing learning;clutter clearing;grasp rate;object manipulation;grasping;image morphological processing;grasp detect algorithm;computational complexity","","6","","24","IEEE","28 Sep 2020","","","IEEE","IEEE Conferences"
"Power System Load Frequency Active Disturbance Rejection Control via Reinforcement Learning-Based Memetic Particle Swarm Optimization","Y. Zheng; Z. Huang; J. Tao; H. Sun; Q. Sun; M. Dehmer; M. Sun; Z. Chen","College of Artificial Intelligence, Nankai University, Tianjin, China; College of Artificial Intelligence, Nankai University, Tianjin, China; College of Artificial Intelligence, Nankai University, Tianjin, China; College of Artificial Intelligence, Nankai University, Tianjin, China; College of Artificial Intelligence, Nankai University, Tianjin, China; Department of Computer Science, Swiss Distance University of Applied Sciences, Brig, Switzerland; College of Artificial Intelligence, Nankai University, Tianjin, China; College of Artificial Intelligence, Nankai University, Tianjin, China","IEEE Access","26 Aug 2021","2021","9","","116194","116206","Load frequency control (LFC) is necessary to guarantee the safe operation of power systems. Aiming at the frequency and power stability problems caused by load disturbances in interconnected power systems, active disturbance rejection control (ADRC) was designed. There are eight parameters that need to be adjusted for an ADRC, which are challenging to adjust manually, thus limiting the development of this approach in industrial applications. Regardless of the theory or application, there is still no unified and efficient parameter optimization method. The traditional particle swarm optimization (PSO) algorithm suffers from premature convergence and a high computational cost. Therefore, in this paper, we utilize an improved PSO algorithm, a reinforcement-learning-based memetic particle swarm optimization (RLMPSO), for the parameter tuning of ADRC to obtain better control performance for the controlled system. Finally, to highlight the advantages of the proposed RLMPSO-ADRC method and to prove its superiority, the results were compared with other control algorithms in both a traditional non-reheat two-area thermal power system and a non-linear power system with a governor dead band (GDB) and a generation rate constraint (GRC). Moreover, the robustness of the proposed method was tested by simulations with parameter perturbations and different working conditions. The simulation results showed that the proposed method can meet the demand for the frequency deviation to stabilize to 0 in LFC with higher performance, and it is worthy of popularization and application.","2169-3536","","10.1109/ACCESS.2021.3099904","National Natural Science Foundation of China(grant numbers:61973172,61973175,62003175,62003177); National Key Research and Development Project(grant numbers:2019YFC1510900); Key Technologies Research and Development Program(grant numbers:19JCZDJC32800); China Postdoctoral Science Foundation(grant numbers:2020M670633); Academy of Finland(grant numbers:315660); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9495808","Interconnected power system;load frequency control;active disturbance rejection control;parameter optimization;reinforcement-learning-based memetic particle swarm optimization","Power system stability;Optimization;Particle swarm optimization;Turbines;Robust control;Memetics;Prediction algorithms","","","","5","","39","CCBY","26 Jul 2021","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Economic Energy Scheduling in Data Center Microgrids","X. Yang; Y. Wang; H. He; C. Sun; Y. Zhang","Department of Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, RI, USA; Department of Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, RI, USA; Department of Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, RI, USA; School of Automation, Southeast University, Nanjing, China; College of Information Engineering, Zhejiang University of Technology, Hangzhou, China","2019 IEEE Power & Energy Society General Meeting (PESGM)","30 Jan 2020","2019","","","1","5","This paper investigates the economic energy scheduling problem for data center microgrids with renewables integration via deep reinforcement learning. A multi-factor economic energy scheduling problem is first formulated to activate renewables & real-time prices-aware consumption activities. Then, we transform the formulated problem as a Markov decision process. On this basis, an online optimization is further performed to cope with system uncertainties through the integration of reinforcement learning and neural networks. Unlike traditional methods relying on a priori knowledge or requiring accurate modeling of system dynamics, our employed deep policy gradient-based learning algorithm is capable to directly learn the optimal actions from available database. Simulations with realistic traces are performed to verify the effectiveness of the proposed algorithm.","1944-9933","978-1-7281-1981-6","10.1109/PESGM40551.2019.8974083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8974083","","","distributed power generation;gradient methods;learning (artificial intelligence);Markov processes;neural nets;power engineering computing;power generation scheduling","Markov decision process;deep reinforcement learning;data center microgrids;renewables integration;multifactor economic energy scheduling problem;deep policy gradient-based learning","","5","","20","IEEE","30 Jan 2020","","","IEEE","IEEE Conferences"
"Multiagent Path Finding Using Deep Reinforcement Learning Coupled With Hot Supervision Contrastive Loss","L. Chen; Y. Wang; Y. Mo; Z. Miao; H. Wang; M. Feng; S. Wang","School of Electrical and Information Engineering, Hunan University, Changsha, China; School of Electrical and Information Engineering, Hunan University, Changsha, China; School of Electrical and Information Engineering, Hunan University, Changsha, China; School of Electrical and Information Engineering, Hunan University, Changsha, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; School of Computer Science and Technology, Xidian University, Xian, China; School of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Transactions on Industrial Electronics","17 Feb 2023","2023","70","7","7032","7040","Multiagent path finding (MAPF) is employed to find collision-free paths to guide agents traveling from an initial to a target position. The advanced decentralized approach utilizes communication between agents to improve their performance in environments with high-density obstacles. However, it dramatically reduces the robustness of multiagent systems. To overcome this difficulty, we propose a novel method for solving MAPF problems. In this method, expert data are transformed into supervised signals by proposing a hot supervised contrastive loss, which is combined with reinforcement learning to teach fully-decentralized policies. Agents reactively plan paths online in a partially observable world while exhibiting implicit coordination without communication with others. We introduce the self-attention mechanism in the policy network, which improves the ability of the policy network to extract collaborative information between agents from the observation data. By designing simulation experiments, we demonstrate that the learned policy achieved good performance without communication between agents. Furthermore, real-world application experiments demonstrate the effectiveness of our method in practical applications.","1557-9948","","10.1109/TIE.2022.3206745","National Key Research and Development Program of China(grant numbers:2021YFB1714700); National Natural Science Foundation of China(grant numbers:62133005,62103141); Natural Science Foundation of Hunan Province(grant numbers:2022JJ40095); China Postdoctoral Science Foundation(grant numbers:2021M690963); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9896761","Imitation learning (IL);multiagent path finding (MAPF);reinforcement learning;supervision contrastive learning","Reinforcement learning;Training;Feature extraction;Data mining;Task analysis;Multi-agent systems;Convolution","collision avoidance;deep learning (artificial intelligence);learning (artificial intelligence);multi-agent systems;reinforcement learning","advanced decentralized approach;collision-free paths;deep reinforcement;fully-decentralized policies;high-density obstacles;hot supervised contrastive loss;hot supervision contrastive loss;learned policy;MAPF problems;multiagent path finding;multiagent systems;policy network;reinforcement learning;supervised signals;target position","","5","","33","IEEE","20 Sep 2022","","","IEEE","IEEE Journals"
"Parallel Deep Reinforcement Learning Method for Gait Control of Biped Robot","C. Tao; J. Xue; Z. Zhang; Z. Gao","School of Electronic and Information Engineering, Suzhou University of Science and Technology, Suzhou, China; School of Electronic and Information Engineering, Suzhou University of Science and Technology, Suzhou, China; Department of Automation, Tsinghua University, Beijing, China; Faculty of Engineering, McMaster University, Hamilton, Canada","IEEE Transactions on Circuits and Systems II: Express Briefs","1 Jun 2022","2022","69","6","2802","2806","In this brief, a parallel Deep Deterministic Policy Gradient (DDPG) algorithm is presented for biped robot gait control. Biped robot gait control is a high-dimensional continuous problem. It is challenging to obtain a fast and stable gait. Traditional methods cannot fully utilize autonomous exploration capability of a biped robot. A multiple Actor-Critic (AC) network is established to expand the scope of exploration and improve training efficiency. For optimizing experience replay mechanism, an experience filtering unit is introduced, and a cosine similarity method is used to classify experience. Then, a Markov Decision Process (MDP) model based on knowledge and experience is designed to solve the problem of sparse rewards. Finally, experimental results show that the parallel DDPG algorithm can make the biped robot walk more quickly and stably, and the speed reaches 0.62 m/s.","1558-3791","","10.1109/TCSII.2022.3145373","National Natural Science Foundation of China(grant numbers:61801323,61972454); China Postdoctoral Science Foundation(grant numbers:2021M691848); Science and Technology Projects Fund of Suzhou(grant numbers:SS2019029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9690599","Parallel deep deterministic policy gradient;biped robot;gait control;experience replay mechanism;knowledge and experience","Legged locomotion;Robots;Training;Aerospace electronics;Robot kinematics;Heuristic algorithms;Optimization","control engineering computing;deep learning (artificial intelligence);gait analysis;legged locomotion;Markov processes;reinforcement learning","parallel deep deterministic policy gradient algorithm;gait control;high-dimensional continuous problem;stable gait;cosine similarity method;parallel DDPG algorithm;biped robot;parallel deep reinforcement learning method;Markov decision process","","5","","22","IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"Vehicle Control in Highway Traffic by Using Reinforcement Learning and Microscopic Traffic Simulation","L. Szoke; S. Aradi; T. Becsi; P. Gaspar","Department of Control for Transportation and Vehicle Systems, Budapest University of Technology and Economics, Megyetem rkp. 3, Budapest, Hungary; Department of Control for Transportation and Vehicle Systems, Budapest University of Technology and Economics, Megyetem rkp. 3, Budapest, Hungary; Department of Control for Transportation and Vehicle Systems, Budapest University of Technology and Economics, Megyetem rkp. 3, Budapest, Hungary; Systems and Control Laboratory, Computer and Automation Research Institute, Hungarian Academy of Sciences, Kende u. 13-17, Budapest, Hungary","2020 IEEE 18th International Symposium on Intelligent Systems and Informatics (SISY)","8 Oct 2020","2020","","","21","26","The paper presents a simple yet powerful and intelligent driver agent, designed to operate in a preset highway situation using Policy Gradient Reinforcement Learning (RL) agent. The goal is to navigate safely in dense highway traffic and proceed through the defined length with the shortest time possible. The algorithm uses a dense neural network as a function approximator for the agent with discrete action space on the control level, e.g., acceleration and steering. The developed simulation environment uses the open-source traffic simulator called Simulation of Urban MObility (SUMO), integrated with an interface, to interact with the agent in real-time. With this tool, numerous driving and highway situations can be created and fed to the agent from which it can learn. The environment opens the opportunity to randomize and customize the other road users' behavior. Thus the experience can be more diverse, and thus the representation becomes more general. The article describes the modeling environment, the details on the learning agent, and the rewarding scheme. After evaluating the experiences gained from the training, some ideas for optimization and further development goals are also proposed.","1949-0488","978-1-7281-7352-8","10.1109/SISY50555.2020.9217076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9217076","","Learning (artificial intelligence);Task analysis;Vehicles;Roads;Training;Microscopy","intelligent transportation systems;interactive systems;learning (artificial intelligence);neural nets;real-time systems;road safety;road traffic control;software agents;traffic engineering computing","vehicle control;microscopic traffic simulation;intelligent driver agent;policy gradient reinforcement learning agent;highway traffic;dense neural network;open source traffic simulator;simulation of urban mobility;SUMO;real time agent interaction;road user behavior","","5","","23","IEEE","8 Oct 2020","","","IEEE","IEEE Conferences"
"Driving on Highway by Using Reinforcement Learning with CNN and LSTM Networks","L. Szőke; S. Aradi; T. Bécsi; P. Gáspár","Department of Control for Transportation and Vehicle Systems, Budapest University of Technology and Economics, H-1111 Budapest, Hungary; Department of Control for Transportation and Vehicle Systems, Budapest University of Technology and Economics, H-1111 Budapest, Hungary; Department of Control for Transportation and Vehicle Systems, Budapest University of Technology and Economics, H-1111 Budapest, Hungary; Systems and Control Laboratory, Computer and Automation Research Institute, Hungarian Academy of Sciences, H-1111 Budapest, Hungary","2020 IEEE 24th International Conference on Intelligent Engineering Systems (INES)","27 Jul 2020","2020","","","121","126","This work presents a powerful and intelligent driver agent, designed to operate in a preset highway situation using Policy Gradient Reinforcement Learning (RL) agent. Our goal is to create an agent that is capable of navigating safely in changing highway traffic and successfully accomplish to get through the defined section keeping the reference speed. Meanwhile, creating a state representation that is capable of extracting information from images based on the actual highway situation. The algorithm uses Convolutional Neural Network (CNN) with Long-Short Term Memory (LSTM) layers as a function approximator for the agent with discrete action space on the control level, e.g., acceleration and lane change. Simulation of Urban MObility (SUMO), an open-source microscopic traffic simulator is chosen as our simulation environment. It is integrated with an open interface to interact with the agent in real-time. The agent can learn from numerous driving and highway situations that are created and fed to it. The representation becomes more general by randomizing and customizing the behavior of the other road users in the simulation, thus the experience of the agent can be much more diverse. The article briefly describes the modeling environment, the details on the learning agent, and the rewarding scheme. After evaluating the experiences gained from the training, some further plans and optimization ideas are briefed.","1543-9259","978-1-7281-1059-2","10.1109/INES49302.2020.9147185","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9147185","","Road transportation;Learning (artificial intelligence);Vehicles;Training;Neural networks;Aerospace electronics;Acceleration","convolutional neural nets;function approximation;learning (artificial intelligence);recurrent neural nets;road traffic;traffic engineering computing","highway traffic;reference speed;state representation;actual highway situation;convolutional neural network;CNN;long-short term memory layers;function approximator;discrete action space;acceleration;lane change;open-source microscopic traffic simulator;simulation environment;open interface;numerous driving;LSTM networks;powerful driver agent;intelligent driver agent;preset highway situation;policy gradient reinforcement learning agent","","5","","21","IEEE","27 Jul 2020","","","IEEE","IEEE Conferences"
"Resilient Optimal Defensive Strategy of TSK Fuzzy-Model-Based Microgrids’ System via a Novel Reinforcement Learning Approach","H. Zhang; D. Yue; C. Dou; X. Xie; K. Li; G. P. Hancke","Institute of Advanced Technology, Nanjing University of Posts and Telecommunications, Nanjing, China; Institute of Advanced Technology, Nanjing University of Posts and Telecommunications, Nanjing, China; Institute of Advanced Technology, Nanjing University of Posts and Telecommunications, Nanjing, China; Institute of Advanced Technology, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Electronic and Electrical Engineering, University of Leeds, Leeds, U.K.; College of Automation, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Transactions on Neural Networks and Learning Systems","4 Apr 2023","2023","34","4","1921","1931","With consideration of false data injection (FDI) on the demand side, it brings a great challenge for the optimal defensive strategy with the security issue, voltage stability, power flow, and economic cost indexes. This article proposes a Takagi–Sugeuo–Kang (TSK) fuzzy system-based reinforcement learning approach for the resilient optimal defensive strategy of interconnected microgrids. Due to FDI uncertainty of the system load, TSK-based deep deterministic policy gradient (DDPG) is proposed to learn the actor network and the critic network, where multiple indexes’ assessment occurs in the critic network, and the security switching control strategy is made in the actor network. Alternating direction method of multipliers (ADMM) method is improved for policy gradient with online coordination between the actor network and the critic network learning, and its convergence and optimality are proved properly. On the basis of security switching control strategy, the penalty-based boundary intersection (PBI)-based multiobjective optimization method is utilized to solve economic cost and emission issues simultaneously with considering voltage stability and rate-of-change of frequency (RoCoF) limits. According to simulation results, it reveals that the proposed resilient optimal defensive strategy can be a viable and promising alternative for tackling uncertain attack problems on interconnected microgrids.","2162-2388","","10.1109/TNNLS.2021.3105668","National Natural Science Fund(grant numbers:61973171); Basic Research Project of Leading Technology of Jiangsu Province(grant numbers:BK20202011); National Key Research and Development Program of China(grant numbers:2018YFA0702200); National Natural Science Key Fund(grant numbers:61833008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9525816","Microgrids;reinforcement learning (RL);resilient optimal defensive;Takagi–Sugeuo–Kang (TSK) fuzzy system","Microgrids;Security;Indexes;Economics;Power system stability;Propagation losses;Uncertainty","convex programming;distributed power generation;fuzzy systems;gradient methods;power engineering computing;power generation economics;reinforcement learning","actor network;critic network learning;economic cost indexes;interconnected microgrids;penalty-based boundary intersection-based multiobjective optimization method;resilient optimal defensive strategy;security switching control strategy;Takagi-Sugeuo-Kang fuzzy system-based reinforcement learning approach;TSK fuzzy-model-based microgrids;TSK-based deep deterministic policy gradient","","5","","34","IEEE","31 Aug 2021","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Mobile Robot Navigation","M. Gromniak; J. Stenzel","Institute of Medical Technology, Technical University of Hamburg, Hamburg, Germany; Department of Automation and Embedded Systems, Fraunhofer Institute for Material Flow and Logistics, Dortmund, Germany","2019 4th Asia-Pacific Conference on Intelligent Robot Systems (ACIRS)","19 Dec 2019","2019","","","68","73","While navigation is arguable the most important aspect of mobile robotics, complex scenarios with dynamic environments or with teams of cooperative robots are still not satisfactory solved yet. Motivated by the recent successes in the reinforcement learning domain, the application of deep reinforcement learning to robot navigation was examined in this paper. In particular this required the development of a training procedure, a set of actions available to the robot, a suitable state representation and a reward function. The setup was evaluated using a simulated real-time environment. A reference setup, different goal-oriented exploration strategies and two different robot kinematics (holonomic, differential) were compared in the evaluation. In a challenging scenario with obstacles at changing locations in the environment the robot was able to reach the desired goal in 93% of the episodes.","","978-1-7281-2229-8","10.1109/ACIRS.2019.8935944","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8935944","reinforcement learning;end-to-end-learning;mobile robot navigation;robot learning","Navigation;Reinforcement learning;Mobile robots;Robot sensing systems;Neural networks;Task analysis","control engineering computing;learning (artificial intelligence);mobile robots;navigation;neurocontrollers;path planning;robot kinematics;robot programming","deep reinforcement learning;mobile robot navigation;mobile robotics;dynamic environments;reinforcement learning domain;state representation;real-time environment;robot kinematics;goal-oriented exploration strategies;reward function;holonomic robot kinematic;differential robot kinematic","","4","","12","IEEE","19 Dec 2019","","","IEEE","IEEE Conferences"
"Resilient Optimal Defensive Strategy of Micro-Grids System via Distributed Deep Reinforcement Learning Approach Against FDI Attack","H. Zhang; D. Yue; C. Dou; G. P. Hancke","Institute of Advanced Technology, Nanjing University of Posts and Telecommunications, Nanjing 210023, China.; Institute of Advanced Technology, Nanjing University of Posts and Telecommunications, Nanjing 210023, China.; Institute of Advanced Technology, Nanjing University of Posts and Telecommunications, Nanjing 210023, China.; College of Automation, Nanjing University of Posts and Telecommunications, Nanjing 210023, China, and also with the School of Engineering, University of Pretoria, Pretoria 0002, South Africa.","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","11","The ever-increasing false data injection (FDI) attack on the demand side brings great challenges to the energy management of interconnected microgrids. To address those aspects, this article proposes a resilient optimal defensive strategy with the distributed deep reinforcement learning (DRL) approach. To evaluate the FDI attack on demand response (DR), an online evaluation approach with the recursive least-square (RLS) method is proposed to evaluate the extent of supply security or voltage stability of the microgrids system is affected by the FDI attack. On the basis of evaluated security confidence, a distributed actor network learning approach is proposed to deduce optimal network weight, which can generate an optimal defensive scheme to ensure the economic and security issue of the microgrids system. From the methodology's view, it can also enhance the autonomy of each microgrid as well as accelerate DRL efficiency. According to those simulation results, it can reveal that the proposed method can evaluate FDI attack impact well and an improved distributed DRL approach can be a viable and promising way for the optimal defense of microgrids against the FDI attack on the demand side.","2162-2388","","10.1109/TNNLS.2022.3175917","National Key Research and Development Program of China(grant numbers:2018YFA0702200); National Natural Science Fund(grant numbers:61973171); Basic Research Project of Leading Technology of Jiangsu Province(grant numbers:BK20202011); National Natural Science Fund of Jiangsu Province(grant numbers:BK20211276); National Natural Science Key Fund(grant numbers:61833008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9783467","Energy management;false data injection (FDI);interconnected microgrids;reinforcement learning;resilient optimal defensive.","Microgrids;Security;Generators;Reinforcement learning;Costs;Power system stability;Uncertainty","","","","4","","","IEEE","27 May 2022","","","IEEE","IEEE Early Access Articles"
"Reinforcement Learning for Mixing Loop Control with Flow Variable Eligibility Trace","A. Overgaard; B. K. Nielsen; C. S. Kallesøe; J. D. Bendtsen","Core Technology -Department of Control Technology, Grundfos Holding A/S, Bjerringbro, Denmark; Core Technology -Department of Control Technology, Grundfos Holding A/S, Bjerringbro, Denmark; Core Technology -Department of Control Technology, Grundfos Holding A/S, Bjerringbro, Denmark; Department of Electronic Systems, section of Automation and Control, alborg University, Aalborg, Denmark","2019 IEEE Conference on Control Technology and Applications (CCTA)","5 Dec 2019","2019","","","1043","1048","Mixing Loops are often used for proper pressurization and temperature control in building thermal systems. Optimal control of the mixing loop maximizes comfort while minimizing cost. To ensure optimal control for mixing loops in a wide range of different buildings with different load conditions, a self learning controller is here proposed. The controller uses Reinforcement Learning with flow variable eligibility trace. The controller is shown to improve performance of the mixing loop control compared to state of the art reinforcement learning and industrial grade controllers. The controller is tested on a hardware in the loop setup for rapid testing of mixing loop control used in building heating.","","978-1-7281-2767-5","10.1109/CCTA.2019.8920398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920398","","Buildings;Learning (artificial intelligence);Heating systems;Hydraulic systems;Delays;Monte Carlo methods","building management systems;function approximation;learning (artificial intelligence);neurocontrollers;optimal control;space heating;temperature control","mixing loop control;flow variable eligibility trace;proper pressurization;temperature control;optimal control;self learning controller;reinforcement learning;industrial grade controllers;loop setup","","4","1","15","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"A Reinforcement Learning-Based Virtual Machine Placement Strategy in Cloud Data Centers","S. Long; Z. Li; Y. Xing; S. Tian; D. Li; R. Yu","Key Laboratory of Hunan Province for Internet of Things and Information Security Hunan International Scientific and Technological Cooperation Base of Intelligent network, School of Computer Science Xiangtan University, Xiangtan, China; Key Laboratory of Hunan Province for Internet of Things and Information Security Hunan International Scientific and Technological Cooperation Base of Intelligent network, School of Computer Science Xiangtan University, Xiangtan, China; School of Computer Science, Xiangtan University, Xiangtan, China; School of Computer Science, Xiangtan University, Xiangtan, China; College of Computer, National University of Defense Technology, Changsha, China; School of Automation, Guangdong University of Technology, Guangzhou, China","2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)","26 Apr 2021","2020","","","223","230","With the widespread use of cloud computing, energy consumption of cloud data centers is increasing which mainly comes from IT equipment and cooling equipment. This paper argues that once the number of virtual machines on the physical machines reaches a certain level, resource competition occurs, resulting in a performance loss of the virtual machines. Unlike most papers, we do not impose placement constraints on virtual machines by giving a CPU cap to achieve the purpose of energy savings in cloud data centers. Instead, we use the measure of performance loss to weigh. We propose a reinforcement learning-based virtual machine placement strategy(RLVMP) for energy savings in cloud data centers. The strategy considers the weight of virtual machine performance loss and energy consumption, which is finally solved with the greedy strategy. Simulation experiments show that our strategy has a certain improvement in energy savings compared with the other algorithms.","","978-1-7281-7649-9","10.1109/HPCC-SmartCity-DSS50907.2020.00028","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407943","Virtual Machine Placement;Cloud Data Centers;Reinforcement Learning;Energy Savings.","Weight measurement;Cloud computing;Data centers;Energy consumption;NP-hard problem;High performance computing;Virtual machining","cloud computing;computer centres;energy conservation;learning (artificial intelligence);power aware computing;virtual machines","virtual machines;physical machines;energy savings;cloud data centers;reinforcement learning-based virtual machine placement strategy;virtual machine performance loss;energy consumption;cloud computing;cooling equipment;CPU cap","","4","","32","IEEE","26 Apr 2021","","","IEEE","IEEE Conferences"
"Communication-Aware Formation Control of AUVs With Model Uncertainty and Fading Channel via Integral Reinforcement Learning","W. Cao; J. Yan; X. Yang; X. Luo; X. Guan","Institute of Electrical Engineering, Yanshan University, Qinhuangdao, China; Institute of Electrical Engineering, Yanshan University, Qinhuangdao, China; Institute of Electrical Engineering, Yanshan University, Qinhuangdao, China; Institute of Electrical Engineering, Yanshan University, Qinhuangdao, China; Institute of Electrical Engineering, Yanshan University, Qinhuangdao, China","IEEE/CAA Journal of Automatica Sinica","6 Jan 2023","2023","10","1","159","176","Most formation approaches of autonomous underwater vehicles (AUVs) focus on the control techniques, ignoring the influence of underwater channel. This paper is concerned with a communication-aware formation issue for AUVs, subject to model uncertainty and fading channel. An integral reinforcement learning (IRL) based estimator is designed to calculate the probabilistic channel parameters, wherein the multivariate probabilistic collocation method with orthogonal fractional factorial design (M-PCM-OFFD) is employed to evaluate the uncertain channel measurements. With the estimated signal-to-noise ratio (SNR), we employ the IRL and M-PCM-OFFD to develop a saturated formation controller for AUVs, dealing with uncertain dynamics and current parameters. For the proposed formation approach, an integrated optimization solution is presented to make a balance between formation stability and communication efficiency. Main innovations lie in three aspects: 1) Construct an integrated communication and control optimization framework; 2) Design an IRL-based channel prediction estimator; 3) Develop an IRL-based formation controller with M-PCM-OFFD. Finally, simulation results show that the formation approach can avoid local optimum estimation, improve the channel efficiency, and relax the dependence of AUV model parameters.","2329-9274","","10.1109/JAS.2023.123021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10007910","Autonomous underwater vehicles (AUVs);communication-aware;formation;reinforcement learning;uncertainty","Fading channels;Autonomous underwater vehicles;Technological innovation;Uncertainty;Channel estimation;Reinforcement learning;Probabilistic logic","autonomous underwater vehicles;control system synthesis;fading channels;mobile robots;multi-robot systems;optimisation;probability;reinforcement learning","autonomous underwater vehicles focus;AUV model parameters;communication-aware formation control;control optimization framework;fading channel;formation stability;integral reinforcement learning based estimator;integrated communication;integrated optimization solution;IRL-based channel prediction estimator;IRL-based formation controller;M-PCM-OFFD;model uncertainty;multivariate probabilistic collocation method with orthogonal fractional factorial design;probabilistic channel parameters;saturated formation controller;signal-to-noise ratio estimation;uncertain channel measurements;underwater channel","","3","","46","","6 Jan 2023","","","IEEE","IEEE Journals"
"Distributed Cooperative Multi-Agent Reinforcement Learning with Directed Coordination Graph","G. Jing; H. Bai; J. George; A. Chakrabortty; P. K. Sharma","School of Automation, Chongqing University, Chongqing, China; Oklahoma State University, Stillwater, OK, USA; DEVCOM U.S. Army Research Laboratory, Adelphi, MD, USA; North Carolina State University, Raleigh, NC, USA; DEVCOM U.S. Army Research Laboratory, Adelphi, MD, USA","2022 American Control Conference (ACC)","5 Sep 2022","2022","","","3273","3278","Existing distributed cooperative multi-agent reinforcement learning (MARL) frameworks usually assume undirected coordination graphs and communication graphs, while estimating a global reward via consensus algorithms for policy evaluation. Such a framework may induce expensive communication costs and exhibit poor scalability due to requirement of global consensus. In this work, we study MARLs with directed coordination graphs, and propose a distributed RL algorithm where the local policy evaluations are based on local value functions. The local value function of each agent is obtained by local communication with its neighbors through a directed learning-induced communication graph, without using any consensus algorithm. A zeroth-order optimization (ZOO) approach based on parameter perturbation is employed to achieve gradient estimation. By comparing with existing ZOO-based RL algorithms, we show that our proposed distributed RL algorithm guarantees high scalability. A distributed resource allocation example is shown to illustrate the effectiveness of our algorithm.","2378-5861","978-1-6654-5196-3","10.23919/ACC53348.2022.9867152","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9867152","Reinforcement learning;multi-agent systems;decomposition;distributed control","Costs;Scalability;Perturbation methods;Estimation;Reinforcement learning;Consensus algorithm;Control systems","distributed algorithms;gradient methods;graph theory;multi-agent systems;reinforcement learning;resource allocation","distributed resource allocation example;distributed cooperative multiagent reinforcement;directed coordination graph;multiagent reinforcement learning frameworks;MARL;undirected coordination graphs;communication graphs;global reward;consensus algorithm;policy evaluation;expensive communication costs;global consensus;distributed RL algorithm;local policy evaluations;local value function;local communication;existing ZOO-based RL algorithms;algorithm guarantees high scalability;zeroth order optimization approach;directed learning induced communication graph","","3","","15","","5 Sep 2022","","","IEEE","IEEE Conferences"
"From Robots to Reinforcement Learning","T. Du; M. T. Cox; D. Perlis; J. Shamwell; T. Oates","Department of Automation, Harbin Engineering University, Harbin, China; Department of Computer Science, University of Maryland, College Park, USA; Department of Computer Science, University of Maryland, College Park, USA; Department of Computer Science, University of Maryland, College Park, USA; Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore, USA","2013 IEEE 25th International Conference on Tools with Artificial Intelligence","10 Feb 2014","2013","","","540","545","In this paper, we review recent advances in Reinforcement Learning (RL) in light of potential applications to robotics, introduce the basic concepts of RL and Markov Decision Process (MDP), and compare different RL algorithms such as Q-learning, Temporal Difference learning, the Actor Critic, and the Natural Actor Critic. We conclude that policy gradient methods are more suitable for solving continuous state/action MDP problems than RL with lookup tables or general function approximators. Further, natural policy gradient methods can efficiently converge to locally optimal solutions. Some simulation results are given to support our arguments. We also present a brief overview of our approach to developing an autonomous robot agent that can perceive, learn from and interact with the environment, and reason about and handle unexpected problems using its knowledge base.","2375-0197","978-1-4799-2972-6","10.1109/ICTAI.2013.86","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6735297","Reinforcement Learning;autonomous robots;value function;policy gradients;Natural Actor Critic;robot knowledge base","Robots;Gradient methods;Function approximation;Approximation algorithms;Knowledge based systems;Convergence;Cognition","control engineering computing;decision theory;gradient methods;knowledge based systems;learning (artificial intelligence);Markov processes;mobile robots","reinforcement learning;robotics;Markov decision process;RL algorithms;Q-learning;temporal difference learning;natural actor critic;continuous state/action MDP problems;natural policy gradient methods;autonomous robot agent;knowledge base","","3","","31","IEEE","10 Feb 2014","","","IEEE","IEEE Conferences"
"Cluster Synchronization of Boolean Networks Under State-Flipped Control With Reinforcement Learning","Z. Zhou; Y. Liu; J. Lu; L. Glielmo","College of Mathematics and Computer Science, Zhejiang Normal University, Jinhua, China; Key Laboratory of Intelligent Education Technology and Application of Zhejiang Province and College of Mathematics and Computer Science, Zhejiang Normal University, Jinhua, China; School of Mathematics, Southeast University, Nanjing, China; Department of Engineering, University of Sannio, Benevento, Italy","IEEE Transactions on Circuits and Systems II: Express Briefs","2 Dec 2022","2022","69","12","5044","5048","In this brief, cluster synchronization of Boolean Networks (BNs) under state-flipped control is considered. We show how the cluster synchronization problem can be transformed into a set stabilization problem, based on which we give a theorem to judge whether the cluster synchronization of BNs can be achieved under a given flip set. Moreover, when the network structure is unknown, the  $Q$ -learning  $(QL)$  algorithm, a model-free reinforcement learning algorithm, is developed to search control sequences to achieve cluster synchronization. Some numerical examples are used to verify the validity of the theoretical results at the end.","1558-3791","","10.1109/TCSII.2022.3199786","National Natural Science Foundation of China(grant numbers:62173308,61903339); National Training Programs of Innovation and Entrepreneurship(grant numbers:202110345050); Natural Science Foundation of Zhejiang Province of China(grant numbers:LR20F030001,LD19A010001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9861670","Boolean networks;cluster synchronization;state-flipped-transition matrix;Q-learning","Synchronization;Transforms;Q-learning;Clustering algorithms;Technological innovation;Stability criteria;Numerical stability","control engineering computing;reinforcement learning;synchronisation","Boolean networks;cluster synchronization problem;control sequences;flip set;model-free reinforcement learning algorithm;Q-learning algorithm;QL algorithm;set stabilization problem;state-flipped control","","3","","25","IEEE","18 Aug 2022","","","IEEE","IEEE Journals"
"Distributed Actor–Critic Algorithms for Multiagent Reinforcement Learning Over Directed Graphs","P. Dai; W. Yu; H. Wang; S. Baldi","School of Mathematics, Southeast University, Nanjing, China; School of Mathematics and the School of Automation, Southeast University, Nanjing, China; School of Mathematics, Southeast University, Nanjing, China; School of Mathematics, Southeast University, Nanjing, China","IEEE Transactions on Neural Networks and Learning Systems","5 Oct 2023","2023","34","10","7210","7221","Actor–critic (AC) cooperative multiagent reinforcement learning (MARL) over directed graphs is studied in this article. The goal of the agents in MARL is to maximize the globally averaged return in a distributed way, i.e., each agent can only exchange information with its neighboring agents. AC methods proposed in the literature require the communication graphs to be undirected and the weight matrices to be doubly stochastic (more precisely, the weight matrices are row stochastic and their expectation are column stochastic). Differently from these methods, we propose a distributed AC algorithm for MARL over directed graph with fixed topology that only requires the weight matrix to be row stochastic. Then, we also study the MARL over directed graphs (possibly not connected) with changing topologies, proposing a different distributed AC algorithm based on the push-sum protocol that only requires the weight matrices to be column stochastic. Convergence of the proposed algorithms is proven for linear function approximation of the action value function. Simulations are presented to demonstrate the effectiveness of the proposed algorithms.","2162-2388","","10.1109/TNNLS.2021.3139138","National Natural Science Foundation of China(grant numbers:61673107,62073076,62073074); Jiangsu Provincial Key Laboratory of Networked Collective Intelligence(grant numbers:BM2017002); Double Innovation Plan(grant numbers:4207012004); Special Funding for Overseas Talents(grant numbers:6207011901); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9677456","Directed graph;distributed actor–critic (AC) algorithm;multiagent reinforcement learning (MARL);push-sum protocol","Directed graphs;Topology;Approximation algorithms;Convergence;Protocols;Q-learning;Function approximation","","","","3","","43","IEEE","11 Jan 2022","","","IEEE","IEEE Journals"
"An online integral reinforcement learning algorithm to solve N-player Nash games","K. G. Vamvoudakis; F. L. Lewis","Center for Control, Dynamical-systems and Computation (CCDC), University of California, Santa Barbara, CA, USA; Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA","2012 IEEE International Symposium on Intelligent Control","31 Dec 2012","2012","","","697","702","In this paper we introduce an online algorithm that uses integral reinforcement knowledge for learning the continuous-time Nash game (zero-sum and non-zero-sum) solution for nonlinear systems with infinite horizon costs and partial knowledge of the system dynamics. This algorithm is a data based approach to the solution of the coupled Hamilton-Jacobi equations and it does not require explicit knowledge on the system's drift dynamics. A novel adaptive control algorithm is given that is based on policy iteration and implemented using an actor/critic structure for every player in the game having 2N adaptive approximator structures. All 2N approximation networks are adapted simultaneously. Novel adaptive control tuning algorithms are given for the critic and actor networks. The convergence to the Nash solution of the game is proven, and stability of the system is also guaranteed. Simulation examples support the theoretical result.","2158-9879","978-1-4673-4600-9","10.1109/ISIC.2012.6398248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398248","Nash games;Coupled Hamilton-Jacobi equations;Coupled Riccati equations;Nash equilibrium;integral reinforcement learning","Games;Equations;Heuristic algorithms;Tuning;Mathematical model;Approximation algorithms;Artificial neural networks","adaptive control;continuous time systems;convergence;game theory;geometry;iterative methods;learning (artificial intelligence);nonlinear systems;tuning","online integral reinforcement learning algorithm;N-player Nash games;reinforcement knowledge;continuous-time Nash game solution;nonlinear systems;infinite horizon costs;system dynamics;coupled Hamilton-Jacobi equations;policy iteration;2N adaptive approximator structures;novel adaptive control tuning algorithms;stability;actor neural network structure;critic neural networks","","2","","21","IEEE","31 Dec 2012","","","IEEE","IEEE Conferences"
"A Multiagent Reinforcement Learning Approach for Wind Farm Frequency Control","Y. Liang; X. Zhao; L. Sun","Intelligent Control and Smart Energy Research Group, School of Engineering, University of Warwick, Coventry, U.K.; Intelligent Control and Smart Energy Research Group, School of Engineering, University of Warwick, Coventry, U.K.; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China","IEEE Transactions on Industrial Informatics","15 Dec 2022","2023","19","2","1725","1734","As wind turbines (WTs) become more prevalent, there is an increasing interest in actively controlling their power output to participate in the frequency regulation for the power grid. Conventional frequency regulation controllers use fixed gains, making it difficult for the WT to adjust its kinetic energy uptake to its operating conditions and to collaborate effectively with other WTs in the wind farm. In addition, the design of conventional frequency controllers does not consider their impacts on the mechanical structure. To address these issues, in this article, we model the cooperative frequency control problem for all the WTs in a wind farm as a decentralized partially observable Markov decision process and use a multiagent deep reinforcement learning algorithm to solve it. We also develop a grid-connected wind farm simulation model based on MATLAB/Simulink and OpenFAST, which can reflect the detailed interactions between the electrical and mechanical components of WTs. Simulation results show that the proposed strategy is effective in reducing frequency drops and has less impact on mechanical structure deflections compared with traditional methods.","1941-0050","","10.1109/TII.2022.3182328","European Union’s Horizon 2020 Research and Innovation Program; Marie Sklodowska-Curie(grant numbers:861398); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9795254","Frequency regulation;multiagent deep reinforcement learning (MADRL);wind farm;wind turbine machinery","Frequency control;Wind farms;Blades;Power capacitors;Poles and towers;Kinetic energy;Reinforcement learning","frequency control;learning (artificial intelligence);load regulation;Markov processes;multi-agent systems;power generation control;power grids;wind power plants;wind turbines","conventional frequency controllers;conventional frequency regulation controllers;decentralized partially observable Markov decision process;frequency control problem;frequency drops;grid-connected wind farm simulation model;kinetic energy uptake;mechanical structure deflections;multiagent deep reinforcement learning algorithm;multiagent reinforcement learning approach;operating conditions;power grid;power output;wind farm frequency control;wind turbines;WTs","","2","","40","IEEE","13 Jun 2022","","","IEEE","IEEE Journals"
"Impedance Adaptation by Reinforcement Learning with Contact Dynamic Movement Primitives","C. Chang; K. Haninger; Y. Shi; C. Yuan; Z. Chen; J. Zhang","Agile Robots AG, Munich, Germany; Department of Automation, Fraunhofer IPK, Berlin, Germany; Department of Informatics, TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg, Hamburg, Germany; Intel Asia-Pacific Research & Development Ltd., Shanghai, China; Agile Robots AG, Munich, Germany; Department of Informatics, TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg, Hamburg, Germany","2022 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)","25 Aug 2022","2022","","","1185","1191","Dynamic movement primitives (DMPs) allow complex position trajectories to be efficiently demonstrated to a robot. In contact-rich tasks, where position trajectories alone may not be safe or robust over variation in contact geometry, DMPs have been extended to include force trajectories. However, different task phases or degrees of freedom may require the tracking of either position or force–e.g., once contact is made, it may be more important to track the force demonstration trajectory in the contact direction. The robot impedance balances between following a position or force reference trajectory, where a high stiffness tracks position and a low stiffness tracks force. This paper proposes using DMPs to learn position and force trajectories from demonstrations, then adapting the impedance parameters online with a higher-level control policy trained by reinforcement learning. This allows one-shot demonstration of the task with DMPs, and improved robustness and performance from the impedance adaptation. The approach is validated on peg-in-hole and adhesive strip application tasks.","2159-6255","978-1-6654-1308-4","10.1109/AIM52237.2022.9863416","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9863416","","Training;Force;Dynamics;Reinforcement learning;Robustness;Trajectory;Impedance","force control;mobile robots;motion control;position control;reinforcement learning;robust control;trajectory control","impedance adaptation;reinforcement learning;contact dynamic movement primitives;DMPs;complex position trajectories;contact geometry;force trajectories;task phases;degrees of freedom;force demonstration trajectory;contact direction;robot impedance;high stiffness tracks position;low stiffness tracks force;impedance parameters;one-shot demonstration;adhesive strip application tasks;force reference trajectory;higher-level control policy","","2","","28","IEEE","25 Aug 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Energy Trading and Management of Regional Interconnected Microgrids","S. Liu; S. Han; S. Zhu","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; Department of Automation, Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Smart Grid","21 Apr 2023","2023","14","3","2047","2059","In this paper, we present a Value-Decomposition Deep Deterministic Policy Gradients (V3DPG) based Reinforcement Learning (RL) method for energy trading and management of regional interconnected microgrids (MGs). In practice, the state of an MG is time-varying and the traded energy flows continuously, which is generally neglected in researches. To address this problem, an Actor-Critic framework is adopted. Each MG has to make energy trading decision based on local observation and has no access to any knowledge of other MGs. We bring in the idea of value-decomposition in the training process to ensure the generation of feasible cooperative policies while maintaining MGs’ privacy and autonomous decision-making ability. Furthermore, in light of the uncertainty and fluctuation of renewable energy generation and users’ demand, a recurrent neural network (RNN) with Burn-In initialization is combined with critic network to achieve implicit predictions. Meanwhile, we also take Energy Storage System (ESS) with operational constraints into consideration and deem it as a virtual market innovatively. Experiments have been carried out under real-world data to verify the merit of the proposed method, compared to existing RL-based works.","1949-3061","","10.1109/TSG.2022.3214202","National Natural Science Foundation of China(grant numbers:62133008,61821004,61922058,62173225); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9917497","Multi-agent system;regional interconnected microgrids;energy trading and management;reinforcement learning;RNN","Renewable energy sources;State of charge;Uncertainty;Microgrids;Energy management;Generators;Fluctuations","decision making;deep learning (artificial intelligence);distributed power generation;gradient methods;learning (artificial intelligence);power engineering computing;recurrent neural nets;reinforcement learning","Actor-Critic framework;critic network;Energy Storage System;energy trading decision;existing RL-based works;feasible cooperative policies;MGs' privacy;regional interconnected microgrids;Reinforcement Learning method;Reinforcement Learning-based Energy trading;renewable energy generation;time-varying;traded energy;V3DPG;Value-Decomposition Deep Deterministic Policy Gradients","","2","","35","IEEE","12 Oct 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning for Energy Optimization Under Human Fatigue Constraints of Power-Assisted Wheelchairs","G. Feng; L. Buşoniu; T. M. Guerra; S. Mohammad","Univ. Valenciennes CNRS, LAMIH, France; Department of Automation, Technical University of Cluj-Napoca, Cluj-Napoca, Romania; Univ. Valenciennes CNRS, LAMIH, France; AutoNomad Mobility Le Mont Houy, Valenciennes Cedex 9, France","2018 Annual American Control Conference (ACC)","16 Aug 2018","2018","","","4117","4122","In the last decade, Power-Assisted Wheelchairs (PAWs) have been widely used for improving the mobility of disabled persons. The main advantage of PAWs is that users can keep a suitable physical activity. Moreover, the metabolic-electrical energy hybridization of PAWs provides more flexibility for optimal control design. In this context, we propose an optimal control for minimizing the electrical energy consumption under human fatigue constraints, including a human fatigue model. The electrical motor has to cooperate with the user over a given distance-to-go. As the human fatigue model is unknown in practice, we use model-free Policy Gradient methods to directly learn controllers for a given driving task. We verify that the model-free solution is near-optimal by computing the model-based controller, which is generated by Approximate Dynamic Programming. Simulation results confirm that the model-free Policy Gradient method provides near-optimal solutions.","2378-5861","978-1-5386-5428-6","10.23919/ACC.2018.8431038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8431038","","Fatigue;Wheelchairs;Force;Computational modeling;Optimal control;Energy consumption;Dynamic programming","control system synthesis;dynamic programming;electric motors;electrical engineering computing;energy conservation;gradient methods;handicapped aids;learning (artificial intelligence);optimal control;wheelchairs","PAWs;disabled persons;metabolic-electrical energy hybridization;optimal control design;human fatigue constraints;human fatigue model;electrical motor;model-based controller;near-optimal solutions;reinforcement learning;energy optimization;approximate dynamic programming;model-free policy gradient method;electrical energy consumption minimization;power-assisted wheelchairs;physical activity","","2","","21","","16 Aug 2018","","","IEEE","IEEE Conferences"
"Intermediate Sensory Feedback Assisted Multi-Step Neural Decoding for Reinforcement Learning Based Brain-Machine Interfaces","X. Shen; X. Zhang; Y. Huang; S. Chen; Z. Yu; Y. Wang","Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Sai Kung, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Sai Kung, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Sai Kung, Hong Kong; Department of Chemical and Biological Engineering, Hong Kong University of Science and Technology, Sai Kung, Hong Kong; School of Automation Science and Engineering, South China University of Technology and Pazhou Laboratory, Guangzhou, China; Department of Electronic and Computer Engineering and Department of Chemical and Biological Engineering, Hong Kong University of Science and Technology, Sai Kung, Hong Kong","IEEE Transactions on Neural Systems and Rehabilitation Engineering","20 Oct 2022","2022","30","","2834","2844","Reinforcement-learning (RL)-based brain-machine interfaces (BMIs) interpret dynamic neural activity into movement intention without patients’ real limb movements, which is promising for clinical applications. A movement task generally requires the subjects to reach the target within one step and rewards the subjects instantaneously. However, a real BMI scenario involves tasks that require multiple steps, during which sensory feedback is provided to indicate the status of the prosthesis, and the reward is only given at the end of the trial. Actually, subjects internally evaluate the sensory feedback to adjust motor activity. Existing RL-BMI tasks have not fully utilized the internal evaluation from the brain upon the sensory feedback to guide the decoder training, and there lacks an effective tool to assign credit for the multi-step decoding task. We propose first to extract intermediate guidance from the medial prefrontal cortex (mPFC) to assist the learning of multi-step decoding in an RL framework. To effectively explore the neural-action mapping in a large state-action space, a temporal difference (TD) method is incorporated into quantized attention-gated kernel reinforcement learning (QAGKRL) to assign the credit over the temporal sequence of movement, but also discriminate spatially in the Reproducing Kernel Hilbert Space (RKHS). We test our approach on the data collected from the primary motor cortex (M1) and the mPFC of rats when they brain control the cursor to reach the target within multiple steps. Compared with the models which only utilize the final reward, the intermediate evaluation interpreted from the mPFC can help improve the prediction accuracy by 10.9% on average across subjects, with faster convergence and more stability. Moreover, our proposed algorithm further increases 18.2% decoding accuracy compared with existing TD-RL methods. The results reveal the possibility of achieving better multi-step decoding performance for more complicated BMI tasks.","1558-0210","","10.1109/TNSRE.2022.3210700","Grants from China Brain Project(grant numbers:2021ZD0200403); National Natural Science Foundation of China(grant numbers:61836003); Seed Fund of the Big Data for Bio-Intelligence Laboratory(grant numbers:Z0428); The Hong Kong University of Science and Technology (HKUST)-Guangzhou University Joint Research Collaboration Fund(grant numbers:GZU22EG01); Special Research Support from Chao Hoi Shuen Foundation(grant numbers:R9051); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9915652","Brain-machine interface (BMI);reinforcement learning;medial prefrontal cortex;sensory feedback;multi-step task;temporal difference learning","Decoding;Task analysis;Neural activity;Trajectory;Kernel;Robot sensing systems;Prosthetics","brain;brain-computer interfaces;Hilbert spaces;learning (artificial intelligence);medical signal processing;neural nets;neurophysiology;trajectory control","intermediate sensory feedback assisted multistep neural decoding;reinforcement-learning-based brain-machine interfaces;dynamic neural activity;movement intention;patients;movement task;BMI scenario;motor activity;RL-BMI tasks;internal evaluation;decoder training;multistep decoding task;intermediate guidance;medial prefrontal cortex;mPFC;RL framework;neural-action mapping;state-action space;quantized attention-gated kernel reinforcement learning;Reproducing Kernel Hilbert Space;brain control;intermediate evaluation;18.2% decoding accuracy;existing TD-RL methods;multistep decoding performance;complicated BMI tasks","Animals;Rats;Brain-Computer Interfaces;Feedback, Sensory;Reinforcement, Psychology;Learning;Movement","2","","42","CCBYNCND","11 Oct 2022","","","IEEE","IEEE Journals"
"Monocular Camera-Based Complex Obstacle Avoidance via Efficient Deep Reinforcement Learning","J. Ding; L. Gao; W. Liu; H. Piao; J. Pan; Z. Du; X. Yang; B. Yin","School of Computer Science, Dalian University of Technology, Dalian, China; School of Computer Science, Dalian University of Technology, Dalian, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; Northwestern Polytechnical University, Xi’an, China; Department of Computer Science, The University of Hong Kong, Hong Kong, China; SIASUN Robot & Automation Company Ltd., Shenyang, China; Dalian University of Technology, Dalian, China; Dalian University of Technology, Dalian, China","IEEE Transactions on Circuits and Systems for Video Technology","3 Feb 2023","2023","33","2","756","770","Deep reinforcement learning has achieved great success in laser-based collision avoidance works because the laser can sense accurate depth information without too much redundant data, which can maintain the robustness of the algorithm when it is migrated from the simulation environment to the real world. However, high-cost laser devices are not only difficult to deploy for a large scale of robots but also demonstrate unsatisfactory robustness towards the complex obstacles, including irregular obstacles, e.g., tables, chairs, and shelves, as well as complex ground and special materials. In this paper, we propose a novel monocular camera-based complex obstacle avoidance framework. Particularly, we innovatively transform the captured RGB images to pseudo-laser measurements for efficient deep reinforcement learning. Compared to the traditional laser measurement captured at a certain height that only contains one-dimensional distance information away from the neighboring obstacles, our proposed pseudo-laser measurement fuses the depth and semantic information of the captured RGB image, which makes our method effective for complex obstacles. We also design a feature extraction guidance module to weight the input pseudo-laser measurement, and the agent has more reasonable attention for the current state, which is conducive to improving the accuracy and efficiency of the obstacle avoidance policy. Besides, we adaptively add the synthesized noise to the laser measurement during the training stage to decrease the sim-to-real gap and increase the robustness of our model in the real environment. Finally, the experimental results show that our framework achieves state-of-the-art performance in several virtual and real-world scenarios.","1558-2205","","10.1109/TCSVT.2022.3203974","National Key Research and Development Program of China(grant numbers:2022ZD0210500,2021ZD0112400); National Natural Science Foundation of China(grant numbers:61972067/U21A20491/U1908214); Innovation Technology Funding of Dalian(grant numbers:2020JJ26GX036); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9875327","Deep reinforcement learning;obstacle avoidance;robot vision;robot navigation","Collision avoidance;Robots;Robot sensing systems;Semantics;Measurement by laser beam;Cameras;Sensors","collision avoidance;control engineering computing;deep learning (artificial intelligence);feature extraction;image capture;image colour analysis;mobile robots;navigation;reinforcement learning;robot vision","deep reinforcement learning;depth information;feature extraction guidance module;monocular camera;obstacle avoidance;obstacle avoidance policy;pseudolaser measurement;RGB image capture;robot navigation;semantic information","","2","","58","IEEE","5 Sep 2022","","","IEEE","IEEE Journals"
"Multi-agent Deep Reinforcement Learning Algorithm for Distributed Economic Dispatch in Smart Grid","L. Ding; Z. Lin; G. Yan","College of Electrical Engineering, Zhejiang University, Hangzhou, China; School of Automation Hangzhou, Dianzi University, Hangzhou, China; College of Electrical Engineering, Zhejiang University, Hangzhou, China","IECON 2020 The 46th Annual Conference of the IEEE Industrial Electronics Society","18 Nov 2020","2020","","","3529","3534","With the development of large-scale power grids, the issue of distributed economic dispatch has received considerable critical attention. However, due to the existence of some effects such as valve-point effects, the nonconvex objective function remains a major challenge for the distributed optimization problem. This paper proposes a cooperative deep reinforcement learning algorithm for distributed economic dispatch with the nonconvex objective function. In the distributed algorithm, all nodes obtain the value of actions by observing the environment and update state-action-value function in coordination with local neighbors. The state-action-value function is approximated by a neural network, which allows the algorithm to be used for large and continuous state spaces. The advantages of the algorithm are demonstrated through several case studies.","2577-1647","978-1-7281-5414-5","10.1109/IECON43393.2020.9255238","National Natural Science Foundation of China; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9255238","Distributed economic dispatch;multi-agent deep reinforcement learning;nonconvex optimization;state-action-value function approximation;neural network;smart grid","Reinforcement learning;Approximation algorithms;Neural networks;Function approximation;Power demand;Power generation;Economics","concave programming;learning (artificial intelligence);multi-agent systems;neural nets;power distribution economics;power engineering computing;power generation dispatch;power generation economics;smart power grids","multiagent deep reinforcement learning algorithm;distributed economic dispatch;smart grid;large-scale power grids;valve-point effects;nonconvex objective function;distributed optimization problem;distributed algorithm;state-action-value function","","1","","15","IEEE","18 Nov 2020","","","IEEE","IEEE Conferences"
"Toward Smart Multizone HVAC Control by Combining Context-Aware System and Deep Reinforcement Learning","X. Deng; Y. Zhang; Y. Zhang; H. Qi","Tsinghua-Berkeley Shenzhen Institute, Tsinghua University, Shenzhen, China; Institute of Future Human Habitats, Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Department of Automation, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Green Development Center, China Construction Science and Technology Group Cooperation, Shenzhen, China","IEEE Internet of Things Journal","21 Oct 2022","2022","9","21","21010","21024","Building energy consumption accounts for a large figure of total energy consumption and keeps a rapid increase. Energy for heating, ventilation, and air conditioning (HVAC) is the main contribution. To save energy with maintaining comfort, control methods have been studied, including rule-based methods, model predictive control, and deep reinforcement learning (DRL). While their performance in real applications can be restricted by the highly nonstationary building environment caused by factors like weather conditions. Especially, for multizone HVAC control with multiple controllers, variation of the controller policy causes potential nonstationarity for each other. Current solutions to the nonstationarity based on model-based methods add complexity for building modeling and decrease the control efficiency. In addition, although massive data are available with the development of the Internet of Things (IoT) in smart buildings, high-level exploitation of data in the context-aware system is not yet explored to detect environment changes for smart building control. To this end, we propose a novel context-aware model-free DRL method called Trans-Context soft actor–critic (SAC) for multizone HVAC control, which combines a transformer-encoder-based context-aware system and the state-of-the-art DRL algorithm SAC. The context-aware system disentangles the nonstationarity by learning context data from IoT sensors. Besides, Trans-Context SAC is a model-free method without the need for building modeling. We evaluate Trans-Context SAC in a simulation-based case study on a multizone commercial building. Results demonstrate that Trans-Context SAC can achieve up to 15.9% of energy saving compared to other baselines with maintaining thermal comfort. Besides, Trans-Context SAC obtains the generalization for unseen environments.","2327-4662","","10.1109/JIOT.2022.3175728","National Natural Science Foundation of China(grant numbers:51838007); Basic Research Program of Shenzhen Science and Technology Innovation Committee(grant numbers:WDZC20200819091614001); Carbon Neutrality Funds of Tsinghua Shenzhen International Graduate School(grant numbers:JC2021001); Tsinghua SIGS-CCSTG Future City Joint Lab funding; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9776508","Context-aware system;deep reinforcement learning (DRL);heating;ventilation;and air-conditioning (HVAC) control;soft actor–critic (SAC);transformer","HVAC;Buildings;Context modeling;Data models;Internet of Things;Computational modeling;Energy management","air conditioning;building management systems;energy conservation;energy consumption;HVAC;Internet of Things;learning (artificial intelligence);predictive control;ubiquitous computing","air conditioning;building modeling;context data;context-aware model-free DRL method;control efficiency;control methods;controller policy;deep reinforcement learning;energy consumption accounts;highly nonstationary building environment;model predictive control;model-based methods;model-free method;multiple controllers;multizone commercial building;rule-based methods;simulation-based case study;smart building control;smart buildings;total energy consumption;toward smart multizone HVAC control;Trans-Context SAC;Trans-Context soft actor-critic;transformer-encoder-based context-aware system","","1","","69","IEEE","17 May 2022","","","IEEE","IEEE Journals"
"Knowledge Graph-Based Reinforcement Federated Learning for Chinese Question and Answering","L. Xu; T. Chen; Z. Hou; W. Zhang; C. Hon; X. Wang; D. Wang; L. Chen; W. Zhu; Y. Tian; H. Ning; F. -Y. Wang","School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer Science and Technology, China University of Petroleum, Qingdao, China; Digital Research Institute, ENN Group, Langfang, China; School of Computer Science and Technology, China University of Petroleum, Qingdao, China; Faculty of Innovation Engineering, Macau University of Science and Technology, Macau, China; School of Artificial Intelligence, Anhui University, Anhui, China; Faculty of Innovation Engineering, Macau University of Science and Technology, Macau, China; Department of Computer and Information Science, University of Macau, Macau, China; Institute of State Key Laboratory of Digital Household Appliances, Qingdao, China; Institute of State Key Laboratory of Digital Household Appliances, Qingdao, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; Institute of Automation, Chinese Academy of Sciences (CAS), Beijing, China","IEEE Transactions on Computational Social Systems","","2023","PP","99","1","11","Knowledge question and answering (Q&A) is widely used. However, most existing semantic parsing methods in Q&A usually use cascading, which can incur error accumulation. In addition, using only one institution’s Q&A data definitely will limit the Q&A performance, while data privacy prevents sharing between institutions. This article proposes a knowledge graph-based reinforcement federated learning (KGRFL)-based Q&A approach to address these challenges. We design an end-to-end multitask semantic parsing model MSP-bidirectional and auto-regressive transformers (BART) that identifies question categories while converting questions into SPARQL statements to improve semantic parsing. Meanwhile, a reinforcement learning (RL)-based model fusion strategy is proposed to improve the effectiveness of federated learning, which enables multi-institution joint modeling and data privacy protection using cross-domain knowledge. In particular, it also reduces the negative impact of low-quality clients on the global model. Furthermore, a prompt learning-based entity disambiguation method is proposed to address the semantic ambiguity problem because of joint modeling. The experiments show that the proposed method performs well on different datasets. The Q&A results of the proposed approach outperform the approach of using only a single institution. Experiments also demonstrate that the proposed approach is resilient to security attacks, which is required for real applications.","2329-924X","","10.1109/TCSS.2023.3246795","National Natural Science Foundation of China(grant numbers:62072469); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10058174","Knowledge graph;multitask semantic parsing MSP-bidirectional and auto-regressive transformers (BART);prompt learning;question and answering (Q&A);reinforcement federated learning (RFL)","Data models;Semantics;Federated learning;Knowledge graphs;Data privacy;Transformers;Task analysis","","","","1","","","IEEE","3 Mar 2023","","","IEEE","IEEE Early Access Articles"
"Reinforcement Learning Models and Algorithms for Diabetes Management","K. -L. A. Yau; Y. -W. Chong; X. Fan; C. Wu; Y. Saleem; P. -C. Lim","Lee Kong Chian Faculty of Engineering and Science (LKCFES), Universiti Tunku Abdul Rahman (UTAR), Kajang, Selangor, Malaysia; National Advanced IPv6 Centre, Universiti Sains Malaysia (USM), Gelugor, Penang, Malaysia; School of Automation and Information Engineering, Xi’an University of Technology, Xi’an, China; Graduate School of Informatics and Engineering, The University of Electro-Communications, Tokyo, Japan; Department of Computer Science, Aberystwyth University, Aberystwyth, Ceredigion, U.K; Department of Pharmacy, Hospital Pulau Pinang, George Town, Penang, Malaysia","IEEE Access","27 Mar 2023","2023","11","","28391","28415","With the advancements in reinforcement learning (RL), new variants of this artificial intelligence approach have been introduced in the literature. This has led to increased interest in using RL to address complex issues in diabetes management. Using RL, a decision maker (or agent) observes decision-making factors (or state) from the dynamic operating environment, selects actions, and subsequently receives delayed rewards. The agent adapts its actions to changes in the operating environment to maximize its cumulative reward and improve system performance. This paper presents how various variants of RL have been used to improve diabetes management, such as a higher time in range during which the blood glucose level is within the normal range and a higher similarity between RL and physician’s policies. Key highlights focus on the application of RL in diabetes management, including a taxonomy of the attributes of RL (e.g., roles and advantages), essential elements for training (e.g., data and simulators), representations of diabetes attributes in RL models, and variants of RL algorithms. In addition, this paper discusses open issues and potential future developments in the use of RL in diabetes management.","2169-3536","","10.1109/ACCESS.2023.3259425","Universiti Tunku Abdul Rahman (UTAR), the Publication Fund under Research Creativity and Management Office, Universiti Sains Malaysia, and the Universiti Sains Malaysia (USM)(grant numbers:304/PNAV/650958/U154); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10077379","Actor-critic reinforcement learning;applied reinforcement learning;deep Q-network;deep reinforcement learning;diabetes;Markov decision process;multi-agent reinforcement learning;reinforcement learning","Diabetes;Glucose;Blood;Insulin;Reinforcement learning;Data models;Deep learning;Multi-agent systems;Q-learning","artificial intelligence;blood;decision making;diseases;learning (artificial intelligence);medical computing;multi-agent systems;reinforcement learning","diabetes management;reinforcement learning models;RL models","","1","","97","CCBYNCND","20 Mar 2023","","","IEEE","IEEE Journals"
"Improving Generalization of Deep Reinforcement Learning-based TSP Solvers","W. Ouyang; Y. Wang; S. Han; Z. Jin; P. Weng","UM-SJTU Joint Institute, Shanghai Jiao Tong University; UM-SJTU Joint Institute, Shanghai Jiao Tong University; UM-SJTU Joint Institute, Shanghai Jiao Tong University; UM-SJTU Joint Institute, Shanghai Jiao Tong University; UM-SJTU Joint Institute, Shanghai Jiao Tong University","2021 IEEE Symposium Series on Computational Intelligence (SSCI)","24 Jan 2022","2021","","","01","08","Recent work applying deep reinforcement learning (DRL) to solve traveling salesman problems (TSP) has shown that DRL-based solvers can be fast and competitive with TSP heuristics for small instances, but do not generalize well to larger instances. In this work, we propose a novel approach named MAGIC that includes a deep learning architecture and a DRL training method. Our architecture, which integrates a multilayer perceptron, a graph neural network, and an attention model, defines a stochastic policy that sequentially generates a TSP solution. Our training method includes several innovations: (1) we interleave DRL policy gradient updates with local search (using a new local search technique), (2) we use a novel simple baseline, and (3) we apply curriculum learning. Finally, we empirically demonstrate that MAGIC is superior to other DRL-based methods on random TSP instances, both in terms of performance and generalizability. Moreover, our method compares favorably against TSP heuristics and other state-of-the-art approach in terms of performance and computational time.","","978-1-7281-9048-8","10.1109/SSCI50451.2021.9659970","Shanghai Jiao Tong University(grant numbers:IPP21141); National Natural Science Foundation of China(grant numbers:61872238); NSF(grant numbers:19ZR1426700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9659970","traveling salesman problem;deep reinforcement learning;local search;curriculum learning","Training;Deep learning;Technological innovation;Runtime;Stochastic processes;Computer architecture;Reinforcement learning","deep learning (artificial intelligence);generalisation (artificial intelligence);graph neural networks;multilayer perceptrons;random processes;reinforcement learning;search problems;stochastic processes;travelling salesman problems","attention model;curriculum learning;deep learning architecture;deep reinforcement learning-based TSP solvers;DRL policy gradient updates;DRL training method;DRL-based solvers;generalizability;generalization;graph neural network;local search technique;MAGIC;multilayer perceptron;random TSP instances;stochastic policy;traveling salesman problems;TSP heuristics","","1","","25","IEEE","24 Jan 2022","","","IEEE","IEEE Conferences"
"Route Planning Based on Deep Reinforcement Learning to Minimize Energy Consumption in UAV Photogrammetry","H. Wang; S. Xu; L. Qin; G. Wu","NA; NA; Department of automation, University of Science and Technology of China, Hefei, P. R. China; NA","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","1757","1762","In photogrammetry, limited battery energy and storage capacity require unmanned aerial vehicles (UAVs) to travel frequently to and from depots. Therefore, route planning is necessary to ensure safety and efficiency. In this paper, the traditional iterative search algorithm is improved through neural network-assisted deep reinforcement learning with an attention layer and the strategy of separating improvement and reconstruction operators, and applied to the vehicle route problem with energy and capacity constraints. Simulation and practice show that energy consumption of the route generated by the improved algorithm is reduced by 10.63%, and the calculation time is only 8.92% that of the optimal traditional algorithm. Therefore, deep reinforcement learning can be applied in the simulation and practice of UAV photogrammetry, and it has been verified to be reliable and efficient, which lays the foundation for minimizing the energy consumption of photogrammetry with multi-aircraft cooperation.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902609","vehicle routing problem;iterative search algorithm;improvement and reconstruction operator","Energy consumption;Solid modeling;Three-dimensional displays;Vehicle routing;Reinforcement learning;Autonomous aerial vehicles;Search problems","autonomous aerial vehicles;collision avoidance;iterative methods;learning (artificial intelligence);military aircraft;mobile robots;optimisation;photogrammetry;remotely operated vehicles;search problems","attention layer;battery energy;capacity constraints;minimize energy consumption;neural network-assisted deep reinforcement learning;optimal traditional algorithm;practice show;reconstruction operators;route planning;safety;storage capacity;traditional iterative search algorithm;UAV photogrammetry;UAVs;unmanned aerial vehicles;vehicle route problem","","1","","32","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Robot Navigation with Interaction-based Deep Reinforcement Learning","Y. Zhai; Y. Miao; H. Wang","China University of Mining and Technology, Jiangsu, China; Department of Information and Control Engineering, The Artificial Intelligence Research Institute, China; Department of Automation, Institute of Medical Robotics, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Jiao Tong University, Shanghai, China","2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)","28 Mar 2022","2021","","","1974","1979","For the scene of dense crowd flow in limited space, it is very important and challenging for the robot to walk through the dense crowd without collision and move to the destination efficiently. As deep reinforcement learning has achieved certain results in human-aware navigation policies, it provides a feasible solution for the robot navigation in dense crowd. But current environment representation method is difficult to represent the intention of human movement, which causes that the policy network cannot make forward-looking decisions. And the previous learning model could not effectively represent any number of pedestrians and maintain stable navigation capability in unfamiliar environment. In this study, we propose a novel model of robot navigation, that is called robot human interaction reinforcement learning (RHIRL). A new environment representation method is proposed which implicitly includes the potential interaction and effectively improves the navigation ability in unfamiliar and dynamic interactive environment. The experiment results show that the proposed model has obvious advantages and excellent navigation performance in dynamic and unfamiliar environment.","","978-1-6654-0535-5","10.1109/ROBIO54168.2021.9739455","Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China; Research and Development; Graduate Research and Innovation Projects of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739455","","Legged locomotion;Navigation;Heuristic algorithms;Conferences;Biomimetics;Reinforcement learning;Robustness","human-robot interaction;learning (artificial intelligence);mobile robots;navigation;path planning","dense crowd flow;human-aware navigation policies;robot navigation;current environment representation method;human movement;previous learning model;stable navigation capability;unfamiliar environment;robot human interaction reinforcement learning;navigation ability;dynamic interactive environment;excellent navigation performance;interaction-based deep reinforcement learning","","1","","24","IEEE","28 Mar 2022","","","IEEE","IEEE Conferences"
"RLSegNet: An Medical Image Segmentation Network Based on Reinforcement Learning","Y. Ding; X. Qin; M. Zhang; J. Geng; D. Chen; F. Deng; C. Song","Network and Data Security Key Laboratory of Sichuan Province, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON, Canada; Network and Data Security Key Laboratory of Sichuan Province, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; Network and Data Security Key Laboratory of Sichuan Province, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; Network and Data Security Key Laboratory of Sichuan Province, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Network and Data Security Key Laboratory of Sichuan Province, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China","IEEE/ACM Transactions on Computational Biology and Bioinformatics","9 Aug 2023","2023","20","4","2565","2576","In the area of medical image segmentation, the spatial information can be further used to enhance the image segmentation performance. And the 3D convolution is mainly used to better utilize the spatial information. However, how to better utilize the spatial information in the 2D convolution is still a challenging task. In this paper, we propose an image segmentation network based on reinforcement learning (RLSegNet), which can translate the image segmentation process into a serial of decision-making problem. The proposed RLSegNet is a U-shaped network, which is composed of three components: the feature extraction network, the Mask Prediction Network (MPNet), and the up-sampling network with the cascade attention module. The deep semantic feature in the image is first extracted by adopting the feature extraction network. Then, the Mask Prediction Network (MPNet) is proposed to generate the prediction mask for the current frame based on the prior knowledge (segmentation result). And the proposed cascade attention module is mainly used to generate the weighted feature mask so that the up-sampling network pays more attention to the interesting region. Specifically, the state, action and reward used in the reinforcement learning are redesigned in the proposed RLSegNet to translate the segmentation process as the decision-making process, which performs as the reinforcement learning to realize the brain tumor segmentation. Extensive experiments are conducted on the BRATS 2015 dataset to evaluate the proposed RLSegNet. The experimental results demonstrate that the proposed method can achieve a better segmentation performance, in comparison with other state-of-the-art methods.","1557-9964","","10.1109/TCBB.2022.3195705","National Natural Science Foundation of China(grant numbers:62076054,62072074,62027827,61902054,62002047); Sichuan Science and Technology Innovation Platform and Talent Plan(grant numbers:2022JDJQ0039,2020JDJQ0020); Sichuan Science and Technology Support Plan(grant numbers:2020YFSY0010,2022YFQ0045,2022YFS0220,2019YJ0636,2021YFG0131); The Verification Platform of Multi-tier Coverage Communication Network for oceans(grant numbers:LZC0020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9847069","Deep learning;reinforcement learning;deep convolutional neural networks;image segmentation;semantic segmentation","Image segmentation;Tumors;Reinforcement learning;Convolution;Three-dimensional displays;Task analysis;Magnetic resonance imaging","biomedical MRI;brain;feature extraction;image classification;image segmentation;medical image processing;reinforcement learning;tumours","brain tumor segmentation;BRATS 2015 dataset;cascade attention module;decision-making problem;decision-making process;deep semantic feature;feature extraction network;image segmentation performance;mask prediction network;medical image segmentation Network;MPNet;reinforcement learning;RLSegNet;U-shaped network;up-sampling network;weighted feature mask","","1","","36","IEEE","1 Aug 2022","","","IEEE","IEEE Journals"
"Multi Agent Safe Graph Reinforcement Learning for PV Inverter s Based Real-Time De centralized Volt/Var Control in Zoned Distribution Networks","R. Yan; Q. Xing; Y. Xu","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore; School of Automation and School of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore","IEEE Transactions on Smart Grid","","2023","PP","99","1","1","To realize real-time voltage/var control (VVC) in active distribution networks (ADNs), this paper proposes a new multi-agent safe graph reinforcement learning method to optimize reactive power output from PV inverters. The network is divided into several zones, and a decentralized framework is proposed for coordinated control of reactive power output in each zone to regulate voltage profiles and minimize network energy loss. The VVC problem is formulated as a multi-agent decentralized partially observable constrained Markov decision process. Each zone has a central control agent that embeds graph convolution networks (GCNs) in the policy network to improve the decision-making capability. The GCN extracts graph-structured features from the ADN topology, reflecting the relationship between VVC and grid topology, and can filter noise and impute missing data. The training process includes primal-dual policy optimization to rigorously satisfy voltage safety constraints. Simulations on a 141-bus distribution system demonstrate that the proposed method can effectively minimize network energy loss and reduce voltage deviations, even in the presence of noisy or incomplete input measurements.","1949-3061","","10.1109/TSG.2023.3277087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10128717","Active distribution networks;voltage/var control;multi-agent reinforcement learning;graph learning","Inverters;Distribution networks;Reactive power;Reinforcement learning;Optimization;Real-time systems;Energy loss","","","","1","","","IEEE","17 May 2023","","","IEEE","IEEE Early Access Articles"
"Improving Reinforcement FALCON Learning in Complex Environment with Much Delayed Evaluation via Memetic Automaton","G. Zhang; L. Feng; Y. Xie; Z. Wu; L. Chen","College of Computer Science, Chongqing University, China; College of Computer Science, Chongqing University, China; Chongqing University Industrial Technology Research Institute, China; School of Automation, Chongqing University, China; Huawei Cloud","2019 IEEE Congress on Evolutionary Computation (CEC)","8 Aug 2019","2019","","","166","173","The Fusion Architecture for Learning, COgnition, and Navigation (FALCON) is an extension of the self-organizing neural network i.e., Adaptive Resonance Theory (ART), which has been successfully applied in many reinforcement learning tasks, and demonstrated fast and stable real-time learning capabilities. However, the learning of reinforcement FALCON relies on the positive feedbacks obtained from the environment, which may not be always available in many real-world applications. Although TD-FALCON has been proposed in the literature, to integrate the temporal difference method to estimate the payoff value when immediate reward is not available, the accuracy of the estimation also relies on the received feedback from environment. In complex environments with much delayed evaluation, the reinforcement FALCON may be hard to learn the proper knowledge to adapt in the given task quickly. To the best of our knowledge, there is no existing work has been conducted to improve the reinforcement FALCON learning in such environment. Taking this cue, inspired from the science of memetics, in this paper, we propose to improve the reinforcement FALCON learning in complex environment where positive reward is hard to achieve, via memetic automaton. In particular, by defining the particular representation of memes in the context of FALCON, the corresponding designs of meme selection and meme transmission for meme evolution are presented, to transfer the knowledge meme from well-learned agents in simple environment to improve the learning performance of FALCON agents in complex environment. Lastly, simulations of FALCON based multi-agent system using the mine navigation task platform, confirmed the efficacy of the proposed memetic model.","","978-1-7281-2153-6","10.1109/CEC.2019.8789988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789988","","Memetics;Reinforcement learning;Learning automata;Computer architecture;Navigation;Task analysis;Problem-solving","ART neural nets;automata theory;learning (artificial intelligence);multi-agent systems;self-organising feature maps","complex environment;memetic automaton;reinforcement learning tasks;TD-FALCON;FALCON based multiagent system;adaptive resonance theory;reinforcement FALCON learning;self-organizing neural network;fusion architecture for learning cognition and navigation","","1","","21","IEEE","8 Aug 2019","","","IEEE","IEEE Conferences"
"Age of Information optimization with Heterogeneous UAVs based on Deep Reinforcement Learning","L. Shi; X. Zhang; X. Xiang; Y. Zhou; S. Sun","College of Computer Science, South-Central Minzu University, Wuhan, China; College of Computer Science, South-Central Minzu University, Wuhan, China; College of Computer Science, South-Central Minzu University, Wuhan, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China","2022 14th International Conference on Advanced Computational Intelligence (ICACI)","29 Jul 2022","2022","","","239","245","Recent years have witnessed increasingly more Unmanned Aerial Vehicle (UAV) applications for data collection in the Internet of Things (IoT). Due to the limited energy and service capacity, it is very challenging for a single UAV to accomplish the data collection while guaranteeing the information freshness of IoT devices or sensor nodes (SNs). In practice, different types of UAVs may have different energy capabilities. In this paper, we propose a more practical heterogeneous UAV swarm path planning problem for optimizing the information freshness, in which the division and cooperation among UAVs with different energy capacities have been taken into consideration. The freshness, i.e., age of information (AoI) collected from each SN is characterized by the data uploading time and the time elapsed since the UAV leaves this SN. We successfully present a deep reinforcement learning algorithm based on attention mechanism by end-to-end training to optimize the average age under UAVs’ energy constraints. The simulation results show that our algorithm has fast convergence, high optimization capability and reliability, and can solve the heterogeneous UAV swarm cooperative AoI optimization problem effectively.","","978-1-6654-7045-2","10.1109/ICACI55529.2022.9837720","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837720","UAV swarm;heterogeneous network;age of information;path planning","Training;Simulation;Reinforcement learning;Data collection;Autonomous aerial vehicles;Information age;Planning","aircraft control;autonomous aerial vehicles;Internet of Things;learning (artificial intelligence);mobile robots;optimisation;particle swarm optimisation;path planning;remotely operated vehicles","information optimization;heterogeneous uavs;data collection;service capacity;information freshness;IoT devices;different energy capabilities;practical heterogeneous UAV swarm path;different energy capacities;deep reinforcement learning algorithm;UAVs' energy constraints;high optimization capability;reliability","","1","","21","IEEE","29 Jul 2022","","","IEEE","IEEE Conferences"
"Digital-Twin-Assisted Task Assignment in Multi-UAV Systems: A Deep Reinforcement Learning Approach","X. Tang; X. Li; R. Yu; Y. Wu; J. Ye; F. Tang; Q. Chen","School of Information and Communication, Guilin University of Electronic Technology, Guilin, China; School of Information and Communication, Guilin University of Electronic Technology, Guilin, China; School of Automation, Guangdong University of Technology, Guangzhou, China; State Key Laboratory of Internet of Things for Smart City and the Department of Computer and Information Science, University of Macau, Macau, China; Guangxi Key Laboratory of Multimedia Communications and Network Technology, School of Computer, Electronics and Information, Guangxi University, Nanning, China; School of Information and Communication, Guilin University of Electronic Technology, Guilin, China; School of Information and Communication, Guilin University of Electronic Technology, Guilin, China","IEEE Internet of Things Journal","23 Aug 2023","2023","10","17","15362","15375","Most existing multiunmanned aerial vehicle (multi-UAV) systems focus on fly path or energy consumption for task assignment, while little attention has been paid to the dynamic feature of the task, resulting in poor task completion ratio. The machine learning (ML) paradigm provides new methodologies for task assignment. However, ML methods are usually of heavy resource-consumption that cannot be directly applied in the UAV. In this article, a digital-twin (DT)-assisted task assignment approach is proposed to improve the resource-intensive utilization and the efficiency of deep reinforcement learning (DRL) in multi-UAV system. The approach has a three-layer network structure which can dynamically assign tasks based on the task time constraints. Moreover, the approach is divided into two stages of initial task-assignment and task-reassignment. In the first stage, airship divides a task into multiple subtasks according to the shortest distance based on genetic algorithm and assigns them to UAVs. In the second stage, the DT can be leveraged to enable the airships to learn from the features of tasks and to generate the  $Q$ -value of the estimated value network of DRL for UAVs via pretrain of DT. The  $Q$ -value can be directly applied for deep  $Q$ -learning network (DQN) in the UAVs to reduce the training episode. Furthermore, the DQN is adopted to train task-reassignment strategy. Simulation results indicate that the DQN with DT can significantly reduce the training episode, improving 30% of the task completion ratio and 19% of the system energy efficiency compared with that of the baseline methods.","2327-4662","","10.1109/JIOT.2023.3263574","Guangxi Natural Science Foundation of China(grant numbers:2019GXNSFFA245007); National Natural Science Foundation of China(grant numbers:U22A2054,61971148); Guangxi Key Laboratory of Wireless Wideband Communication and Signal Processing(grant numbers:GXKL06200103); National Key Research and Development Program of China(grant numbers:2020YFB1807802); Research Grant of University of Macau(grant numbers:MYRG2020-00107-IOTSC); Guangdong-Macau Joint Laboratory for Advanced and Intelligent Computing(grant numbers:GDST2020B1212030003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10089851","Deep reinforcement learning (DRL);digital twin (DT);multiunmanned aerial vehicle (multi-UAV) system;task assignment","Task analysis;Training;Heuristic algorithms;Internet of Things;Real-time systems;Planning;Energy consumption","autonomous aerial vehicles;deep learning (artificial intelligence);digital twins;genetic algorithms;multi-robot systems;reinforcement learning","deep Q-learning network;deep reinforcement learning;digital-twin-assisted task assignment approach;dynamic feature;fly path;heavy resource-consumption;initial task-assignment;machine learning paradigm;multiUAV system;multiunmanned aerial vehicle;resource-intensive utilization;system energy efficiency;task completion ratio;task time constraints;task-reassignment strategy;UAV","","1","","31","CCBY","31 Mar 2023","","","IEEE","IEEE Journals"
"A Real-time Demand Response Strategy of Home Energy Management by Using Distributed Deep Reinforcement Learning","W. Liu; Y. Wang; F. Jiang; Y. Cheng; J. Rong; C. Wang; J. Peng","School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China","2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)","30 May 2022","2021","","","988","995","Home energy management system (HEMS) autonomously schedules the energy usage of home electricity consuming devices to reduce the electricity cost based on real-time electricity price. However, it is very challenging to design an optimal energy management strategy under the uncertainty of the resident's behavior, outdoor temperature and renewable generation. To solve this problem, a real-time demand response (DR) strategy based on deep reinforcement learning (DRL) is proposed. Firstly, the real-time DR management problem is modeled as a Markov game, and then a home energy management algorithm is proposed based on Multi-Agent Twin Delayed Deep Deterministic Policy Gradient (MATD3). It is worth mentioning that the proposed algorithm builds the thermal dynamics model and takes energy storage system (ESS) into account, which improves the utilization of renewable power generation effectively. The simulation result based on real-world data demonstrates the effectiveness of the proposed algorithm.","","978-1-6654-9457-1","10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00157","National Nature Science Foundation of China(grant numbers:62172448); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780944","home energy management;MATD3;demand response;Markov game","Renewable energy sources;Schedules;Costs;Uncertainty;Heuristic algorithms;Simulation;Reinforcement learning","building management systems;deep learning (artificial intelligence);demand side management;energy storage;Markov processes;power engineering computing;power generation economics;pricing;reinforcement learning","real-time demand response strategy;distributed deep reinforcement learning;home energy management system;energy usage;electricity cost;real-time electricity price;resident behaviour;outdoor temperature;multiagent twin delayed deep deterministic policy gradient;energy storage system;renewable power generation;HEMS;DRL;Markov game;thermal dynamics model;ESS","","1","","27","IEEE","30 May 2022","","","IEEE","IEEE Conferences"
"Perceptual Interaction-Based Path Tracking Control of Autonomous Vehicles Under DoS Attacks: A Reinforcement Learning Approach","Y. Xu; Z. -G. Wu; Y. -J. Pan","School of Automation, Beijing Institute of Technology, Beijing, China; Institute of Intelligence Science and Engineering, Shenzhen Polytechnic, Shenzhens, China; Department of Mechanical Engineering, Dalhousie University, Halifax, NS, Canada","IEEE Transactions on Vehicular Technology","","2023","PP","99","1","11","This paper considers the distributed optimal path tracking control problem of autonomous ground vehicles (AGVs) with matched uncertainties under denial-of-service (DoS) attacks. First, to solve the uncertain system parameters, an inverse reinforcement learning (RL) technique is PROPOSED to seek the optimal controller instead of directly solving the robust control problem, which can ensure that the designed controller is globally optimal with respect to the proposal performance function. An off-policy RL algorithm is further proposed to learn the optimal control solution online without requiring any knowledge of the vehicle dynamics. Then, to lower the communication load among vehicles, a perception-based optimal resilient controller and its corresponding perception-based static deadband condition are developed, respectively. Moreover, the perception-based dynamic deadband control mechanism is further exploited to consider the secure path tracking control problem. The proposed algorithms verify that the RL-based optimal path tracking control can be solved, and the occurrence of the Zeno phenomenon can be avoided. Finally, a numerical simulation is demonstrated to verify the effectiveness of the proposed approach.","1939-9359","","10.1109/TVT.2023.3287272","National Natural Science Foundation of China(grant numbers:62103047,U1966202); Beijing Institute of Technology Research; Young Elite Scientists Sponsorship Program by BAST(grant numbers:BYESS2023365); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155261","Autonomous ground vehicles (AGVs);Robust control;Path tracking control;Reinforcement learning (RL);DoS attacks","Vehicle dynamics;Prediction algorithms;Wheels;Transmission line matrix methods;Bandwidth;Vehicular ad hoc networks;Trajectory tracking","","","","1","","","IEEE","19 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Combining Hindsight with Goal-enhanced Prediction for Multi-goal Reinforcement Learning","R. Yang; F. Luo; X. Li","Department of Automation, Tsinghua University, China; Shenzhen International Graduate School, Tsinghua University, China; Shenzhen International Graduate School, Tsinghua University, China","2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)","21 Dec 2021","2021","","","314","321","In multi-goal reinforcement learning (RL), efficient learning from sparse rewards remains a major challenge. One of the most successful solutions to the challenge is Hindsight Experience Replay (HER), a model-free algorithm that relabels desired goals with achieved goals. However, HER and its previous variants are still limited in efficiency and require millions of samples for training. In this paper, leveraging the power of a learned dynamics model, we propose Hindsight Experience Replay with Model-based Prediction (HERO) to further improve the sample efficiency of HER. The core technique of HERO is a two-stage value estimation algorithm combining hindsight relabeled rewards and model-based predictive rewards. To effectively model complex dynamics of robot manipulation tasks, we introduce the goal-enhanced predictive model (GPM) and the achieved-goal variance prioritization (AVP). GPM pays more attention to predicting the achieved goal in the next state, while AVP prioritizes trajectories based on the variance of achieved goals in each trajectory. In our experiments, we evaluate HERO on a set of challenging robot manipulation tasks. Empirical results demonstrate that HERO achieves significantly higher sample efficiency than previous multi-goal RL algorithms.","2375-0197","978-1-6654-0898-1","10.1109/ICTAI52525.2021.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9643318","Deep reinforcement learning;multi-goal reinforcement learning","Training;Heuristic algorithms;Estimation;Reinforcement learning;Predictive models;Prediction algorithms;Trajectory","manipulators;reinforcement learning","achieved-goal variance prioritization;HERO;robot manipulation tasks;sample efficiency;multigoal RL algorithms;multigoal reinforcement learning;sparse rewards;hindsight experience replay;model-free algorithm;learned dynamics model;model-based prediction;two-stage value estimation algorithm;hindsight relabeled rewards;model-based predictive rewards;goal-enhanced predictive model;goal-enhanced prediction;GPM;AVP","","1","","30","IEEE","21 Dec 2021","","","IEEE","IEEE Conferences"
"URNAI: A Multi-Game Toolkit for Experimenting Deep Reinforcement Learning Algorithms","M. A. S. Araùjo; L. P. C. Alves; C. A. G. Madeira; M. M. Nóbrega","Metropolis Digital Institute, Federal University of Rio Grande do Norte, Natal, Brazil; Automation and Computing Department, Federal University of Rio Grande do Norte, Natal, Brazil; Metropolis Digital Institute, Federal University of Rio Grande do Norte, Natal, Brazil; Metropolis Digital Institute, Federal University of Rio Grande do Norte, Natal, Brazil","2020 19th Brazilian Symposium on Computer Games and Digital Entertainment (SBGames)","25 Dec 2020","2020","","","178","187","In the last decade, several game environments have been popularized as testbeds for experimenting reinforcement learning algorithms, an area of research that has shown great potential for artificial intelligence based solutions. These game environments range from the simplest ones like CartPole to the most complex ones such as StarCraft II. However, in order to experiment an algorithm in each of these environments, researchers need to prepare all the settings for each one, a task that is very time consuming since it entails integrating the game environment to their software and treating the game environment variables. So, this paper introduces URNAI, a new multi-game toolkit that enables researchers to easily experiment with deep reinforcement learning algorithms in several game environments. To do this, URNAI implements layers that integrate existing reinforcement learning libraries and existing game environments, simplifying the setup and management of several reinforcement learning components, such as algorithms, state spaces, action spaces, reward functions, and so on. Moreover, URNAI provides a framework prepared for GPU supercomputing, which allows much faster experiment cycles. The first toolkit results are very promising.","2159-6662","978-1-7281-8432-6","10.1109/SBGames51465.2020.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9291647","game environment;toolkit;deep reinforcement learning;experimentation setup","Games;Libraries;Training;Artificial intelligence;Training data;Reinforcement learning;Computer architecture","artificial intelligence;computer games;learning (artificial intelligence);learning systems","URNAI;multigame toolkit;deep reinforcement learning algorithms;game environment variables;existing reinforcement learning libraries;existing game environments;reinforcement learning components","","","","22","IEEE","25 Dec 2020","","","IEEE","IEEE Conferences"
"Credit-of-Q-value for Multi-Agent Reinforcement Learning","S. Li; X. Li; J. Cui","The Department of Automation, Tsinghua University, Beijing, P. R. China; Tsinghua Shenzhen International Graduate School, Shenzhen, P. R. China; Peng Cheng Laboratory, Shenzhen, P. R. China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","6982","6988","Recently, multi-agent reinforcement learning algorithms based on value function factorization have emerged to address the collaborative settings in multi-agent systems. These methods usually fit a joint Q-value function Qtot, which is the ground truth of the whole system and used for centralized training. Qtot can be decomposed into a set of local performance functions Qis, which have no specific meaning and are used as a reference for agents to take actions in a distributed manner. Inspired by the Age of Information concept and multi-scale continuous learning process in Biological Reinforcement Learning, this paper proposes an innovative concept Credit-of-Q-value (CoQ), which means how confident we can take actions based on the current Q-value function. Based on this concept, we propose a corresponding algorithm. We evaluate our method on a challenging set of StarCraft II tasks, and the results show that CoQ significantly improves state-of-the-art value-based multi-agent reinforcement learning methods.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902237","MARL;CTDE;CoQ","Training;Costs;Collaboration;Reinforcement learning;Information age;Biology;Stability analysis","multi-agent systems;reinforcement learning","age of information;biological reinforcement learning;centralized training;collaborative settings;CoQ;credit-of-Q-value;ground truth;local performance functions;multiagent reinforcement learning;multiagent systems;multiscale continuous learning;Q-value function;Qis;Qtot;StarCraft II tasks;value function factorization;value-based multiagent reinforcement learning","","","","30","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Opinion Dynamics in Gossiper-Media Networks Based on Multiagent Reinforcement Learning","C. Zhang; D. Fang; S. Sen; X. Li; Z. Feng; W. Xue; D. An; X. Zhao; R. Chen","College of Information Science and Technology, Dalian Maritime University, Dalian, China; College of Information Science and Technology, Dalian Maritime University, Dalian, China; University of Tulsa, Tulsa, OK, USA; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China; Department of Automation Science and Technology, Xi'an Jiaotong University, Xi'an, China; College of Information Science and Technology, Dalian Maritime University, Dalian, China; College of Information Science and Technology, Dalian Maritime University, Dalian, China","IEEE Transactions on Network Science and Engineering","23 Feb 2023","2023","10","2","1143","1156","In social networks, to increase the number of followers, media express their opinions through posts, articles, and published content to cater to public preferences. Meanwhile, the evolution of public opinions is affected by both the media and peers. In this work, we investigate how the interactions between media affect the dynamics of public opinions in social networks. We propose a reinforcement learning framework to model the interactions between the public (gossipers) and media agents. We model each gossiper as an individually rational agent, which updates its opinion using the bounded confidence model (BCM). Media agents are interested in maximizing the number of following gossipers competitively, and an algorithm, i.e., WoLS-CALA, is proposed to achieve that goal. We analyze and experimentally verify that WoLS-CALA can learn Nash equilibria (NE) for games with continuous action space. In addition, the opinion dynamics of gossipers are theoretically analyzed, which shows that the existence of media will strengthen the consistency of gossipers' opinions. We then evaluate the framework in two synthetic networks, i.e., a fully connected network and a small-world network, and one real data network from Facebook. Extensive empirical simulation reveals that our framework facilitates the consensus of opinions and confirms the theoretical analysis.","2327-4697","","10.1109/TNSE.2022.3229770","National Natural Science Foundation of China(grant numbers:61906027,61906135); China Postdoctoral Science Foundation(grant numbers:2019M661080); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9996138","Analysis of agent-based simulations;multiagent reinforcement learning;social opinion dynamics","Media;Behavioral sciences;Social networking (online);Analytical models;Reinforcement learning;Nash equilibrium;Mathematical models","game theory;learning (artificial intelligence);multi-agent systems;reinforcement learning;social networking (online)","bounded confidence model;data network;fully connected network;gossiper-media networks;gossipers;individually rational agent;media agents;multiagent reinforcement learning;opinion dynamics;public opinions;public preferences;reinforcement learning framework;small-world network;social networks;WoLS-CALA","","","","44","IEEE","21 Dec 2022","","","IEEE","IEEE Journals"
"Dynamic Policy Programming with Descending Regularization for Efficient Reinforcement Learning Control","R. Li; Z. Shang; C. Zheng; H. Li; Q. Liang; Y. Cui","School of Software Engineering, University of Science and Technology of China, Hefei, China; Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Sciences, Shenzhen, China; Department of Automation, University of Science and Technology of China, Hefei, China; Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Sciences, Shenzhen, China","2022 5th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)","4 Oct 2022","2022","","","6","11","In this work, a novel value function-based reinforcement learning (RL) approach, descending dynamic policy programming (DDPP) is proposed to address the issues of sample-efficiency and learning stability in control problems. Extended from the state-of-the-art Kullback-Leibler divergence regularized RL method dynamic policy programming (DPP) that punishes the over-large policy update during learning, DDPP employs a descending strategy of the parameters to dynamically control the penalty term. Evaluated by several benchmark control tasks in OpenAI gym, the proposed method successfully demonstrates its superiorities in both learning stability and sample-efficiency compared with the related baseline approaches and therefore indicates a great potential of the descending strategy of Kullback-Leibler divergence regularization towards more practical implementations using RL.","","978-1-6654-9916-3","10.1109/PRAI55851.2022.9904283","National Natural Science Foundation of China; National Key Research and Development Program of China; Technology Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9904283","reinforcement learning;sample-efficiency;stable learning;descending regularization","Costs;Reinforcement learning;Programming;Benchmark testing;Dynamic programming;Temperature control;Pattern recognition","dynamic programming;reinforcement learning","reinforcement learning control;value function-based reinforcement learning approach;DDPP;learning stability;Kullback-Leibler divergence;RL method dynamic policy programming;policy update;descending strategy;Kullback-Leibler divergence regularization;OpenAI gym","","","","29","IEEE","4 Oct 2022","","","IEEE","IEEE Conferences"
"A Real-World Reinforcement Learning Framework for Safe and Human-Like Tactical Decision-Making","M. U. Yavas; T. Kumbasar; N. K. Ure","Department of Mechatronics Engineering, Istanbul Technical University, Istanbul, Turkey; Department of Control and Automation Engineering, Istanbul Technical University, Istanbul, Turkey; Artificial Intelligence and Data Science Research Center, Istanbul Technical University, Istanbul, Turkey","IEEE Transactions on Intelligent Transportation Systems","","2023","PP","99","1","12","Lane-change decision-making for vehicles is a challenging task for many reasons, including traffic rules, safety, and the stochastic nature of driving. Because of its success in solving complex problems, deep reinforcement learning (DRL) has been suggested for addressing these issues. However, the studies on DRL to date have gone no further than validation in simulation and failed to address what are arguably the most critical issues, namely, the mismatch between simulation and reality, human-likeness, and safety. This paper introduces a real-world DRL framework for decision-making to design safe and human-like agents that can operate in the real world without extra tuning. We propose a new learning paradigm for DRL integrated with Real2Sim transfer, which comprises training, validation, and testing phases. The approach involves two simulator environments with different levels of fidelity, which are parameterized via real-world data. Within the framework, a large amount of randomized experience is generated with a low-fidelity simulator, whereupon the learned skills are validated regularly in a high-fidelity simulator to avoid overfitting. Finally, in the testing phase, the agent is examined concerning safety and human-like decision-making. Extensive simulation and real-world evaluations show the superiority of the proposed approach. To the best of the authors’ knowledge, this is the first application of DRL lane-changing policy in the real world.","1558-0016","","10.1109/TITS.2023.3292981","Turkish Academy of Sciences of Turkey (TÚBA) through TÚBA Outstanding Young Scientist Award Program (GEBÍP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10194473","Autonomous vehicles;reinforcement learning;artificial intelligence;intelligent vehicles","Safety;Training;Decision making;Testing;Behavioral sciences;Task analysis;Measurement","","","","","","","IEEE","25 Jul 2023","","","IEEE","IEEE Early Access Articles"
"Deep Reinforcement Learning-based Edge Caching and Multi-link Cooperative Communication in Internet-of-Vehicles","T. Ma; X. Chen; L. Jiao; Y. Chen","School of Automation, Beijing Information Science & Technology University, Beijing, China; School of Computer, Beijing Information Science & Technology University, Beijing, China; School of Computer, Beijing Information Science & Technology University, Beijing, China; School of Computer, Beijing Information Science & Technology University, Beijing, China","2021 17th International Conference on Mobility, Sensing and Networking (MSN)","13 Apr 2022","2021","","","567","574","With the rapid development of 5G technologies, Internet-of-Vehicles (IoV) has become a promising and important research hotspot. The high-speed mobility of vehicles brings great challenges for services with low delay and high stability requirements. To address these challenges, this paper takes the relative movement between vehicles into account and analyzes the mobility in detail based on probability distribution. We propose a proactive caching and multi-link cooperative communication scheme to cope with mobility. According to the driving and content request information of vehicle users, the requested content is cached in the road side units (RSUs) and neighboring vehicles in advance. Furthermore, the optimal bandwidth is allocated for each communication link in order to improve the stability of vehicle communication and data transmission efficiency. We propose a Deep Reinforcement Learning-based Proactive Caching and Bandwidth Allocation Algorithm (DPCBA) by considering the high-dimensional continuity of the state and action space. The extensive simulation results demonstrate that our DPCBA scheme can effectively improve the Quality-of-Experience (QoE) of vehicle users in various situations, and outperforms traditional benchmark algorithms.","","978-1-6654-0668-0","10.1109/MSN53354.2021.00088","National Natural Science Foundation of China; Beijing Municipal Education Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751565","Internet-of-Vehicles;Proactive Caching;Bandwidth Allocation;Cooperative Communication;Deep Reinforcement Learning.","Space vehicles;Cooperative communication;Simulation;Roads;Channel allocation;Stability analysis;Probability distribution","bandwidth allocation;cache storage;cooperative communication;deep learning (artificial intelligence);mobile computing;probability;quality of experience;reinforcement learning;vehicular ad hoc networks","vehicle users;deep reinforcement learning-based edge caching;multilink cooperative communication;Internet-of-Vehicles;high-speed mobility;high stability requirements;relative movement;communication scheme;content request information;requested content;communication link;vehicle communication;data transmission efficiency;Deep Reinforcement Learning-based Proactive Caching;Bandwidth Allocation Algorithm;high-dimensional continuity;quality-of-experience","","","","16","IEEE","13 Apr 2022","","","IEEE","IEEE Conferences"
"Optimal Drug Dosage Control Strategy of Immune Systems Using Reinforcement Learning","L. Chen; Y. -W. Zhang; S. -C. Zhang","Scientific Research Center, The Seventh Affiliated Hospital, Sun Yat-sen University, Shenzhen, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Internet Finance and Information Engineering, Guangdong University of Finance, Guangzhou, China","IEEE Access","6 Jan 2023","2023","11","","1269","1279","In this article, a reinforcement learning-based drug dosage control strategy is developed for immune systems with input constraints and dynamic uncertainties to sustain the number of tumor and immune cells in an acceptable level. First of all, the state of the immune system and the desired number of tumor and immune cells are constructed into an augmented state to derive an augmented immune system. By designing a discounted non-quadratic performance index function, the robust tracking control problem of immune systems with uncertainties is transformed into an optimal tracking control problem of nominal immune systems and the drug dosage can be limited within the specified range. Hereafter, a reinforcement learning algorithm and a critic-only structure are adopted to acquire the approximate optimal drug dosage control strategy. Furthermore, theoretical proof reveals that the proposed reinforcement learning-based drug dosage control strategy ensures the number of tumor and immune cells reaches the preset level under limited drug dosages and model uncertainties. Finally, simulation study verifies the availability of the developed drug dosage control strategy in different growth models of tumor cell.","2169-3536","","10.1109/ACCESS.2022.3233567","Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515110022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10004921","Reinforcement learning;immune systems;immunotherapy;drug dosage control;robust control;neural networks","Immune system;Tumors;Drugs;Cancer;Reinforcement learning;Mathematical models;Regulation;Neural networks;Robust control","control engineering computing;drugs;medical computing;medical control systems;optimal control;reinforcement learning;robust control;tracking;tumours","augmented immune system;critic-only structure;immune cells;nominal immune systems;optimal drug dosage control strategy;optimal tracking control problem;reinforcement learning;robust tracking control problem;tumor cell","","","","56","CCBYNCND","2 Jan 2023","","","IEEE","IEEE Journals"
"Historical Information Stability based Reward for Reinforcement Learning in Continuous Integration Testing","T. Cao; Z. Li; R. Zhao; Y. Yang","College of Information Science and Technology Beijing University of Chemical Technology, Beijing, P.R.China; College of Information Science and Technology Beijing University of Chemical Technology, Beijing, P.R.China; College of Information Science and Technology Beijing University of Chemical Technology, Beijing, P.R.China; School of Mechanical Engineering and Automation Zhejiang Sci-Tech University, Zhejiang, P.R.China","2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS)","10 Mar 2022","2021","","","231","242","In the continuous integration, test case prioritization can effectively alleviate the resource-intensive problems associated with frequent integration commits. Test case prioritization in continuous integration is a sequential decision problem from which reinforcement learning is applied and can effectively adapt and learn from a changing environment. However, continuous integration testing brings new problems of sparse rewards to reinforcement learning because of frequent integration with low test failure and this problem can be addressed by increasing the number of rewarded test cases. In this paper, we propose a reinforcement learning reward object selection strategy based on Test Case Synchronization and Diversity (TCSD) that rewards failed test cases and with an additional selection of passed test cases with potential failure ability. The experiments on six real-world industrial data sets show that TCSD improves the learning efficiency and fault detection ability of reinforcement learning 6.35% in average NAPFD compared with the traditional strategies.","2693-9177","978-1-6654-5813-9","10.1109/QRS54544.2021.00034","National Natural Science Foundation of China(grant numbers:61872026,61902015,62077003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9724758","continuous integration;test case prioritization;reinforcement learning;reward strategy;test case synchronization;test case diversity","Fault detection;Conferences;Reinforcement learning;Software quality;Software reliability;Synchronization;Security","fault diagnosis;learning (artificial intelligence);program testing","historical information stability;continuous integration testing;test case prioritization;resource-intensive problems;frequent integration commits;sequential decision problem;sparse rewards;low test failure;rewarded test cases;reinforcement learning reward object selection strategy;Test Case Synchronization;failed test cases;passed test cases;learning efficiency","","","","53","IEEE","10 Mar 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Energy Management System for Grid-Connected PV plants and Energy-Stored Quasi-Z-source Cascaded H-Bridge Multilevel Inverter","E. Hosseini; P. García-Triviño; R. Sarrias-Mena; C. G. Vázquez; L. M. Fernández-Ramírez","Department of Electrical Engineering, SURET Research Group, University of Cadiz (UCA), Algeciras, Spain; Department of Electrical Engineering, SURET Research Group, University of Cadiz (UCA), Algeciras, Spain; Dept. Engineering in Automation, Elect. Comp. Arch. & Netw. (UCA), SURET Research Group, Algeciras, Spain; Department of Electrical Engineering, SURET Research Group, University of Cadiz (UCA), Algeciras, Spain; Department of Electrical Engineering, SURET Research Group, University of Cadiz (UCA), Algeciras, Spain","2023 IEEE International Conference on Environment and Electrical Engineering and 2023 IEEE Industrial and Commercial Power Systems Europe (EEEIC / I&CPS Europe)","3 Aug 2023","2023","","","1","6","A multiagent reinforcement learning (RL) algorithm is used in this paper as the energy management system (EMS) of an energy stored quasi-Z-source cascaded H-bridge multilevel inverter with (ES-qZS-CHBMLIs) for grid-connected photovoltaic (PV) cells. The duty of the new EMS is meeting the requested power while sorting or releasing power between the battery energy storages (BES). A photovoltaic plant with 4.8 kW capacity in 3 cascaded modules configuration connected to a single-phase grid is simulated in MATLAB to assess the performance of the suggested RL algorithm. For each module, different battery parameters, variable grid references and varying irradiance values are considered to evaluate the suggested EMS. In order to control DC-link voltage, dq dimensional current components, quality of grid power, and optimal operation of the power sources conventional control methods are applied..","","979-8-3503-4743-2","10.1109/EEEIC/ICPSEurope57605.2023.10194878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10194878","reinforcement learning;battery storage;energy management;PV cells","Photovoltaic systems;Radiation effects;Simulation;Reinforcement learning;Multilevel inverters;Batteries;State of charge","battery powered vehicles;control engineering computing;energy management systems;invertors;maximum power point trackers;photovoltaic power systems;power engineering computing;power generation control;power grids;reinforcement learning;voltage control","battery energy storages;cascaded modules configuration;energy management system;energy-stored quasiZ-source cascaded H-bridge multilevel inverter;ES-qZS-CHBMLIs;grid power;grid-connected photovoltaic cells;grid-connected PV plants;multiagent reinforcement learning algorithm;photovoltaic plant;power 4.8 kW;power sources conventional control methods;RL algorithm;single-phase grid;suggested EMS;variable grid references;varying irradiance values","","","","20","IEEE","3 Aug 2023","","","IEEE","IEEE Conferences"
"Parameter optimization design of MFAC based on Reinforcement Learning","S. Liu; X. Jia; H. Ji; L. Fan","School of Electrical and Control Engineering, North China University of Technology, Beijing, People's Republic of China; School of Electrical and Control Engineering, North China University of Technology, Beijing, People's Republic of China; School of Electrical and Control Engineering, North China University of Technology, Beijing, People's Republic of China; School of Automation, Beijing Information Science and Technology University, Beijing, People's Republic of China","2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)","7 Jul 2023","2023","","","1036","1043","In this paper, we focus on parameter optimization for model-free adaptive control, which is currently lacking. Therefore, a novel data-driven model-free adaptive control method based on Reinforcement Learning (RL-MFAC) is proposed for a class of dis-crete-time single-input and single-output nonlinear systems. In RL-MFAC, a dynamic linearization method is adopted, and the concept of pseudo-partial derivative (PPD) is introduced to design the controller. Moreover, the reward and punishment mechanism of Reinforcement Learning (RL) was used to optimize the parameters of MFAC in the controller design, such that the controller not only has good robustness, but also can solve the problem that MFAC needs to reset the optimal parameters on different control objects. The feature of the RL-MFAC is that the self-learning ability of RL is used to optimize the parameters of MFAC controller, and only the input/output (I/O) measurement data of the system is used. Furthermore, in the Python environment, two different nonlinear systems are used as objects for numerical simulation. The simulation results show that the performance of the MFAC controller is significantly improved in terms of overshoot and vibration prevention during regulation after the parameters are optimized by RL which proves the effectiveness of the proposed method.","2767-9861","979-8-3503-2105-0","10.1109/DDCLS58216.2023.10167283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10167283","Reinforcement Learning;Data-driven Control;Parameter Optimization","Vibrations;Adaptation models;Simulation;Optimization methods;Reinforcement learning;Numerical simulation;Robustness","adaptive control;control system synthesis;nonlinear control systems;Python;reinforcement learning","control objects;controller design;data-driven model-free adaptive control method;discrete-time single-input and single-output nonlinear systems;dynamic linearization method;I/O;input/output measurement data;MFAC controller;nonlinear systems;optimal parameters;parameter optimization design;Python environment;reinforcement learning;RL-MFAC","","","","24","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"A Smart Flight Controller based on Reinforcement Learning for Unmanned Aerial Vehicle (UAV)","F. S. Khan; M. N. H. Mohd; R. M. Larik; M. D. Khan; M. I. Abbasi; S. Bagchi","Faculty of Electrical and Electronic Engineering, Universiti Tun Hussein Onn Malaysia & CONVSYS (Pvt) Ltd., Parit Raja, Malaysia; Faculty of Electrical and Electronic Engineering, Universiti Tun Hussein Onn Malaysia, Parit Raja, Malaysia; Department of Electrical Engineering, NED Univesrity of Engg. & Tech. (NEDUET), Karachi, Pakistan; Dept. of Machine Learning Automation & Innovation, CONVSYS (Pvt) Ltd, Islamabad, Pakistan; Dept. of Electrical and Computer Engg., Universiti Teknikal Melaka Malaysia (UTeM), Melaka, Malaysia; Faculty of Electrical and Electronic Engineering, Universiti Tun Hussein Onn Malaysia, Parit Raja, Malaysia","2021 IEEE International Conference on Signal and Image Processing Applications (ICSIPA)","20 Oct 2021","2021","","","203","208","Traditional flight controllers consist of Proportional Integral Derivates (PID), that although have dominant stability control but required high human interventions. In this study, a smart flight controller is developed for controlling UAVs which produces operator less mechanisms for flight controllers. It uses a neural network that has been trained using reinforcement learning techniques. Engineered with a variety of actuators (pitch, yaw, roll, and speed), the next-generation flight controller is directly trained to control its own decisions in flight. It also optimizes learning algorithms different from the traditional Actor and Critic networks. The agent gets state information from the environment and calculates the reward function depending on the sensors data from the environment. The agent then receives the observations to identify the state and reward functions and the agent activates the algorithm to perform actions. It shows the performance of a trained neural network consisting of a reward function in both simulation and real-time UAV control. Experimental results show that it can respond with relative precision. Using the same framework shows that UAVs can reliably hover in the air, even under adverse initialization conditions with obstacles. Reward functions computed during the flight for 2500, 5000, 7500 and 10000 episodes between the normalized values 0 and −4000. The computation time observed during each episode is 15 micro sec.","2642-6471","978-1-6654-3592-5","10.1109/ICSIPA52582.2021.9576806","Ministry of Education; Universiti Tun Hussein Onn Malaysia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9576806","UAV;Reinforcement Learning;Smart flight controller;Reward Function","Image processing;Conferences;Neural networks;Reinforcement learning;Unmanned aerial vehicles;Stability analysis;Real-time systems","aircraft control;attitude control;autonomous aerial vehicles;control system synthesis;learning (artificial intelligence);mobile robots;neural nets;path planning;reinforcement learning;remotely operated vehicles;stability;three-term control","reinforcement learning techniques;next-generation flight controller;reward function;trained neural network;UAV;smart flight controller;unmanned aerial vehicle;traditional flight controllers;dominant stability control;proportional integral derivates","","","","18","IEEE","20 Oct 2021","","","IEEE","IEEE Conferences"
"Hierarchical Reinforcement Learning Based on Continuous Subgoal Space","C. Wang; F. Zeng; S. S. Ge; X. Jiang","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China","2020 IEEE International Conference on Real-time Computing and Robotics (RCAR)","30 Dec 2020","2020","","","74","80","Multi-agents are designed at different temporal levels to decomposed a complex task into several simple ones in hierarchical reinforcement learning. At the beginning of neural network training, the great changes of the low-level policy would cause the unstable transitions of the high-level one. In this paper, we propose a hierarchical policy combined with PPO and DDPG to deal with the simultaneous training of multi-agents. To create an end-to-end policy, neural networks are employed to extract scene features in both low-level and high-level policies. In the meanwhile, a novel internal reward function is designed to enhance the goal achieving ability of low-level policy. A lightweight and fast gridworld Gym environment, MiniGrid, is used to test its validity. We found that the hierarchical policy is able to explore and plan without dense rewards. This attribute has a considerable influence on the study of robot navigation, especially in large and complex environment.","","978-1-7281-7293-4","10.1109/RCAR49640.2020.9303280","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303280","","Training;Reinforcement learning;Task analysis;Navigation;Neural networks;Markov processes;Robots","feature extraction;learning (artificial intelligence);multi-agent systems","hierarchical reinforcement learning;continuous subgoal space;multiagents;neural network training;low-level policy;end-to-end policy;neural networks;high-level policy;DDPG;PPO;scene feature extraction;mini grid","","","","21","IEEE","30 Dec 2020","","","IEEE","IEEE Conferences"
"Distributed Deep Reinforcement Learning-based Approach for Fast Preventive Control Considering Transient Stability Constraints","H. Zeng; Y. Zhou; Q. Guo; Z. Cai; H. Sun","Department of Electrical Engineering, State Key Laboratory of Power Systems, Tsinghua University, Beijing, China; Department of Electrical Engineering, State Key Laboratory of Power Systems, Tsinghua University, Beijing, China; Department of Electrical Engineering, State Key Laboratory of Power Systems, Tsinghua University, Beijing, China; Department of Automation Science and Technology, Xi'an Jiaotong University, Xi'an, China; Department of Electrical Engineering, State Key Laboratory of Power Systems, Tsinghua University, Beijing, China","CSEE Journal of Power and Energy Systems","10 Feb 2023","2023","9","1","197","208","Preventive transient stability control is an effective measure for the power system to withstand high-probability severe contingencies. It is mathematically an optimal power flow problem with transient stability constraints. Due to the constraints involved for differential algebraic equations of transient stability, it is difficult and time-consuming to solve this problem. To address these issues, this paper presents a novel deep reinforcement learning (DRL) framework for preventive transient stability control of power systems. A distributed deep deterministic policy gradient is utilized to train a DRL agent that can learn its control policy through massive interactions with a grid simulator. Once properly trained, the DRL agent can instantaneously provide effective strategies to adjust the system to a safe operating position with a near-optimal operational cost. The effectiveness of the proposed method is verified through numerical experiments conducted on a New England 39-bus system and NPCC 140-bus system.","2096-0042","","10.17775/CSEEJPES.2020.04610","National Natural Science Foundation of China(grant numbers:U22B2097); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606946","Deep reinforcement learning;preventive Control;transient stability","Power system stability;Transient analysis;Numerical stability;Stability criteria;Training;Generators;Mathematical models","deep learning (artificial intelligence);differential algebraic equations;gradient methods;learning (artificial intelligence);load flow;optimisation;power engineering computing;power system control;power system stability;power system transient stability;reinforcement learning","control policy;deep reinforcement learning framework;distributed deep deterministic policy gradient;distributed deep reinforcement learning-based approach;DRL agent;fast preventive control considering transient stability constraints;high-probability severe contingencies;optimal power flow problem;power system;preventive transient stability control","","","","38","","9 Nov 2021","","","CSEE","CSEE Journals"
"Binocular Vision-Based Motion Planning of An AUV: A Deep Reinforcement Learning Approach","J. Yan; K. You; W. Cao; X. Yang; X. Guan","Institute of Electrical Engineering, Yanshan University, Qinhuangdao, China; Institute of Electrical Engineering, Yanshan University, Qinhuangdao, China; Institute of Electrical Engineering, Yanshan University, Qinhuangdao, China; Institute of Electrical Engineering, Yanshan University, Qinhuangdao, China; School of Automation, Shanghai JiaoTong University, Shanghai, China","IEEE Transactions on Intelligent Vehicles","","2023","PP","99","1","17","Vision-based motion planning of autonomous underwater vehicles (AUVs) is regarded as a critical requirement for marine intelligent transportation systems. However, the limited vision range and the uncertain model parameters of an AUV make it difficult to fulfill this requirement. This study focuses on a binocular-vision-based motion planning issue for an AUV. First, we develop an intelligent AUV system that mainly comprises binocular cameras for patrolling the target, a localization unit for acquiring the position information, and an acoustic modem for communicating with buoys. Accordingly, the parallax angles from the AUV to target are used to construct an optimal motion planning problem. To solve the aforementioned problem, we develop a deep reinforcement learning method called the improved twin delayed deep deterministic (TD3) policy gradient algorithm in order to minimize the reward function, such that the AUV can perpendicularly patrol the target with a fixed distance. The advantages of our solution are as follows: 1) the binocular-vision-based motion planning method can achieve a trade-off between motion stability and observation effectiveness; 2) the improved TD3 algorithm can accelerate the convergence compared to other algorithms, while it can simultaneously overcome the dependency on the model parameters of the AUV. Finally, simulation and experimental studies are conducted to verify the effectiveness.","2379-8904","","10.1109/TIV.2023.3321884","National Natural Science Foundation of China(grant numbers:62222314,61973263,62033011); Youth Talent Program of Hebei(grant numbers:BJ2020031); Distinguished Young Foundation of Hebei Province(grant numbers:F2022203001); Central Guidance Local Foundation of Hebei Province(grant numbers:226Z3201G); Three-Three-Three Foundation of Hebei Province(grant numbers:C20221019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10271703","Autonomous underwater vehicle (AUV);motion planning;binocular vision;deep reinforcement learning","Planning;Cameras;Sonar;Sonar navigation;Intelligent vehicles;Heuristic algorithms;Path planning","","","","","","","IEEE","4 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Addressing Sample Efficiency and Model-bias in Model-based Reinforcement Learning","A. S. Anand; J. Erik Kveen; F. Abu-Dakka; E. I. Grøtli; J. Tommy Gravdahl","Dept. of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Dept. of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Department of Electrical Engineering and Automation (EEA), Intelligent Robotics Group, Aalto University, Aalto, Finland; Dept. of Mathematics and Cybernetics, SINTEF Digital, Trondheim, Norway; Dept. of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, Norway","2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)","23 Mar 2023","2022","","","1","6","Model-based reinforcement learning promises to be an effective way to bring reinforcement learning to real-world robotic systems by offering a sample efficient learning approach compared to model-free reinforcement learning. However, model-based reinforcement learning approaches at present struggle to match the performance of model-free ones. This work attempts to fill this gap by improving the performance of model-based reinforcement learning while further improving its sample efficiency. To improve the sample efficiency, an exploration strategy is formulated which maximizes the information gain. The asymptotic performance is improved by compensating for the model-bias using a model-free critic. We have evaluated our proposed approach on four reinforcement learning benchmarking tasks in the openAI gym framework.","","978-1-6654-6283-9","10.1109/ICMLA55696.2022.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10069984","Model based reinforcement learning;sample efficient learning;model predictive control","Reinforcement learning;Benchmark testing;Data models;Task analysis;Robots","deep learning (artificial intelligence);reinforcement learning","model-based reinforcement learning;model-bias;model-free reinforcement learning;sample efficiency;sample efficient learning","","","","30","IEEE","23 Mar 2023","","","IEEE","IEEE Conferences"
"Adaptive Multi-Agent Deep Mixed Reinforcement Learning for Traffic Light Control","L. Li; R. Zhu; S. Wu; M. Xu; J. Lu","School of Computer and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; School of Computer and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; School of Computer and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; School of Computer and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; Beijing National Research Center for Information Science and Technology (BNRIST), Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Vehicular Technology","","2023","PP","99","1","14","Despite significant advancements in Multi-Agent Deep Reinforcement Learning (MADRL) approaches for Traffic Light Control (TLC), effectively coordinating agents in diverse traffic environments remains a challenge. Studies in MADRL for TLC often focus on repeatedly constructing the same intersection models with sparse experience. However, real road networks comprise Multi-Type of Intersections (MTIs) rather than being limited to intersections with four directions. In the scenario with MTIs, each type of intersection exhibits a distinctive topology structure and phase set, leading to disparities in the spaces of state and action. This paper introduces Adaptive Multi-agent Deep Mixed Reinforcement Learning (AMDMRL) for addressing tasks with multiple types of intersections in TLC. AMDMRL adopts a two-level hierarchy, where high-level proxies guide lowlevel agents in decision-making and updating. All proxies are updated by value decomposition to obtain the globally optimal policy. Moreover, the AMDMRL approach incorporates a mixed cooperative mechanism to enhance cooperation among agents, which adopts a mixed encoder to aggregate the information from correlated agents.We conduct comparative experiments involving four traditional and four DRL-based approaches, utilizing three training and four testing datasets. The results indicate that the AMDMRL approach achieves average reductions of 41% than traditional approaches, and 16% compared to DRL-based approaches in traveling time on three training datasets. During testing, the AMDMRL approach exhibits a 37% improvement in reward compared to the MADRL-based approaches.","1939-9359","","10.1109/TVT.2023.3319698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10269000","Multi-Agent Deep Reinforcement Learning;Traffic Light Control;Multi-Type of Intersections;Value Decomposition","Training;Topology;Reinforcement learning;Decision making;Computational modeling;Adaptation models;Testing","","","","","","","IEEE","2 Oct 2023","","","IEEE","IEEE Early Access Articles"
"UAC: Offline Reinforcement Learning with Uncertain Action Constraint","J. Guan; S. Gu; Z. Li; J. Hou; Y. Yang; G. Chen; C. Jiang","School of Automotive Engineering and Department of Computer Science, Tongji University, Shanghai, China; Department of Informatics, Technical University of Munich, Munich, Germany; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Automotive Engineering and Department of Computer Science, Tongji University, Shanghai, China; Department of Automation and Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; School of Automotive Engineering and Department of Computer Science, Tongji University, Shanghai, China; Department of Computer Science, Tongji University, Shanghai, China","IEEE Transactions on Cognitive and Developmental Systems","","2023","PP","99","1","1","Offline reinforcement learning (RL) algorithms promise to learn policies directly from offline datasets without environmental interaction. This arrangement enables successful RL applications in the real world, particularly in robots and autonomous driving, where sampling is costly and dangerous. However, the existing offline RL algorithms suffer from insufficient performance attributed to extrapolation error caused by out-of-distribution (OOD) actions. In this work, we propose an offline RL algorithm with an uncertain action constraint (UAC). The design principle of UAC is to minimize the extrapolation error via eliminating unknown and uncertain actions. Concretely, we first theoretically analyze the effects of different types of actions on the extrapolation error. Based on this, we propose an action-constrained strategy that exploits the uncertainty of the environmental dynamics model to eliminate unknown and uncertain actions in the Q-value evaluation process. Furthermore, the convex combination of trajectory information and Gaussian noise is novelly leveraged to enhance the generation probability of the optimal actions. Finally, we carry out the comparison and ablation experiments on the standard D4RL dataset. Experimental results indicate that UAC achieves competitive performance, especially in the field of robotic manipulation.","2379-8939","","10.1109/TCDS.2023.3287987","Shanghai Municipal Science and Technology Major Project(grant numbers:2018SHZDZX01); ZJ Lab, and Shanghai Center for Brain Science and Brain- Inspired Technology, and the Shanghai Rising Star Program(grant numbers:21QC1400900); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10158415","Offline reinforcement learning;uncertain actions;out-of-distribution actions","Extrapolation;Trajectory;Uncertainty;Standards;Estimation;Behavioral sciences;Robots","","","","","","","IEEE","20 Jun 2023","","","IEEE","IEEE Early Access Articles"
"RLSF: Multimodal Sleep Improvement Based Reinforcement Learning","N. Che; T. Zhang; Y. Li; F. Yu; H. Wang","Department of Computing Science and Technology, Harbin University of Science and Technology, Harbin, China; Department of Computing Science and Technology, Harbin University of Science and Technology, Harbin, China; Department of Automation, Jiangsu University, Zhenjiang, China; Zhejiang Laboratory, Hangzhou, China; Zhejiang Laboratory, Hangzhou, China","IEEE Access","22 May 2023","2023","11","","47712","47724","As informatization 3.0 accelerates the pace of people’s life and work, people’s happiness index and physical and mental health have become the focus of attention and research in sociology, psychology and medicine. Currently, neurological diseases represented by insomnia have become common chronic diseases. However, existing insomnia treatment methods mainly focus on drug therapy and EEG-based expert intervention, ignoring the individual variability of insomnia patients, the high cost of expert intervention, and the privacy of the user’s treatment environment. Therefore, aiming at the effect of white noise lite on sleep quality, this paper proposes a time-frequency domain correlation multimodal sleep enhancement framework based on reinforcement learning (RLSF), which is a closed-loop feedback sleep improvement framework that includes hardware and software. Specifically, the individual sleep state is fed for learning through EEG sensors’ input, and the agent is gradually trained to suit the sleep habit. This paper provides a reinforcement learning environment in which different agents can be deployed easily; then, we propose the Deep Net Sleep Improvement agent (DNSI agent) and the Time and Frequency-based Lightweight Sleep Improvement agent (TFLSI agent) for RLSF. Finally, the substantial experiments compare DNSI and TFLSI agent performance, and the results indicate that these two agents both have decision-making ability, and three volunteers’ Pittsburgh Sleep Quality Index significantly reduces by 3–7 points within two months and the average time to sleep is reduced by 131.4 seconds. Our code is publicly available at https://github.com/TerryZAG/RLSF. Our self-made dataset is publicly available at https://github.com/TerryZAG/TGAM-datasets-for-RLSF.","2169-3536","","10.1109/ACCESS.2023.3266094","Key Research Project of Zhejiang Laboratory(grant numbers:K2022PD1BB01); Initial Research Fund of Highly Specialized Personnel of Jiangsu University(grant numbers:20JDG13); Natural Science Foundation of the Jiangsu Higher Education Institutions of China(grant numbers:21KJB520003); Doctor Program of Mass Entrepreneurship and Innovation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10098592","Sleep improvement;reinforcement learning;time-frequency domain correlation","Sleep;Electroencephalography;White noise;Brain modeling;Reinforcement learning;Sensors;Hardware","decision making;diseases;drugs;electroencephalography;learning (artificial intelligence);medical disorders;medical signal processing;neurophysiology;sleep;white noise","chronic diseases;closed-loop feedback sleep improvement framework;DNSI agent;drug therapy;EEG sensors;EEG-based expert intervention;Improvement agent;Improvement based reinforcement learning;individual sleep state;individual variability;insomnia patients;insomnia treatment methods;medicine;mental health;neurological diseases;physical health;psychology;reinforcement learning environment;sleep habit;sleep quality;TFLSI agent;time 131.4 s;time-frequency domain correlation multimodal sleep enhancement framework;white noise lite","","","","30","CCBYNCND","10 Apr 2023","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Tracking Control of Unmanned Vehicle with Safety Guarantee","Z. Luo; J. Zhou; G. Wen","Department of Systems Science, Southeast University, Nanjing, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China; Department of Systems Science, Southeast University, Nanjing, China","2022 13th Asian Control Conference (ASCC)","20 Jul 2022","2022","","","1893","1898","It is well known that the development of efficient real-time path following strategy and collision avoidance mechanism is critical to the practical implementation of autonomous driving technique. Within this context, this paper presents a new kind of hybrid control strategy consisting of the robot Stanley's trajectory tracking algorithm [1] and deep reinforcement learning (DRL) technique to achieve the goal of tracking control of unmanned vehicle with safety guarantee. By introducing the DRL technique, the tracking accuracy of the robot Stanley's trajectory tracking algorithm is improved and a safe control algorithm with collision avoidance is obtained. Furthermore, the complexity of the learning algorithm involved in the tracking controller is significantly reduced by using the Stanley's trajectory tracking algorithm, which makes the learning converge fast. Finally, numerical simulations are performed to verify that the proposed tracking algorithm has obviously advantages on tracking accuracy and training efficiency over some existing ones.","2770-8373","978-89-93215-23-6","10.23919/ASCC56756.2022.9828057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9828057","Unmanned vehicle;deep reinforcement learning;tracking control;safety control","Training;Trajectory tracking;Reinforcement learning;Numerical simulation;Real-time systems;Safety;Collision avoidance","collision avoidance;continuous systems;control engineering computing;deep learning (artificial intelligence);discrete systems;mobile robots;reinforcement learning;remotely operated vehicles;trajectory control","deep reinforcement learning;unmanned vehicle;safety guarantee;collision avoidance;autonomous driving;hybrid control strategy;DRL;tracking controller;path following strategy;Stanley trajectory tracking","","","","22","","20 Jul 2022","","","IEEE","IEEE Conferences"
"Incentive-Driven Long-term Optimization for Edge Learning by Hierarchical Reinforcement Mechanism","Y. Liu; L. Wu; Y. Zhan; S. Guo; Z. Hong","Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; School of Automation, Beijing Institute of Technology, Beijing, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China","2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)","4 Oct 2021","2021","","","35","45","Edge Learning is an emerging distributed machine learning in mobile edge network. Limited works have designed mechanisms to incentivize edge nodes to participate in edge learning. However, their mechanisms only consider myopia optimization on resource consumption, which results in the lack of learning algorithm performance guarantee and longterm sustainability. In this paper, we propose Chiron, an incentive-driven long-term mechanism for edge learning based on hierarchical deep reinforcement learning. First, our optimization goal combines learning-algorithms metric (i.e., model accuracy) with system metric (i.e., learning time, and resource consumption), which can improve edge learning quality under a fixed training budget. Second, we present a two-layer H-DRL design with exterior and inner agents to achieve both long-term and short-term optimization for edge learning, respectively. Finally, experiments on three different real-world datasets are conducted to demonstrate the superiority of our proposed approach. In particular, compared with the state-of-the-art methods under the same budget constraint, the final global model accuracy and time efficiency can be increased by 6.5 % and 39 %, respectively. Our implementation is available at https://github.com/Joey61Liuyi/Chiron.","2575-8411","978-1-6654-4513-9","10.1109/ICDCS51616.2021.00013","National Natural Science Foundation of China(grant numbers:61872310); Shenzhen Science and Technology Innovation Commission(grant numbers:R2020A045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9546525","Federated Learning;Incentive Mechanism;Deep Reinforcement Learning;Mobile Edge Computing","Training;Measurement;Machine learning algorithms;System performance;Reinforcement learning;Pricing;Sustainable development","deep learning (artificial intelligence);mobile computing;optimisation","edge learning;hierarchical deep reinforcement learning;short-term optimization;long-term optimization;mobile edge network;incentive-driven long-term mechanism","","","","31","IEEE","4 Oct 2021","","","IEEE","IEEE Conferences"
"Asynchronous Localization for Underwater Acoustic Sensor Networks: A Continuous Control Deep Reinforcement Learning Approach","C. Zhou; M. Liu; S. Zhang; R. Zheng; S. Dong; Z. Liu","College of Electrical Engineering and the National Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; College of Electrical Engineering and the National Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China; College of Electrical Engineering and the National Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China; College of Electrical Engineering and the National Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China; School of Automation, Northwestern Polytechnical University, Xi’an, China","IEEE Internet of Things Journal","","2023","PP","99","1","1","The localization of underwater acoustic sensor networks (UASNs) has emerged as a critical research area in the marine information fusion field. Generally, the convex optimization method is adopted to solve the localization problem. However, this method has limitations in complex underwater environments, since it is difficult to transform the non-convex optimization problem into a convex optimization problem under such conditions. Recently, deep reinforcement learning (DRL) has shown great potential and promise in solving intricate optimization tasks. Motivated by this, we propose to adopt DRL for UASNs localization to improve accuracy and robustness. The key challenge is that existing DRL-based methods require discretization of the environment, which leads to a compromise between search time and localization precision. To address this challenge, we first model the localization problem as a Markov Decision Process (MDP) with continuous state and action spaces and subsequently introduce a continuous control DRL framework to solve the localization problem. Within this framework, we develop three continuous control DRL-based localization estimators to address the localization problem in unsupervised, supervised, and semisupervised scenarios. Comprehensive simulations demonstrate the effectiveness of our approach, as the proposed solutions exhibit several advantageous features compared to traditional methods, such as: 1) compared with convex optimization-based method, the convex relaxation is not required; 2) compared with least squares method, the proposed estimators are capable of converging to a global optimal state; 3) compared with discrete control DRL method, the proposed estimators reduce localization time and enhance localization accuracy significantly.","2327-4662","","10.1109/JIOT.2023.3324392","National Natural Science Foundation of China(grant numbers:62173299,U1909206); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LZ23F030006); Fundamental Research Funds for the Central Universities(grant numbers:xtr072022001); Joint Fund of Ministry of Education for Pre-research of Equipment(grant numbers:8091B022147); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10285371","Localization;underwater acoustic sensor networks;deep reinforcement learning;continuous control","Location awareness;Clocks;Internet of Things;Protocols;Optimization;Wireless sensor networks;Underwater acoustics","","","","","","","IEEE","13 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Allocating defense resources for spatial cyber-physical power systems based on deep reinforcement learning","Z. Dong; M. Tang; M. Tian","School of Automation, Wuhan University of Technology, Wuhan, China; Ordnance NCO Academy, Army Engineering University of PLA, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China","2023 IEEE 6th International Conference on Industrial Cyber-Physical Systems (ICPS)","24 May 2023","2023","","","1","6","Allocating defense resources to specific lines can enhance the resilience of power systems against external damages. Considering the impact of information systems, a defense resource allocation model for cyber-physical power systems (CPPS) is developed with the length of power lines as the defense cost. It is assumed that defense resources can reduce the probability of successful attacks. For this nonlinear programming (NLP) problem, an optimization-seeking method based on the deep $Q$-network (DQN) algorithm is proposed. The model and algorithm are evaluated based on the IEEE-39 bus system. The results show that for small action sets, the method is in general agreement with the results obtained by the optimization solver BONMIN. In addition, the allocation strategies with different scales of resources and action sets are analyzed. These studies can provide ideas for the application of deep reinforcement learning (DRL) in resource allocation for power systems.","2769-3899","979-8-3503-1125-9","10.1109/ICPS58381.2023.10128014","Fundamental Research Funds for the Central Universities(grant numbers:2042021kf0011); National Natural Science Foundation of China(grant numbers:52177109); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10128014","cyber-physical power systems;defense resource allocation;length constraint;deep $Q$-network algorithm","Deep learning;Training;Reinforcement learning;Programming;Power systems;Resource management;Optimization","computer network security;cyber-physical systems;deep learning (artificial intelligence);information systems;integer programming;nonlinear programming;power engineering computing;power system security;probability;reinforcement learning;resource allocation","allocation strategies;deep reinforcement learning;deep$Q$-network algorithm;defense cost;defense resource allocation model;defense resources;IEEE-39 bus system;information systems;power lines;spatial cyber-physical power systems;specific lines","","","","16","IEEE","24 May 2023","","","IEEE","IEEE Conferences"
"Transformer-Based Imitative Reinforcement Learning for Multirobot Path Planning","L. Chen; Y. Wang; Z. Miao; Y. Mo; M. Feng; Z. Zhou; H. Wang","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; School of Computer Science and Technology, Xidian University, Xi'an, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Industrial Informatics","11 Aug 2023","2023","19","10","10233","10243","Multirobot path planning leads multiple robots from start positions to designated goal positions by generating efficient and collision-free paths. Multirobot systems realize coordination solutions and decentralized path planning, which is essential for large-scale systems. The state-of-the-art decentralized methods utilize imitation learning and reinforcement learning methods to teach fully decentralized policies, dramatically improving their performance. However, these methods cannot enable robots to perform tasks efficiently in relatively dense environments without communication between robots. We introduce the transformer structure into policy neural networks for the first time, dramatically enhancing the ability of policy neural networks to extract features that facilitate collaboration between robots. It mainly focuses on improving the performance of policies in relatively dense multirobot environments under conditions where robots do not communicate with each other. Furthermore, a novel imitation reinforcement learning framework is proposed by combining contrastive learning and double deep Q-network to solve the problem of difficulty training policy neural networks after introducing the transformer structure. We present results in the simulation environment and compare the resulting policy against advanced multirobot path-planning methods in terms of success rate. Simulation results show that our policy achieves state-of-the-art performance when there is no communication between robots. Finally, we experimented with a real-world case using a total of three robots in our robotic laboratory.","1941-0050","","10.1109/TII.2023.3240585","National Key Research and Development Program of China(grant numbers:2021YFB1714700); National Natural Science Foundation of China(grant numbers:62273138,62133005,62027810,62103141); Science and Technology Innovation Program of Hunan Province(grant numbers:2021RC3060); Postgraduate Scientific Research Innovation Project of Hunan Province(grant numbers:QL20220082); Natural Science Foundation of Hunan Province(grant numbers:2021JC0004); Natural Science Foundation of Hunan Province(grant numbers:2021JJ20029,2022JJ40095); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10032100","Feature extraction;imitation learning;multirobot path planning (MRPP);reinforcement learning;robot learning;supervision contrastive learning","Robots;Collision avoidance;Robot kinematics;Transformers;Path planning;Feature extraction;Training","deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;multi-robot systems;path planning;reinforcement learning","advanced multirobot path-planning methods;collision-free paths;contrastive learning;decentralized path planning;designated goal positions;difficulty training policy neural networks;double deep Q-network;fully decentralized policies;imitation learning;large-scale systems;multiple robots;multirobot path planning;multirobot systems;novel imitation reinforcement learning framework;reinforcement learning methods;relatively dense environments;relatively dense multirobot environments;resulting policy;robotic laboratory;start positions;state-of-the-art decentralized methods;transformer structure;transformer-based imitative reinforcement","","","","34","IEEE","30 Jan 2023","","","IEEE","IEEE Journals"
"Reinforcement Learning for Mean-Field Linear Quadratic Control with Partial System Information","Y. Lin; Q. Qi","Institute of Complexity Science, College of Automation, Qingdao University, Qingdao, P. R. China; Qingdao Innovation and Development Center, Harbin Engineering University, Qingdao, P. R. China","2023 2nd Conference on Fully Actuated System Theory and Applications (CFASTA)","12 Sep 2023","2023","","","198","203","In this paper, the infinite horizon linear quadratic (LQ) optimal control problem for mean-field systems with partial system information is solved by using the reinforcement learning (RL) approach. Although the introduction of the mean-field terms in system dynamics and the cost function will destroy the adaptiveness of the control law, the optimal stabilization control is derived based on the proposed online RL algorithm and the Bellman dynamic programming. Moreover, the proposed algorithm requires only the local state data of the mean-field system to compute the optimal control, and the stepwise stability of the stabilizers is shown. Finally, numerical example is given to illustrate the effectiveness of the proposed algorithm.","","979-8-3503-3216-2","10.1109/CFASTA57821.2023.10243211","National Natural Science Foundation of China; Natural Science Foundation of Shandong Province; China Postdoctoral Science Foundation; Natural Science Foundation of Shandong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10243211","Reinforcement learning;mean-field system;partial system information","System dynamics;Heuristic algorithms;Reinforcement learning;Cost function;Dynamic programming;Trajectory;Infinite horizon","dynamic programming;infinite horizon;linear quadratic control;optimal control;reinforcement learning;stability","control law;infinite horizon linear quadratic optimal control problem;mean-field linear quadratic control;mean-field system;mean-field terms;online RL algorithm;optimal stabilization control;partial system information;reinforcement learning approach;system dynamics","","","","22","IEEE","12 Sep 2023","","","IEEE","IEEE Conferences"
"Safety-based Reinforcement Learning Longitudinal Decision for Autonomous Driving in Crosswalk Scenarios","F. Xiong; D. Ren; M. Fan; S. Ding; Z. Liu","Meituan, Beijing, China; Meituan, Beijing, China; Wenzhou University, Zhejiang, China; Meituan, Beijing, China; Institute of Automation Chinese Academy of Sciences, Beijing, China","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","01","08","Autonomous vehicles (AVs) need to make driving decisions to interact with other traffic participants. By adapting to different scenarios with specific parameters, traditional strategies attempt to leverage rule-based methods to solve the decision problems. In this paper, we present a novel reinforcement learning method for resolving interaction uncertainty in the decision-making problem. We construct prior knowledge by introducing traffic regulations and constraints and then converting them into rules that govern the learning of driving policies. To promote safe driving, a safety-aware module equipped with a mathematical collision correlation analysis is developed to anticipate and handle dangerous traffic scenarios. A realistic scenario involving an AV approaching a crosswalk is used to validate the proposed method. The experimental results indicate that the proposed method improves driving safety and efficiency significantly when compared to alternative approaches and can be generalized to more difficult scenarios.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892236","Beijing Nova Program(grant numbers:Z201100006820046); National Natural Science Foundation of China(grant numbers:61772373); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892236","Autonomous Driving;Reinforcement Learning;Decision Making","Uncertainty;Correlation;Heuristic algorithms;Decision making;Neural networks;Reinforcement learning;Regulation","decision making;learning (artificial intelligence);road safety;road traffic;road vehicles;traffic engineering computing","driving decisions;traffic participants;traditional strategies attempt;leverage rule-based methods;decision problems;reinforcement learning method;interaction uncertainty;decision-making problem;traffic regulations;driving policies;safe driving;safety-aware module;mathematical collision correlation analysis;dangerous traffic scenarios;realistic scenario;AV;difficult scenarios;safety-based reinforcement learning longitudinal decision;autonomous driving;crosswalk scenarios;autonomous vehicles","","","","27","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Evolutionary Multitasking via Reinforcement Learning","S. Li; W. Gong; L. Wang; Q. Gu","School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; Department of Automation, Tsinghua University, Beijing, China; School of Computer Engineering, Hubei University of Arts and Science, Xiangyang, China","IEEE Transactions on Emerging Topics in Computational Intelligence","","2023","PP","99","1","14","Different from traditional evolutionary algorithms (EAs), the multifactorial evolutionary algorithm (MFEA) is proposed to optimize multiple optimization tasks concurrently. Through the knowledge transfer between different tasks, MFEA has been proved to be superior to single-task EAs in the solution quality and convergence speed. Recently, various MFEAs have been developed. Most of them are based on a common model in MFEA, where a fixed knowledge transfer parameter, i.e., random mating probability ($rmp$), is used. In addition, a single evolutionary search operator is employed in the whole evolutionary process. However, in this model, the fixed $rmp$ is difficult to adapt to multiple different tasks. Besides, a single evolutionary search operator may not be suitable for problems with different properties, thus limiting the performance of the algorithm. Based on these considerations, in this article, a reinforcement learning based multifactorial evolutionary algorithm (RLMFEA) is presented. In RLMFEA, it allows different evolutionary search operators to be embedded in MFEA, and each task has a changing $rmp$ that is adaptively adjusted by reinforcement learning. The effectiveness of RLMFEA has been verified on a series of single-objective multitask optimization benchmark functions and a real-world application.","2471-285X","","10.1109/TETCI.2023.3281876","National Natural Science Foundation of China(grant numbers:62076225,62273193); Natural Science Foundation for Distinguished Young Scholars of Hubei(grant numbers:2019CFA081); State Scholarship Fund of China; Major Research Development Program of Hubei Province(grant numbers:2022BAD084); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10144924","Multifactorial evolutionary;multitask optimization;reinforcement learning;knowledge transfer","Task analysis;Reinforcement learning;Optimization;Knowledge transfer;Evolutionary computation;Search problems;Multitasking","","","","","","","IEEE","6 Jun 2023","","","IEEE","IEEE Early Access Articles"
"End-To-End Deep Reinforcement Learning for First-Person Pedestrian Visual Navigation in Urban Environments","H. Xue; R. Song; J. Petzold; B. Hein; H. Hamann; E. Rueckert","Institute for Robotics and Cognitive Systems, University of Luebeck, Luebeck; Institute for Robotics and Cognitive Systems, University of Luebeck, Luebeck; Institute of Computer Engineering, University of Luebeck, Luebeck; Institute of Automation Technology, Helmut Schmidt University, Ham-burg; Institute of Computer Engineering, University of Luebeck, Luebeck; Chair of Cyber-Physical-Systems, Montanuniversität Leoben, Leoben","2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids)","5 Jan 2023","2022","","","350","357","We solve a pedestrian visual navigation problem with a first-person view in an urban setting via deep reinforcement learning in an end-to-end manner. The major challenges lie in severe partial observability and sparse positive experiences of reaching the goal. To address partial observability, we propose a novel 3D-temporal convolutional network to encode sequential historical visual observations, its effectiveness is verified by comparing to a commonly-used Frame-Stacking approach. For sparse positive samples, we propose an improved automatic curriculum learning algorithm NavACL+, which proposes meaningful curricula starting from easy tasks and gradually generalizing to challenging ones. NavACL+ is shown to facilitate the learning process with 21% earlier convergence, to improve the task success rate on difficult tasks by 40% compared to the original NavACL algorithm [1] and to offer enhanced generalization to different initial poses compared to training from a fixed initial pose.","2164-0580","979-8-3503-0979-9","10.1109/Humanoids53995.2022.10000201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10000201","","Training;Deep learning;Visualization;Three-dimensional displays;Navigation;Urban areas;Humanoid robots","convolutional neural nets;deep learning (artificial intelligence);learning (artificial intelligence);pedestrians;reinforcement learning","challenging ones;deep reinforcement learning;End-To-End Deep Reinforcement;first-person pedestrian visual navigation;first-person view;Frame-Stacking approach;improved automatic curriculum learning algorithm;learning process;novel 3D-temporal convolutional network;original NavACL algorithm;pedestrian visual navigation problem;sequential historical visual observations;severe partial observability;sparse positive experiences;sparse positive samples;urban environments;urban setting","","","","32","IEEE","5 Jan 2023","","","IEEE","IEEE Conferences"
"Off-Policy Risk-Sensitive Reinforcement Learning-Based Constrained Robust Optimal Control","C. Li; Q. Liu; Z. Zhou; M. Buss; F. Liu","Chair of Automatic Control Engineering, Technical University of Munich, Munich, Germany; Department of Automation, University of Science and Technology of China, Hefei, China; Chair of Automatic Control Engineering, Technical University of Munich, Munich, Germany; Chair of Automatic Control Engineering, Technical University of Munich, Munich, Germany; Research Center of Intelligent Control and Systems, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","16 Mar 2023","2023","53","4","2478","2491","This article proposes an off-policy risk-sensitive reinforcement learning (RL)-based control framework to jointly optimize the task performance and constraint satisfaction in a disturbed environment. The risk-aware value function, constructed using the pseudo control and risk-sensitive input and state penalty terms, is introduced to convert the original constrained robust stabilization problem into an equivalent unconstrained optimal control problem. Then, an off-policy RL algorithm is developed to learn the approximate solution to the risk-aware value function. During the learning process, the associated approximate optimal control policy is able to satisfy both input and state constraints under disturbances. By replaying experience data to the off-policy weight update law of the critic neural network, the weight convergence is guaranteed. Moreover, online and offline algorithms are developed to serve as principled ways to record informative experience data to achieve a sufficient excitation required for the weight convergence. The proofs of system stability and weight convergence are provided. The Simulation results reveal the validity of the proposed control framework.","2168-2232","","10.1109/TSMC.2022.3213750","Science and Technology Major Project of Anhui Province(grant numbers:202203a06020011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9927372","Adaptive dynamic programming (ADP);input saturation;off-policy risk-sensitive reinforcement learning (RL);robust control;state constraint","Convergence;Optimization;Optimal control;Process control;Robustness;Robust control;Periodic structures","approximation theory;control engineering computing;convergence of numerical methods;neurocontrollers;optimal control;reinforcement learning;robust control","associated approximate optimal control policy;constraint satisfaction;disturbed environment;equivalent unconstrained optimal control problem;learning process;off-policy risk-sensitive reinforcement learning-based control framework;off-policy RL algorithm;off-policy weight update law;original constrained robust stabilization problem;risk-aware value function;risk-sensitive input;robust optimal control;state constraints;task performance;weight convergence","","","","36","IEEE","25 Oct 2022","","","IEEE","IEEE Journals"
"Long-Term Tracking of Evasive Urban Target Based on Intention Inference and Deep Reinforcement Learning","P. Yan; J. Guo; X. Su; C. Bai","School of Astronautics, Harbin Institute of Technology, Harbin, China; School of Astronautics, Harbin Institute of Technology, Harbin, China; College of Automation, Chongqing University, Chongqing, China; School of Astronautics, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","15","Unmanned aerial vehicles (UAVs) have been widely used in urban target-tracking tasks, where long-term tracking of evasive targets is of great significance for public safety. However, the tracked targets are easily lost due to the evasive behavior of the targets and the unstructured characteristics of the urban environment. To address this issue, this article proposes a hybrid target-tracking approach based on target intention inference and deep reinforcement learning (DRL). First, a target intention inference model based on convolution neural networks (CNNs) is built to infer target intentions by fusing urban environment information and observed target trajectory. Then, the prediction of the target trajectory can be inspired by the inferred target intentions, which can further provide effective guidance to the target search process. In order to fully explore the policy space, the target search policy is developed under a DRL framework, where the search policy is modeled as a deep neural network (DNN) and trained by interacting with the task environment. The simulation results show that the inference of the target intentions can effectively guide the UAV to search for the target and significantly improve the target-tracking performance. Meanwhile, the generalization results indicate that the proposed DRL-based search policy has high robustness to the uncertainty of the target behavior.","2162-2388","","10.1109/TNNLS.2023.3298944","National Natural Science Foundation of China(grant numbers:61973101); Natural Science Foundation of Heilongjiang Province of China(grant numbers:YQ2022F012); Siyuan Alliance Open Ended Fund(grant numbers:YQ2022F012); Young Elite Scientists Sponsorship Program by CAST(grant numbers:2021QNRC001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10214982","Deep reinforcement learning (DRL);evasive target tracking;intention inference;unmanned aerial vehicle (UAV);urban environment","Target tracking;Autonomous aerial vehicles;Trajectory;Behavioral sciences;Predictive models;Urban areas;Search problems","","","","","","","IEEE","11 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Efficient Reinforcement Learning for 3D LiDAR Navigation of Mobile Robot","Y. Zhai; Z. Liu; Y. Miao; H. Wang","China University of Mining and Technology, Jiangsu, P. R. China; MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University, Shanghai, P. R. China; China University of Mining and Technology, Jiangsu, P. R. China; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, P. R. China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","3755","3760","Developing an efficient automatic navigation system for mobile robots is challenging in the strange scenarios where robots can only observe the environment of the surrounding limited area. While other distributed automatic navigation systems exist, they often require extracting semantic information to calculate navigation action, which requires extra modules to provide perceptual information and is not robust. We propose an end-to-end automatic navigation system based on the reinforcement learning technology. In particular, the raw 3D LiDAR data is used to directly map an efficient navigation policy. We design a novel dense reward function to handle the reward sparsity issue and provide a graphical representation method to enable the efficient feature learning from the raw 3D LiDAR data in our navigation system. In addition, an imitation learning based policy initialization is introduced before the subsequent reinforcement learning, which increases the learning efficiency and, in the meantime, still encouraging the robot to explore all the potential states to achieve advanced performance than the imitated experts. Our navigation model is trained in the Webots environment and the experimental results show that our model has efficient and flexible navigation performance in complex environments. More importantly, trained model can be easily extended to unfamiliar environments.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902734","End-to-end Navigation;Mobile robot;Reinforcement Learning;3D LiDAR","Representation learning;Laser radar;Three-dimensional displays;Navigation;Semantics;Reinforcement learning;Feature extraction","learning (artificial intelligence);mobile robots;navigation;optical radar;path planning","3D LiDAR navigation;distributed automatic navigation systems;efficient automatic navigation system;efficient feature;efficient navigation performance;efficient navigation policy;efficient reinforcement;end-to-end automatic navigation system;extra modules;extracting semantic information;flexible navigation performance;graphical representation method;learning efficiency;mobile robot;navigation action;navigation model;novel dense reward function;perceptual information;raw 3D LiDAR data;reinforcement learning technology;reward sparsity issue;strange scenarios;subsequent reinforcement learning;surrounding limited area","","","","24","","11 Oct 2022","","","IEEE","IEEE Conferences"
"End-to-end Multi-Objective Deep Reinforcement Learning for Autonomous Navigation","Q. Shen; G. Xiang; S. Feng; Z. Pan; K. Xu","College of Automation, Guangxi University of Science and Technology; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, ShenZhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, ShenZhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, ShenZhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, ShenZhen, China","2023 IEEE International Conference on Real-time Computing and Robotics (RCAR)","20 Sep 2023","2023","","","979","984","Autonomous navigation with an end-to-end reinforcement learning paradigm for ground vehicles poses significant safety and multi-objective challenges, limiting its practical implementation in real-world scenarios. This research proposes a reinforcement learning approach to address the challenges of end-to-end navigation policy that accounts for dynamic multi-objectives along with safety concerns. An action selection law is designed to ensure smooth sequential actions. The dynamic weighting of multiple objectives enhances adaptivity in policy learning. To improve safety, intensive rewards are used to penalize sparse risky actions. The proposed approach is validated using three deep reinforcement learning frameworks in a 2D world navigation task of pursuing dynamic goals while avoiding obstacles. This research presents a promising solution to achieve a multi-objective end-to-end policy for handling dynamic and complex scenarios.","","979-8-3503-2718-2","10.1109/RCAR58764.2023.10249369","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10249369","","Deep learning;Limiting;Navigation;Reinforcement learning;Real-time systems;Safety;Trajectory","collision avoidance;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;navigation;path planning;reinforcement learning","2D world navigation task;action selection law;autonomous navigation;deep reinforcement learning frameworks;dynamic multiobjectives;dynamic scenarios;dynamic weighting;end-to-end navigation policy;end-to-end reinforcement;ground vehicles;multiobjective challenges;multiobjective deep reinforcement;multiobjective end-to-end policy;multiple objectives enhances adaptivity;policy learning;significant safety;smooth sequential actions;sparse risky actions","","","","23","IEEE","20 Sep 2023","","","IEEE","IEEE Conferences"
"Knowledge Transfer for On-Device Deep Reinforcement Learning in Resource Constrained Edge Computing Systems","I. Jang; H. Kim; D. Lee; Y. -S. Son; S. Kim","Autonomous IoT Research Section, Electronics and Telecommunications Research Institute (ETRI), Daejeon, South Korea; Autonomous IoT Research Section, Electronics and Telecommunications Research Institute (ETRI), Daejeon, South Korea; Autonomous IoT Research Section, Electronics and Telecommunications Research Institute (ETRI), Daejeon, South Korea; Autonomous IoT Research Section, Electronics and Telecommunications Research Institute (ETRI), Daejeon, South Korea; Autonomous IoT Research Section, Electronics and Telecommunications Research Institute (ETRI), Daejeon, South Korea","IEEE Access","17 Aug 2020","2020","8","","146588","146597","Deep reinforcement learning (DRL) is a promising approach for developing control policies by learning how to perform tasks. Edge devices are required to control their actions by exploiting DRL to solve tasks autonomously in various applications such as smart manufacturing and autonomous driving. However, the resource limitations of edge devices make it unfeasible for them to train their policies from scratch. It is also impractical for such an edge device to use the policy with a large number of layers and parameters, which is pre-trained by a centralized cloud infrastructure with high computational power. In this paper, we propose a method, on-device DRL with distillation (OD3), to efficiently transfer distilled knowledge of how to behave for on-device DRL in resource-constrained edge computing systems. Our proposed method makes it possible to simultaneously perform knowledge transfer and policy model compression in a single training process on edge devices with considering their limited resource budgets. The novelty of our method is to apply a knowledge distillation approach to DRL based edge device control in integrated edge cloud environments. We analyze the performance of the proposed method by implementing it on a commercial embedded system-on-module equipped with limited hardware resources. The experimental results show that 1) edge policy training with the proposed method achieves near-cloud-performance in terms of average rewards, although the size of the edge policy network is significantly smaller compared to that of the cloud policy network and 2) the training time elapsed for edge policy training with our method is reduced significantly compared to edge policy training from scratch.","2169-3536","","10.1109/ACCESS.2020.3014922","Electronics and Telecommunications Research Institute (ETRI); Korean Government (Core Technologies of Distributed Intelligence Things for Solving Industry and Society Problems)(grant numbers:20ZR1100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162017","Deep reinforcement learning;edge computing;edge AI;knowledge transfer;policy model compression;on-device training","Training;Cloud computing;Task analysis;Computational modeling;Hardware;Edge computing;Artificial intelligence","cloud computing;embedded systems;learning (artificial intelligence)","DRL based edge device control;integrated edge cloud environments;edge policy network;cloud policy network;edge policy training;knowledge transfer;on-device deep reinforcement learning;resource constrained edge;resource-constrained edge computing systems;knowledge distillation approach;on-device DRL with distillation;OD3","","16","","38","CCBY","7 Aug 2020","","","IEEE","IEEE Journals"
"DDPG Reinforcement Learning Experiment for Improving the Stability of Bipedal Walking of Humanoid Robots","Y. Chun; J. Choi; I. Min; M. Ahn; J. Han","Department of Convergence Robot System, Hanyang University, Ansan, Republic of Korea; Department of Mechatronics Engineering, Hanyang University, Ansan, Republic of Korea; Department of Convergence Robot System, Hanyang University, Ansan, Republic of Korea; Department of Mechanical and Aerospace Engineering, University of California, Los Angeles, CA, USA; Department of Robotics, Hanyang University, Ansan, Republic of Korea","2023 IEEE/SICE International Symposium on System Integration (SII)","15 Feb 2023","2023","","","1","7","To improve the stability of bipedal walking of humanoid robots, we developed a method of setting trajectory parameters using reinforcement learning on a treadmill like testbed in a real-world environment. A deep deterministic policy gradient (DDPG) was used as the reinforcement learning algorithm. By improving the reward using a zero moment point (ZMP), the optimum value of walking stability and walking speed was determined. The robot was designed to measure the ZMP and mount weights on the upper body. In addition, a treadmill was manufactured to operate at the same speed as the walking speed of the robot. Reinforcement learning was divided into unweighted cases and cases with a weight of 1kg. At approximately 100 min, 300 episodes were performed, and reward improvements of 16.71% and 26.25% reward improvements were made. The ZMP measurements indicated that bipedal walking was performed in a safe area. Therefore, we demonstrated that the biped walking performance of a humanoid robot can be improved by the reinforcement learning of walking speed and ZMP similarity.","2474-2325","979-8-3503-9868-7","10.1109/SII55687.2023.10039306","MOTIE (Ministry of Trade, Industry, and Energy) in Korea; Korea Institute for Advancement of Technology (KIAT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10039306","Humanoid and Bipedal Locomotion;Reinforcement Learning;Bipedal Walking","Legged locomotion;Weight measurement;Service robots;Humanoid robots;Virtual environments;Reinforcement learning;System integration","deep learning (artificial intelligence);gradient methods;humanoid robots;learning systems;legged locomotion;motion control;reinforcement learning;robot dynamics;stability","biped walking performance;bipedal walking stability;DDPG reinforcement learning experiment;deep deterministic policy gradient;humanoid robot;reward improvement;trajectory parameters;treadmill;walking speed;zero moment point;ZMP measurements","","1","","24","IEEE","15 Feb 2023","","","IEEE","IEEE Conferences"
"CoPace: Edge Computation Offloading and Caching for Self-Driving With Deep Reinforcement Learning","H. Tian; X. Xu; L. Qi; X. Zhang; W. Dou; S. Yu; Q. Ni","School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Information Science and Engineering, Qufu Normal University, Rizhao, China; Department of Computing, Macquarie University, Sydney, NSW, Australia; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia; School of Computing and Communications, Lancaster University, Lancaster, U.K.","IEEE Transactions on Vehicular Technology","17 Dec 2021","2021","70","12","13281","13293","Currently, self-driving, emerging as a key automatic application, has brought a huge potential for the provision of in-vehicle services (e.g., automatic path planning) to mitigate urban traffic congestion and enhance travel safety. To provide high-quality vehicular services with stringent delay constraints, edge computing (EC) enables resource-hungry self-driving vehicles (SDVs) to offload computation-intensive tasks to the edge servers (ESs). In addition, caching highly reusable contents decreases the redundant transmission time and improves the quality of services (QoS) of SDVs, which is envisioned as a supplement to the computation offloading. However, the high mobility and time-varying requests of SDVs make it challenging to provide reliable offloading decisions while guaranteeing the resource utilization of content caching. To this end, in this paper we propose a collaborative computation offloading and content caching method, named CoPace, by leveraging deep reinforcement learning (DRL) in EC for self-driving system. Specifically, we first introduce OSTP to predict the future time-varying content popularity, taking into account the temporal-spatial attributes of requests. Moreover, a DRL-based algorithm is developed to jointly optimize the offloading and caching decisions, as well as the resource allocation (i.e., computing and communication resources) strategies. Extensive experiments with real-world datasets in Shanghai, China, are conducted to evaluate the performance, which demonstrates that CoPace is both effective and well-performed.","1939-9359","","10.1109/TVT.2021.3121096","Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20211284); Financial and Science Technology Plan Project of Xinjiang Production and Construction Corps(grant numbers:2020DB005); National Natural Science Foundation of China(grant numbers:61872219); Priority Academic Program Development of Jiangsu Higher Education Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9580706","Self-driving;deep reinforcement learning;edge computing;computation offloading;content caching","Resource management;Deep learning;Reinforcement learning;Edge computing;Quality of service;Prediction algorithms;Autonomous automobiles","cache storage;cloud computing;Internet;learning (artificial intelligence);mobile computing;mobile radio;optimisation;path planning;quality of service;resource allocation;road safety;road traffic;telecommunication traffic;traffic engineering computing","edge computation offloading;deep reinforcement learning;key automatic application;in-vehicle services;automatic path planning;urban traffic congestion;travel safety;high-quality vehicular services;stringent delay constraints;edge computing;resource-hungry self-driving vehicles;SDVs;computation-intensive tasks;edge servers;highly reusable contents;redundant transmission time;high mobility;time-varying requests;reliable offloading decisions;resource utilization;collaborative computation offloading;content caching method;named CoPace;self-driving system;future time-varying content popularity;DRL-based algorithm;caching decisions;resource allocation;communication resources","","17","","38","IEEE","19 Oct 2021","","","IEEE","IEEE Journals"
"Dynamic Edge Computation Offloading for Internet of Vehicles With Deep Reinforcement Learning","L. Yao; X. Xu; M. Bilal; H. Wang","School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing 210044, China.; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing 210044, China, also with the Jiangsu Collaborative Innovation Center of Atmospheric Environment and Equipment Technology (CICAEET), Engineering Research Center of Digital Forensics, Ministry of Education, Nanjing University of Information Science and Technology, Nanjing 210044, China, and also with the State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; Department of Computer and Electronics Systems Engineering, Hankuk University of Foreign Studies, Yongin-si, Gyeonggi-do 17035, South Korea.; Cybersecurity Program, St. Bonaventure University, St. Bonaventure, NY 14778 USA.","IEEE Transactions on Intelligent Transportation Systems","","2022","PP","99","1","9","Recent developments in the Internet of Vehicles (IoV) enabled the myriad emergence of a plethora of data-intensive and latency-sensitive vehicular applications, posing significant difficulties to traditional cloud computing. Vehicular edge computing (VEC), as an emerging paradigm, enables the vehicles to utilize the resources of the edge servers to reduce the data transfer burden and computing stress. Although the utilization of VEC is a favourable support for IoV applications, vehicle mobility and other factors further complicate the challenge of designing and implementing such systems, leading to incremental delay and energy consumption. In recent times, there have been attempts to integrate deep reinforcement learning (DRL) approaches with IoV-based systems, to facilitate real-time decision-making and prediction. We demonstrate the potential of such an approach in this paper. Specifically, the dynamic computation offloading problem is constructed as a Markov decision process (MDP). Then, the twin delayed deep deterministic policy gradient (TD3) algorithm is utilized to achieve the optimal offloading strategy. Finally, findings from the simulation demonstrate the potential of our proposed approach.","1558-0016","","10.1109/TITS.2022.3178759","Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20211284); Financial and Science Technology Plan Project of Xinjiang Production and Construction Corps(grant numbers:2020DB005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9789187","Internet of Vehicles;deep reinforcement learning;edge computing.","Task analysis;Vehicle dynamics;Delays;Computational modeling;Dynamic scheduling;Edge computing;Processor scheduling","","","","2","","","IEEE","6 Jun 2022","","","IEEE","IEEE Early Access Articles"
"Computing Offloading With Fairness Guarantee: A Deep Reinforcement Learning Method","H. Hao; C. Xu; W. Zhang; S. Yang; G. -M. Muntean","Shandong Computer Science Center (National Supercomputing Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Shandong Computer Science Center (National Supercomputing Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Performance Engineering Laboratory, School of Electronic Engineering, Dublin City University, Dublin 9, Ireland","IEEE Transactions on Circuits and Systems for Video Technology","3 Oct 2023","2023","33","10","6117","6130","Edge computing can reduce service latency and save backhaul bandwidth by completing services at network edges, providing support for diverse computation-intensive and delay-sensitive services. However, it is not practical to support all services at edge nodes due to the limited network resources. The decision that which services can be provided locally and which services should been offloaded to cloud significantly impacts the user experience. Cloud-edge computing offloading becomes an important issue in edge computing. In this paper, we take the fairness into the optimization objective of computing offloading problem, and consider both computing capacity and storage space as problem constraints. The problem is formulated as a long-term average optimization problem to maximize the  $\alpha $ -fair utility function of saved time, and further translated as a Markov decision process. As the optimization problem with fairness guarantee and huge action space, we cannot solve it with traditional methods. Therefore, an innovative multi-update deep reinforcement learning algorithm is proposed which can optimize the objective with  $\alpha $ -fair utility function and reduce dramatically the size of action space. We also prove the convergence of our algorithm theoretically. To our best knowledge, the long-term average optimization of computing offloading with fairness guarantee is rarely seen in literature. Extensive simulation experiments show that our algorithm can converge quickly and has better performance in terms of service delay and fairness.","1558-2205","","10.1109/TCSVT.2023.3255229","National Natural Science Foundation of China (NSFC)(grant numbers:62225105); National Natural Science Foundation of Shandong Province(grant numbers:ZR2022QF040); Qilu University of Technology (QLU) Pilot Project of Integration of Science, Education and Production(grant numbers:2022PX083,2022GH007); Science Foundation Ireland (SFI) (Fradis)(grant numbers:21/FFP-P/10244); Insight(grant numbers:12/RC/2289_P2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10065525","Mobile edge computing;computing offloading;deep reinforcement learning;fairness guarantee","Optimization;Cloud computing;Delays;Computational modeling;Reinforcement learning;Deep learning;Markov processes","","","","1","","40","CCBY","10 Mar 2023","","","IEEE","IEEE Journals"
"SON Coordination in Heterogeneous Networks: A Reinforcement Learning Framework","O. -C. Iacoboaiea; B. Sayrac; S. Ben Jemaa; P. Bianchi","Orange Labs, Issy-les-Moulineaux, France; Orange Labs, Issy-les-Moulineaux, France; Orange Labs, Issy-les-Moulineaux, France; Telecom ParisTech, Paris, France","IEEE Transactions on Wireless Communications","8 Sep 2016","2016","15","9","5835","5847","An important problem of today's mobile network operators is to bring down the capital expenditures and operational expenditures. One strategy is to automate the parameter tuning on the small cells through the so-called self-organizing network (SON) functionalities, such as cell range expansion, mobility robustness optimization, or enhanced Inter-Cell Interference Coordination. Having several of these functionalities in the network will surely create conflicts, as, for example, they may try to change the same parameter in the opposite directions. This raises that the need for an SON COordinator (SONCO) meant to arbitrate the parameter change requests of the SON functions, ensuring some degree of fairness. It is difficult to anticipate the impact of accepting several simultaneous requests. In this paper, we provide a SONCO design based on reinforcement learning (RL) as it allows us to learn from previous experiences and improve our future decisions. Typically, RL algorithms are complex. To reduce this complexity, we employ two flavors of function approximation and provide a study-case. Results show that the proposed SONCO design is capable of biasing this fairness among the SON functions by means of weights attributed to the SON functions. Also, we evaluate the tracking capability of the algorithms.","1558-2248","","10.1109/TWC.2016.2571695","European Union Seventh Framework Programme (FP7/2007-2013) through the FP7 SEMAFOUR Project [30](grant numbers:316384); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476897","SON coordination;CRE;MRO;eICIC;SON instances;LTE;reinforcement learning;function approximation;state aggregation;heterogeneous networks","Optimization;Algorithm design and analysis;Wireless communication;Learning (artificial intelligence);Heterogeneous networks;Mobile computing;Mobile communication","cellular radio;function approximation;learning (artificial intelligence);radiofrequency interference;self-organising feature maps;telecommunication computing","SON coordination;heterogeneous networks;reinforcement learning framework;mobile network operators;capital expenditures;operational expenditures;self-organizing network;small cells parameter tuning automation;cell range expansion;mobility robustness optimization;enhanced inter-cell interference coordination;SONCO design;RL algorithms;function approximation","","34","","37","IEEE","23 May 2016","","","IEEE","IEEE Journals"
"Low complexity SON coordination using reinforcement learning","O. Iacoboaiea; B. Sayrac; S. Ben Jemaa; P. Bianchi","Orange Labs, Issy les Moulineaux, France; Orange Labs, Issy les Moulineaux, France; Orange Labs, Issy les Moulineaux, France; Telecom ParisTech., Paris, France","2014 IEEE Global Communications Conference","12 Feb 2015","2014","","","4406","4411","The continuously increasing traffic demand faces us with increased CAPital Expenditures (CAPEX) and Operational Expenditure (OPEX). Self Organizing Network (SON) functions aim to lower these costs by automating the network tuning. A SON instance is a realization of a SON function which can tune one or a set of cells. Having several uncoordinated SON functions in the network creates a risk for conflicts and instability. This raises the need for a SON Coordinator (SONCO) meant to deal with these issues. In this work we consider that on each cell we have one SON instance of each SON function. We present the design of a SONCO which arbitrates conflicts based on weights attributed to the SON functions. The design makes use of Reinforcement Learning (RL) with function approximation. We provide a low complexity approximation of the action-value function based on a number of parameters that scales linearly with the number of cells. We present a study case with the Mobility Load Balancing (tuning the Cell Individual Offset (CIO)) and Mobility Robustness Optimization (tuning the CIO and the handover hysteresis) functions, where the SONCO deals with the conflicts on the CIOs. Numerical results prove that we can orchestrate the SON functions through SONCO configurations that reflect different operator policies.","1930-529X","978-1-4799-3512-3","10.1109/GLOCOM.2014.7037501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7037501","SON Coordination;MLB;MRO;SON instances;LTE;reinforcement learning;function approximation","Function approximation;Complexity theory;Hysteresis;Artificial neural networks;Kernel;Wireless communication;Learning (artificial intelligence)","computational complexity;function approximation;learning (artificial intelligence);mobility management (mobile radio);optimisation;resource allocation;telecommunication computing;telecommunication traffic","low complexity SON coordination;reinforcement learning;traffic demand;capital expenditures;CAPEX;operational expenditure;OPEX;self organizing network functions;network tuning automation;SONCO;SON coordinator;function approximation;low complexity approximation;mobility load balancing;cell individual offset;CIO;mobility robustness optimization;handover hysteresis","","3","","15","IEEE","12 Feb 2015","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Algorithm for Latency-Oriented IIoT Resource Orchestration","P. Zhang; Y. Zhang; N. Kumar; C. -H. Hsu","College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; Department of Computer Science and Engineering, Thapar Institute of Engineering and Technology (Deemed to be University), Patiala, India; Department of Computer Science and Information Engineering, Asia University, Taichung, Taiwan","IEEE Internet of Things Journal","6 Apr 2023","2023","10","8","7153","7163","Due to geographical factors and resource constraints, the traditional Internet architecture cannot meet the needs of the space–air–ground-integrated network (SAGIN) resource layout in the Industrial Internet of Things (IIoT) service. How to arrange network resources in SAGIN quickly and efficiently to meet the quality of service requirements of users has become a hot research topic in the industry. Based on the characteristics of SAGIN with multiple network segments, we convert the resource scheduling problem of SAGIN into a multidomain virtual network embedding (VNE) problem. This article proposes a latency-sensitive VNE algorithm based on deep reinforcement learning (DDRL-VNE) in the SAGIN environment. Unlike traditional latency optimization algorithms, we consider the effect of traffic size and hop count on latency when evaluating latency. We constructed a learning agent composed of a five-layer policy network and extracted a feature matrix as its training environment based on the network attributes of SAGIN. The node embedding is completed according to the probability that each node is embedded in the training, and then the breadth-first search strategy is used to complete the link embedding. The experimental results effectively illustrate the effectiveness of the algorithm in the SAGIN resource allocation problem.","2327-4662","","10.1109/JIOT.2022.3229270","Shandong Provincial Natural Science Foundation, China(grant numbers:ZR2020MF006,ZR2022LZH015); Industry-University Research Innovation Foundation of Ministry of Education of China(grant numbers:2021FNA01001); Major Scientific and Technological Projects of CNPC(grant numbers:ZD2019-183-006); Open Foundation of State Key Laboratory of Integrated Services Networks (Xidian University)(grant numbers:ISN23-09); Open Foundation of State Key Laboratory of Networking and Switching Technology (Beijing University of Posts and Telecommunications)(grant numbers:SKLNST-2021-1-17); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9984843","Deep reinforcement learning (DRL);Industrial Internet of Things (IIoT);latency;space–air–ground-integrated network (SAGIN)","Space-air-ground integrated networks;Industrial Internet of Things;Internet of Things;Resource management;Reinforcement learning;Quality of service;Security","deep learning (artificial intelligence);Internet of Things;optimisation;reinforcement learning;resource allocation;virtualisation","deep reinforcement learning;five-layer policy network;geographical factors;Internet of Things service;latency-oriented IIoT resource orchestration;latency-sensitive VNE algorithm;learning agent;multidomain virtual network;multiple network segments;network resources;resource constraints;resource scheduling problem;SAGIN environment;SAGIN resource allocation problem;space-air-ground-integrated network resource layout;traditional Internet architecture;traditional latency optimization algorithms","","2","","39","IEEE","14 Dec 2022","","","IEEE","IEEE Journals"
"Control Strategy for Denitrification Efficiency of Coal-Fired Power Plant Based on Deep Reinforcement Learning","J. Fu; H. Xiao; H. Wang; J. Zhou","Faculty of Computer, Guangdong University of Technology, Guangzhou, China; Faculty of Computer, Guangdong University of Technology, Guangzhou, China; Department of Computer Science, Norwegian University of Science and Technology, Gjøvik, Norway; Faculty of Information Technology, Macau University of Science and Technology, Macau, China","IEEE Access","15 Apr 2020","2020","8","","65127","65136","The optimal control of denitrification system in coal-fired power plants in China has recently received widespread attention. The accurate prediction of denitrification efficiency and formulate control strategy of denitrification efficiency can guide the control and operation of the denitrification system better. Meanwhile, it can achieve the effect of energy conservation and Nitrogen oxides (NOx) reduction. In this paper, we take a domestic 1000 MW unit as an example, consider each of the major factors that affect the denitrification efficiency of selective catalytic reduction (SCR). We put forward a deep reinforcement learning (DRL) model by combining the Long short-term memory (LSTM) model and the Asynchronous Advantage Actor - Critic algorithm (A3C). We first use the LSTM to build a prediction model for denitrification efficiency. We then use the DRL model to obtain a control strategy for SCR denitrification efficiency in coal-fired power plants. The experimental results demonstrate that the accuracy of denitrification efficiency prediction model we established is better than other machine learning models, reaching 91.7%. Our control strategy model is industrially feasible and universally applicable.","2169-3536","","10.1109/ACCESS.2020.2985233","Research on self-organizing elasticity enhancement strategy of intelligent manufacturing IoT network, National Natural Science Foundation of China(grant numbers:61672170); R&D and Application of a New Generation of Intelligent Industrial Robots, Core Technology Research Project, Foshan City(grant numbers:1920001001367); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9055395","Coal-fired power plant;denitrification efficiency;selective catalytic reduction (SCR);long short-term memory (LSTM);asynchronous advantage actor critic (A3C);deep reinforcement learning","Power generation;Predictive models;Coal;Reinforcement learning;Combustion;Boilers","coal;energy conservation;learning (artificial intelligence);neurocontrollers;optimal control;power system control;recurrent neural nets;steam power stations;thermal power stations","SCR denitrification efficiency;coal-fired power plant;denitrification efficiency prediction model;control strategy model;optimal control;denitrification system;deep reinforcement learning model;long short-term memory;LSTM model;A3C algorithm;asynchronous advantage actor - critic algorithm;power 1000.0 MW","","9","","35","CCBY","2 Apr 2020","","","IEEE","IEEE Journals"
"RL-OPC: Mask Optimization with Deep Reinforcement Learning","X. Liang; Y. Ouyang; H. Yang; B. Yu; Y. Ma","Microelectronics Thrust, The Hong Kong University of Science and Technology, Guangzhou, China; Microelectronics Thrust, The Hong Kong University of Science and Technology, Guangzhou, China; Nvidia, Austin, TX, USA; Department of Computer Science and Engineering, NT, The Chinese University of Hong Kong, Hong Kong, Hong Kong; Microelectronics Thrust, The Hong Kong University of Science and Technology, Guangzhou, China","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2023","PP","99","1","1","Mask optimization is a vital step in the VLSI manufacturing process in advanced technology nodes. As one of the most representative techniques, optical proximity correction (OPC) is widely applied to enhance printability. Since conventional OPC methods consume prohibitive computational overhead, recent research has applied machine learning techniques for efficient mask optimization. However, existing discriminative learning models rely on a given dataset for supervised training, and generative learning models usually leverage a proxy optimization objective for end-to-end learning, which may limit the feasibility. In this paper, we pioneer introducing the reinforcement learning (RL) model for mask optimization, which directly optimizes the preferred objective without leveraging a differentiable proxy. Intensive experiments show that our method outperforms state-of-the-art solutions, including academic approaches and commercial toolkits.","1937-4151","","10.1109/TCAD.2023.3309745","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10233698","","Optimization;Computational modeling;Measurement;Lithography;Robustness;Very large scale integration;Q-learning","","","","","","","IEEE","29 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Reinforcement Learning-Based Cascade Motion Policy Design for Robust 3D Bipedal Locomotion","G. A. Castillo; B. Weng; W. Zhang; A. Hereid","Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH, USA; Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH, USA; Department of Mechanical and Energy Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Mechanical and Aerospace Engineering, The Ohio State University, Columbus, OH, USA","IEEE Access","28 Feb 2022","2022","10","","20135","20148","This paper presents a novel reinforcement learning (RL) framework to design cascade feedback control policies for 3D bipedal locomotion. Existing RL algorithms are often trained in an end-to-end manner or rely on prior knowledge of some reference joint or task space trajectories. Unlike these studies, we propose a policy structure that decouples the bipedal locomotion problem into two modules that incorporate the physical insights from the nature of the walking dynamics and the well-established Hybrid Zero Dynamics approach for 3D bipedal walking. As a result, the overall RL framework has several key advantages, including lightweight network structure, sample efficiency, and less dependence on prior knowledge. The proposed solution learns stable and robust walking gaits from scratch and allows the controller to realize omnidirectional walking with accurate tracking of the desired velocity and heading angle. The learned policies also perform robustly against various adversarial forces applied to the torso and walking blindly on a series of challenging and unstructured terrains. These results demonstrate that the proposed cascade feedback control policy is suitable for navigation of 3D bipedal robots in indoor and outdoor environments.","2169-3536","","10.1109/ACCESS.2022.3151771","The Ohio State University Materials and Manufacturing for Sustainability (M&MS) Discovery Theme Initiative; National Natural Science Foundation of China(grant numbers:62073159); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714352","Motion control;legged locomotion;machine learning","Robots;Legged locomotion;Trajectory;Robot kinematics;Hardware;Regulation;Torso","feedback;gait analysis;humanoid robots;learning (artificial intelligence);legged locomotion;motion control;robot dynamics;stability","reinforcement learning-based cascade motion policy design;robust 3D bipedal locomotion;reinforcement learning framework;cascade feedback control policy;RL algorithms;reference joint;policy structure;bipedal locomotion problem;incorporate the physical insights;walking dynamics;Hybrid Zero Dynamics approach;RL framework;lightweight network structure;robust walking gaits;omnidirectional walking;learned policies;3D bipedal robots","","8","","38","CCBY","15 Feb 2022","","","IEEE","IEEE Journals"
"An Autonomous Illumination System for Vehicle Documentation Based on Deep Reinforcement Learning","L. Leontaris; N. Dimitriou; D. Ioannidis; K. Votis; D. Tzovaras; E. Papageorgiou","Information Technologies Institute, Centre for Research and Technology Hellas, Thessaloniki, Greece; Information Technologies Institute, Centre for Research and Technology Hellas, Thessaloniki, Greece; Information Technologies Institute, Centre for Research and Technology Hellas, Thessaloniki, Greece; Information Technologies Institute, Centre for Research and Technology Hellas, Thessaloniki, Greece; Information Technologies Institute, Centre for Research and Technology Hellas, Thessaloniki, Greece; Department of Energy Systems, Faculty of Technology, University of Thessaly at Geopolis, Larissa, Greece","IEEE Access","25 May 2021","2021","9","","75336","75348","A common problem for machine vision applications is uncontrolled illumination conditions that cause undesired artifacts on sensorial data. For instance, quality inspection using color cameras, while having wide industrial application, requires manual illumination adjustment and is severely affected by external lighting sources and the physical properties of the inspected object. To overcome this problem, we propose an autonomous illumination solution, that adjusts illumination via a Deep Reinforcement Learning (DRL) agent following a goal-oriented reward that takes into account image entropy and specularity. The system is validated in a challenging vehicle documentation use case where vehicle images are captured under various lighting conditions using a camera and an in-house built illumination system. The DRL agent learns to control illumination levels directly from high-dimensional visual inputs by mapping the interactions from the environment to the reward-driven control actions of the illumination system, targeting an optimal illumination zone even under the appearance of abrupt illumination changes in the environment.","2169-3536","","10.1109/ACCESS.2021.3081736","European Commission through the Optimizing Manufacturing Processes through Artificial Intelligence and Virtualization (OPTIMAI) Project funded by the Horizon 2020 Research and Innovation Program(grant numbers:958264); European Union through the General Secretariat of Research and Technology, Ministry of Development and Investments by the Bilateral Science & Technology (S&T) Cooperation Program Greece—Germany (2017)(grant numbers:INVIVO/T2DGE-0951); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9435376","Artificial intelligence;autonomous systems;computer vision;deep reinforcement learning;illumination control","Lighting;Reinforcement learning;Job shop scheduling;Robots;Task analysis;Light emitting diodes;Cameras","automatic optical inspection;cameras;computer vision;deep learning (artificial intelligence);image capture;image colour analysis;lighting;road vehicles;traffic engineering computing","quality inspection;color cameras;industrial application;external lighting sources;physical properties;inspected object;autonomous illumination solution;goal-oriented reward;image entropy;specularity;vehicle images;lighting conditions;in-house built illumination system;DRL agent;vehicle documentation;sensorial data;illumination conditions;machine vision applications;Deep Reinforcement Learning;autonomous illumination system;abrupt illumination changes;optimal illumination zone;reward-driven control actions;high-dimensional visual inputs;illumination levels","","2","","64","CCBY","19 May 2021","","","IEEE","IEEE Journals"
"Intelligent IoT Connectivity: Deep Reinforcement Learning Approach","M. Kwon; J. Lee; H. Park","Department of Electronic and Electrical Engineering, Ewha Womans University, Seoul, South Korea; Department of Electronic and Electrical Engineering, Ewha Womans University, Seoul, South Korea; Department of Electronic and Electrical Engineering, Ewha Womans University, Seoul, South Korea","IEEE Sensors Journal","6 Feb 2020","2020","20","5","2782","2791","In this paper, we propose a distributed solution to design a multi-hop ad hoc Internet of Things (IoT) network where mobile IoT devices strategically determine their wireless transmission ranges based on a deep reinforcement learning approach. We consider scenarios where only a limited networking infrastructure is available but a large number of IoT devices are deployed in building a multi-hop ad hoc network to deliver source data to the destination. An IoT device is considered as a decision-making agent that strategically determines its transmission range in a way that maximizes network throughput while minimizing the corresponding transmission power consumption. Each IoT device collects information from its partial observations and learns its environment through a sequence of experiences. Hence, the proposed solution requires only a minimal amount of information from the system. We show that the actions that the IoT devices take from its policy are determined as to activate or inactivate its transmission, i.e., only necessary relay nodes are activated with the maximum transmit power, and nonessential nodes are deactivated to minimize power consumption. Using extensive experiments, we confirm that the proposed solution builds a network with higher network performance than the current state-of-the-art solutions in terms of system goodput and connectivity ratio.","1558-1748","","10.1109/JSEN.2019.2949997","Institute of Information & Communications Technology Planning & Evaluation (IITP) through the Korea Government (MSIT) (Supervised Agile Machine Learning Techniques for Network Automation based on Network Data Analytics Function)(grant numbers:2019-0-00024); National Research Foundation of Korea(grant numbers:NRF-2017R1A2B4005041); National Science Foundation; Robert and Janice McNair Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8886442","Intelligent IoT connectivity;network formation;network topology design;deep reinforcement learning;wireless ad hoc networks;mobile relay networks","Internet of Things;Intelligent sensors;Reinforcement learning;Decision making;Spread spectrum communication","Internet of Things;learning (artificial intelligence);mobile ad hoc networks","intelligent IoT connectivity;deep reinforcement learning approach;mobile IoT devices;wireless transmission ranges;networking infrastructure;IoT device;transmission power consumption;multihop ad hoc Internet of Things network","","34","","34","IEEE","30 Oct 2019","","","IEEE","IEEE Journals"
"Federated Reinforcement Learning for Electric Vehicles Charging Control on Distribution Networks","J. Qian; Y. Jiang; X. Liu; Q. Wang; T. Wang; Y. Shi; W. Chen","MoE Engineering Research Center of Software/Hardware Co-design Technology and Application, Shanghai Key Lab. of Trustworthy Computing, China; Automatic Control Laboratory, EPFL, Switzerland; School of Information Science and Technology, China; State Grid Beijing Electric Power Company, China; MoE Engineering Research Center of Software/Hardware Co-design Technology and Application, Shanghai Key Lab. of Trustworthy Computing, China; School of Information Science and Technology, China; Department of Electronic Engineering, Tsinghua University, China","IEEE Internet of Things Journal","","2023","PP","99","1","1","With the growing popularity of electric vehicles (EVs), maintaining power grid stability has become a significant challenge. To address this issue, EV charging control strategies have been developed to manage the switch between vehicle-to-grid (V2G) and grid-to-vehicle (G2V) modes for EVs. In this context, multi-agent deep reinforcement learning (MADRL) has proven its effectiveness in EV charging control. However, existing MADRL-based approaches fail to consider the natural power flow of EV charging/discharging in the distribution network and ignore driver privacy. To deal with these problems, this paper proposes a novel approach that combines multi-EV charging/discharging with a radial distribution network (RDN) operating under optimal power flow (OPF) to distribute power flow in real time. A mathematical model is developed to describe the RDN load. The EV charging control problem is formulated as a Markov Decision Process (MDP) to find an optimal charging control strategy that balances V2G profits, RDN load, and driver anxiety. To effectively learn the optimal EV charging control strategy, a federated deep reinforcement learning algorithm named FedSAC is further proposed. Comprehensive simulation results demonstrate the effectiveness and superiority of our proposed algorithm in terms of the diversity of the charging control strategy, the power fluctuations on RDN, the convergence efficiency, and the generalization ability.","2327-4662","","10.1109/JIOT.2023.3306826","Shenzhen Science and Technology Plan Project(grant numbers:CJGJZD20210408092400001); Open Research Fund of Engineering Research Center of Software/Hardware Co-design Technology and Application, Ministry of Education (East China Normal University); Shanghai Sailing Program(grant numbers:22YF1428500); Swiss National Science Foundation under the NCCR Automation(grant numbers:51NF40_180545); Natural Science Foundation of Shanghai(grant numbers:21ZR1442700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10225344","V2G;Electrical Vehicle;Optimal Power Flow;Reinforcement Learning;Federated Learning","Electric vehicle charging;Distribution networks;Vehicle-to-grid;Load flow;Optimization;Anxiety disorders;Upper bound","","","","","","","IEEE","21 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Deep Reinforcement Learning-Based Intelligent Reflecting Surface for Secure Wireless Communications","H. Yang; Z. Xiong; J. Zhao; D. Niyato; L. Xiao; Q. Wu","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China","IEEE Transactions on Wireless Communications","12 Jan 2021","2021","20","1","375","388","In this paper, we study an intelligent reflecting surface (IRS)-aided wireless secure communication system, where an IRS is deployed to adjust its reflecting elements to secure the communication of multiple legitimate users in the presence of multiple eavesdroppers. Aiming to improve the system secrecy rate, a design problem for jointly optimizing the base station (BS)'s beamforming and the IRS's reflecting beamforming is formulated considering different quality of service (QoS) requirements and time-varying channel conditions. As the system is highly dynamic and complex, and it is challenging to address the non-convex optimization problem, a novel deep reinforcement learning (DRL)-based secure beamforming approach is firstly proposed to achieve the optimal beamforming policy against eavesdroppers in dynamic environments. Furthermore, post-decision state (PDS) and prioritized experience replay (PER) schemes are utilized to enhance the learning efficiency and secrecy performance. Specifically, a modified PDS scheme is presented to trace the channel dynamic and adjust the beamforming policy against channel uncertainty accordingly. Simulation results demonstrate that the proposed deep PDS-PER learning based secure beamforming approach can significantly improve the system secrecy rate and QoS satisfaction probability in IRS-aided secure communication systems.","1558-2248","","10.1109/TWC.2020.3024860","National Research Foundation (NRF), Singapore, through Singapore Energy Market Authority (EMA), Energy Resilience(grant numbers:NRF2017EWTEP003-041); Singapore NRF(grant numbers:NRF2015-NRFISF001-2277); Singapore NRF National Satellite of Excellence, Design Science, and Technology for Secure Critical Infrastructure(grant numbers:NSoE DeST-SCI2019-0007); A*STAR-NTU-SUTD Joint Research Grant on Artificial Intelligence for the Future of Manufacturing(grant numbers:RGANS1906); Wallenberg AI, Autonomous Systems, and Software Program and Nanyang Technological University (WASP/NTU)(grant numbers:M4082187 (4080)); Singapore Ministry of Education (MOE)(grant numbers:Tier 1 RG16/20); Alibaba Group through Alibaba Innovative Research (AIR) Program; Alibaba-NTU Singapore Joint Research Institute (JRI); Nanyang Technological University (NTU) Startup Grant, Singapore Ministry of Education Academic Research Fund(grant numbers:Tier 1 RG128/18,Tier 1 RG115/19,Tier 1 RT07/19,Tier 1 RT01/19,Tier 2 MOE2019-T2-1-176); NTU-WASP Joint Project, Singapore National Research Foundation through its Strategic Capability Research Centers Funding Initiative: Strategic Centre for Research in Privacy-Preserving Technologies and Systems; Energy Research Institute @NTU, Singapore NRF National Satellite of Excellence, Design Science, and Technology for Secure Critical Infrastructure(grant numbers:NSoE DeST-SCI2019-0012); AI Singapore 100 Experiments (100E) programme; NTU Project for Large Vertical Take-Off and Landing Research Platform; Natural Science Foundation of China(grant numbers:61971366); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206080","Secure communication;intelligent reflecting surface;beamforming;secrecy rate;deep reinforcement learning","Array signal processing;Optimization;Wireless communication;Quality of service;Reinforcement learning;Time-varying channels","array signal processing;concave programming;deep learning (artificial intelligence);quality of service;radio networks;telecommunication computing;telecommunication security;time-varying channels;wireless channels","system secrecy rate;base station;time-varying channel conditions;nonconvex optimization problem;deep reinforcement learning-based secure beamforming approach;optimal beamforming policy;learning efficiency;modified PDS scheme;deep PDS-PER learning;IRS-aided secure wireless communication system;eavesdroppers;intelligent reflecting surface-aided elements;DRL;PDS scheme;post-decision state scheme;quality of service requirements","","231","","40","IEEE","25 Sep 2020","","","IEEE","IEEE Journals"
"Hierarchical Game-Theoretic and Reinforcement Learning Framework for Computational Offloading in UAV-Enabled Mobile Edge Computing Networks With Multiple Service Providers","A. Asheralieva; D. Niyato","Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Internet of Things Journal","9 Oct 2019","2019","6","5","8753","8769","We present a novel game-theoretic (GT) and reinforcement learning (RL) framework for computational offloading in the mobile edge computing (MEC) network operated by multiple service providers (SPs). The network is formed by MEC servers installed at stationary base stations (BSs) and unmanned aerial vehicles (UAVs) deployed as quasi-stationary BSs. Since computing powers of MEC servers are limited, the BSs in proximity can form coalitions with shared data processing resources to serve their users more efficiently. However, as BSs can be privately owned or controlled by different SPs, in any coalition, the BSs: 1) take only the actions that maximize their long-term payoffs and 2) do not coordinate their actions with other BSs in the coalition. That is, inside each coalition, BSs act in an independent and self-interested manner. Therefore, the interactions among BSs cannot be described by conventional coalitional games. Instead, the network operation is modeled by a two-level hierarchical model. The upper level is a cooperative game that defines the process of coalition formation. The lower level comprises the set of noncooperative subgames to represent a self-interested and independent behavior of BSs in coalitions. To enable each BS to select a coalition and decide on its action maximizing its long-term payoff, we propose two algorithms that combine coalition formation with RL and prove that these algorithms converge to the states where the coalitional structure is strongly stable and the strategies of BSs are in the mixed-strategy Nash equilibrium (NE).","2327-4662","","10.1109/JIOT.2019.2923702","A*STAR-NTUSUTD Joint Research Grant Call on Artificial Intelligence for the Future of Manufacturing(grant numbers:RGANS1906); WASP/NTU(grant numbers:M4082187 (4080)); Singapore Ministry of Education (MOE) Tier 1(grant numbers:2017-T1-002-007 RG122/17); MOE Tier 2(grant numbers:MOE2014-T2-2-015 ARC4/15); Singapore(grant numbers:NRF2015-NRF-ISF001-2277); Energy Market Authority of Singapore(grant numbers:NRF2017EWT-EP003-041); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8740949","Edge computing;game theory;heterogeneous networks;mobile cloud computing (MEC);reinforcement learning (RL);resource allocation;unmanned aerial vehicle (UAV) communication","Servers;Games;Task analysis;Quality of service;Computational modeling;Internet of Things;Stochastic processes","game theory;learning (artificial intelligence);mobile computing;optimisation;remotely operated vehicles","reinforcement learning framework;computational offloading;UAV-enabled mobile edge computing networks;multiple service providers;mobile edge computing network;MEC servers;long-term payoff;conventional coalitional games;network operation;combine coalition formation;hierarchical game-theoretic learning framework","","98","","45","IEEE","19 Jun 2019","","","IEEE","IEEE Journals"
"Intelligent Reflecting Surface Assisted Anti-Jamming Communications: A Fast Reinforcement Learning Approach","H. Yang; Z. Xiong; J. Zhao; D. Niyato; Q. Wu; H. V. Poor; M. Tornatore","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; State Key Laboratory of Internet of Things for Smart City, University of Macau, Zhuhai, Macau; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA; Department of Electronics, Information and Bioengineering, Politecnico di Milano, Milan, Italy","IEEE Transactions on Wireless Communications","9 Mar 2021","2021","20","3","1963","1974","Malicious jamming launched by smart jammers can attack legitimate transmissions, which has been regarded as one of the critical security challenges in wireless communications. With this focus, this paper considers the use of an intelligent reflecting surface (IRS) to enhance anti-jamming communication performance and mitigate jamming interference by adjusting the surface reflecting elements at the IRS. Aiming to enhance the communication performance against a smart jammer, an optimization problem for jointly optimizing power allocation at the base station (BS) and reflecting beamforming at the IRS is formulated while considering quality of service (QoS) requirements of legitimate users. As the jamming model and jamming behavior are dynamic and unknown, a fuzzy win or learn fast-policy hill-climbing (WoLF-CPHC) learning approach is proposed to jointly optimize the anti-jamming power allocation and reflecting beamforming strategy, where WoLF-CPHC is capable of quickly achieving the optimal policy without the knowledge of the jamming model, and fuzzy state aggregation can represent the uncertain environment states as aggregate states. Simulation results demonstrate that the proposed anti-jamming learning-based approach can efficiently improve both the IRS-assisted system rate and transmission protection level compared with existing solutions.","1558-2248","","10.1109/TWC.2020.3037767","National Research Foundation (NRF), Singapore, under Singapore Energy Market Authority (EMA), Energy Resilience(grant numbers:NRF2017EWT-EP003-041,Singapore NRF2015-NRF-ISF001-2277); Singapore NRF National Satellite of Excellence, Design Science and Technology for Secure Critical Infrastructure(grant numbers:NSoE DeST-SCI2019-0007); A*STAR-NTU-SUTD Joint Research Grant on Artificial Intelligence for the Future of Manufacturing(grant numbers:RGANS1906); Wallenberg AI, Autonomous Systems and Software Program and Nanyang Technological University (WASP/NTU)(grant numbers:M4082187 (4080)); Singapore Ministry of Education (MOE) Tier 1(grant numbers:RG16/20); Alibaba Group through the Alibaba Innovative Research (AIR) Program, Alibaba-NTU Singapore Joint Research Institute (JRI), Nanyang Technological University (NTU) Startup Grant, Singapore Ministry of Education Academic Research Fund(grant numbers:Tier 1 RG128/18,Tier 1 RG115/19,Tier 1 RT07/19,Tier 1 RT01/19,Tier 2 MOE2019-T2-1-176); NTU-WASP Joint Project, Singapore National Research Foundation under its Strategic Capability Research Centers Funding Initiative: Strategic Centre for Research in Privacy-Preserving Technologies & Systems, Energy Research Institute @NTU, Singapore NRF National Satellite of Excellence, Design Science and Technology for Secure Critical Infrastructure(grant numbers:NSoE DeST-SCI2019-0012); AI Singapore 100 Experiments (100E) programme, NTU Project for Large Vertical Take-Off & Landing Research Platform; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264659","Anti-jamming;intelligent reflecting surface;power allocation;beamforming;reinforcement learning","Jamming;Array signal processing;Wireless communication;Optimization;Communication systems;Relays;Quality of service","array signal processing;fuzzy set theory;interference suppression;jamming;learning (artificial intelligence);optimisation;quality of service;radiocommunication;telecommunication computing;telecommunication security","malicious jamming;smart jammer;legitimate transmissions;critical security challenges;wireless communications;IRS;anti-jamming communication performance;optimization problem;jamming model;jamming behavior;fast-policy hill-climbing learning approach;anti-jamming power allocation;reflecting beamforming strategy;optimal policy;anti-jamming learning-based approach;transmission protection level;intelligent reflecting surface assisted anti-jamming communications;fast reinforcement learning approach;jamming interference mitigation;surface reflecting elements;base station;power allocation optimization;quality of service requirements;QoS requirements;fuzzy win;WoLF-CPHC learning approach;fuzzy state aggregation;uncertain environment states;IRS-assisted system rate","","93","","48","IEEE","19 Nov 2020","","","IEEE","IEEE Journals"
"UAV-Assisted Wireless Energy and Data Transfer With Deep Reinforcement Learning","Z. Xiong; Y. Zhang; W. Y. B. Lim; J. Kang; D. Niyato; C. Leung; C. Miao","Alibaba-NTU Joint Research Institute and School of Computer Science and Engineering, Nanyang Technological University, Singapore; Hubei Key Laboratory of Transportation Internet of Things, School of Computer Science and Technology, Wuhan University of Technology, Wuhan, China; Alibaba Group and Alibaba-NTU Joint Research Institute, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY) and School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Cognitive Communications and Networking","5 Mar 2021","2021","7","1","85","99","As a typical scenario in future generation communication network applications, UAV-assisted communication can perform autonomous data delivery for massive machine type communication (mMTC), where the data generated from Internet of Things (IoT) devices can be carried and delivered to the corresponding locations with no direct communication channels to the IoT devices. Wireless energy transfer technique can recharge the UAV when the system is in operation, assisting the UAV to continuously collect and deliver data. In this work, we formulate a Markov decision process (MDP) model to describe the energy and data transfer optimization problem for the UAV. To maximize the long-term utility of the UAV, the MDP model is solved by value iteration algorithm to obtain the optimal strategies of the UAV to collect data, deliver data, and receive transferred energy to replenish on-device battery energy storage. Furthermore, to tackle the issues of system state uncertainties, partially observable states, and large state space in UAV-assisted communication systems, we extend the MDP model and solve it by using a Q -learning and a deep reinforcement learning (DRL) schemes. Simulations and numerical results validate that, compared with baseline schemes, the proposed MDP model with DRL based scheme can achieve better wireless energy and data transfer strategies in terms of the higher long-term utility of the UAV.","2332-7731","","10.1109/TCCN.2020.3027696","National Natural Science Foundation of China(grant numbers:62071343); National Research Foundation (NRF), Singapore, under Singapore Energy Market Authority (EMA), Energy Resilience(grant numbers:NRF2017EWT-EP003-041); Singapore(grant numbers:NRF2015-NRF-ISF001-2277); Singapore NRF National Satellite of Excellence, Design Science and Technology for Secure Critical Infrastructure NSoE DeSTSCI2019-0007, A*STAR-NTU-SUTD Joint Research Grant on Artificial Intelligence for the Future of Manufacturing RGANS1906, Wallenberg AI, Autonomous Systems and Software Program and Nanyang Technological University (WASP/NTU)(grant numbers:M4082187 (4080)); Singapore Ministry of Education (MOE) Tier 1(grant numbers:RG16/20); Alibaba Group through Alibaba Innovative Research (AIR) Program and Alibaba-NTU Singapore Joint Research Institute (JRI); Open Research Project of the State Key Laboratory of Industrial Control Technology, Zhejiang University, China(grant numbers:ICT20044); Alibaba Group through Alibaba Innovative Research (AIR) Program and Alibaba-NTU Singapore Joint Research Institute (JRI), Nanyang Technological University, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9209109","Unmanned aerial vehicle;wireless energy transfer;Internet of Things;Markov decision process;deep reinforcement learning","Wireless communication;Unmanned aerial vehicles;Wireless sensor networks;Charging stations;Internet of Things;Markov processes;Data models","autonomous aerial vehicles;Internet of Things;iterative methods;learning (artificial intelligence);Markov processes;optimisation","UAV-assisted wireless energy;future generation communication network applications;autonomous data delivery;massive machine type communication;Things devices;direct communication channels;IoT devices;wireless energy transfer technique;Markov decision process model;long-term utility;MDP model;transferred energy;on-device battery energy storage;UAV-assisted communication systems;deep reinforcement learning schemes","","51","","45","IEEE","29 Sep 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Massive Access Management for Ultra-Reliable Low-Latency Communications","H. Yang; Z. Xiong; J. Zhao; D. Niyato; C. Yuen; R. Deng","School of Computer Science and Engineering, Nanyang Technological University, Singapore; Alibaba-NTU Joint Research Institute, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Engineering Product Development (EPD) Pillar, Singapore University of Technology and Design, Singapore; College of Control Science and Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Wireless Communications","7 May 2021","2021","20","5","2977","2990","With the rapid deployment of the Internet of Things (IoT), fifth-generation (5G) and beyond 5G networks are required to support massive access of a huge number of devices over limited radio spectrum radio. In wireless networks, different devices have various quality-of-service (QoS) requirements, ranging from ultra-reliable low latency communications (URLLC) to high transmission data rates. In this context, we present a joint energy-efficient subchannel assignment and power control approach to manage massive access requests while maximizing network energy efficiency (EE) and guaranteeing different QoS requirements. The latency constraint is transformed into a data rate constraint which makes the optimization problem tractable before modelling it as a multi-agent reinforcement learning problem. A distributed cooperative massive access approach based on deep reinforcement learning (DRL) is proposed to address the problem while meeting both reliability and latency constraints on URLLC services in massive access scenario. In addition, transfer learning and cooperative learning mechanisms are employed to enable communication links to work cooperatively in a distributed manner, which enhances the network performance and access success probability. Simulation results clearly show that the proposed distributed cooperative learning approach outperforms other existing approaches in terms of meeting EE and improving the transmission success probability in massive access scenario.","1558-2248","","10.1109/TWC.2020.3046262","Nanyang Technological University (NTU) Startup Grant; Singapore, Ministry of Education Academic Research Fund(grant numbers:Tier 1 RG128/18,Tier 1 RG115/19,Tier 1 RT07/19,Tier 1 RT01/19,Tier 2 MOE2019-T2-1-176); NTU-WASP Joint Project, Singapore, National Research Foundation under its Strategic Capability Research Centers Funding Initiative: Strategic Center for Research in Privacy-Preserving Technologies & Systems, Energy Research Institute @NTU, Singapore NRF National Satellite of Excellence, Design Science and Technology for Secure Critical Infrastructure(grant numbers:NSoE DeST-SCI2019-0012); AI Singapore 100 Experiments (100E) programme, NTU Project for Large Vertical Take-Off & Landing Research Platform, National Research Foundation (NRF), Singapore, through Singapore Energy Market Authority (EMA), Energy Resilience(grant numbers:NRF2017EWT-EP003-041,NRF2015-NRF-ISF001-2277); Singapore NRF National Satellite of Excellence, Design Science and Technology for Secure Critical Infrastructure(grant numbers:NSoE DeST-SCI2019-0007); A*STARNTU-SUTD Joint Research Grant on Artificial Intelligence for the Future of Manufacturing(grant numbers:RGANS1906); Wallenberg AI, Autonomous Systems and Software Program and Nanyang Technological University (WASP/NTU)(grant numbers:M4082187 (4080)); Singapore Ministry of Education (MOE)(grant numbers:Tier 1 (RG16/20)); Open Research Project of the State Key Laboratory of Industrial Control Technology, Zhejiang University, China(grant numbers:ICT20044); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9311792","Wireless networks;massive access;URLLC;spectrum and power management;multi-agent reinforcement learning;deep distributed cooperative learning","Learning systems;Wireless networks;Simulation;Transfer learning;Power control;Quality of service;Reinforcement learning","5G mobile communication;cooperative communication;deep learning (artificial intelligence);Internet of Things;multi-agent systems;optimisation;power control;quality of service;telecommunication computing;telecommunication control;telecommunication network reliability;telecommunication power management","distributed cooperative learning approach;Internet of Things;beyond 5G networks;quality-of-service;QoS requirements;optimization problem;distributed cooperative massive access approach;reliability constraints;latency constraints;access success probability;network performance;communication links;cooperative learning mechanisms;transfer learning;massive access scenario;URLLC services;multiagent reinforcement learning problem;optimization problem tractable;data rate constraint;latency constraint;network energy efficiency;massive access requests;power control approach;joint energy-efficient subchannel assignment;high transmission data rates;ultra-reliable low latency communications;quality-of-service requirements;wireless networks;radio spectrum radio;fifth-generation networks;ultra-reliable low-latency communications;massive access management;deep reinforcement learning","","33","","43","IEEE","31 Dec 2020","","","IEEE","IEEE Journals"
"Robust Lane Change Decision Making for Autonomous Vehicles: An Observation Adversarial Reinforcement Learning Approach","X. He; H. Yang; Z. Hu; C. Lv","School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Intelligent Vehicles","23 Jan 2023","2023","8","1","184","193","Reinforcementlearning holds the promise of allowing autonomous vehicles to learn complex decision making behaviors through interacting with other traffic participants. However, many real-world driving tasks involve unpredictable perception errors or measurement noises which may mislead an autonomous vehicle into making unsafe decisions, even cause catastrophic failures. In light of these risks, to ensure safety under perception uncertainty, autonomous vehicles are required to be able to cope with the worst case observation perturbations. Therefore, this paper proposes a novel observation adversarial reinforcement learning approach for robust lane change decision making of autonomous vehicles. A constrained observation-robust Markov decision process is presented to model lane change decision making behaviors of autonomous vehicles under policy constraints and observation uncertainties. Meanwhile, a black-box attack technique based on Bayesian optimization is implemented to approximate the optimal adversarial observation perturbations efficiently. Furthermore, a constrained observation-robust actor-critic algorithm is advanced to optimize autonomous driving lane change policies while keeping the variations of the policies attacked by the optimal adversarial observation perturbations within bounds. Finally, the robust lane change decision making approach is evaluated in three stochastic mixed traffic flows based on different densities. The results demonstrate that the proposed method can not only enhance the performance of an autonomous vehicle but also improve the robustness of lane change policies against adversarial observation perturbations.","2379-8904","","10.1109/TIV.2022.3165178","Schaeffler Hub for Advanced Research; Smart Mechatronic Lab for Industrial Collaborative Robotics in Manufacturing(grant numbers:I2001E0067 Schaeffler - PA5,I2001E0067 IAF-ICP - PA5); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9750867","Adversarial attack;autonomous vehicle;lane change decision making;reinforcement learning;robust decision making","Decision making;Autonomous vehicles;Perturbation methods;Optimization;Bayes methods;Task analysis;Safety","Bayes methods;decision making;learning (artificial intelligence);Markov processes;reinforcement learning;road traffic;traffic engineering computing","autonomous driving lane change policies;autonomous vehicle;complex decision making behaviors;constrained observation-robust actor-critic algorithm;constrained observation-robust Markov decision process;model lane change decision;novel observation adversarial reinforcement learning approach;optimal adversarial observation perturbations;robust lane change decision making approach;worst case observation perturbations","","26","","40","IEEE","6 Apr 2022","","","IEEE","IEEE Journals"
"Optimal and Low-Complexity Dynamic Spectrum Access for RF-Powered Ambient Backscatter System With Online Reinforcement Learning","N. Van Huynh; D. T. Hoang; D. N. Nguyen; E. Dutkiewicz; D. Niyato; P. Wang","School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Electrical Engineering & Computer Science, York University, Toronto, ON, Canada","IEEE Transactions on Communications","14 Aug 2019","2019","67","8","5736","5752","Ambient backscatter has been introduced with a wide range of applications for low power wireless communications. In this paper, we propose an optimal and low-complexity dynamic spectrum access framework for the RF-powered ambient backscatter system. In this system, the secondary transmitter not only harvests energy from ambient signals but also reflects these signals to transmit its modulated data to the receiver. Under the dynamics of the ambient signals, we first adopt the Markov decision process (MDP) framework to obtain the optimal policy for the secondary transmitter, aiming to maximize the system throughput. However, the MDP-based optimization requires complete knowledge of environment parameters, e.g., the probability of a channel to be idle and the probability of a successful packet transmission, that may not be practical to obtain. To cope with such incomplete knowledge of the environment, we develop a low-complexity online reinforcement learning algorithm that allows the secondary transmitter to “learn” from its decisions and then attain the optimal policy. Simulation results show that the proposed learning algorithm not only efficiently deals with the dynamics of the environment but also improves the average throughput up to 50% and reduces the blocking probability and delay up to 80% compared with conventional methods.","1558-0857","","10.1109/TCOMM.2019.2913871","A*STAR-NTU-SUTD Joint Research Grant Call on Artificial Intelligence for the Future of Manufacturing(grant numbers:RGANS1906,WASP/NTU M4082187 (4080)); Singapore MOE Tier 1(grant numbers:2017-T1-002-007 RG122/17); MOE Tier 2(grant numbers:MOE2014-T2-2-015 ARC4/15); Singapore(grant numbers:NRF2015-NRF-ISF001-2277); Singapore EMA Energy Resilience(grant numbers:NRF2017EWT-EP003-041); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703118","Ambient backscatter;RF energy harvesting;dynamic spectrum access;Markov decision process;reinforcement learning","Backscatter;Heuristic algorithms;Receivers;Throughput;Optimization;Radio frequency;Energy harvesting","backscatter;learning (artificial intelligence);Markov processes;optimisation;probability;radio spectrum management","RF-powered ambient backscatter system;online reinforcement learning;reinforcement learning algorithm;MDP-based optimization;system throughput;optimal policy;Markov decision process framework;ambient signals;secondary transmitter;low-complexity dynamic spectrum access framework;optimal complexity dynamic spectrum access framework","","22","","46","IEEE","30 Apr 2019","","","IEEE","IEEE Journals"
