@inproceedings{10.1145/3576841.3585919,
author = {Wang, Yixuan and Zhan, Simon and Wang, Zhilu and Huang, Chao and Wang, Zhaoran and Yang, Zhuoran and Zhu, Qi},
title = {Joint Differentiable Optimization and Verification for Certified Reinforcement Learning},
year = {2023},
isbn = {9798400700361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576841.3585919},
doi = {10.1145/3576841.3585919},
abstract = {Model-based reinforcement learning has been widely studied for controller synthesis in cyber-physical systems (CPSs). In particular, for safety-critical CPSs, it is important to formally certify system properties (e.g., safety, stability) under the learned RL controller. However, as existing methods typically conduct formal verification after the controller has been learned, it is often difficult to obtain any certificate, even after many iterations between learning and verification. To address this challenge, we propose a framework that jointly conducts reinforcement learning and formal verification by formulating and solving a novel bilevel optimization problem, which is end-to-end differentiable by the gradients from the value function and certificates formulated by linear programs and semi-definite programs. In experiments, our framework is compared with a baseline model-based stochastic value gradient (SVG) method and its extension to solve constrained Markov Decision Processes (CMDPs) for safety. The results demonstrate the significant advantages of our framework in finding feasible controllers with certificates, i.e., Barrier functions and Lyapunov functions that formally ensure system safety and stability, available on Github.},
booktitle = {Proceedings of the ACM/IEEE 14th International Conference on Cyber-Physical Systems (with CPS-IoT Week 2023)},
pages = {132–141},
numpages = {10},
keywords = {RL, lyapunov function, stability, barrier function, safety},
location = {San Antonio, TX, USA},
series = {ICCPS '23}
}

@inproceedings{10.1145/3604915.3610641,
author = {Xi, Xumei and Zhao, Yuke and Liu, Quan and Ouyang, Liwen and Wu, Yang},
title = {Integrating Offline Reinforcement Learning with Transformers for Sequential Recommendation},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3610641},
doi = {10.1145/3604915.3610641},
abstract = {We consider the problem of sequential recommendation, where the current recommendation is made based on past interactions. This recommendation task requires efficient processing of the sequential data and aims to provide recommendations that maximize the long-term reward. To this end, we train a farsighted recommender by using an offline RL algorithm with the policy network in our model architecture that has been initialized from a pre-trained transformer model. The pre-trained model leverages the superb ability of the transformer to process sequential information. Compared to prior works that rely on online interaction via simulation, we focus on implementing a fully offline RL framework that is able to converge in a fast and stable way. Through extensive experiments on public datasets, we show that our method is robust across various recommendation regimes, including e-commerce and movie suggestions. Compared to state-of-the-art supervised learning algorithms, our algorithm yields recommendations of higher quality, demonstrating the clear advantage of combining RL and transformers.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {1},
numpages = {1},
keywords = {recommender systems, transformers, reinforcement learning},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@article{10.1145/3446617,
author = {Yao, Jing and Dou, Zhicheng and Xu, Jun and Wen, Ji-Rong},
title = {RLPS: A Reinforcement Learning–Based Framework for Personalized Search},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3446617},
doi = {10.1145/3446617},
abstract = {Personalized search is a promising way to improve search qualities by taking user interests into consideration. Recently, machine learning and deep learning techniques have been successfully applied to search result personalization. Most existing models simply regard the personal search history as a static set of user behaviors and learn fixed ranking strategies based on all the recorded data. Though improvements have been achieved, the essence that the search process is a sequence of interactions between the search engine and user is ignored. The user’s interests may dynamically change during the search process, therefore, it would be more helpful if a personalized search model could track the whole interaction process and adjust its ranking strategy continuously. In this article, we adapt reinforcement learning to personalized search and propose a framework, referred to as RLPS. It utilizes a Markov Decision Process (MDP) to track sequential interactions between the user and search engine, and continuously update the underlying personalized ranking model with the user’s real-time feedback to learn the user’s dynamic interests. Within this framework, we implement two models: the listwise RLPS-L and the hierarchical RLPS-H. RLPS-L interacts with users and trains the ranking model with document lists, while RLPS-H improves model training by designing a layered structure and introducing document pairs. In addition, we also design a feedback-aware personalized ranking component to capture the user’s feedback, which impacts the user interest profile for the next query. Significant improvements over existing personalized search models are observed in the experiments on the public AOL search log and a commercial log.},
journal = {ACM Trans. Inf. Syst.},
month = {may},
articleno = {27},
numpages = {29},
keywords = {Personalized search, reinforcement learning, Markov decision process (MDP)}
}

@inproceedings{10.1145/3459637.3482203,
author = {Ding, Kaize and Shan, Xuan and Liu, Huan},
title = {Towards Anomaly-Resistant Graph Neural Networks via Reinforcement Learning},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482203},
doi = {10.1145/3459637.3482203},
abstract = {In general, graph neural networks (GNNs) adopt the message-passing scheme to capture the information of a node (i.e., nodal attributes, and local graph structure) by iteratively transforming, aggregating the features of its neighbors. Nonetheless, recent studies show that the performance of GNNs can be easily hampered by the existence of abnormal or malicious nodes due to the vulnerability of neighborhood aggregation. Thus it is necessary to learn anomaly-resistant GNNs without the prior knowledge of ground-truth anomalies, given the fact that labeling anomalies is costly and requires intensive domain knowledge. Though removing anomalies through unsupervised anomaly detection methods could be a possible solution, it may render unreasonable GNN model performance on target tasks due to the non-differentiable gap between the two learning procedures. In order to keep the effectiveness of GNNs on anomaly-contaminated graphs, in this paper, we propose a new framework named RARE-GNN (Reinforced Anomaly-REsistant Graph Neural Networks) which can detect anomalies from the input graph and learn anomaly-resistant GNNs simultaneously. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed framework.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {2979–2983},
numpages = {5},
keywords = {graph neural networks, reinforcement learning, anomaly detection},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3219819.3219886,
author = {Zhao, Xiangyu and Zhang, Liang and Ding, Zhuoye and Xia, Long and Tang, Jiliang and Yin, Dawei},
title = {Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219886},
doi = {10.1145/3219819.3219886},
abstract = {Recommender systems play a crucial role in mitigating the problem of information overload by suggesting users' personalized items or services. The vast majority of traditional recommender systems consider the recommendation procedure as a static process and make recommendations following a fixed strategy. In this paper, we propose a novel recommender system with the capability of continuously improving its strategies during the interactions with users. We model the sequential interactions between users and a recommender system as a Markov Decision Process (MDP) and leverage Reinforcement Learning (RL) to automatically learn the optimal strategies via recommending trial-and-error items and receiving reinforcements of these items from users' feedback. Users' feedback can be positive and negative and both types of feedback have great potentials to boost recommendations. However, the number of negative feedback is much larger than that of positive one; thus incorporating them simultaneously is challenging since positive feedback could be buried by negative one. In this paper, we develop a novel approach to incorporate them into the proposed deep recommender system (DEERS) framework. The experimental results based on real-world e-commerce data demonstrate the effectiveness of the proposed framework. Further experiments have been conducted to understand the importance of both positive and negative feedback in recommendations.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {1040–1048},
numpages = {9},
keywords = {recommender system, pairwise deep Q-network, deep reinforcement learning},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{10.5555/2188385.2343705,
author = {Tamar, Aviv and Di Castro, Dotan and Meir, Ron},
title = {Integrating a Partial Model into Model Free Reinforcement Learning},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
number = {null},
issn = {1532-4435},
abstract = {In reinforcement learning an agent uses online feedback from the environment in order to adaptively select an effective policy. Model free approaches address this task by directly mapping environmental states to actions, while model based methods attempt to construct a model of the environment, followed by a selection of optimal actions based on that model. Given the complementary advantages of both approaches, we suggest a novel procedure which augments a model free algorithm with a partial model. The resulting hybrid algorithm switches between a model based and a model free mode, depending on the current state and the agent's knowledge. Our method relies on a novel definition for a partially known model, and an estimator that incorporates such knowledge in order to reduce uncertainty in stochastic approximation iterations. We prove that such an approach leads to improved policy evaluation whenever environmental knowledge is available, without compromising performance when such knowledge is absent. Numerical simulations demonstrate the effectiveness of the approach on policy gradient and Q-learning algorithms, and its usefulness in solving a call admission control problem.},
journal = {J. Mach. Learn. Res.},
month = {jun},
pages = {1927–1966},
numpages = {40},
keywords = {markov decision processes, hybrid model based model free algorithms, temporal difference, stochastic approximation, reinforcement learning}
}

@inproceedings{10.5555/3545946.3598703,
author = {Wu, Haochen and Sequeira, Pedro and Pynadath, David V.},
title = {Multiagent Inverse Reinforcement Learning via Theory of Mind Reasoning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We approach the problem of understanding how people interact with each other in collaborative settings, especially when individuals know little about their teammates, via Multiagent Inverse Reinforcement Learning (MIRL), where the goal is to infer the reward functions guiding the behavior of each individual given trajectories of a team's behavior during some task. Unlike current MIRL approaches, we do not assume that team members know each other's goals a priori; rather, that they collaborate by adapting to the goals of others perceived by observing their behavior, all while jointly performing a task. To address this problem, we propose a novel approach to MIRL via Theory of Mind (MIRL-ToM). For each agent, we first use ToM reasoning to estimate a posterior distribution over baseline reward profiles given their demonstrated behavior. We then perform MIRL via decentralized equilibrium by employing single-agent Maximum Entropy IRL to infer a reward function for each agent, where we simulate the behavior of other teammates according to the time-varying distribution over profiles. We evaluate our approach in a simulated 2-player search-and-rescue operation where the goal of the agents, playing different roles, is to search for and evacuate victims in the environment. Our results show that the choice of baseline profiles is paramount to the recovery of the ground-truth rewards, and that MIRL-ToM is able to recover the rewards used by agents interacting both with known and unknown teammates.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {708–716},
numpages = {9},
keywords = {decentralized equilibrium, theory of mind, inverse reinforcement learning, cooperation, multiagent systems},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3352593.3352625,
author = {Kaushik, Meha and Singhania, Nirvan and S., Phaniteja and Krishna, K. Madhava},
title = {Parameter Sharing Reinforcement Learning Architecture for Multi Agent Driving},
year = {2020},
isbn = {9781450366502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352593.3352625},
doi = {10.1145/3352593.3352625},
abstract = {Multi-agent learning provides a potential solution for frameworks to learn and simulate traffic behaviors. This paper proposes a novel architecture to learn multiple driving behaviors in a traffic scenario. The proposed architecture can learn multiple behaviors independently as well as simultaneously. We take advantage of the homogeneity of agents and learn in a parameter sharing paradigm. To further speed up the training process asynchronous updates are employed into the architecture. While learning different behaviors simultaneously, the given framework was also able to learn cooperation between the agents, without any explicit communication. We applied this framework to learn two important behaviors in driving: 1) Lane-Keeping and 2) Over-Taking. Results indicate faster convergence and learning of a more generic behavior, that is scalable to any number of agents. When compared the results with existing approaches, our results indicate equal and even better performance in some cases.},
booktitle = {Proceedings of the Advances in Robotics 2019},
articleno = {31},
numpages = {7},
keywords = {Multi-agents, Autonomous vehicles, Parameter sharing, Reinforcement learning},
location = {Chennai, India},
series = {AIR 2019}
}

@inproceedings{10.5555/3545946.3599013,
author = {Querido, Gon\c{c}alo and Sardinha, Alberto and Melo, Francisco S.},
title = {Learning to Perceive in Deep Model-Free Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This work proposes a novel model-free Reinforcement Learning (RL) agent that is able to learn how to complete an unknown task by having access to only a part of the input observation. We extend the recurrent attention model (RAM) and combine it with the proximal policy optimization (PPO) algorithm. Despite the visual limitation, we show that our model matches the performance of PPO+LSTM in two of the three games tested.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2595–2597},
numpages = {3},
keywords = {attention mechanism, hard attention, model-free, reinforcement learning, active perception, visual attention},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3463952.3464143,
author = {Goindani, Mahak and Neville, Jennifer},
title = {Towards Decentralized Social Reinforcement Learning via Ego-Network Extrapolation},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this work, we consider the problem of multi-agent reinforcement learning in directed social networks with a large number of agents. Network dependencies among user activities impact the reward for individual actions and need to be incorporated into policy learning, however, directed interactions entail that the network is partially observable to each user. When estimating policies locally, the insufficient state information makes it challenging for users to effectively learn network dependencies. To address this, we use parameter sharing and ego-network extrapolation in a decentralized policy learning and execution framework. This is in contrast to previous work on social RL that assumes a centralized controller to capture inter-agent dependencies for joint policy learning. We evaluate our proposed approach on Twitter datasets and show that our decentralized learning approach achieves performance nearly equivalent to that of centralized learning approach and superior performance to other baselines.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1512–1514},
numpages = {3},
keywords = {multi-agent learning, social networks, reinforcement learning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3397271.3401200,
author = {Chen, Limin and Tang, Zhiwen and Yang, Grace Hui},
title = {Balancing Reinforcement Learning Training Experiences in Interactive Information Retrieval},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401200},
doi = {10.1145/3397271.3401200},
abstract = {Interactive Information Retrieval (IIR) and Reinforcement Learning (RL) share many commonalities, including an agent who learns while interacts, a long-term and complex goal, and an algorithm that explores and adapts. To successfully apply RL methods to IIR, one challenge is to obtain sufficient relevance labels to train the RL agents, which are infamously known as sample inefficient. However, in a text corpus annotated for a given query, it is not the relevant documents but the irrelevant documents that predominate. This would cause very unbalanced training experiences for the agent and prevent it from learning any policy that is effective. Our paper addresses this issue by using domain randomization to synthesize more relevant documents for the training. Our experimental results on the Text REtrieval Conference (TREC) Dynamic Domain (DD) 2017 Track show that the proposed method is able to boost an RL agent's learning effectiveness by 22\% in dealing with unseen situations.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1525–1528},
numpages = {4},
keywords = {deep reinforcement learning, dynamic search, interactive IR},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.1145/3243064.3243067,
author = {Bougie, Nicolas and Cheng, Li Kai and Ichise, Ryutaro},
title = {Combining Deep Reinforcement Learning with Prior Knowledge and Reasoning},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/3243064.3243067},
doi = {10.1145/3243064.3243067},
abstract = {Recent improvements in deep reinforcement learning have allowed to solve problems in many 2D domains such as Atari games. However, in complex 3D environments, numerous learning episodes are required which may be too time consuming or even impossible especially in real-world scenarios. We present a new architecture to combine external knowledge and deep reinforcement learning using only visual input. A key concept of our system is augmenting image input by adding environment feature information and combining two sources of decision. We evaluate the performances of our method in 3D partially-observable environments from the Microsoft Malmo platform. Experimental evaluation exhibits higher performance and faster learning compared to a single reinforcement learning model.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {jul},
pages = {33–45},
numpages = {13},
keywords = {reinforcement learning, deep learning, external knowledge, object recognition, knowledge reasoning}
}

@inproceedings{10.5555/3463952.3464094,
author = {Sim\~{a}o, Thiago D. and Jansen, Nils and Spaan, Matthijs T. J.},
title = {AlwaysSafe: Reinforcement Learning without Safety Constraint Violations during Training},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Deploying reinforcement learning (RL) involves major concerns around safety. Engineering a reward signal that allows the agent to maximize its performance while remaining safe is not trivial. Safe RL studies how to mitigate such problems. For instance, we can decouple safety from reward using constrained Markov decision processes (CMDPs), where an independent signal models the safety aspects. In this setting, an RL agent can autonomously find tradeoffs between performance and safety. Unfortunately, most RL agents designed for CMDPs only guarantee safety after the learning phase, which might prevent their direct deployment. In this work, we investigate settings where a concise abstract model of the safety aspects is given, a reasonable assumption since a thorough understanding of safety-related matters is a prerequisite for deploying RL in typical applications. Factored CMDPs provide such compact models when a small subset of features describe the dynamics relevant for the safety constraints. We propose an RL algorithm that uses this abstract model to learn policies for CMDPs safely, that is without violating the constraints. During the training process, this algorithm can seamlessly switch from a conservative policy to a greedy policy without violating the safety constraints. We prove that this algorithm is safe under the given assumptions. Empirically, we show that even if safety and reward signals are contradictory, this algorithm always operates safely and, when they are aligned, this approach also improves the agent's performance.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1226–1235},
numpages = {10},
keywords = {CMDP, safe reinforcement learning, reinforcement learning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@article{10.1145/3404197,
author = {Raza, Syed Ali and Williams, Mary-Anne},
title = {Human Feedback as Action Assignment in Interactive Reinforcement Learning},
year = {2020},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3404197},
doi = {10.1145/3404197},
abstract = {Teaching by demonstrations and teaching by assigning rewards are two popular methods of knowledge transfer in humans. However, showing the right behaviour (by demonstration) may appear more natural to a human teacher than assessing the learner’s performance and assigning a reward or punishment to it. In the context of robot learning, the preference between these two approaches has not been studied extensively. In this article, we propose a method that replaces the traditional method of reward assignment with action assignment (which is similar to providing a demonstration) in interactive reinforcement learning. The main purpose of the suggested action is to compute a reward by seeing if the suggested action was followed by the self-acting agent or not. We compared action assignment with reward assignment via a user study conducted over the web using a two-dimensional maze game. The logs of interactions showed that action assignment significantly improved users’ ability to teach the right behaviour. The survey results showed that both action and reward assignment seemed highly natural and usable, reward assignment required more mental effort, repeatedly assigning rewards and seeing the agent disobey commands caused frustration in users, and many users desired to control the agent’s behaviour directly.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {aug},
articleno = {14},
numpages = {24},
keywords = {learning from human teachers, reward shaping, reinforcement learning, Interactive machine learning}
}

@inproceedings{10.5555/2615731.2615762,
author = {Bogert, Kenneth and Doshi, Prashant},
title = {Multi-Robot Inverse Reinforcement Learning under Occlusion with Interactions},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We consider the problem of learning the behavior of multiple mobile robots executing fixed trajectories in a common space and possibly interacting with each other in their execution. The mobile robots are observed by a subject robot from a vantage point from which it can observe a portion of their trajectories only. This problem exhibits wide-ranging applications and the specific application we consider here is that of the subject robot who desires to penetrate a simple perimeter patrol by two interacting robots and reach a goal location. Our approach extends single-agent inverse reinforcement learning (IRL) to a multi-robot setting and partial observability, and models the interaction between the mobile robots as equilibrium behavior. IRL provides weights over the features of the robots' reward functions, thereby allowing us to learn their preferences. Subsequently, we derive a Markov decision process based policy for each other robot. We extend a predominant IRL technique and empirically evaluate its performance in our application setting. We show that our approach in the application setting results in significant improvement in the subject's ability to predict the patroller positions at different points in time with a corresponding increase in its successful penetration rate.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {173–180},
numpages = {8},
keywords = {inverse reinforcement, patrolling, machine learning, multi-robot systems},
location = {Paris, France},
series = {AAMAS '14}
}

@inproceedings{10.5555/3306127.3331967,
author = {Diddigi, Raghuram Bharadwaj and Reddy, D. Sai Koti and K.J., Prabuchandran and Bhatnagar, Shalabh},
title = {Actor-Critic Algorithms for Constrained Multi-Agent Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent reinforcement learning has gained lot of popularity primarily owing to the success of deep function approximation architectures. However, many real-life multi-agent applications often impose constraints on the joint action sequence that can be taken by the agents. In this work, we formulate such problems in the framework of constrained cooperative stochastic games. Under this setting, the goal of the agents is to obtain joint action sequence that minimizes a total cost objective criterion subject to total cost penalty/budget functional constraints. To this end, we utilize the Lagrangian formulation and propose actor-critic algorithms. Through experiments on a constrained multi-agent grid world task, we demonstrate that our algorithms converge to near-optimal joint action sequences satisfying the given constraints.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1931–1933},
numpages = {3},
keywords = {actor-critic algorithms, multi-agent learning, constrained reinforcement learning, cooperative stochastic game},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3488560.3498526,
author = {Montazeralghaem, Ali and Allan, James},
title = {Learning Relevant Questions for Conversational Product Search Using Deep Reinforcement Learning},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498526},
doi = {10.1145/3488560.3498526},
abstract = {We propose RelQuest, a conversational product search model based on reinforcement learning to generate questions from product descriptions in each round of the conversation, directly maximizing any desired metrics (i.e., the ultimate goal of the conversation), objectives, or even an arbitrary user satisfaction signal. By enabling systems to ask questions about user needs, conversational product search has gained increasing attention in recent years. Asking the right questions through conversations helps the system collect valuable feedback to create better user experiences and ultimately increase sales. In contrast, existing conversational product search methods are based on an assumption that there is a set of effectively pre-defined candidate questions for each product to be asked. Moreover, they make strong assumptions to estimate the value of questions in each round of the conversation. Estimating the true value of questions in each round of the conversation is not trivial since it is unknown. Experiments on real-world user purchasing data show the effectiveness of RelQuest at generating questions that maximize standard evaluation measures such as NDCG.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {746–754},
numpages = {9},
keywords = {intelligent assistants, reinforcement learning, conversational product search, relevant question},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3514221.3526181,
author = {Chunduri, Pramod and Bang, Jaeho and Lu, Yao and Arulraj, Joy},
title = {Zeus: Efficiently Localizing Actions in Videos Using Reinforcement Learning},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3526181},
doi = {10.1145/3514221.3526181},
abstract = {Detection and localization of actions in videos is an important problem in practice. State-of-the-art video analytics systems are unable to efficiently and effectively answer such action queries because actions often involve a complex interaction between objects and are spread across a sequence of frames; detecting and localizing them requires computationally expensive deep neural networks. It is also important to consider the entire sequence of frames to answer the query effectively.In this paper, we present ZEUS, a video analytics system tailored for answering action queries. We present a novel technique for efficiently answering these queries using deep reinforcement learning. ZEUS trains a reinforcement learning agent that learns to adaptively modify the input video segments that are subsequently sent to an action classification network. The agent alters the input segments along three dimensions - sampling rate, segment length, and resolution. To meet the user-specified accuracy target, ZEUS's query optimizer trains the agent based on an accuracy-aware, aggregate reward function. Evaluation on three diverse video datasets shows that ZEUS outperforms state-of-the-art frame- and window-based filtering techniques by up to 22.1x and 4.7x, respectively. It also consistently meets the user-specified accuracy target across all queries.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {545–558},
numpages = {14},
keywords = {video database management systems, video analytics, reinforcement learning, action localization},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@article{10.1613/jair.1.13833,
author = {Ma, Xiaoteng and Ma, Shuai and Xia, Li and Zhao, Qianchuan},
title = {Mean-Semivariance Policy Optimization via Risk-Averse Reinforcement Learning},
year = {2022},
issue_date = {Dec 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {75},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13833},
doi = {10.1613/jair.1.13833},
abstract = {Keeping risk under control is often more crucial than maximizing expected reward in real-world decision-making situations, such as finance, robotics, autonomous driving, etc. The most natural choice of risk measures is variance, while it penalizes the upside volatility as much as the downside part. Instead, the (downside) semivariance, which captures the negative deviation of a random variable under its mean, is more suitable for risk-averse proposes. This paper aims at optimizing the mean-semivariance (MSV) criterion in reinforcement learning w.r.t. steady rewards. Since semivariance is time-inconsistent and does not satisfy the standard Bellman equation, the traditional dynamic programming methods are inapplicable to MSV problems directly. To tackle this challenge, we resort to the Perturbation Analysis (PA) theory and establish the performance difference formula for MSV. We reveal that the MSV problem can be solved by iteratively solving a sequence of RL problems with a policy-dependent reward function. Further, we propose two on-policy algorithms based on the policy gradient theory and the trust region method. Finally, we conduct diverse experiments from simple bandit problems to continuous control tasks in MuJoCo, which demonstrate the effectiveness of our proposed methods.},
journal = {J. Artif. Int. Res.},
month = {dec},
numpages = {27}
}

@inproceedings{10.1145/3534678.3539211,
author = {Wang, Haozhe and Du, Chao and Fang, Panyan and Yuan, Shuo and He, Xuming and Wang, Liang and Zheng, Bo},
title = {ROI-Constrained Bidding via Curriculum-Guided Bayesian Reinforcement Learning},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539211},
doi = {10.1145/3534678.3539211},
abstract = {Real-Time Bidding (RTB) is an important mechanism in modern online advertising systems. Advertisers employ bidding strategies in RTB to optimize their advertising effects subject to various financial requirements, especially the return-on-investment (ROI) constraint. ROIs change non-monotonically during the sequential bidding process, and often induce a see-saw effect between constraint satisfaction and objective optimization. While some existing approaches show promising results in static or mildly changing ad markets, they fail to generalize to highly dynamic ad markets with ROI constraints, due to their inability to adaptively balance constraints and objectives amidst non-stationarity and partial observability. In this work, we specialize in ROI-Constrained Bidding in non-stationary markets. Based on a Partially Observable Constrained Markov Decision Process, our method exploits an indicator-augmented reward function free of extra trade-off parameters and develops a Curriculum-Guided Bayesian Reinforcement Learning (CBRL) framework to adaptively control the constraint-objective trade-off in non-stationary ad markets. Extensive experiments on a large-scale industrial dataset with two problem settings reveal that CBRL generalizes well in both in-distribution and out-of-distribution data regimes, and enjoys superior learning efficiency and stability.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4021–4031},
numpages = {11},
keywords = {online advertising, bayesian learning, reinforcement learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3487075.3487094,
author = {Huang, Conghui and Wang, Chaozhe and Tong, Qi},
title = {Infrared Air Combat Simulation Model for Deep Reinforcement Learning},
year = {2021},
isbn = {9781450389853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487075.3487094},
doi = {10.1145/3487075.3487094},
abstract = {Aiming at the problem of lacking credible and realistic infrared air combat simulation platform for applying the deep reinforcement learning method, this paper explored the design requirements for the construction of simulation system, built the overall architecture of infrared air combat simulation system, described the structure, principle and working process of the fighter jet, infrared air-to-air missile, point source decoy and environment model. The implementation method of the simulation system was given, and the credibility of the system was verified through attack and defense simulation examples and error analysis of missile anti-jamming probability, which indicated that the simulation system can be used for the training and testing of agents based on deep reinforcement learning in infrared air combat scenarios.},
booktitle = {Proceedings of the 5th International Conference on Computer Science and Application Engineering},
articleno = {19},
numpages = {7},
keywords = {Deep reinforcement learning, Agent, Air combat simulation},
location = {Sanya, China},
series = {CSAE '21}
}

@inproceedings{10.1145/3442442.3453148,
author = {Lian, Yijiang and Chen, Zhijie and Pei, Xin and Li, Shuang and Wang, Yifei and Qiu, Yuefeng and Zhang, Zhiheng and Tao, Zhipeng and Yuan, Liang and Guan, Hanju and Zhang, Kefeng and Li, Zhigang and Liu, Xiaochun},
title = {Optimizing AD Pruning of Sponsored Search with Reinforcement Learning},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3453148},
doi = {10.1145/3442442.3453148},
abstract = {Industrial sponsored search system (SSS) can be logically divided into three modules: keywords matching, ad retrieving, and ranking. The ad candidates grow exponentially during ad retrieving. Due to limited latency and computing resources, the candidates have to be pruned earlier. Suppose we set a pruning line to cut SSS into two parts: upstream and downstream. The problem we are going to address is: how to pick out the best K items from N candidates provided by the upstream to maximize the total system’s revenue. Since the industrial downstream is very complicated and updated quickly, a crucial restriction in this problem is that the selection scheme should get adapted to the downstream. In this paper, we propose a novel model-free reinforcement learning approach to fixing this problem. Our approach considers the downstream as a black-box environment, and the agent sequentially selects items and finally feeds into the downstream, where revenue would be estimated and used as a reward to improve the selection policy. The idea has been successfully realized in Baidu’s sponsored search system, and online long time A/B test shows remarkable improvements on revenue.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {123–127},
numpages = {5},
keywords = {global system optimization, reinforcement learning, ad selection, Sponsored search},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1145/3529649,
author = {Hsueh, Yu-Ling and Chou, Tai-Liang},
title = {A Task-Oriented Chatbot Based on LSTM and Reinforcement Learning},
year = {2022},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3529649},
doi = {10.1145/3529649},
abstract = {Thanks to the advancements in deep learning, chatbots are widely used in messaging applications. Undoubtedly, a chatbot is a new way of interaction between humans and machines. However, most of the chatbots act as a simple question answering system that responds with formulated answers. Traditional conversational chatbots usually adopt a retrieval-based model that requires a large amount of conversational data for retrieving various intents. Hence, training a chatbot model that uses low-resource conversational data to generate more diverse dialogues is desirable. We propose a method to build a task-oriented chatbot using a sentence generation model that generates sequences based on the generative adversarial network. The architecture of our model contains a generator that generates a diverse sentence and a discriminator that judges the sentences by comparing the generated and the ground-truth sentences. In the generator, we combine the attention model with the sequence-to-sequence model using hierarchical long short-term memory to extract sentence information. For the discriminator, our reward mechanism assigns low rewards for repeated sentences and high rewards for diverse sentences. Extensive experiments are presented to demonstrate the utility of our model that generates more diverse and information-rich sentences than those of the existing approaches.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
articleno = {19},
numpages = {27},
keywords = {deep learning, natural language processing, chatbots, reinforcement learning, Dialogue generation}
}

@inproceedings{10.5555/3463952.3464113,
author = {Xu, Hang and Wang, Rundong and Raizman, Lev and Rabinovich, Zinovi},
title = {Transferable Environment Poisoning: Training-Time Attack on Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Studying adversarial attacks on Reinforcement Learning (RL) agents has become a key aspect of developing robust, RL-based solutions. Test-time attacks, which target the post-learning performance of an RL agent's policy, have been well studied in both white- and black-box settings. More recently, however, state-of-the-art works have shifted to investigate training-time attacks on RL agents, i.e., forcing the learning process towards a target policy designed by the attacker. Alas, these SOTA works continue to rely on white-box settings and/or use a reward-poisoning approach. In contrast, this paper studies environment-dynamics poisoning attacks at training time. Furthermore, while environment-dynamics poisoning presumes a transfer-learning capable agent, it also allows us to expand our approach to black-box attacks. Our overall framework, inspired by hierarchical RL, seeks the minimal environment-dynamics manipulation that will prompt the momentary policy of the agent to change in a desired manner. We show the attack efficiency by comparing it with the reward-poisoning approach, and empirically demonstrate the transferability of the environment-poisoning attack strategy. Finally, we seek to exploit the transferability of the attack strategy to handle black-box settings.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1398–1406},
numpages = {9},
keywords = {security, reinforcement learning, environment poisoning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3377811.3380399,
author = {Reddy, Sameer and Lemieux, Caroline and Padhye, Rohan and Sen, Koushik},
title = {Quickly Generating Diverse Valid Test Inputs with Reinforcement Learning},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380399},
doi = {10.1145/3377811.3380399},
abstract = {Property-based testing is a popular approach for validating the logic of a program. An effective property-based test quickly generates many diverse valid test inputs and runs them through a parameterized test driver. However, when the test driver requires strict validity constraints on the inputs, completely random input generation fails to generate enough valid inputs. Existing approaches to solving this problem rely on whitebox or greybox information collected by instrumenting the input generator and/or test driver. However, collecting such information reduces the speed at which tests can be executed. In this paper, we propose and study a black-box approach for generating valid test inputs. We first formalize the problem of guiding random input generators towards producing a diverse set of valid inputs. This formalization highlights the role of a guide which governs the space of choices within a random input generator. We then propose a solution based on reinforcement learning (RL), using a tabular, on-policy RL approach to guide the generator. We evaluate this approach, RLCheck, against pure random input generation as well as a state-of-the-art greybox evolutionary algorithm, on four real-world benchmarks. We find that in the same time budget, RLCheck generates an order of magnitude more diverse valid inputs than the baselines.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1410–1421},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3512290.3528767,
author = {Bishop, Jordan T. and Gallagher, Marcus and Browne, Will N.},
title = {Pittsburgh Learning Classifier Systems for Explainable Reinforcement Learning: Comparing with XCS},
year = {2022},
isbn = {9781450392372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512290.3528767},
doi = {10.1145/3512290.3528767},
abstract = {Interest in reinforcement learning (RL) has recently surged due to the application of deep learning techniques, but these connectionist approaches are opaque compared with symbolic systems. Learning Classifier Systems (LCSs) are evolutionary machine learning systems that can be categorised as eXplainable AI (XAI) due to their rule-based nature. Michigan LCSs are commonly used in RL domains as the alternative Pittsburgh systems (e.g. SAMUEL) suffer from complex algorithmic design and high computational requirements; however they can produce more compact/interpretable solutions than Michigan systems. We aim to develop two novel Pittsburgh LCSs to address RL domains: PPL-DL and PPL-ST. The former acts as a "zeroth-level" system, and the latter revisits SAMUEL's core Monte Carlo learning mechanism for estimating rule strength. We compare our two Pittsburgh systems to the Michigan system XCS across deterministic and stochastic FrozenLake environments. Results show that PPL-ST performs on-par or better than PPL-DL and outperforms XCS in the presence of high levels of environmental uncertainty. Rulesets evolved by PPL-ST can achieve higher performance than those evolved by XCS, but in a more parsimonious and therefore more interpretable fashion, albeit with higher computational cost. This indicates that PPL-ST is an LCS well-suited to producing explainable policies in RL domains.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {323–331},
numpages = {9},
keywords = {XAI, XCS, SAMUEL, reinforcement learning, learning classifier systems},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.1145/3490354.3494445,
author = {Lima Paiva, Francisco Caio and Felizardo, Leonardo Kanashiro and Bianchi, Reinaldo Augusto da Costa and Costa, Anna Helena Reali},
title = {Intelligent Trading Systems: A Sentiment-Aware Reinforcement Learning Approach},
year = {2022},
isbn = {9781450391481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490354.3494445},
doi = {10.1145/3490354.3494445},
abstract = {The feasibility of making profitable trades on a single asset on stock exchanges based on patterns identification has long attracted researchers. Reinforcement Learning (RL) and Natural Language Processing have gained notoriety in these single-asset trading tasks, but only a few works have explored their combination. Moreover, some issues are still not addressed, such as extracting market sentiment momentum through the explicit capture of sentiment features that reflect the market condition over time and assessing the consistency and stability of RL results in different situations. Filling this gap, we propose the Sentiment-Aware RL (SentARL) intelligent trading system that improves profit stability by leveraging market mood through an adaptive amount of past sentiment features drawn from textual news. We evaluated SentARL across twenty assets, two transaction costs, and five different periods and initializations to show its consistent effectiveness against baselines. Subsequently, this thorough assessment allowed us to identify the boundary between news coverage and market sentiment regarding the correlation of price-time series above which SentARL's effectiveness is outstanding.},
booktitle = {Proceedings of the Second ACM International Conference on AI in Finance},
articleno = {40},
numpages = {9},
keywords = {sentiment analysis, stock markets, deep reinforcement learning},
location = {Virtual Event},
series = {ICAIF '21}
}

@inproceedings{10.5555/3398761.3398941,
author = {Yang, Jiachen and Borovikov, Igor and Zha, Hongyuan},
title = {Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill Discovery},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Human players in professional team sports achieve high level coordination by dynamically choosing complementary skills and executing primitive actions to perform these skills. As a step toward creating intelligent agents with this capability for fully cooperative multi-agent settings, we propose a two-level hierarchical multi-agent reinforcement learning (MARL) algorithm with unsupervised skill discovery. Agents learn useful and distinct skills at the low level via independent Q-learning, while they learn to select complementary latent skill variables at the high level via centralized multi-agent training with an extrinsic team reward. The set of low-level skills emerges from an intrinsic reward that solely promotes the decodability of latent skill variables from the trajectory of a low-level skill, without the need for hand-crafted rewards for each skill. For scalable decentralized execution, each agent independently chooses latent skill variables and primitive actions based on local observations. Our overall method enables the use of general cooperative MARL algorithms for training high level policies and single-agent RL for training low level skills. Experiments on a stochastic high dimensional team game show the emergence of useful skills and cooperative team play. The interpretability of the learned skills show the promise of the proposed method for achieving human-AI cooperation in team sports games.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1566–1574},
numpages = {9},
keywords = {multi-agent learning, option discovery, hierarchical learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.5555/3306127.3332057,
author = {Sun, Fan-Yun and Chang, Yen-Yu and Wu, Yueh-Hua and Lin, Shou-De},
title = {A Regulation Enforcement Solution for Multi-Agent Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Human behaviors are regularized by a variety of norms or regulations, either to maintain orders or to enhance social welfare. However, if artificially intelligent (AI) agents make decisions on behalf of human beings, it is possible that an AI agent can opt to disobey the regulations (being defective) for self-interests. In this paper, we aim to answer the following question: In a decentralized environment (no centralized authority can control agents), given that not all agents are compliant to regulations at first, can we develop a mechanism such that it is in the self-interest of non-compliant agents to comply after all. We first introduce the problem as Regulation Enforcement and formulate it using reinforcement learning and game theory. Then we propose our solution based on the key idea that although we could not alter how defective agents choose to behave, we can, however, leverage the aggregated power of compliant agents to boycott the defective ones. We conducted simulated experiments on two scenarios: Replenishing Resource Management Dilemma and Diminishing Reward Shaping Enforcement, using deep multi-agent reinforcement learning algorithms. We further use empirical game-theoretic analysis to show that the method alters the resulting empirical payoff matrices in a way that promotes compliance (making mutual compliant a Nash Equilibrium).},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2201–2203},
numpages = {3},
keywords = {multi-agent reinforcement learning, reward shaping, empirical game-theoretic analysis},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3341069.3341082,
author = {Ke, Fengkai and Zhao, Daxing and Sun, Guodong and Feng, Wei},
title = {Precise Evaluation for Continuous Action Control in Reinforcement Learning},
year = {2019},
isbn = {9781450371858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341069.3341082},
doi = {10.1145/3341069.3341082},
abstract = {With the development of deep learning, reinforcement learning also gradually into the eye, reinforcement learning has made remarkable achievements in games, go games and other fields, but most of the control problems involved in these fields or tasks are discrete action control with sufficient rewards. Continuous action control in reinforcement learning is closer to the actual control problem, and is considered as one of the main channels leading to artificial intelligence, so it is also one of the research hotspots of researchers. The traditional continuous control algorithm for reinforcement learning evaluates the network with multiple outputs of a single scalar value. In this paper, an accurate evaluation mechanism and corresponding objective function are proposed to accelerate the reinforcement learning training process. The experimental results show that the accurate evaluation of log-cosh objective function can make the robot arm grasp the task more quickly, converge and complete the training task.},
booktitle = {Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference},
pages = {67–70},
numpages = {4},
keywords = {Reinforcement Learning, Continuous Control, Robot, Precise Evaluation},
location = {Guangzhou, China},
series = {HPCCT '19}
}

@inproceedings{10.5555/3545946.3598772,
author = {Wang, Caroline and Warnell, Garrett and Stone, Peter},
title = {D-Shape: Demonstration-Shaped Reinforcement Learning via Goal-Conditioning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {While combining imitation learning (IL) and reinforcement learning (RL) is a promising way to address poor sample efficiency in autonomous behavior acquisition, methods that do so typically assume that the requisite behavior demonstrations are provided by an expert that behaves optimally with respect to a task reward. If, however, suboptimal demonstrations are provided, a fundamental challenge appears in that the demonstration-matching objective of IL conflicts with the return-maximization objective of RL. This paper introduces D-Shape, a new method for combining IL and RL that uses ideas from reward shaping and goal-conditioned RL to resolve the above conflict. D-Shape allows learning from suboptimal demonstrations while retaining the ability to find the optimal policy with respect to the task reward. We experimentally validate D-Shape in sparse-reward gridworld domains, showing that it both improves over RL in terms of sample efficiency and converges consistently to the optimal policy in the presence of suboptimal demonstrations.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1267–1275},
numpages = {9},
keywords = {suboptimal demonstrations, goal-conditioned reinforcement learning, imitation from observation, reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3468218.3469037,
author = {Carmack, Joseph and Schmidt, Steve and Kuzdeba, Scott},
title = {Multi-Agent Reinforcement Learning Approaches to RF Fingerprint Enhancement},
year = {2021},
isbn = {9781450385619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468218.3469037},
doi = {10.1145/3468218.3469037},
abstract = {Deep learning based RF Fingerprinting has shown great promise for IoT device security. This work explores various multi-agent reinforcement learning approaches to enable RF Fingerprint enhancement for an ensemble of transmitters. A RiftNetTM Reconstruction Model (RRM) is used to learn a latent Wi-Fi signal representation and how to reconstruct from that latent representation at the transmitter such that the reconstruction uniquely excites parts of the front-end to enhance the fingerprint. Deep reinforcement learning is then employed to learn the RRM control policy. Details on the design of the control interface, state representation, and rewards structure are presented for four different policy approaches. The resulting computational and security characteristics are discussed.},
booktitle = {Proceedings of the 3rd ACM Workshop on Wireless Security and Machine Learning},
pages = {67–72},
numpages = {6},
location = {Abu Dhabi, United Arab Emirates},
series = {WiseML '21}
}

@article{10.1613/jair.1.14398,
author = {Xu, Jiawei and Li, Shuxing and Yang, Rui and Yuan, Chun and Han, Lei},
title = {Efficient Multi-Goal Reinforcement Learning via Value Consistency Prioritization},
year = {2023},
issue_date = {Jun 2023},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {77},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.14398},
doi = {10.1613/jair.1.14398},
abstract = {Goal-conditioned reinforcement learning (RL) with sparse rewards remains a challenging problem in deep RL. Hindsight Experience Replay (HER) has been demonstrated to be an effective solution, where HER replaces desired goals in failed experiences with practically achieved states. Existing approaches mainly focus on either exploration or exploitation to improve the performance of HER. From a joint perspective, exploiting specific past experiences can also implicitly drive exploration. Therefore, we concentrate on prioritizing both original and relabeled samples for efficient goal-conditioned RL. To achieve this, we propose a novel value consistency prioritization (VCP) method, where the priority of samples is determined by the consistency of ensemble Q-values. This distinguishes the VCP method with most existing prioritization approaches which prioritizes samples based on the uncertainty of ensemble Q-values. Through extensive experiments, we demonstrate that VCP achieves significantly higher sample efficiency than existing algorithms on a range of challenging goal-conditioned manipulation tasks. We also visualize how VCP prioritizes good experiences to enhance policy learning.},
journal = {J. Artif. Int. Res.},
month = {jun},
numpages = {22}
}

@inproceedings{10.5555/3463952.3464044,
author = {Li, Sheng and Gupta, Jayesh K. and Morales, Peter and Allen, Ross and Kochenderfer, Mykel J.},
title = {Deep Implicit Coordination Graphs for Multi-Agent Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent reinforcement learning (MARL) requires coordination to efficiently solve certain tasks. Fully centralized control is often infeasible in such domains due to the size of joint action spaces. Coordination graph based formalization allows reasoning about the joint action based on the structure of interactions. However,they often require domain expertise in their design. This paper introduces the deep implicit coordination graph (DICG) architecture for such scenarios. DICG consists of a module for inferring the dynamic coordination graph structure which is then used by a graph neural network based module to learn to implicitly reason about the joint actions or values. DICG allows learning the tradeoff between full centralization and decentralization via standard actor-critic methods to significantly improve coordination for domains with large number of agents. We apply DICG to both centralized-training-centralized-execution and centralized-training-decentralized-execution regimes. We demonstrate that DICG solves the relative over generalization pathology in predatory-prey tasks as well as outperforms various MARL baselines on the challenging StarCraft II Multi-agent Challenge (SMAC) and traffic junction environments.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {764–772},
numpages = {9},
keywords = {multi-agent reinforcement learning, deep reinforcement learning, coordination, multi-agent system, graph neural network},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3571306.3571358,
author = {Rajashekar, Kolichala},
title = {Reinforcement Learning for Real-Time Multi-Access Edge Computing},
year = {2023},
isbn = {9781450397964},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571306.3571358},
doi = {10.1145/3571306.3571358},
abstract = {We examine data-intensive real-time applications, such as forest fire detection, medical emergency services, oil pipeline monitoring, etc., that require relatively low response time in processing data from the Internet of Things (IoT) devices. Typically, in such circumstances, the edge computing paradigm is utilised to drastically reduce the processing delay of such applications. However, with the growing IoT devices, the edge device cluster needs to be configured properly such that the real-time requirements are met. Therefore, the cluster configuration must be dynamically adapted to the changing network topology of the edge cluster in order to minimise the observed overall communication delay incurred by edge devices when processing data from IoT devices. To this end, we propose an intelligent assignment of IoT devices to edge devices based on Reinforcement Learning such that communication delay is minimised and none of the edge devices is overloaded. We demonstrate, with some preliminary results, that our algorithm outperforms the state-of-the-art.},
booktitle = {Proceedings of the 24th International Conference on Distributed Computing and Networking},
pages = {296–297},
numpages = {2},
keywords = {gaze detection, neural networks, text tagging, datasets},
location = {Kharagpur, India},
series = {ICDCN '23}
}

@article{10.5555/3455716.3455886,
author = {Hoiles, William and Krishnamurthy, Vikram and Pattanayak, Kunal},
title = {Rationally Inattentive Inverse Reinforcement Learning Explains Youtube Commenting Behavior},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {We consider a novel application of inverse reinforcement learning with behavioral economics constraints to model, learn and predict the commenting behavior of YouTube viewers. Each group of users is modeled as a rationally inattentive Bayesian agent which solves a contextual bandit problem. Our methodology integrates three key components. First, to identify distinct commenting patterns, we use deep embedded clustering to estimate framing information (essential extrinsic features) that clusters users into distinct groups. Second, we present an inverse reinforcement learning algorithm that uses Bayesian revealed preferences to test for rationality: does there exist a utility function that rationalizes the given data, and if yes, can it be used to predict commenting behavior? Finally, we impose behavioral economics constraints stemming from rational inattention to characterize the attention span of groups of users. The test imposes a R\'{e}nyi mutual information cost constraint which impacts how the agent can select attention strategies to maximize their expected utility. After a careful analysis of a massive YouTube dataset, our surprising result is that in most YouTube user groups, the commenting behavior is consistent with optimizing a Bayesian utility with rationally inattentive constraints. The paper also highlights how the rational inattention model can accurately predict commenting behavior. The massive YouTube dataset and analysis used in this paper are available on GitHub and completely reproducible.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {170},
numpages = {39},
keywords = {R\'{e}nyi mutual information, deep embedded clustering, youtube, framing, behavioral economics, Bayesian revealed preference, inverse reinforcement learning, contextual bandits, rational inattention}
}

@inproceedings{10.5555/3306127.3331672,
author = {Palmer, Gregory and Savani, Rahul and Tuyls, Karl},
title = {Negative Update Intervals in Deep Multi-Agent Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In Multi-Agent Reinforcement Learning (MA-RL), independent cooperative learners must overcome a number of pathologies to learn optimal joint policies. Addressing one pathology often leaves approaches vulnerable towards others. For instance, hysteretic Q-learning citematignon2007hysteretic addresses miscoordination while leaving agents vulnerable towards misleading stochastic rewards. Other methods, such as leniency, have proven more robust when dealing with multiple pathologies simultaneously citeJMLR:v17:15-417. However, leniency has predominately been studied within the context of strategic form games (bimatrix games) and fully observable Markov games consisting of a small number of probabilistic state transitions. This raises the question of whether these findings scale to more complex domains. For this purpose we implement a temporally extend version of the Climb Game citeclaus1998dynamics, within which agents must overcome multiple pathologies simultaneously, including relative overgeneralisation, stochasticity, the alter-exploration and moving target problems, while learning from a large observation space. We find that existing lenient and hysteretic approaches fail to consistently learn near optimal joint-policies in this environment. To address these pathologies we introduce Negative Update Intervals-DDQN (NUI-DDQN), a Deep MA-RL algorithm which discards episodes yielding cumulative rewards outside the range of expanding intervals. NUI-DDQN consistently gravitates towards optimal joint-policies in our environment, overcoming the outlined pathologies.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {43–51},
numpages = {9},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{10.5555/3546258.3546468,
author = {Rakhsha, Amin and Radanovic, Goran and Devidze, Rati and Zhu, Xiaojin and Singla, Adish},
title = {Policy Teaching in Reinforcement Learning via Environment Poisoning Attacks},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {We study a security threat to reinforcement learning where an attacker poisons the learning environment to force the agent into executing a target policy chosen by the attacker. As a victim, we consider RL agents whose objective is to find a policy that maximizes reward in infinite-horizon problem settings. The attacker can manipulate the rewards and the transition dynamics in the learning environment at training-time, and is interested in doing so in a stealthy manner. We propose an optimization framework for finding an optimal stealthy attack for different measures of attack cost. We provide lower/upper bounds on the attack cost, and instantiate our attacks in two settings: (i) an offline setting where the agent is doing planning in the poisoned environment, and (ii) an online setting where the agent is learning a policy with poisoned feedback. Our results show that the attacker can easily succeed in teaching any target policy to the victim under mild conditions and highlight a significant security threat to reinforcement learning agents in practice.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {210},
numpages = {45},
keywords = {policy teaching, security threat, reinforcement learning, training-time adversarial attacks, environment poisoning}
}

@article{10.1145/3610300,
author = {Zhadan, Anastasia and Allahverdyan, Alexander and Kondratov, Ivan and Mikheev, Vikenty and Petrosian, Ovanes and Romanovskii, Aleksei and Kharin, Vitaliy},
title = {Multi-Agent Reinforcement Learning-Based Adaptive Heterogeneous DAG Scheduling},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3610300},
doi = {10.1145/3610300},
abstract = {Static scheduling of computational workflow represented by a directed acyclic graph (DAG) is an important problem in many areas of computer science. The main idea and novelty of the proposed algorithm is an adaptive heuristic or graph metric that uses a different heuristic rule at each scheduling step depending on local workflow. It is also important to note that multi-agent reinforcement learning is used to determine scheduling policy based on adaptive metrics. To prove the efficiency of the approach, a comparison with the state-of-the-art DAG scheduling algorithms is provided: DONF, CPOP, HCPT, HPS, and PETS. Based on the simulation results, the proposed algorithm shows an improvement of up to 30\% on specific graph topologies and an average performance gain of 5.32\%, compared to the best scheduling algorithm, DONF (suitable for large-scale scheduling), on a large number of random DAGs. Another important result is that using the proposed algorithm it was possible to cover 30.01\% of the proximity interval from the best scheduling algorithm to the global optimal solution. This indicates that the idea of an adaptive metric for DAG scheduling is important and requires further research and development.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {oct},
articleno = {87},
numpages = {26},
keywords = {proximal policy optimization, deep learning, directed acyclic graph, scheduling, Multi-agent deep reinforcement learning}
}

@article{10.1145/3039902.3039915,
author = {Su, Jiang and Liu, Jianxiong and Thomas, David B. and Cheung, Peter Y.K.},
title = {Neural Network Based Reinforcement Learning Acceleration on FPGA Platforms},
year = {2017},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0163-5964},
url = {https://doi.org/10.1145/3039902.3039915},
doi = {10.1145/3039902.3039915},
abstract = {Deep Q-learning (DQN) is a recently proposed reinforcement learning algorithm where a neural network is applied as a non-linear approximator to its value function. The exploitation-exploration mechanism allows the training and prediction of the NN to execute simultaneously in an agent during its interaction with the environment. Agents often act independently on battery power, so the training and prediction must occur within the agent and on a limited power budget. In this work, We propose an FPGA acceleration system design for Neural Network Q-learning (NNQL). Our proposed system has high flexibility due to the support to run-time network parameterization, which allows neuroevolution algorithms to dynamically restructure the network to achieve better learning results. Additionally, the power consumption of our proposed system is adaptive to the network size because of a new processing element design. Based on our test cases on networks with hidden layer size ranging from 32 to 16384, our proposed system achieves 7x to 346x speedup compared to GPU implementation and 22x to 77x speedup to hand-coded CPU counterpart.},
journal = {SIGARCH Comput. Archit. News},
month = {jan},
pages = {68–73},
numpages = {6}
}

@inproceedings{10.5555/3545946.3598816,
author = {Gangopadhyay, Briti and Dasgupta, Pallab and Dey, Soumyajit},
title = {Counterexample-Guided Policy Refinement in Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-Agent Reinforcement Learning (MARL) policies are being incorporated into a wide range of safety-critical applications. It is important for these policies to be free of counterexamples and adhere to safety requirements. We present a methodology for the counterexample-guided refinement of an optimized MARL policy with respect to given safety specifications. The proposed algorithm refines a calibrated MARL policy to become safer by eliminating counterexamples found during testing, using targeted gradient updates. We empirically validate our method on different cooperative multi-agent tasks and demonstrate that targeted gradient updates induce safety in MARL policies.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1606–1614},
numpages = {9},
keywords = {multi-agent proximal policy optimization, counterexample-guided refinement, multi-agent reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/1018411.1018907,
author = {Szer, Daniel and Charpillet, Francois},
title = {Coordination through Mutual Notification in Cooperative Multiagent Reinforcement Learning},
year = {2004},
isbn = {1581138644},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We present a new algorithm for cooperative reinforcement learning in multiagent systems. Our main concern is the correct coordination between the members of the team: We seek to obtain an optimal solution for the team as a whole while keeping the learning as much decentralized as possible. We consider autonomous and independently learning agents that do not store any explicit information about their teammates\'{y} behavior, as well as possibly different reward functions for each agent. Coordination between agents occurs through communication, namely the mutual notification algorithm.},
booktitle = {Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3},
pages = {1254–1255},
numpages = {2},
location = {New York, New York},
series = {AAMAS '04}
}

@inproceedings{10.1145/3067695.3076035,
author = {Peng, Yiming and Chen, Gang and Holdaway, Scott and Mei, Yi and Zhang, Mengjie},
title = {Automated State Feature Learning for Actor-Critic Reinforcement Learning through NEAT},
year = {2017},
isbn = {9781450349390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3067695.3076035},
doi = {10.1145/3067695.3076035},
abstract = {Actor-Critic (AC) algorithms are important approaches to solving sophisticated reinforcement learning problems. However, the learning performance of these algorithms rely heavily on good state features that are often designed manually. To address this issue, we propose to adopt an evolutionary approach based on NeuroEvolution of Augmenting Topology (NEAT) to automatically evolve neural networks that directly transform the raw environmental inputs into state features. Following this idea, we have successfully developed a new algorithm called NEAT+AC which combines Regular-gradient Actor-Critic (RAC) with NEAT. It can simultaneously learn suitable state features as well as good policies that are expected to significantly improve the reinforcement learning performance. Preliminary experiments on two benchmark problems confirm that our new algorithm can clearly outperform the baseline algorithm, i.e., NEAT.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {135–136},
numpages = {2},
keywords = {feature extraction, NEAT, reinforcement learning, feature learning, NeuroEvolution, actor-critic},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.1145/3548606.3560690,
author = {Gohil, Vasudev and Guo, Hao and Patnaik, Satwik and Rajendran, Jeyavijayan},
title = {ATTRITION: Attacking Static Hardware Trojan Detection Techniques Using Reinforcement Learning},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3560690},
doi = {10.1145/3548606.3560690},
abstract = {Stealthy hardware Trojans (HTs) inserted during the fabrication of integrated circuits can bypass the security of critical infrastructures. Although researchers have proposed many techniques to detect HTs, several critical limitations exist, including: (i) a low success rate of HT detection, (ii) high algorithmic complexity, and (iii) a large number of test patterns. Furthermore, as we show in this work the most pertinent drawback of prior (including state-of-the-art) detection techniques stems from an incorrect evaluation methodology, i.e., they assume that an adversary inserts HTs randomly. Such inappropriate adversarial assumptions enable detection techniques to claim high HT detection accuracy, leading to a "false sense of security." To the best of our knowledge, despite more than a decade of research on detecting HTs inserted during fabrication, there have been no concerted efforts to perform a systematic evaluation of HT detection techniques. In this paper, we play the role of a realistic adversary and question the efficacy of HT detection techniques by developing an automated, scalable, and practical attack framework, ATTRITION, using reinforcement learning (RL). ATTRITION evades eight detection techniques (published in premier security venues, well-cited in academia, etc.) across two HT detection categories, showcasing its agnostic behavior. ATTRITION achieves average attack success rates of 47x and 211x compared to randomly inserted HTs against state-of-the-art logic testing and side channel techniques. To demonstrate ATTRITION's ability in evading detection techniques, we evaluate different designs ranging from the widely-used academic suites (ISCAS-85, ISCAS-89) to larger designs such as the open-source MIPS and mor1kx processors to AES and a GPS module. Additionally, we showcase the impact of ATTRITION generated HTs through two case studies (privilege escalation and kill switch) on mor1kx processor. We envision that our work, along with our released HT benchmarks and models fosters the development of better HT detection techniques.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1275–1289},
numpages = {15},
keywords = {hardware trojans, reinforcement learning, attacks},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@inproceedings{10.1145/3502827.3502844,
author = {Tan, Pu},
title = {COVID-19 Vaccine Distribution Policy Design with Reinforcement Learning},
year = {2022},
isbn = {9781450385183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502827.3502844},
doi = {10.1145/3502827.3502844},
abstract = {COVID-19 has become a global crisis and the vaccine has been seen as an effective approach to stop the epidemic spread. However, the resources for distributing and allocating different types of vaccines are limited and we need a better vaccine distribution policy design to prevent the spread of COVID-19 more efficiently. In this study, a pipeline of combing a random forest model and a DQN model is proposed. The random forest model is built to predict the daily new confirmed cases with the vaccine data as the inputs. And the DQN model is built to design the daily allocation ratio of three types of vaccines, with the aim to minimize the new confirmed cases. The experimental results based on the real-world datasets collected in San Diego validate the effectiveness of the proposed pipeline.},
booktitle = {Proceedings of the 5th International Conference on Advances in Image Processing},
pages = {103–108},
numpages = {6},
keywords = {Vaccine, COVID-19, Machine learning, Reinforcement learning},
location = {Chengdu, China},
series = {ICAIP '21}
}

@inproceedings{10.1145/3377929.3398128,
author = {Rosenbauer, Lukas and Stein, Anthony and Maier, Roland and P\"{a}tzel, David and H\"{a}hner, J\"{o}rg},
title = {XCS as a Reinforcement Learning Approach to Automatic Test Case Prioritization},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3398128},
doi = {10.1145/3377929.3398128},
abstract = {Testing is a crucial part in the development of new products. With the rise of test automation methods, companies start relying on an even higher number of tests. Sometimes it is not feasible to run all tests and the goal is to determine which tests are crucial and which are less important. This prioritization problem has just recently gotten into the focus of reinforcement learning. A neural network combined with prioritized experience replay (ER) was used to identify critical tests. We are the first to apply XCS classifier systems (XCS) for this use case and reveal that XCS is not only suitable for this problem, but can also be superior to the aforementioned neural network and leads to more stable results. In this work, we adapt XCS's learning mechanism to the task by introducing a batch update which is based on Monte Carlo control. Further, we investigate if prioritized ER has the same positive effects on XCS as on the neural network for this test prioritization problem. Our experiments show that in general this is not the case for XCS.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1798–1806},
numpages = {9},
keywords = {artificial intelligence, reinforcement learning, experience replay, XCS classifier system, test automation},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3297156.3297217,
author = {Yang, Fan and Wang, Ping and Wang, XinHong},
title = {Continuous Control in Car Simulator with Deep Reinforcement Learning},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297217},
doi = {10.1145/3297156.3297217},
abstract = {Deep reinforcement learning (DRL), which can be trained without abundant labeled data required in supervised learning, plays an important role in autonomous vehicle researches. According to action space, DRL can be further divided into two classes: discrete domain and continuous domain. In this work, we focus on continuous steering control since it's impossible to switch among different discrete steering values at short intervals in reality. We first define the steering smoothness to quantify the degree of continuity. Then we propose a new penalty in reward shaping. We carry experiments based on Deep Deterministic Policy Gradient (DDPG) and Asynchronous Advantage Actor Critic (A3C), which are the state of the art in continuous domain. Results show that the proposed penalty improves the steering smoothness with both algorithms.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {566–570},
numpages = {5},
keywords = {Continuous Control, Autonomous Driving, A3C, Deep Reinforcement Learning},
location = {Shenzhen, China},
series = {CSAI '18}
}

@article{10.14778/3579075.3579076,
author = {Li, Mingxuan and Wang, Yazhe and Ma, Shuai and Liu, Chao and Huo, Dongdong and Wang, Yu and Xu, Zhen},
title = {Auto-Tuning with Reinforcement Learning for Permissioned Blockchain Systems},
year = {2023},
issue_date = {January 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3579075.3579076},
doi = {10.14778/3579075.3579076},
abstract = {In a permissioned blockchain, performance dictates its development, which is substantially influenced by its parameters. However, research on auto-tuning for better performance has somewhat stagnated because of the difficulty posed by distributed parameters; thus, it is possible only with difficulty to propose an effective auto-tuning optimization scheme. To alleviate this issue, we lay a solid basis for our research by first exploring the relationship between parameters and performance in Hyperledger Fabric, a permissioned blockchain, and we propose Athena, a Fabric-based auto-tuning system that can automatically provide parameter configurations for optimal performance. The key of Athena is designing a new Permissioned Blockchain Multi-Agent Deep Deterministic Policy Gradient (PB-MADDPG) to realize heterogeneous parameter-tuning optimization of different types of nodes in Fabric. Moreover, we select parameters with the most significant impact on accelerating recommendation. In its application to Fabric, a typical permissioned blockchain system, with 12 peers and 7 orderers, Athena achieves a throughput improvement of 470.45\% and a latency reduction of 75.66\% over the default configuration. Compared with the most advanced tuning schemes (CDBTune, Qtune, and ResTune), our method is competitive in terms of throughput and latency.},
journal = {Proc. VLDB Endow.},
month = {jan},
pages = {1000–1012},
numpages = {13}
}

@inproceedings{10.1145/3460231.3474256,
author = {Wu, Yaxiong and Macdonald, Craig and Ounis, Iadh},
title = {Partially Observable Reinforcement Learning for Dialog-Based Interactive Recommendation},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3474256},
doi = {10.1145/3460231.3474256},
abstract = {A dialog-based interactive recommendation task is where users can express natural-language feedback when interacting with the recommender system. However, the users’ feedback, which takes the form of natural-language critiques about the recommendation at each iteration, can only allow the recommender system to obtain a partial portrayal of the users’ preferences. Indeed, such partial observations of the users’ preferences from their natural-language feedback make it challenging to correctly track the users’ preferences over time, which can result in poor recommendation performances and a less effective satisfaction of the users’ information needs when in presence of limited iterations. Reinforcement learning, in the form of a partially observable Markov decision process (POMDP), can simulate the interactions between a partially observable environment (i.e. a user) and an agent (i.e. a recommender system). To alleviate such a partial observation issue, we propose a novel dialog-based recommendation model, the Estimator-Generator-Evaluator (EGE) model, with Q-learning for POMDP, to effectively incorporate the users’ preferences over time. Specifically, we leverage an Estimator to track and estimate users’ preferences, a Generator to match the estimated preferences with the candidate items to rank the next recommendations, and an Evaluator to judge the quality of the estimated preferences considering the users’ historical feedback. Following previous work, we train our EGE model by using a user simulator which itself is trained to describe the differences between the target users’ preferences and the recommended items in natural language. Thorough and extensive experiments conducted on two recommendation datasets – addressing images of fashion products (namely dresses and shoes) – demonstrate that our proposed EGE model yields significant improvements in comparison to the existing state-of-the-art baseline models.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {241–251},
numpages = {11},
keywords = {multimodal, reinforcement learning, interactive recommendation},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@inproceedings{10.1145/3583780.3615474,
author = {Agrawal, Sanjay and Merugu, Srujana and Sembium, Vivek},
title = {Enhancing E-Commerce Product Search through Reinforcement Learning-Powered Query Reformulation},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615474},
doi = {10.1145/3583780.3615474},
abstract = {Query reformulation (QR) is a widely used technique in web and product search. In QR, we map a poorly formed or low coverage user query to a few semantically similar queries that are rich in product coverage, thereby enabling effective targeted searches with less cognitive load on the user. Recent QR approaches based on generative language models are superior to informational retrieval-based methods but exhibit key limitations: (i) generated reformulations often have low lexical diversity and fail to retrieve a large set of relevant products of a wider variety, (ii) the training objective of generative models does not incorporate a our goal of improving product coverage. In this paper, we propose RLQR (Reinforcement Learning for Query Reformulations), for generating high quality diverse reformulations which aim to maximize the product coverage (number of distinct relevant products returned). We evaluate our approach against supervised generative models and strong RL-based methods. Our experiments demonstrate a 28.6\% increase in product coverage compared to a standard generative model, outperforming SOTA benchmarks by a significant margin. We also conduct our experiments on an external Amazon shopping dataset and demonstrate increased product coverage over SOTA algorithms.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4488–4494},
numpages = {7},
keywords = {natural language generation, e-commerce, product search, query reformulation, reinforcement learning},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

