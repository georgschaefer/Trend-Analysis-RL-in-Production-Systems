@inproceedings{10.1145/3615587.3615988,
author = {Xu, Tingli and Wang, Ran and Hao, Jie},
title = {Deep Reinforcement Learning-Based Multi-Objective Optimization for Mobile Charging Services in Internet of Electric Vehicles},
year = {2023},
isbn = {9798400703416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615587.3615988},
doi = {10.1145/3615587.3615988},
abstract = {With increasing environmental awareness, electric vehicles (EVs) are becoming the preferred choice for future transportation. However, the inconvenience of charging has been a significant obstacle to the development of EVs. Mobile charging has emerged as a promising solution, providing additional charging capacity for EVs. This paper investigates the scheduling problem of a mobile charging vehicle (MCV) that aims to meet charging service requests from a group of randomly distributed EVs within their expected time windows. The objective is to minimize the total distance traveled by the MCV and the time penalty cost while ensuring that all EVs' energy demands are met. To address this multi-objective problem, we propose a deep reinforcement learning approach that decomposes the problem into subproblems using weighted grouping techniques. Each subproblem is then modeled and solved using a deep neural network based on the transformer attention mechanism. To train this model, we utilize a reinforcement learning algorithm combined with a neighbor parameter transfer training method. Extensive simulation results demonstrate that our approach outperforms traditional methods such as NSGA-II and MOEA/D in terms of online solving speed, optimization performance and generalization abilities.},
booktitle = {Proceedings of the 18th Workshop on Mobility in the Evolving Internet Architecture},
pages = {1–6},
numpages = {6},
keywords = {mobile charging service, multi-objective optimization, transformer, deep reinforcement learning},
location = {Madrid, Spain},
series = {MobiArch '23}
}

@inproceedings{10.5555/3522802.3522944,
author = {Liu, Lixing and Gurney, Nikolos and McCullough, Kyle and Ustun, Volkan},
title = {Graph Neural Network Based Behavior Prediction to Support Multi-Agent Reinforcement Learning in Military Training Simulations},
year = {2022},
publisher = {IEEE Press},
abstract = {We introduce a computational behavioral model for non-player characters (NPCs) that endows them with the ability to adapt to their experiences --- including interactions with human trainees. Most existing NPC behavioral models for military training simulations are either rule-based or reactive with minimal built-in intelligence. Such models are unable to adapt to the characters' experiences, be they with other NPCs, the environment, or human trainees. Multi-agent Reinforcement Learning (MARL) presents opportunities to train adaptive models for both friendly and opposing forces to improve the quality of NPCs. Still, military environments present significant challenges since they can be stochastic, partially observable, and non-stationary. We discuss our MARL framework to devise NPCs exhibiting dynamic, authentic behavior and introduce a novel Graph Neural Network based behavior prediction model to strengthen their cooperation. We demonstrate the efficacy of our behavior prediction model in a proof-of-concept multi-agent military scenario.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {172},
numpages = {12},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/3580305.3599218,
author = {Wang, Chu and Wang, Yingfei and Luo, Haipeng and Jiang, Daniel and He, Jinghai and Zheng, Zeyu},
title = {2nd Workshop on Multi-Armed Bandits and Reinforcement Learning: Advancing Decision Making in E-Commerce and Beyond},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599218},
doi = {10.1145/3580305.3599218},
abstract = {The areas of reinforcement learning and multi-armed bandits have recently seen significant innovation, while many application domains, such as e-commerce, are full of problems and challenges to which vanilla RL or MAB methods cannot directly apply. This workshop aims at filling this communication gap by creating a platform for researchers and practitioners from both the method/theory side and application side of the community. Having this platform now instead of at a later time is beneficial to all sides of the community: practitioners and frontline scientists are able to avoid re-inventing existing techniques; theory-oriented researchers can find motivation in industry problems, working within more realistic settings, and making real-world impact. The 2nd Multi-armed Bandits and Reinforcement Learning Workshop was a half day workshop co-located with the 29th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining (KDD 2023) in Long Beach, California.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5890},
numpages = {1},
keywords = {e-commerce, multi-armed bandits, reinforcement learning, decision maing},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.5555/3545946.3598748,
author = {Chen, Xu and Liu, Shuo and Di, Xuan},
title = {A Hybrid Framework of Reinforcement Learning and Physics-Informed Deep Learning for Spatiotemporal Mean Field Games},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Mean field games (MFG) are developed to solve equilibria in multi-agent systems (MAS) with many agents. The majority of literature on MFGs is focused on finite states and actions. In many engineering applications such as autonomous driving, however, each agent (e.g., an autonomous vehicle) makes a continuous-time-space (or spatiotemporal dynamic) decision to optimize a nonlinear cumulative reward. In this paper, we focus on a class of generic MFGs with continuous states and actions defined over a spatiotemporal domain for a finite horizon, named "spatiotemporal MFG (ST-MFG)." The mean field equilibria (MFE) for such games are challenging to solve using numerical methods to meet a satisfactory resolution in time and space, while it is critical to deploy smooth dynamic control in autonomous driving. Thus, we propose two methods, one is a joint reinforcement learning (RL) and machine learning framework, which iteratively solves agents' optimal policies using RL, and propagates population density using physics-informed deep learning (PIDL). The other is a pure PIDL framework that updates agents' states and population density altogether using deep neural networks. Both the proposed methods are mesh-free (i.e., not restricted by mesh granularity), and have shown to be efficient in learning equilibria in autonomous driving MFGs. The PIDL method alone is faster to train than the RL-PIDL integrated method, when the environment dynamic is known.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1079–1087},
numpages = {9},
keywords = {mean field games, reinforcement learning, physics-informed deep learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3427773.3427869,
author = {Kathirgamanathan, Anjukan and Twardowski, Kacper and Mangina, Eleni and Finn, Donal P.},
title = {A Centralised Soft Actor Critic Deep Reinforcement Learning Approach to District Demand Side Management through CityLearn},
year = {2020},
isbn = {9781450381932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427773.3427869},
doi = {10.1145/3427773.3427869},
abstract = {Reinforcement learning is a promising model-free and adaptive controller for demand side management, as part of the future smart grid, at the district level. This paper presents the results of the algorithm that was submitted for the CityLearn Challenge, which was hosted in early 2020 with the aim of designing and tuning a reinforcement learning agent to flatten and smooth the aggregated curve of electrical demand of a district of diverse buildings. The proposed solution secured second place in the challenge using a centralised 'Soft Actor Critic' deep reinforcement learning agent that was able to handle continuous action spaces. The controller was able to achieve an averaged score of 0.967 on the challenge dataset comprising of different buildings and climates. This highlights the potential application of deep reinforcement learning as a plug-and-play style controller, that is capable of handling different climates and a heterogenous building stock, for district demand side management of buildings.},
booktitle = {Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings \&amp; Cities},
pages = {11–14},
numpages = {4},
keywords = {Demand Side Management, Smart Grid, Deep Reinforcement Learning},
location = {Virtual Event, Japan},
series = {RLEM'20}
}

@inproceedings{10.1145/3597065.3597446,
author = {Zhu, Yonghui and Zhang, Ronghui and Cui, Yuanhao and Wu, Sheng and Jiang, Chunxiao and Jing, Xiaojun},
title = {UAV-Aided Partial Task Offloading for Integrated Sensing, Computation, and Communications Systems via Deep Reinforcement Learning},
year = {2023},
isbn = {9798400702150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597065.3597446},
doi = {10.1145/3597065.3597446},
abstract = {The emergence of Metaverse applications has driven the integrated sensing, computation, and communications (ISC2) technology into edge networks to optimize performance. Unmanned aerial vehicles (UAVs) have become a feasible and cost-effective platform for improving ISC2 coverage and enabling low-latency applications due to high mobility. In this paper, we present a UAV-aided ISC2 system that employs partial data offloading to enhance the sensing data processing capabilities of edge network users. We propose a scheme that uses deep reinforcement learning to minimize the sum of communication and computation delays in the system. To this end, we optimize the data offloading and UAV's flight action based on deep deterministic policy gradient. Extensive simulations are conducted to evaluate the performance of our proposed method under various parameter conditions, which demonstrate its significant superiority over other methods.},
booktitle = {Proceedings of the 2nd Workshop on Integrated Sensing and Communications for Metaverse},
pages = {1–6},
numpages = {6},
keywords = {integrated sensing and communications, deep reinforcement learning, edge computing, metaverse, UAV},
location = {Helsinki, Finland},
series = {ISACom '23}
}

@article{10.5555/3586589.3586718,
author = {Mondal, Washim Uddin and Agarwal, Mridul and Aggarwal, Vaneet and Ukkusuri, Satish V.},
title = {On the Approximation of Cooperative Heterogeneous Multi-Agent Reinforcement Learning (MARL) Using Mean Field Control (MFC)},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {Mean field control (MFC) is an effective way to mitigate the curse of dimensionality of cooperative multi-agent reinforcement learning (MARL) problems. This work considers a collection of Npop heterogeneous agents that can be segregated into K classes such that the k-th class contains Nk homogeneous agents. We aim to prove approximation guarantees of the MARL problem for this heterogeneous system by its corresponding MFC problem. We consider three scenarios where the reward and transition dynamics of all agents are respectively taken to be functions of (1) joint state and action distributions across all classes, (2) individual distributions of each class, and (3) marginal distributions of the entire population. We show that, in these cases, the K-class MARL problem can be approximated by MFC with errors given as $e_1=mathcal{O}(frac{sqrt{|mathcal{X}|}+sqrt{|mathcal{U}|}}{N_{mathrm{pop}}}sum_{k}sqrt{N_k})$, $e_2=mathcal{O}(left[sqrt{|mathcal{X}|}+sqrt{|mathcal{U}|}right]sum_{k}frac{1}{sqrt{N_k}})$ and $e_3=mathcal{O}left(left[sqrt{|mathcal{X}|}+sqrt{|mathcal{U}|}right]left[frac{A}{N_{mathrm{pop}}}sum_{kin[K]}sqrt{N_k}+frac{B}{sqrt{N_{mathrm{pop}}}}right]right)$, respectively, where A, B are some constants and |χ|, |U| are the sizes of state and action spaces of each agent. Finally, we design a Natural Policy Gradient (NPG) based algorithm that, in the three cases stated above, can converge to an optimal MARL policy within O(ej) error with a sample complexity of O(ej-3), j ∈ {1, 2, 3}, respectively.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {129},
numpages = {46},
keywords = {policy gradient algorithm, approximation guarantees, mean-field control, heterogeneous systems, multi-agent learning}
}

@inproceedings{10.1109/DS-RT52167.2021.9576130,
author = {Campoverde, Luis Miguel Samaniego and Tropea, Mauro and De Rango, Floriano},
title = {An IoT Based Smart Irrigation Management System Using Reinforcement Learning Modeled through a Markov Decision Process},
year = {2022},
isbn = {9781665433266},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DS-RT52167.2021.9576130},
doi = {10.1109/DS-RT52167.2021.9576130},
abstract = {In this paper, the proposal of an irrigation system exploiting IoT is provided. The proposed system, based on IoT sensors and smart platforms such as Raspberry PI and Arduino, is able to manage farm operations in term of irrigation. The control of water pumps for irrigation is driven by two important parameters: soil moisture and evapotranspiration. The system uses a Reinforcement Learning approach based on Markov Decision Process in order to learn the right water amount needed by the plants. This approach allows to reduce both water and energy consumption. The proposal has been compared with a conventional irrigation system that normally is based on a soil humidity threshold and takes its decision considering the threshold setting. Conducted experiments show the water and energy saving by the use of the proposed Smart Irrigation system.},
booktitle = {Proceedings of the 2021 IEEE/ACM 25th International Symposium on Distributed Simulation and Real Time Applications},
articleno = {16},
numpages = {4},
keywords = {reinforcement learning, markovian decision process, smart irrigation, internet of things},
location = {Valencia, Spain},
series = {DS-RT '21}
}

@inproceedings{10.1145/3447555.3466589,
author = {Jang, Doseok and Spangher, Lucas and Khattar, Manan and Agwan, Utkarsha and Spanos, Costas},
title = {Using Meta Reinforcement Learning to Bridge the Gap between Simulation and Experiment in Energy Demand Response},
year = {2021},
isbn = {9781450383332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447555.3466589},
doi = {10.1145/3447555.3466589},
abstract = {Our team is proposing to run a full-scale energy demand response experiment in an office building. Although this is an exciting endeavor which will provide value to the community, collecting training data for the reinforcement learning agent is costly and will be limited. In this work, we apply a meta-learning architecture to warm start the experiment with simulated tasks, to increase sample efficiency. We present results that demonstrate a similar a step up in complexity still corresponds with better learning.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Future Energy Systems},
pages = {483–487},
numpages = {5},
keywords = {microgrid, transactive energy, aggregation, prosumer, reinforcement learning},
location = {Virtual Event, Italy},
series = {e-Energy '21}
}

@inproceedings{10.1145/3195970.3196034,
author = {Kang, Wonkyung and Yoo, Sungjoo},
title = {Dynamic Management of Key States for Reinforcement Learning-Assisted Garbage Collection to Reduce Long Tail Latency in SSD},
year = {2018},
isbn = {9781450357005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195970.3196034},
doi = {10.1145/3195970.3196034},
abstract = {Garbage collection (GC) is one of main causes of the long-tail latency problem in storage systems. Long-tail latency due to GC is more than 100 times greater than the average latency at the 99th percentile. Therefore, due to such a long tail latency, real-time systems and quality-critical systems cannot meet the system requirements. In this study, we propose a novel key state management technique of reinforcement learning-assisted garbage collection. The purpose of this study is to dynamically manage key states from a significant number of state candidates. Dynamic management enables us to utilize suitable and frequently recurring key states at a small area cost since the full states do not have to be managed. The experimental results show that the proposed technique reduces by 22--25\% the long-tail latency compared to a state-of-the-art scheme with real-world workloads.},
booktitle = {Proceedings of the 55th Annual Design Automation Conference},
articleno = {8},
numpages = {6},
keywords = {flash storage system, garbage collection, long-tail latency, SSD},
location = {San Francisco, California},
series = {DAC '18}
}

@inproceedings{10.1145/3449726.3463172,
author = {Zhao, Ying and Hemberg, Erik and Derbinsky, Nate and Mata, Gabino and O'Reilly, Una-May},
title = {Simulating a Logistics Enterprise Using an Asymmetrical Wargame Simulation with Soar Reinforcement Learning and Coevolutionary Algorithms},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3463172},
doi = {10.1145/3449726.3463172},
abstract = {We demonstrate an innovative framework (CoEvSoarRL) that leverages machine learning algorithms to optimize and simulate a resilient and agile logistics enterprise to improve the readiness and sustainment, as well as reduce the operational risk. The CoEvSoarRL is an asymmetrical wargame simulation that leverages reinforcement learning and coevolutionary algorithms to improve the functions of a total logistics enterprise value chain. We address two of the key challenges: (1) the need to apply holistic prediction, optimization, and wargame simulation to improve the total logistics enterprise readiness; (2) the uncertainty and lack of data which require large-scale systematic what-if scenarios and analysis of alternatives to simulate potential new and unknown situations. Our CoEvSoarRL learns a model of a logistic enterprise environment from historical data with Soar reinforcement learning. Then the Soar model is used to evaluate new decisions and operating conditions. We simulate the logistics enterprise vulnerability (risk) and evolve new and more difficult operating conditions (tests); meanwhile we also coevolve better logistics enterprise decision (solutions) to counter the tests. We present proof-of-concept results from a US Marine Corps maintenance and supply chain data set.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1907–1915},
numpages = {9},
keywords = {coevolutionary algorithms, reinforcement learning, logistics, risk},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{10.1145/3583131.3590436,
author = {Goel, Diksha and Neumann, Aneta and Neumann, Frank and Nguyen, Hung and Guo, Mingyu},
title = {Evolving Reinforcement Learning Environment to Minimize Learner's Achievable Reward: An Application on Hardening Active Directory Systems},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590436},
doi = {10.1145/3583131.3590436},
abstract = {We study a Stackelberg game between one attacker and one defender in a configurable environment. The defender picks a specific environment configuration. The attacker observes the configuration and attacks via Reinforcement Learning (RL trained against the observed environment). The defender's goal is to find the environment with minimum achievable reward for the attacker. We apply Evolutionary Diversity Optimization (EDO) to generate diverse population of environments for training. Environments with clearly high rewards are killed off and replaced by new offsprings to avoid wasting training time. Diversity not only improves training quality but also fits well with our RL scenario: RL agents tend to improve gradually, so a slightly worse environment earlier on may become better later. We demonstrate the effectiveness of our approach by focusing on a specific application, Active Directory (AD). AD is the default security management system for Windows domain networks. AD environment describes an attack graph, where nodes represent computers/accounts/etc., and edges represent accesses. The attacker aims to find the best attack path to reach the highest-privilege node. The defender can change the graph by removing a limited number of edges (revoke accesses). Our approach generates better defensive plans than the existing approach and scales better.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1348–1356},
numpages = {9},
keywords = {active directory, evolutionary diversity optimization, attack graph, reinforcement learning},
location = {Lisbon, Portugal},
series = {GECCO '23}
}

@inproceedings{10.1145/3432261.3432262,
author = {Wang, Yu-Cheng and Chou, Jerry and Chung, I-Hsin},
title = {A Deep Reinforcement Learning Method for Solving Task Mapping Problems with Dynamic Traffic on Parallel Systems},
year = {2021},
isbn = {9781450388429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3432261.3432262},
doi = {10.1145/3432261.3432262},
abstract = {Efficient mapping of application communication patterns to the network topology is a critical problem for optimizing the performance of communication bound applications on parallel computing systems. The problem has been extensively studied in the past, but they mostly formulate the problem as finding an isomorphic mapping between two static graphs with edges annotated by traffic volume and network bandwidth. But in practice, the network performance is difficult to be accurately estimated, and communication patterns are often changing over time and not easily obtained. Therefore, this work proposes a deep reinforcement learning&nbsp;(DRL) approach to explore better task mappings by utilizing the performance prediction and runtime communication behaviors provided from a simulator to learn an efficient task mapping algorithm. We extensively evaluated our approach using both synthetic and real applications with varied communication patterns on Torus and Dragonfly networks. Compared with several existing approaches from literature and software library, our proposed approach found task mappings that consistently achieved comparable or better application performance. Especially for a real application, the average improvement of our approach on Torus and Dragonfly networks are 11\% and 16\%, respectively. In comparison, the average improvements of other approaches are all less than 6\%.},
booktitle = {The International Conference on High Performance Computing in Asia-Pacific Region},
pages = {1–10},
numpages = {10},
keywords = {Parallel applications, Task mapping, Deep Learning, Algorithm},
location = {Virtual Event, Republic of Korea},
series = {HPC Asia 2021}
}

@inproceedings{10.5555/3283552.3283553,
author = {wu, Chao and Ji, Cheng and Li, Qiao and Fu, Chenchen and Xue, Chun Jason},
title = {Maximizing I/O Throughput and Minimizing Performance Variation via Reinforcement Learning Based I/O Merging for SSDs: Work-in-Progress},
year = {2018},
isbn = {9781538655641},
publisher = {IEEE Press},
abstract = {Merge technique is widely adopted by I/O schedulers to maximize system I/O throughput. However merging operation could degrade the latency of individual I/O, thus incurring prolonged I/O latencies and enlarged I/O variations of I/O requests. In this case, the requirement of QoS (Quality of Service) performance will be violated. In order to improve QoS performance meanwhile providing high I/O throughput, this paper proposed a Reinforcement Learning based I/O merge approach. Through learning the characteristic of various I/O patterns, the proposed approach make merge decisions adaptive to different I/O workloads.},
booktitle = {Proceedings of the International Conference on Compilers, Architecture and Synthesis for Embedded Systems},
articleno = {1},
numpages = {2},
keywords = {worst-case latency, merge technique, throughput, I/O scheduler, performance variation, reinforcement learning},
location = {Turin, Italy},
series = {CASES '18}
}

@inproceedings{10.1145/3468891.3468898,
author = {Feng, Wenlong and Dong, Wei and Zhai, Shouchao and Zhang, Guohua and Sun, Xinya and Ji, Yindong},
title = {A Deep Reinforcement Learning Method for Freight Train Driving Based on Domain Knowledge and Mass Estimation Network},
year = {2021},
isbn = {9781450389402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468891.3468898},
doi = {10.1145/3468891.3468898},
abstract = {In train marshalling, the mass of the freight train will dynamically change in a wide range, which is the main difficulty in realizing its automatic driving. This paper proposes a deep reinforcement learning method that combines domain knowledge and mass estimation network (MEN). The domain knowledge of excellent drivers is utilized to accelerate the convergence speed of the algorithm and improve the driving performance. Furthermore, the MEN is introduced for estimating the mass of the entire train during driving. Finally, the deep reinforcement learning algorithm selects the output gear based on the estimated mass. The simulation results show that the proposed method has significant effects on performance optimization such as reducing parking error, improving marshalling efficiency, optimizing coupler force and reducing jerk.},
booktitle = {Proceedings of the 2021 6th International Conference on Machine Learning Technologies},
pages = {41–46},
numpages = {6},
keywords = {reinforcement learning, domain knowledge, automatic driving, freight train},
location = {Jeju Island, Republic of Korea},
series = {ICMLT '21}
}

@inproceedings{10.1109/MODELS-C.2019.00039,
author = {Elyasaf, Achiya and Sadon, Aviran and Weiss, Gera and Yaacov, Tom},
title = {Using Behavioural Programming with Solver, Context, and Deep Reinforcement Learning for Playing a Simplified RoboCup-Type Game},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00039},
doi = {10.1109/MODELS-C.2019.00039},
abstract = {We describe four scenario-based implementations of controllers for a player in a simplified RoboCup-type game. All four implementations are based on the behavioural programming (BP) approach. We first describe a simple controller for the player using the state-of-the-art BPjs tool and then show how it can be extended in various ways. The first extension is based on a version of BP where the Z3 SMT solver is used to provide mechanisms for richer composition of modules within the BP model. This allows for modules with higher cohesion and lower coupling. It also allows incrementality: we could use the scenarios we developed for the challenge of MDETOOLS'18 and extend the model to handle the new system. The second extension of BP demonstrated in this paper is a set of idioms for subjecting model components to context. One of the differences between this year's challenge and the challenge we dealt with last year is that following the ball is not the only task that a player needs to handle, there is much more to care for. We demonstrate how we used the idioms for handling context to parametrize scenarios like "go to a target" in a dynamic and natural fashion such that modelers can efficiently specify reusable components similar to the way modern user manuals for advanced products are written. Lastly, in an attempt to make the instructions to the robot even more natural, we demonstrate a third extension based on deep reinforcement learning. Towards substantiating the observation that it is easier to explain things to an intelligent agent than to dumb compiler, we demonstrate how the combination of BP and deep reinforcement learning (DRL) allows for giving abstract instructions to the robot and for teaching it to follow them after a short training session.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems},
pages = {243–251},
numpages = {9},
location = {Munich, Germany},
series = {MODELS '19}
}

@article{10.1145/3466618,
author = {Mysore, Siddharth and Mabsout, Bassel and Saenko, Kate and Mancuso, Renato},
title = {How to Train Your Quadrotor: A Framework for Consistently Smooth and Responsive Flight Control via Reinforcement Learning},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3466618},
doi = {10.1145/3466618},
abstract = {We focus on the problem of reliably training Reinforcement Learning (RL) models (agents) for stable low-level control in embedded systems and test our methods on a high-performance, custom-built quadrotor platform. A common but often under-studied problem in developing RL agents for continuous control is that the control policies developed are not always smooth. This lack of smoothness can be a major problem when learning controllers as it can result in control instability and hardware failure.Issues of noisy control are further accentuated when training RL agents in simulation due to simulators ultimately being imperfect representations of reality—what is known as the reality gap. To combat issues of instability in RL agents, we propose a systematic framework, REinforcement-based transferable Agents through Learning (RE+AL), for designing simulated training environments that preserve the quality of trained agents when transferred to real platforms. RE+AL is an evolution of the Neuroflight infrastructure detailed in technical reports prepared by members of our research group. Neuroflight is a state-of-the-art framework for training RL agents for low-level attitude control. RE+AL improves and completes Neuroflight by solving a number of important limitations that hindered the deployment of Neuroflight to real hardware. We benchmark RE+AL on the NF1 racing quadrotor developed as part of Neuroflight. We demonstrate that RE+AL significantly mitigates the previously observed issues of smoothness in RL agents. Additionally, RE+AL is shown to consistently train agents that are flight capable and with minimal degradation in controller quality upon transfer. RE+AL agents also learn to perform better than a tuned PID controller, with better tracking errors, smoother control, and reduced power consumption. To the best of our knowledge, RE+AL agents are the first RL-based controllers trained in simulation to outperform a well-tuned PID controller on a real-world controls problem that is solvable with classical control.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {sep},
articleno = {36},
numpages = {24},
keywords = {Neural networks, continuous control, quadrotor}
}

@inproceedings{10.1145/3607947.3608004,
author = {Ghode, Shilpa and Digalwar, Mayuri},
title = {A Novel Model Based Energy Management Strategy for Plug-in Hybrid Electric Vehicles Using Deep Reinforcement Learning},
year = {2023},
isbn = {9798400700224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607947.3608004},
doi = {10.1145/3607947.3608004},
abstract = {Over the last few years, Hybrid Electric Vehicles (HEVs) have become increasingly popular due to their potential to simplify fuel consumption and greenhouse gas emissions. The energy management of HEVs is a critical task that involves controlling the power split between the Internal Combustion Engine (ICE) and electric motor based on the vehicle’s state and driving conditions. Traditional rule-based strategies for HEV energy management may not be able to adapt to varying driving conditions or optimize the vehicle’s performance in real-time. To address this, researchers have explored the potential of advanced machine learning techniques, such as Deep Reinforcement Learning (DRL), as a more effective approach for HEV energy management. DRL is a subfield of machine learning that combines deep neural networks with reinforcement learning to learn an optimal control policy. Among various DRL algorithms, Deep Dyna-Q learning is a hybrid approach that combines model-based and model-free learning. Our paper introduces an innovative strategy for energy management for HEVs using Deep Dyna-Q learning that optimizes the power split in real-time based on the vehicle’s state and driving conditions. We evaluate the proposed strategy on two driving cycles and compare it with Deep Q-Learning (DQL). The findings indicate that the energy management approach presented in this paper surpasses DQL in terms of vehicle performance and fuel efficiency for both driving cycles.},
booktitle = {Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing},
pages = {289–293},
numpages = {5},
keywords = {Electric Vehicle, Energy Management, Deep Reinforcement Learning},
location = {Noida, India},
series = {IC3-2023}
}

@inproceedings{10.5555/1999416.1999484,
author = {Qela, Blerim and Mouftah, Hussein},
title = {Synergy of the Reinforcement Learning and Agent-Based Technique for Finding Optimal Solution in a Predefined Interval},
year = {2010},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {In this paper, a new algorithm for finding the optimal solution, in particular, finding maximum of a function in a predefined interval efficiently, by integrating reinforcement and agent based technique is presented. "Reinforcement Learning and Agent-based Search" application was implemented in C# to observe the algorithm at work and demonstrate its main features. The simulation results for several different functions are presented in order to demonstrate the result of synergy among 'reinforcement learning' and 'agent based' technique. In addition, its usefulness, in embedded systems with limited memory and/or processing power, such as the wireless sensor and/or actuator nodes are discussed.},
booktitle = {Proceedings of the 2010 Summer Computer Simulation Conference},
pages = {531–536},
numpages = {6},
keywords = {optimization, agent-based computing, computational intelligence, reinforcement learning},
location = {Ottawa, Ontario, Canada},
series = {SCSC '10}
}

@inproceedings{10.1145/3489517.3530518,
author = {Gohil, Vasudev and Patnaik, Satwik and Guo, Hao and Kalathil, Dileep and Rajendran, Jeyavijayan (JV)},
title = {DETERRENT: Detecting Trojans Using Reinforcement Learning},
year = {2022},
isbn = {9781450391429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489517.3530518},
doi = {10.1145/3489517.3530518},
abstract = {Insertion of hardware Trojans (HTs) in integrated circuits is a pernicious threat. Since HTs are activated under rare trigger conditions, detecting them using random logic simulations is infeasible. In this work, we design a reinforcement learning (RL) agent that circumvents the exponential search space and returns a minimal set of patterns that is most likely to detect HTs. Experimental results on a variety of benchmarks demonstrate the efficacy and scalability of our RL agent, which obtains a significant reduction (169\texttimes{}) in the number of test patterns required while maintaining or improving coverage (95.75\%) compared to the state-of-the-art techniques.},
booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference},
pages = {697–702},
numpages = {6},
keywords = {hardware trojans, reinforcement learning},
location = {San Francisco, California},
series = {DAC '22}
}

@inproceedings{10.1145/3594739.3612912,
author = {Zhang, Hengxi and Tang, Huaze and Ding, Wenbo and Zhang, Xiao-Ping},
title = {Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks},
year = {2023},
isbn = {9798400702006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594739.3612912},
doi = {10.1145/3594739.3612912},
abstract = {The Space-Air-Ground Integrated Network (SAGIN), integrating heterogeneous devices including low earth orbit (LEO) satellites, unmanned aerial vehicles (UAVs), and ground users (GUs), holds significant promise for the advancing smart city applications. However, resource management of the SAGIN is a challenge requiring urgent study in that inappropriate resource management will cause poor data transmission, and hence affect the services in smart cities. In this paper, we develop a comprehensive SAGIN system that encompasses five distinct communication links and propose an efficient cooperative multi-type multi-agent deep reinforcement learning (CMT-MARL) method to address the resource management issue. The experimental results highlight the efficacy of proposed CMT-MARL, as evidenced by key performance indicators such as the overall transmission rate and transmission success rate. These results underscore the potential value and feasibility of future implementation of the SAGIN.},
booktitle = {Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing \&amp; the 2023 ACM International Symposium on Wearable Computing},
pages = {712–717},
numpages = {6},
keywords = {resource management, SAGIN, multi-agent reinforcement learning},
location = {Cancun, Quintana Roo, Mexico},
series = {UbiComp/ISWC '23 Adjunct}
}

@inproceedings{10.1145/1553374.1553406,
author = {Diuk, Carlos and Li, Lihong and Leffler, Bethany R.},
title = {The Adaptive K-Meteorologists Problem and Its Application to Structure Learning and Feature Selection in Reinforcement Learning},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553406},
doi = {10.1145/1553374.1553406},
abstract = {The purpose of this paper is three-fold. First, we formalize and study a problem of learning probabilistic concepts in the recently proposed KWIK framework. We give details of an algorithm, known as the Adaptive k-Meteorologists Algorithm, analyze its sample-complexity upper bound, and give a matching lower bound. Second, this algorithm is used to create a new reinforcement-learning algorithm for factored-state problems that enjoys significant improvement over the previous state-of-the-art algorithm. Finally, we apply the Adaptive k-Meteorologists Algorithm to remove a limiting assumption in an existing reinforcement-learning algorithm. The effectiveness of our approaches is demonstrated empirically in a couple benchmark domains as well as a robotics navigation problem.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {249–256},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{10.1145/3568562.3568636,
author = {Phan, The Duy and Duc Luong, Tran and Hoang Quoc An, Nguyen and Nguyen Huu, Quyen and Nghi, Hoang Khoa and Pham, Van-Hau},
title = {Leveraging Reinforcement Learning and Generative Adversarial Networks to Craft Mutants of Windows Malware against Black-Box Malware Detectors},
year = {2022},
isbn = {9781450397254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568562.3568636},
doi = {10.1145/3568562.3568636},
abstract = {To build an effective malware detector, it is required to collect a diversity of malware samples and their evolution, since malware authors always try to evade detectors through strategies of malware mutation. So, this paper explores the ability to craft mutants of malware for gathering numerous mutated samples in training a machine learning (ML)-based malware detector. Specifically, we leverage Reinforcement Learning (RL) and Generative Adversarial Networks (GAN) to generate adversarial malware samples against ML-based detectors. The more we use this approach with different targeted antivirus and malware samples in training the RL agent as a malware mutator, the more it learns how to avoid black box malware detectors. The experimental results in real-world dataset indicate that RL can help GAN in crafting variants of malware with executability preservation to evade ML-based detectors and VirusTotal. Finally, this approach can be used as an automated tool for benchmarking the robustness of malware detectors against the metamorphic malwares.},
booktitle = {Proceedings of the 11th International Symposium on Information and Communication Technology},
pages = {31–38},
numpages = {8},
keywords = {generative adversarial networks, Metamorphic malware, evasion attack, reinforcement learning, Windows malware},
location = {Hanoi, Vietnam},
series = {SoICT '22}
}

@inproceedings{10.5555/1516744.1516964,
author = {Perron, Jimmy and Hogan, Jimmy and Moulin, Bernard and Berger, Jean and B\'{e}langer, Micheline},
title = {A Hybrid Approach Based on Multi-Agent Geosimulation and Reinforcement Learning to Solve a UAV Patrolling Problem},
year = {2008},
isbn = {9781424427086},
publisher = {Winter Simulation Conference},
abstract = {In this paper we address a dynamic distributed patrolling problem where a team of autonomous unmanned aerial vehicles (UAVs) patrolling moving targets over a large area must coordinate. We propose a hybrid approach combining multi-agent geosimulation and reinforcement learning enabling a group of agents to find near optimal solutions in realistic geo-referenced virtual environments. We present the COLMAS System which implements the proposed approach and show how a set of UAV can automatically find patrolling patterns in a dynamic environment characterized by unknown obstacles and moving targets. We also comment the value of the approach based on limited computational results.},
booktitle = {Proceedings of the 40th Conference on Winter Simulation},
pages = {1259–1267},
numpages = {9},
location = {Miami, Florida},
series = {WSC '08}
}

@inproceedings{10.5555/3408352.3408464,
author = {Settaluri, Keertana and Haj-Ali, Ameer and Huang, Qijing and Hakhamaneshi, Kourosh and Nikolic, Borivoje},
title = {AutoCkt: Deep Reinforcement Learning of Analog Circuit Designs},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Domain specialization under energy constraints in deeply-scaled CMOS has been driving the need for agile development of Systems on a Chip (SoCs). While digital subsystems have design flows that are conducive to rapid iterations from specification to layout, analog and mixed-signal modules face the challenge of a long human-in-the-middle iteration loop that requires expert intuition to verify that post-layout circuit parameters meet the original design specification. Existing automated solutions that optimize circuit parameters for a given target design specification have limitations of being schematic-only, inaccurate, sample-inefficient or not generalizable. This work presents AutoCkt, a machine learning optimization framework trained using deep reinforcement learning that not only finds post-layout circuit parameters for a given target specification, but also gains knowledge about the entire design space through a sparse subsampling technique. Our results show that for multiple circuit topologies, AutoCkt is able to converge and meet all target specifications on at least 96.3\% of tested design goals in schematic simulation, on average 40x faster than a traditional genetic algorithm. Using the Berkeley Analog Generator, AutoCkt is able to design 40 LVS passed operational amplifiers in 68 hours, 9.6x faster than the state-of-the-art when considering layout parasitics.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {490–495},
numpages = {6},
keywords = {analog sizing, reinforcement learning, transfer learning, automation of analog design},
location = {Grenoble, France},
series = {DATE '20}
}

@inproceedings{10.5555/2691365.2691411,
author = {Hantao, Huang and Manoj, P. D. Sai and Xu, Dongjun and Yu, Hao and Hao, Zhigang},
title = {Reinforcement Learning Based Self-Adaptive Voltage-Swing Adjustment of 2.5D I/Os for Many-Core Microprocessor and Memory Communication},
year = {2014},
isbn = {9781479962778},
publisher = {IEEE Press},
abstract = {A reinforcement learning based I/O management is developed for energy-efficient communication between many-core microprocessor and memory. Instead of transmitting data under a fixed large voltage-swing, an online reinforcement Q-learning algorithm is developed to perform a self-adaptive voltage-swing control of 2.5D through-silicon interposer (TSI) I/O circuits. Such a voltage-swing adjustment is formulated as a Markov decision process (MDP) problem solved by model-free reinforcement learning under constraints of both power budget and bit-error-rate (BER). Experimental results show that the adaptive 2.5D TSI I/Os designed in 65nm CMOS can achieve an average of 12.5mw I/O power, 4GHz bandwidth and 3.125pJ/bit energy efficiency for one channel under 10−6 BER, which has 18.89\% power saving and 15.11\% improvement of energy efficiency on average.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Conference on Computer-Aided Design},
pages = {224–229},
numpages = {6},
location = {San Jose, California},
series = {ICCAD '14}
}

@article{10.1145/3452008,
author = {Li, Shilei and Li, Meng and Su, Jiongming and Chen, Shaofei and Yuan, Zhimin and Ye, Qing},
title = {PP-PG: Combining Parameter Perturbation with Policy Gradient Methods for Effective and Efficient Explorations in Deep Reinforcement Learning},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3452008},
doi = {10.1145/3452008},
abstract = {Efficient and stable exploration remains a key challenge for deep reinforcement learning (DRL) operating in high-dimensional action and state spaces. Recently, a more promising approach by combining the exploration in the action space with the exploration in the parameters space has been proposed to get the best of both methods. In this article, we propose a new iterative and close-loop framework by combining the evolutionary algorithm (EA), which does explorations in a gradient-free manner directly in the parameters space with an actor-critic, and the deep deterministic policy gradient (DDPG) reinforcement learning algorithm, which does explorations in a gradient-based manner in the action space to make these two methods cooperate in a more balanced and efficient way. In our framework, the policies represented by the EA population (the parametric perturbation part) can evolve in a guided manner by utilizing the gradient information provided by the DDPG and the policy gradient part (DDPG) is used only as a fine-tuning tool for the best individual in the EA population to improve the sample efficiency. In particular, we propose a criterion to determine the training steps required for the DDPG to ensure that useful gradient information can be generated from the EA generated samples and the DDPG and EA part can work together in a more balanced way during each generation. Furthermore, within the DDPG part, our algorithm can flexibly switch between fine-tuning the same previous RL-Actor and fine-tuning a new one generated by the EA according to different situations to further improve the efficiency. Experiments on a range of challenging continuous control benchmarks demonstrate that our algorithm outperforms related works and offers a satisfactory trade-off between stability and sample efficiency.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jun},
articleno = {35},
numpages = {21},
keywords = {EA, parameter perturbation, policy gradient, Deep reinforcement learning, Computing methodologies →Artificial intelligence, Machine learning, Modeling and simulation, DDPG}
}

@inproceedings{10.1145/3378891.3378900,
author = {A\~{n}azco, Edwin Valarezo and Lopez, Patricio Rivera and Park, Hyemin and Park, Nahyeon and Oh, Jiheon and Lee, Sangmin and Byun, Kyungmin and Kim, Tae-Seong},
title = {Human-like Object Grasping and Relocation for an Anthropomorphic Robotic Hand with Natural Hand Pose Priors in Deep Reinforcement Learning},
year = {2020},
isbn = {9781450365130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378891.3378900},
doi = {10.1145/3378891.3378900},
abstract = {Anthropomorphic manipulators such as robotic hands (i.e., agent) can be used to perform complex object grasping tasks similar to human hands. However, teaching an agent to perform human-like grasping of objects is still a challenge in the robotics community. In this work, natural hand poses (i.e., priors) according to the shape of objects are used in training an agent via Deep Reinforcement Learning (DRL). To validate the effect of human-like grasping priors, the agent is trained to perform object grasping and relocation tasks including a single apple, milk box, and can. The results show that the trained agent with human-like hand pose priors in DRL is able to grasp the objects more naturally and performs object relocation with significantly improved success rate against the trained agent with only DRL.},
booktitle = {Proceedings of the 2019 2nd International Conference on Robot Systems and Applications},
pages = {46–50},
numpages = {5},
keywords = {Human-Like Object Grasping, Deep Reinforcement Learning, Natural Hand Pose Prior, Anthropomorphic Hand Manipulation},
location = {Moscow, Russian Federation},
series = {ICRSA '19}
}

@inproceedings{10.1145/3360322.3360992,
author = {Lahariya, Manu and Sadeghianpourhamami, Nasrin and Develder, Chris},
title = {Reduced State Space and Cost Function in Reinforcement Learning for Demand Response Control of Multiple EV Charging Stations},
year = {2019},
isbn = {9781450370059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360322.3360992},
doi = {10.1145/3360322.3360992},
abstract = {Electric vehicle (EV) charging stations represent a substantial load with significant flexibility. Balancing such load with model-free demand response (DR) based on reinforcement learning (RL) is an attractive approach. We build on previous RL research using a Markov decision process (MDP) to simultaneously coordinate multiple charging stations. The previously proposed approach is computationally expensive in terms of large training times, limiting its feasibility and practicality. We propose to a priori force the control policy to always fulfill any charging demand that does not offer any flexibility at a given point, and thus use an updated cost function. We compare the policy of the newly proposed approach with the original (costly) one, for the case of load flattening, in terms of (i) processing time to learn the RL-based charging policy, and (ii) overall performance of the policy decisions in terms of meeting the target load for unseen test data.},
booktitle = {Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {344–345},
numpages = {2},
keywords = {electric vehicle, Markov decision process, smart charging, Smart grid, demand response, reinforcement learning},
location = {New York, NY, USA},
series = {BuildSys '19}
}

@inproceedings{10.1145/3309772.3309802,
author = {Klose, Patrick and Mester, Rudolf},
title = {Simulated Autonomous Driving in a Realistic Driving Environment Using Deep Reinforcement Learning and a Deterministic Finite State Machine},
year = {2019},
isbn = {9781450360852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3309772.3309802},
doi = {10.1145/3309772.3309802},
abstract = {In the field of autonomous driving, the system controlling the vehicle can be seen as an agent acting in a complex environment and thus naturally fits into the modern framework of reinforcement learning. However, learning to drive can be a challenging task and current results are often restricted to simplified driving environments. To advance the field, we present a method to adaptively restrict the action space of the agent according to its current driving situation and show that it can be used to swiftly learn to drive in a realistic environment based on the deep Q-learning algorithm.},
booktitle = {Proceedings of the 2nd International Conference on Applications of Intelligent Systems},
articleno = {30},
numpages = {6},
keywords = {autonomous driving, machine learning, state machine, artificial intelligence, reinforcement learning},
location = {Las Palmas de Gran Canaria, Spain},
series = {APPIS '19}
}

@inproceedings{10.5555/3398761.3398987,
author = {Chen, Gang},
title = {A New Framework for Multi-Agent Reinforcement Learning – Centralized Training and Exploration with Decentralized Execution via Policy Distillation},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent deep reinforcement learning demands for highly coordinated environment exploration among all the participating agents. Previous research attempted to address this challenge through learning centralized value functions. However, the common strategy for every agent to learn their local policies directly may fail to nurture inter-agent collaboration and can be sample inefficient whenever agents alter their communication channels. To address these issues, we propose a new framework known as centralized training and exploration with decentralized execution via policy distillation. Guided by this framework, we will first train agents' policies with shared global component to foster coordinated and effective learning. Locally executable policies will be derived subsequently from the trained global policies via policy distillation.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1801–1803},
numpages = {3},
keywords = {deep reinforcement learning, policy distillation, multi-agent learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3580305.3599527,
author = {Yuan, Jianyong and Zhang, Jiayi and Cai, Zinuo and Yan, Junchi},
title = {Towards Variance Reduction for Reinforcement Learning of Industrial Decision-Making Tasks: A Bi-Critic Based Demand-Constraint Decoupling Approach},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599527},
doi = {10.1145/3580305.3599527},
abstract = {Learning to plan and schedule receives increasing attention due to its efficiency in problem-solving and potential to outperform heuristics. In particular, actor-critic-based reinforcement learning (RL) has been widely adopted for uncertain environments. Yet one standing challenge for applying RL to real-world industrial decision-making problems is the high variance during training. Existing efforts design novel value functions to alleviate the issue but still suffer. In this paper, we address this issue from the perspective of adjusting the actor-critic paradigm. We start by making an observation ignored in many industrial problems---the environmental dynamics for an agent consist of two parts physically independent of each other: the exogenous task demand over time and the hard constraint for action. And we theoretically show that decoupling these two effects in the actor-critic technique would reduce variance. Accordingly, we propose to decouple and model them separately in the state transition of the Markov decision process (MDP). In the demand-encoding process, the temporal task demand, e.g., the passengers for elevator scheduling is encoded followed by a critic for scoring. While in the constraint-encoding process, an actor-critic module is adopted for action embedding, and the two critics are then used for a revised advantaged function calculation. Experimental results show that our method can adaptively handle different dynamic planning and scheduling tasks and outperform recent learning-based models and traditional heuristic algorithms.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3162–3172},
numpages = {11},
keywords = {variance of gradient estimates reduction, reinforcement learning, industrial decision-making problem},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@article{10.1145/3609108,
author = {Basaklar, Toygun and Goksoy, A. Alper and Krishnakumar, Anish and Gumussoy, Suat and Ogras, Umit Y.},
title = {DTRL: Decision Tree-Based Multi-Objective Reinforcement Learning for Runtime Task Scheduling in Domain-Specific System-on-Chips},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3609108},
doi = {10.1145/3609108},
abstract = {Domain-specific systems-on-chip (DSSoCs) combine general-purpose processors and specialized hardware accelerators to improve performance and energy efficiency for a specific domain. The optimal allocation of tasks to processing elements (PEs) with minimal runtime overheads is crucial to achieving this potential. However, this problem remains challenging as prior approaches suffer from non-optimal scheduling decisions or significant runtime overheads. Moreover, existing techniques focus on a single optimization objective, such as maximizing performance. This work proposes DTRL, a decision-tree-based multi-objective reinforcement learning technique for runtime task scheduling in DSSoCs. DTRL trains a single global differentiable decision tree (DDT) policy that covers the entire objective space quantified by a preference vector. Our extensive experimental evaluations using our novel reinforcement learning environment demonstrate that DTRL captures the trade-off between execution time and power consumption, thereby generating a Pareto set of solutions using a single policy. Furthermore, comparison with state-of-the-art heuristic–, optimization–, and machine learning-based schedulers shows that DTRL achieves up to 9\texttimes{} higher performance and up to 3.08\texttimes{} reduction in energy consumption. The trained DDT policy achieves 120 ns inference latency on Xilinx Zynq ZCU102 FPGA at 1.2 GHz, resulting in negligible runtime overheads. Evaluation on the same hardware shows that DTRL achieves up to 16\% higher performance than a state-of-the-art heuristic scheduler.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {sep},
articleno = {113},
numpages = {22},
keywords = {multi-objective optimization, reinforcement learning, decision trees, Domain-specific system-on-chip, task scheduling, resource management}
}

@inproceedings{10.1145/3396851.3402658,
author = {Chi, Ce and Ji, Kaixuan and Marahatta, Avinab and Song, Penglei and Zhang, Fa and Liu, Zhiyong},
title = {Jointly Optimizing the IT and Cooling Systems for Data Center Energy Efficiency Based on Multi-Agent Deep Reinforcement Learning},
year = {2020},
isbn = {9781450380096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396851.3402658},
doi = {10.1145/3396851.3402658},
abstract = {With the development and application of cloud computing, the increasing amount of data centers has resulted in huge energy consumption and severe environmental problems. Improving the energy efficiency of data centers has become a necessity. In this paper, in order to improve the energy efficiency of both IT and cooling systems for data centers, a model-free deep reinforcement learning (DRL) based joint optimization approach MACEEC is proposed. To improve the cooperation between IT and cooling system while handling the high-dimensional state space and the large hybrid discrete-continuous action space, a hybrid AC-DDPG multi-agent structure is developed. A scheduling baseline comparison method is proposed to enhance the stability of the architecture. And an asynchronous control optimization algorithm is developed to solve the different responding time issue between IT and cooling system. Experiments based on real-world traces data validate that MACEEC can effectively improve the overall energy efficiency for data centers while ensuring the temperature constraint and service quality compared with existing joint optimization approaches.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Future Energy Systems},
pages = {489–495},
numpages = {7},
keywords = {cooling system, data center, multiagent, scheduling algorithm, energy efficiency, deep reinforcement learning},
location = {Virtual Event, Australia},
series = {e-Energy '20}
}

@inproceedings{10.1145/3563357.3564080,
author = {Fochesato, Marta and Khayatian, Fazel and Lima, Doris Fonseca and Nagy, Zoltan},
title = {On the Use of Conditional TimeGAN to Enhance the Robustness of a Reinforcement Learning Agent in the Building Domain},
year = {2022},
isbn = {9781450398909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563357.3564080},
doi = {10.1145/3563357.3564080},
abstract = {This paper develops an end-to-end data-driven pipeline to improve the out-of-sample performance of a Reinforcement Learning (RL) agent operating in the domain of building energy management. The approach can benefit researchers and practitioners that are confronted with the challenge of training robust control architectures when only few historical data are available to them. Under these circumstances, in fact, the RL agent is generally unable to respond robustly to unseen (possible, rare) events. To tackle this issue, we propose a data-driven procedure composed of two steps: (i) we develop a novel Generative Adversarial Network (GAN) architecture to create synthetic time series profiles of building performance; (ii) we infuse these artificial profiles into the original training dataset. The procedure is found to increase the robustness of the RL agent to rare events, without compromising the performance during "standard" operations. Extended simulations conducted on the CityLearn OpenAI Gym environement show that the GAN-enhanced RL agent's response displays better performance metrics with respect to a rule-based controller, with results generally improving with the data-enhancement process.},
booktitle = {Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {208–217},
numpages = {10},
keywords = {reinforcement learning, generative adversarial network, time-series forecasting, building energy management},
location = {Boston, Massachusetts},
series = {BuildSys '22}
}

@article{10.1109/TCBB.2020.3019042,
author = {Su, Yu-Ting and Lu, Yao and Chen, Mei and Liu, An-An},
title = {Deep Reinforcement Learning-Based Progressive Sequence Saliency Discovery Network for Mitosis Detection In Time-Lapse Phase-Contrast Microscopy Images},
year = {2020},
issue_date = {March-April 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2020.3019042},
doi = {10.1109/TCBB.2020.3019042},
abstract = {Mitosis detection plays an important role in the analysis of cell status and behavior and is therefore widely utilized in many biological research and medical applications. In this article, we propose a deep reinforcement learning-based progressive sequence saliency discovery network (PSSD)for mitosis detection in time-lapse phase contrast microscopy images. By discovering the salient frames when cell state changes in the sequence, PSSD can more effectively model the mitosis process for mitosis detection. We formulate the discovery of salient frames as a Markov Decision Process (MDP)that progressively adjusts the selection positions of salient frames in the sequence, and further leverage deep reinforcement learning to learn the policy in the salient frame discovery process. The proposed method consists of two parts: 1)the saliency discovery module that selects the salient frames from the input cell image sequence by progressively adjusting the selection positions of salient frames; 2)the mitosis identification module that takes a sequence of salient frames and performs temporal information fusion for mitotic sequence classification. Since the policy network of the saliency discovery module is trained under the guidance of the mitosis identification module, PSSD can comprehensively explore the salient frames that are beneficial for mitosis detection. To our knowledge, this is the first work to implement deep reinforcement learning to the mitosis detection problem. In the experiment, we evaluate the proposed method on the largest mitosis detection dataset, C2C12-16. Experiment results show that compared with the state-of-the-arts, the proposed method can achieve significant improvement for both mitosis identification and temporal localization on C2C12-16.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {aug},
pages = {854–865},
numpages = {12}
}

@article{10.5555/3546258.3546526,
author = {Raffin, Antonin and Hill, Ashley and Gleave, Adam and Kanervisto, Anssi and Ernestus, Maximilian and Dormann, Noah},
title = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {STABLE-BASELINES3 provides open-source implementations of deep reinforcement learning (RL) algorithms in Python. The implementations have been benchmarked against reference codebases, and automated unit tests cover 95\% of the code. The algorithms follow a consistent interface and are accompanied by extensive documentation, making it simple to train and compare different RL algorithms.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {268},
numpages = {8},
keywords = {software, PyTorch, open-source, reinforcement learning, Python, baselines}
}

@article{10.1145/3594264.3594267,
author = {Dong, Liang (Leon) and Qian, Yuchen and Gonzalez, Paulina and \"{O}z, Orhan K. and Sun, Xiankai},
title = {Advancing Drug Discovery with Deep Learning: Harnessing Reinforcement Learning and One-Shot Learning for Molecular Design in Low-Data Situations},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1559-6915},
url = {https://doi.org/10.1145/3594264.3594267},
doi = {10.1145/3594264.3594267},
abstract = {Drug discovery is a complex process that involves exploring vast chemical spaces to identify potential candidates for the development of effective drugs. While deep learning techniques have shown significant promise in data mining and can be used for molecular design, most drug discovery projects face limitations in low-data situations, making it difficult to train deep learning neural networks. In response to this challenge, this paper proposes a novel drug design system based on deep learning that adopts one-shot learning and reinforcement learning to operate in low-data conditions and generate new molecules with desired properties. Numerical experimental results show that our system can produce valid molecules with desired properties, including high negative logarithm of the half maximal inhibitory concentration (pIC50) values and logarithmic partition co-efficients (log P) values between 0 and 5. This model is applicable to other molecular design projects with limited data sets, thereby enhancing drug discovery efficiency and effectiveness.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {apr},
pages = {36–48},
numpages = {13},
keywords = {stacked gated recurrent unit, low-data situations, molecular design, one-shot learning, deep learning, graph neural networks, drug discovery, reinforcement learning}
}

@inproceedings{10.5555/3522802.3522912,
author = {Rinciog, Alexandru and Meyer, Anne},
title = {Fabricatio-Rl: A Reinforcement Learning Simulation Framework for Production Scheduling},
year = {2022},
publisher = {IEEE Press},
abstract = {Production scheduling is the task of assigning job operations to processing resources such that a target goal is optimized. constraints on job structure and resource capabilities, including stochastic influences, e.g. job arrivals, define individual problems. Reinforcement learning (RL) solvers are adaptive and potentially robust in highly stochastic settings. However, benchmarking RL solutions for stochastic problems is challenging, requiring the simulation of complex production settings while guaranteeing reproducible stochasticity. No such simulation is currently available. To cover this gap, we introduce FabricatioRL, an RL compatible, customizable and extensible benchmarking simulation framework. Our contribution is twofold: We first derive requirements to ensure that generic production setups can be covered, the simulation framework can interface with both traditional approaches and RL, and experiments are reproducible. Then, we detail the FabricatioRL design and implementation satisfying the obtained requirements in terms of framework input, core simulation process, and the interface with different scheduling systems.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {139},
numpages = {12},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/3449726.3459475,
author = {Hallawa, Ahmed and Born, Thorsten and Schmeink, Anke and Dartmann, Guido and Peine, Arne and Martin, Lukas and Iacca, Giovanni and Eiben, A. E. and Ascheid, Gerd},
title = {Evo-RL: Evolutionary-Driven Reinforcement Learning},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3459475},
doi = {10.1145/3449726.3459475},
abstract = {In this work, we propose a novel approach for reinforcement learning driven by evolutionary computation. Our algorithm, dubbed as Evolutionary-Driven Reinforcement Learning (Evo-RL), embeds the reinforcement learning algorithm in an evolutionary cycle, where we distinctly differentiate between purely evolvable (instinctive) behaviour versus purely learnable behaviour. Furthermore, we propose that this distinction is decided by the evolutionary process, thus allowing Evo-RL to be adaptive to different environments. In addition, Evo-RL facilitates learning on environments with reward-less states, which makes it more suited for real-world problems with incomplete information. To show that Evo-RL leads to state-of-the-art performance, we present the performance of different state-of-the-art reinforcement learning algorithms when operating within Evo-RL and compare it with the case when these same algorithms are executed independently. Results show that reinforcement learning algorithms embedded within our Evo-RL approach significantly outperform the stand-alone versions of the same RL algorithms on OpenAI Gym control problems with rewardless states constrained by the same computational budget.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {153–154},
numpages = {2},
keywords = {evolutionary computation, reinforcement learning, artificial life},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{10.1145/3297858.3304058,
author = {Cho, Hyungmin and Oh, Pyeongseok and Park, Jiyoung and Jung, Wookeun and Lee, Jaejin},
title = {FA3C: FPGA-Accelerated Deep Reinforcement Learning},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304058},
doi = {10.1145/3297858.3304058},
abstract = {Deep Reinforcement Learning (Deep RL) is applied to many areas where an agent learns how to interact with the environment to achieve a certain goal, such as video game plays and robot controls. Deep RL exploits a DNN to eliminate the need for handcrafted feature engineering that requires prior domain knowledge. The Asynchronous Advantage Actor-Critic (A3C) is one of the state-of-the-art Deep RL methods. In this paper, we present an FPGA-based A3C Deep RL platform, called FA3C. Traditionally, FPGA-based DNN accelerators have mainly focused on inference only by exploiting fixed-point arithmetic. Our platform targets both inference and training using single-precision floating-point arithmetic. We demonstrate the performance and energy efficiency of FA3C using multiple A3C agents that learn the control policies of six Atari 2600 games. Its performance is better than a high-end GPU-based platform (NVIDIA Tesla P100). FA3C achieves 27.9\% better performance than that of a state-of-the-art GPU-based implementation. Moreover, the energy efficiency of FA3C is 1.62x better than that of the GPU-based implementation.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {499–513},
numpages = {15},
keywords = {FPGA, reinforcement learning, deep neural networks},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@inproceedings{10.1145/3491315.3491334,
author = {Gou, Yu and Zhang, Tong and Liu, Jun and Yang, Tingting and Song, Shanshan and Cui, Jun-Hong},
title = {Achieving Time-Sharing and Spatial-Reuse Underwater Wireless Sensor Networks with Communication Fairness: A Distributed Deep Multi-Agent Reinforcement Learning Approach},
year = {2022},
isbn = {9781450395625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491315.3491334},
doi = {10.1145/3491315.3491334},
abstract = {It is difficult to provide qualified and fair communications for time-sharing and spatial reuse underwater wireless sensor networks when energy supplements are limited, the environment is non-stationary, and communication interference is strong. Due to the physical separation of underwater nodes, transmissions are intended to occur concurrently to maximize network capacity. Currently available approaches for improving fairness are frequently at the expense of network capacity. Methods that seek a better trade-off between fairness and network capacity are required. This paper proposes a novel approach to maximize network capacity and improve communication fairness by increasing simultaneous communications, achieving time-sharing, and spatial-reuse UWSNs. It is a distributed, multi-agent reinforcement learning approach that utilizes an observation encoder and a local utility network to coordinate collaboration across underwater nodes by adaptively tuning transmit parameters in response to local observations. In terms of network capacity, fairness, and reuse, we compared the suggested methodology to standard methods. Experiments reveal that, when compared to other ways, ours maximizes reuse and produces a significantly superior trade-off between network capacity and fairness, while still meeting lifetime and energy restrictions. The work presented in this article is anticipated to develop into valuable tools for designing and optimizing UWSNs.},
booktitle = {Proceedings of the 15th International Conference on Underwater Networks \&amp; Systems},
articleno = {26},
numpages = {5},
keywords = {Cooperative multi-agent system (MAS), resource management., network capacity optimization, resource-constrained networks, Underwater Wireless Sensor Networks (UWSNs)},
location = {Shenzhen, Guangdong, China},
series = {WUWNet '21}
}

@article{10.5555/3546258.3546335,
author = {Fujita, Yasuhiro and Nagarajan, Prabhat and Kataoka, Toshiki and Ishikawa, Takahiro},
title = {ChainerRL: A Deep Reinforcement Learning Library},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {In this paper, we introduce ChainerRL, an open-source deep reinforcement learning (DRL) library built using Python and the Chainer deep learning framework. ChainerRL implements a comprehensive set of DRL algorithms and techniques drawn from state-of-the-art research in the field. To foster reproducible research, and for instructional purposes, ChainerRL provides scripts that closely replicate the original papers' experimental settings and reproduce published benchmark results for several algorithms. Lastly, ChainerRL offers a visualization tool that enables the qualitative inspection of trained agents.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {77},
numpages = {14},
keywords = {chainer, reinforcement learning, deep reinforcement learning, open source software, reproducibility}
}

@inproceedings{10.1145/2557500.2557533,
author = {Lampe, Thomas and Fiederer, Lukas D.J. and Voelker, Martin and Knorr, Alexander and Riedmiller, Martin and Ball, Tonio},
title = {A Brain-Computer Interface for High-Level Remote Control of an Autonomous, Reinforcement-Learning-Based Robotic System for Reaching and Grasping},
year = {2014},
isbn = {9781450321846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2557500.2557533},
doi = {10.1145/2557500.2557533},
abstract = {We present an Internet-based brain-computer interface (BCI) for controlling an intelligent robotic device with autonomous reinforcement-learning. BCI control was achieved through dry-electrode electroencephalography (EEG) obtained during imaginary movements. Rather than using low-level direct motor control, we employed a high-level control scheme of the robot, acquired via reinforcement learning, to keep the users cognitive load low while allowing control a reaching-grasping task with multiple degrees of freedom. High-level commands were obtained by classification of EEG responses using an artificial neural network approach utilizing time-frequency features and conveyed through an intuitive user interface. The novel ombination of a rapidly operational dry electrode setup, autonomous control and Internet connectivity made it possible to conveniently interface subjects in an EEG laboratory with remote robotic devices in a closed-loop setup with online visual feedback of the robots actions to the subject. The same approach is also suitable to provide home-bound patients with the possibility to control state-of-the-art robotic devices currently confined to a research environment. Thereby, our BCI approach could help severely paralyzed patients by facilitating patient-centered research of new means of communication, mobility and independence.},
booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
pages = {83–88},
numpages = {6},
keywords = {robots, semi-autonomous systems, camera-based uis, machine learning and data mining},
location = {Haifa, Israel},
series = {IUI '14}
}

@inproceedings{10.5555/1030818.1031010,
author = {Cao, Heng and Xi, Haifeng and Smith, Stephen F.},
title = {Supply Chain Planning: A Reinforcement Learning Approach to Production Planning in the Fabrication/Fulfillment Manufacturing Process},
year = {2003},
isbn = {0780381327},
publisher = {Winter Simulation Conference},
abstract = {We have used Reinforcement Learning together with Monte Carlo simulation to solve a multi-period production planning problem in a two-stage hybrid manufacturing process (a combination of build-to-plan with build-to-order) with a capacity constraint. Our model minimizes inventory and penalty costs while considering real-world complexities such as different component types sharing the same manufacturing capacity, multi-end-products sharing common components, multi-echelon bill-of-material (BOM), random lead times, etc. To efficiently search in the huge solution space, we designed a two-phase learning scheme where "good" capacity usage ratios are first found for different decision epochs, based on which a detailed production schedule is further improved through learning to minimize costs. We will illustrate our approach through an example and conclude the paper with a discussion of future research directions.},
booktitle = {Proceedings of the 35th Conference on Winter Simulation: Driving Innovation},
pages = {1417–1423},
numpages = {7},
location = {New Orleans, Louisiana},
series = {WSC '03}
}

@inproceedings{10.1109/ISCA.2008.21,
author = {Ipek, Engin and Mutlu, Onur and Mart\'{\i}nez, Jos\'{e} F. and Caruana, Rich},
title = {Self-Optimizing Memory Controllers: A Reinforcement Learning Approach},
year = {2008},
isbn = {9780769531748},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISCA.2008.21},
doi = {10.1109/ISCA.2008.21},
abstract = {Efficiently utilizing off-chip DRAM bandwidth is a critical issuein designing cost-effective, high-performance chip multiprocessors(CMPs). Conventional memory controllers deliver relativelylow performance in part because they often employ fixed,rigid access scheduling policies designed for average-case applicationbehavior. As a result, they cannot learn and optimizethe long-term performance impact of their scheduling decisions,and cannot adapt their scheduling policies to dynamic workloadbehavior.We propose a new, self-optimizing memory controller designthat operates using the principles of reinforcement learning (RL)to overcome these limitations. Our RL-based memory controllerobserves the system state and estimates the long-term performanceimpact of each action it can take. In this way, the controllerlearns to optimize its scheduling policy on the fly to maximizelong-term performance. Our results show that an RL-basedmemory controller improves the performance of a set of parallelapplications run on a 4-core CMP by 19\% on average (upto 33\%), and it improves DRAM bandwidth utilization by 22\%compared to a state-of-the-art controller.},
booktitle = {Proceedings of the 35th Annual International Symposium on Computer Architecture},
pages = {39–50},
numpages = {12},
keywords = {Memory Controller, Machine Learning, Chip Multiprocessors, Memory Systems, Reinforcement Learning},
series = {ISCA '08}
}

@article{10.1145/1394608.1382172,
author = {Ipek, Engin and Mutlu, Onur and Mart\'{\i}nez, Jos\'{e} F. and Caruana, Rich},
title = {Self-Optimizing Memory Controllers: A Reinforcement Learning Approach},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/1394608.1382172},
doi = {10.1145/1394608.1382172},
abstract = {Efficiently utilizing off-chip DRAM bandwidth is a critical issuein designing cost-effective, high-performance chip multiprocessors(CMPs). Conventional memory controllers deliver relativelylow performance in part because they often employ fixed,rigid access scheduling policies designed for average-case applicationbehavior. As a result, they cannot learn and optimizethe long-term performance impact of their scheduling decisions,and cannot adapt their scheduling policies to dynamic workloadbehavior.We propose a new, self-optimizing memory controller designthat operates using the principles of reinforcement learning (RL)to overcome these limitations. Our RL-based memory controllerobserves the system state and estimates the long-term performanceimpact of each action it can take. In this way, the controllerlearns to optimize its scheduling policy on the fly to maximizelong-term performance. Our results show that an RL-basedmemory controller improves the performance of a set of parallelapplications run on a 4-core CMP by 19\% on average (upto 33\%), and it improves DRAM bandwidth utilization by 22\%compared to a state-of-the-art controller.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {39–50},
numpages = {12},
keywords = {Memory Controller, Reinforcement Learning, Machine Learning, Memory Systems, Chip Multiprocessors}
}

@inproceedings{10.5555/1605285.1605289,
author = {Litman, Diane and Singh, Satinder and Kearns, Michael and Walker, Marilyn},
title = {NJFun: A Reinforcement Learning Spoken Dialogue System},
year = {2000},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper describes NJFun, a real-time spoken dialogue system that provides users with information about things to do in New Jersey. NJFun automatically optimizes its dialogue strategy over time, by using a methodology for applying reinforcement learning to a working dialogue system with human users.},
booktitle = {Proceedings of the ANLP-NAACL 2000 Workshop on Conversational Systems},
pages = {17–20},
numpages = {4},
location = {Seattle, Washington},
series = {ConversationalSys '00}
}

@inproceedings{10.1145/1297231.1297250,
author = {Taghipour, Nima and Kardan, Ahmad and Ghidary, Saeed Shiry},
title = {Usage-Based Web Recommendations: A Reinforcement Learning Approach},
year = {2007},
isbn = {9781595937308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1297231.1297250},
doi = {10.1145/1297231.1297250},
abstract = {Information overload is no longer news; the explosive growth of the Internet has made this issue increasingly serious for Web users. Users are very often overwhelmed by the huge amount of information and are faced with a big challenge to find the most relevant information in the right time. Recommender systems aim at pruning this information space and directing users toward the items that best meet their needs and interests. Web Recommendation has been an active application area in Web Mining and Machine Learning research. In this paper we propose a novel machine learning perspective toward the problem, based on reinforcement learning. Unlike other recommender systems, our system does not use the static patterns discovered from web usage data, instead it learns to make recommendations as the actions it performs in each situation. We model the problem as Q-Learning while employing concepts and techniques commonly applied in the web usage mining domain. We propose that the reinforcement learning paradigm provides an appropriate model for the recommendation problem, as well as a framework in which the system constantly interacts with the user and learns from her behavior. Our experimental evaluations support our claims and demonstrate how this approach can improve the quality of web recommendations.},
booktitle = {Proceedings of the 2007 ACM Conference on Recommender Systems},
pages = {113–120},
numpages = {8},
keywords = {machine learning, web usage mining, recommender systems, reinforcement learning, personalization},
location = {Minneapolis, MN, USA},
series = {RecSys '07}
}

@article{10.1145/3414685.3417796,
author = {Hu, Ruizhen and Xu, Juzhan and Chen, Bin and Gong, Minglun and Zhang, Hao and Huang, Hui},
title = {TAP-Net: Transport-and-Pack Using Reinforcement Learning},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3414685.3417796},
doi = {10.1145/3414685.3417796},
abstract = {We introduce the transport-and-pack (TAP) problem, a frequently encountered instance of real-world packing, and develop a neural optimization solution based on reinforcement learning. Given an initial spatial configuration of boxes, we seek an efficient method to iteratively transport and pack the boxes compactly into a target container. Due to obstruction and accessibility constraints, our problem has to add a new search dimension, i.e., finding an optimal transport sequence, to the already immense search space for packing alone. Using a learning-based approach, a trained network can learn and encode solution patterns to guide the solution of new problem instances instead of executing an expensive online search. In our work, we represent the transport constraints using a precedence graph and train a neural network, coined TAP-Net, using reinforcement learning to reward efficient and stable packing. The network is built on an encoder-decoder architecture, where the encoder employs convolution layers to encode the box geometry and precedence graph and the decoder is a recurrent neural network (RNN) which inputs the current encoder output, as well as the current box packing state of the target container, and outputs the next box to pack, as well as its orientation. We train our network on randomly generated initial box configurations, without supervision, via policy gradients to learn optimal TAP policies to maximize packing efficiency and stability. We demonstrate the performance of TAP-Net on a variety of examples, evaluating the network through ablation studies and comparisons to baselines and alternative network designs. We also show that our network generalizes well to larger problem instances, when trained on small-sized inputs.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {232},
numpages = {15},
keywords = {reinforcement learning, transport-and-pack, neural networks for combinatorial optimization, packing problem}
}

@inproceedings{10.5555/3437539.3437570,
author = {Kiourti, Panagiota and Wardega, Kacper and Jha, Susmit and Li, Wenchao},
title = {TrojDRL: Evaluation of Backdoor Attacks on Deep Reinforcement Learning},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
abstract = {We present TrojDRL, a tool for exploring and evaluating backdoor attacks on deep reinforcement learning agents. TrojDRL exploits the sequential nature of deep reinforcement learning (DRL) and considers different gradations of threat models. We show that untargeted attacks on state-of-the-art actor-critic algorithms can circumvent existing defenses built on the assumption of backdoors being targeted. We evaluated TrojDRL on a broad set of DRL benchmarks and showed that the attacks require only poisoning as little as 0.025\% of the training data. Compared with existing works of backdoor attacks on classification models, TrojDRL provides a first step towards understanding the vulnerability of DRL agents.},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {31},
numpages = {6},
keywords = {C.1.3.i neural nets, I.2.6.g machine learning, K.4.4.f security, G.4.g reliability and robustness},
location = {Virtual Event, USA},
series = {DAC '20}
}

