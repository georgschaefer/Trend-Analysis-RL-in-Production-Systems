"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Compliant Robotic Assembly based on Deep Reinforcement Learning","Z. Zhou; P. Ni; X. Zhu; Q. Cao","School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China","2021 International Conference on Machine Learning and Intelligent Systems Engineering (MLISE)","25 Nov 2021","2021","","","6","9","Assembly with industrial robots in an unstructured environment is more and more desirable and going to play an indispensable role in modern manufacturing industries. The classical peg-in-hole robotic assembly has been extensively researched as a common industrial task, but high-precision robotic assembly, which means the assembly precision of mechanical parts required may exceed that of robots, remains an open problem. Meanwhile, the conventional impedance control method used in the current manufacturing, requires numerous parameters to be artificially tuned before deployment, tedious and awfully time-consuming. Moreover, it has a poor success rate. In this paper, we propose a novel method with off-policy, model-free deep reinforcement-learning for position-controlled robots to perform assembly tasks compliantly through training, and there is no need for manual tuning. Through conducting a series of comparative experiments, this method is proved to have a preferable performance and assembly efficiency.","","978-1-6654-1736-5","10.1109/MLISE54096.2021.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9611647","Deep Reinforcement learning;Robotic assembly;Impendance control;Compliance control","Robotic assembly;Training;Manufacturing industries;Service robots;Reinforcement learning;Manuals;Manufacturing","compliance control;force control;learning (artificial intelligence);motion control;position control;robotic assembly","compliant robotic assembly;deep reinforcement learning;industrial robots;modern manufacturing industries;peg-in-hole robotic assembly;industrial task;high-precision robotic assembly;mechanical parts;open problem;impedance control method;model-free deep reinforcement-learning;position-controlled robots;assembly tasks","","1","","15","IEEE","25 Nov 2021","","","IEEE","IEEE Conferences"
"Data-driven scheduling for smart shop floor via reinforcement learning with model-based clustering algorithm","Y. Li; W. Gu; X. Wang; Z. Chen","Department of Mechanical and Electrical Engineering, Hohai University, Changzhou, China; Department of Mechanical and Electrical Engineering, Hohai University, Changzhou, China; Department of Mechanical and Electrical Engineering, Hohai University, Changzhou, China; Department of Mechanical and Electrical Engineering, Hohai University, Changzhou, China","2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)","19 Jul 2021","2021","4","","1310","1314","Various information technologies provide the manufacturing system massive data, which can support the decision optimization of product lifecycle management. However, the lack of effective use for advanced technologies prevents manufacturing industry from being automated and intelligent. Therefore, this paper proposes the smart shop floor and implementation mechanism. Meanwhile, based on the clustering and reinforcement learning, the brain agent and scheduling agent are designed to determine the approriate rule according to the shop floor state information, thus realizing the data-driven real-time scheduling. Experimental results show that the smart shop floor can effectively deal with disturbance events and has better performance compared with composite dispatching rules.","2693-2776","978-1-7281-8535-4","10.1109/IMCEC51613.2021.9482089","National Science Foundation; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9482089","data-driven;real-time scheduling;smart shop floor;clustering;reinforcement learning","Product lifecycle management;Manufacturing industries;Job shop scheduling;Reinforcement learning;Production;Real-time systems;Information management","decision support systems;factory automation;information technology;integrated manufacturing systems;job shop scheduling;learning (artificial intelligence);manufacturing industries;multi-agent systems;optimisation;pattern clustering;product life cycle management;production engineering computing","smart shop floor;information technologies;manufacturing system massive data;product lifecycle management;shop floor state information;data-driven real-time scheduling;reinforcement learning;model-based clustering algorithm;decision optimization;manufacturing industry;brain agent;scheduling agent","","1","","10","IEEE","19 Jul 2021","","","IEEE","IEEE Conferences"
"Demand Response optimization of Cement Manufacturing Industry Based on Reinforcement Learning Algorithm","X. -Y. Ye; Z. -W. Liu; M. Chi; M. -F. Ge; Z. Xi","School of Artificial Intelligence and Automation Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation Huazhong University of Science and Technology, Wuhan, China; School of Mechanical Engineering and Electronic Information, China University of Geosciences, Wuhan, China; School of Artificial Intelligence and Automation Huazhong University of Science and Technology, Wuhan, China","2022 IEEE International Conference on Cyborg and Bionic Systems (CBS)","8 May 2023","2023","","","402","406","For industrial manufacturing to meet the country’s carbon neutrality goals by 2026, energy efficiency optimization is becoming a crucial development orientation. Demand response has played a significant role in improving energy efficiency and balancing supply and demand. The dynamic industrial environment requires the high level of precision and reliability of demand response. Reinforcement learning methods can make more accurate judgments and optimal decisions in response to complex dynamic environments compared with traditional optimization methods, and meet the relevant requirements of demand response. The paper studies cement manufacturing, a typical energy-intensive industry. As a first step, an in-depth modeling analysis of cement manufacturing’s main energy-consuming equipment is conducted based on industrial load characteristics. Then, industrial demand response scheduling methods based on a reinforcement learning algorithm are designed. The effectiveness and feasibility of the scheme are verified by simulation experiments.","","978-1-6654-9028-3","10.1109/CBS55922.2023.10115387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10115387","","Manufacturing industries;Analytical models;Job shop scheduling;Supply and demand;Biological system modeling;Reinforcement learning;Demand response","cement industry;demand side management;energy conservation;optimisation;power engineering computing;reinforcement learning","cement manufacturing industry;demand response optimization;energy efficiency optimization;energy-consuming equipment;industrial demand response scheduling methods;industrial manufacturing;reinforcement learning algorithm;typical energy-intensive industry","","","","11","IEEE","8 May 2023","","","IEEE","IEEE Conferences"
"Sim2Real Deep Reinforcement Learning of Compliance-based Robotic Assembly Operations","O. Petrovic; L. Schäper; S. Roggendorf; S. Storms; C. Brecher","Department of Automation and Control Chair of Machine Tools, Laboratory for Machine Tools and Production Engineering (WZL), RWTH Aachen University, Aachen, Germany; Department of Automation and Control Chair of Machine Tools, Laboratory for Machine Tools and Production Engineering (WZL), RWTH Aachen University, Aachen, Germany; Department of Automation and Control Chair of Machine Tools, Laboratory for Machine Tools and Production Engineering (WZL), RWTH Aachen University, Aachen, Germany; Department of Automation and Control Chair of Machine Tools, Laboratory for Machine Tools and Production Engineering (WZL), RWTH Aachen University, Aachen, Germany; Department of Automation and Control Chair of Machine Tools, Laboratory for Machine Tools and Production Engineering (WZL), RWTH Aachen University, Aachen, Germany","2022 26th International Conference on Methods and Models in Automation and Robotics (MMAR)","8 Sep 2022","2022","","","300","305","Reinforcement learning (RL) enables robots to learn goal-oriented behavior. In production processes with high variances, such as joining operations in end-of-line assembly, this is particularly interesting to save significant programming effort. Due to a large amount of required training data, simulative training is becoming increasingly important. In this paper, we present an approach to learn a contact-rich peg-in-hole assembly task utilizing deep reinforcement learning (DRL) and a compliant robot controller. The DRL-Agent learns directly in the Cartesian space (task space) and not in the joint space of the robot, to increase the robustness and efficiency of the algorithms. To further increase the robustness of the policy and to shorten training times, geometric limitations are imposed by introducing an admissible workspace using a trajectory generator. Furthermore, these limitations result in nearly identical behavior in the simulation and on the real robot, allowing the DRL training process to be purely simulative. The learned policy is experimentally investigated both in the simulation environment and on a real robot, to evaluate its transferability from simulation to reality (sim2real).","","978-1-6654-6858-9","10.1109/MMAR55195.2022.9874304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9874304","","Training;Robotic assembly;Training data;Reinforcement learning;Aerospace electronics;Robustness;Behavioral sciences","control engineering computing;data handling;deep learning (artificial intelligence);production engineering computing;reinforcement learning;robotic assembly;trajectory control","sim2real deep reinforcement;goal oriented behavior;production processes;end-of-line assembly;required training data;simulative training;deep reinforcement learning;DRL agent;Cartesian space;joint space;geometric limitations;identical behavior;DRL training process;learned policy;simulation environment;contact rich peg-in-hole assembly task;compliance based robotic assembly operations","","","","20","IEEE","8 Sep 2022","","","IEEE","IEEE Conferences"
"An MDP Model-Based Reinforcement Learning Approach for Production Station Ramp-Up Optimization: Q-Learning Analysis","S. Doltsinis; P. Ferreira; N. Lohse","Manufacturing Division, University of Nottingham, University Park, Nottingham, U.K.; Manufacturing Division, University of Nottingham, University Park, Nottingham, U.K.; Manufacturing Division, University of Nottingham, University Park, Nottingham, U.K.","IEEE Transactions on Systems, Man, and Cybernetics: Systems","20 May 2017","2014","44","9","1125","1138","Ramp-up is a significant bottleneck for the introduction of new or adapted manufacturing systems. The effort and time required to ramp-up a system is largely dependent on the effectiveness of the human decision making process to select the most promising sequence of actions to improve the system to the required level of performance. Although existing work has identified significant factors influencing the effectiveness of ramp-up, little has been done to support the decision making during the process. This paper approaches ramp-up as a sequential adjustment and tuning process that aims to get a manufacturing system to a desirable performance in the fastest possible time. Production stations and machines are the key resources in a manufacturing system. They are often functionally decoupled and can be treated in the first instance as independent ramp-up problems. Hence, this paper focuses on developing a Markov decision process (MDP) model to formalize ramp-up of production stations and enable their formal analysis. The aim is to capture the cause-and-effect relationships between an operator's adaptation or adjustment of a station and the station's response to improve the effectiveness of the process. Reinforcement learning has been identified as a promising approach to learn from ramp-up experience and discover more successful decision-making policies. Batch learning in particular can perform well with little data. This paper investigates the application of a Q-batch learning algorithm combined with an MDP model of the ramp-up process. The approach has been applied to a highly automated production station where several ramp-up processes are carried out. The convergence of the Q-learning algorithm has been analyzed along with the variation of its parameters. Finally, the learned policy has been applied and compared against previous ramp-up cases.","2168-2232","","10.1109/TSMC.2013.2294155","European Commission as a part of the CP-FP 246083-2 IDEAS and CP-FP 229208-2 FRAME Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702489","Decision-making;learning systems;manufacturing automation;Markov processes;Decision-making;learning systems;manufacturing automation;Markov processes","Decision making;Manufacturing systems;Learning (artificial intelligence);Personnel;Algorithm design and analysis","decision making;learning (artificial intelligence);manufacturing systems;Markov processes;production engineering computing","MDP model-based reinforcement learning approach;production station ramp-up optimization;manufacturing systems;human decision making process;system improvement;performance level improvement;sequential adjustment;tuning process;production machines;Markov decision process model;formal analysis;cause-and-effect relationships;operator adaptation;station response adjustment;process effectiveness improvement;Q-batch learning algorithm;automated production station;convergence analysis","","42","","34","OAPA","9 Jan 2014","","","IEEE","IEEE Journals"
"Opportunities for Reinforcement Learning in Industrial Automation","Q. Xin; G. Wu; W. Fang; J. Cao; Y. Ping","Nanhu Lab, Jiaxing, China; National University of Defense Technology, Changsha, China; Nanhu Lab, Jiaxing, China; Academy of Military Science, Beijing, China; Academy of Military Science, Beijing, China","2021 7th International Conference on Big Data and Information Analytics (BigDIA)","6 Dec 2021","2021","","","496","504","With the development of technology and the increasingly fierce competitive environment, the manufacturing system becomes more and more complex and requires higher coordination. Manufacturing companies are facing the challenge of highly non-linear and flexible business requirements, while the current degree of automation in the smart era has been unable to meet the needs of the environment. In the next stage, the leading manufacturing enterprises should focus on the application of digitalization and even intelligence in the automated manufacturing industry. Additionally, the successful implementation of intelligent applications in the industry and the realization of their ideal value are also the part that researchers must consider. The development of artificial intelligence (AI) and machine learning (ML) has shown that there is great potential to change the manufacturing field through advanced intelligent system tools, especially through reinforcement learning (QL). Therefore, the focuses of this article have been shown as follows: (1) Review the practical application of recent reinforcement learning in various fields, especially in the industrial field. (2) Analyze the method of reinforcement learning applied in industrial applications and its unique performance. (3) Identify the challenges and opportunities for further application of reinforcement learning in automated manufacturing, and discuss the future development of reinforcement learning to better meet the needs of intelligent manufacturing. In order to achieve these goals, this article uses a large number of literature reviews to elaborate a hierarchical analysis of the extensive practice of reinforcement learning in industrial applications. Through these analyses, we try to find the applicability of reinforcement learning in automated manufacturing systems. In addition, we take industrial process systems, human-machine assistance supervision and control, process monitoring, prevention and post-processing, and finally full material process management into consideration, and hope to achieve the desired characteristics in the process and control of the automated manufacturing industry.","","978-1-6654-2466-0","10.1109/BigDIA53151.2021.9619637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9619637","Reinforcement learning;artificial intelligence;industrial automation","Manufacturing industries;Technological innovation;Automation;Costs;Process control;Reinforcement learning;Companies","intelligent manufacturing systems;manufacturing industries;process monitoring;production engineering computing;reinforcement learning","industrial applications;automated manufacturing systems;automated manufacturing industry;industrial automation;manufacturing system;manufacturing companies;flexible business requirements;leading manufacturing enterprises;artificial intelligence;machine learning;manufacturing field;recent reinforcement learning;intelligent manufacturing;intelligent system tools;smart era;hierarchical analysis;human-machine assistance supervision;process monitoring","","","","34","IEEE","6 Dec 2021","","","IEEE","IEEE Conferences"
"Dynamic assembly sequence selection using reinforcement learning","G. Lowe; B. Shirinzadeh","School of Computer Science and Software Engineering, Monash University, Australia; Depanment of Mechanical Engineering, Monash University, Australia","IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004","6 Jul 2004","2004","3","","2633","2638 Vol.3","Determining the most appropriate sequence for assembling products requires assessment of the process, product, and the technology applied. Most production engineers apply constraint based evaluation and history to identify the solution sequence. What if their solution is sub-optimal? In this paper a self-learning technique for selecting a sequence and dynamically changing the sequence is presented, selection is based on the history of assemblies. The evaluation is dependent on part properties rather than parts and their relationships, thus no previous knowledge of parts and their interaction is required in the decision making process. The method assumes assembly is without constraint, for example, a highly flexible robotic assembly cell. This maximises the ability of the algorithm to select sequences for new products and optimise them. The heart of the algorithm is a reinforcement-learning model, which punishes failed assembly steps, this facilitates feedback sequence selection, when: current methods are merely feedforward. This feedback approach addresses combinatorial explosion that can cripple assembly planners.","1050-4729","0-7803-8232-3","10.1109/ROBOT.2004.1307458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1307458","","Learning;Robotic assembly;Production;Application specific processors;Humans;History;Feedback;Process planning;Product design;Computer science","assembly planning;learning (artificial intelligence);process control;knowledge based systems;decision making;robotic assembly","dynamic assembly sequence selection;reinforcement learning;self-learning technique;decision making process;flexible robotic assembly cell;assembly planning","","8","","13","IEEE","6 Jul 2004","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Robotic Control in High-Dexterity Assembly Tasks - A Reward Curriculum Approach","L. Leyendecker; M. Schmitz; H. A. Zhou; V. Samsonov; M. Rittstieg; D. Lütticke","Fraunhofer Institute for Production Technology IPT, 52074 Aachen, Germany; Departments of Data Analytics and Innovation, Digitalization at BMW Group, 80807 Munich, Germany; Institute of Information Management in Mechanical Engineering, RWTH Aachen University, 52068 Aachen, Germany; Institute of Information Management in Mechanical Engineering, RWTH Aachen University, 52068 Aachen, Germany; Departments of Data Analytics and Innovation, Digitalization at BMW Group, 80807 Munich, Germany; Institute of Information Management in Mechanical Engineering, RWTH Aachen University, 52068 Aachen, Germany","2021 Fifth IEEE International Conference on Robotic Computing (IRC)","7 Feb 2022","2021","","","35","42","For years, the fully-automated robotic assembly has been a highly sought-after technology in large-scale manufacturing. Yet it still struggles to find widespread implementation in industrial environments. Traditional programming has so far proven to be insufficient to provide the required flexibility and dexterity to solve complex assembly tasks. Although research in robotic control using deep reinforcement learning (DRL) advances quickly, the transfer to real-world applications in industrial settings is lagging behind. In this study, we apply DRL for robotic motion control at the use-case of a multi-body contact automotive assembly task and focus on optimizing the final performance on the real-world setup. We propose a reward-curriculum learning approach in combination with domain randomization to obtain both force-sensitivity and generalizability. We train an agent exclusively in simulation and successfully perform the Sim-to-Real transfer. Finally, we evaluate the controller’s performance and robustness on an industrial setup and reflect its adherence to the high automotive production standards.","","978-1-6654-3416-4","10.1109/IRC52146.2021.00012","Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9699929","Reinforcement Learning;Machine Learning;Robot Control;Curriculum Learning;Reward Shaping;Peg In Hole Assembly;Manufacturing","Robotic assembly;Robot motion;Service robots;Reinforcement learning;Production;Programming;Robustness","automobile industry;deep learning (artificial intelligence);dexterous manipulators;motion control;production engineering computing;reinforcement learning;robotic assembly","high-dexterity assembly tasks;robotic assembly;large-scale manufacturing;industrial environments;complex assembly tasks;DRL;robotic motion control;multibody contact automotive assembly task;reward-curriculum learning approach;controller;industrial setup;high automotive production standards;deep reinforcement learning;domain randomization;Sim-to-Real transfer","","4","","45","IEEE","7 Feb 2022","","","IEEE","IEEE Conferences"
"The application of a reinforcement learning agent to a multi-product manufacturing facility","D. C. Creighton; S. Nahavandi","School of Engineering and Technology, Deakin University, Geelong, VIC, Australia; School of Engineering and Technology, Deakin University, Geelong, VIC, Australia","2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.","2 Apr 2003","2002","2","","1229","1234 vol.2","An intelligent agent-based scheduling system, consisting of a reinforcement learning agent and a simulation model has been developed and tested on a classic scheduling problem. The production facility studied is a multiproduct serial line subject to stochastic failure. The agent goal is to minimise total production costs, through selection of job sequence and batch size. To explore state space the agent used reinforcement learning. By applying an independent inventory control policy for each product, the agent successfully identified optimal operating policies for a real production facility.","","0-7803-7657-9","10.1109/ICIT.2002.1189350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1189350","","Learning;Production facilities;Job shop scheduling;Intelligent manufacturing systems;Intelligent agent;System testing;Stochastic processes;Job production systems;Costs;Space exploration","computer aided production planning;learning (artificial intelligence);software agents;manufacturing industries;stochastic processes;minimisation;state-space methods;stock control","reinforcement learning agent;multiproduct manufacturing facility;intelligent agent-based scheduling system;multiproduct serial line;stochastic failure;total production cost minimisation;job sequence selection;batch size selection;independent inventory control policy;optimal operating policies","","6","","17","IEEE","2 Apr 2003","","","IEEE","IEEE Conferences"
"Fuzzy Logic-Driven Variable Time-Scale Prediction-Based Reinforcement Learning for Robotic Multiple Peg-in-Hole Assembly","Z. Hou; Z. Li; C. Hsu; K. Zhang; J. Xu","Department of Mechanical Engineering, State Key Laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, Tsinghua University, Beijing, China; Department of Mechanical Engineering, State Key Laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, Tsinghua University, Beijing, China; Department of Mechanical Engineering, State Key Laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, Tsinghua University, Beijing, China; Department of Mechanical Engineering, State Key Laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, Tsinghua University, Beijing, China; Department of Mechanical Engineering, State Key Laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, Tsinghua University, Beijing, China","IEEE Transactions on Automation Science and Engineering","5 Jan 2022","2022","19","1","218","229","Reinforcement learning (RL) has been increasingly used for single peg-in-hole assembly, where assembly skill is learned through interaction with the assembly environment in a manner similar to skills employed by human beings. However, the existing RL algorithms are difficult to apply to the multiple peg-in-hole assembly because the much more complicated assembly environment requires sufficient exploration, resulting in a long training time and less data efficiency. To this end, this article focuses on how to predict the assembly environment and how to use the predicted environment in assembly action control to improve the data efficiency of the RL algorithm. Specifically, first, the assembly environment is exactly predicted by a variable time-scale prediction (VTSP) defined as general value functions (GVFs), reducing the unnecessary exploration. Second, we propose a fuzzy logic-driven variable time-scale prediction-based reinforcement learning (FLDVTSP-RL) for assembly action control to improve the efficiency of the RL algorithm, in which the predicted environment is mapped to the impedance parameter in the proposed impedance action space by a fuzzy logic system (FLS) as the action baseline. To demonstrate the effectiveness of VTSP and the data efficiency of the FLDVTSP-RL methods, a dual peg-in-hole assembly experiment is set up; the results show that FLDVTSP-deep Q-learning (DQN) decreases the assembly time about 44% compared with DQN and FLDVTSP-deep deterministic policy gradient (DDPG) decreases the assembly time about 24% compared with DDPG. Note to Practitioners—The complicated assembly environment of the multiple peg-in-hole assembly results in a contact state that cannot be recognized exactly from the force sensor. Therefore, contact-model-based methods that require tuning of the control parameters based on the contact state recognition cannot be applied directly in this complicated environment. Recently, reinforcement learning (RL) methods without contact state recognition have recently attracted scientific interest. However, the existing RL methods still rely on numerous explorations and a long training time, which cannot be directly applied to real-world tasks. This article takes inspiration from the manner in which human beings can learn assembly skills with a few trials, which relies on the variable time-scale predictions (VTSPs) of the environment and the optimized assembly action control strategy. Our proposed fuzzy logic-driven variable time-scale prediction-based reinforcement learning (FLDVTSP-RL) can be implemented in two steps. First, the assembly environment is predicted by the VTSP defined as general value functions (GVFs). Second, assembly action control is realized in an impedance action space with a baseline defined by the impedance parameter mapped from the predicted environment by the fuzzy logic system (FLS). Finally, a dual peg-in-hole assembly experiment is conducted; compared with deep Q-learning (DQN), FLDVTSP-DQN can decrease the assembly time about 44%; compared with deep deterministic policy gradient (DDPG), FLDVTSP-DDPG can decrease the assembly time about 24%.","1558-3783","","10.1109/TASE.2020.3024725","National Key Research and Development Program of China(grant numbers:2016YFE0206200); National Natural Science Foundation of China(grant numbers:U1613205,51675291,51935010); Beijing Municipal Natural Science Foundation(grant numbers:L192001); Funding for Basic Scientific Research Program(grant numbers:JCKY2018205B029); State Key Laboratory of China(grant numbers:SKL2020C15); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9210190","Fuzzy logic system (FLS);multiple peg-in-hole;prediction learning;reinforcement learning (RL);robotic assembly","Impedance;Robotic assembly;Task analysis;Force;Fuzzy logic;Robot sensing systems","assembling;force sensors;fuzzy logic;learning (artificial intelligence);robotic assembly","existing RL algorithms;multiple peg-in-hole assembly;complicated assembly environment;long training time;data efficiency;predicted environment;RL algorithm;fuzzy logic-driven variable time-scale prediction;impedance action space;fuzzy logic system;FLDVTSP-RL methods;dual peg-in-hole assembly experiment;FLDVTSP-deep Q-learning;assembly time;peg-in-hole assembly results;complicated environment;reinforcement learning methods;existing RL methods;assembly skill;optimized assembly action control strategy","","16","","40","IEEE","30 Sep 2020","","","IEEE","IEEE Journals"
"Multi-Agent Reinforcement Learning for the Energy Optimization of Cyber-Physical Production Systems","J. Bakakeu; D. Kisskalt; J. Franke; S. Baer; H. -H. Klos; J. Peschke","Institute for Factory Automation and Production Systems, Friedrich-Alexander-Universität Erlangen-Nürnberg, Egerlandstr. 7-9, Erlangen, Germany; Institute for Factory Automation and Production Systems, Friedrich-Alexander-Universität Erlangen-Nürnberg, Egerlandstr. 7-9, Erlangen, Germany; Institute for Factory Automation and Production Systems, Friedrich-Alexander-Universität Erlangen-Nürnberg, Egerlandstr. 7-9, Erlangen, Germany; Digital Factory Division, Siemens AG, Gleiwitzer Str. 555, Nürnberg, Germany; Digital Factory Division, Siemens AG, Gleiwitzer Str. 555, Nürnberg, Germany; Digital Factory Division, Siemens AG, Gleiwitzer Str. 555, Nürnberg, Germany","2020 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)","19 Nov 2020","2020","","","1","8","The paper proposes an artificial intelligence-based solution for the efficient operation of a heterogeneous cluster of flexible manufacturing machines with energy generation and storage capabilities in an electricity micro-grid featuring high volatility of electricity prices. The problem of finding the optimal control policy is first formulated as a game-theoretic sequential decision-making problem under uncertainty, where at every time step the uncertainty is characterized by future weather-dependent energy prices, high demand fluctuation, as well as random unexpected disturbances on the factory floor. Because of the parallel interaction of the machines with the grid, the local viewpoints of an agent are non-stationary and non-Markovian. Therefore, traditional methods such as standard reinforcement learning approaches that learn a specialized policy for a single machine are not applicable. To address this problem, we propose a multi-agent actor-critic method that takes into account the policies of other participants to achieve explicit coordination between a large numbers of actors. We show the strength of our approach in mixed cooperative and competitive scenarios where different production machines were able to discover different coordination strategies in order to increase the energy efficiency of the whole factory floor.","2576-7046","978-1-7281-5442-8","10.1109/CCECE47787.2020.9255795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9255795","Flexible Manufacturing System;Load Management;Reinforcement Learning;Proximal Policy Optimization;Actor-Critic;Multi-Agent System;Industrie 4.0;Autocurricula","Optimization;Reinforcement learning;Training;Production facilities;Uncertainty;Renewable energy sources;Task analysis","cyber-physical systems;decision making;distributed power generation;flexible manufacturing systems;game theory;learning (artificial intelligence);Markov processes;multi-agent systems;optimal control;optimisation;power markets;production engineering computing","coordination strategies;nonstationary agent;energy efficiency;production machines;multiagent actor-critic method;single machine;standard reinforcement learning approaches;nonMarkovian agent;parallel interaction;factory floor;random unexpected disturbances;future weather-dependent energy prices;game-theoretic sequential decision-making problem;optimal control policy;electricity prices;electricity microgrid;storage capabilities;energy generation;flexible manufacturing machines;heterogeneous cluster;artificial intelligence-based solution;cyber-physical production systems;energy optimization;multiagent reinforcement learning","","6","","46","IEEE","19 Nov 2020","","","IEEE","IEEE Conferences"
"Demonstrating Reinforcement Learning for Maintenance Scheduling in a Production Environment","J. Giner; R. Lamprecht; V. Gallina; C. Laflamme; L. Sielaff; W. Sihn","Factory Planning and Production Management Fraunhofer Austria Research GmbH, Vienna, Austria; Center for Cyber Cognitive Intelligence (CCI), Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Stuttgart, Germany; Production Planning and Order Managagement Fraunhofer Austria Research GmbH, Vienna, Austria; Factory Planning and Production Management Fraunhofer Austria Research GmbH, Vienna, Austria; Maintenance and Asset Management Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Stuttgart, Germany; Institute of Management Science, TU Wien Fraunhofer Austria Research GmbH, Vienna, Austria","2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA )","30 Nov 2021","2021","","","1","8","As the automation of production lines in modern manufacturing environments becomes ubiquitous, their flexibility and resilience become increasingly important. Consequently, the scheduling of maintenance activities is growing more complex and at the same time ever more crucial for ensuring adequate system availability. In this paper a digital model of a production environment is presented, using building blocks and restrictions that can be found in most modern production environments. Maintenance and repair activities in the model are scheduled by a reinforcement learning agent for different proof-of-concept scenarios, which can be optimised using measures such as maximizing production capacity and minimizing maintenance costs. The results of this paper provide the basis for further work to improve the working conditions of human maintenance planners by providing a reliable decision support system which facilitates the task of scheduling planned and unplanned maintenance activities.","","978-1-7281-2989-1","10.1109/ETFA45728.2021.9613205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9613205","reinforcement learning;maintenance scheduling;automation;decision support;simulation","Degradation;Decision support systems;Adaptation models;Job shop scheduling;Object oriented modeling;Production;Reinforcement learning","decision support systems;learning (artificial intelligence);maintenance engineering;scheduling","maximizing production capacity;minimizing maintenance costs;human maintenance planners;unplanned maintenance activities;maintenance scheduling;production environment;production lines;modern manufacturing environments;flexibility;adequate system availability;digital model;building blocks;modern production environments;repair activities;reinforcement learning agent","","","","20","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Reinforcement learning approach to re-entrant manufacturing system scheduling","Chang-chun Liu; Hui-yu Jin; Yu Tian; Hai-bin Yu","Shenyang Institute of Automation, Chinese Academy and Sciences, Shenyang, China; Shenyang Institute of Automation, Chinese Academy and Sciences, Shenyang, China; Shenyang Institute of Automation, Chinese Academy and Sciences, Shenyang, China; Shenyang Institute of Automation, Chinese Academy and Sciences, Shenyang, China","2001 International Conferences on Info-Tech and Info-Net. Proceedings (Cat. No.01EX479)","6 Aug 2002","2001","3","","280","285 vol.3","In this paper, we focus on the problem of optimally scheduling a closed re-entrant system with one type of parts and two service centers, each of which consisting of one machine. An algorithm based on reinforcement learning is proposed. The results of the experiments indicate that reinforcement learning can outperform some familiar heuristic methods and is closed to the workload balancing policy.","","0-7803-7010-4","10.1109/ICII.2001.983070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=983070","","Learning;Manufacturing systems;Job shop scheduling;Dynamic programming;Manufacturing automation;Cities and towns;Semiconductor device manufacture;Uncertainty;Heuristic algorithms;Scheduling algorithm","production control;computer aided production planning;learning (artificial intelligence);dynamic programming","reentrant manufacturing system;reinforcement learning;temporal difference learning;scheduling;heuristic methods;workload balancing policy;dynamic programming","","3","","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Heterogeneous Multi-Robot Task Allocation for Garment Transformable Production using Deep Reinforcement Learning","R. Bezerra; K. Ohno; S. Kojima; H. A. Aryadi; K. Gunji; M. Kuwahara; Y. Okada; M. Konyo; S. Tadokoro","Tough Cyberphysical AI Research Center, Tohoku University, Japan; Tough Cyberphysical AI Research Center, Tohoku University, Japan; Tough Cyberphysical AI Research Center, Tohoku University, Japan; Graduate School of Information Sciences, Tohoku University, Japan; Graduate School of Information Sciences, Tohoku University, Japan; Graduate School of Information Sciences, Tohoku University, Japan; Graduate School of Information Sciences, Tohoku University, Japan; Graduate School of Information Sciences, Tohoku University, Japan; Graduate School of Information Sciences, Tohoku University, Japan","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","8","This paper addresses the need for a more practical, flexible and scalable approach to garment manufacturing amidst the fashion industry's current shift towards transformable production. The current trend for personalized, on-demand orders has resulted in a need for greater involvement of static units capable of carrying out certain tasks (e.g. assembly, cutting, ironing) and mobile robots during the garment production process. Previous research has dealt with the development of a scheduler for these types of robots once all orders have been received. Although meaningful, this model remains impractical in realistic production settings. This paper introduces TransPRES (Transformable Production REsource Scheduler), a deep reinforcement learning-based scheduler that assigns three different kinds of static and mobile robots to individual tasks based on demand. This newly proposed method uses a tailor-made state to represent the transformable production, and an action that schedules the corresponding resource based on its type. To evaluate this approach, we developed a transformable production simulator that generates garment manufacturing orders with varying numbers of tasks, processing times, hierarchy, and resources required to complete them. Furthermore, we extended a heuristic-based approach and an additional reinforcement learning-based approach for comparison within a transformable production environment. Results show that our method outperforms previous approaches by being able to schedule 11% more tasks using 2 ms per flop which, in turn, demonstrates its effectiveness for the future of transformable production.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260437","","Schedules;Job shop scheduling;Clothing;Production;Reinforcement learning;Market research;Manufacturing","clothing industry;deep learning (artificial intelligence);mobile robots;multi-robot systems;production engineering computing;reinforcement learning;resource allocation;scheduling","deep reinforcement learning-based scheduler;fashion industry;flexible approach;garment manufacturing orders;garment transformable production;heterogeneous multirobot task allocation;heuristic-based approach;mobile robots;on-demand orders;realistic production settings;reinforcement learning-based approach;scalable approach;static robots;time 2.0 ms;transformable production environment;transformable production resource scheduler;transformable production simulator","","","","24","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"A reinforcement learning approach to support setup decisions in distributed manufacturing systems","P. McDonnell; S. Joshi","Industrial and Manufacturing Engineering, Pennsylvania State University, University Park, PA, USA; Industrial and Manufacturing Engineering, Pennsylvania State University, University Park, PA, USA","1997 IEEE 6th International Conference on Emerging Technologies and Factory Automation Proceedings, EFTA '97","6 Aug 2002","1997","","","221","225","A reinforcement learning approach to specifying payoffs for setup games is presented. Setup games are normal form, noncooperative games used by heterarchical machine controllers to evaluate reconfiguration decisions. While past work utilizing heuristic measures to approximate the effect of setup decisions has demonstrated promising performance, the lack of an accurate long-term model of system dynamics in these heuristic approaches limits their usefulness. The reinforcement learning approach iteratively learns the long term costs of setup decisions, accounting for both immediate decision effects and the effects of likely downstream decisions.","","0-7803-4192-9","10.1109/ETFA.1997.616272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=616272","","Learning;Manufacturing systems;Control systems;Centralized control;Costs;Automatic control;Electrical equipment industry;Manufacturing industries;Fault tolerant systems;Dynamic programming","production control;learning (artificial intelligence);game theory","reinforcement learning;setup decisions;distributed manufacturing systems;normal form noncooperative games;heterarchical machine controllers;reconfiguration decisions;iterative learning","","1","","8","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A Dynamic Chemical Production Scheduling Method based on Reinforcement Learning","Z. Wu; Y. Wang; L. Jia","School of Control Science and Engineering, Dalian University of Technology, Dalian, China; School of Control Science and Engineering, Dalian University of Technology, Dalian, China; Shenzhen Decard Smartcard Tech Co.,Ltd, Shenzhen, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","4841","4846","The capacity and demands of the modern chemical industry are increasing day by day. Dynamic chemical production scheduling refers to the processing strategy implemented in response to changing order demands. This problem is difficult to solve by traditional methods due to the uncertainty of task objectives. The proximal policy optimization algorithms, which is a kind of reinforcement learning methods, is introduced into the dynamic chemical production scheduling problem in this paper. And an improved state function considering the difference between short-term and long-term orders is proposed. It effectively solved the chemical production scheduling problem with uncertainty. In addition, experiments are also carried out on the dynamic chemical production scheduling model, and compared with the policy gradient algorithm. The results show that the method proposed in this paper obtains more rewards in scheduling, faster convergence, and less performance fluctuation. Which proved that the methods presented in this paper can stably deal with the complexity and uncertainty of chemical production scheduling.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055985","reinforcement learning;chemical industry;optimization;scheduling","Job shop scheduling;Uncertainty;Heuristic algorithms;Production;Reinforcement learning;Dynamic scheduling;Stability analysis","chemical engineering computing;chemical industry;gradient methods;optimisation;reinforcement learning;scheduling","chemical industry;dynamic chemical production scheduling method;dynamic chemical production scheduling model;dynamic chemical production scheduling problem;order demands;policy gradient algorithm;proximal policy optimization algorithms;reinforcement learning methods","","","","17","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Knowledge-based generation of a plant-specific reinforcement learning framework for energy reduction of production plants","E. Schmidl; E. Fischer; M. Wenk; J. Franke","Department of Mechanical & Environmental Engineering, Technical University of Applied, Sciences Amberg-Weiden, Amberg, Germany; Institute for Factory Automation and Production Systems, University of Erlangen-Nürnberg, (FAU), Erlangen, Germany; Department of Mechanical & Environmental Engineering, Technical University of Applied, Sciences Amberg-Weiden, Amberg, Germany; Institute for Factory Automation and Production Systems, University of Erlangen-Nürnberg, (FAU), Erlangen, Germany","2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)","5 Oct 2020","2020","1","","1","4","In research, there are more and more successful approaches to operate production plants more resource-efficiently and more productively with the help of reinforcement learning. An important point is the reduction of the energy demand of production plants. Instead of manually implementing complex standby strategies rigidly in a PLC, an intelligent system can train to derive decisions about the optimal energetic state of each component autonomously. Since learning in a virtual environment has decisive advantages, simulation models with sufficient accuracy are necessary. Many of the previous implementations of reinforcement learning approaches in an industrial environment are usually tailored to a specific plant. In particular, the agent's scope of action and its connection to the environment must be adapted manually for a new plant. The presented solution shows an intelligent system which automatically optimizes the virtual learning environment with the support of plant knowledge and adapts the reinforcement learning agent to the respective production plant.","1946-0759","978-1-7281-8956-7","10.1109/ETFA46521.2020.9211957","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211957","reinforcement learning;knowledge-based;energy reduction;virtual models","Adaptation models;Electronic learning;Conferences;Knowledge based systems;Virtual environments;Reinforcement learning;Production","learning (artificial intelligence);production engineering computing;virtualisation","energy reduction;production plants;energy demand;intelligent system;virtual learning environment;knowledge-based generation;plant-specific reinforcement learning","","4","","19","IEEE","5 Oct 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning With Composite Rewards for Production Scheduling in a Smart Factory","T. Zhou; D. Tang; H. Zhu; L. Wang","College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IEEE Access","5 Jan 2021","2021","9","","752","766","Rapid advances of sensing and cloud technologies transform the manufacturing system into a data-rich environment and make production scheduling increasingly complex. Traditional offline scheduling methods are limited in the ability to handle low-volume-high-mix workorders with diverse design specifications. Simulation-based methods show the promise for distributed scheduling of manufacturing jobs but are mostly implemented with historical data and empirical rules in a static manner. Recently, artificial intelligence (AI) algorithms fuel increasing interests to solve dynamic scheduling problems in the manufacturing setting. However, it’s difficult to utilize high-dimensional data for production scheduling while considering multiple practical objectives for smart manufacturing (e.g., minimize the makespan, reduce production costs, balance workloads). Therefore, this paper presents a new AI scheduler with composite reward functions for data-driven dynamic scheduling of manufacturing jobs under uncertainty in a smart factory. Internet-enabled sensor networks are deployed in the smart factory to track real-time statuses of workorders, machines, and material handling systems. A novel manufacturing value network is developed to take high-dimensional data as the input and then learn the state-action values for real-time decision making. Based on reinforcement learning (RL), composite rewards help the AI scheduler learn efficiently to achieve multiple objectives for production scheduling in real time. The proposed methodology is evaluated and validated with experimental studies in a smart manufacturing setting. Experimental results show that the new AI scheduler not only improves the multi-objective performance metrics in the production scheduling problem but also effectively copes with unexpected events (e.g., urgent workorders, machine failures) in manufacturing systems.","2169-3536","","10.1109/ACCESS.2020.3046784","National Key Research and Development Program of China(grant numbers:2020YFB1710500); National Natural Science Foundation of China(grant numbers:52075257); Fundamental Research Funds for the Central Universities(grant numbers:NP2020304); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305707","Production scheduling;reinforcement learning;composite reward;smart factory;neural network","Job shop scheduling;Production;Optimal scheduling;Manufacturing;Smart manufacturing;Artificial intelligence;Dynamic scheduling","artificial intelligence;decision making;dynamic scheduling;Internet;learning (artificial intelligence);manufacturing systems;materials handling;production control;production engineering computing;production facilities;scheduling","production scheduling problem;smart manufacturing setting;novel manufacturing value network;data-driven dynamic scheduling;composite reward functions;AI scheduler;high-dimensional data;dynamic scheduling problems;artificial intelligence algorithms fuel;manufacturing jobs;distributed scheduling;low-volume-high-mix workorders;traditional offline scheduling methods;data-rich environment;manufacturing system;smart factory;composite rewards;reinforcement learning","","25","","33","CCBY","23 Dec 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning-based Sim-to-Real Impedance Parameter Tuning for Robotic Assembly","Y. -G. Kim; M. Na; J. -B. Song","Department of Mechanical Engineering, Korea University, Seoul, Korea; Department of Mechanical Engineering, Korea University, Seoul, Korea; Department of Mechanical Engineering, Korea University, Seoul, Korea","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","833","836","When performing robotic assembly, a task should be conducted through force-based control such as impedance control. Using impedance control, it is possible to control the contact force by appropriately adjusting the impedance parameters. However, the impedance parameters should be set by the user because it is difficult to accurately recognize the dynamics of the contact environment, which takes a lot of time because it should be performed whenever the assembly task changes. Moreover, the parameters may not be optimal because it depends on the experience and skill level of the user. To this end, a reinforcement learning-based impedance parameter tuning method is proposed in this study. Since this method uses only the physics-based robotic simulation on the virtual environment, there is no risk of damaging the robots or parts and learning time can be significantly reduced. The proposed method was verified by assembling an HDMI connector with a tolerance of 0.03 mm. Impedance parameters were learned in the virtual environment and transferred to the real environment. Finally, it was confirmed that parameter tuning for impedance without the aid of the user is possible by using the proposed method.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649923","Reinforcement learning;Robotic assembly;Connector assembly","Connectors;Robotic assembly;Learning systems;Contacts;Force;Virtual environments;Impedance","force control;reinforcement learning;robot programming;robotic assembly","reinforcement learning;robotic assembly;impedance control;contact force;physics-based robotic simulation;virtual environment;sim-to-real impedance parameter tuning;force based control;HDMI connector","","1","","5","","28 Dec 2021","","","IEEE","IEEE Conferences"
"A reinforcement learning approach to production planning in the fabrication/fulfillment manufacturing process","Cao; Xi; Smith","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Carnegie Mellon University, Pittsburgh, PA, USA","Proceedings of the 2003 Winter Simulation Conference, 2003.","30 Jan 2004","2003","2","","1417","1423 vol.2","We have used reinforcement learning together with Monte Carlo simulation to solve a multiperiod production planning problem in a two-stage hybrid manufacturing process (a combination of build-to-plan with build-to-order) with a capacity constraint. Our model minimizes inventory and penalty costs while considering real-world complexities such as different component types sharing the same manufacturing capacity, multiend-products sharing common components, multiechelon bill-of-material (BOM), random lead times, etc. To efficiently search in the huge solution space, we designed a two-phase learning scheme where ""good"" capacity usage ratios are first found for different decision epochs, based on which a detailed production schedule is further unproved through learning to minimize costs. We illustrate our approach through an example and conclude discussion of future research directions.","","0-7803-8131-9","10.1109/WSC.2003.1261584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1261584","","Learning;Production planning;Fabrication;Manufacturing processes;Costs;Bills of materials;Robotic assembly;Capacity planning;Partitioning algorithms;Monte Carlo methods","learning (artificial intelligence);production planning;Monte Carlo methods;push-pull production;supply chain management;manufacturing processes;bills of materials;discrete event simulation","reinforcement learning;Monte Carlo simulation;multiperiod production planning;hybrid manufacturing process;inventory minimization;production scheduling;cost minimization;fabrication;manufacturing process","","2","1","9","IEEE","30 Jan 2004","","","IEEE","IEEE Conferences"
"Autonomous decision-making method of transportation process for flexible job shop scheduling problem based on reinforcement learning","J. Yan; Z. Liu; T. Zhang; Y. Zhang","Institute of Advanced Manufacturing and Intelligent Technology, Faculty of Materials and Manufacturing, Beijing University of Technology, Beijing, China; Key Laboratory of CNC Equipment Reliability, Ministry of Education, School of Mechanical and Aerospace Engineering Jilin University, Jilin, China; Machinery Industry Key Laboratory of Heavy Machine Tool Digital Design and Testing Technology, Beijing University of Technology, Beijing, China; Institute of Advanced Manufacturing and Intelligent Technology, Faculty of Materials and Manufacturing, Beijing University of Technology, Beijing, China","2021 International Conference on Machine Learning and Intelligent Systems Engineering (MLISE)","25 Nov 2021","2021","","","234","238","With the development of intelligent manufacturing technology, automatic guided vehicle (AGV) has been widely used in workshop logistics transportation. However, the complex and changeable production process requires efficient and dynamic transportation and distribution. In this paper, based on the results of process scheduling, a method of independent decision-making transportation process is studied. A transportation strategy training method with breakpoint continuation and hierarchical feedback is proposed based on deep Q-network (DQN). Transportation scheduling can be quickly decided and adjusted to adapt to dynamic and changeable orders. The method is applied to the FJSP problem, and the data experiments show the effectiveness of the method.","","978-1-6654-1736-5","10.1109/MLISE54096.2021.00049","National Natural Science Foundation of China(grant numbers:51975019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9611659","component;Logistics transportation;Reinforcement learning;Deep Q-network;Flexible job shop scheduling","Training;Schedules;Job shop scheduling;Decision making;Transportation;Reinforcement learning;Production","automatic guided vehicles;decision making;deep learning (artificial intelligence);hierarchical systems;intelligent manufacturing systems;job shop scheduling;logistics;production engineering computing;reinforcement learning;transportation","autonomous decision-making method;flexible job shop scheduling problem;reinforcement learning;intelligent manufacturing technology;workshop logistics transportation;complex production process;changeable production process;dynamic transportation;process scheduling;independent decision-making transportation process;transportation strategy training method;FJSP problem;automatic guided vehicle;AGV;dynamic distribution;breakpoint continuation;hierarchical feedback;deep Q-network transportation scheduling;DQN","","6","","14","IEEE","25 Nov 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning-based Real-time Scheduling Under Random Machine Breakdowns and Other Disturbances: A Case Study","M. Ghaleb; H. A. Namoura; S. Taghipour",Ryerson University; University of Alberta; Ryerson University,"2021 Annual Reliability and Maintainability Symposium (RAMS)","22 Nov 2021","2021","","","1","8","In practice, production lines are dynamic and subject to several disruptions, unforeseen events, and requirements. Examples of such disruptions and events include random machine breakdowns, new order arrivals, order cancellations, due date changes, and shortage of material. Production schedules are adapted to such events by conducting rescheduling continuously, using real-time information about the current status of work-in-progress, machines, and resources on a shop-floor. This level of connectivity and real-time information sharing is achieved with the help of advanced initiatives in manufacturing technologies and industrial informatics such as Industry 4.0. Industry 4.0, driven by many emerging technologies, such as cyber-physical systems (CPS), internet of things (IoT), and internet of services (IoS), delivers real-time actionable data for smart decision-making in manufacturing. Several optimization approaches have been proposed to take advantage of such technology by incorporating the use of real-time information in the optimization process. Recently, with the increasing power of new machine learning (ML) algorithms in solving real-world problems, several ML approaches have been introduced to production planning and scheduling.In this paper, to achieve the Industry 4.0 vision in production control, we apply a reinforcement learning (RL) approach to real-time scheduling (RTS). The proposed RL based RTS uses a multiple dispatching rules (MDRs) strategy to enhance the production performance. A case study of a smart manufacturing firm is considered to apply the proposed approach. The firm is located in Ontario (Canada) and specializes in thermoplastic injection molding of various components and assemblies.The production schedules on the shop floor are sensitive to the changes resulting from random breakdowns and their associated maintenance activities. The production managers are using the data from the continuous monitoring system to update production schedules. The updating process is conducted manually based on their knowledge and a single dispatching rule (SDR) strategy. We believe that the proposed RTS system will help the company utilize the installed Industry 4.0 concepts and achieve the Industry 4.0 vision in the production control.The performance of the proposed RTS system is compared to the current strategy applied in the company. Results show the efficiency of the proposed RTS system compared to the current strategy.","2577-0993","978-1-7281-8017-5","10.1109/RAMS48097.2021.9605791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9605791","Real-time scheduling;reinforcement learning;machine learning in manufacturing;Industry 4.0","Training;Schedules;Job shop scheduling;Electric breakdown;Production;Companies;Real-time systems","decision making;dispatching;injection moulding;Internet;Internet of Things;learning (artificial intelligence);maintenance engineering;manufacturing systems;optimisation;production control;production engineering computing;production planning;work in progress","reinforcement learning-based real-time scheduling;random machine breakdowns;production lines;order arrivals;order cancellations;production schedules;shop-floor;manufacturing technologies;industrial informatics;cyber-physical systems;smart decision-making;machine learning algorithms;ML approaches;production planning;production control;multiple dispatching rules strategy;smart manufacturing firm;random breakdowns;continuous monitoring system;Industry 4.0;work-in-progress;real-time information sharing;Internet of Things;Internet of Services;optimization process;RL based RTS;MDR;Ontario;thermoplastic injection molding;maintenance activities;single dispatching rule strategy;SDR","","4","","20","IEEE","22 Nov 2021","","","IEEE","IEEE Conferences"
"Multiobjective Reinforcement Learning for Reconfigurable Adaptive Optimal Control of Manufacturing Processes","J. Dornheim; N. Link","Intelligent Systems Research Group (ISRG), Karlsruhe University of Applied Sciences, Karlsruhe, Germany; Intelligent Systems Research Group (ISRG), Karlsruhe University of Applied Sciences, Karlsruhe, Germany","2018 International Symposium on Electronics and Telecommunications (ISETC)","20 Dec 2018","2018","","","1","5","In industrial applications of adaptive optimal control often multiple contrary objectives have to be considered. The relative importance (weights) of the objectives are often not known during the design of the control and can change with changing production conditions and requirements. In this work a novel model-free multi objective reinforcement learning approach for adaptive optimal control of manufacturing processes is proposed. The approach enables sample-efficient learning in sequences of control configurations, given by particular objective weights.","2475-7861","978-1-5386-5925-0","10.1109/ISETC.2018.8583854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8583854","Multiobjective Reinforcement Learning;Transfer Learning;Manufacturing Process Optimization;Adaptive Optimal Control","Task analysis;Artificial neural networks;Optimal control;Manufacturing processes;Optimization;Approximation algorithms;Harmonic analysis","adaptive control;control engineering computing;control system synthesis;learning (artificial intelligence);manufacturing processes;optimal control;process control;production engineering computing","reconfigurable adaptive optimal control;manufacturing processes;industrial applications;production conditions;sample-efficient learning;model-free multiobjective reinforcement learning approach","","3","","10","IEEE","20 Dec 2018","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach for Learning Coordination Rules in Production Networks","W. Dangelmaier; T. Rust; A. Doring; B. Klopper","Heinz Nixdorf Institute, University of Paderborn, Paderborn, Germany; Heinz Nixdorf Institute, University of Paderborn, Paderborn, Germany; Heinz Nixdorf Institute, University of Paderborn, Paderborn, Germany; Heinz Nixdorf Institute, University of Paderborn, Paderborn, Germany","2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06)","8 Jan 2007","2006","","","84","84","In production networks companies need fast reactions due to changes of supply and demand. To realize such a change management in an effective way the involved companies have to synchronize their quantities and capacities collaboratively. For these purposes the multiagent system MASCOPP was developed at the Heinz Nixdorf Institute, which tries to eliminate conflicts in a production network, based on changes of plans, through bilateral communication between the involved companies. Human experts have to configure the system by creating coordination rules to solve the conflicts. In this paper we introduce a machine learning concept to learn these coordination rules objectively by a reinforcement learning approach.","","0-7695-2731-0","10.1109/CIMCA.2006.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052723","","Production systems;Collaboration;Humans;Supply and demand;Multiagent systems;Machine learning;Control systems;Computational intelligence;Raw materials;Manufacturing","groupware;learning (artificial intelligence);management of change;multi-agent systems;production engineering computing;production management;supply and demand","reinforcement learning approach;learning coordination rules;production networks;supply and demand;change management;multiagent system;MASCOPP;bilateral communication","","2","","13","IEEE","8 Jan 2007","","","IEEE","IEEE Conferences"
"Comparison study of two reinforcement learning based real-time control policies for two-machine-one-buffer production system","W. Zheng; Y. Lei; Q. Chang","College of Mechanical Engineering, Zhejiang University, Hanzhou, China; State Key Laboratory of Fluid Power & Mechatronic Systems, Zhejiang University, Hanzhou, China; Department of Mechanical Engineering, Stony Brook University, New York, America","2017 13th IEEE Conference on Automation Science and Engineering (CASE)","15 Jan 2018","2017","","","1163","1168","Real-time control policy of production system is attractive to reduce the total cost that is mainly composed of the production cost, the penalty of the permanent production loss, and the Work-In-Process(WIP) inventory level cost. Because of the starved and blocked phenomena, the random failures and the maintenances, it is difficult to analyze production system, let alone to find a good control policy. Two reinforcement learning based control decision policies are proposed based on the actions of switching the machines on or off at the start of each time slot. Samples collected from a simulated model are used to obtain two sub-optimal policies named LSPI and T H. TH policy is a simplified form of LSPI, while LSPI performs better in reducing total production cost.","2161-8089","978-1-5090-6781-7","10.1109/COASE.2017.8256260","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8256260","","Silicon;Production systems;Buffer storage;Switches;Mathematical model;Learning (artificial intelligence)","cost reduction;failure analysis;iterative methods;learning (artificial intelligence);least squares approximations;manufacturing systems;production control;real-time systems;work in progress","production loss;sub-optimal policy;two-machine-one-buffer production system;work-in-process inventory level cost;production cost reduction;reinforcement learning based real-time control policy;simulation model;maintenance process;machine failure;TH policy;least-square policy iteration;decision policies","","2","","17","IEEE","15 Jan 2018","","","IEEE","IEEE Conferences"
"Learning adaptive dispatching rules for a manufacturing process system by using reinforcement learning approach","Shuhui Qu; Jie Wang; G. Shivani","Center for Sustainable Development & Global Competitiveness, Stanford University Stanford, US; Center for Sustainable Development & Global Competitiveness, Stanford University Stanford, US; Center for Sustainable Development & Global Competitiveness, Stanford University Stanford, US","2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA)","7 Nov 2016","2016","","","1","8","Advanced manufacturing, which employs the latest information and communication technologies to facilitate interconnected, efficient, and adaptive manufacturing systems, has become a prominent research topic in both academics and industry in recent years. One critical aspect of advanced manufacturing is how to match various complex job requirements with a manufacturer's real-time processing capabilities and its other resources within a job shop to optimally schedule manufacturing processes with multiple objectives. In general, a manufacturing scheduling problem is a non-deterministic, polynomial-time (NP)-hard problem. Due to its complexity, scheduling problems present a number of challenges to find the best possible solutions. In order to deal with a dynamic job shop scheduling problem-particularly, a dispatching problem-for a manufacturing system that is able to handle multiple product types through multi-stages and multi-machines with dynamic orders, stochastic processing time and setup time. Based on a generic framework, this research develops a solution by using a reinforcement learning-based scheduling approach that can adaptively update the production schedule by utilizing the real-time product and process events information during executions. More specifically, we first propose a framework that describes the process of dealing with a complex scheduling problem. Next, to learn the dispatching pattern, we formally define the scheduling problem through the construction of objective functions and related constraints for the manufacturing tasks. We then apply a reinforcement learning approach that incorporates the real-time production environment to generate optimal policies under various manufacturing-process conditions. When tested under different objectives and constraint conditions, our results demonstrate that the proposed learning-based method provides better performance than most common dispatching rules.","","978-1-5090-1314-2","10.1109/ETFA.2016.7733712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733712","reinforcement learning;ontology;dynamic job shop;dispatching rules;approximate method;multi-stage;multi-products;multi-machines;setup cost;stochastic processing time;adaptiveness","Job shop scheduling;Dispatching;Learning (artificial intelligence);Manufacturing systems;Real-time systems","computational complexity;job shop scheduling;learning (artificial intelligence);manufacturing processes;manufacturing systems;optimisation;production engineering computing;stochastic processes","manufacturing process system;reinforcement learning approach;learning adaptive dispatching rule;advanced manufacturing;complex job requirements;optimal manufacturing process scheduling problem;nondeterministic polynomial-time-hard problem;NP-hard problem;dynamic job shop scheduling problem;dispatching problem;dynamic order;stochastic processing time;setup time;reinforcement learning-based scheduling approach;production scheduling;real-time product;process event information;complex scheduling problem;objective function;real-time production environment","","2","","30","IEEE","7 Nov 2016","","","IEEE","IEEE Conferences"
"Deep reinforcement learning for semiconductor production scheduling","B. Waschneck; A. Reichstaller; L. Belzner; T. Altenmüller; T. Bauernhansl; A. Knapp; A. Kyek","GSaME, Universität Stuttgart, Stuttgart, Germany; Institute for Software & Systems Engineering, University of Augsburg, Germany; Lenz Belzner AI Consulting, Munich, Germany; Infineon Technologies AG, Neubiberg, Germany; Fraunhofer Institute for Manufacturing, Engineering and Automation IPA, Stuttgart, Germany; Institute for Software & Systems Engineering, University of Augsburg, Germany; Infineon Technologies AG, Neubiberg, Germany","2018 29th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC)","7 Jun 2018","2018","","","301","306","Despite producing tremendous success stories by identifying cat videos [1] or solving computer as well as board games [2], [3], the adoption of deep learning in the semiconductor industry is moderatre. In this paper, we apply Google DeepMind's Deep Q Network (DQN) agent algorithm for Reinforcement Learning (RL) to semiconductor production scheduling. In an RL environment several cooperative DQN agents, which utilize deep neural networks, are trained with flexible user-defined objectives. We show benchmarks comparing standard dispatching heuristics with the DQN agents in an abstract frontend-of-line semiconductor production facility. Results are promising and show that DQN agents optimize production autonomously for different targets.","2376-6697","978-1-5386-3748-7","10.1109/ASMC.2018.8373191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8373191","Production Scheduling;Reinforcement Learning;Machine Learning;Semiconductor Manufacturing","Neural networks;Job shop scheduling;Dispatching;Production facilities;Machine learning;Optimization","learning (artificial intelligence);neural nets;production engineering computing;scheduling;semiconductor industry","semiconductor production scheduling;board games;semiconductor industry;RL environment;DQN agents;deep neural networks;deep reinforcement learning;cat videos identification;standard dispatching heuristics;user-defined objectives;frontend-of-line semiconductor production facility;Google DeepMind's Deep Q Network agent algorithm","","52","","25","IEEE","7 Jun 2018","","","IEEE","IEEE Conferences"
"Recurrent Reinforcement Learning for Predictive Overall Equipment Effectiveness","D. -Y. Liao; W. -P. Tsai; H. -T. Chen; Y. -P. Ting; C. -Y. Chen; H. -C. Chen; S. -C. Chang","National Taiwan University, Taipei, Taiwan; Taiwan Semiconductor Manufacturing Co, Hsinchu, Taiwan; Taiwan Semiconductor Manufacturing Co, Hsinchu, Taiwan; Taiwan Semiconductor Manufacturing Co, Hsinchu, Taiwan; National Taiwan University, Taipei, Taiwan; National Taiwan University, Taipei, Taiwan; National Taiwan University, Taipei, Taiwan","2018 e-Manufacturing & Design Collaboration Symposium (eMDC)","1 Nov 2018","2018","","","1","4","With the increasing huge amount of real-time data being collected in modern manufacturing systems, the conventional indices defined to evaluate productivity, quality and performance become less effective. Compared to the conventional Overall Equipment Effectiveness (OEE), the predictive OEE (POEE) evaluates and monitors the forthcoming effectiveness of a single tool. Its predictive effectiveness is based on the extra production time due to anomaly tool conditions and undesired product quality. This research develops a recurrent reinforcement learning model to predict the predictable elements in calculating the POEE. Our model combines supervised Long-Short Term Memory (LSTM) and reinforced Deep Q-Network (DQN) techniques in predicting stochastic dynamics in production and quality. A Chemical Vapor Deposition (CVD) tool is taken as an exemplary case to illustrate the calculation of POEE.","","978-9-8691-7154-0","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8514233","Recurrent Neural Network;Reinforcement Learning;OEE;Predictive;CVD","Tools;Production;Predictive models;Measurement;Training","chemical vapour deposition;learning (artificial intelligence);manufacturing systems;product quality;production engineering computing;productivity;stochastic processes","POEE;long-short term memory;chemical vapor deposition tool;product quality;conventional overall equipment effectiveness;predictive overall equipment effectiveness;reinforced Deep Q-Network techniques;stochastic dynamics;recurrent reinforcement learning model;anomaly tool conditions;predictive OEE;productivity;modern manufacturing systems;real-time data","","","","10","","1 Nov 2018","","","IEEE","IEEE Conferences"
"Multi-Agent Reinforcement Learning for Job Shop Scheduling in Flexible Manufacturing Systems","S. Baer; J. Bakakeu; R. Meyes; T. Meisen","Siemens AG, RWTH Aachen University, Nuremberg, Germany; Siemens AG, Friedrich-Alexander-Universität, Nuremberg, Germany; University Wuppertal, Wuppertal, Germany; University Wuppertal, Wuppertal, Germany","2019 Second International Conference on Artificial Intelligence for Industries (AI4I)","9 Mar 2020","2019","","","22","25","In this paper, we first outline the motivation and the need for a new approach for online scheduling in flexible manufacturing systems (FMS) based on reinforcement learning (RL). We then present an initial concept for such an approach. In our method, we use Deep RL agents, who have learned to efficiently guide products through the plant and achieve near-optimal timing regarding resource allocation. Each product is controlled by its own agent, which can handle unforeseen machine failures, re-configurations of the plant topology and the consideration of local and global optimization goals. We created a virtual representation of the FMS using a Petri net, modelling the plant topology and the product flow. The agents' task is to navigate the product to the corresponding machine by choosing the according transition of the Petri net. The training consists of four stages from learning rough rules in order to fulfill a job in a Single-Agent RL setup to learning thoughtful collaboration between agents in a Multi-Agent RL (MARL) setup. We proved the feasibility of the first and second training stage by applying it to the virtual depiction of a specific research plant and obtained promising results. With this concept of a self-learning control policy, online-scheduling can be applied with less effort required to customize the setup for various FMSs.","","978-1-7281-4087-2","10.1109/AI4I46381.2019.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9027776","Job Shop Scheduling, Flexible Manufacturing System, Reinforcement Learning, Multi-Agent System","Optimization;Training;Task analysis;Topology;Frequency modulation;Job shop scheduling;Petri nets","failure analysis;flexible manufacturing systems;industrial plants;job shop scheduling;learning (artificial intelligence);multi-agent systems;optimisation;Petri nets;production engineering computing;resource allocation","plant topology;product flow;Petri net;self-learning control policy;multiagent reinforcement learning;job shop scheduling;flexible manufacturing systems;FMS;resource allocation;machine failures;global optimization;virtual representation;multiagent RL setup;single-agent RL setup","","25","","12","IEEE","9 Mar 2020","","","IEEE","IEEE Conferences"
"A Summary Of Using Reinforcement Learning Strategies For Treating Project And Production Management Problems","G. Koulinas; A. Xanthopoulos; A. Kiatipis; D. Koulouriotis","Department of Production and Management Engineering, Democritus University of Thrace, Xanthi, Greece; Department of Production and Management Engineering, Democritus University of Thrace, Xanthi, Greece; Fujitsu Technology Solutions GmbH, Munich, Germany; Department of Production and Management Engineering, Democritus University of Thrace, Xanthi, Greece","2018 Thirteenth International Conference on Digital Information Management (ICDIM)","26 Sep 2019","2018","","","33","38","Recently, Reinforcement Learning (RL) strategies have attracted researchers' interest as a powerful approach for effective treating important problems in the field of production and project management. Generally, RL are autonomous machine learning algorithms that include a learning process that interacts with the problem, which is under study in order to search for good quality solutions in reasonable time. At each decision point of the algorithm, the current state of the problem is revised and decisions about the future of the searching strategy are taken. The objective of this work is to summarize, in brief, recently proposed studies using reinforcement learning strategies for solving project scheduling problems and production scheduling problems, as well. Based on the review, we suggest directions for future research about approaches that can be proved interesting in practice.","","978-1-5386-5244-2","10.1109/ICDIM.2018.8847099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8847099","Reinforcement Learning;Project Scheduling;Production Scheduling;Resource Constrained Project Scheduling Problem;Job Shop Scheduling Problem","Job shop scheduling;Conferences;Reinforcement learning;Search problems;Machine learning algorithms;Research and development management","learning (artificial intelligence);production engineering computing;production management;project management;scheduling","project management;RL;learning process;searching strategy;reinforcement learning strategies;project scheduling problems;production scheduling problems;production management problems","","3","","33","IEEE","26 Sep 2019","","","IEEE","IEEE Conferences"
"Prediction of battery manufacturing capacity based on reinforcement learning network combination model","N. Li; Y. Wang; Z. Wang; Y. Wang","School of IOT Engineering, Jiangnan University, Wuxi, P. R. China; School of IOT Engineering, Jiangnan University, Wuxi, P. R. China; School of IOT Engineering, Jiangnan University, Wuxi, P. R. China; School of IOT Engineering, Jiangnan University, Wuxi, P. R. China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","3932","3936","Aiming at the problem of the battery manufacturing capacity prediction, this paper presents a prediction method based on reinforcement learning network combination model. First, the combined model expression for the battery manufacturing capacity prediction is designed. Then, reinforcement learning is used to construct the hidden layer learning environment of recurrent neural network and long-short-termmemory network model, to obtain the optimal number of hidden layers, and then to construct the weight learning environment of the battery manufacturing capacity combination prediction model and a combined forecasting model of battery manufacturing capacity after iterative training. Finally, a case simulation on actual battery workshop data shows the effectiveness and practicability of the proposed algorithm on solving the battery manufacturing capacity prediction problem.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10054924","Research and Development; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054924","battery manufacturing capacity;prediction;reinforcement learning;combination model","Training;Recurrent neural networks;Reinforcement learning;Predictive models;Prediction algorithms;Data models;Batteries","capacity planning (manufacturing);iterative methods;power engineering computing;production engineering computing;recurrent neural nets;reinforcement learning","battery manufacturing capacity combination prediction model;battery workshop data;hidden layer learning environment;long-short-term memory network model;recurrent neural network;reinforcement learning network combination model;weight learning environment","","","","17","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning with Shaping Exploration Space for Robotic Assembly","C. Wang; C. Lin; B. Liu; C. Su; P. Xu; L. Xie","Shien-Ming Wu School of Intelligent Engineering South China University of Technology, Guangzhou, China; Shien-Ming Wu School of Intelligent Engineering South China University of Technology, Guangzhou, China; Shien-Ming Wu School of Intelligent Engineering South China University of Technology, Guangzhou, China; Shien-Ming Wu School of Intelligent Engineering South China University of Technology, Guangzhou, China; Shien-Ming Wu School of Intelligent Engineering South China University of Technology, Guangzhou, China; Shien-Ming Wu School of Intelligent Engineering South China University of Technology, Guangzhou, China","2021 3rd International Symposium on Robotics & Intelligent Manufacturing Technology (ISRIMT)","15 Nov 2021","2021","","","345","351","Assembly is an essential part of complex product manufacturing. Due to the complexity of assembly operations and rapidly changing market demands, it is a labor-intensive industry with insufficient automation improvement. Force control methods are developed to allow industrial robots to do Contact-Rich manipulation with force sensors fixed at the end-effect. Deep Reinforcement Learning (DRL) provides a method to learn assembly skills by trial and error. Nevertheless, the performance of the framework combining DRL and force control is sensitive to the hyper-parameters, which limits the application in manufacturing. In this work, we analyze the force control based on primary impedance control and propose a method that designs hyper-parameters according to task information, thus speeding up learning and making the process safer by adding a constraint to the exploration space.","","978-1-6654-3718-9","10.1109/ISRIMT53730.2021.9596687","National Natural Science Foundation of China(grant numbers:52075177); Shenzhen Institute of Artificial Intelligence and Robotics for Society(grant numbers:AC01202005011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9596687","Deep Reinforcement Learning;Force control;Shaping exploration;Robotic Assembly;Hyper-parameters design","Shape;Service robots;Design methodology;Reinforcement learning;Aerospace electronics;Space exploration;Manufacturing","deep learning (artificial intelligence);force control;industrial manipulators;production engineering computing;reinforcement learning;robot programming;robotic assembly","shaping exploration space;robotic assembly;complex product manufacturing;market demands;labor-intensive industry;force control;industrial robots;contact-rich manipulation;force sensors;end-effect;DRL;assembly skills;impedance control;deep reinforcement learning","","1","","24","IEEE","15 Nov 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Production Ramp-Up: A Q-Batch Learning Approach","S. Doltsinis; P. Ferreira; N. Lohse","Manufacturing Division, The University of Nottingham, Nottingham, United Kingdom; Manufacturing Division, The University of Nottingham, Nottingham, United Kingdom; Manufacturing Division, The University of Nottingham, Nottingham, United Kingdom","2012 11th International Conference on Machine Learning and Applications","10 Jan 2013","2012","1","","610","615","The ramp-up process is a significant bottleneck during the development of manufacturing systems. The effort and time required to ramp-up a system is largely dependent on the effectiveness of the human decision making process to select the most promising action and improve the system. Although existing work has identified significant factors influencing ramp-up performance, little has been done to support the actual process. This work approaches ramp-up as sequence of technical changes which aim to get a manufacturing system to a desirable performance in the fastest time. A reinforcement learning approach is proposed to support decisions during ramp-up. The aim is to capture the dynamics between an operator and the system and support time reduction of the process. A batch learning approach has been identified as promising since it matches the practical aspect of decision making during ramp-up. It is combined with a Q-learning algorithm which provides theoretical foundation of optimum convergence. The learning approach has been demonstrated on a highly automated production station during its ramp-up and the generated policy was shown to have significant impact on the ramp-up time reduction.","","978-1-4673-4651-1","10.1109/ICMLA.2012.113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6406634","Reinforcement Learning;Manufacturing;Ramp-Up;Decision Making Systems","Learning;Decision making;Humans;Machine learning;Manufacturing systems","decision making;learning (artificial intelligence);manufacturing systems;production engineering computing","reinforcement learning;production ramp-up process;Q-batch learning approach;manufacturing system development;human decision making process;ramp-up performance;automated production station;ramp-up time reduction","","8","","17","IEEE","10 Jan 2013","","","IEEE","IEEE Conferences"
"Research on Intelligent Peg-in-Hole Assembly Strategy Based on Deep Reinforcement Learning","G. Wang; J. Yi; X. Ji","Department of Mechanical Engineering, East China University of Science and Technology, Shanghai, China; Department of Mechanical Engineering, East China University of Science and Technology, Shanghai, China; Department of Mechanical Engineering, East China University of Science and Technology, Shanghai, China","2023 3rd Asia-Pacific Conference on Communications Technology and Computer Science (ACCTCS)","9 Jun 2023","2023","","","468","473","Due to the error of visual positioning and camera calibration in the peg_in_hole assembly method based on visual positioning, there is a certain position and angle deviation between shafts and holes, which leads to the failure of the assembly. In this paper, a flexible assembly algorithm with variable admittance parameters, which incorporates reinforcement learning, is proposed to improve the flexibility of the assembly process in unknown environments. First, Archimedes spiral hole searching method with admittance control is used to search the hole on the workpiece surface. And then, the DDPG algorithm is used to train the admittance control parameters in the process of assembly to improve the safety of the robot inserting hole. We generalize the robot inserting operation and enable the robot to safely handle more assembly problems in unknown environments. Finally, the reliability and safety of the methods are verified in the Pybullet.","","979-8-3503-1080-1","10.1109/ACCTCS58815.2023.00060","National Natural Science Foundation of China; National Defense Basic Scientific Research Program of China; Innovation Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10145251","admittance control;deep reinforcement learning;peg_in_hole assembly","Shafts;Training;Visualization;Spirals;Force;Process control;Reinforcement learning","assembling;calibration;cameras;control engineering computing;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;position control;reinforcement learning;robot programming;robotic assembly;trajectory control","admittance control parameters;angle deviation;Archimedes spiral hole;assembly problems;assembly process;camera calibration;DDPG algorithm;deep reinforcement learning;flexible assembly algorithm;intelligent peg-in-hole assembly strategy;peg_in_hole assembly method;robot inserting hole;shafts;variable admittance parameters;visual positioning","","","","6","IEEE","9 Jun 2023","","","IEEE","IEEE Conferences"
"Fuzzy logic based reinforcement learning of admittance control for automated robotic manufacturing","S. M. Prabhu; D. P. Garg","CGN and Associates, Inc., Cary, NC, USA; Department of Mechanical Engg. & Materials Sc., Duke University, Durham, NC, USA","Proceedings of 1st International Conference on Conventional and Knowledge Based Intelligent Electronic Systems. KES '97","6 Aug 2002","1997","2","","478","487 vol.2","An approach to admittance control using fuzzy logic based reinforcement learning is proposed for the robotic automation of typical manufacturing operations. Use of fuzzy logic enables the knowledge of the manufacturing process operator to be incorporated into the controller design, which is then further refined using reinforcement learning techniques. Automated robotic deburring offers an attractive alternative to manual deburring in terms of reduced costs and improved quality of the finished parts, and hence it is used as an example of a typical manufacturing task. Simulation results are presented which demonstrate the effectiveness of the proposed controller in controlling the automated robotic deburring task.","","0-7803-3755-7","10.1109/KES.1997.619426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=619426","","Fuzzy logic;Learning;Admittance;Automatic control;Robotics and automation;Manufacturing automation;Deburring;Manufacturing processes;Refining;Costs","fuzzy logic;learning systems;industrial manipulators;industrial control;fuzzy neural nets;fuzzy control;simulation;manipulator dynamics","fuzzy logic based reinforcement learning;admittance control;automated robotic manufacturing;robotic automation;manufacturing operations;manufacturing process operator knowledge;controller design;automated robotic deburring;finished part quality;simulation","","4","","13","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Generating Reinforcement Learning Environments for Industrial Communication Protocols","A. Csiszar; V. Krimstein; J. Bogner; A. Verl","Institute for Control Engineering of Machine Tools and Manufacturing Units (ISW); Institute for Control Engineering of Machine Tools and Manufacturing Units (ISW); Institute of Software Engineering (ISTE), University of Stuttgart, Stuttgart, Germany; Institute for Control Engineering of Machine Tools and Manufacturing Units (ISW)","2021 4th International Conference on Artificial Intelligence for Industries (AI4I)","13 Oct 2021","2021","","","57","60","An important part of any reinforcement learning application is interfacing the agent to its environment. To enable an easier use of reinforcement learning agents in manufacturing and automation-related real-world environments, we propose an environment generator which acts as an adapter between the interface of the agent and existing industrial communication protocols. This paper describes the functionality and architecture of such an environment generator.","","978-1-6654-3410-2","10.1109/AI4I51902.2021.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9565507","reinforcement learning;manufacturing;automation;communication protocols","Training;Productivity;Protocols;Industrial communication;Reinforcement learning;Generators;Software","control engineering computing;industrial control;learning (artificial intelligence);protocols","existing industrial communication protocols;environment generator;manufacturing automation-related real-world environments;reinforcement learning application;generating reinforcement learning environments","","1","","8","IEEE","13 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based and Parametric Production-Maintenance Control Policies for a Deteriorating Manufacturing System","A. S. Xanthopoulos; A. Kiatipis; D. E. Koulouriotis; S. Stieger","Department of Production and Management Engineering, Democritus University of Thrace, Xanthi, Greece; Fujitsu Technology Solutions GmbH, Munich, Germany; Department of Production and Management Engineering, Democritus University of Thrace, Xanthi, Greece; Fujitsu Technology Solutions GmbH, Munich, Germany","IEEE Access","14 Feb 2018","2018","6","","576","588","The model of a stochastic production/inventory system that is subject to deterioration failures is developed and examined in this paper. Customer interarrival times are assumed to be random and backorders are allowed. The system experiences a number of deterioration stages before it ultimately fails and is rendered inoperable. Repair and maintenance activities restore the system to its initial and previous deterioration state, respectively. The duration of both repair and maintenance is assumed to be stochastic. We address the problem of minimizing the expected sum of two conflicting objective functions: the average inventory level and the average number of backorders. The solution to this problem consists of finding the optimal tradeoff between maintaining a high service level and carrying as low inventory as possible. The primary goal of this research is to obtain optimal or near-optimal joint production/maintenance control policies, by means of a novel reinforcement learning-based approach. Furthermore, we examine parametric production and maintenance policies that are often used in practical situations, namely, Kanban, (s, S), threshold-type condition based maintenance and periodic maintenance. The proposed approach is compared with the parametric policies in an extensive series of simulation experiments and it is found to clearly outperform them in all cases. Based on the numerical results obtained by the experiments, the behavior of the parametric policies as well as the structure of the control policies derived by the Reinforcement Learning-based approach is investigated.","2169-3536","","10.1109/ACCESS.2017.2771827","BigStorage: Storage-Based Convergence Between HPC and Cloud to Handle Big Data project from the European Union through the Marie Skłodowska-Curie Actions framework(grant numbers:H2020-MSCA-ITN-2014-642963); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8114172","Inventory control;preventive maintenance;reinforcement learning;intelligent manufacturing systems","Maintenance engineering;Production facilities;Stochastic processes;Learning (artificial intelligence);Electronic mail;Optimization","kanban;learning (artificial intelligence);manufacturing systems;Markov processes;optimisation;preventive maintenance;stock control","backorders;periodic maintenance;novel reinforcement learning-based approach;manufacturing systems;stochastic production;repair activities;inventory system;Kanban policy;threshold-type condition based maintenance;optimisation;Markov processes;preventive maintenance;customer interarrival times;deterioration failures;parametric production-maintenance control policies","","53","","29","OAPA","17 Nov 2017","","","IEEE","IEEE Journals"
"Fault diagnosis method of automation equipment in independent and controllable substation based on deep reinforcement learning","H. Du; Z. Li; D. Liu; Y. Huang","Jinhua Power Supply Company, State Grid Zhejiang Electric Power Co., Ltd., Jinhua, China; Jinhua Power Supply Company, State Grid Zhejiang Electric Power Co., Ltd., Jinhua, China; Jinhua Power Supply Company, State Grid Zhejiang Electric Power Co., Ltd., Jinhua, China; State Grid Zhejiang Electric Power Research Institute, Hangzhou, China","2022 7th Asia Conference on Power and Electrical Engineering (ACPEE)","1 Jun 2022","2022","","","1992","1996","Fast and accurate fault diagnosis is the key to ensure the safe and stable operation of the substation after the fault occurs to the automation equipment in the independent and controllable intelligent substation. Firstly, different types of faults that may occur in intelligent substation automation equipment are analyzed, and the electrical characteristic information of corresponding fault types is characterized respectively. Then, the basic structure of enhanced deep convolutional neural network (EDCNN) and the applicability and superiority of EDCNN in automatic equipment fault diagnosis are analyzed, and the model and algorithm of automatic equipment fault diagnosis based on EDCNN are built on the basis of data preprocessing. Finally, the proposed algorithm and other three algorithms are compared and analyzed under the same conditions based on actual cases. The results show that for different types of faults, the proposed algorithm has higher fault diagnosis accuracy and faster convergence speed.","","978-1-6654-1819-5","10.1109/ACPEE53904.2022.9783925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9783925","Independent and controllable substation;Automation equipment;Fault diagnosis;Deep reinforcement learning;EDCNN","Fault diagnosis;Electrical engineering;Automation;Substation automation;Data preprocessing;Reinforcement learning;Electric variables","convolutional neural nets;data handling;deep learning (artificial intelligence);fault diagnosis;power engineering computing;power system faults;reinforcement learning;substation automation","deep reinforcement learning;controllable intelligent substation;intelligent substation automation equipment;enhanced deep convolutional neural network;EDCNN;automatic equipment fault diagnosis;electrical characteristic information;data preprocessing;convergence speed","","2","","14","IEEE","1 Jun 2022","","","IEEE","IEEE Conferences"
"Simulation-Based Deep Reinforcement Learning For Modular Production Systems","N. Feldkamp; S. Bergmann; S. Strassburger","Information Technology in Production and Logistics, Technische Universität Ilmenau, Ilmenau, GERMANY; Information Technology in Production and Logistics, Technische Universität Ilmenau, Ilmenau, GERMANY; Information Technology in Production and Logistics, Technische Universität Ilmenau, Ilmenau, GERMANY","2020 Winter Simulation Conference (WSC)","29 Mar 2021","2020","","","1596","1607","Modular production systems aim to supersede the traditional line production in the automobile industry. The idea here is that highly customized products can move dynamically and autonomously through a system of flexible workstations without fixed production cycles. This approach has challenging demands regarding planning and organization of such systems. Since each product can define its way through the system freely and individually, implementing rules and heuristics that leverage the flexibility in the system in order to increase performance can be difficult in this dynamic environment. Transport tasks are usually carried out by automated guided vehicles (AGVs). Therefore, integration of AI-based control logics offer a promising alternative to manually implemented decision rules for operating the AGVs. This paper presents an approach for using reinforcement learning (RL) in combination with simulation in order to control AGVs in modular production systems. We present a case study and compare our approach to heuristic rules.","1558-4305","978-1-7281-9499-8","10.1109/WSC48552.2020.9384089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384089","","Production systems;Reinforcement learning;Organizations;Workstations;Planning;Vehicle dynamics;Task analysis","automatic guided vehicles;automobile industry;control engineering computing;flexible manufacturing systems;learning (artificial intelligence);production engineering computing","modular production systems;traditional line production;highly customized products;fixed production cycles;simulation-based deep reinforcement learning;automated guided vehicles;AGV;AI-based control logics;RL;heuristic rules","","6","","33","IEEE","29 Mar 2021","","","IEEE","IEEE Conferences"
"Predictive Maintenance Decision Making Based on Reinforcement Learning in Multistage Production Systems","M. Feng; Y. Li","School of Foreign Language Studies, Chang’an University, Shaanxi, Xi’an, China; Department of Industrial Engineering, School of Mechanical Engineering, Northwestern Polytechnical University, Shaanxi, Xi’an, China","IEEE Access","21 Feb 2022","2022","10","","18910","18921","Predictive maintenance has become increasingly prevalent in modern production systems that are challenged by high-mix low-volume production and short production life cycle. It is very helpful to prevent costly equipment failures, and reduce significant production loss caused by unscheduled machine breakdown. Although important, decision models for joint predictive maintenance and production in manufacturing systems have not been fully explored. Therefore, we propose a reinforcement learning based decision model, that brings together production system modeling and approximate dynamic programming. We start from the development of a state-based model by analyzing the dynamics of a multistage production system with predictive maintenance. It provides an approach to quantitatively evaluate the various disruptions as well as the maintenance decision’s impact on production. Then a reinforcement learning method is proposed to explore optimal maintenance policies, that optimize the production and maintenance cost. To further improve the performance of the production system, machine stoppage bottlenecks are defined. An event-based indicator is proved to identify bottlenecks with production data. We test the proposed models in simulation case studies. The proposed predictive maintenance decision model is compared with three policies, which are state-based policy (SBP), time-based policy (TBP) and greedy policy (GP). The numerical studies show that the proposed decision model outperforms the policies, and it has the lowest system cost that is 9.68%, 39.07%, and 39.56% lower than SBP, TBP, and GP, respectively. In addition, the research shows that bottleneck identification and mitigation could help manufacturing systems to achieve more than 9.00% throughput improvement.","2169-3536","","10.1109/ACCESS.2022.3151170","National Natural Science Foundation of China(grant numbers:52175485); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712336","Production system analysis;Markov chain model;predictive maintenance;decision making;approximate dynamic programming;bottleneck","Maintenance engineering;Production systems;Production;Predictive maintenance;Predictive models;Markov processes;Costs","condition monitoring;costing;decision making;dynamic programming;government policies;maintenance engineering;manufacturing systems;product life cycle management;production engineering computing;reinforcement learning","approximate dynamic programming;multistage production system;reinforcement learning method;maintenance policies;maintenance cost;predictive maintenance decision model;greedy policy;manufacturing systems;high-mix low-volume production;short production life cycle;equipment failures prevention;production loss reduction;joint predictive maintenance;unscheduled machine breakdown;state-based policy;time-based policy;machine stoppage bottleneck identification","","10","","31","CCBY","11 Feb 2022","","","IEEE","IEEE Journals"
"A Reinforcement Learning Algorithm for Optimal Dynamic Policies of Joint Condition-based Maintenance and Condition-based Production","H. Rasay; F. Azizi; M. Salmani; F. Naderkhani","Kermanshah University of Technology, Kermanshah, Iran; Department of Statistic, Faculty of Mathematical Science, Alzahra University, Tehran, Iran; Concordia Institute for Information System Engineering, Concordia University, Montreal, QC, Canada; Concordia Institute for Information System Engineering, Concordia University, Montreal, QC, Canada","2023 IEEE International Conference on Prognostics and Health Management (ICPHM)","2 Aug 2023","2023","","","200","204","This paper focuses on development of joint optimal maintenance and production policy for a specific type of production system that allows for adjustable production rates. The rate of deterioration of the system is directly related to the production rate, with higher production rates resulting in greater expected deterioration. The system's deterioration can be controlled through two main actions: (1) scheduling and conducting maintenance actions referred to as maintenance policy; and (2) adjusting the production rate referred to as production policy. To determine the optimal actions given the system's state, a Markov decision process (MDP) is developed and a reinforcement learning algorithm, specifically a Q-learning algorithm, is utilized. The algorithm's hyper parameters are tuned using a value-iteration algorithm of dynamic programming. The goal is to minimize expected costs for the system over a finite planning horizon.","2166-5656","979-8-3503-4625-1","10.1109/ICPHM57936.2023.10194057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10194057","condition-based maintenance;condition-based production;reinforcement learning;Markov decision process","Production systems;Schedules;Q-learning;Costs;Heuristic algorithms;Maintenance engineering;Markov processes","condition monitoring;dynamic programming;maintenance engineering;Markov processes;production engineering computing;reinforcement learning","condition-based production;joint optimal maintenance;maintenance policy;Markov decision process;MDP;optimal dynamic policies;production policy;Q-learning algorithm;reinforcement learning algorithm","","","","11","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach to Robust Scheduling of Semiconductor Manufacturing Facilities","I. -B. Park; J. Huh; J. Kim; J. Park","Department of Industrial Engineering, Seoul National University, Seoul, South Korea; Department of Business Administration, Korea Polytechnic University, Siheung, South Korea; Data Science Team, Kakaopay Corporation, Seongnam, South Korea; Department of Industrial Engineering, Seoul National University, Seoul, South Korea","IEEE Transactions on Automation Science and Engineering","1 Jul 2020","2020","17","3","1420","1431","As semiconductor manufacturers, recently, have focused on producing multichip products (MCPs), scheduling semiconductor manufacturing operations become complicated due to the constraints related to reentrant production flows, sequence-dependent setups, and alternative machines. At the same time, the scheduling problems need to be solved frequently to effectively manage the variabilities in production requirements, available machines, and initial setup status. To minimize the makespan for an MCP scheduling problem, we propose a setup change scheduling method using reinforcement learning (RL) in which each agent determines setup decisions in a decentralized manner and learns a centralized policy by sharing a neural network among the agents to deal with the changes in the number of machines. Furthermore, novel definitions of state, action, and reward are proposed to address the variabilities in production requirements and initial setup status. Numerical experiments demonstrate that the proposed approach outperforms the rule-based, metaheuristic, and other RL methods in terms of the makespan while incurring shorter computation time than the metaheuristics considered.","1558-3783","","10.1109/TASE.2019.2956762","National Research Foundation of Korea (NRF); Korea Government (MSIP)(grant numbers:NRF-2015R1D1A1A01057496); Professional Consulting Group; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946870","Flexible job-shop scheduling;multichip product (MCP);neural networks (NNs);reinforcement learning (RL);semiconductor manufacturing","Job shop scheduling;Processor scheduling;Training;Artificial neural networks;Schedules","learning (artificial intelligence);neural nets;production control;production facilities;scheduling;semiconductor device manufacture","reinforcement learning;robust scheduling;semiconductor manufacturing facilities;semiconductor manufacturers;multichip products;MCPs;sequence-dependent setups;scheduling problems;production requirements;MCP scheduling problem;setup change scheduling method;setup decisions;semiconductor manufacturing operations scheduling;reentrant production flows;neural network","","37","","48","IEEE","31 Dec 2019","","","IEEE","IEEE Journals"
"Managing Two-Level Manufacturing in Transportation with Reinforcement Learning","V. Tsyganov","Laboratory of Active Systems, V.A. Trapeznikov Institute of Control Sciences of RAS, Moscow, Russia","2022 15th International Conference Management of large-scale system development (MLSD)","9 Nov 2022","2022","","","1","5","We consider a model of hierarchical transport corporation, in which the Executive and his subordinate factory manager (the Manager) organize two-level manufacturing. They are controlled by corporation's governance (in short, the Chief) with the Consultant. The Executive knows the capabilities of increasing manufacturing output only at own level. The Manager knows the manufacturing potential of the factory. Neither the Chief nor his Consultant knows the Executive and Manager capabilities. So the Executive can manipulate own output in order to influence the decisions of the Consultant and the Chief to growth own reward. But the Executive itself does not know the potential of the factory. This also can be used by its Manager in own favor. So, the Executive needs to learn to control the Manager. The interests of the Executive and the Manager are reflected in the model by introducing their reward functions. On the basis of this model, sufficient conditions have been found for the synthesis of a hierarchy of controllers for managing output, ensuring the use of random possibilities of its increasing. In these conditions, the Chief needs consultations with the help of a self-learning Consultant. Through these consultations, the Chief can norm the Executive's output. The Learning controller encourages the Executive, firstly, to maximize auxiliary output and, secondly, to implement an adaptive controller for the Manager. Such an adaptive controller includes planning and incentive procedures that encourage the Manager to maximize the manufacturing in the factory. The application of this approach is illustrated by the example of managing output in a car repair corporation.","","978-1-6654-9701-5","10.1109/MLSD55143.2022.9934455","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9934455","corporation;manufacturing;output;stochastic;controller;reinforcement learning","Training;Sufficient conditions;Production planning;Neural networks;Transportation;Reinforcement learning;Maintenance engineering","adaptive control;automobile industry;continuous improvement;incentive schemes;learning (artificial intelligence);learning systems;transportation","two-level manufacturing;subordinate factory manager;Chief;increasing manufacturing output;manufacturing potential;Manager capabilities;managing output;consultations;self-learning Consultant;Executive's output;adaptive controller","","","","12","IEEE","9 Nov 2022","","","IEEE","IEEE Conferences"
"Real-Time Scheduling for Dynamic Partial-No-Wait Multiobjective Flexible Job Shop by Deep Reinforcement Learning","S. Luo; L. Zhang; Y. Fan","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Automation Science and Engineering","13 Oct 2022","2022","19","4","3020","3038","In modern discrete flexible manufacturing systems, dynamic disturbances frequently occur in real time and each job may contain several special operations in partial-no-wait constraint due to technological requirements. In this regard, a hierarchical multiagent deep reinforcement learning (DRL)-based real-time scheduling method named hierarchical multi-agent proximal policy optimization (HMAPPO) is developed to address the dynamic partial-no-wait multiobjective flexible job shop scheduling problem (DMOFJSP-PNW) with new job insertions and machine breakdowns. The proposed HMAPPO contains three proximal policy optimization (PPO)-based agents operating in different spatiotemporal scales, namely, objective agent, job agent, and machine agent. The objective agent acts as a higher controller periodically determining the temporary objectives to be optimized. The job agent and machine agent are lower actuators, respectively, choosing a job selection rule and machine assignment rule to achieve the temporary objective at each rescheduling point. Five job selection rules and six machine assignment rules are designed to select an uncompleted job and assign the next operation of which together with its successors in no-wait constraint on the corresponding processing machines. A hierarchical PPO-based training algorithm is developed. Extensive numerical experiments have confirmed the effectiveness and superiority of the proposed HMAPPO compared with other well-known dynamic scheduling methods. Note to Practitioners—The motivation of this article stems from the need to develop real-time scheduling methods for modern discrete flexible manufacturing factories, such as aerospace product manufacturing and steel manufacturing, where dynamic events frequently occur, and each job may contain several operations subjected to the no-wait constraint. Traditional dynamic scheduling methods, such as metaheuristics or dispatching rules, either suffer from poor time efficiency or fail to ensure good solution quality for multiple objectives in the long-term run. Meanwhile, few of the previous studies have considered the partial-no-wait constraint among several operations from the same job, which widely exists in many industries. In this article, we propose a hierarchical multiagent deep reinforcement learning (DRL)-based real-time scheduling method named HMAPPO to address the dynamic partial-no-wait multiobjective flexible job shop scheduling problem (DMOFJSP-PNW) with new job insertions and machine breakdowns. The proposed HMAPPO uses three DRL-based agents to adaptively select the temporary objectives and choose the most feasible dispatching rules to achieve them at different rescheduling points, through which the rescheduling can be made in real time and a good compromise among different objectives can be obtained in the long-term schedule. Extensive experimental results have demonstrated the effectiveness and superiority of the proposed HMAPPO. For industrial applications, this method can be extended to many other production scheduling problems, such as hybrid flow shops and open shop with different uncertainties and objectives.","1558-3783","","10.1109/TASE.2021.3104716","National Key Research and Development Program of China(grant numbers:2018YFB1703103); Research and Development Program of Tsinghua University–Weichai Power Company Ltd., Intelligent Manufacturing Institute(grant numbers:JIM02/20182912121); Dongguan Innovative Research Team Program(grant numbers:2018607202007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521514","Deep reinforcement learning (DRL);flexible job shop;multiagent;multiobjective;partial-no-wait","Dynamic scheduling;Job shop scheduling;Real-time systems;Multi-agent systems;Flexible manufacturing systems;Reinforcement learning","deep learning (artificial intelligence);dynamic scheduling;flexible manufacturing systems;flow shop scheduling;genetic algorithms;job shop scheduling;multi-agent systems;production engineering computing;reinforcement learning;search problems","dynamic partial-no-wait multiobjective flexible job shop;flexible manufacturing factories;production scheduling problems;DRL-based agents;dynamic scheduling methods;steel manufacturing;aerospace product manufacturing;real-time scheduling methods;hierarchical PPO-based training algorithm;machine assignment rules;job selection rules;machine assignment rule;job selection rule;job agent;objective agent;proximal policy optimization-based agents;machine breakdowns;job insertions;multiobjective flexible job shop scheduling problem;HMAPPO;hierarchical multiagent proximal policy optimization;hierarchical multiagent deep reinforcement learning-based;discrete flexible manufacturing systems","","22","","55","IEEE","24 Aug 2021","","","IEEE","IEEE Journals"
"GaN Distributed RF Power Amplifer Automation Design with Deep Reinforcement Learning","Y. Sun; M. Benosman; R. Ma","Mtsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA; Mtsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA; Mtsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA","2022 IEEE 4th International Conference on Artificial Intelligence Circuits and Systems (AICAS)","5 Sep 2022","2022","","","54","57","Radio frequency (RF) circuit design demands rich experience of practical know-how and extensive simulation. Complicated interactions among different building components must be considered. This becomes more challenging at higher frequency and for sophisticated circuits. In this study, we proposed a novel design automation methodology based on deep reinforcement learning (RL). For the first time, we applied RL to design a wideband non-uniform distributed RF power amplifier known for its high dimensional design challenges. Our results show that the design principles can be learned effectively and the agent can generate the optimal circuit parameters to meet the design specifications including operating frequency range (2-18GHz), output power (>37dBm), gain flatness (<4dB) and average return loss (>5.8 dB) with GaN technology. Notably, our well-trained RL agent outperforms human expert given the same design task, with 78% accuracy and offers generalizability, which is lacked in the conventional optimization approach to shorten the time-to-market.","","978-1-6654-0996-4","10.1109/AICAS54282.2022.9869961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869961","RF circuits;deep reinforcement learning;automation design","Radio frequency;Automation;Power amplifiers;Reinforcement learning;Radiofrequency integrated circuits;Task analysis;Wideband","deep learning (artificial intelligence);electronic design automation;integrated circuit design;network synthesis;power amplifiers;radiofrequency amplifiers;reinforcement learning","design principles;optimal circuit parameters;design specifications;average return loss;RL agent;GaN distributed RF power amplifer automation design;deep reinforcement learning;radio frequency circuit design;sophisticated circuits;design automation methodology;power amplifier;high-dimensional design challenges;RF circuit design;wideband nonuniform distributed RF power amplifier","","","","13","IEEE","5 Sep 2022","","","IEEE","IEEE Conferences"
"Self Learning in Flexible Manufacturing Units: A Reinforcement Learning Approach","D. Schwung; J. N. Reimann; A. Schwung; S. X. Ding","Dept. of Electrical Engineering, South Westfalia University of Applied Sciences, Soest, Germany; Dept. of Electrical Engineering, South Westfalia University of Applied Sciences, Soest, Germany; Dept. of Electrical Engineering, South Westfalia University of Applied Sciences, Soest, Germany; Dept. of Automation and Komplex Systems, University of Duisburg-Essen, Duisburg, Germany","2018 International Conference on Intelligent Systems (IS)","9 May 2019","2018","","","31","38","This paper presents a novel approach for self-learning as well as plug-and-play control of highly flexible, modular manufacturing units. The approach is inspired by recent encouraging results of reinforcement learning (RL) in computer science. However, instead of learning the entire control behavior which results in long training times and requires huge data sets, we restrict the learning process to the supervisor level by defining appropriate parameter from the basic control level (BCL) to be learned by learning agents. To this end, we define a set of interface parameter to the BCL programmed by IEC61131 compatible code, which will be used for learning. Typical control parameters include switching thresholds, timing parameters and transition conditions. We apply the approach to a laboratory testbed consisting of different production modules which underlines the efficiency improvements for manufacturing units. In addition, plug-and-produce control is enabled by the approach as different configuration of production modules can efficiently be put in operation by re-learning the parameter sets.","1541-1672","978-1-5386-7097-2","10.1109/IS.2018.8710460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8710460","Reinforcement learning;manufacturing control;self-learning;PLC-control","Control systems;Manufacturing processes;Reinforcement learning;Electrical engineering;Training","control engineering computing;flexible manufacturing systems;IEC standards;laboratory techniques;learning (artificial intelligence);production engineering computing","modular manufacturing units;computer science;basic control level;IEC61131 compatible code;reinforcement learning approach;flexible manufacturing units;self learning;plug-and-play control;laboratory testbed;production modules;plug-and-produce control;training","","13","","24","IEEE","9 May 2019","","","IEEE","IEEE Conferences"
"Digital Twin Enhanced Assembly Based on Deep Reinforcement Learning","J. Li; D. Pang; Y. Zheng; X. Le","Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China","2021 11th International Conference on Information Science and Technology (ICIST)","3 Jun 2021","2021","","","432","437","Discrete manufacturing is becoming a popular modality, which places a higher demand on the flexibility of the production line. Traditional assembly lines require extensive manual design and cannot meet the need for flexibility. Due to the rise of reinforcement learning, we suspect that modern algorithms are crucial to further improve the flexibility of assembly. In this paper, we propose a digital twin enhanced assembly method with deep reinforcement learning. A digital twin model of the assembly line is first built. Then, the deep deterministic policy gradient based reinforcement learning agent is trained on the digital twin model. The simulation of the reinforcement learning environment is based on a mixture of simulation engine and real signals. Thus, we can balance the training efficiency and the simulation accuracy. Finally, to validate our proposed method, peg-in-hole assembly experiments were conducted and good results were observed.","2573-3311","978-1-6654-1266-7","10.1109/ICIST52614.2021.9440555","Shanghai Rising-Star Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440555","deep reinforcement learning;digital twin;deep deterministic policy gradient (DDPG);flexible manufacture;discrete manufacturing","Training;Information science;Digital twin;Reinforcement learning;Production;Manuals;Manufacturing","assembling;gradient methods;learning (artificial intelligence)","deep deterministic policy gradient;reinforcement learning agent;digital twin model;reinforcement learning environment;peg-in-hole assembly experiments;deep reinforcement learning;discrete manufacturing;popular modality;higher demand;production line;traditional assembly lines;extensive manual design;digital twin enhanced assembly method;assembly line","","5","","18","IEEE","3 Jun 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Real-Time Assembly Planning in Robot-Based Prefabricated Construction","A. Zhu; T. Dai; G. Xu; P. Pauwels; B. De Vries; M. Fang","Department of Information Systems in the Built Environment, Eindhoven University of Technology, Eindhoven, AZ, The Netherlands; Department of Computing Science, University of Aberdeen, Aberdeen, U.K; Department of Aeronautical and Aviation Engineering, The Hong Kong Polytechnic University, Jurong West, Hong Kong; Department of Information Systems in the Built Environment, Eindhoven University of Technology, Eindhoven, AZ, The Netherlands; Department of Information Systems in the Built Environment, Eindhoven University of Technology, Eindhoven, AZ, The Netherlands; Department of Computer Science, University of Liverpool, Liverpool, U.K","IEEE Transactions on Automation Science and Engineering","30 Jun 2023","2023","20","3","1515","1526","The adoption of robotics is promising to improve the efficiency, quality, and safety of prefabricated construction. Besides technologies that improve the capability of a single robot, the automated assembly planning for robots at construction sites is vital for further improving the efficiency and promoting robots into practices. However, considering the highly dynamic and uncertain nature of a construction environment, and the varied scenarios in different construction sites, it is always challenging to make appropriate and up-to-date assembly plans. Therefore, this paper proposes a Deep Reinforcement Learning (DRL) based method for automated assembly planning in robot-based prefabricated construction. Specifically, a re-configurable simulator for assembly planning is developed based on a Building Information Model (BIM) and an open game engine, which could support the training and testing of various optimization methods. Furthermore, the assembly planning problem is modelled as a Markov Decision Process (MDP) and a set of DRL algorithms are developed and trained using the simulator. Finally, experimental case studies in four typical scenarios are conducted, and the performance of our proposed methods have been verified, which can also serve as benchmarks for future research works within the community of automated construction. Note to Practitioners— This paper is conducted based on the comprehensive analysis of real-life assembly planning processes in prefabricated construction, and the methods proposed could bring many benefits to practitioners. Firstly, the proposed simulator could be easily re-configured to simulate diverse scenarios, which can be used to evaluate and verify the operations’ optimization methods and new construction technologies. Secondly, the proposed DRL-based optimization methods can be directly adopted in various robot-based construction scenarios, and can also be tailored to support the assembly planning in traditional human-based or human-robot construction environments. Thirdly, the proposed DRL methods and their performance in the four typical scenarios can serve as benchmarks for proposing new advanced construction technologies and optimization methods in assembly planning.","1558-3783","","10.1109/TASE.2023.3236805","China Scholarship Council(grant numbers:202007720036); National Natural Science Foundation of China(grant numbers:72174042); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021301","Prefabricated construction;assembly planning;deep reinforcement learning (DRL);automated construction;building information modelling (BIM)","Planning;Robots;Prefabricated construction;Task analysis;Real-time systems;Safety;Decision making","assembly planning;building information modelling;construction industry;deep learning (artificial intelligence);design engineering;Markov processes;optimisation;prefabricated construction;reinforcement learning;telecommunication computing","advanced construction technologies;assembly planning problem;automated assembly planning;automated construction;construction environment;Deep Reinforcement;different construction sites;DRL methods;DRL-based optimization methods;human-robot construction environments;operations;promoting robots;real-life assembly;real-time assembly planning;robot-based construction scenarios;robot-based prefabricated construction;typical scenarios;up-to-date assembly plans","","","","84","IEEE","18 Jan 2023","","","IEEE","IEEE Journals"
"Reinforcement Learning Based Decision Making of Operational Indices in Process Industry Under Changing Environment","C. Liu; J. Ding; J. Sun","State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China","IEEE Transactions on Industrial Informatics","11 Jan 2021","2021","17","4","2727","2736","The plant-wide production process is composed of multiple unit processes, in which the operational indices of each unit process are assigned and adjusted according to product quality, yield, and actual operating modes. Due to the changing operational conditions of the production process, the operational indices cannot be effectively adjusted by most of the model-based methods or evolutionary computation. In this article, the decision making of operational indices is formulated as a continuous state, continuous action reinforcement learning (RL) problem and a model-free RL algorithm is proposed, which learns a decision policy to determine the operational indices according to the actual operational conditions. Different from the existing methods, this article presents a multiactor networks ensemble algorithm and an actor-critic framework with stochastic policy to avoid falling into local optimums. The relatively overall optimal policy is obtained by extracting the results of parallel training of multiactor networks, which guarantees the optimality of the obtained policy. In addition, by using the experience replay, it is particularly valuable to effectively deal with the problem that lacking of sampling data in the model-free RL. Simulation studies are conducted on actual data of a mineral processing plant and the results demonstrate the effectiveness of the proposed algorithm.","1941-0050","","10.1109/TII.2020.3005207","National Natural Science Foundation of China(grant numbers:61988101,61525302,61590922,61773068,61733005); National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB1701104); Xingliao Plan of Liaoning Province(grant numbers:XLYC1808001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9127490","Actor-critic (AC) algorithm;experience replay;multiactor networks ensemble (MAE);operational indices;process industry;reinforcement learning (RL)","Production;Decision making;Optimization;Industries;Quality assessment;Product design;Reinforcement learning","decision making;evolutionary computation;learning (artificial intelligence)","multiple unit processes;operational indices;unit process;actual operating modes;changing operational conditions;continuous action reinforcement learning problem;actual operational conditions;mineral processing plant;process industry;plant-wide production process","","9","","24","IEEE","29 Jun 2020","","","IEEE","IEEE Journals"
"Multi-Agent Reinforcement Learning for Real-Time Dynamic Production Scheduling in a Robot Assembly Cell","D. Johnson; G. Chen; Y. Lu","Department of Mechanical and Mechatronics Engineering, The University of Auckland, Auckland, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; Department of Mechanical and Mechatronics Engineering, The University of Auckland, Auckland, New Zealand","IEEE Robotics and Automation Letters","29 Jun 2022","2022","7","3","7684","7691","As industry rapidly shifts towards mass personalisation, the need for a decentralised multi-agent system capable of dynamic flexible job shop scheduling (FJSP) is evident. Traditional heuristic and meta-heuristic scheduling methods cannot achieve satisfactory results and have limited application to static environments. Recent Reinforcement Learning (RL) approaches that consider dynamic FJSP, lack flexibility and autonomy as they use a single-agent centralised model, assuming global observability. As such, we propose a Multi-Agent Reinforcement Learning (MARL) system for scheduling dynamically arriving assembly jobs in a robot assembly cell. We applied a Double DQN-based algorithm and proposed a generalised observation, action and reward design for the dynamic FJSP setting. Using a centralised training phase, each agent (i.e., robot) in the assembly cell executes decentralised scheduling decisions based on local observations. Our solution demonstrated improved performance against rule-based heuristic methods, for optimising makespan. We also reported the impact of different observation sizes of each agent on optimisation performance.","2377-3766","","10.1109/LRA.2022.3184795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9801608","Reinforcement learning;intelligent and flexible manufacturing;double DQN;flexible job shop scheduling problem (FJSP);multi-robot systems;mass personalisation","Robots;Job shop scheduling;Dynamic scheduling;Robot kinematics;Computer architecture;Microprocessors;Heuristic algorithms","job shop scheduling;learning (artificial intelligence);multi-agent systems;robotic assembly;scheduling","real-time dynamic production scheduling;robot assembly cell;mass personalisation;decentralised multiagent system;dynamic flexible job shop scheduling;traditional heuristic;meta-heuristic scheduling methods;static environments;Recent Reinforcement;lack flexibility;single-agent centralised model;global observability;MultiAgent Reinforcement Learning system;assembly jobs;Double DQN-based algorithm;reward design;dynamic FJSP setting;centralised training phase;assembly cell executes;scheduling decisions;local observations;rule-based heuristic methods;different observation sizes","","13","","24","IEEE","20 Jun 2022","","","IEEE","IEEE Journals"
"A framework for scheduling in cloud manufacturing with deep reinforcement learning","Y. Liu; L. Zhang; L. Wang; Y. Xiao; X. Xu; M. Wang","Center for Smart Manufacturing Systems, Xidian University, Xi’an, China; Center for Smart Manufacturing Systems, Xidian University, Xi’an, China; Center for Smart Manufacturing Systems, Xidian University, Xi’an, China; State Key Laboratory of Intelligent, Manufacturing System Technology, Beijing Institute of Electronic System Engineering, Beijing, China; Department of Mechanical Engineering, The University of Auckland, Auckland, New Zealand; State Key Laboratory of Intelligent, Manufacturing System Technology, Beijing Institute of Electronic System Engineering, Beijing, China","2019 IEEE 17th International Conference on Industrial Informatics (INDIN)","30 Jan 2020","2019","1","","1775","1780","Cloud manufacturing is a novel service-oriented networked manufacturing paradigm that aims to provide on-demand manufacturing cloud services to consumers. Scheduling is a critical means for achieving that aim. Currently, research on scheduling in cloud manufacturing is still in its infancy, and current frequently adopted meta-heuristic algorithm-based approaches have some shortcomings, e.g. they require complex design processes and lack adaptability to dynamic environments. Deep reinforcement learning (DRL) that combines advantages of reinforcement learning and deep learning provides an efficient, adaptive and intelligent approach for solving scheduling problems in cloud manufacturing. However, to the best of our knowledge, there has been no application of DRL to scheduling in cloud manufacturing. This work conducts a preliminary exploration over this issue. First, a DRL-based framework for scheduling in cloud manufacturing is proposed. Then a DRL model for online single-task scheduling in cloud manufacturing is presented to demonstrate the effectiveness of the framework. DRL as a promising technique will find wide applications in cloud manufacturing, and this work can provide some reference for future research on this.","2378-363X","978-1-7281-2927-3","10.1109/INDIN41052.2019.8972157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8972157","cloud manufacturing;scheduling;deep reinforcement learning","","cloud computing;learning (artificial intelligence);manufacturing systems;neural nets;production engineering computing;scheduling;Web services","cloud manufacturing;deep reinforcement learning;service-oriented networked manufacturing;cloud services;DRL;online single-task scheduling","","3","","32","IEEE","30 Jan 2020","","","IEEE","IEEE Conferences"
"Efficient Insertion Control for Precision Assembly Based on Demonstration Learning and Reinforcement Learning","Y. Ma; D. Xu; F. Qin","School of Computer and Software, Nanjing Vocational University of Industry Technology, Nanjing, China; Research Center of Precision Sensing and Control, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Research Center of Precision Sensing and Control, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Industrial Informatics","5 Apr 2021","2021","17","7","4492","4502","Multiple peg-in-hole insertion control is one of the challenging tasks in precision assembly for its complex contact dynamics. In this article, an insertion policy learning method is proposed for multiple peg-in-hole precision assembly. The insertion policy learning process is separated into two phases: initial policy learning and residual policy learning. In initial policy learning, a state-to-action policy mapping model based on the Gaussian mixture model (GMM) is established. And Gaussian mixture regression (GMR) is used to generalize the policy reuse. In residual policy learning, a reinforcement learning method named normalized advantage function (NAF) is employed to refine the insertion policy via agent's exploration in the insertion environment. Moreover, an adaptive action exploration (AAE) strategy is designed to improve the performance of exploration, and the prioritized experience replay strategy is introduced to make the residual policy learning from historical experience more efficient. Besides, the hierarchical reward function is designed considering the contact dynamics as well as the efficiency and safety of precision insertion. Finally, comprehensive experiments are conducted to validate the effectiveness of the proposed insertion policy learning method.","1941-0050","","10.1109/TII.2020.3020065","National Natural Science Foundation of China(grant numbers:61873266,61733004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9180070","Demonstration learning;insertion policy learning;multiple peg-in-hole insertion;precision assembly;reinforcement learning","Learning (artificial intelligence);Task analysis;Learning systems;Gaussian distribution;Informatics;Automation;Data models","assembling;Gaussian processes;learning (artificial intelligence);mixture models;precision engineering;production engineering computing;regression analysis","adaptive action exploration;Gaussian mixture regression;Gaussian mixture model;reinforcement learning;state-to-action policy mapping model;residual policy learning;initial policy learning;insertion policy learning process;multiple peg-in-hole precision assembly;insertion policy learning method;multiple peg-in-hole insertion control;demonstration learning","","15","","24","IEEE","28 Aug 2020","","","IEEE","IEEE Journals"
"Data-Efficient Hierarchical Reinforcement Learning for Robotic Assembly Control Applications","Z. Hou; J. Fei; Y. Deng; J. Xu","State Key Laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, and Department of Mechanical Engineering, Tsinghua University, Beijing, China; State Key Laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, and Department of Mechanical Engineering, Tsinghua University, Beijing, China; State Key Laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, and Department of Mechanical Engineering, Tsinghua University, Beijing, China; State Key Laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, and Department of Mechanical Engineering, Tsinghua University, Beijing, China","IEEE Transactions on Industrial Electronics","29 Jul 2021","2021","68","11","11565","11575","Hierarchical reinforcement learning (HRL) can learn the decomposed subpolicies corresponding to the local state-space; therefore, it is a promising solution to complex robotic assembly control tasks with fewer interactions with environments. Most existing HRL algorithms often require on-policy learning, where resampling is necessary for every training step. In this article, we propose a data-efficient HRL via off-policy learning with three main contributions. First, two augmented MDPs (Markov decision processes) are reformulated to learn the higher level policy and lower level policy from the same samples. Second, to learn higher level policy that leads to efficient exploration, a softmax gating policy is derived to determine the lower level policy for interacting with the environment. Third, to learn the lower level policies via off-policy samples from one lower level replay buffer, the higher level policy derived by the option-value network is adopted to select the appropriate option for learning the corresponding lower level policy. The data-efficiency performance of our algorithm is validated on two simulations and real-world robotic dual peg-in-hole assembly tasks.","1557-9948","","10.1109/TIE.2020.3038072","National Key R&D Program of China(grant numbers:2017YFC0822204); National Natural Science Foundation of China(grant numbers:U1613205,51675291,51935010); Beijing Municipal Natural Science Foundation(grant numbers:L192001); Funding for Basic Scientific Research Program(grant numbers:JCKY2018205B029); State Key Laboratory of China(grant numbers:SKL2020C15); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264727","Data-efficiency;hierarchical reinforcement learning;robotic assembly control","Task analysis;Robots;Robotic assembly;Aerospace electronics;Trajectory;Training;Reinforcement learning","control engineering computing;learning (artificial intelligence);Markov processes;robotic assembly","off-policy learning;softmax gating policy;data-efficiency performance;peg-in-hole assembly tasks;data-efficient hierarchical reinforcement learning;local state-space;on-policy learning;data-efficient HRL;robotic assembly control;Markov decision processes;augmented MDPs","","26","","41","IEEE","19 Nov 2020","","","IEEE","IEEE Journals"
"Control of a re-entrant line manufacturing model with a reinforcement learning approach","J. A. Ramirez-Hernandez; E. Fernandez","Department of Electrical & Computer Engineering, University of Cincinnati, Cincinnati, OH, USA; Department of Electrical & Computer Engineering, University of Cincinnati, Cincinnati, OH, USA","Sixth International Conference on Machine Learning and Applications (ICMLA 2007)","25 Feb 2008","2007","","","330","335","This paper presents the application of a reinforcement learning (RL) approach for the near-optimal control of a re-entrant line manufacturing (RLM) model. The RL approach utilizes an algorithm based on a gradient-descent TD(lambda) method to obtain both estimates of the optimal cost function and the control actions. Numerical experiments demonstrated the efficacy of the approach in estimating optimal actions by showing close approximations in performance w.r.t. the optimal strategy. Generalizations of the RL approach may have the advantage of scaling appropriately for RLM models with different dimensions in the state and action spaces.","","978-0-7695-3069-7","10.1109/ICMLA.2007.78","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4457252","","Virtual manufacturing;Learning;Optimal control;Control systems;Computer aided manufacturing;Electrical equipment industry;Manufacturing industries;Application software;Pulp manufacturing;Cost function","gradient methods;industrial control;learning (artificial intelligence);optimal control","re-entrant line manufacturing model;reinforcement learning approach;near-optimal control;gradient-descent method","","5","","20","IEEE","25 Feb 2008","","","IEEE","IEEE Conferences"
"Development of a Reinforcement Learning-Based Adaptive Scheduling Algorithm for Block Assembly Production Line","J. H. Woo; Y. I. Cho; S. H. Nam; J. -H. Nam","Department of Naval Architecture and Ocean Engineering, Seoul National University, Seoul, SOUTH KOREA; Department of Naval Architecture and Ocean Engineering, Seoul National University, Seoul, SOUTH KOREA; Department of Naval Architecture and Ocean Engineering, Seoul National University, Seoul, SOUTH KOREA; Major of Naval Architecture and Ocean Systems Engineering, Korea Maritime and Ocean University, Busan, SOUTH KOREA","2021 Winter Simulation Conference (WSC)","23 Feb 2022","2021","","","1","12","Rule-based heuristic algorithms and meta-heuristic algorithms have been studied to solve the scheduling problems of production systems. In recent research, reinforcement learning-based adaptive scheduling algorithms have been applied to solve complex problems with high-dimensional and vast state space. A production system in shipyards is a high-variable system, in which various production factors such as space, workforce, and resources are related. Thus, adaptive scheduling according to the changes in the production system and surrounding environment must be performed in shipyards. In this study, a basic reinforcement learning model for scheduling problems of shipyards was developed. A simplified model of the panel block shop in shipyards was assumed and the optimal policy for determining the input sequence of blocks was learned to reduce the flow time. The open source-based discrete event simulation (DES) kernel SimPy was incorporated into the environment of the reinforcement learning model.","1558-4305","978-1-6654-3311-2","10.1109/WSC52266.2021.9715509","Ministry of Trade, Industry and Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9715509","","Adaptation models;Production systems;Adaptive scheduling;Job shop scheduling;Heuristic algorithms;Reinforcement learning;Discrete event simulation","adaptive scheduling;assembling;discrete event simulation;knowledge based systems;learning (artificial intelligence);production planning;productivity;scheduling;shipbuilding industry","open source-based discrete event simulation kernel SimPy;reinforcement learning-based adaptive scheduling algorithm;block assembly production line;rule-based heuristic algorithms;meta-heuristic algorithms;scheduling problems;production system;high-dimensional state space;vast state space;shipyards;high-variable system;production factors;basic reinforcement learning model","","1","","16","IEEE","23 Feb 2022","","","IEEE","IEEE Conferences"
"A centralized reinforcement learning approach for proactive scheduling in manufacturing","S. Qu; T. Chu; J. Wang; J. Leckie; W. Jian","Center for Sustainable Development & Global Competitiveness, Stanford University, Stanford, USA; Center for Sustainable Development & Global Competitiveness, Stanford University, Stanford, USA; Center for Sustainable Development & Global Competitiveness, Stanford University, Stanford, USA; Center for Sustainable Development & Global Competitiveness, Stanford University, Stanford, USA; Center for Sustainable Development & Global Competitiveness, Stanford University, Stanford, USA","2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA)","26 Oct 2015","2015","","","1","8","Due to rapid development of information and communications technology (ICT) and the impetus for more effective, efficient and adaptive manufacturing, the concept of ICT based advanced manufacturing has increasingly become a prominent research topic across academia and industry during recent years. One critical aspect of advanced manufacturing is how to incorporate real time information and then optimally schedule manufacturing processes with multiple objectives. Due to its complexity and the need for adaptation, the manufacturing scheduling problem presents challenges for utilizing advanced ICT and thus calls for new approaches. The paper proposes a centralized reinforcement learning approach for optimally scheduling of a manufacturing system of multi-stage processes and multiple machines for multiple types of products. The approach, which employs learning and control algorithms to enable real time cooperation of each processing unit inside the system, is able to adaptively respond to dynamic scheduling changes. More specifically, we first formally define the scheduling problem through the construction of an objective function and related heuristic constraints for the underlying manufacturing tasks. Next, to effectively deal with the problem we defined, we maintain a distributed weighted vector to capture the cooperative pattern of massive action space and apply the reinforcement-learning approach to achieve the optimal policies for a set of processing machines according to a real time production environment, including dynamic requests for various products. Numerical experiments demonstrate that compared to different heuristic methods and multi-agent algorithms, the proposed centralized reinforcement learning method can provide more reliable solutions for the scheduling problem.","1946-0759","978-1-4673-7929-8","10.1109/ETFA.2015.7301417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301417","scheduling;reinforcement learning;multi-stage;multi-product;advanced manufacturing;real-time information;centralized system","Job shop scheduling;Learning (artificial intelligence);Processor scheduling;Optimal scheduling;Manufacturing;Real-time systems","learning (artificial intelligence);manufacturing processes;production engineering computing;scheduling","centralized reinforcement learning approach;proactive scheduling;information and communications technology;ICT based advanced manufacturing;real time information;optimal manufacturing process scheduling;multistage process;multiple machines;processing unit;dynamic scheduling;objective function;heuristic constraints;distributed weighted vector;cooperative pattern;action space;optimal policies;processing machines;real time production environment;dynamic requests","","14","","25","IEEE","26 Oct 2015","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Method for Inventory Control Under State-based Stochastic Demand","H. Yin; Q. Jiang; J. Ruan; C. Zhang","Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, China","2022 5th World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM)","27 Jan 2023","2022","","","527","532","With the development of intelligent manufacturing, managers increasingly realize that to improve the efficiency of production, we should not only improve the level of manufacturing technology, but also enhance the ability of supply chain management. In supply chain management, inventory control is always essential but troubling. Because the formulation of the optimal strategy in different inventory scenarios is very complicated, the model in the literature often makes compromise when compared to the reality. This paper deals with the non-stationary demand considering the impact of promotion and seasonal factors. Specifically, the paper constructs an open-AI gym environment for inventory control problems with state-based demand, and proposes a complete modeling method. A reinforcement learning method based on SARSA is developed to solve the problem. We conduct a series of numerical experiments, which provide guidance for the selection of the hyper-parameter $\varepsilon$, and confirm the ability of reinforcement learning to capture the unknown potential demand changes in the environment. Finally, some suggestions are drawn on how to deploy reinforcement learning in real inventory scenarios.","","978-1-6654-7369-9","10.1109/WCMEIM56910.2022.10021568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021568","component;inventory control;reinforcement learning;non-stationary;SARSA","Supply chain management;Stochastic processes;Reinforcement learning;Production;Inventory control;Manufacturing;Numerical models","numerical analysis;optimisation;production engineering computing;reinforcement learning;stochastic processes;stock control;supply chain management","complete modeling method;intelligent manufacturing;inventory control problems;manufacturing technology;nonstationary demand;numerical experiments;open-AI gym environment;optimal strategy;reinforcement learning method;SARSA;state-based demand;state-based stochastic demand;supply chain management","","","","16","IEEE","27 Jan 2023","","","IEEE","IEEE Conferences"
"Reasoning over OPC UA Information Models using Graph Embedding and Reinforcement Learning","J. Bakakeu; F. Schäfer; J. Franke; S. Baer; H. -H. Klos; J. Peschke","Institute for Factory Automation and Production Systems, Friedrich-Alexander-Universität Erlangen-Nürnberg, 91058 Erlangen, Germany; Institute for Factory Automation and Production Systems, Friedrich-Alexander-Universität Erlangen-Nürnberg, 91058 Erlangen, Germany; Institute for Factory Automation and Production Systems, Friedrich-Alexander-Universität Erlangen-Nürnberg, 91058 Erlangen, Germany; Digital Factory Division, Siemens AG, 90475 Nürnberg, Germany; Digital Factory Division, Siemens AG, 90475 Nürnberg, Germany; Digital Factory Division, Siemens AG, 90475 Nürnberg, Germany","2020 Third International Conference on Artificial Intelligence for Industries (AI4I)","13 Nov 2020","2020","","","40","47","With the fourth industrial revolution, the OPC Unified Architecture (OPC UA) has emerged as the standard communication framework for the implementation of cyber-physical production systems since it can be used for both, communication and information modeling. Even though OPC UA helps bridge the interoperability gap at the automation level, its semantic has not yet been formally defined and generated or manually created OPC UA information models are often incomplete and inconsistent making an efficient automated reasoning and knowledge inference on the OPC UA address space particularly challenging.In this paper, we show that it is possible to train a machine learning system on OPC UA information models, such that it performs automated reasoning over OPC UA knowledge graphs with high precision and recall. More specifically, we present a reinforcement learning based solution that learns to reason on semantically incomplete OPC UA information models by constructing multi-hop relational paths along an embedded vector space of the knowledge graph representing the information model. The construction of such relational paths allows the discovery of missing relations between the entities and at the same time the evaluation of the truth of the encoded triples, thus enabling consistency checks and questions answering.","","978-1-7281-8701-3","10.1109/AI4I49448.2020.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9253080","OPC UA;machine learning;graph embedding;knowledge inference;reinforcement learning;multi-hop reasoning;link prediction;entity relation extraction","Production systems;Semantics;Reinforcement learning;Predictive models;Knowledge discovery;Cognition;Standards","factory automation;graph theory;inference mechanisms;learning (artificial intelligence);production engineering computing;question answering (information retrieval)","cyber-physical production systems;information modeling;OPC UA address space;machine learning system;OPC UA knowledge graphs;OPC UA information models;OPC Unified Architecture;automated reasoning;fourth industrial revolution;graph embedding;reinforcement learning;consistency checks;questions answering;multihop relational paths;embedded vector space","","2","","28","IEEE","13 Nov 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Optimization of Bayesian Networks for Generating Feasible Vehicle Configuration Suggestions","S. Dürr; R. Lamprecht; M. Kauffmann; M. F. Huber","Dr. Ing. h.c. F. Porsche AG, Stuttgart; Center for Cyber Cognitive Intelligence (CCI), Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Stuttgart; Dr. Ing. h.c. F. Porsche AG, Stuttgart; Institute of Industrial Manufacturing and Management IFF, Stuttgart","2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)","5 Oct 2021","2021","","","16","22","A promising method in the automotive industry to anticipate future customer demands is the concept of planned orders. Due to multi-variant products, changing customer demands, and dynamic environments the process of generating planned orders is challenging. This paper introduces an approach using graphical models to generate planned order suggestions in a multi-variant order management process. Bayesian networks are modelled by learning the structure from different data sources, which enable the possibility to directly sample configuration suggestions. To find an optimized graph structure, a method using hierarchical correlation clustering and reinforcement learning is applied, taking into account technical and sales-operated feasibility constraints. The method has high potential in practical usage and is evaluated by a realworld use case of the Dr. Ing. h.c. F. Porsche AG.","2161-8089","978-1-6654-1873-7","10.1109/CASE49439.2021.9551428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551428","","Measurement;Industries;Correlation;Transfer learning;Reinforcement learning;Production;Bayes methods","automobile industry;belief networks;learning (artificial intelligence);optimisation;order processing;pattern clustering;production engineering computing;vehicles","sales-operated feasibility;hierarchical correlation clustering;vehicle configuration;optimization;reinforcement learning;multivariant order management process;graphical models;multivariant products;planned orders;customer demands;automotive industry;bayesian networks","","","","25","IEEE","5 Oct 2021","","","IEEE","IEEE Conferences"
"Self-Optimization in Smart Production Systems using Distributed Reinforcement Learning","D. Schwung; M. Modali; A. Schwung","South Westphalia University of Applied Sciences, Soest, Germany; South Westphalia University of Applied Sciences, Soest, Germany; South Westphalia University of Applied Sciences, Soest, Germany","2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)","28 Nov 2019","2019","","","4063","4068","This paper introduces a novel approach for self-learning in highly flexible, modular manufacturing systems enabling fast reconfiguration and online adaptation to changing production requirements. The approach is based on a distributed optimization scheme such that production modules are equipped with their own optimization agent with its local objectives to be optimized. The communication and coordination of the agent is limited to the basically required amount. The approach is based on the recently developed deep deterministic policy gradient (DDPG) approach, a high performing algorithm from the family of actor-critic reinforcement learning algorithms. As DDPG is based on single agent learning, we develop a fully distributed multi-agent learning setting with different levels of information about the neighbors. We apply the approach to a laboratory scale distributed bulk good production testbed with very encouraging results. Particularly, we found very reasonable control strategies by learning the agents from scratch.","2577-1655","978-1-7281-4569-3","10.1109/SMC.2019.8914088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8914088","","Optimization;Reinforcement learning;Manufacturing processes;Service-oriented architecture;Decentralized control","flexible manufacturing systems;intelligent manufacturing systems;learning (artificial intelligence);multi-agent systems;optimisation;production engineering computing","actor-critic reinforcement learning algorithms;DDPG;single agent learning;multiagent learning setting;smart production systems;distributed reinforcement learning;highly flexible manufacturing systems;modular manufacturing systems;online adaptation;production requirements;distributed optimization scheme;production modules;optimization agent;deep deterministic policy gradient approach;laboratory scale distributed bulk good production testbed","","6","","29","IEEE","28 Nov 2019","","","IEEE","IEEE Conferences"
"Multi-index Evaluation based Reinforcement Learning Method for Cyclic Optimization of Multiple Energy Utilization in Steel Industry","Z. Wang; L. Wang; Z. Han; J. Zhao","Key Laboratory of Intelligent Control and Optimization for Industrial Equipment, Dalian University of Technology, Ministry of Education; Key Laboratory of Intelligent Control and Optimization for Industrial Equipment, Dalian University of Technology, Ministry of Education; Key Laboratory of Intelligent Control and Optimization for Industrial Equipment, Dalian University of Technology, Ministry of Education; Key Laboratory of Intelligent Control and Optimization for Industrial Equipment, Dalian University of Technology, Ministry of Education","2020 39th Chinese Control Conference (CCC)","9 Sep 2020","2020","","","5766","5771","Energy including coal, electricity, gas, etc., are directly related to the production process of steel industry. As such, a scientific and reasonable scheme for energy utilization is undoubtedly support the fulfillment of the production plan along with reduce energy consumption and environmental pollution. Aiming at optimizing the input amount of each energy resource in the Integrated Energy System (IES) of steel industry, a series of optimization models along with cyclic solution method based on Actor-Critic are proposed in this study, which can efficiently obtain the optimal energy utilization scheme considering both the global and local index. Firstly, the Energy Consumption per Ton Steel in the perspective of the whole IES and the emission of the byproduct gas regarding the energy subsystem are defined as the optimization objective respectively. Then, with the consideration of the practical constraints of the energy network, the mathematical programming models are accordingly established. Due to the existence of nonlinear constraints which brings about the difficulty for the conventional optimization method, a reinforcement learning based strategy is then designed to cyclically solve the optimization models for obtaining the final energy utilization scheme. Based on the practical data of steel industry, the experiment results demonstrate that compared with the commonly deployed methods, the proposed approach exhibits obvious superiority in controlling energy consumption as well as improving energy efficiency, which provides effective support for enterprises on energy saving.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9189540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9189540","Integrated Energy System (IES);Steel industry;Cyclic Reinforcement Learning;Multi-index evaluation","Optimization;Production;Steel industry;Energy consumption;Indexes;Steel;Mathematical model","air pollution control;coal;energy conservation;energy consumption;learning (artificial intelligence);mathematical programming;production engineering computing;production planning;steel industry","multiindex evaluation based reinforcement learning method;steel industry;production plan;energy consumption;integrated energy system;IES;cyclic solution method;energy subsystem;energy network;reinforcement learning based strategy;energy efficiency;energy utilization scheme;environmental pollution;cyclic optimization model;mathematical programming models;nonlinear constraints;energy saving","","1","","16","","9 Sep 2020","","","IEEE","IEEE Conferences"
"A deep reinforcement learning algorithm for order acceptance decision of individualized product assembling","H. Zhang; J. Leng; H. Zhang; G. Ruan; M. Zhou; Y. Zhang","School of Electromechanical Engineering Guangdong University of Technology, Guangzhou, China; School of Electromechanical Engineering Guangdong University of Technology, Guangzhou, China; School of Electromechanical Engineering Guangdong University of Technology, Guangzhou, China; School of Electromechanical Engineering Guangdong University of Technology, Guangzhou, China; School of Electromechanical Engineering Guangdong University of Technology, Guangzhou, China; School of management Jinan university, Guangzhou, China","2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI)","22 Sep 2021","2021","","","21","24","Aiming at the order acceptance decision problem of modular production enterprises, a semi Markov decision process-based order acceptance decision model is constructed. The enterprise's production capacity is divided into four independent production units, and each production unit has its daily capacity and total capacity limit. The deep reinforcement learning algorithm is used to explore and learn in the model to maximize the overall revenue when the production capacity and order delivery time are limited. At the same time, it also solves the problem that the state space is complex that the traditional Q-learning algorithm is not competent. The strategy's effectiveness is verified by comparing the trained strategy with First-Come-First-Serve(FCFS) strategy in the same environment.","","978-1-6654-3337-2","10.1109/DTPI52967.2021.9540190","National Key Research and Development Program of China(grant numbers:2018AAA0101700); National Natural Science Foundation of China(grant numbers:52075107); Science and Technology Planning Project of Guangdong Province of China(grant numbers:2019A050503010); China Postdoctoral Science Foundation(grant numbers:2020M672541); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9540190","Deep reinforcement learning;Order acceptance;Modular production;Mass customization","Job shop scheduling;Digital twin;Conferences;Production;Reinforcement learning;Markov processes;Manufacturing","assembling;deep learning (artificial intelligence);Markov processes;order processing;production engineering computing","total capacity limit;deep reinforcement learning algorithm;production capacity;Q-learning algorithm;product assembling;modular production enterprises;semiMarkov decision process-based order acceptance decision model;independent production units;first-come-first-serve strategy;FCFS","","1","","13","IEEE","22 Sep 2021","","","IEEE","IEEE Conferences"
"Post-layout simulation automation based on Reinforcement Learning using Schematic to Layout sync module","J. Jeong; H. Kim; T. -H. Kim; E. Cheon","Memory Division, Samsung Electronics Co. Ltd, Hwaseong, Republic of Korea; Memory Division, Samsung Electronics Co. Ltd, Hwaseong, Republic of Korea; Memory Division, Samsung Electronics Co. Ltd, Hwaseong, Republic of Korea; Memory Division, Samsung Electronics Co. Ltd, Hwaseong, Republic of Korea","2023 International Conference on Electronics, Information, and Communication (ICEIC)","10 Mar 2023","2023","","","1","4","Due to complexity and difficulty of circuit design, attempts to automate circuit design increase recently. In particular, reinforcement learning based circuit optimization increases. Traditional research focused on pre-layout simulation because of difficulty of automation layout. In this paper, we proposed post-layout simulation automation method using 3 kinds of modules. Modules are consisted of pre-processing data module, schematic to layout sync module, and parasitic extraction tool automation module. These modules contribute to optimize circuit accurately. As a result, modifying layout DB time decreases nearly 75.9%, and the variation of the skew and the duty cycle error decrease 53.4%, 12.0% respectively.","2767-7699","979-8-3503-2021-3","10.1109/ICEIC57457.2023.10049861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049861","Post-layout simulation;design automation;Reinforcement learning;Q-learning;circuit optimization","Automation;Monte Carlo methods;Layout;Reinforcement learning;Manuals;Circuit synthesis;Synchronization","circuit analysis computing;circuit optimisation;integrated circuit design;reinforcement learning","automation layout;circuit design;layout DB time;layout sync module;parasitic extraction tool automation module;post-layout simulation automation method;pre-processing data module;reinforcement learning based circuit optimization","","","","11","IEEE","10 Mar 2023","","","IEEE","IEEE Conferences"
"Reinforcement-Learning Based Robotic Assembly of Fractured Objects Using Visual and Tactile Information","X. Song; N. Lamb; S. Banerjee; N. K. Banerjee","Department of Computer Science, Clarkson University Potsdam, New York, USA; Department of Computer Science, Clarkson University Potsdam, New York, USA; Department of Computer Science, Clarkson University Potsdam, New York, USA; Department of Computer Science, Clarkson University Potsdam, New York, USA","2023 9th International Conference on Automation, Robotics and Applications (ICARA)","23 May 2023","2023","","","170","174","Though several approaches exist to automatically generate repair parts for fractured objects, there has been little prior work on the automatic assembly of generated repair parts. Assembly of repair parts to fractured objects is a challenging problem due to the complex high-frequency geometry at the fractured region, which limits the effectiveness of traditional controllers. We present an approach using reinforcement learning that combines visual and tactile information to automatically assemble repair parts to fractured objects. Our approach overcomes the limitations of existing assembly approaches that require objects to have a specific structure, that require training on a large dataset to generalize to new objects, or that require the assembled state to be easily identifiable, such as for peg-in-hole assembly. We propose two visual metrics that provide estimation of assembly state with 3 degrees of freedom. Tactile information allows our approach to assemble objects under occlusion, as occurs when the objects are nearly assembled. Our approach is able to assemble objects with complex interfaces without placing requirements on object structure.","2767-7745","978-1-6654-8921-8","10.1109/ICARA56516.2023.10125938","NSF(grant numbers:IIS-2023998); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10125938","reinforcement learning;fractured shape repair;machine learning;robotic assembly;sim-to-real","Training;Measurement;Geometry;Visualization;Solid modeling;Shape;Reinforcement learning","assembling;learning (artificial intelligence);production engineering computing;reinforcement learning;robotic assembly","assembled state;assembly approaches;assembly state;automatic assembly;fractured objects;fractured region;generated repair parts;object structure;peg-in-hole assembly;reinforcement learning;reinforcement-learning;robotic assembly;tactile information","","","","26","IEEE","23 May 2023","","","IEEE","IEEE Conferences"
"A Multiresolution Analysis-Assisted Reinforcement Learning Approach to Run-by-Run Control","R. Ganesan; T. K. Das; K. M. Ramachandran","Department of Systems Engineering and Operations Research, George Mason University, Fairfax, VA, USA; Department of Industrial and Management Systems Engineering, University of South Florida, Tampa, FL, USA; Department of Mathematics, University of South Florida, Tampa, FL, USA","IEEE Transactions on Automation Science and Engineering","10 Apr 2007","2007","4","2","182","193","In recent years, the run-by run (RbR) control mechanism has emerged as a useful tool for keeping complex semiconductor manufacturing processes on target during repeated short production runs. Many types of RbR controllers exist in the literature of which the exponentially weighted moving average (EWMA) controller is widely used in the industry. However, EWMA controllers are known to have several limitations. For example, in the presence of multiscale disturbances and lack of accurate process models, the performance of EWMA controller deteriorates and often fails to control the process. Also, the control of complex manufacturing processes requires sensing of multiple parameters that may be spatially distributed. New control strategies that can successfully use spatially distributed sensor data are required. This paper presents a new multiresolution analysis (wavelet) assisted reinforcement learning (RL)-based control strategy that can effectively deal with both multiscale disturbances in processes and the lack of process models. The novel idea of a wavelet-aided RL-based controller represents a paradigm shift in the control of large-scale stochastic dynamic systems of which the control problem is a subset. Henceforth, we refer our new control strategy as a WRL-RbR controller. The WRL-RbR controller is tested on a multiple-input-multiple-output chemical mechanical planarization process of wafer fabrication for which the process model is available. Results show that the RL controller outperforms EWMA-based controllers for low autocorrelation. The new controller also performs quite well for strongly autocorrelated processes for which the EWMA controllers are known to fail. Convergence analysis of the new breed of the WRL-RbR controller is presented. Further enhancement of the controller to deal with model-free processes and for inputs coming from spatially distributed environments are also discussed. Note to Practitioners-This work was motivated by the need to develop an intelligent and efficient RbR process controller, especially for the control of processes with short production runs as in the case of the semiconductor manufacturing industry. A novel controller that is presented here is capable of generating optimal control actions in the presence of multiple time-frequency disturbances, and allows the use of realistic (often complex) process models without sacrificing robustness and speed of execution. Performance measures, such as reduction of variability in process output and control recipe, minimization of initial bias, and ability to control processes with high autocorrelations are shown to be superior in comparison to the commercially available exponentially weighted moving average controllers. The WRL-RbR controller is very generic, and can be easily extended to processes with drifts and sudden shifts in the mean and variance. The viability of extending the controller to distributed input parameter sensing environments, including those for which process models are not available, is also discussed","1558-3783","","10.1109/TASE.2006.879915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4147550","Chemical mechanical planarization (CMP);exponentially weighted moving average (EWMA);multiresolution;reinforcement learning;run-by-run control;wavelet;wavelet-modulated reinforcement learning–run-by run (WRL–RbR)","Learning;Process control;Control systems;Autocorrelation;Manufacturing processes;Production;Manufacturing industries;Distributed control;Semiconductor device modeling;Optimal control","chemical mechanical polishing;distributed sensors;integrated circuit manufacture;intelligent control;learning (artificial intelligence);MIMO systems;moving average processes;optimal control;planarisation;statistical process control;stochastic systems;wavelet transforms","multiresolution analysis-assisted reinforcement learning approach;complex semiconductor manufacturing processes;exponentially weighted moving average controller;spatially distributed sensor data;wavelet assisted reinforcement learning-based control strategy;multiscale disturbances;large-scale stochastic dynamic systems;multiple-input-multiple-output process;MIMO chemical mechanical planarization process;wafer fabrication;autocorrelated process;convergence analysis;model-free processes;intelligent process controller;semiconductor manufacturing industry;optimal control;multiple time-frequency disturbances;wavelet-modulated reinforcement learning-run-by run control","","16","","41","IEEE","10 Apr 2007","","","IEEE","IEEE Journals"
"Model-driven reinforcement learning and action dimension extension method for efficient asymmetric assembly","Y. Gai; J. Guo; D. Wu; K. Chen","Department of Mechanical Engineering, Tsinghua University, Beijing, China; Department of Mechanical Engineering, Tsinghua University, Beijing, China; Department of Mechanical Engineering, Tsinghua University, Beijing, China; Department of Mechanical Engineering, Tsinghua University, Beijing, China","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","9867","9873","Complex assembly tasks remain huge challenge for robots because the traditional control methods rely on complicated contact state analysis. Reinforcement learning (RL) becomes one of the preferred embodiments to construct the control strategy of complex tasks. In this paper, the method of model-driven RL (MDRL) is employed to construct the control strategy. Then a completely innovative action dimension extension (ADE) mechanism is proposed to further accelerate the training process of RL. The simulation and experiment results demonstrate that the control strategy obtained through combining MDRL and ADE guarantees a more compliant assembly process. Besides, ADE method will enhance the data-efficiency of RL algorithms greatly (about 30%~40%) as well as increase the stable reward.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811792","robot force control;assembly;model-driven reinforcement learning;action dimension;efficient exploration","Training;Industries;Automation;Service robots;Force;Process control;Reinforcement learning","assembling;learning (artificial intelligence);robotic assembly","model-driven RL;control strategy;completely innovative action dimension extension mechanism;compliant assembly process;ADE method;data-efficiency;RL algorithms;model-driven reinforcement learning;action dimension extension method;efficient asymmetric assembly;complex assembly tasks;traditional control methods;complicated contact state analysis;preferred embodiments","","","","23","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Industrial Insert Robotic Assembly Based on Model-based Meta-Reinforcement Learning","D. Liu; X. Zhang; Y. Du; D. Gao; M. Wang; M. Cong","Dalian University of Technology, Dalian, China; Dalian University of Technology, Dalian, China; Dalian Jiaotong University, Dalian, China; Tangshan Polytechnic College, Tangshan, China; Dalian University of Technology, Dalian, China; Dalian University of Technology, Dalian, China","2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)","28 Mar 2022","2021","","","1508","1512","The high-rigidity working environment in the industrial insert robotic assembly is likely to cause damage to the robot and parts, so it is difficult to use traditional position control methods under these circumstances. The combination of compliance control and reinforcement learning brings new ideas to this problem. In this paper, a robotic industrial insertion learning method based on model-based meta-reinforcement learning is proposed. By interacting with the environment and updating the dynamic model in real-time, the online adaptation of assembly tasks is realized. A simulation environment is used to train a neural network dynamics model, with a model-based reinforcement learning method. Finally, we compare our method with other methods in the real environment to verify the effectiveness of the algorithm.","","978-1-6654-0535-5","10.1109/ROBIO54168.2021.9739258","National Natural Science Foundation of China; Technology Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739258","","Robotic assembly;Training;Adaptation models;Service robots;Position control;Reinforcement learning;Robustness","compliance control;compliant mechanisms;neural nets;position control;reinforcement learning;robotic assembly","industrial insert robotic assembly;position control methods;robotic industrial insertion;neural network dynamics model;model-based reinforcement learning method;high-rigidity working environment;compliance control;simulation environment","","1","","27","IEEE","28 Mar 2022","","","IEEE","IEEE Conferences"
"Towards Real-World Force-Sensitive Robotic Assembly through Deep Reinforcement Learning in Simulations","M. Hebecker; J. Lambrecht; M. Schmitz","Chair Industry Grade Networks and Clouds, Technische Universität Berlin, Berlin, Germany; Chair Industry Grade Networks and Clouds, Technische Universität Berlin, Berlin, Germany; BMW Group and the Chair of IT-Management, FAU Erlangen-Nürnberg, Nürnberg, Germany","2021 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)","24 Aug 2021","2021","","","1045","1051","In manufacturing industries, vast potential exists in regards to the adaptability of automated systems through efficient robotic skill acquisition. This paper examines the use of deep reinforcement learning to automate the process of contact rich compliant assembly. Thereby, we consider an exemplary real-world use case in car assembly. To obtain training data, we use a simulated representation of the production system comprising a robotic arm which is controlled through a deep reinforcement learning agent trained with proximal policy optimization. Furthermore, we conduct a basic system analysis to improve the similarity between our physical and simulated environments. After iteratively training and evaluating different models, which distinguish through the reward design and the grade of environment variation, we validate the results on the physical hardware. We successfully obtain agents that generate expedient trajectories, which can generalize to changing environments. Success rates clearly above 90% can be achieved in simulation even with high grades of variation of the target position and the parts’ surface friction. For the transfer to the physical assembly system, we conclude that further optimization is necessary to obtain truly compliant behavior.","2159-6255","978-1-6654-4139-1","10.1109/AIM46487.2021.9517356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9517356","","Training;Robotic assembly;Service robots;Pipelines;Training data;Reinforcement learning;Manipulators","learning (artificial intelligence);optimisation;robotic assembly","robotic arm;deep reinforcement learning agent;proximal policy optimization;basic system analysis;physical environments;simulated environments;environment variation;physical assembly system;automated systems;contact rich compliant assembly;car assembly;training data;force-sensitive robotic assembly;robotic skill acquisition","","3","","31","IEEE","24 Aug 2021","","","IEEE","IEEE Conferences"
"Deep reinforcement learning for solving hybrid flow shop scheduling problem with unrelated parallel machines","J. Luo; B. Wang; S. Yuan; W. Zhang; T. Li","School of Economics and Management, University of Science and Technology, Beijing; School of Economics and Management, University of Science and Technology, Beijing; School of Economics and Management, University of Science and Technology, Beijing; School of Economics and Management, University of Science and Technology, Beijing; School of Economics and Management, University of Science and Technology, Beijing","2023 8th International Conference on Intelligent Computing and Signal Processing (ICSP)","19 Sep 2023","2023","","","1642","1645","For the hybrid flow shop scheduling problem with unrelated parallel machines with dynamically arrived jobs, an online scheduling method is proposed based on Deep-Q-Network (DQN) and scheduling rules to minimize total tardiness. In order to optimize production scheduling and minimize tardiness, we combine classic scheduling rules with the proposed DQN algorithm. The algorithm selects jobs and assigns them to feasible machines based on their completion or arrival time, while seven state features represent the production status at each scheduling point. We validate our approach through numerical experiments on various production configurations, showing significant advantages over traditional scheduling rules.","","979-8-3503-0245-5","10.1109/ICSP58490.2023.10248543","National Natural Science Foundation of China; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10248543","hybrid flow shop;online scheduling;reinforcement learning;unrelated parallel machines;DQN","Job shop scheduling;Processor scheduling;Heuristic algorithms;Signal processing algorithms;Production;Reinforcement learning;Parallel machines","deep learning (artificial intelligence);flow shop scheduling;job shop scheduling;minimisation;parallel machines;production control;reinforcement learning;scheduling","classic scheduling rules;Deep reinforcement;Deep-Q-Network;dynamically arrived jobs;feasible machines;hybrid flow shop scheduling problem;online scheduling method;production scheduling;scheduling point;traditional scheduling rules;unrelated parallel machines","","","","10","IEEE","19 Sep 2023","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Algorithm for Optimal Dynamic Policies of Joint Condition-based Maintenance and Condition-based Production","H. Rasay; F. Azizi; M. Salmani; F. Naderkhani","Kermanshah University of Technology, Kermanshah, Iran; Department of Statistic, Faculty of Mathematical Science, Alzahra University, Tehran, Iran; Concordia Institute for Information System Engineering, Concordia University, Montreal, QC, Canada; Concordia Institute for Information System Engineering, Concordia University, Montreal, QC, Canada","2023 IEEE International Conference on Prognostics and Health Management (ICPHM)","2 Aug 2023","2023","","","134","138","This paper focuses on development of joint optimal maintenance and production policy for a specific type of production system that allows for adjustable production rates. The rate of deterioration of the system is directly related to the production rate, with higher production rates resulting in greater expected deterioration. The system's deterioration can be controlled through two main actions: (1) scheduling and conducting maintenance actions referred to as maintenance policy; and (2) adjusting the production rate referred to as production policy. To determine the optimal actions given the system's state, a Markov decision process (MDP) is developed and a reinforcement learning algorithm, specifically a Q-learning algorithm, is utilized. The algorithm's hyper parameters are tuned using a value-iteration algorithm of dynamic programming. The goal is to minimize expected costs for the system over a finite planning horizon.","2166-5656","979-8-3503-4625-1","10.1109/ICPHM57936.2023.10193968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10193968","condition-based maintenance;condition-based production;reinforcement learning;Markov decision process","Production systems;Schedules;Q-learning;Costs;Heuristic algorithms;Maintenance engineering;Markov processes","dynamic programming;maintenance engineering;Markov processes;production engineering computing;reinforcement learning;scheduling","condition-based maintenance;finite planning horizon;maintenance policy;Markov decision process;MDP;optimal dynamic policies;reinforcement learning algorithm","","","","11","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Object Shape Error Correction using Deep Reinforcement Learning for Multi-Station Assembly Systems","S. Sinha; P. Franciosa; D. Ceglarek","Digital Lifecycle Management WMG, University of Warwick, Coventry, U.K.; Digital Lifecycle Management WMG, University of Warwick, Coventry, U.K.; Digital Lifecycle Management WMG, University of Warwick, Coventry, U.K.","2021 IEEE 19th International Conference on Industrial Informatics (INDIN)","11 Oct 2021","2021","","","1","8","The paper proposes a novel approach, Object Shape Error Correction (OSEC), to determine corrective action in order to mitigate root cause(s) (RCs) of dimensional and geometric product shape errors. It leverages Deep Deterministic Policy Gradient (DDPG) algorithm to learn optimal process parameters update policies based on high dimensional state estimates of multi-station assembly systems (MAS). These policies can be interpreted in engineering terms as sequential corrective adjustments of process parameters that are necessary to mitigate RCs of product shape errors. The approach has the capability to estimate adjustments of process parameters related to fixturing and joining while simultaneously accounting for (i) RC uncertainty estimation, (ii) Key Performance Indicator (KPI) improvement, (iii) MAS design architecture; and, (iv) MAS inherent stochasticity. In addition, the OSEC methodology leverages a reward function parameterized by user interpretable functional coefficients for optimal tradeoff involving various corrections requirements. Benchmarking using an industrial, automotive cross-member assembly system demonstrates a 40% increase in the effectiveness of corrective actions when compared to current approaches.","","978-1-7281-4395-8","10.1109/INDIN45523.2021.9557359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9557359","Deep Reinforcement Learning;Assembly Correction;Manufacturing;Deep Deterministic Policy Gradient (DDPG)","Assembly systems;Uncertainty;Shape;Key performance indicator;Estimation;Reinforcement learning;Production","assembling;automotive components;deep learning (artificial intelligence);design engineering;error correction;optimisation;production engineering computing;state estimation","multistation assembly systems;geometric product shape errors;deep deterministic policy gradient algorithm;optimal process parameters update policies;high dimensional state estimates;sequential corrective adjustments;industrial cross-member assembly system;automotive cross-member assembly system;OSEC methodology;deep reinforcement learning;object shape error correction;DDPG;RC uncertainty estimation;key performance indicator;KPI;user interpretable functional coefficients;MAS design architecture","","1","","29","IEEE","11 Oct 2021","","","IEEE","IEEE Conferences"
"Multi-AGVs dispatching strategy in automobile assembly line based on Deep Reinforcement Learning","G. Wang; X. Wang; L. Wang; M. Shao; Y. Yu; X. Cheng","College of Control Science and Engineering, Shandong University, Jinan, China; College of Control Science and Engineering, Shandong University, Jinan, China; College of Control Science and Engineering, Shandong University, Jinan, China; Haihui New Energy Motor Co., Ltd., Rizhao, China; Haihui New Energy Motor Co., Ltd., Rizhao, China; Haihui New Energy Motor Co., Ltd., Rizhao, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","6382","6386","AGVs (Automated Guided Vehicles) are widely used in automobile assembly lines, optimizing the AGVs dispatching strategy in automobile assembly line is of important academic significance and application value. In actual production process, assembly lines require multiple types of AGVs to work in coordination, which requires a intelligent dispatching strategies to solve this complex dispatching problem and can be applied to different situations. Hence, a deep reinforcement learning method based on DQN algorithm is proposed to optimize the dispatching strategy. Appropriate reward function is designed according to the demand materials of every station, and experience replay mechanism is used to improve the algorithm performance. In addition, this method is applied to AGVs dispatching simulation, and the results are given.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727515","National Natural Science Foundation of China; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727515","multi-AGVs;dispatching;reinforcement learning;DQN","Remotely guided vehicles;Correlation;Reinforcement learning;Production;Markov processes;Approximation algorithms;Dispatching","assembling;automatic guided vehicles;dispatching;learning (artificial intelligence)","multiAGVs dispatching strategy;automobile assembly line;AGVs dispatching strategy;important academic significance;application value;assembly lines;intelligent dispatching strategies;complex dispatching problem;deep reinforcement learning method;AGVs dispatching simulation","","","","14","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Machine Preventive Replacement Policy for Serial Production Lines Based on Reinforcement Learning","J. Huang; Q. Chang; N. Chakraborty","Department of Aerospace and Mechanical Engineering and Department of Engineering Systems and Environment, University of Virginia, Charlottesville, VA, USA; Department of Aerospace and Mechanical Engineering and Department of Engineering Systems and Environment, University of Virginia, Charlottesville, VA, USA; Department of Mechanical Engineering, Stony Brook University, Stony Brook, NY, USA","2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)","19 Sep 2019","2019","","","523","528","In the manufacturing industry, the random failures of machines introduce unexpected disruptions to the production. It is preferable to replace those aged machines with new ones before they fail. In this paper, the machine preventive replacement problem in serial production lines is discussed. To obtain an optimal machine replacement policy, the problem is formulated as a reinforcement learning problem and solved with the Q-learning algorithm. A reward function is proposed based on the production loss evaluation of the serial production lines. The data-driven modeling of serial production lines is used during the training process of the agent. A simulation study is conducted to evaluate the efficiency and effectiveness of the proposed method.","2161-8089","978-1-7281-0356-3","10.1109/COASE.2019.8843338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8843338","","Production;Maintenance engineering;Reinforcement learning;Computational modeling;Aging;Degradation","learning (artificial intelligence);optimisation;preventive maintenance;production control;production engineering computing","serial production lines;machine preventive replacement policy;aged machines;machine preventive replacement problem;optimal machine replacement policy;reinforcement learning problem;production loss evaluation;Q-learning algorithm;reward function;data-driven modeling;training process;manufacturing industry;random failures","","6","","18","IEEE","19 Sep 2019","","","IEEE","IEEE Conferences"
"Rate Adaptation by Reinforcement Learning for Wi-Fi Industrial Networks","G. Peserico; T. Fedullo; A. Morato; S. Vitturi; F. Tramarin","Autec s.r.l., University of Padova, Italy; Dept. of Management and Engineering, University of Padova, Italy; CMZ Sistemi Elettronici s.r.l., University of Padova, Italy; National Research Council of Italy, CNR–IEIIT; Dept. of Engineering “Enzo Ferrari”, University of Modena and Reggio Emilia, Italy","2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)","5 Oct 2020","2020","1","","1139","1142","Wireless technologies play a key role in the Industrial Internet of Things (IIoT) scenario, for the development of increasingly flexible and interconnected factory systems. Wi-Fi remains particularly attracting due to its pervasiveness and high achievable data rates. Furthermore, its Rate Adaptation (RA) capabilities make it suitable to the harsh industrial environments, provided that specifically designed RA algorithms are deployed. To this aim, this paper proposes to exploit Reinforcement Learning (RL) techniques to design an industry-specific RA algorithm. The RL is spreading in many fields since it allows to design intelligent systems by means of a stochastic discrete–time system based approach. In this work we propose to enhance the Robust Rate Adaptation Algorithm (RRAA) by means of a RL approach. The preliminary assessment of the designed RA algorithm is carried out through meaningful OMNeT++ simulations, that allow to recognize the beneficial impact of the introduction of RL with respect to several industry-specific performance indicators.","1946-0759","978-1-7281-8956-7","10.1109/ETFA46521.2020.9212060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9212060","Factory Automation;Wi–Fi;Rate Adaptation;Reinforcement Learning;SARSA","Wireless communication;Adaptation models;Conferences;Reinforcement learning;Production facilities;Intelligent systems;Wireless fidelity","discrete time systems;factory automation;Internet of Things;learning (artificial intelligence);wireless LAN","IIoT;increasingly flexible factory systems;interconnected factory systems;Rate Adaptation capabilities;harsh industrial environments;Reinforcement Learning techniques;industry-specific RA algorithm;intelligent systems;stochastic discrete-time system based approach;Robust Rate Adaptation Algorithm;RL approach;designed RA algorithm;industry-specific performance indicators;Wi-Fi Industrial Networks;wireless technologies;Industrial Internet of Things scenario","","6","","10","IEEE","5 Oct 2020","","","IEEE","IEEE Conferences"
"Industrial General Reinforcement Learning Control Framework System based on Intelligent Edge","K. Kim; Y. -G. Hong","ETRI, Daejeon, Korea; ETRI, Daejeon, Korea","2020 22nd International Conference on Advanced Communication Technology (ICACT)","9 Apr 2020","2020","","","414","418","This paper is about the intelligent edge-based reinforcement learning control framework technology for manufacturing field solution and features large-scale learning, scalable edge distribution technology that can be applied to various task. In this paper, two items are proposed as features. The first proposes the General Reinforcement Learning Framework(GRLF) in manufacturing, and the second proposes the edge solution technology based on the GRLF in manufacturing. We apply the industrial solution such as grid sorter system for example. As a result of the industrial GRLF based grid sorter system, it was confirmed that when a total of 100 deliveries are randomly received into the grid sorter system by any emitter, all shipments are 100% accurate. It also classifies approximately 0.5 deliveries per step. This shows the efficiency of classifying around 30 deliveries per minute, assuming a step is performed per second.","1738-9445","979-11-88428-04-5","10.23919/ICACT48636.2020.9061542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9061542","reinforcement learning;deep learning;intelligent edge;supervised learning","Learning (artificial intelligence);Machine learning;IEC;Industries;Smart manufacturing","control engineering computing;distributed processing;industrial control;learning (artificial intelligence);manufacturing systems;production engineering computing","manufacturing field solution;large-scale learning;scalable edge distribution technology;edge solution technology;industrial solution;industrial GRLF;intelligent edge;industrial general reinforcement learning control framework system","","2","","7","","9 Apr 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning Enabled Autonomous Manufacturing Using Transfer Learning and Probabilistic Reward Modeling","M. F. Alam; M. Shtein; K. Barton; D. Hoelzle","Department of Mechanical and Aerospace Engineering, The Ohio State University, Columbus, OH, USA; Department of Materials Science and Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Mechanical and Aerospace Engineering, The Ohio State University, Columbus, OH, USA","IEEE Control Systems Letters","11 Aug 2022","2023","7","","508","513","Here we propose a reinforcement learning enabled physical autonomous manufacturing system (AMS) that is capable of learning the manufacturing process parameters to autonomously fabricate a complex-geometry artifact with desired performance characteristics. The poor sample efficiency of traditional RL algorithms challenges real-world manufacturing decision making due to a high variable cost from raw material, machine utilization, and labor costs. To make decision making sample efficient, we propose to leverage a first-principles based source task for training, transfer effective representations from trained knowledge, and then use these representations to interact with the physical system to learn a probabilistic model of the target reward function. We deploy this idea to a novel dataset obtained from a custom physical AMS machine that can autonomously manufacture phononic crystals, a complex geometry artifact with spectral response as performance characteristic. We demonstrate that our method uses as low as 25 artifacts to model the interesting part of the target reward function and find an artifact with high reward. This task typically requires manual design of phononic crystals and extensive empirical iterations on the order of hundreds.","2475-1456","","10.1109/LCSYS.2022.3188014","NSF Award CMMI(grant numbers:1727894); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9814884","Reinforcement learning;transfer learning;Gaussian process;autonomous manufacturing","Task analysis;Decision making;Transfer learning;Probabilistic logic;Reinforcement learning;Costs;Gaussian processes","decision making;manufacturing processes;manufacturing systems;probability;production engineering computing;reinforcement learning","transfer learning;probabilistic reward modeling;autonomous manufacturing system;manufacturing process parameters;RL;decision making;first-principles based source task;target reward function;AMS;complex geometry artifact;reinforcement learning","","1","","18","IEEE","4 Jul 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Dynamic Error Compensation in 3D Printing","D. Wang; Z. Shen; X. Dong; Q. Fang; W. Wang; X. Dong; G. Xiong","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Beijing Engineering Research Center of Intelligent Systems and Technology, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","7","In the 3D printing process, various error factors can affect the accuracy of the final printing quality. However, current 3D printing error compensation methods have limited effects and usually cannot work in real-time. The 3D printing error compensation process is modeled as a Markov decision process (MDP) in this paper, and Deep Reinforcement Learning (DRL) is applied for dynamic error compensation. This method learns autonomously through trial and error by interacting with the printing environment, which makes it adaptable to various types of 3D printers without specific training. Then, we simulate the digital light processing (DLP) 3D printing. Due to the huge state and action space of sliced images, applying the DRL algorithm to DLP is challenging. We propose an error compensation method based on morphological image operation and use Autoencoder to extract error features to reduce the state space. We then implement our method using a Twin Delayed Deep Deterministic policy gradient algorithm (TD3). The results demonstrate the effects of our method in compensating 3D printing errors.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260588","National Natural Science Foundation of China(grant numbers:U1909218,U19B2029,92267103,U190920015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260588","","Solid modeling;Three-dimensional displays;Heuristic algorithms;Error compensation;Reinforcement learning;Three-dimensional printing;Feature extraction","deep learning (artificial intelligence);error compensation;feature extraction;Markov processes;production engineering computing;reinforcement learning;three-dimensional printing","3D printing error compensation process;3D printing process;autoencoder;deep reinforcement learning;digital light processing 3D printing;DLP 3D printing;dynamic error compensation;error factors;error feature extraction;Markov decision process;morphological image operation;printing quality;sliced images;twin delayed deep deterministic policy gradient algorithm","","","","34","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Intelligent Scheduling for Permutation Flow Shop with Dynamic Job Arrival via Deep Reinforcement Learning","S. Yang; Z. Xu","Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China","2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","5 Apr 2021","2021","","","2672","2677","The dynamic permutation flow shop scheduling problem (PFSP) is receiving increasing attention in recent years. To provide intelligent scheduling for the dynamic PFSP, we solved the dynamic PFSP with new job arrival using deep reinforcement learning (DRL). The mathematical model is established with the objective of minimizing the total tardiness cost of all jobs arriving at the system. The double deep Q network (DDQN) is adapted to solve the studied problem. A large range of instances is provided to train the DDQN-based scheduling agent. The training curve shows the DDQN-based scheduling agent learned to choose appropriate actions at rescheduling points during the training process. After training, the trained model is saved and used to compare with several well-known dispatching rules on a set of test instances. The comparison results show that our trained scheduling agent performs significantly better than these dispatching rules. Our work can provide intelligent decision-making of scheduling for a flow shop under a dynamic production environment.","2689-6621","978-1-7281-8028-1","10.1109/IAEAC50856.2021.9390893","National Natural Science Foundation of China; Natural Science Foundation of Liaoning Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9390893","dynamic scheduling;deep reinforcement learning;PFSP;deep Q network (DQN);intelligent scheduling;dynamic job arrival","Training;Job shop scheduling;Heuristic algorithms;Reinforcement learning;Dynamic scheduling;Dispatching;Mathematical model","dispatching;flow shop scheduling;job shop scheduling;learning (artificial intelligence);minimisation;production control;scheduling","total tardiness cost;double deep Q network;DDQN-based;training curve;training process;mathematical model;dynamic PFSP;dynamic permutation flow shop scheduling problem;deep reinforcement learning;dynamic job arrival;intelligent scheduling;dynamic production environment;trained scheduling agent;trained model","","5","","51","IEEE","5 Apr 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning-based Job Shop Scheduling for Remanufacturing Production","Y. Bai; Y. Lv","School of Transportation and Logistics Engineering, Wuhan University of Technology, Wuhan, China; School of Transportation and Logistics Engineering, Wuhan University of Technology, Wuhan, China","2022 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)","26 Dec 2022","2022","","","0246","0251","With the development of industry 4.0 and intelligent manufacturing, higher requirements for the manufacturing industry in terms of green, low-carbon and sustainability are highly desired. Hence, maintenance and remanufacturing industry has become a new focus. Different from traditional manufacturing process, remanufacturing factory encountered with incoming ""raw material"" of different quality which required various non-uniform production processes. It brought a big challenge for remanufacturing job shop scheduling on production efficiency. To tackle this problem, this paper deeply analyzes production processes in remanufacturing workshop, and establishes a mathematical model with the minimum total production time as the objective function. Due to the advantages of reinforcement learning (RL) in solving the job shop scheduling problem (JSP), this paper adopts Q-learning and DQN to solve the remanufacturing scheduling problem, where system states are extracted and five common-used heuristic scheduling rules are selected as the action set, and the reward function was designed consistent with the objective function. Comparison study was carried on with heuristic rules alone, genetic algorithm (GA) and RL and the benchmarking results prove the superiority of RL in solving this problem.","","978-1-6654-8687-3","10.1109/IEEM55944.2022.9989643","Ministry of Education; National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9989643","Remanufacturing;job shop scheduling;reinforcement learning;heuristic scheduling rules","Job shop scheduling;Q-learning;Heuristic algorithms;Conferences;Production;Maintenance engineering;Benchmark testing","benchmark testing;carbon;genetic algorithms;job shop scheduling;maintenance engineering;manufacturing processes;mathematical analysis;production engineering computing;raw materials;reinforcement learning","benchmarking;DQN;genetic algorithm;industry 4.0;intelligent manufacturing;job shop scheduling problem;low-carbon;mathematical model;nonuniform production processes;Q-learning;raw material;reinforcement learning-based job shop scheduling;remanufacturing industry;sustainability","","","","17","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"Truss assembly by space robot and task error recovery via reinforcement learning","K. Senda; T. Matsumoto","Graduate School of Engineering, Osaka Prefecture University, Sakai, Osaka, Japan; Graduate School of Engineering, Osaka Prefecture University, Japan","Proceedings. 2000 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2000) (Cat. No.00CH37113)","6 Aug 2002","2000","1","","410","415 vol.1","This paper addresses an experimental system simulating a free-flying space robot, which has been constructed to study autonomous space robots. The experimental system consists of a space robot model, a frictionless table system, a computer system, and a vision sensor system. The robot model composed of two manipulators and a satellite vehicle can move freely on a two-dimensional planar table without friction by using air-bearings. The robot model has successfully performed the automatic truss structure construction including many jobs, e.g., manipulator berthing, component manipulation, arm trajectory control avoiding collision, assembly considering contact with the environment, etc. Moreover, even if the robot fails in a task planned in advance, the robot accomplishes it by task re-planning through reinforcement learning. The experiment demonstrates the possibility of the automatic construction and the usefulness of space robots.","","0-7803-6348-5","10.1109/IROS.2000.894639","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=894639","","Robotic assembly;Orbital robotics;Robot sensing systems;Robotics and automation;Manipulators;Computer errors;Computational modeling;Robot vision systems;Computer vision;Machine vision","aerospace robotics;manipulators;position control;learning (artificial intelligence);collision avoidance","truss assembly;space robot;task error recovery;reinforcement learning;free-flying space robot;autonomous space robots;frictionless table system;vision sensor system;manipulators;satellite vehicle;two-dimensional planar table;robot model;manipulator berthing;component manipulation;arm trajectory control;space robots","","1","","22","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Integrating Robot Assignment and Maintenance Management: A Multi-Agent Reinforcement Learning Approach for Holistic Control","K. Bhatta; Q. Chang","Department of Mechanical and Aerospace Engineering, University of Virginia, Charlottesville, VA, USA; Department of Mechanical and Aerospace Engineering, University of Virginia, Charlottesville, VA, USA","IEEE Robotics and Automation Letters","19 Jul 2023","2023","8","9","5338","5344","Modern manufacturing requires effective integration of production control and maintenance scheduling to improve productivity and quality. However, there have been few studies on this integrated control due to a lack of a comprehensive manufacturing system model. In response to this challenge, this letter presents a mathematical model framework for a mobile multi-skilled robot-operated manufacturing system that integrates three essential control aspects: robot assignment, maintenance scheduling, and product quality. Furthermore, an integrated control scheme is formulated in the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) framework to showcase the proposed method's efficacy in control. Results show that the proposed integrated model outperforms models that consider only system-level parameters, as well as those that only address maintenance scheduling and quality-related parameters.","2377-3766","","10.1109/LRA.2023.3294717","National Science Foundation(grant numbers:1853454); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179969","AI and machine learning in manufacturing and logistics systems;collaborative robots in manufacturing;computer integrated manufacturing;intelligent and flexible manufacturing;reinforcement learning","Robots;Maintenance engineering;Workstations;Job shop scheduling;Production;Quality assessment;Product design","decision theory;maintenance engineering;manufacturing systems;Markov processes;mobile robots;multi-agent systems;product quality;quality control;reinforcement learning","address maintenance scheduling;comprehensive manufacturing system model;Decentralized Partially Observable Markov Decision Process framework;essential control aspects;holistic control;integrated control scheme;integrated model outperforms models;maintenance management;mathematical model framework;modern manufacturing;multiagent reinforcement learning approach;multiskilled robot-operated;product quality;production control;productivity;quality-related parameters;robot assignment;system-level parameters","","","","16","IEEE","12 Jul 2023","","","IEEE","IEEE Journals"
"A Reinforcement Learning Based Large-Scale Refinery Production Scheduling Algorithm","Y. Chen; J. Ding; Q. Chen","State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","15","Refinery production scheduling is a mixed-integer programming problem, which exists the issue of combinational explosion. Thus, solving a large-scale refinery production scheduling problem is time-consuming. This article proposes an approximate solution framework based on reinforcement learning (RL) for large-scale long-time refinery production scheduling problems to rapidly obtain a satisfactory solution. In the proposed algorithm, the Proximal Policy Optimization algorithm is used to process the continuous action. To address the cold start issue of RL in refinery scheduling problem, we present an initialization method for the actor of agent, which utilizes the operation knowledge of tractable small-scale problems to initialize the actor network, and the agent is trained in the environment of large-scale problems. Hence, the convergence of the RL algorithm is greatly accelerated. In addition, the product flowrate concept is used to express the state, making the scheduling agent scalable in terms of scheduling horizon. Experimental studies show, to large-scale refinery scheduling problems, the proposed algorithm can obtain better solutions than that of the CPLEX solver and the existing evolutionary algorithm in a much shorter solving time of the two methods. Note to Practitioners—Scheduling is a link between planning and execution, and it can bring huge economic benefits to the refinery enterprises. With the enlargement of scheduling horizon, the scale of scheduling problems increases dramatically. How to deal with this large-scale scheduling problem caused by a long scheduling horizon is a significant problem. In this paper, the proposed method learns a decision-maker by reinforcement learning and applies to large-scale problems to obtain a good solution quickly. The proposed method is essentially a heuristic algorithm, and it is easy to implement in practice. At present, more and more things will be integrated into one model, leading to the traditional solver cannot meet the application needs. The fast solution method is necessary to be used to solve this problem in the new era.","1558-3783","","10.1109/TASE.2023.3321612","National Natural Science Foundation of China(grant numbers:61988101,62203101); Liaoning Province Central Leading Local Science and Technology Development Special Project(grant numbers:2022JH6/100100055); China Postdoctoral Science Foundation(grant numbers:2023T160086); Scientific Research Foundation of the Fujian University of Technology(grant numbers:GY-Z220295,GY-Z22071); Third Batch of Innovative Star Talent Project in Fujian Province(grant numbers:GY-Z23068); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10285719","Large-scale optimization;reinforcement learning;refinery;scheduling","Production;Job shop scheduling;Mathematical models;Oils;Reinforcement learning;Optimization;Petroleum","","","","","","","IEEE","13 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Reinforcement Learning based on Stochastic Dynamic Programming for Condition-based Maintenance of Deteriorating Production Processes","H. Rasay; F. Naderkhani; A. M. Golmohammadi","Department of Industrial Engineering, Kermanshah University of Technology, Kermanshah, Iran; Concordia University, Canada; Arak University, Arak, Iran","2022 IEEE International Conference on Prognostics and Health Management (ICPHM)","7 Jul 2022","2022","","","17","24","In this paper, a stochastic dynamic programming model is developed for maintenance planning on a deteriorating multistate production system. The quality of the bath/lot of items produced in each stage is employed as a condition monitoring for condition-based maintenance. The machine has m-1 operational states plus a non-operational state referred as the failure state. At the start of each stage, four actions are available for the management: (1) renew the system; (2) implement maintenance; (3) continue the production, and (4) inspect the system. It is assumed that the impact of the maintenance is imperfect which means after the maintenance, the system is restored to any non-worse states with known probabilities. As the system states change Markovianlly at the end of each stage, and the quality of the items produced depends on the system state, the system can be modeled based on a Markov decision process (MDP). As the MDP is the core of reinforcement learning, for the large-scale problem, it is discussed that the proposed stochastic dynamic programming can be employed to develop reinforcement learning algorithms. To this end, Q-learning algorithm is proposed.","","978-1-6654-6615-8","10.1109/ICPHM53196.2022.9815668","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815668","Maintenance;Markov decision process;Dynamic programming;reinforcement learning","Condition monitoring;Production systems;Q-learning;Sensitivity analysis;Heuristic algorithms;Maintenance engineering;Markov processes","condition monitoring;dynamic programming;maintenance engineering;Markov processes;probability;production engineering computing;reinforcement learning;stochastic programming","condition-based maintenance;production processes;stochastic dynamic programming;maintenance planning;deteriorating multistate production system;condition monitoring;m-1 operational states;failure state;reinforcement learning algorithms;Markov decision process;Q-learning","","4","","15","IEEE","7 Jul 2022","","","IEEE","IEEE Conferences"
"Self-Adaptive Traffic Control Model With Behavior Trees and Reinforcement Learning for AGV in Industry 4.0","H. Hu; X. Jia; K. Liu; B. Sun","Department of Mechanical Engineering and Automation, School of Mechanical Engineering, Northwestern Polytechnical University, Xi'an, China; Department of Mechanical Engineering and Automation, School of Mechanical Engineering, Northwestern Polytechnical University, Xi'an, China; Department of Mechanical Engineering and Automation, School of Mechanical Engineering, Northwestern Polytechnical University, Xi'an, China; Department of Mechanical Engineering and Automation, School of Mechanical Engineering, Northwestern Polytechnical University, Xi'an, China","IEEE Transactions on Industrial Informatics","26 Aug 2021","2021","17","12","7968","7979","Automated guided vehicles (AGVs) are considered as an enabling technology to realize smart manufacturing in the upcoming Industrial 4.0 era. However, several challenges including efficiency, timeliness, and safety still exist in AGVs system in discrete manufacturing shopfloor. To address these challenges, a self-adaptive traffic control model combining behavior trees (BTs) and reinforcement learning (RL) is proposed to implement optimal decisions according to diverse, dynamic and complex situations in Industry 4.0 environments. A cyber-physical systems using multiagent system technology is designed in which components such as AGVs and traffic commander are defined as specific agent that cooperates autonomously with each other. Then, the behavior construction model is constructed by BTs to enumerate all the possible states in AGVs traffic control. An RL model is further developed based on the BTs. By using this approach, in this article, AGVs have the ability to adaptively choose the optimal rule-based strategy from existing optional strategies. The case study of the scenario avoiding collisions at intersections illustrates that the proposed model can enhance self-adaptive capability of AGVs traffic control and simultaneously guarantees efficiency, timeliness, and safety.","1941-0050","","10.1109/TII.2021.3059676","National Natural Science Foundation of China(grant numbers:52075452); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355019","Automated guided vehicle (AGV);behavior trees (BTs);Industrial 4.0;reinforcement learning (RL);self-adaptive control","Traffic control;Industries;Production;Adaptation models;Decision making;Task analysis;Robots","adaptive control;automatic guided vehicles;collision avoidance;control engineering computing;cyber-physical systems;decision making;learning (artificial intelligence);multi-agent systems;production engineering computing;road traffic control;security of data;trees (mathematics)","self-adaptive traffic control model;behavior trees;reinforcement learning;automated guided vehicles;smart manufacturing;Industrial 4.0 era;discrete manufacturing shopfloor;BTs;cyber-physical systems;multiagent system technology;traffic commander;behavior construction model;AGVs traffic control;RL model;optimal rule-based strategy","","26","","40","IEEE","16 Feb 2021","","","IEEE","IEEE Journals"
"Towards Optimal Assembly Line Order Sequencing with Reinforcement Learning: A Case Study","S. Shafiq; C. Mayr-Dorn; A. Mashkoor; A. Egyed","Johannes Kepler University, Linz, Austria; Johannes Kepler University, Linz, Austria; SCCH GmbH & Johannes Kepler University, Linz, Austria; Johannes Kepler University, Linz, Austria","2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)","5 Oct 2020","2020","1","","982","989","The new era of Industry 4.0 is leading towards self-learning and adaptable production systems requiring efficient and intelligent decision making. Achieving high production rate in a short span of time, continuous improvement, and better utilization of resources is crucial for such systems. This paper discusses an approach to achieve production optimization by finding optimal sequences of orders, which yield high throughput using reinforcement learning. The feasibility of our approach is evaluated by simulating a plant modelled on a higher level of abstraction taken from a real assembly line. The applicability of the proposed approach is demonstrated in the form of code utilizing the simulation model. The obtained results show promising accuracy of sequences against corresponding throughput during the simulation process.","1946-0759","978-1-7281-8956-7","10.1109/ETFA46521.2020.9211982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211982","Order sequencing;Optimization;Reinforcement learning","Productivity;Sequential analysis;Production systems;Reinforcement learning;Switches;Predictive models;Throughput","assembling;decision making;learning (artificial intelligence);optimisation;production engineering computing","Industry 4.0;optimal assembly line order sequencing;simulation model;production optimization;continuous improvement;high production rate;intelligent decision making;adaptable production systems;reinforcement learning","","2","","22","IEEE","5 Oct 2020","","","IEEE","IEEE Conferences"
"Research on Attention-Based Predictive Model in Multi-Agent Reinforcement Learning","W. Hong; H. Wu; Z. Shen; G. Xiong; B. Hu; X. Dong; F. -Y. Wang","The State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The Beijing Engineering Research Center of Intelligent Systems and Technology, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The Beijing Engineering Research Center of Intelligent Systems and Technology, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","3789","3794","Agent actions planning is a challenging problem in multi-agent reinforcement learning. Recent methods typically build their predictive models by full connection layers, but the shortage of utilizing action and observation information in complex multi-agent environment limits the performance of those methods. To address this issue, we propose a novel predictive model based on self-attention mechanism which can effectively use information in the environment. Specifically, we use the MADDPG algorithm to make a dataset where the input is the observations and actions and the output is the predicted observations and rewards. We also introduce the multi-head attention mechanism into the predictive model so that the predictive model can better combine different information for prediction. We perform extensive experiments to assess the proposed method and compare it with the representative baseline approach. Results demonstrate that our method outperforms competitors with higher accuracy in different tasks. Our implementation will be publicly available.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055085","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055085","predictive model;multi-agent;reinforcement learning;multi-head attention model","Automation;Reinforcement learning;Predictive models;Prediction algorithms;Planning;Task analysis","learning (artificial intelligence);multi-agent systems;reinforcement learning","agent actions planning;attention-based predictive model;complex multiagent environment;multiagent reinforcement learning;multihead attention mechanism;observation information;predicted observations;self-attention mechanism","","","","13","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Meta Reinforcement Learning for Robust and Adaptable Robotic Assembly Tasks","A. Hafez; M. I. Awad; W. Gomaa; S. A. Maged","Mechatronics Engineering Department, Ain Shams University, Cairo, Egypt; Mechatronics Engineering Department, Ain Shams University, Cairo, Egypt; Cyber Physical Systems Lab., Egypt Japan University of Science and Technology, Alexandria, Egypt; Mechatronics Engineering Department, Ain Shams University, Cairo, Egypt","2021 16th International Conference on Computer Engineering and Systems (ICCES)","28 Jan 2022","2021","","","1","7","Robots have achieved great success in assembly tasks in known and structured environments. However, assembly tasks in unknown and unstructured environments are still an open problem due to the limited adaptability and robustness of conventional methods and algorithms. Reinforcement learning represents a potential solution to this problem as it can learn any task through interaction with the environment without the need for prior knowledge or information, that's why it can be adaptable to new tasks. However, reinforcement learning requires a lot of interaction data with the environment to properly learn a given task, and changes in the task may require learning again from scratch, that's why it is sample inefficient. Meta reinforcement learning can be both adaptable and data efficient during test time when facing a new task, as it learns a fast adaptation procedure using prior knowledge gained through training on various tasks. In this paper, we show how meta reinforcement learning can be used to successfully preform peg-in-hole assembly tasks in unknown environments, with high uncertainty in the hole position and poor performance sensors, within small number of interaction trajectories with the environment.","","978-1-6654-0867-7","10.1109/ICCES54031.2021.9686128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9686128","peg-in-hole;robotic assembly;reinforcement learning;meta reinforcement learning","Robotic assembly;Training;Uncertainty;Preforms;Reinforcement learning;Robot sensing systems;Robustness","control engineering computing;production engineering computing;reinforcement learning;robotic assembly","fast adaptation procedure;meta reinforcement learning;peg-in-hole assembly tasks;adaptable robotic assembly tasks;known structured environments;unknown environments;unstructured environments;adaptable data;interaction trajectories","","","","28","IEEE","28 Jan 2022","","","IEEE","IEEE Conferences"
"Continuity and Smoothness Analysis and Possible Improvement of Traditional Reinforcement Learning Methods","T. Chen; W. Jia; J. Yuan; S. Ma; L. Cheng","School of Mechatronic Engineering and Automation, Shanghai Key Laboratory of Intelligent Manufacturing and Robotics Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai Key Laboratory of Intelligent Manufacturing and Robotics Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai Key Laboratory of Intelligent Manufacturing and Robotics Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai Key Laboratory of Intelligent Manufacturing and Robotics Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai Key Laboratory of Intelligent Manufacturing and Robotics Shanghai University, Shanghai, China","2020 IEEE International Conference on Mechatronics and Automation (ICMA)","26 Oct 2020","2020","","","1722","1727","At present, the deep reinforcement learning method has become one of the important branches in the field of artificial intelligence. Its model-free feature makes it considered as one of the ways to achieve the goal of strong artificial intelligence. In addition to discrete decision-making tasks, deep reinforcement learning has been gradually applied to continuous control tasks. Nevertheless, compared with classical control strategies and methods, the instability of deep reinforcement learning limits its extensive application in real scenarios. The instability of reinforcement learning mainly comes from two aspects: the first is the inherent discontinuity of reinforcement learning action strategies; the second is the randomness of reinforcement learning action strategies. This paper will discuss theoretical reasons of this instability and evaluate the instability of the current mainstream reinforcement learning algorithms by time-frequency analysis. Finally, we give an improved framework based on stochastic differential equation, and theoretically solve the inherent discontinuity of reinforcement learning action strategy.","2152-744X","978-1-7281-6416-8","10.1109/ICMA49215.2020.9233547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9233547","Reinforcement Learning;Learning and Adaptive Systems;AI-Based Methods;Continuity","Time-frequency analysis;Mechatronics;Service robots;Time series analysis;Stochastic processes;Reinforcement learning;Differential equations","differential equations;learning (artificial intelligence);stochastic processes;time-frequency analysis","deep reinforcement learning method;model-free feature;artificial intelligence;decision-making tasks;continuous control tasks;action strategies;stochastic differential equation;time-frequency analysis","","","","14","IEEE","26 Oct 2020","","","IEEE","IEEE Conferences"
"A Practical Deep Reinforcement Learning Approach to Semiconductor Equipment Scheduling","C. Lee; S. Lee","Mechatronics R&D Center, Samsung Electronics, Hwaseong, Republic of Korea; Department of Solution PE, Memory Division Samsung Electronics, Hwaseong, Republic of Korea","2021 22nd IEEE International Conference on Industrial Technology (ICIT)","18 Jun 2021","2021","1","","979","985","The efficiency of utilizing semiconductor equipment is critical to maximizing profits. The design work of a semiconductor equipment scheduler becomes a difficult task because it requires efficient operation in various situations. In this paper, we propose an approach based on deep reinforcement learning to overcome the difficulties of scheduling. This new approach designs a scheduler that controls the wafer transport robot inside the equipment. A deep neural network applied with a Q-network is used to calculate the benefit of the robot's motion under various conditions. The experimental results show the feasibility of applying deep reinforcement learning to the equipment scheduler. It also shows that pre-trained models can increase productivity by further learning in a variety of production environments.","","978-1-7281-5730-6","10.1109/ICIT46573.2021.9453533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9453533","semiconductor manufacturing;deep reinforcement learning;deep learning;Q-learning","Semiconductor device modeling;Training;Productivity;Deep learning;Job shop scheduling;Conferences;Neural networks","control engineering computing;deep learning (artificial intelligence);integrated circuit manufacture;mobile robots;neural nets;production engineering computing;production equipment;semiconductor device manufacture","practical deep reinforcement learning;semiconductor equipment scheduling;design work;semiconductor equipment scheduler;deep neural network;Q-network;production environments;wafer transport robot","","2","","14","IEEE","18 Jun 2021","","","IEEE","IEEE Conferences"
"Inverse Reinforcement Learning Framework for Transferring Task Sequencing Policies from Humans to Robots in Manufacturing Applications","O. M. Manyar; Z. McNulty; S. Nikolaidis; S. K. Gupta","Realization of Robotic Systems Lab, University of Southern California, Los Angeles, CA, USA; Realization of Robotic Systems Lab, University of Southern California, Los Angeles, CA, USA; Department of Computer Science, University of Southern California, Los Angeles, CA, USA; Realization of Robotic Systems Lab, University of Southern California, Los Angeles, CA, USA","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","849","856","In this work, we present an inverse reinforcement learning approach for solving the problem of task sequencing for robots in complex manufacturing processes. Our proposed framework is adaptable to variations in process and can perform sequencing for entirely new parts. We prescribe an approach to capture feature interactions in a demonstration dataset based on a metric that computes feature interaction coverage. We then actively learn the expert's policy by keeping the expert in the loop. Our training and testing results reveal that our model can successfully learn the expert's policy. We demonstrate the performance of our method on a real-world manufacturing application where we transfer the policy for task sequencing to a manipulator. Our experiments show that the robot can perform these tasks to produce human-competitive performance. Code and video can be found at: https://sites.google.com/usc.edu/irlfortasksequencing","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160687","National Science Foundation(grant numbers:1925084); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160687","","Measurement;Training;Sequential analysis;Manufacturing processes;Reinforcement learning;Robustness;Manufacturing","human computer interaction;industrial manipulators;manufacturing processes;production engineering computing;reinforcement learning","computes feature interaction;expert policy;human-competitive performance;humans to robots;inverse reinforcement learning approach;inverse reinforcement learning framework;manufacturing processes;real-world manufacturing;transferring task sequencing policies","","","","35","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Invited- NVCell: Standard Cell Layout in Advanced Technology Nodes with Reinforcement Learning","H. Ren; M. Fojtik","NVIDIA, Austin, TX, USA; NVIDIA, Durham, NC, USA","2021 58th ACM/IEEE Design Automation Conference (DAC)","8 Nov 2021","2021","","","1291","1294","High quality standard cell layout automation in advanced technology nodes is still challenging in the industry today because of complex design rules. In this paper we introduce an automatic standard cell layout generator called NVCell that can generate layouts with equal or smaller area for over 90% of single row cells in an industry standard cell library on an advanced technology node. NVCell leverages reinforcement learning (RL) to fix design rule violations during routing and to generate efficient placements.","0738-100X","978-1-6654-3274-0","10.1109/DAC18074.2021.9586188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586188","","Industries;Design automation;Layout;Reinforcement learning;Simulated annealing;Routing;Libraries","integrated circuit design;integrated circuit layout;learning (artificial intelligence)","single row cells;industry standard cell library;advanced technology node;reinforcement learning;high quality standard cell layout automation;complex design rules;automatic standard cell layout generator;rule violations","","3","","21","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"A Deep-Reinforcement-Learning-Based Optimization Approach for Real-Time Scheduling in Cloud Manufacturing","H. Zhu; M. Li; Y. Tang; Y. Sun","School of Automation, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Telecommunications and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Access","16 Jan 2020","2020","8","","9987","9997","Resource scheduling problems (RSPs) in cloud manufacturing (CMfg) often manifest as dynamic scheduling problems in which scheduling strategies depend on real-time environments and demands. Generally, multiple resources in the CMfg scheduling process cause difficulties in system modeling. To solve this problem, we propose Sharer, a deep reinforcement learning (DRL)-based method that converts scheduling problems with multiple resources into one learning target and learns effective strategies automatically. Our preliminary results show that Sharer is comparable to the latest heuristics, adapts to different conditions, converges quickly, and subsequently learns wise strategies.","2169-3536","","10.1109/ACCESS.2020.2964955","National Natural Science Foundation of China(grant numbers:61772286,61802208); Natural Science Fund for Colleges and Universities in Jiangsu Province(grant numbers:18KJB520036); China Postdoctoral Science Foundation(grant numbers:2019M651923); Natural Science Foundation of Jiangsu Province(grant numbers:BK20191381); Key Research and Development Program of Jiangsu(grant numbers:BE2019742); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952684","Cloud manufacturing;deep reinforcement learning;real-time;dynamic data;resource scheduling;optimization","Job shop scheduling;Production facilities;Manufacturing;Task analysis;Cloud computing;Optimization;Real-time systems","cloud computing;dynamic scheduling;learning (artificial intelligence);manufacturing systems;optimisation;production engineering computing","dynamic scheduling problems;resource scheduling problems;cloud manufacturing;deep-reinforcement-learning-based optimization approach;system modeling;real-time environments","","20","","34","CCBY","8 Jan 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning using Cyclical Learning Rates","R. Gulde; M. Tuscher; A. Csiszar; O. Riedel; A. Verl","Institute of Control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, 70174 Stuttgart, Germany; Institute of Control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, 70174 Stuttgart, Germany; Institute of Control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, 70174 Stuttgart, Germany; Institute of Control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, 70174 Stuttgart, Germany; Institute of Control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, 70174 Stuttgart, Germany","2020 Third International Conference on Artificial Intelligence for Industries (AI4I)","13 Nov 2020","2020","","","32","35","Deep Reinforcement Learning (DRL) methods often rely on the meticulous tuning of hyperparameters to successfully resolve problems. One of the most influential parameters in optimization procedures based on stochastic gradient descent (SGD) is the learning rate. We investigate cyclical learning and propose a method for defining a general cyclical learning rate for various DRL problems. In this paper we present a method for cyclical learning applied to complex DRL problems. Our experiments show that, utilizing cyclical learning achieves similar or even better results than highly tuned fixed learning rates. This paper presents the first application of cyclical learning rates in DRL settings and is a step towards overcoming manual hyperparameter tuning.","","978-1-7281-8701-3","10.1109/AI4I49448.2020.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9253119","Deep Reinforcement Learning, Cyclic Learning Rates, OpenAI, Gym, Cyclic, Learning","Training;Industries;Reinforcement learning;Manuals;Artificial intelligence;Tuning;Optimization","gradient methods;learning (artificial intelligence);neural nets;stochastic processes","deep reinforcement;general cyclical learning rate;complex DRL problems;fixed learning rates;stochastic gradient descent;SGD;hyperparameter tuning","","","","14","IEEE","13 Nov 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Secondary Energy Scheduling in Steel Industry","T. Zhang; F. Zhou; J. Zhao; W. Wang","School of Control Science and Engineering, Dalian University of Technology, Dalian, China; School of Control Science and Engineering, Dalian University of Technology, Dalian, China; School of Control Science and Engineering, Dalian University of Technology, Dalian, China; School of Control Science and Engineering, Dalian University of Technology, Dalian, China","2020 2nd International Conference on Industrial Artificial Intelligence (IAI)","30 Nov 2020","2020","","","1","5","Considering that the blast furnace gas(BFG) tank level scheduling is of great significance for the steel plant's secondary energy system balance, this paper proposed a scheduling model based on deep reinforcement learning. In this model, BFG gas tank scheduling was transformed into searching the best production state under a certain operating condition, and a deep Q-learning network was used to search this state. Moreover, in order to speed up convergence and improve algorithm stability, an experience based pre-training was added to the training session. In order to verify the effectiveness of the proposed method, experiments are carried out with the secondary energy system production data of a domestic steel enterprise. The results show that the proposed method is more effective than artificial scheduling.","","978-1-7281-8216-2","10.1109/IAI50351.2020.9262196","National Key R&D Program of China(grant numbers:2017YFA0700300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9262196","deep reinforcement learning;Secondary energy system;Gas tank scheduling","Job shop scheduling;Optimization;Training;Predictive models;Reinforcement learning;Steel;Intelligent control","blast furnaces;learning (artificial intelligence);production engineering computing;scheduling;steel industry;tanks (containers)","deep Q-learning network;deep reinforcement learning;steel industry;secondary energy scheduling;blast furnace gas tank level scheduling;BFG","","1","","8","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Mastering the Working Sequence in Human-Robot Collaborative Assembly Based on Reinforcement Learning","T. Yu; J. Huang; Q. Chang","Department of Mechanical and Aerospace Engineering, University of Virginia, Charlottesville, USA; Department of Mechanical and Aerospace Engineering, University of Virginia, Charlottesville, USA; Department of Mechanical and Aerospace Engineering, University of Virginia, Charlottesville, USA","IEEE Access","16 Sep 2020","2020","8","","163868","163877","A long-standing goal of the Human-Robot Collaboration (HRC) in manufacturing systems is to increase the collaborative working efficiency. In line with the trend of Industry 4.0 to build up the smart manufacturing system, the collaborative robot in the HRC system deserves better designing to be more self-organized and to find the superhuman proficiency by self-learning. Inspired by the impressive machine learning algorithms developed by Google Deep Mind like Alphago Zero, in this paper, the human-robot collaborative assembly working process is formatted into a chessboard and the selection of moves in the chessboard is used to analogize the decision-making by both human and robot in the HRC assembly working process. To obtain the optimal policy of the working sequence to maximize the working efficiency, agents in the system are trained with a self-play algorithm based on reinforcement learning, without guidance or domain knowledge beyond game rules. A convolution neural network (CNN) is also trained to predict the distribution of the priority of move selections and whether a working sequence is the one resulting in the maximum of the HRC efficiency. A height-adjustable standing desk assembly is used to demonstrate the proposed HRC assembly algorithm and its efficiency in real-time task planning.","2169-3536","","10.1109/ACCESS.2020.3021904","U.S. National Science Foundation(grant numbers:1351160,1853454); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186588","Human-Robot collaboration;real-time task planning;reinforcement learning;convolution neural network","Task analysis;Robotic assembly;Job shop scheduling;Collaboration;Optimal scheduling;Service robots","assembling;control engineering computing;human-robot interaction;learning (artificial intelligence);neural nets;robotic assembly","reinforcement learning;smart manufacturing system;human-robot collaborative assembly working process;HRC assembly working process;industry 4.0;machine learning algorithm;google deep mind;Alphago zero;decision-making;self-play algorithm;convolution neural network;real-time task planning","","24","","30","CCBY","4 Sep 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning for Robotic Assembly Using Non-Diagonal Stiffness Matrix","M. Oikawa; T. Kusakabe; K. Kutsuzawa; S. Sakaino; T. Tsuji","Department Electrical and Electronic Systems, Saitama University, Saitama, Japan; Department Electrical and Electronic Systems, Saitama University, Saitama, Japan; Graduate School of Engineering, Department of Robotics, Tohoku University, Sendai, Japan; Graduate School of Systems and Information Engineering, University of Tsukuba, Ibaraki, Japan; Department Electrical and Electronic Systems, Saitama University, Saitama, Japan","IEEE Robotics and Automation Letters","18 Mar 2021","2021","6","2","2737","2744","Contact-rich tasks, wherein multiple contact transitions occur in a series of operations, have been extensively studied for task automation. Precision assembly, a typical example of contact-rich tasks, requires high time constants to cope with the change in contact state. Therefore, this letter proposes a local trajectory planning method for precision assembly with high time constants. Because the non-diagonal component of a stiffness matrix can induce motion at high sampling frequencies, we use this concept to design a stiffness matrix to guide the motion of an object and propose a method to control it. We introduce reinforcement learning (RL) for the selection of the stiffness matrix because the relationship between the desired direction and the sensor response is difficult to model. An architecture with various sampling rates for RL and admittance control has the advantage of rapid response owing to a high time constant of the local trajectory modification. The effectiveness of the method is verified experimentally on two contact-rich tasks: inserting a peg into a hole and inserting a gear. Using the proposed method, the average total time needed to insert the peg in the hole is 1.64 s, which is less than half the time reported by the best of the existing state of the art studies.","2377-3766","","10.1109/LRA.2021.3060389","Saitama Leading Edge Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361338","Compliance and impedance control;compliant assembly;force and tactile sensing;reinforcement learning","Task analysis;Robots;Admittance;Trajectory;Force;Gears;Robot sensing systems","elasticity;learning (artificial intelligence);matrix algebra;mobile robots;path planning;robotic assembly;trajectory control","high sampling frequencies;nondiagonal component;local trajectory planning method;contact state;high time constants;precision assembly;task automation;multiple contact transitions;contact-rich tasks;nondiagonal stiffness matrix;robotic assembly;reinforcement learning","","20","","33","IEEE","23 Feb 2021","","","IEEE","IEEE Journals"
"A Digital Twin-Based Production-Maintenance Joint Scheduling Framework with Reinforcement Learning","Q. Hao; Y. Lv","Sourthern Marine Science and Engineering Guangdong Labortory (Zhuhai), School of Transportation and Logistics Engineering, Wuhan University of Technology, Wuhan, China; School of Transportation and Logistics Engineering, Wuhan University of Technology, Wuhan, China","2023 8th International Conference on Control and Robotics Engineering (ICCRE)","26 Jun 2023","2023","","","51","56","The bridge of job scheduling and production equipment maintenance is usually the main joint scheduling task of a production system. However, the predicament of data acquisition in real systems leads to the difficulty of verifying the effectiveness of scheduling algorithms. In order to make joint scheduling work easier to implement in real production systems, this paper presents a joint scheduling framework for production systems based on digital twin and reinforcement learning. Firstly, the virtual mapping of physical production system, namely digital twin system, is established by using AnyLogic software and multi-agent modeling technology. Then, a joint scheduling agent is trained by Deep Q Network (DQN) algorithm and the virtual data generated by the twinning system. And the experimental results demonstrate the effectiveness of proposed framework in production systems with uncertainties, and it has higher production efficiency and lower machine failure frequency compared with a scheduling scheme based on common-used heuristic rules.","2835-3722","979-8-3503-4565-0","10.1109/ICCRE57112.2023.10155592","National Natural Science Foundation of China(grant numbers:72101194); National Key R&D Program of China(grant numbers:2022YFE0125200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155592","production system;joint scheduling;digital twin;reinforcement learning","Production systems;Uncertainty;Scheduling algorithms;Software algorithms;Reinforcement learning;Software;Scheduling","data acquisition;deep learning (artificial intelligence);digital twins;failure analysis;job shop scheduling;maintenance engineering;multi-agent systems;production engineering computing;production equipment;reinforcement learning","AnyLogic software;data acquisition;deep Q network algorithm;digital twin-based production-maintenance joint scheduling framework;DQN;heuristic rules;job scheduling;machine failure frequency;multiagent modeling technology;physical production system;production equipment maintenance;reinforcement learning;twinning system","","","","11","IEEE","26 Jun 2023","","","IEEE","IEEE Conferences"
"Collaborative clustering parallel reinforcement learning for edge-cloud digital twins manufacturing system","F. Yang; T. Feng; F. Xu; H. Jiang; C. Zhao","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China; China Tendering Center for Mechanical and Electrical Equipment, Ministry of Industry and Information Technology, Beijing 100142, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China","China Communications","18 Aug 2022","2022","19","8","138","148","To realize high-accuracy physical-cyber digital twin (DT) mapping in a manufacturing system, a huge amount of data need to be collected and analyzed in real-time. Traditional DTs systems are deployed in cloud or edge servers independently, whilst it is hard to apply in real production systems due to the high interaction or execution delay. This results in a low consistency in the temporal dimension of the physical-cyber model. In this work, we propose a novel efficient edge-cloud DT manufacturing system, which is inspired by resource scheduling technology. Specifically, an edge-cloud collaborative DTs system deployment architecture is first constructed. Then, deterministic and uncertainty optimization adaptive strategies are presented to choose a more powerful server for running DT-based applications. We model the adaptive optimization problems as dynamic programming problems and propose a novel collaborative clustering parallel Q-learning (CCPQL) algorithm and prediction-based CCPQL to solve the problems. The proposed approach reduces the total delay with a higher convergence rate. Numerical simulation results are provided to validate the approach, which would have great potential in dynamic and complex industrial internet environments.","1673-5447","","10.23919/JCC.2022.08.011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9861230","edge-cloud collaboration;digital twins;job shop scheduling;parallel reinforcement learning","Servers;Delays;Cloud computing;Production;Collaboration;Real-time systems;Optimization","cloud computing;convergence of numerical methods;dynamic programming;groupware;manufacturing systems;parallel processing;pattern clustering;production engineering computing;reinforcement learning;resource allocation;scheduling;software architecture","edge-cloud collaborative DTs system deployment architecture;adaptive optimization;high-accuracy physical-cyber digital twin mapping;production systems;resource scheduling;edge-cloud DT manufacturing system;collaborative clustering parallel reinforcement learning;edge-cloud digital twin manufacturing system;edge servers;cloud servers;uncertainty optimization adaptive strategies;deterministic optimization adaptive strategies;dynamic programming;collaborative clustering parallel Q-learning;prediction-based CCPQL;convergence rate;numerical simulation;industrial Internet environment","","1","","","","17 Aug 2022","","","IEEE","IEEE Magazines"
"Deep Reinforcement Learning for Prefab Assembly Planning in Robot-based Prefabricated Construction","A. Zhu; G. Xu; P. Pauwels; B. de Vries; M. Fang","Department of the Built Environment, Information Systems in the Built Environment, Eindhoven University of Technology, Eindhoven, The Netherlands; School of Architecture, Harbin Institute of Technology, Shenzhen (HITsz), Shenzhen, China; Department of the Built Environment, Information Systems in the Built Environment, Eindhoven University of Technology, Eindhoven, The Netherlands; Department of the Built Environment, Information Systems in the Built Environment, Eindhoven University of Technology, Eindhoven, The Netherlands; Department of Mathematics and Computer Science, Eindhoven University of Technology, The Netherlands","2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)","5 Oct 2021","2021","","","1282","1288","Smart construction has raised higher automation requirements of construction processes. The traditional construction planning does not match the demands of integrating smart construction with other technologies such as robotics, building information modelling (BIM), and internet of things (IoT). Therefore, more precise and meticulous construction planning is necessary. In this paper, leveraging recent advances in deep Reinforcement Learning (DRL), we design simulated construction environments for deep reinforcement learning and integrate these environments with deep Q-learning methods. We develop reliable controllers for assembly planning for prefabricated construction. For this, we first show that hand-designed rewards work well for these tasks; then we show deep neural policies can achieve good performance for some simple tasks.","2161-8089","978-1-6654-1873-7","10.1109/CASE49439.2021.9551402","China Scholarship Council(grant numbers:202007720036); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551402","","Prefabricated construction;Automation;Computer aided software engineering;Conferences;Reinforcement learning;Planning;Internet of Things","assembly planning;building information modelling;construction industry;control engineering computing;deep learning (artificial intelligence);design engineering;industrial robots;Internet of Things;prefabricated construction","design simulated construction environments;DRL;IoT;internet of things;BIM;building information modelling;smart construction planning;automation requirements;robot-based prefabricated construction;prefab assembly planning;deep reinforcement learning;deep neural policies;deep Q-learning methods","","3","","23","IEEE","5 Oct 2021","","","IEEE","IEEE Conferences"
"Fusing Vision and Force: A Framework of Reinforcement Learning for Elastic Peg-in-Hole Assembly","R. Dang; Z. Hou; W. Yang; R. Chen; J. Xu","Department of Mechanical Engineering, State key laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra- Precision Manufacturing Equipment Control, Tsinghua University, Beijing, China; Department of Mechanical Engineering, State key laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra- Precision Manufacturing Equipment Control, Tsinghua University, Beijing, China; Department of Mechanical Engineering, State key laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra- Precision Manufacturing Equipment Control, Tsinghua University, Beijing, China; Department of Mechanical Engineering, State key laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra- Precision Manufacturing Equipment Control, Tsinghua University, Beijing, China; Department of Mechanical Engineering, State key laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra- Precision Manufacturing Equipment Control, Tsinghua University, Beijing, China","2023 WRC Symposium on Advanced Robotics and Automation (WRC SARA)","27 Sep 2023","2023","","","1","6","Elastic Peg-in-Hole assembly has a wide range of applications in both industrial and home environments. However, accurately representing the infinite continuum of states and obtaining an accurate deformation model of the elastic deformable peg remains a challenge. Reinforcement learning (RL) has demonstrated its ability to learn manipulation skills from interactive experiences without the need for an exact physical model. Nonetheless, current RL methods rely on complex multimodal representations obtained using neural networks and lack clear physical interpretations. This article proposes a practical framework based on RL to interpret vision and force feedback through a concise, reformulated action space. The vision and force-based guidance are fused in the action space, and any continuous RL method can be adopted to learn the fusion pattern. Several experiments were conducted on an elastic peg-in-hole assembly platform to validate the effectiveness of the learned fusion scheme and its comparison to existing control and RL methods.","2835-3358","979-8-3503-0732-0","10.1109/WRCSARA60131.2023.10261820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261820","","Deformable models;Automation;Deformation;Neural networks;Force feedback;Force;Reinforcement learning","assembling;elastic deformation;force feedback;manipulators;reinforcement learning","clear physical interpretations;continuous RL method;elastic deformable peg;elastic peg-in-hole assembly platform;exact physical model;force feedback;force-based guidance;industrial home environments;learned fusion scheme;neural networks;reinforcement learning","","","","25","IEEE","27 Sep 2023","","","IEEE","IEEE Conferences"
