"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Reinforcement Learning Control of A Novel Magnetic Actuated Flexible-joint Robotic Camera System for Single Incision Laparoscopic Surgery","D. Xu; Y. Zhang; W. Tan; H. Wei","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Mechanical Engineering and Automation, Beihang University, Beijing, China; School of Mechanical Engineering and Automation, Beihang University, Beijing, China","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","1236","1241","This paper describes the control of a novel Magnetic Actuated Flexible-joint Robotic Surgical (MAFRS) camera system with four degrees of freedom (4-DOF) for single incision laparoscopic surgery. Based on the idea of motion decoupling, we designed a novel MAFRS system which is consists of an external driving device and a motor-free insertable wireless robotic device with a hollow flexible joint. Due to the problems of abdominal wall obstruction and variability in abdominal wall thickness during the actual application of the MAFRS system, as well as the existence of multiple permanent magnets and magnetically conductive media, high- precision position and attitude control of the insertable device without onboard motors has always been a challenge. We use the external driving device to generate a magnetic field to control the position and attitude of the internal robotic device. Aiming at the automatic precise tilt motion control of the novel MAFRS camera system, we have developed a closed-loop control scheme using the Deep Deterministic Policy Gradient (DDPG) algorithm. By referring to the damping characteristics of human muscles, a virtual-muscle method is proposed to eliminate the chattering problem of the MAFRS camera at specific angles. The experimental investigations indicate that the internal robotic device can be effectively controlled under different abdominal wall thicknesses. The tilt motion control accuracy is within 0.5°, and it has good adaptability and antiinterference performance.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560927","Nature; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560927","","Motion planning;Wireless communication;Minimally invasive surgery;Attitude control;Magnetomechanical effects;Robot vision systems;Reinforcement learning","attitude control;closed loop systems;damping;medical robotics;motion control;muscle;position control;surgery","flexible-joint robotic camera system;single incision laparoscopic surgery;novel Magnetic Actuated Flexible-joint Robotic Surgical;4-DOF;motion decoupling;MAFRS system;external driving device;motor-free insertable wireless robotic device;hollow flexible joint;abdominal wall obstruction;abdominal wall thickness;multiple permanent magnets;magnetically conductive media;high- precision position;attitude control;insertable device;onboard motors;magnetic field;internal robotic device;automatic precise tilt motion control;MAFRS camera system;closed-loop control scheme;Deep Deterministic Policy Gradient algorithm;different abdominal wall thicknesses;motion control accuracy","","","","17","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Generalized model learning for Reinforcement Learning on a humanoid robot","T. Hester; M. Quinlan; P. Stone","Department of Computer Science, University of Texas, Austin, Austin, TX, USA; Department of Computer Science, University of Texas, Austin, Austin, TX, USA; Department of Computer Science, University of Texas, Austin, Austin, TX, USA","2010 IEEE International Conference on Robotics and Automation","15 Jul 2010","2010","","","2369","2374","Reinforcement learning (RL) algorithms have long been promising methods for enabling an autonomous robot to improve its behavior on sequential decision-making tasks. The obvious enticement is that the robot should be able to improve its own behavior without the need for detailed step-by-step programming. However, for RL to reach its full potential, the algorithms must be sample efficient: they must learn competent behavior from very few real-world trials. From this perspective, model-based methods, which use experiential data more efficiently than model-free approaches, are appealing. But they often require exhaustive exploration to learn an accurate model of the domain. In this paper, we present an algorithm, Reinforcement Learning with Decision Trees (RL-DT), that uses decision trees to learn the model by generalizing the relative effect of actions across states. The agent explores the environment until it believes it has a reasonable policy. The combination of the learning approach with the targeted exploration policy enables fast learning of the model. We compare RL-DT against standard model-free and model-based learning methods, and demonstrate its effectiveness on an Aldebaran Nao humanoid robot scoring goals in a penalty kick scenario.","1050-4729","978-1-4244-5038-1","10.1109/ROBOT.2010.5509181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5509181","","Humanoid robots;Decision trees;Machine learning;Learning systems;Robotics and automation;USA Councils;Computer science;Decision making;Robot programming;Helicopters","decision making;decision trees;generalisation (artificial intelligence);humanoid robots;learning (artificial intelligence);mobile robots;multi-robot systems","generalized model learning;reinforcement learning;autonomous robot;decision making;robot programming;decision trees;Aldebaran Nao humanoid robot","","51","2","14","IEEE","15 Jul 2010","","","IEEE","IEEE Conferences"
"Learning-Driven Exploration for Reinforcement Learning","M. Usama; D. E. Chang","School of Electrical Engineering, KAIST, Daejeon, Korea; School of Electrical Engineering, KAIST, Daejeon, Korea","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","1146","1151","Effective and intelligent exploration remains an unresolved problem for reinforcement learning. Most contemporary reinforcement learning relies on simple heuristic strategies which are unable to intelligently distinguish the well-explored and the unexplored regions of state space, which can lead to inefficient use of training time. We introduce entropy-based exploration (EBE) that enables an agent to explore efficiently the unexplored regions of state space. EBE quantifies the agent's learning in a state using state-dependent action values and adaptively explores the state space, i.e. more exploration for the unexplored region of the state space. We perform experiments on a diverse set of environments and demonstrate that EBE enables efficient exploration that ultimatelyresults in faster learning withouthaving totune anyhyperparameter. The code to reproduce the experimentsisgiven at https://github.com/Usama1002/EBE-Exploration and the supplementary video is given at https://youtu.be/nJggIjjzKic.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649810","IITP(grant numbers:2021-0-00590); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649810","reinforcement learning;exploration;robotics","Training;Codes;Automation;Reinforcement learning;Aerospace electronics;Control systems;Entropy","entropy;optimisation;reinforcement learning;state-space methods","state space;state-dependent action values;intelligent exploration;reinforcement learning;heuristic strategy;entropy-based exploration;agent exploration","","3","","25","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Reinforcement Strategy Using Quantum Amplitude Amplification for Robot Learning","D. Daoyi; C. Chunlin; Li Hanxiong","Department of Manufacturing Engineering and Engineering Management, City University of Hong Kong, Hong Kong, China; Department of Control and Systems Engineering, Nanjing University, Nanjing, China; Department of Manufacturing Engineering and Engineering Management, City University of Hong Kong, Hong Kong, China","2007 Chinese Control Conference","15 Oct 2007","2007","","","571","575","Quantum amplitude amplification is a kind of useful technique in quantum computation and it can boost the success probability of some quantum algorithms. Reinforcement strategy in reinforcement learning is essentially to boost the selection probability of ""good"" action. Considering the common characteristics, this paper uses the idea of amplitude amplification to reinforcement learning as a new reinforcement strategy, proposes a learning algorithm based on quantum amplitude amplification and demonstrates its effectiveness through simulated experiments.","1934-1768","978-7-81124-055-9","10.1109/CHICC.2006.4347206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4347206","Reinforcement Learning;Quantum Amplitude Amplification;Robot Learning;Reinforcement Strategy","Learning;Quantum computing;Artificial intelligence;Robot sensing systems;Control systems;Intelligent robots;Quantum mechanics;Mobile robots;Information processing;Fuzzy logic","learning (artificial intelligence);probability;quantum computing;robots","quantum amplitude amplification;robot learning;quantum computation;success probability;quantum algorithm;reinforcement learning;selection probability","","2","","26","IEEE","15 Oct 2007","","","IEEE","IEEE Conferences"
"Coordination of hydraulic manipulators by reinforcement learning","M. Karpenko; J. Anderson; N. Sepehri","Department of Mechanical and Manufacturing Engineering, University of Manitoba, Winnipeg, MAN, Canada; Department of Computer Science, University of Manitoba, Canada; Department of Mechanical and Manufacturing Engineering, University of Manitoba, Winnipeg, MAN, Canada","2006 American Control Conference","24 Jul 2006","2006","","","6 pp.","","In this paper, a reinforcement learning method is applied to coordinate a pair of horizontal hydraulic actuators engaged in the cooperative positioning of an object. The goal is to enable the actuators to discover how to intelligently select control actions that tend to reduce the interaction forces directed along the axis of motion, while maintaining the desired trajectory. First, a detailed and realistic dynamic model of the entire system is derived. A multi-layer reinforcement learning neural network control architecture is designed next to regulate the interaction force during positioning. To regulate the interaction force, the neural network measures the interaction force and proposes a modification to the a priori prescribed formation constrained position trajectory. Each actuator system is outfitted with such a neural controller so that a decentralized reinforcement learning control system results. Simulations demonstrate the efficacy of the approach towards reducing the interaction forces and minimizing the associated object internal force in a single degree of freedom","2378-5861","1-4244-0210-7","10.1109/ACC.2006.1657214","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657214","","Learning;Force measurement;Intelligent actuators;Force control;Neural networks;Control systems;Hydraulic actuators;Intelligent control;Motion control;Multi-layer neural network","control system synthesis;decentralised control;intelligent control;learning (artificial intelligence);manipulator dynamics;motion control;neurocontrollers;position control","hydraulic manipulator coordination;horizontal hydraulic actuators;cooperative positioning;intelligent control;motion control;multilayer reinforcement learning;neural network control architecture;formation constrained position trajectory;decentralized control system","","2","","13","","24 Jul 2006","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Path Planning of the Mobile Agents with Constrained Locomotion for the Material Handling Applications","S. Veeramani; S. Muthuswamy","Centre for AI, IoT and Robotics Department of Mechanical Engineering, Indian Institue of Information Technology, Design and Manufacturing, Kancheepuram, Chennai, India; Centre for AI, IoT and Robotics Department of Mechanical Engineering, Indian Institue of Information Technology, Design and Manufacturing, Kancheepuram, Chennai, India","2020 IEEE 4th Conference on Information & Communication Technology (CICT)","8 Jan 2021","2020","","","1","5","This paper presents the intelligent path planning model of the mobile base agent of SwarmItFIX robot with novel Swing and Dock (SaD) locomotion for material handling/transfer applications. In this work, the Markov Decision Process (MDP) path planning problem of SaD agent is solved using two Reinforcement Learning (RL) based dynamic programming methods viz Policy Iteration (PI), and Value Iteration (VI). Being tested with 16 different test cases, both the algorithms return the optimal sequence of steps with reduced makespan for the mobile agents to reach the goal positions positively. The results of both the methods were compared with each other in the section V, and found to be convincing. Hence the proposed control scheme is being implemented in the SwarmItFIX setup available at the University of Genova, Italy.","","978-0-7381-2447-6","10.1109/CICT51604.2020.9311923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9311923","SaD agent;SwarmItFIX intelligent robots;Material Handling System;Path planning;Dynamic programming","Path planning;Computational modeling;Legged locomotion;Mathematical model;Materials handling;Heuristic algorithms;Mobile agents","dynamic programming;iterative methods;learning (artificial intelligence);Markov processes;materials handling;mobile robots;path planning","Value Iteration;dynamic programming methods viz Policy;SaD agent;Markov Decision Process path planning problem;SwarmItFIX robot;mobile base agent;intelligent path planning model;material handling applications;constrained locomotion;mobile agents","","2","","9","IEEE","8 Jan 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Robots Path Planning with Rule-based Shallow-trial","K. Tang; H. Fu; H. Jiang; C. Liu; L. Wang","Department of Control and Systems Engineenng, School of Management and Engineering, Nanjing University, Nanjing, China; Department of Control and Systems Engineerin, School of Management and Engineering, Nanjing University, Mianyang, China; Southwest University of Science and Technology, Mianyang, Sichuan, CN; School of Manufacturing Science and Engineenng, Key Laboratory of Testing Technology for Manufacturing Process of Ministry of Education, Southwest University of Science and Technology, Mianyang, China; Department of Control and Systems Engineenng, Nanjing University, Nanjing, China","2019 IEEE 16th International Conference on Networking, Sensing and Control (ICNSC)","24 Jun 2019","2019","","","340","345","A key skill for mobile robots is the ability to navigate efficiently through their environment, and reinforcement learning is widely used in path planning for mobile robots. However, this algorithm has a slow convergence speed and a large number of iterations. There are few studies on how to improve learning efficiently from the perspective of acquisition in rule-based shallow-trial strategy. In biological world, animals depend on their own empirical knowledge when making path planing. Humanity has transcendental knowledge, which is of great help to peoples navigation. We take the transcendental knowledge of human behavior, and express it acts as shallow-trial rules, then apply the rule-based shallow-trial reinforcement learning(RSRL) to the navigation learning of robot and improve learning efficiently.","","978-1-7281-0084-5","10.1109/ICNSC.2019.8743192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8743192","Reinforcement learning;Shallow-trial;Path planing;Rule-based","Path planning;Navigation;Reinforcement learning;Task analysis;Robot sensing systems;Mobile robots","learning (artificial intelligence);mobile robots;path planning","mobile robots;rule-based shallow-trial strategy;empirical knowledge;path planing;transcendental knowledge;rule-based shallow-trial reinforcement learning;robot path planning;convergence speed","","1","","21","IEEE","24 Jun 2019","","","IEEE","IEEE Conferences"
"Learning Emergent Discrete Message Communication for Cooperative Reinforcement Learning","S. Li; Y. Zhou; R. Allen; M. J. Kochenderfer","Department of Aeronautics and Astronautics, Stanford University; Lincoln Laboratory, Massachusetts Institute of Technology; Lincoln Laboratory, Massachusetts Institute of Technology; Department of Aeronautics and Astronautics, Stanford University","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","5511","5517","Communication is an important factor that en-ables agents to work cooperatively in multi-agent reinforcement learning (MARL) contexts. Prior work used continuous message communication whose high representational capacity comes at the expense of interpretability. Allowing agents to learn their own discrete emergent message communication protocols can increase the interpretability for human designers and other agents. This paper proposes a method to generate discrete messages analogous to human languages. Discrete message communication is achieved by a broadcast-and-listen mecha-nism based on self-attention. We show that discrete message communication has performance comparable to continuous message communication but with a much smaller vocabulary size. Discrete message communication protocols can potentially be used for human-agent interaction.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812285","","Measurement;Vocabulary;Protocols;Automation;Reinforcement learning;Bandwidth","learning (artificial intelligence);multi-agent systems;protocols;telecommunication computing","discrete emergent message communication protocols;discrete messages;continuous message communication;discrete message communication protocols;human-agent interaction;cooperative reinforcement learning;multiagent reinforcement learning contexts;high representational capacity;learning emergent discrete message communication;MARL contexts;broadcast-and-listen mechanism;self-attention","","1","","25","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Intrinsically Motivated Self-supervised Learning in Reinforcement Learning","Y. Zhao; C. Du; H. Zhao; T. Li","Peking University, Haidian, Beijing, China; Tsinghua University, Haidian, Beijing, China; Tsinghua University, Haidian, Beijing, China; Peking University, Haidian, Beijing, China","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","3605","3615","In vision-based reinforcement learning (RL) tasks, it is prevalent to assign auxiliary tasks with a surrogate self-supervised loss so as to obtain more semantic representations and improve sample efficiency. However, abundant information in self-supervised auxiliary tasks has been disregarded, since the representation learning part and the decision-making part are separated. To sufficiently utilize information in auxiliary tasks, we present a simple yet effective idea to employ self-supervised loss as an intrinsic reward, called Intrinsically Motivated Self-Supervised learning in Reinforcement learning (IM-SSR). We formally show that the self-supervised loss can be decomposed as exploration for novel states and robustness improvement from nuisance elimination. IM-SSR can be effortlessly plugged into any reinforcement learning with self-supervised auxiliary objectives with nearly no additional cost. Combined with IM-SSR, the previous underlying algorithms achieve salient improvements on both sample efficiency and generalization in various vision-based robotics tasks from the DeepMind Control Suite, especially when the reward signal is sparse.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812213","","Representation learning;Costs;Automation;Semantics;Decision making;Reinforcement learning;Self-supervised learning","decision making;image representation;reinforcement learning;robot vision;supervised learning","vision-based reinforcement learning tasks;semantic representations;self-supervised auxiliary tasks;decision-making part;self-supervised loss;intrinsic reward;IM-SSR;vision-based robotics tasks;intrinsically motivated self-supervised learning;DeepMind Control Suite","","1","","25","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Learning Crowd-Aware Robot Navigation from Challenging Environments via Distributed Deep Reinforcement Learning","S. Matsuzaki; Y. Hasegawa","Honda R&D Co., LTD.; Honda R&D Co., LTD.","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","4730","4736","This paper presents a deep reinforcement learning (DRL) sframework for safe and efficient navigation in crowded environments. Here, the robot learns cooperative behavior using a new reward function that penalizes robot actions interfering with the pedestrian's movement. Also, we propose a simulated pedestrian policy reflecting data from actual pedestrian movements. Furthermore, we introduce a collision detection that considers the pedestrian's personal space to generate affinity robot behavior. To efficiently explore this simulation environment, we propose distributed learning using Ape-X [1]. We deployed the robot in a real environment and verified its crowd-aware navigation performance compared with an actual human in terms of path length, travel time, and the number of abrupt avoidances.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812011","","Training;Computer aided instruction;Automation;Navigation;Distance learning;Reinforcement learning;Behavioral sciences","human-robot interaction;learning (artificial intelligence);mobile robots;navigation;path planning;pedestrians;road traffic","learning crowd-aware robot navigation;distributed deep reinforcement learning;deep reinforcement learning sframework;safe navigation;efficient navigation;crowded environments;reward function;robot actions;simulated pedestrian policy;actual pedestrian movements;affinity robot behavior;simulation environment;crowd-aware navigation performance","","1","","37","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Reinforcement learning of path-finding behaviour by a mobile robot","K. Malmstrom; L. Munday; J. Sitte","School of Mechanical, Manufacturing, and Medical Engineering, Queensland University of Technology; School of Mechanical, Manufacturing, and Medical Engineering, Queensland University of Technology; School of Computing Science, Queensland University of Technology","1996 Australian New Zealand Conference on Intelligent Information Systems. Proceedings. ANZIIS 96","6 Aug 2002","1996","","","334","337","We describe how a simple autonomous mobile robot can learn to navigate towards a goal while avoiding obstacles. A neural network determines the actions of the robot in response to the inputs from an array of infrared sensors. A reinforcement learning algorithm adjusts the weights of the neural network until the appropriate ""action mapping"" from sensor input to action output is found. Learning takes place in real time in the robot. The learning method is generic and therefore suitable for any robot with similar sensor and effectors.","","0-7803-3667-4","10.1109/ANZIIS.1996.573977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=573977","","Learning;Mobile robots;Robot sensing systems;Infrared sensors;Navigation;Neural networks;Sensor arrays;Sensor systems;Diodes;Infrared detectors","mobile robots;path planning;intelligent control;learning (artificial intelligence);navigation;position control;neurocontrollers;infrared imaging;real-time systems","reinforcement learning;path-finding behaviour;mobile robot;navigation;obstacle avoidance;neural network;infrared sensors;weight adjustment;action mapping;sensor input;action output;real time;sensor;effectors","","","","6","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Sample-Efficient Goal-Conditioned Reinforcement Learning via Predictive Information Bottleneck for Goal Representation Learning","Q. Zou; E. Suzuki","Graduate School of Systems Life Sciences, Kyushu University, Fukuoka, Japan; Graduate School and Faculty of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","9523","9529","We propose Predictive Information bottleneck for Goal representation learning (PI-Goal), a self-supervised method for sample-efficient goal-conditioned reinforcement learning (RL). Goal-conditioned RL learns to reach commanded goals with reward signals. A goal could be given in a noisy or abstract form, and thus jeopardizes sample efficiency. Previous methods usually assume that the agent can map a state to an achievable goal. In this work, we consider a setting in which the goal space is unknown to the agent and the agent cannot recognize a goal in a specific state (referred to as a goal state) until the goal is commanded. Our PI-Goal learns a goal representation which contains only the predictive information of a goal state, i.e., the mutual information between a current state and a future state, and guarantees the optimality of the learned policy. Experimental results show that PI-Goal consistently outperforms the baseline methods in tasks with unknown goal spaces, e.g., object manipulation, object search, and embodied question answering.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161213","","Representation learning;Training;Automation;Reinforcement learning;Search problems;Question answering (information retrieval);Trajectory","learning (artificial intelligence);question answering (information retrieval);reinforcement learning;supervised learning","achievable goal;commanded goals;Goal representation learning;goal space;goal state;Goal-conditioned RL;learned policy;PI-Goal;Predictive Information bottleneck;sample efficiency;sample-efficient goal-conditioned reinforcement learning","","","","44","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Efficient Preference-Based Reinforcement Learning Using Learned Dynamics Models","Y. Liu; G. Datta; E. Novoseller; D. S. Brown",UC Berkeley; UC Berkeley; Army Research Lab; University of Utah,"2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","2921","2928","Preference-based reinforcement learning (PbRL) can enable robots to learn to perform tasks based on an individual's preferences without requiring a hand-crafted re-ward function. However, existing approaches either assume access to a high-fidelity simulator or analytic model or take a model-free approach that requires extensive, possibly unsafe online environment interactions. In this paper, we study the benefits and challenges of using a learned dynamics model when performing PbRL. In particular, we provide evidence that a learned dynamics model offers the following benefits when performing PbRL: (1) preference elicitation and policy optimization require significantly fewer environment interactions than model-free PbRL, (2) diverse preference queries can be synthesized safely and efficiently as a byproduct of standard model-based RL, and (3) reward pre-training based on suboptimal demonstrations can be performed without any environmental interaction. Our paper provides empirical ev-idence that learned dynamics models enable robots to learn customized policies based on user preferences in ways that are safer and more sample efficient than prior preference learning approaches. Supplementary materials and code are available at https://sites.google.com/berkeley.edu/mop-rl.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161081","","Analytical models;Codes;Automation;Reinforcement learning;Noise measurement;Task analysis;Robots","control engineering computing;optimisation;reinforcement learning;robots","analytic model;efficient preference-based reinforcement learning;learned dynamics model;model-free approach;model-free PbRL;performing PbRL;policy optimization;preference learning approaches;standard model-based RL;unsafe online environment interactions","","","","64","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Offline Learning of Counterfactual Predictions for Real-World Robotic Reinforcement Learning","J. Jin; D. Graves; C. Haigh; J. Luo; M. Jagersand","Noah's Ark Lab, Huawei Technologies Canada, Ltd., Edmonton, AB., Canada; Noah's Ark Lab, Huawei Technologies Canada, Ltd., Edmonton, AB., Canada; Noah's Ark Lab, Huawei Technologies Canada, Ltd., Edmonton, AB., Canada; Noah's Ark Lab, Huawei Technologies Canada, Ltd., Edmonton, AB., Canada; Department of Computing Science, University of Alberta, Edmonton, AB., Canada","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","3616","3623","We consider real-world reinforcement learning (RL) of robotic manipulation tasks that involve both visuomotor skills and contact-rich skills. We aim to train a policy that maps multimodal sensory observations (vision and force) to a manipulator's joint velocities under practical considerations. We propose to use offline samples to learn a set of general value functions (GVFs) that make counterfactual predictions from the visual inputs. We show that combining the offline learned counterfactual predictions with force feedbacks in online policy learning allows efficient reinforcement learning given only a terminal (success/failure) reward. We argue that the learned counterfactual predictions form a compact and informative representation that enables sample efficiency and provides auxiliary reward signals that guide online explorations towards contact-rich states. Various experiments in simulation and real-world settings were performed for evaluation. Recordings of the real-world robot training can be found via https://sites.google.com/view/realrl.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811963","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811963","","Training;Learning systems;Visualization;Automation;Force;Force feedback;Reinforcement learning","manipulators;reinforcement learning","offline learning;real-world robotic reinforcement learning;robotic manipulation tasks;visuomotor skills;contact-rich skills;multimodal sensory observations;manipulator;offline samples;general value functions;force feedbacks;online policy learning;efficient reinforcement;terminal reward;learned counterfactual predictions;sample efficiency;contact-rich states;real-world robot training","","","","52","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"A Customizable Reinforcement Learning Environment for Semiconductor Fab Simulation","B. Kovács; P. Tassel; M. Gebser; G. Seidel","Alpen-Adria-Universität Klagenfurt, Klagenfurt am Wörthersee, AUSTRIA; Alpen-Adria-Universität Klagenfurt, Klagenfurt am Wörthersee, AUSTRIA; Alpen-Adria-Universität Klagenfurt, Klagenfurt am Wörthersee, AUSTRIA; Infineon Technologies Austria AG, Villach, AUSTRIA","2022 Winter Simulation Conference (WSC)","23 Jan 2023","2022","","","2663","2674","Reinforcement learning based methods are increasingly used to solve NP-hard combinatorial optimization problems. By learning from the problem structure, or the characteristics of instances, the approach has high potential compared to alternative techniques solving all instances from scratch. This work introduces a novel framework for creating (deep) reinforcement learning environments simulating up to real-world scale semiconductor fab scheduling problem instances. The highly configurable framework supports creating single- and multi-agent environments where the simulation factory is either partially or fully controlled by the learning agents. The action and observation spaces and the reward function are customizable based on pre-defined features. Our toolkit creates environments with a standard interface that can be integrated with various algorithms in a few minutes. The simulated datasets may involve challenging features like downtimes, batching, rework, and sequence-dependent setups. These can also be turned off and simulated datasets be automatically downscaled during the prototyping phase.","1558-4305","978-1-6654-7661-4","10.1109/WSC57314.2022.10015524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015524","","Deep learning;Adaptation models;Job shop scheduling;Reinforcement learning;Benchmark testing;Semiconductor device manufacture;Aerospace electronics","combinatorial mathematics;computational complexity;computer simulation;multi-agent systems;optimisation;production engineering computing;reinforcement learning;scheduling;semiconductor device manufacture","configurable framework;customizable reinforcement learning environment;learning agents;multiagent environments;NP-hard combinatorial optimization problems;observation spaces;pre-defined features;problem structure;real-world scale semiconductor fab scheduling problem instances;reinforcement learning environments;semiconductor fab simulation;simulated datasets;simulation factory","","","","25","IEEE","23 Jan 2023","","","IEEE","IEEE Conferences"
"Automated Excavator Based on Reinforcement Learning and Multibody System Dynamics","I. Kurinov; G. Orzechowski; P. Hämäläinen; A. Mikkola","Department of Mechanical Engineering, LUT University, Lappeenranta, Finland; Department of Mechanical Engineering, LUT University, Lappeenranta, Finland; Department of Computer Science, Aalto University, Espoo, Finland; Department of Mechanical Engineering, LUT University, Lappeenranta, Finland","IEEE Access","7 Dec 2020","2020","8","","213998","214006","Fully autonomous earth-moving heavy equipment able to operate without human intervention can be seen as the primary goal of automated earth construction. To achieve this objective requires that the machines have the ability to adapt autonomously to complex and changing environments. Recent developments in automation have focused on the application of different machine learning approaches, of which the use of reinforcement learning algorithms is considered the most promising. The key advantage of reinforcement learning is the ability of the system to learn, adapt and work independently in a dynamic environment. This article investigates an application of reinforcement learning algorithm for heavy mining machinery automation. To this end, the training associated with reinforcement learning is done using the multibody approach. The procedure used combines a multibody approach and proximal policy optimization with a covariance matrix adaptation learning algorithm to simulate an autonomous excavator. The multibody model includes a representation of the hydraulic system, multiple sensors observing the state of the excavator and deformable ground. The task of loading a hopper with soil taken from a chosen point on the ground is simulated. The excavator is trained to load the hopper effectively within a given time while avoiding collisions with the ground and the hopper. The proposed system demonstrates the desired behavior after short training times.","2169-3536","","10.1109/ACCESS.2020.3040246","European Union’s Horizon 2020 Research and Innovation Programme through the Marie Sklodowska-Curie Project(grant numbers:845600 (RealFlex)); Academy of Finland #316106; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268069","Autonomous agents;discrete event dynamic automation systems;learning and adaptive systems;real-time simulation;multibody system dynamics;reinforcement learning;PPO-CMA","Reinforcement learning;Adaptation models;Automation;Computational modeling;Training;Task analysis;Load modeling","covariance matrices;excavators;industrial robots;learning (artificial intelligence);machinery;mechanical engineering computing;mobile robots;optimisation","dynamic environment;reinforcement learning algorithm;heavy mining machinery automation;multibody approach;autonomous excavator;multibody system dynamics;autonomous earth-moving heavy equipment;automated earth construction;complex environments;different machine learning approaches","","21","","28","CCBY","24 Nov 2020","","","IEEE","IEEE Journals"
"A Reinforcement Learning Model to Assess Market Power Under Auction-Based Energy Pricing","V. Nanduri; T. K. Das","University of South Florida, Tampa, FL, USA; University of South Florida, Tampa, FL, USA","IEEE Transactions on Power Systems","29 Jan 2007","2007","22","1","85","95","Auctions serve as a primary pricing mechanism in various market segments of a deregulated power industry. In day-ahead (DA) energy markets, strategies such as uniform price, discriminatory, and second-price uniform auctions result in different price settlements and thus offer different levels of market power. In this paper, we present a nonzero sum stochastic game theoretic model and a reinforcement learning (RL)-based solution framework that allow assessment of market power in DA markets. Since there are no available methods to obtain exact analytical solutions of stochastic games, an RL-based approach is utilized, which offers a computationally viable tool to obtain approximate solutions. These solutions provide effective bidding strategies for the DA market participants. The market powers associated with the bidding strategies are calculated using well-known indexes like Herfindahl-Hirschmann index and Lerner index and two new indices, quantity modulated price index (QMPI) and revenue-based market power index (RMPI), which are developed in this paper. The proposed RL-based methodology is tested on a sample network","1558-0679","","10.1109/TPWRS.2006.888977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4077130","Auctions;average reward stochastic games;competitive Markov decision processes (CMDPs);deregulated electricity markets;market power;reinforcement learning (RL)","Learning;Pricing;Stochastic processes;Electricity supply industry deregulation;Electricity supply industry;Costs;Power generation;Power industry;Game theory;Testing","learning (artificial intelligence);power engineering computing;power markets;pricing;stochastic games","reinforcement learning model;market power;auction-based energy pricing;deregulated power industry;day-ahead energy markets;nonzero sum stochastic game theoretic model;bidding strategies;Herfindahl-Hirschmann index;Lerner index;quantity modulated price index;revenue-based market power index","","103","16","52","IEEE","29 Jan 2007","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Resource Allocation in Multi-UAV-Aided MEC Networks","J. Chen; X. Cao; P. Yang; M. Xiao; S. Ren; Z. Zhao; D. O. Wu","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong","IEEE Transactions on Communications","13 Jan 2023","2023","71","1","296","309","Resource allocation for mobile edge computing (MEC) in unmanned aerial vehicle (UAV) networks has been a popular research issue. Different from existing works, this paper considers a multi-UAV-aided uplink communication scenario and investigates a resource allocation problem of minimizing the total system latency and the energy consumption, subject to constraints on transmit power of mobile users (MUs), system latency caused by transmission and computation. The problem is confirmed to be a challenging time-series mixed-integer non-convex programming problem, and we propose a joint UAV Movement control, MU Association and MU Power control (UMAP) algorithm to solve it effectively, where three sub-problems are optimized iteratively. Specifically, UAV movement and MU association are optimized utilizing deep reinforcement learning (DRL) to decrease the energy consumption and system latency. Next, a closed-form solution of the MU transmit power is derived. Finally, simulation results show that the UMAP algorithm can significantly decrease the system latency and energy consumption and increase the coverage rate compared with benchmark algorithms.","1558-0857","","10.1109/TCOMM.2022.3226193","National Natural Science Foundation of China(grant numbers:91738301,61827901); Beihang University(grant numbers:KG21005501,KZ37102901); Application Research of UAV Inspection System in Facilities and Bridges of Shuohuang Railway(grant numbers:GJNY-19-90); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9968236","MEC;UAV;resource allocation;movement control;DRL","Energy consumption;Resource management;Autonomous aerial vehicles;Task analysis;Optimization;Trajectory;Energy efficiency","autonomous aerial vehicles;concave programming;convex programming;integer programming;iterative methods;learning (artificial intelligence);nonlinear programming;optimisation;power control;remotely operated vehicles;resource allocation","challenging time-series mixed-integer nonconvex programming problem;energy consumption;joint UAV Movement control;mobile edge computing;MU transmit power;multiUAV-aided MEC networks;multiUAV-aided uplink communication scenario;optimized utilizing deep reinforcement learning;popular research issue;resource allocation problem;unmanned aerial vehicle networks","","6","","42","IEEE","1 Dec 2022","","","IEEE","IEEE Journals"
"RLPGB-Net: Reinforcement Learning of Feature Fusion and Global Context Boundary Attention for Infrared Dim Small Target Detection","Z. Wang; T. Zang; Z. Fu; H. Yang; W. Du","Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","23 Aug 2023","2023","61","","1","15","In infrared scenes, humans can easily observe objects in the scene with their eyes, even dim ones. To make the robot have the same visual ability, this article proposes a pyramid-feature fusion target detection network, called RLPGB-Net, which combines reinforcement learning with aerial targets in the infrared scene. It makes use of the powerful decision-making ability of reinforcement learning to give corresponding weights to the extracted features and highlight the significant features of infrared dim small targets. In reinforcement learning, we use priori strategy guidance and long-term training methods to train weight-regulating agents. To eliminate the local influence on the detection results, such as bright interference points similar to the target, and to solve the problem of dim target detection effectively, the global context boundary attention (GB) module is introduced to eliminate the disadvantage of local comparison using the global characteristics of different dimensions. At the same time, it can prevent the edge information of the refined target from being submerged in the background. Experimental results on the SAITD and SIRST datasets show the effectiveness of the proposed method.","1558-0644","","10.1109/TGRS.2023.3304755","Shanghai Science and Technology Program “Federated based cross-domain and crosstask incremental learning”(grant numbers:21511100800); Natural Science Foundation of China(grant numbers:62076094); Chinese Defense Program of Science and Technology(grant numbers:2021-JCJQ-JJ-0041); China Aerospace Science and Technology Corporation Industry-University-Research Cooperation Foundation of the Eighth Research Institute(grant numbers:SAST2021-007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10215504","Global context boundary attention (GB);infrared dim target;pyramid feature fusion;reinforcement learning","Feature extraction;Object detection;Semantics;Reinforcement learning;Training;Tensors;Deep learning","decision making;feature extraction;image fusion;learning (artificial intelligence);object detection","aerial targets;called RLPGB-Net;dim ones;dim target detection;global context boundary attention module;infrared dim small targets;infrared scene;long-term training methods;powerful decision-making ability;priori strategy guidance;pyramid-feature fusion target detection network;refined target;reinforcement learning","","","","66","IEEE","14 Aug 2023","","","IEEE","IEEE Journals"
"On the Application of Reinforcement Learning in Multi-debris Active Removal Mission Planning","J. Yang; Y. H. Hu; Y. Liu; X. Hou; Q. Pan","School of Automation, Northwestern Polytechnical University, Xian, China; Dept. Electrical and Computer Engineering, University of Wisconsin-Madison, Madison, USA; School of Automation, Northwestern Polytechnical University, Xian, China; School of Automation, Northwestern Polytechnical University, Xian, China; School of Automation, Northwestern Polytechnical University, Xian, China","2019 IEEE 28th International Symposium on Industrial Electronics (ISIE)","1 Aug 2019","2019","","","605","610","We formulate the active multi-debris removal mission planning task as a Reinforcement-Learning (RL) problem and developed an adjusted Deep Q-Learning (DQN) solution. We propose novel definitions of the state space, action sets, and rewards in the context of active multi-debris removal mission planning. These definitions facilitate recasting the mission planning problem into a RL problem. As such, a powerful DQN algorithm may be applied to solve the mission planning problem using an RL approach. We test this new approach using a subset of Iridium 33 debris cloud. Very encouraging results are observed. Future applications to a reactive autonomous space mission planner are also discussed.","2163-5145","978-1-7281-3666-0","10.1109/ISIE.2019.8781167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8781167","Multi-debris active removal;space mission planning;reinforcement learning","Planning;Space missions;Reinforcement learning;Optimization;Orbits;Automation;Space vehicles","aerospace computing;learning (artificial intelligence);space debris","Iridium 33 debris cloud;reactive autonomous space mission planner;multidebris active removal mission planning;RL problem;reinforcement learning;DQN algorithm;deep Q-learning solution","","3","","30","IEEE","1 Aug 2019","","","IEEE","IEEE Conferences"
"Sample Efficient Reinforcement Learning Using Graph-Based Memory Reconstruction","Y. Kang; E. Zhao; Y. Zang; L. Li; K. Li; P. Tao; J. Xing","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Artificial Intelligence","","2023","PP","99","1","12","Reinforcement learning (RL) algorithms typically require orders of magnitude more interactions than humans to learn effective policies. Research on memory in neuroscience suggests that humans' learning efficiency benefits from associating their experiences and reconstructing potential events. Inspired by this finding, we introduce a human brain-like memory structure for agents and build a general learning framework based on this structure to improve the RL sampling efficiency. Since this framework is similar to the memory reconstruction process in psychology, we name the newly proposed RL framework as Graph-Based Memory Reconstruction (GBMR). In particular, GBMR first maintains an attribute graph on the agent's memory and then retrieves its critical nodes to build and update potential paths among these nodes. This novel pipeline drives the RL agent to learn faster with its memory-enhanced value functions and reduces interactions with the environment by reconstructing its valuable paths. Extensive experimental analyses and evaluations in the Grid Maze and some challenging Atari environments demonstrate GBMR's superiority over traditional RL methods. We will release the source code and trained models to facilitate further studies in this research direction.","2691-4581","","10.1109/TAI.2023.3268612","National Key R&D Program of China(grant numbers:2022ZD0116401); Natural Science Foundation of China(grant numbers:62076238,62222606,61902402); CCF-Tencent Open Fund, and in part by the Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDA27000000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10105983","Experience replay;graph model;memory reconstruction;reinforcement learning;sample efficiency","Memory management;Task analysis;Neuroscience;Brain modeling;Games;Automation;Writing","","","","","","","IEEE","20 Apr 2023","","","IEEE","IEEE Early Access Articles"
"Personalized Car Following for Autonomous Driving with Inverse Reinforcement Learning","Z. Zhao; Z. Wang; K. Han; R. Gupta; P. Tiwari; G. Wu; M. J. Barth","Toyota Motor North America R&D, InfoTech Labs, Mountain View, CA; Toyota Motor North America R&D, InfoTech Labs, Mountain View, CA; Toyota Motor North America R&D, InfoTech Labs, Mountain View, CA; Toyota Motor North America R&D, InfoTech Labs, Mountain View, CA; Toyota Motor North America R&D, InfoTech Labs, Mountain View, CA; Department of Electrical and Computer Engineering, Center for Environmental Research and Technology, University of California, Riverside, CA; Department of Electrical and Computer Engineering, Center for Environmental Research and Technology, University of California, Riverside, CA","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","2891","2897","Driving automation is gradually replacing human driving maneuvers in different applications such as adaptive cruise control and lane keeping. However, contemporary driving automation applications based on expert systems or prede-fined control strategies are not in line with individual human driver's preference. To overcome this problem, we propose a Personalized Adaptive Cruise Control (P-ACC) system that can learn the driver's car-following preferences from historical data using model-based maximum entropy Inverse Reinforcement Learning (IRL). Once activated in real-time, the P-ACC system first classifies the driver type and the weather type (at that moment). The vehicle is then controlled using the pre-trained IRL model on the cloud of the associated class. The personalized IRL model on the cloud will be updated as more human driving data is collected from various scenarios. Numerical simulation with real-world naturalistic driving data shows that, the accuracy of reproducing the real-world driving profile improves up to 30.1% in terms of speed and 36.5% in terms of distance gap, when P-ACC is compared with the Intelligent Driver Model (IDM). Game engine-based human-in-the-loop simulation demonstrates that, the takeover frequency of the driver during the usage of P-ACC decreases up to 93.4%, compared with that during the usage of IDM-based ACC.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812446","","Automation;Clouds;Reinforcement learning;Numerical simulation;Data models;Real-time systems;Numerical models","adaptive control;automobiles;behavioural sciences;driver information systems;entropy;learning (artificial intelligence);road accidents;road safety;road traffic control;road vehicles","P-ACC system;driver type;weather type;pre-trained IRL model;personalized IRL model;human driving data;real-world naturalistic driving data shows;real-world driving profile;Intelligent Driver Model;game engine-based human-in-the-loop simulation;IDM-based ACC;Personalized car;autonomous driving;Inverse Reinforcement Learning;human driving maneuvers;lane keeping;contemporary driving automation applications;expert systems;prede-fined control strategies;individual human driver;Personalized Adaptive Cruise Control system;historical data;model-based maximum entropy Inverse Reinforcement","","10","","28","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"A new approach of adaptive reinforcement learning control","Boo-Ho Yang; H. Asada","Center for Information-Driven Mechanical Systems Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA; Center for Information-Driven Mechanical Systems Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA","Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)","6 Aug 2002","1993","1","","627","630 vol.1","A new learning algorithm for connectionist networks that solves a class of optimal control problems is presented. The algorithm, called adaptive reinforcement learning algorithm, employs a second network to model immediate reinforcement provided from the task environment and adaptively identify it through experience. Output perturbation and correlation techniques are used to translate mere critic signals into useful learning signals for the connectionist controller. Compared with the direct approaches of reinforcement learning, this algorithm shows faster and guaranteed improvement in the control performance. Robustness against inaccuracy of the model is also discussed.","","0-7803-1421-2","10.1109/IJCNN.1993.713993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=713993","","Programmable control;Adaptive control;Learning;Robotic assembly;Force sensors;Force measurement;Control systems;Mechanical systems;Mechanical engineering;Optimal control","neural nets;learning (artificial intelligence);intelligent control;optimal control;correlation methods;adaptive control","adaptive reinforcement learning control;connectionist networks;optimal control;output perturbation;output correlation;critic signals;neural nets","","5","","12","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Reinforcement learning on an omnidirectional mobile robot","R. Hafner; M. Riedmiller","Informatik Lehrstuhl I, Universität Dortmund, Dortmund, Germany; Informatik Lehrstuhl I, Universität Dortmund, Dortmund, Germany","Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453)","8 Dec 2003","2003","1","","418","423 vol.1","With this paper we describe a well suited, scalable problem for reinforcement learning approaches in the field of mobile robots. We show a suitable representation of the problem for a reinforcement approach and present our results with a model based standard algorithm. Two different approximators for the value function are used, a grid based approximator and a neural network based approximator.","","0-7803-7860-1","10.1109/IROS.2003.1250665","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250665","","Mobile robots;Wheels;Robot kinematics;Robotic assembly;Neural networks;Testing;Gears;Machine learning;Learning systems","learning (artificial intelligence);mobile robots;neural nets;approximation theory","reinforcement learning;omnidirectional mobile robot;value function;grid based approximator;neural network based approximator","","2","","10","IEEE","8 Dec 2003","","","IEEE","IEEE Conferences"
"Value learning from trajectory optimization and Sobolev descent: A step toward reinforcement learning with superlinear convergence properties","A. Parag; S. Kleff; L. Saci; N. Mansard; O. Stasse","Artificial and Natural Intelligence Toulouse Institute, France; Artificial and Natural Intelligence Toulouse Institute, France; Artificial and Natural Intelligence Toulouse Institute, France; Artificial and Natural Intelligence Toulouse Institute, France; Artificial and Natural Intelligence Toulouse Institute, France","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","01","07","The recent successes in deep reinforcement learning largely rely on the capabilities of generating masses of data, which in turn implies the use of a simulator. In particular, current progress in multi body dynamic simulators are under-pinning the implementation of reinforcement learning for end-to-end control of robotic systems. Yet simulators are mostly considered as black boxes while we have the knowledge to make them produce a richer information. In this paper, we are proposing to use the derivatives of the simulator to help with the convergence of the learning. For that, we combine model-based trajectory optimization to produce informative trials using 1st- and 2nd-order simulation derivatives. These locally-optimal runs give fair estimates of the value function and its derivatives, that we use to accelerate the convergence of the critics using Sobolev learning. We empirically demonstrate that the algorithm leads to a faster and more accurate estimation of the value function. The resulting value estimate is used in model-predictive controller as a proxy for shortening the preview horizon. We believe that it is also a first step toward superlinear reinforcement learning algorithm using simulation derivatives, that we need for end-to-end legged locomotion.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811993","","Supervised learning;Optimal control;Reinforcement learning;Production;Network architecture;Approximation algorithms;Trajectory optimization","gradient methods;learning (artificial intelligence);legged locomotion;optimal control;optimisation;predictive control","richer information;model-based trajectory optimization;informative trials;2nd-order simulation derivatives;locally-optimal runs;value function;Sobolev learning;resulting value estimate;model-predictive controller;superlinear reinforcement learning;end-to-end legged locomotion;value learning;Sobolev descent;superlinear convergence properties;recent successes;deep reinforcement learning;particular progress;multibody dynamic simulators;end-to-end control","","1","","37","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Research of Reinforcement Learning Control of Intelligent Robot Based on Fuzzy-CMAC Network","L. Pan; Y. -b. Tong","College of Information Science and Engineering, Wuhan University of Science and Technology, Wuhan, China; College of Information Science and Engineering, Wuhan University of Science and Technology, Wuhan, China","2009 International Symposium on Computer Network and Multimedia Technology","8 Jan 2010","2009","","","1","4","Intelligent grasping by servo robot-hand of monocular vision is a complex nonlinear control problem. In order to achieve high accuracy, model structure and parameters design of Fuzzy-CMAC are studied in this paper, and improved reinforcement learning algorithm is introduced into FCMAC. According to the reinforcement signals, different strategies are used to improve convergence speed and algorithm optimization. Based on the theory research, we do experimental research and try FCMAC to approach the nonlinear part of the dynamic model of the robot-hand. By analyzing the experimental results, it is concluded that this control strategy can solve the nonlinear problem effectively and the trained robot-hand can grasp the target object fleetly and accurately.","","978-1-4244-5272-9","10.1109/CNMT.2009.5374686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5374686","","Learning;Robot control;Intelligent robots;Service robots;Servomechanisms;Robot vision systems;Orbital robotics;Robotic assembly;Mobile robots;Fuzzy neural networks","fuzzy set theory;intelligent robots;learning systems;nonlinear control systems","reinforcement learning control;intelligent robot;fuzzy-CMAC network;monocular vision;nonlinear control problem;Intelligent grasping;servo robot hand;convergence speed;algorithm optimization","","1","","8","IEEE","8 Jan 2010","","","IEEE","IEEE Conferences"
"Reinforcement learning algorithm with network extension for pulse neural network","K. Takita; Y. Osana; M. Hagiwara","Faculty of Science and Technology, Keio University, Yokohama, Japan; Faculty of Science and Technology, Keio University, Yokohama, Japan; Faculty of Science and Technology, Keio University, Yokohama, Japan","Smc 2000 conference proceedings. 2000 ieee international conference on systems, man and cybernetics. 'cybernetics evolving to systems, humans, organizations, and their complex interactions' (cat. no.0","6 Aug 2002","2000","4","","2586","2591 vol.4","In this paper, we propose a new hierarchical pulse neural network and its reinforcement learning algorithm with network extension. The proposed pulse neural network has three layers, and all of the neurons are pulse neurons. This network learns relations between input pulse sequences and the desired outputs by updating connection weights and by adding neurons dynamically. We carried out a computer simulation to confirm the performance of the proposed algorithm.","1062-922X","0-7803-6583-6","10.1109/ICSMC.2000.884383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884383","","Learning;Neural networks;Neurons;Biological neural networks;Biological system modeling;Information processing;Biological information theory;Computer simulation;Computer architecture;Assembly","neural nets;learning (artificial intelligence);virtual machines;sequences;pulse circuits","reinforcement learning algorithm;network extension;hierarchical pulse neural network;pulse neurons;input pulse sequences;input-output relation learning;connection weight updating;dynamic neuron addition;computer simulation;algorithm performance","","","","12","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Robotic Disassembly Sequence Planning Considering Robotic Movement State Based on Deep Reinforcement Learning","C. Yang; W. Xu; J. Liu; B. Yao; Y. Hu","School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; China Ship Development and Design Center, Wuhan, China","2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD)","20 May 2022","2022","","","183","189","Remanufacturing provides an alternative way to realize natural resources saving and environment protection. Disassembly, as a key step in remanufacturing, has been attracted much attention in recent years. To make up for the deficiency of manual disassembly which is low efficiency and high cost, industry robots have great advantages in handling large volume and repeatable disassembly activities. Besides, proper disassembly sequence planning helps to improve the disassembly efficiency. In this paper, the framework of robotic disassembly sequence planning using deep reinforcement learning (DRL) is proposed to solve robotic disassembly sequence planning (RDSP) problem. Considering the smoothness of starting and stopping in robotic movement, dynamic moving speed model is built for moving time in disassembly. Firstly, a disassembly precedence matrix (DPM) is constructed according to the structure of disassembly products. After that, RDSP is modeled as Markov decision process and the state, action and reward of the agent in DRL environment are designed. The deep reinforcement learning network model is trained to obtain the optimal disassembly sequence in RDSP. Finally, case study based on double coupling shaft with 21 components proves that the DRL algorithm used in this paper can obtain a disassembly sequence for better performance compared with other two meta-heuristic methods.","","978-1-6654-0527-0","10.1109/CSCWD54268.2022.9776113","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9776113","remanufacturing;robotic disassembly;dynamic moving speed;disassembly sequence planning;deep reinforcement learning","Shafts;Industries;Couplings;Costs;Service robots;Reinforcement learning;Manuals","assembling;assembly planning;design for disassembly;industrial robots;learning (artificial intelligence);Markov processes;optimisation;recycling;shafts","natural resources;environment protection;remanufacturing;manual disassembly;industry robots;repeatable disassembly activities;proper disassembly sequence planning;disassembly efficiency;robotic disassembly sequence planning problem;RDSP;dynamic moving speed model;disassembly precedence matrix;disassembly products;deep reinforcement learning network model;optimal disassembly sequence;robotic disassembly sequence planning considering robotic movement state","","1","","16","IEEE","20 May 2022","","","IEEE","IEEE Conferences"
"Wind Farm Maintenance Scheduling Using Soft Actor-Critic Deep Reinforcement Learning","F. J. Zhao; Y. Zhou","School of Mechanical Engineering, Southeast University, Nanjing, China; School of Mechanical Engineering, Southeast University, Nanjing, China","2022 Global Reliability and Prognostics and Health Management (PHM-Yantai)","14 Nov 2022","2022","","","1","6","The maintenance scheduling problem of windfarms is a recently arisen research topic, which contains uncertain factors introduced by weather conditions. However, most existing methods cannot generate maintenance schedules dynamically according to stochastic weather conditions. This paper formulates the maintenance scheduling problem as a Markov decision process (MDP). The Soft Actor-Critic (SAC) method is used to solve the MDP that has an extremely large state space. SAC is an off-policy deep reinforcement learning algorithm that considers entropy regularization during action selection. This mechanism accelerates the training process of the agent and prevents premature convergence to a local optimum solution. Numerical examples are used to verify the performance of SAC in maintenance scheduling. Result shows that the proposed method can obtain higher total production than the deep Q network and the genetic algorithm when the stochastic wind speed is considered.","","978-1-6654-9631-5","10.1109/PHM-Yantai55411.2022.9942116","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9942116","maintenance scheduling;Soft Actor-Critic;deep reinforcement learning;stochastic wind speed;wind energy","Training;Schedules;Wind energy;Wind speed;Reinforcement learning;Production;Maintenance engineering","genetic algorithms;learning (artificial intelligence);maintenance engineering;Markov processes;scheduling;stochastic processes;wind power plants","maintenance schedules;maintenance scheduling problem;Markov decision process;MDP;off-policy deep reinforcement learning algorithm;recently arisen research topic;Soft Actor-Critic deep reinforcement learning;Soft Actor-Critic method;stochastic weather conditions;stochastic wind speed;wind farm maintenance scheduling","","1","","12","IEEE","14 Nov 2022","","","IEEE","IEEE Conferences"
"Distribution network load prediction based on deep reinforcement learning","J. Zhang","School of Electrical Engineering, Shandong University, Jinan, Shandong Province","2022 2nd International Conference on Electrical Engineering and Control Science (IC2ECS)","6 Apr 2023","2022","","","321","324","Distribution network load forecasting is one of the important tasks of the power sector, which is of great significance to the production and life of the whole society. Accurate load forecasting helps rationalize power consumption planning and ensure reliable and safe operation of the grid. In this paper, the actual analysis is carried out and compared with the traditional regression analysis method to prove the superiority of the algorithm.","","979-8-3503-9916-5","10.1109/IC2ECS57645.2022.10087992","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10087992","load forecasting;deep learning;reinforcement learning;BP neural network","Deep learning;Load forecasting;Error analysis;Distribution networks;Reinforcement learning;Production;Planning","load forecasting;power consumption;power engineering computing;regression analysis;reinforcement learning","accurate load forecasting;deep reinforcement learning;distribution network load forecasting;distribution network load prediction;power consumption planning;power sector;reliable operation;safe operation;traditional regression analysis method","","","","13","IEEE","6 Apr 2023","","","IEEE","IEEE Conferences"
"Adaptive Multiagent Model Based on Reinforcement Learning for Distributed Generation Systems","D. Divényi; A. D'n","Department of Electric Power Engineering, Budapest University of Technology and Economics, Budapest, Hungary; NA","2012 23rd International Workshop on Database and Expert Systems Applications","11 Oct 2012","2012","","","303","307","Distributed generation have been widely spread in the last decades raising a lot of questions regarding the safe and high-quality operation of the power systems. The investigation of these questions requires a proper model considering the different technical, economical and legal aspects. The goal of our research was to develop a multiagent system where rational agents control each distributed generation unit. Based on intelligent agent-program the agents are able to optimize their operations taking several viewpoints into account, like fulfilling the contractual obligations, considering the technical constraints and maximizing the realized profit in a continuously varying market environment. This paper describes a simple reinforcement learning method resulting in an adaptive agent-program. The agents are informed about their realized profits and they apply this information to evaluate their former decisions and to adjust the parameters of their agent-program. The verification of the model proved that the developed agent-program provides acceptable results compared to the real productions.","2378-3915","978-1-4673-2621-6","10.1109/DEXA.2012.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327443","distributed generation;multiagent modeling;state-based method;strategies;reinforcement learning","Production;Multiagent systems;Distributed power generation;Power systems;Cogeneration;Learning","contract law;distributed power generation;learning (artificial intelligence);multi-agent systems;power engineering computing;socio-economic effects","adaptive multiagent model;reinforcement learning;distributed generation systems;safe operation;high-quality operation;power systems;technical aspects;economical aspects;legal aspects;multiagent system;rational agents;intelligent agent-program;contractual obligations;technical constraints;market environment;adaptive agent-program","","","","14","IEEE","11 Oct 2012","","","IEEE","IEEE Conferences"
"Game Theoretic Reinforcement Learning Framework For Industrial Internet of Things","T. M. Ho; K. -K. Nguyen; M. Cheriet","Synchromedia Lab, École de Technologie Supérieure, Université du Québec, QC, Canada; Synchromedia Lab, École de Technologie Supérieure, Université du Québec, QC, Canada; Synchromedia Lab, École de Technologie Supérieure, Université du Québec, QC, Canada","2022 IEEE Wireless Communications and Networking Conference (WCNC)","16 May 2022","2022","","","2112","2117","The fifth-generation (5G) wireless net-work provides high-rate, ultra-low latency, and high-reliability connections that can meet the industrial IoT requirements in factory automation especially for swarm robotics communication. In this paper, we address 5G service provisioning in an automated warehouse scenario where swarm robotics is controlled by an industrial controller that provides routing and job instructions over the 5G network. Leveraging the co-ordinated multipoint (CoMP), we formulate a joint CoMP clustering and 5G ultra-reliable low-latency communication (URLLC) beamforming design problem to control the robots that move around the automated warehouse for goods storage with the planed reference tracks. Traditional iterative optimization approaches are impractical in such dynamic wireless environments due to high computational time. We propose a game-theoretic CoMP clustering algorithm combined with the Proximal Policy Optimization method to obtain a stationary solution closed to that of the exhaustive search algorithm considered as the global optimal solution.","1558-2612","978-1-6654-4266-4","10.1109/WCNC51071.2022.9771864","Mitacs; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9771864","5G network;swarm robotics;industrial automation;URLLC;coordinated multipoint","Wireless communication;5G mobile communication;Array signal processing;Heuristic algorithms;Clustering algorithms;Swarm robotics;Optimization methods","array signal processing;cellular radio;factory automation;game theory;industrial control;Internet of Things;iterative methods;learning (artificial intelligence);multi-robot systems;optimisation;pattern clustering;radiofrequency interference;resource allocation;search problems;telecommunication scheduling","game theoretic reinforcement learning framework;fifth-generation wireless net-work;ultra-low latency;high-reliability connections;industrial IoT requirements;factory automation;swarm robotics communication;5G service provisioning;automated warehouse scenario;industrial controller;job instructions;co-ordinated multipoint;communication beamforming design problem;planed reference tracks;dynamic wireless environments;high computational time;game-theoretic CoMP clustering algorithm;Proximal Policy Optimization method","","","","15","IEEE","16 May 2022","","","IEEE","IEEE Conferences"
"Optimization control of a fed-batch process using an improved reinforcement learning algorithm","P. Zhang; J. Zhang; B. Hu; Y. Long","School of Engineering, Newcastle University, Newcastle upon Tyne, UK; School of Engineering, Newcastle University, Newcastle upon Tyne, UK; School of Computing, Newcastle University, Newcastle upon Tyne, UK; Department of Computer science, Duram University, Newcastle upon Tyne, UK","2019 IEEE Conference on Control Technology and Applications (CCTA)","5 Dec 2019","2019","","","314","319","Batch processes are important manufacturing route for the agile manufacturing of high value added products and they are typically difficult to control due to highly non-linear characteristic, unknown disturbance and model plant mismatches. Neural networks and traditional reinforcement learning have been applied to control and optimize batch processes. However, they usually lack robustness and accuracy leading to unsatisfactory performance. To overcome these problems, this paper proposes a stochastic multi-step action Q-learning algorithm (SMSA) based on multiple step action Q-learning (MSA). In MSA, the action space is divided into some same time steps, which means that some non-optimal actions will be continuously and compulsively applied in a long time and the speed of learning might be slow. Compared with MSA, the modification of SMSA is that several time steps are different and a modified greedy algorithm is used to improve the speed, efficiency and flexibility of algorithm. The proposed method is applied to a simulated fed-batch process and it gives better optimization control performance than other control strategies.","","978-1-7281-2767-5","10.1109/CCTA.2019.8920472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920472","Batch process;optimal control;reinforcement learning","Learning (artificial intelligence);Batch production systems;Process control;Optimization;Neural networks;Mathematical model;Inductors","agile manufacturing;batch processing (industrial);greedy algorithms;learning (artificial intelligence);neurocontrollers;optimal control;stochastic processes","stochastic multistep action Q-learning algorithm;modified greedy algorithm;fed-batch process;optimization control performance;reinforcement learning algorithm;agile manufacturing;high value added products;neural networks;nonlinear characteristics;optimal control","","2","","19","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Quantum Multi-Agent Reinforcement Learning via Variational Quantum Circuit Design","W. J. Yun; Y. Kwak; J. P. Kim; H. Cho; S. Jung; J. Park; J. Kim","School of Electrical Engineering, Korea University, Seoul, Republic of Korea; School of Electrical Engineering, Korea University, Seoul, Republic of Korea; School of Electrical Engineering, Korea University, Seoul, Republic of Korea; School of Electronic and Electrical Engineering, Sungkyunkwan University, Suwon, Republic of Korea; School of Software, Hallym University, Chuncheon, Republic of Korea; School of Information Technology, Deakin University, Geelong, Victoria, Australia; School of Electrical Engineering, Korea University, Seoul, Republic of Korea","2022 IEEE 42nd International Conference on Distributed Computing Systems (ICDCS)","13 Oct 2022","2022","","","1332","1335","In recent years, quantum computing (QC) has been getting a lot of attention from industry and academia. Especially, among various QC research topics, variational quantum circuit (VQC) enables quantum deep reinforcement learning (QRL). Many studies of QRL have shown that the QRL is superior to the classical reinforcement learning (RL) methods under the constraints of the number of training parameters. This paper extends and demonstrates the QRL to quantum multi-agent RL (QMARL). However, the extension of QRL to QMARL is not straightforward due to the challenge of the noise intermediate-scale quantum (NISQ) and the non-stationary properties in classical multi-agent RL (MARL). Therefore, this paper proposes the centralized training and decentralized execution (CTDE) QMARL framework by designing novel VQCs for the framework to cope with these issues. To corroborate the QMARL framework, this paper conducts the QMARL demonstration in a single-hop environment where edge agents offload packets to clouds. The extensive demonstration shows that the proposed QMARL framework enhances 57.7% of total reward than classical frameworks.","2575-8411","978-1-6654-7177-0","10.1109/ICDCS54860.2022.00151","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9912289","Quantum deep learning;Multi-agent reinforcement learning;Quantum computing","Training;Industries;Reinforcement learning;Quantum circuit;Distributed computing","centralised control;learning (artificial intelligence);multi-agent systems;quantum computing","quantum multiagent reinforcement learning;variational quantum circuit design;quantum computing;QC research topics;quantum deep reinforcement learning;QRL;classical reinforcement;noise intermediate-scale quantum;classical multiagent RL;decentralized execution QMARL framework;QMARL demonstration;edge agents;QMARL framework enhances;classical frameworks","","11","","12","IEEE","13 Oct 2022","","","IEEE","IEEE Conferences"
"PackerBot: Variable-Sized Product Packing with Heuristic Deep Reinforcement Learning","Z. Yang; S. Yang; S. Song; W. Zhang; R. Song; J. Cheng; Y. Li","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China","2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","16 Dec 2021","2021","","","5002","5008","Product packing is a typical application in ware-house automation that aims to pick objects from unstructured piles and place them into bins with optimized placing policy. However, it still remains a significant challenge to finish the product packing tasks in general logistics scenarios where the objects are variable-sized and the configurations are complex. In this work, we present the PackerBot, a complete robotic pipeline for performing variable-sized product packing in unstructured scenes. First, by leveraging the imperfect experience of human packer, we propose a heuristic DRL framework for learning optimal online 3D bin packing policy. Then we integrate it with a 6-DoF suction-based picking module and a product size estimation module, leading to a complete product packing system, namely the PackerBot. Extensive experimental results show that our method achieves the state-of-the-art performance in both simulated and real-world tests. The video demonstration is available at: https://vsislab.github.io/packerbot.","2153-0866","978-1-6654-1714-3","10.1109/IROS51168.2021.9635914","Research and Development; National Natural Science Foundation of China; Natural Science Foundation of Shandong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635914","","Three-dimensional displays;Automation;Pipelines;Buildings;Estimation;Reinforcement learning;Encoding","bin packing;deep learning (artificial intelligence);industrial robots;reinforcement learning;warehouse automation","warehouse automation;optimized placing policy;PackerBot;variable-sized product packing;3D bin packing policy;product size estimation module;heuristic deep reinforcement learning;robotic pipeline;product packing system","","5","","35","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Approach for Autonomous Agents in Consumer-centric Electricity Market","Y. Liu; D. Zhang; C. Deng; X. Wang","Artificial Intelligence Application Department, China Electric Power Research Institute, Beijing, China; Artificial Intelligence Application Department, China Electric Power Research Institute, Beijing, China; Artificial Intelligence Application Department, China Electric Power Research Institute, Beijing, China; Artificial Intelligence Application Department, China Electric Power Research Institute, Beijing, China","2020 5th IEEE International Conference on Big Data Analytics (ICBDA)","27 May 2020","2020","","","37","41","With the development of distributed energy, many novel electricity market mechanisms are emerging. The consumer-centric electricity market, which includes the peer-to-peer (P2P) model and the community-based market, arouses more attention in academia and industry area. This paper applies the Deep Q-Learning (DQN) for autonomous agents in the consumer-centric electricity market. Both the local energy priority transactions and public shared energy facilities are taking into consideration. We generated a test dataset and compared results in 5 different scenarios. This study verifies that the applied data-driven methods can handle the peer-to-peer (P2P) decision-making problem as well as promote the profitability of the whole community in the electricity market. Furthermore, multi-agent cooperation with public resources is more appropriate than other situations.","","978-1-7281-4111-4","10.1109/ICBDA49040.2020.9099946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9099946","peer-to-peer energy trading;big data;artificial intelligence;smart grid","Electricity supply industry;Machine learning;Autonomous agents;Power systems;Peer-to-peer computing;Linear programming","Big Data;decision making;learning (artificial intelligence);multi-agent systems;peer-to-peer computing;power engineering computing;power markets","consumer-centric electricity market;community-based market;autonomous agents;peer-to-peer decision-making problem;deep reinforcement learning approach;distributed energy;P2P model;deep Q-learning;DQN;local energy priority transactions;energy facilities;data-driven methods;profitability;public resources","","4","","24","IEEE","27 May 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning based Usage Aware Spectrum Access Scheme","Y. Teraki; X. Wang; M. Umehira; Y. Ji","Graduate School of Science and Engineering, Ibaraki University, Ibaraki, Japan; Graduate School of Science and Engineering, Ibaraki University, Ibaraki, Japan; Faculty of Science and Technology, Nanzan University, Nagoya, Japan; Information Systems Architecture Research Division, National Institute of Informatics, Tokyo, Japan","2021 24th International Symposium on Wireless Personal Multimedia Communications (WPMC)","7 Feb 2022","2021","","","1","6","To deal with the spectrum-shortage problem, dynamic spectrum access (DSA) has attracted a great deal of attention in both academia and industry. In DSA, secondary users (SUs) are allowed to exploit the whitespace of the primary users (PUs) on an instant-by-instant basis. The goal is to improve the system’s spectral utilization efficiency in a manner that limits the interference from SUs to PUs. To this end, in this paper, we proposed an usage aware spectrum access scheme by exploiting deep reinforcement learning. We evaluated its performance by extensive simulations, and validate the superiority of the proposed scheme by comparing it with existing methods.","1882-5621","978-1-6654-2760-9","10.1109/WPMC52694.2021.9700468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9700468","Dynamic spectrum access;deep reinforcement learning;spectral utilization efficiency","Wireless communication;Industries;Simulation;Dynamic spectrum access;Reinforcement learning;Interference;Complexity theory","cognitive radio;deep learning (artificial intelligence);radio spectrum management;reinforcement learning;telecommunication computing","DSA;secondary users;primary users;instant-by-instant basis;usage aware spectrum access scheme;deep reinforcement learning;spectrum-shortage problem;dynamic spectrum access","","3","","13","IEEE","7 Feb 2022","","","IEEE","IEEE Conferences"
"DDPG-based Deep Reinforcement Learning for Loitering Munition Mobility Control: Algorithm Design and Visualization","H. Lee; W. J. Yun; S. Jung; J. -H. Kim; J. Kim","School of Electrical Engineering, Korea University, Seoul, Republic of Korea; School of Electrical Engineering, Korea University, Seoul, Republic of Korea; School of Software, Hallym University, Chuncheon, Republic of Korea; Department of Electrical and Computer Engineering, Ajou University, Suwon, Republic of Korea; School of Electrical Engineering, Korea University, Seoul, Republic of Korea","2022 IEEE VTS Asia Pacific Wireless Communications Symposium (APWCS)","10 Oct 2022","2022","","","112","116","Drone technology is estimated for its potential to be applied in many industries, including logistics, broadcasting, telecommunications, and warfare technology. In particular, in the field of modern warfare such as the current war in Ukraine, the use of drones has become an essential element. This paper includes a loitering munition to attack a single ground target in the scenario. A simulation environment for drone attack is built based on the 3D platform Unity, and learning is performed by applying DDPG, a reinforcement learning algorithm that can be used in continuous action space. Through the specific result, it is possible to achieve our purpose to attack target exactly.","","978-1-6654-7121-3","10.1109/APWCS55727.2022.9906493","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9906493","Drone;Loitering munition;Reinforcement learning;DDPG;Unity","Wireless communication;Industries;Visualization;Solid modeling;Three-dimensional displays;Weapons;Reinforcement learning","autonomous aerial vehicles;data visualisation;deep learning (artificial intelligence);military aircraft;military computing;reinforcement learning;robot vision;stereo image processing;target tracking;weapons","single ground target;drone attack;DDPG-based deep reinforcement learning;algorithm design;loitering munition mobility control;3D Unity platform;visualization;ontinuous action space","","1","","18","IEEE","10 Oct 2022","","","IEEE","IEEE Conferences"
"Fuzzy logic control with reinforcement learning on slow time-varying electro-mechanical systems","R. Pantonial; S. M. Famador","Motion Control Team, Corporation Cebu City, Philippines; Department of Computer Science, University of the Philippines-Cebu College Cebu City, Philippines","2014 IEEE Symposium on Industrial Electronics & Applications (ISIEA)","28 Sep 2017","2014","","","177","182","This paper demonstrates the design and implementation of a Fuzzy Logic Controller (FLC) with Reinforcement Learning (RL) on Electro-mechanical system in the industrial and commercial setting. FLC, which is a nonlinear controller, is based on the linguistic description of the system and not its mathematical model. RL on the other hand is a class of learning task wherein an agent maximizes a scalar evaluation by environment interaction. Combining these two schemes, an online adaptation is achieved on FLC without the need of a training set. FLC design is presented first in which expert knowledge are incorporated at the start of a system's life to reduce heavy learning phase. Afterwards, RL is added to fine tune the conclusion part of the FLC. Then, a C-based Simulation is demonstrated on a position controlled time-varying electro-mechanical system. The experimental results show the plausibility and applicability of such design in the industry.","2472-7660","978-1-4799-5590-9","10.1109/ISIEA.2014.8049894","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8049894","Fuzzy Logic Controller;Reinforcement Learning;Electro-mechanical Plant","Fuzzy logic;Learning (artificial intelligence);Motion control;Industrial electronics;Industries;Adaptation models","control engineering computing;control system synthesis;expert systems;fuzzy control;learning (artificial intelligence);mathematical analysis;nonlinear control systems;position control;time-varying systems","fuzzy logic control;reinforcement learning;slow time-varying electromechanical system;RL;nonlinear controller;linguistic description;mathematical model;environment interaction;FLC design;expert knowledge;C-based Simulation;position controlled time-varying electromechanical system;scalar evaluation maximization","","","","20","IEEE","28 Sep 2017","","","IEEE","IEEE Conferences"
"Improving Simulated Annealing Algorithm for FPGA Placement Based on Reinforcement Learning","C. Tian; L. Chen; Y. Wang; S. Wang; J. Zhou; Y. Zhang; G. Li","College of Integrated Circuits, Peking University, Beijing, China; Beijing Microelectronic Technology Institute, Beijing, China; College of Integrated Circuits, Peking University, Beijing, China; Beijing Microelectronic Technology Institute, Beijing, China; Beijing Microelectronic Technology Institute, Beijing, China; Beijing Microelectronic Technology Institute, Beijing, China; Beijing Microelectronic Technology Institute, Beijing, China","2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","3 Aug 2022","2022","10","","1912","1919","As the increasing complexity and capacity of large-scale integrated circuit devices, Field Programmable Gate Array (FPGA) has been widely concerned and applied with its high degree of concurrency., customizable and reconfigurable features. Thereby., the importance of efficient Electronic Design Automation (EDA) tools for modern FPGA is hard to overestimate. As a key link in the FPGA EDA design flow, the significance of placement technology for FPGA is self-evident. Simulated annealing is widely used in FPGA placement as an independent algorithm or an enhancement step of analytical placement algorithms. However, as the inherent properties of circuit cannot be utilized by the traditional simulated annealing algorithm, it depends on tremendous random swap operations, which is very time-consuming, making it cannot keep up with the increasing design scale and FPGA chip resources. In this paper, we propose an improving simulated annealing placer based on reinforcement learning. Many types of search region construction methods are proposed in which the placer can explore the solution space more efficiently, and thus can avoid exploring the redundant design space. Then, an intelligent strategy for selecting the most powerful search regions based on reinforcement learning is investigated to further ameliorate the applicability of the placer. Experimental results show that the proposed scheme can reduce the runtime while maintaining the wirelength and critical path delays compared with the simulated annealing algorithm.","2693-2865","978-1-6654-2207-9","10.1109/ITAIC54216.2022.9836761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9836761","FPGA;reinforcement learning;simulated annealing;placement","Technological innovation;Runtime;Design automation;Simulated annealing;Reinforcement learning;Logic gates;Space exploration","electronic design automation;field programmable gate arrays;simulated annealing","reconfigurable features;efficient Electronic Design Automation tools;modern FPGA;FPGA EDA design flow;placement technology;FPGA placement;independent algorithm;analytical placement algorithms;traditional simulated annealing algorithm;tremendous random swap operations;increasing design scale;placer;reinforcement learning;search region construction methods;redundant design space;improving simulated annealing algorithm;large-scale integrated circuit devices;Field Programmable Gate Array;customizable features","","","","18","IEEE","3 Aug 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-Based Pin Map Optimization of BGA Package for EMC","R. Song; R. Xu; Z. Gu; E. -P. Li","Advanced Micro/Nano Electronic Devices and Smart Systems and Applications the College of ISEE, Zhejiang University, Hangzhou, China; Advanced Micro/Nano Electronic Devices and Smart Systems and Applications the College of ISEE, Zhejiang University, Hangzhou, China; Advanced Micro/Nano Electronic Devices and Smart Systems and Applications the College of ISEE, Zhejiang University, Hangzhou, China; Advanced Micro/Nano Electronic Devices and Smart Systems and Applications the College of ISEE, Zhejiang University, Hangzhou, China","2023 International Applied Computational Electromagnetics Society Symposium (ACES-China)","22 Sep 2023","2023","","","01","03","The radiation from the solder balls in the Ball Grid Array (BGA) packaging has become one of the main sources of electromagnetic leaks, and the pin mapping significantly impacts the package's electromagnetic compatibility (EMC). In this paper, a deep reinforcement learning (DRL)-based pin distribution optimization is proposed. Combined with the VBGT-Matrix fast calculation method, it can learn the radiation characteristics of various environments through self-training and then produce targeted pin map solutions. The outcome demonstrates that the optimization algorithm has fast speed and low power consumption, verifying the design expertise of the present industry and providing additional design insights.","","978-1-7335096-5-7","10.23919/ACES-China60289.2023.10249549","National Natural Science Foundation of China(grant numbers:62201499,62071424,62027805); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10249549","component;Ball Grid Array (BGA) Packaging;pin map;Deep reinforcement learning (DRL);Electromagnetic Compatibility (EMC)","Industries;Costs;Power demand;Reinforcement learning;Packaging;Electromagnetic compatibility;Large scale integration","","","","","","7","","22 Sep 2023","","","IEEE","IEEE Conferences"
"Fault Localization for Reinforcement Learning","J. Morán; A. Bertolino; C. De La Riva; J. Tuya","Department of Computing, University Of Oviedo, Gijón, Spain; ISTI-CNR, Consiglio Nazionale delle Ricerche, Pisa, Italy; Department of Computing, University Of Oviedo, Gijón, Spain; Department of Computing, University Of Oviedo, Gijón, Spain","2023 IEEE International Conference On Artificial Intelligence Testing (AITest)","29 Aug 2023","2023","","","49","50","Reinforcement Learning is widely adopted in industry to approach control tasks in intelligent way. The quality of these programs is important especially when they are used for critical tasks like autonomous driving. Testing and debugging these programs are complex because they behave autonomously without providing insights about the reasons of the decisions taken. Even these decisions could be wrong if they learned from faults. In this paper, we present the first approach to automatically locate faults in Reinforcement Learning programs. This approach called SBFL4RL analyses several executions to extract those internal states that commonly reduce the performance of the program when they are covered. Locating these states can help testers to understand a known fault, or even detect an unknown fault. SBFL4RL is validated in 2 case studies locating correctly an injected fault. Initial results suggest that the faults of reinforcement learning programs can be automatically located, and there is room for further research.","2835-3560","979-8-3503-3629-0","10.1109/AITest58265.2023.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10229476","software testing;debugging;fault localization;reinforcement learning","Location awareness;Software testing;Industries;Reinforcement learning;Debugging;Task analysis;Artificial intelligence","fault diagnosis;learning (artificial intelligence);program debugging;program testing;reinforcement learning;software fault tolerance","approach control tasks;autonomous driving;correctly an injected fault;fault localization;known fault;reinforcement learning programs;SBFL4RL analyses several executions;unknown fault","","","","8","IEEE","29 Aug 2023","","","IEEE","IEEE Conferences"
"Performance analysis of Reinforcement Learning for Miner Selection in Blockchain","R. R. Mahatungade; P. Patil; M. K. Pawar","Department of CSE, KLE Technological University, Hubli, India; Department of MCA, KLE Technological University, Hubli, India; Department of CSE, KLE Technological University, Hubli, India","2023 4th International Conference for Emerging Technology (INCET)","10 Jul 2023","2023","","","1","4","Blockchain is an emerging technology in the industry and business applications because of its characteristics such as tamper-proof, decentralized, immutable, transparent, and secure. Though it has several advantages, it still suffers from main challenges like privacy and scalability. Scalability is defined in terms of throughput, capacity, and response time. The response time includes the time of adding the block to the Blockchain. It consists of the miner's time who adds the block to the blockchain. The paper proposes a reinforcement learning method to improve the scalability of blockchain systems. The proposed methodology discusses the performance analysis of the reinforcement learning algorithm to select the miners for block mining.","","979-8-3503-3575-0","10.1109/INCET57972.2023.10170107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10170107","Blockchain;Miners;Scalability;Consensus Algorithm;Reinforcement Learning Algorithm","Industries;Privacy;Scalability;Reinforcement learning;Throughput;Blockchains;Performance analysis","blockchains;cryptocurrencies;data mining;data privacy;reinforcement learning","blockchain systems;business applications;miner selection;miners;performance analysis;reinforcement learning method;response time;tamper-proof","","","","17","IEEE","10 Jul 2023","","","IEEE","IEEE Conferences"
"Simulation of Vehicle Interaction Behavior in Merging Scenarios: A Deep Maximum Entropy- Inverse Reinforcement Learning Method Combined with Game Theory","W. Li; F. Qiu; L. Li; Y. Zhang; K. Wang","Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Education, Chongqing University of Technology, Chongqing, China; Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Education, Chongqing University of Technology, Chongqing, China; Transportation and Autonomous Systems Institute (TASI), Indiana University'Purdue University Indianapolis, Indianapolis, IN, USA; Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Education, Chongqing University of Technology, Chongqing, China; Department of Automotive Active Safety Testing Technology, Chongqing Key Laboratory of Industry and Informatization, Chongqing, China","IEEE Transactions on Intelligent Vehicles","","2023","PP","99","1","15","Simulation testing based on virtual scenarios can improve the efficiency of safety testing for high-level autonomous vehicles (AVs). In most traffic scenarios, such as merging scenarios, the interactions between vehicles are a game process. Therefore, a critical factor is to accurately simulate the game and interaction processes between the background vehicle (BV) and AV in the test environment. With the increasing availability of natural driving data, a data-driven approach can be introduced to identify the underlying driving behavior patterns in actual driving data. Thus, this paper proposes a data-driven method for modeling BV behavior for AV testing in virtual scenarios. The method describes the vehicle decision process in the merging scenario as a standard Markov decision process (MDP). Based on game theory, we considered the BV as a game subject to illustrate the vehicle interaction process. Furthermore, a deep maximum entropy-inverse reinforcement learning combined with the game matrix is proposed to identify the reward function that describes BV behavior. The obtained reward function is used to design a deep Q-network algorithm to simulate the behavior of BV. Finally, the effectiveness and feasibility of the proposed method are verified by comparing it with natural driving data. Moreover, we performed comparative tests with the other two baseline methods; the results show that the proposed method can accurately simulate the interaction behaviors between vehicles in the virtual scenarios.","2379-8904","","10.1109/TIV.2023.3323138","Major Project of Chongqing Technology Innovation and Application Development Special Project(grant numbers:CSTB2022TIAD-STX0003); National Natural Science Foundation of China(grant numbers:cstc2021jcyj-msxmX0183); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10274818","data-driven;deep reinforcement learning;game theory;inverse reinforcement learning","Games;Behavioral sciences;Merging;Reinforcement learning;Testing;Vehicle dynamics;Roads","","","","","","","IEEE","9 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Deep Reinforcement Learning-Based Recommender Algorithm Optimization and Intelligent Systems Construction for Business Data Analysis","F. Xingyu; L. Yang","Xidian University, Xi’an, China; Marquette University, Wisconsin Milwaukee, USA","2022 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)","23 May 2022","2022","","","402","405","Business analysis plays an essential role in numerous companies and industries, which involves data collection, data analysis, and the construction of intelligent systems to make critical business decisions. By using a data model to filter through consumers' data, it is easier than ever to make recommendations to consumers for what they would enjoy using or purchasing. Added a real-time product Recommendation System framework, the Reinforcement Learning model is used to automatically learn the optimal recommendation strategy. Also, the Deep Q-Network (DQN) algorithm is modified by replacing the convolutional neural network with the Long Short-Term Memory (LSTM). The solution analyzes and predicts consumers' preferences in real-time, and makes up for the lack of iterative optimization of traditional Recommendation Systems in real scenarios.","","978-1-6654-0902-5","10.1109/IPEC54454.2022.9777623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777623","Business Analysis;Recommend System;Reinforcement Learning;DQN;LSTM","Industries;Data analysis;Computational modeling;Reinforcement learning;Prediction algorithms;User experience;Real-time systems","data analysis;learning (artificial intelligence);neural nets;recommender systems","iterative optimization;traditional Recommendation Systems;Deep Reinforcement;recommender algorithm optimization;intelligent Systems construction;business data analysis;business analysis;numerous companies;data collection;critical business decisions;data model;consumers;real-time product Recommendation System framework;Reinforcement Learning model;optimal recommendation strategy;Deep Q-Network algorithm;convolutional neural network","","","","9","IEEE","23 May 2022","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning-based resource allocation mechanism for XR applications*","B. Feng","College of Electronic Information and Optical Engineering, Nankai University, Tianjin","2023 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)","16 Aug 2023","2023","","","1","6","Recently, the attention of academia and industry has turned towards 6G technology. Within 6G, eXtended Reality (XR) has emerged as an important scenario that is attracting researchers due to its requirements for computation-intensive, latency-sensitive, and high-bandwidth applications. Efficient resource allocation in XR is crucial for delivering a seamless and smooth user experience. However, existing research on resource allocation for XR applications does not consider the interaction between XR content and users. In this paper, we propose a deep Q network(DQN) approach to minimize service latency in XR applications. We formulate the problem as a Markov Decision Process(MDP) and apply DQN as the solution. Simulation results demonstrate the efficiency of the proposed scheme in terms of both latency and acceptance ratio.","2155-5052","979-8-3503-2152-4","10.1109/BMSB58369.2023.10211567","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10211567","XR;Reinforcement learning;Resource Allocation;6G","6G mobile communication;Industries;Extended reality;Simulation;Markov processes;Broadcasting;User experience","6G mobile communication;deep learning (artificial intelligence);Markov processes;reinforcement learning;resource allocation;telecommunication computing","deep reinforcement learning-based resource allocation mechanism;high-bandwidth applications;seamless user experience;smooth user experience;XR applications","","","","16","IEEE","16 Aug 2023","","","IEEE","IEEE Conferences"
"Reinforcement learning-based joint task offloading and migration schemes optimization in mobility-aware MEC network","D. Wang; X. Tian; H. Cui; Z. Liu","Key Laboratory of Universal Wireless Communications, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Universal Wireless Communications, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Universal Wireless Communications, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Universal Wireless Communications, Beijing University of Posts and Telecommunications, Beijing, China","China Communications","9 Sep 2020","2020","17","8","31","44","Intelligent edge computing carries out edge devices of the Internet of things (IoT) for data collection, calculation and intelligent analysis, so as to proceed data analysis nearby and make feedback timely. Because of the mobility of mobile equipments (MEs), if MEs move among the reach of the small cell networks (SCNs), the offloaded tasks cannot be returned to MEs successfully. As a result, migration incurs additional costs. In this paper, joint task offloading and migration schemes in mobility-aware Mobile Edge Computing (MEC) network based on Reinforcement Learning (RL) are proposed to obtain the maximum system revenue. Firstly, the joint optimization problems of maximizing the total revenue of MEs are put forward, in view of the mobility-aware MEs. Secondly, considering time-varying computation tasks and resource conditions, the mixed integer non-linear programming (MINLP) problem is described as a Markov Decision Process (MDP). Then we propose a novel reinforcement learning-based optimization framework to work out the problem, instead traditional methods. Finally, it is shown that the proposed schemes can obviously raise the total revenue of MEs by giving simulation results.","1673-5447","","10.23919/JCC.2020.08.003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9190128","MEC;computation offloading;mobility-aware;migration scheme;Markov decision process;reinforcement learning","Task analysis;Servers;Computational modeling;Resource management;Optimization;Wireless communication;Learning (artificial intelligence)","cellular radio;integer programming;learning (artificial intelligence);linear programming;manufacturing systems;Markov processes;mobile computing;nonlinear programming;optimisation","migration schemes optimization;mobility-aware MEC network;intelligent edge computing;edge devices;intelligent analysis;data analysis;mobile equipments;offloaded tasks;migration incurs additional costs;joint task offloading;mobility-aware Mobile Edge Computing network;maximum system revenue;joint optimization problems;total revenue;mobility-aware MEs;time-varying computation tasks;mixed integer nonlinear programming problem;reinforcement learning-based optimization framework","","33","","","","9 Sep 2020","","","IEEE","IEEE Magazines"
"Parameter Optimization of VLSI Placement Through Deep Reinforcement Learning","A. Agnesina; K. Chang; S. K. Lim","Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","20 Mar 2023","2023","42","4","1295","1308","Critical to achieving power–performance–area goals, a human engineer typically spends a considerable amount of time tuning the multiple settings of a commercial placer. This article proposes a deep reinforcement learning (RL) framework to optimize the placement parameters of a commercial electronic design automation (EDA) tool. We build an autonomous agent that learns to tune parameters without human intervention and domain knowledge, trained solely by RL from self-search. To generalize to unseen netlists, we use a mixture of handcrafted features from graph topology theory and graph embeddings generated using unsupervised graph neural networks. Our RL algorithms are chosen to overcome the sparsity of data and latency of placement runs. As a result, our trained RL agent achieves up to 11% and 2.5% wire length improvements on unseen netlists compared with a human engineer and a state-of-the-art tool auto-tuner in just one placement iteration ( $20\times $  and  $50\times $  fewer iterations). In addition, the success of the RL agent is measured using a statistical test with theoretical guarantees and an optimized sample size.","1937-4151","","10.1109/TCAD.2022.3193647","National Science Foundation(grant numbers:CNS 16-24731); Industry Members of the Center for Advanced Electronics in Machine Learning; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9839293","Deep learning;physical design;very-large-scale integration (VLSI) placement","Tuning;Wires;Optimization;Engines;Very large scale integration;Reinforcement learning;Physical design","deep learning (artificial intelligence);electronic design automation;graph neural networks;graph theory;optimisation;reinforcement learning;unsupervised learning;VLSI","autonomous agent;commercial electronic design automation tool;commercial placer;deep reinforcement learning framework;domain knowledge;EDA tool;graph embeddings;graph topology theory;handcrafted features;human engineer;human intervention;multiple settings;optimized sample size;parameter optimization;placement iteration;placement parameters;placement runs;power performance;RL algorithms;state-of-the-art tool auto-tuner;trained RL agent;unsupervised graph neural networks;VLSI placement;wire length improvements","","1","","32","IEEE","25 Jul 2022","","","IEEE","IEEE Journals"
"MSN: Mapless Short-Range Navigation Based on Time Critical Deep Reinforcement Learning","B. Li; Z. Huang; T. W. Chen; T. Dai; Y. Zang; W. Xie; B. Tian; K. Cai","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Faculty of Sciences, Engineering and Technology, School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Command and Control, Army Engineering University of PLA, Nanjing, China; Tianbot Robotics Company Ltd., Science and Innovation Base, Nanjing Economic and Technological Development Zone, Nanjing, China; College of Automation, Zhongkai University of Agriculture and Engineering, Guangzhou, China","IEEE Transactions on Intelligent Transportation Systems","2 Aug 2023","2023","24","8","8628","8637","Automated vehicle(AV) based on reinforcement learning is an important part of the intelligent transportation system. However, currently, the performance of AV heavily that relies on the quality of maps and mapless navigation is one potential method for navigation in a strange and dynamic changing environment. Although many efforts are made on mapless navigation, they either need prior knowledge, rely on an exceptional constructed environment or simple feature fusion mechanism in the networks. In this paper, we proposed a deep reinforcement learning method, namely TC-DDPG, which is consisted of DDPG, multi-challenge deep learning networks and time-critical reward function. By comparing to existing approaches, TC-DDPG takes the cost of time into consideration and achieves better performance and converges more easily. A new open source simulator is proposed and extensive experiments are conducted to demonstrate the performance of the TC-DDPG, which outperforms comparing methods and achieves 62.9% less in time cost, 12.0% less in distance cost and about 90% fewer in numbers of model parameters.","1558-0016","","10.1109/TITS.2022.3192480","National Natural Science Foundation of China(grant numbers:61728204); Key Laboratory of Safety Critical Software through the Ministry of Industry and Information Technology(grant numbers:NJ2018014); Fundamental Research Funds for the Central Universities(grant numbers:NP2020415); Guangzhou Science and Technology Program(grant numbers:202002030246); Special Projects in Key Areas of Guangdong’s Colleges(grant numbers:2021ZDZX4061); CCF-Huawei Database System Innovation Research Plan(grant numbers:CCF HUAWEIDBIR2020001A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9861762","Reinforcement learning;mapless navigation;DDPG;path planning;robot motion planning","Navigation;Robots;Reinforcement learning;Service robots;Collision avoidance;Production facilities;Transportation","deep learning (artificial intelligence);intelligent transportation systems;mobile robots;navigation;path planning;reinforcement learning","achieves;automated vehicle;AV;deep reinforcement learning method;dynamic changing environment;exceptional constructed environment;intelligent transportation system;mapless navigation;mapless short-range navigation;multichallenge deep learning networks;simple feature fusion mechanism;strange changing environment;TC-DDPG;time critical deep reinforcement learning;time-critical reward function","","","","44","IEEE","18 Aug 2022","","","IEEE","IEEE Journals"
"A novel estimation of distribution algorithm using graph-based chromosome representation and reinforcement learning","X. Li; B. Li; S. Mabu; K. Hirasawa","Graduate School of Information, Production and Systems, Waseda University, Japan; Graduate School of Information, Production and Systems, Waseda University, Japan; Graduate School of Information, Production and Systems, Waseda University, Japan; Graduate School of Information, Production and Systems, Waseda University, Japan","2011 IEEE Congress of Evolutionary Computation (CEC)","14 Jul 2011","2011","","","37","44","This paper proposed a novel EDA, where a directed graph network is used to represent its chromosome. In the proposed algorithm, a probabilistic model is constructed from the promising individuals of the current generation using reinforcement learning, and used to produce the new population. The node connection probability is studied to develop the probabilistic model, therefore pairwise interactions can be demonstrated to identify and recombine building blocks in the proposed algorithm. The proposed algorithm is applied to a problem of agent control, i.e., autonomous robot control. The experimental results show the superiority of the proposed algorithm comparing with the conventional algorithms.","1941-0026","978-1-4244-7835-4","10.1109/CEC.2011.5949595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949595","","Probabilistic logic;Economic indicators;Biological cells;Robot sensing systems;Mobile robots;Learning","directed graphs;genetic algorithms;intelligent robots;learning (artificial intelligence);mobile robots;multi-agent systems;probability","distribution algorithm;graph-based chromosome representation;reinforcement learning;EDA;directed graph network;probabilistic model;node connection probability;pairwise interactions;agent control;autonomous robot control;conventional algorithms","","11","","28","IEEE","14 Jul 2011","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Detection for State Estimation Under False Data Injection","W. Jiang; W. Yang; J. Zhou; W. Ding; Y. Luo; Y. Liu","Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China","IEEE Access","7 May 2021","2021","9","","66498","66508","We consider the problem of network security under false data injection attacks over wireless sensor networks.To resist the attacks which can inject false data into communication channels according to a certain probability, we formulate the online attack detection problem as a partially observable Markov decision process problem and design a detector for each sensor based on the framework of model-free reinforcement learning. By numerical simulations, we illustrate the effectiveness of the proposed reinforcement learning algorithm and show the performance of the proposed detector compared with the typical detector in the existing works.","2169-3536","","10.1109/ACCESS.2021.3076538","National Natural Science Foundation of China(grant numbers:61973123); projects sponsored by the Development Fund for Shanghai Talents; Shanghai Natural Science Foundation(grant numbers:18ZR1409700); Programme of Introducing Talents of Discipline to Universities (the 111 Project)(grant numbers:B17017); Shuguang Program supported by the Shanghai Education Development Foundation and Shanghai Municipal Education Commission; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9419373","Wireless sensor network;false data injection attack;reinforcement learning;partially observable Markov decision process","Detectors;Reinforcement learning;Technological innovation;Wireless sensor networks;Smart grids;Markov processes;Image edge detection","learning (artificial intelligence);Markov processes;probability;state estimation;wireless sensor networks","reinforcement learning-based detection;state estimation;network security;false data injection attacks;wireless sensor networks;communication channels;online attack detection problem;partially observable Markov decision process problem;model-free reinforcement learning;reinforcement learning algorithm","","3","","29","CCBY","29 Apr 2021","","","IEEE","IEEE Journals"
"A continuous estimation of distribution algorithm by evolving graph structures using reinforcement learning","X. Li; B. Li; S. Mabu; K. Hirasawa","Graduate School of Information, Production and Systems, Waseda University, Japan; Graduate School of Information, Production and Systems, Waseda University, Japan; Graduate School of Information, Production and Systems, Waseda University, Japan; Graduate School of Information, Production and Systems, Waseda University, Japan","2012 IEEE Congress on Evolutionary Computation","2 Aug 2012","2012","","","1","8","A novel graph-based Estimation of Distribution Algorithm (EDA) named Probabilistic Model Building Genetic Network Programming (PMBGNP) has been proposed. Inspired by classical EDAs, PMBGNP memorizes the current best individuals and uses them to estimate a distribution for the generation of the new population. However, PMBGNP can evolve compact programs by representing its solutions as graph structures. Therefore, it can solve a range of problems different from conventional ones in EDA literature, such as data mining and Reinforcement Learning (RL) problems. This paper extends PMBGNP from discrete to continuous search space, which is named PMBGNP-AC. Besides evolving the node connections to determine the optimal graph structures using conventional PMBGNP, Gaussian distribution is used for the distribution of continuous variables of nodes. The mean value μ and standard deviation σ are constructed like those of classical continuous Population-based incremental learning (PBILc). However, a RL technique, i.e., Actor-Critic (AC), is designed to update the parameters (μ and σ). AC allows us to calculate the Temporal-Difference (TD) error to evaluate whether the selection of the continuous value is better or worse than expected. This scalar reinforcement signal can decide whether the tendency to select this continuous value should be strengthened or weakened, allowing us to determine the shape of the probability density functions of the Gaussian distribution. The proposed algorithm is applied to a RL problem, i.e., autonomous robot control, where the robot's wheel speeds and sensor values are continuous. The experimental results show the superiority of PMBGNP-AC comparing with the conventional algorithms.","1941-0026","978-1-4673-1509-8","10.1109/CEC.2012.6256481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6256481","","","Gaussian distribution;genetic algorithms;graph theory;learning (artificial intelligence);mobile robots;search problems","continuous estimation;evolving graph structures;reinforcement learning;graph-based estimation of distribution algorithm;graph-based EDA;probabilistic model building genetic network programming;continuous search space;PMBGNP-AC;optimal graph structure determination;Gaussian distribution;node connections;continuous variables distribution;standard deviation;mean value;continuous population-based incremental learning;PBILc;RL technique;actor-critic technique;temporal difference error;TD error;continuous value selection;scalar reinforcement signal;probability density functions;autonomous robot control;wheel speeds;sensor values","","3","","27","IEEE","2 Aug 2012","","","IEEE","IEEE Conferences"
"Elevator group supervisory control system using genetic network programming with reinforcement learning","J. Zhou; T. Eguchi; K. Hirasawa; J. Hu; S. Markon","Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; World Headquarters, Fujitec Company Limited, Ibaraki, Osaka, Japan","2005 IEEE Congress on Evolutionary Computation","12 Dec 2005","2005","1","","336","342 Vol.1","Since genetic network programming (GNP) has been proposed as a new method of evolutionary computation, many studies have been done on its applications which cover not only virtual world problems but also real world systems like elevator group supervisory control system (EGSCS) which is a very large scale stochastic dynamic optimization problem. From those researches, most of the significant features of GNP have been verified comparing to genetic algorithm (GA) and genetic programming (GP). Especially, the improvement of the performances on EGSCS using GNP showed an interesting and promising prospect in this field. On the other hand, some studies based on GNP with reinforcement learning (RL) revealed a better performance over conventional GNP on some problems such as tile-world models. As a basic study, reinforcement learning is introduced in this paper expecting to enhance EGSCS controller using GNP","1941-0026","0-7803-9363-5","10.1109/CEC.2005.1554703","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1554703","","Elevators;Supervisory control;Learning;Economic indicators;Genetic programming;Dynamic programming;Evolutionary computation;Large-scale systems;Stochastic systems;Optimization methods","control system analysis computing;controllers;directed graphs;dynamic programming;genetic algorithms;learning (artificial intelligence);lifts;stochastic programming","elevator group supervisory control system controller;genetic network programming;reinforcement learning;evolutionary computation;stochastic dynamic optimization problem","","2","","15","IEEE","12 Dec 2005","","","IEEE","IEEE Conferences"
"Guiding the evolution of Genetic Network Programming with reinforcement learning","Q. Meng; S. Mabu; Y. Wang; K. Hirasawa","Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan","IEEE Congress on Evolutionary Computation","27 Sep 2010","2010","","","1","8","Genetic Network Programming (GNP) is one of the evolutionary algorithms. It adopts a directed graph structure to represent a solution to a given problem. Agents judge situations and execute actions sequentially following the node transitions in the graph. On one hand, GNP possesses an advantage of node reusability, which makes it possible to realize a compact graph structure that represents a solution. On the other hand, the compact structure suggests that any connection might play a significant role in the solution, i.e., a slight change to the connections could tremendously influence the performance of the agents for the given task. The conventional GNP, however, lacks an effective way to evaluate and to take advantage of the connections. This paper thus proposes a reinforcement learning approach to learn GNP's subgraphs that contain a relatively small number of connections, and further proposes a partial reconstruction approach to modify the solution with the obtained subgraphs. These two approaches are combined together to form a new evolutionary learning model named GNP with Evolution-oriented Reinforcement Learning (GNP-ERL). Some experiments are conducted on the Tileworld testbed to verify the effectiveness of GNP-ERL, and the simulation results demonstrate that it outperforms the conventional GNP in both training and testing phases.","1941-0026","978-1-4244-6911-6","10.1109/CEC.2010.5586398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586398","","Economic indicators;Learning;Programming;Evolutionary computation;Genetic programming;Delay effects","directed graphs;genetic algorithms;learning (artificial intelligence)","genetic network programming;evolutionary algorithm;directed graph structure;node reusability;evolutionary learning model;evolution-oriented reinforcement learning","","1","","12","IEEE","27 Sep 2010","","","IEEE","IEEE Conferences"
"Decision-making with Triple Density Awareness for Autonomous Driving using Deep Reinforcement Learning","S. Zhang; Y. Wu; H. Ogai; S. Tateno","Graduate School of Information, Production and Systems, Waseda University, Kitakyushu, Japan; Graduate School of Information, Production and Systems, Waseda University, Kitakyushu, Japan; Graduate School of Information, Production and Systems, Waseda University, Kitakyushu, Japan; Graduate School of Information, Production and Systems, Waseda University, Kitakyushu, Japan","2022 IEEE 96th Vehicular Technology Conference (VTC2022-Fall)","18 Jan 2023","2022","","","1","7","Deep reinforcement learning (DRL) has been a recent trend to solve autonomous driving decision-making (DM) problems. The goals of DM are to produce actions with ensuring safety and improving efficiency according to the information of perception and localization. Intuitively, the complexity and density of the driving scene will affect the DM while few works focus on it. This work is the first to integrate density characteristics in DRL. Specifically, we model three density indicators, 1) local density to describe the vehicle density relationship around the ego vehicle, 2) lane density to describe the denseness of vehicles on each lane, 3) global density to describe the denseness of current region of interest of DM. Among them, local density and lane density are considered as input features to make the agent aware of density properties to distinguish dense scenes while global density is used as a factor of the reward function to balance the safety and efficiency. The agent learns to make full use of density information to help the current DM process through learning. We train and test the models in an environment with different density scenarios. The experimental results show that considering density in DRL significantly helps the agent learn to make more secure and efficient decisions for autonomous driving.","2577-2465","978-1-6654-5468-1","10.1109/VTC2022-Fall57202.2022.10013060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10013060","autonomous driving;decision-making;deep reinforcement learning;density","Deep learning;Measurement;Location awareness;Adaptation models;Vehicular and wireless technologies;Decision making;Reinforcement learning","decision making;deep learning (artificial intelligence);reinforcement learning;traffic engineering computing","autonomous driving decision-making problems;deep reinforcement learning;dense scenes;density characteristics;density indicators;density information;density properties;DM process;driving scene;DRL;ego vehicle;global density;triple density awareness;vehicle density relationship","","","","32","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning-based Collision Avoidance for UAV","Ð. Jevtić; Z. Miljković; M. Petrović; A. Jokić","Department of Production Engineering, Laboratory for industrial robotics and artificial intelligence (ROBOTICS&AI), Kraljice Marije 16, University of Belgrade-Faculty of Mechanical Engineering, Belgrade 35, The Republic of Serbia; Department of Production Engineering, Laboratory for industrial robotics and artificial intelligence (ROBOTICS&AI), Kraljice Marije 16, University of Belgrade-Faculty of Mechanical Engineering, Belgrade 35, The Republic of Serbia; Department of Production Engineering, Laboratory for industrial robotics and artificial intelligence (ROBOTICS&AI), Kraljice Marije 16, University of Belgrade-Faculty of Mechanical Engineering, Belgrade 35, The Republic of Serbia; Department of Production Engineering, Laboratory for industrial robotics and artificial intelligence (ROBOTICS&AI), Kraljice Marije 16, University of Belgrade-Faculty of Mechanical Engineering, Belgrade 35, The Republic of Serbia","2023 10th International Conference on Electrical, Electronic and Computing Engineering (IcETRAN)","27 Jul 2023","2023","","","1","6","One of the significant aspects for enabling the intelligent behavior to the Unmanned Aerial Vehicles (UAVs) is by providing an algorithm for navigation through the dynamic and unseen environment. Therefore, to be autonomous, they need sensors to perceive their surroundings and utilize gathered information to decide which action to take. Having that in mind, in this paper, the authors designed the system for obstacle avoidance and also investigate the elements of the Markov decision process and their influence on each other. The flying mobile robot used within the considered problem is quadrotor type and has an integrated Lidar sensor which is utilized to detect obstacles. The sequential decision-making model based on Q-learning is trained within the MATLAB Simulink environment. The simulation results demonstrate that the UAV can navigate through the environment in most algorithm runs without colliding with surrounding obstacles.","","979-8-3503-0711-5","10.1109/IcETRAN59631.2023.10192168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10192168","unmanned aerial vehicles;collision avoidance;reinforcement learning;Q-learning","Q-learning;Laser radar;Uncertainty;Navigation;Simulation;Heuristic algorithms;Autonomous aerial vehicles","autonomous aerial vehicles;collision avoidance;decision making;helicopters;learning (artificial intelligence);Markov processes;mobile robots;navigation;optical radar;path planning;reinforcement learning","algorithm runs;dynamic environment;flying mobile robot;gathered information;integrated Lidar sensor;intelligent behavior;Markov decision process;MATLAB Simulink environment;navigation;obstacle avoidance;Q-learning;quadrotor type;reinforcement learning-based collision avoidance;sequential decision-making model;significant aspects;surrounding obstacles;UAV;Unmanned Aerial Vehicles;unseen environment","","","","7","IEEE","27 Jul 2023","","","IEEE","IEEE Conferences"
"Rover-IRL: Inverse Reinforcement Learning With Soft Value Iteration Networks for Planetary Rover Path Planning","M. Pflueger; A. Agha; G. S. Sukhatme","Department of Computer Science, University of Southern California, Los Angeles, CA, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Department of Computer Science, University of Southern California, Los Angeles, CA, USA","IEEE Robotics and Automation Letters","18 Feb 2019","2019","4","2","1387","1394","Planetary rovers, such as those currently on Mars, face difficult path planning problems, both before landing during the mission planning stages as well as once on the ground. In this work, we present a new approach to these planning problems based on inverse reinforcement learning using deep convolutional networks and value iteration networks (VIN) as important internal structures. VIN are an approximation of the value iteration (VI) algorithm implemented with convolutional neural networks to make VI fully differentiable. We propose a modification to the value iteration recurrence, referred to as the soft value iteration network (SVIN). SVIN is designed to produce more effective training gradients through the VIN. It relies on an internal soft policy model, where the policy is represented with a probability distribution over all possible actions, rather than a deterministic policy that returns only the best action. We demonstrate the effectiveness of our proposed architecture in both a grid world dataset as well as a highly realistic synthetic dataset generated from currently deployed rover mission planning tools and real Mars imagery.","2377-3766","","10.1109/LRA.2019.2895892","Achievement Rewards for College Scientists Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629318","Space robotics and automation;deep learning in robotics and automation;learning from demonstration","Planning;Reinforcement learning;Space vehicles;Orbits;Navigation;Path planning;Mars","control engineering computing;convolutional neural nets;learning (artificial intelligence);mobile robots;path planning;planetary rovers;probability","inverse reinforcement learning;deep convolutional networks;VIN;convolutional neural networks;soft value iteration network;internal soft policy model;rover-IRL;planetary rover path planning;rover mission;internal structures;probability distribution;deterministic policy;grid world dataset;realistic synthetic dataset;rover mission planning tools;real Mars imagery","","31","","20","IEEE","29 Jan 2019","","","IEEE","IEEE Journals"
"Formulation of deep reinforcement learning architecture toward autonomous driving for on-ramp merge","P. Wang; C. -Y. Chan","California PATH Program, University of California, Berkeley Richmond, California, U.S.; California PATH Program, University of California, Berkeley Richmond, California, U.S.","2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)","15 Mar 2018","2017","","","1","6","Multiple automakers have in development or in production automated driving systems (ADS) that offer freeway-pilot functions. This type of ADS is typically limited to restricted-access freeways only, that is, the transition from manual to automated modes takes place only after the ramp merging process is completed manually. One major challenge to extend the automation to ramp merging is that the automated vehicle needs to incorporate and optimize long-term objectives (e.g. successful and smooth merge) when near-term actions must be safely executed. Moreover, the merging process involves interactions with other vehicles whose behaviors are sometimes hard to predict but may influence the merging vehicle's optimal actions. To tackle such a complicated control problem, we propose to apply Deep Reinforcement Learning (DRL) techniques for finding an optimal driving policy by maximizing the long-term reward in an interactive environment. Specifically, we apply a Long Short-Term Memory (LSTM) architecture to model the interactive environment, from which an internal state containing historical driving information is conveyed to a Deep Q-Network (DQN). The DQN is used to approximate the Q-function, which takes the internal state as input and generates Q-values as output for action selection. With this DRL architecture, the historical impact of interactive environment on the long-term reward can be captured and taken into account for deciding the optimal control policy. The proposed architecture has the potential to be extended and applied to other autonomous driving scenarios such as driving through a complex intersection or changing lanes under varying traffic flow conditions.","2153-0017","978-1-5386-1526-3","10.1109/ITSC.2017.8317735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8317735","Autonomous Driving;Highway On-Ramp Merge;Deep Reinforcement Learning;Long Short-Term Memory;Deep Q-Network;Control Policy","Merging;Computer architecture;Machine learning;Acceleration;Learning (artificial intelligence);Conferences;Intelligent transportation systems","driver information systems;learning (artificial intelligence);optimal control;road traffic control;road vehicles;traffic engineering computing","deep reinforcement learning architecture;driving systems;ADS;freeway-pilot functions;restricted-access freeways;automated modes;ramp merging process;automation;automated vehicle;long-term objectives;near-term actions;vehicles whose behaviors;merging vehicle;complicated control problem;optimal driving policy;long-term reward;interactive environment;Long Short-Term Memory architecture;internal state;historical driving information;Deep Q-Network;DQN;action selection;DRL architecture;optimal control policy;autonomous driving scenarios;Q-function approximation;on-ramp merge;DRL techniques;automated driving systems","","70","","17","IEEE","15 Mar 2018","","","IEEE","IEEE Conferences"
"Automated Aircraft Stall Recovery using Reinforcement Learning and Supervised Learning Techniques","D. Singh Tomar; J. Gauci; A. Dingli; A. Muscat; D. Z. Mangion","Institute of Aerospace Technologies, University of Malta, Msida, Malta; Institute of Aerospace Technologies, University of Malta, Msida, Malta; Faculty of ICT, University of Malta, Msida, Malta; QuAero Ltd., Mosta, Malta; Institute of Aerospace Technologies, University of Malta, Msida, Malta","2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC)","15 Nov 2021","2021","","","1","7","Despite the on-board automation and protection systems of modern commercial aircraft, aerodynamic stall events are still a possible occurrence. This paper proposes Machine Learning algorithms – based on Reinforcement Learning and Supervised Learning – to automatically recover an aircraft from two types of aerodynamic stall: unaccelerated wings level (1G) stall and a stall during a turn. The algorithms were tested by exposing them to 105 simulated stall scenarios with different initial conditions (including altitude, bank angle and wind speed) and an acceptable stall recovery was achieved in 85.7% of the test cases. The overall recovery time increased with an increase in altitude, with the best and worst recovery times obtained at 10,000ft and 30,000ft respectively. Further work will focus on improving the performance of the algorithms such as by reducing the time to recover from a stall, decreasing the altitude loss and training the algorithms over a larger range of altitudes, up to cruise level.","2155-7209","978-1-6654-3420-1","10.1109/DASC52595.2021.9594316","Technology Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9594316","Loss of Control;Deep Learning;Deep Deterministic Policy Gradient;Artificial Intelligence","Training;Machine learning algorithms;Automation;Wind speed;Supervised learning;Reinforcement learning;Aerospace electronics","aerodynamics;aerospace components;aerospace computing;aerospace simulation;aerospace testing;aircraft;reinforcement learning;supervised learning","commercial aircraft;aerodynamic stall events;machine learning;reinforcement learning;unaccelerated wings level stall;acceptable stall recovery;automated aircraft stall recovery;supervised learning;on-board automation;protection systems","","","","17","IEEE","15 Nov 2021","","","IEEE","IEEE Conferences"
"Price-Based Residential Demand Response Management in Smart Grids: A Reinforcement Learning-Based Approach","Y. Wan; J. Qin; X. Yu; T. Yang; Y. Kang","Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; School of Engineering, RMIT University, Australia; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; Department of Automation, State Key Laboratory of Fire Science, Institute of Advanced Technology, University of Science and Technology of China, Hefei, China","IEEE/CAA Journal of Automatica Sinica","20 Oct 2021","2022","9","1","123","134","This paper studies price-based residential demand response management (PB-RDRM) in smart grids, in which non-dispatchable and dispatchable loads (including general loads and plug-in electric vehicles (PEVs)) are both involved. The PB-RDRM is composed of a bi-level optimization problem, in which the upper-level dynamic retail pricing problem aims to maximize the profit of a utility company (UC) by selecting optimal retail prices (RPs), while the lower-level demand response (DR) problem expects to minimize the comprehensive cost of loads by coordinating their energy consumption behavior. The challenges here are mainly two-fold: 1) the uncertainty of energy consumption and RPs; 2) the flexible PEVs' temporally coupled constraints, which make it impossible to directly develop a model-based optimization algorithm to solve the PB-RDRM. To address these challenges, we first model the dynamic retail pricing problem as a Markovian decision process (MDP), and then employ a model-free reinforcement learning (RL) algorithm to learn the optimal dynamic RPs of UC according to the loads' responses. Our proposed RL-based DR algorithm is benchmarked against two model-based optimization approaches (i.e., distributed dual decomposition-based (DDB) method and distributed primal-dual interior (PDI)-based method), which require exact load and electricity price models. The comparison results show that, compared with the benchmark solutions, our proposed algorithm can not only adaptively decide the RPs through on-line learning processes, but also achieve larger social welfare within an unknown electricity market environment.","2329-9274","","10.1109/JAS.2021.1004287","National Natural Science Foundation of China(grant numbers:61922076,61725304,61873252,61991403,61991400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9583162","Demand response management (DRM);Markovian decision process (MDP);Monte Carlo simulation;reinforcement learning (RL);smart grid","Energy consumption;Adaptation models;Uncertainty;Heuristic algorithms;Pricing;Demand response;Smart grids","demand side management;energy consumption;learning (artificial intelligence);load dispatching;Markov processes;optimisation;pricing;smart power grids","RL-based DR algorithm;model-based optimization approaches;distributed primal-dual interior;electricity price models;on-line learning processes;smart grids;reinforcement learning-based approach;PB-RDRM;dispatchable loads;plug-in electric vehicles;bi-level optimization problem;upper-level dynamic retail pricing problem;optimal retail prices;lower-level demand response problem;energy consumption behavior;model-based optimization algorithm;model-free reinforcement learning algorithm;price-based residential demand response management","","27","","38","","20 Oct 2021","","","IEEE","IEEE Journals"
"Decentralized Multi-AGV Task Allocation based on Multi-Agent Reinforcement Learning with Information Potential Field Rewards","M. Li; B. Guo; J. Zhang; J. Liu; S. Liu; Z. Yu; Z. Li; L. Xiang","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; College of Computer Science Xiangtan University, Xiangtan, China; John Hopcroft Center for Computer Science, Shanghai Jiao Tong University, Shanghai, China","2021 IEEE 18th International Conference on Mobile Ad Hoc and Smart Systems (MASS)","13 Dec 2021","2021","","","482","489","Automated Guided Vehicles (AGVs) have been widely used for material handling in flexible shop floors. Each product requires various raw materials to complete the assembly in production process. AGVs are used to realize the automatic handling of raw materials in different locations. Efficient AGVs task allocation strategy can reduce transportation costs and improve distribution efficiency. However, the traditional centralized approaches make high demands on the control center’s computing power and real-time capability. In this paper, we present decentralized solutions to achieve flexible and self-organized AGVs task allocation. In particular, we propose two improved multi-agent reinforcement learning algorithms, MAD-DPG-IPF (Information Potential Field) and BiCNet-IPF, to realize the coordination among AGVs adapting to different scenarios. To address the reward-sparsity issue, we propose a reward shaping strategy based on information potential field, which provides stepwise rewards and implicitly guides the AGVs to different material targets. We conduct experiments under different settings (3 AGVs and 6 AGVs), and the experiment results indicate that, compared with baseline methods, our work obtains up to 47% task response improvement and 22% training iterations reduction.","2155-6814","978-1-6654-4935-9","10.1109/MASS52906.2021.00066","National Science Fund for Distinguished Young Scholars; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9637774","Multi-agent reinforcement learning;AGVs;decentralized task allocation;information potential field","Training;Uncertainty;Remotely guided vehicles;Transportation;Reinforcement learning;Production;Raw materials","automatic guided vehicles;control engineering computing;industrial robots;materials handling;mobile robots;multi-agent systems;multi-robot systems;process control;production engineering computing;reinforcement learning;resource allocation","decentralized multiAGV task allocation;multiagent reinforcement learning;flexible shop floors;raw material handling;reward shaping strategy;information potential field rewards;flexible AGV task allocation;self-organized AGV task allocation;production process;transportation cost reduction;distribution efficiency;BiCNet-IPF;MAD-DPG-IPF","","6","","26","IEEE","13 Dec 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Wind Farm Layout Optimization","T. C. Vyshnav; M. C. Lavanya; K. C. Sindhu Thampatty","Department of Electrical and Electronic Engineering, Amrita School of Engineering, Coimbatore Amrita Vishwa Vidyapeetham, India; Deputy Director(Technical) Research and Development(R&D), National Institute of Wind Energy, Chennai, India; Department of Electrical and Electronic Engineering, Amrita School of Engineering, Coimbatore Amrita Vishwa Vidyapeetham, India","2022 International Conference on Electronics and Renewable Systems (ICEARS)","13 Apr 2022","2022","","","1393","1398","Wind farm layout optimization is a the major decision factor for maximum utilisation of wind energy for large scale wind farms. As more methods are being researched to reduce losses in the wind power plants, none more effective in reducing the over all loss than the loss due to wake effect. The arrangement of location of the turbines influence the power generation as well as levelized cost of energy. In order to minimise over all loss of the power plant, effective positioning of the turbines is needed. In this study, a novel turbine layout optimization method utilizing reinforcement learning is implemented for a wind farm. Turbulence intensity and the deficit velocity due to wake loss from Gaussian wake effect is used as the input for the model. The simulated results from the wind resource assessment software, WAsP suggests that the proposed method is effective for the number of turbines used in the study.","","978-1-6654-8425-1","10.1109/ICEARS53579.2022.9752054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9752054","Wind farm layout optimization;reinforcement learning optimization;Gaussian wake model;wake effect;wind assessment;annual energy production","Wind;Renewable energy sources;Wind energy;Roads;Layout;Reinforcement learning;Wind farms","learning (artificial intelligence);optimisation;power engineering computing;wakes;wind power;wind power plants;wind turbines","deficit velocity;Turbulence intensity;optimization method;power generation;wind turbines;wind resource assessment software;Gaussian wake effect;wind power plants;wind energy;wind farm layout optimization;reinforcement learning","","2","","22","IEEE","13 Apr 2022","","","IEEE","IEEE Conferences"
"System Identification and Machine Learning Model Construction for Reinforcement Learning Control Strategies Applied to LENS System","G. G. Jaman; A. Monson; K. R. Chowdhury; M. Schoen; T. Walters","Dept. of Mechanical Engineering, Idaho State University, Pocatello, USA; Dept. of Mechanical Engineering, Idaho State University, Pocatello, USA; Dept. of Mechanical Engineering, Idaho State University, Pocatello, USA; Dept. of Mechanical Engineering, Idaho State University, Pocatello, USA; Dept. of Mechanical Engineering, Idaho State University, Pocatello, USA","2022 Intermountain Engineering, Technology and Computing (IETC)","20 Jun 2022","2022","","","1","6","Identifying and controlling of additive manufacturing processes has the potential to improve part quality during the build process. The melt pool size of direct energy deposition processes has been related to part quality. In this paper, we investigate the use of system identification tools to device closed-loop controllers that are capable of regulating the melt pool size during the build process. Based on the results of linear models, machine learning approaches are investigated with the goal to obtain higher fidelity models, capable of characterizing the nonlinearities existing in such processes. In addition, a reinforcement learning controller is proposed that can accommodate the nonlinear behavior and the initial uncertainty in the model. Experiments with a direct energy deposition setup show improved part geometry using the linear model and controller. Simulation results employing the developed reinforcement learning controller show promise in enhanced control performance.","","978-1-6654-8653-8","10.1109/IETC54973.2022.9796761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796761","System Identification;Automated Machine Learning;Deep Neural Network;Reinforcement Learning;Deep Deterministic Gradient Policy;LENS","Uncertainty;PI control;Simulation;Process control;Thermocouples;Reinforcement learning;Aerospace electronics","closed loop systems;control engineering computing;learning systems;production engineering computing;quality control;rapid prototyping (industrial);reinforcement learning","enhanced control performance;machine learning model construction;LENS system;additive manufacturing processes;part quality;build process;melt pool size;direct energy deposition processes;system identification tools;device closed-loop controllers;linear model;machine learning approaches;higher fidelity models;reinforcement learning controller;direct energy deposition setup;improved part geometry;reinforcement learning control strategies","","","","13","IEEE","20 Jun 2022","","","IEEE","IEEE Conferences"
"Self-Supervised Discovering of Interpretable Features for Reinforcement Learning","W. Shi; G. Huang; S. Song; Z. Wang; T. Lin; C. Wu","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; State Key Laboratory of Intelligent Manufacturing System Technology, Beijing Institute of Electronic System Engineering, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","1 Apr 2022","2022","44","5","2712","2724","Deep reinforcement learning (RL) has recently led to many breakthroughs on a range of complex control tasks. However, the agent’s decision-making process is generally not transparent. The lack of interpretability hinders the applicability of RL in safety-critical scenarios. While several methods have attempted to interpret vision-based RL, most come without detailed explanation for the agent’s behavior. In this paper, we propose a self-supervised interpretable framework, which can discover interpretable features to enable easy understanding of RL agents even for non-experts. Specifically, a self-supervised interpretable network (SSINet) is employed to produce fine-grained attention masks for highlighting task-relevant information, which constitutes most evidence for the agent’s decisions. We verify and evaluate our method on several Atari 2600 games as well as Duckietown, which is a challenging self-driving car simulator environment. The results show that our method renders empirical evidences about how the agent makes decisions and why the agent performs well or badly, especially when transferred to novel scenes. Overall, our method provides valuable insight into the internal decision-making process of vision-based RL. In addition, our method does not use any external labelled data, and thus demonstrates the possibility to learn high-quality mask through a self-supervised manner, which may shed light on new paradigms for label-free vision learning such as self-supervised segmentation and detection.","1939-3539","","10.1109/TPAMI.2020.3037898","National Science and Technology Major Project of the Ministry of Science and Technology of China(grant numbers:2018AAA0100701,2018YFB1702903); National Natural Science Foundation of China(grant numbers:61906106,61936009,62022048); Institute for Guo Qiang of Tsinghua University; Beijing Academy of Artificial Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9259236","Deep reinforcement learning;interpretability;attention map;decision-making process","Task analysis;Decision making;Perturbation methods;Reinforcement learning;Jacobian matrices;Visualization;Games","","","Algorithms;Learning;Reinforcement, Psychology","3","","67","IEEE","13 Nov 2020","","","IEEE","IEEE Journals"
"An End-to-End Deep Reinforcement Learning Approach for Job Shop Scheduling","L. Zhao; W. Shen; C. Zhang; K. Peng","School of mechanical Science and Engeering, Huazhong University of Science and Technology, Wuhan, China; School of mechanical Science and Engeering, Huazhong University of Science and Technology, Wuhan, China; School of mechanical Science and Engeering, Huazhong University of Science and Technology, Wuhan, China; School of Management, Wuhan University of Science and Technology, Wuhan, China","2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD)","20 May 2022","2022","","","841","846","Job shop scheduling problem (JSSP) is a typical scheduling problem in manufacturing. Traditional scheduling methods fail to guarantee both efficiency and quality in complex and changeable production environments. This paper proposes an end-to-end deep reinforcement learning (DRL) method to address the JSSP. In order to improve the quality of solutions, a network model based on transformer and attention mechanism is constructed as the actor to enable a DRL agent to search in its solution space. The Proximal policy optimization (PPO) algorithm is utilized to train the network model to learn optimal scheduling policies. The trained model generates sequential decision actions as the scheduling solution. Numerical experiment results demonstrate the superiority and generality of the proposed method compared with other three classic heuristic rules.","","978-1-6654-0527-0","10.1109/CSCWD54268.2022.9776116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9776116","Job shop scheduling problem;deep reinforcement learning;proximal policy optimization;end-to-end method;transformer;attention mechanism","Job shop scheduling;Sensitivity;Decision making;Optimal scheduling;Production;Reinforcement learning;Dynamic scheduling","job shop scheduling;learning (artificial intelligence);production engineering computing","job shop scheduling problem;complex production environments;end-to-end deep reinforcement learning method;DRL agent;proximal policy optimization algorithm;power transformer;PPO algorithm","","2","","16","IEEE","20 May 2022","","","IEEE","IEEE Conferences"
"FRobs_RL: A Flexible Robotics Reinforcement Learning Library","J. M. Fajardo; F. G. Roldan; S. Realpe; J. D. Hernández; Z. Ji; P. -F. Cardenas","Mechanical and Mechatronics Deparment, National University of Colombia, Bogota, Colombia; Mechanical and Mechatronics Deparment, National University of Colombia, Bogota, Colombia; Mechanical and Mechatronics Deparment, National University of Colombia, Bogota, Colombia; School of Computer Science and Informatics, Cardiff University, UK; School of Engineering, Cardiff University, UK; Mechanical and Mechatronics Deparment, National University of Colombia, Bogota, Colombia","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","1104","1109","Reinforcement learning (RL) has become an interesting topic in robotics applications as it can solve complex problems in specific scenarios. The small amount of RL-tools focused on robotics, plus the lack of features such as easy transfer of simulated environments to real hardware, are obstacles to the widespread use of RL in robotic applications. FRobs_RL is a Python library that aims to facilitate the implementation, testing, and deployment of RL algorithms in intelligent robotic applications using robot operating system (ROS), Gazebo, and OpenAI Gym. FRobs_RL provides an Application Programming Interface (API) to simplify the creation of RL environments, where users can import a wide variety of robot models as well as different simulated environments. With the FRobs_RL library, users do not need to be experts in ROS, Gym, or Gazebo to create a realistic RL application. Using the library, we created and tested two environments containing common robotic tasks; one is a reacher task using a robotic manipulator, and the other is a mapless navigation task using a mobile robot. The library is available in GitHub 1.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926586","Deep Learning Methods;Reinforcement Learning;Software Architecture for Robotic and Automation","Automation;Navigation;Operating systems;Reinforcement learning;Libraries;Task analysis;Robots","application program interfaces;control engineering computing;intelligent robots;learning (artificial intelligence);learning systems;mobile robots;navigation;path planning;public domain software;robot programming;robot vision;software libraries","flexible robotics reinforcement learning library;robotics applications;RL-tools;Python library;RL algorithms;intelligent robotic applications;robot operating system;Application Programming Interface;RL environments;robot models;different simulated environments;FRobs_RL library;realistic RL application;common robotic tasks;robotic manipulator;mobile robot","","","","33","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Framework Based on an Attention Mechanism and Disjunctive Graph Embedding for the Job-Shop Scheduling Problem","R. Chen; W. Li; H. Yang","School of Mechanical and Electrical Engineering, Soochow University, Suzhou, China; School of Management, Shanghai University, Shanghai, China; School of Mechanical and Electrical Engineering, Soochow University, Suzhou, China","IEEE Transactions on Industrial Informatics","15 Dec 2022","2023","19","2","1322","1331","The job-shop scheduling problem (JSSP) is a classical NP-hard combinatorial optimization problem, and the operating efficiency of manufacturing system is affected directly by the quality of its scheduling scheme. In this article, a novel deep reinforcement learning framework is proposed for solving the classical JSSP, where each machine has to process each job exactly once. This method based on an attention mechanism and disjunctive graph embedding, and a sequence-to-sequence pattern is used to model the JSSP in the framework. A disjunctive graph embedding process based on node2vec is used to learn the disjunctive graph representations containing JSSP characteristics, thereby generalizing the model considerably. An improved transformer architecture based on a multihead attention mechanism is used to generate solutions. Containing a parallel-computing encoder and a recurrent-computing decoder, it is adept at learning long-range dependencies and effective at solving large-scale scheduling problems. Experimental results verified the effectiveness of the proposed method.","1941-0050","","10.1109/TII.2022.3167380","National Natural Science Foundation of China(grant numbers:52075354); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9757835","Attention mechanism;deep reinforcement learning (DRL);graph embedding;job-shop scheduling;transformer model","Job shop scheduling;Transformers;Scheduling;Feature extraction;Adaptation models;Reinforcement learning;Computer architecture","computational complexity;deep learning (artificial intelligence);graph theory;job shop scheduling;manufacturing systems;optimisation;production engineering computing;recurrent neural nets;reinforcement learning","classical JSSP characteristic;classical NP-hard combinatorial optimization problem;deep reinforcement learning framework;disjunctive graph embedding process;disjunctive graph representations;job-shop scheduling problem;large-scale scheduling problems;manufacturing system;multihead attention mechanism;operating efficiency;parallel-computing encoder;recurrent-computing decoder;sequence-to-sequence pattern;transformer architecture","","8","","35","IEEE","14 Apr 2022","","","IEEE","IEEE Journals"
"A Computational Framework for Automatic Online Path Generation of Robotic Inspection Tasks via Coverage Planning and Reinforcement Learning","W. Jing; C. F. Goh; M. Rajaraman; F. Gao; S. Park; Y. Liu; K. Shimada","A*STAR Artificial Intelligence Initiative, Singapore; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, US; A*STAR Artificial Intelligence Initiative, Singapore; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; A*STAR Artificial Intelligence Initiative, Singapore; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Access","18 Oct 2018","2018","6","","54854","54864","Surface/shape inspection is a common and highly repetitive task in the factory production line. Using robots to automate the inspection process could help to reduce the costs and improve the productivities. In robotized surface/shape inspection application, the planning problem is to find a near-optimal sequence of robotic actions that inspect the surface areas of the target objects in a minimum cycle time, while satisfying the coverage requirement. In this paper, we propose a novel computational framework to automatically generate efficient robotic path online for surface/shape inspection application. Within the computational framework, a Markov decision process (MDP) formulation is proposed for the coverage planning problem in the industrial surface inspection with a robotic manipulator. A reinforcement learning-based search algorithm is also proposed in the computational framework to generate planning policy online with the MDP formulation of the robotic inspection problem for robotic inspection applications. Several case studies are conducted to validate the effectiveness of the proposed method. It is observed that the proposed method could automatically generate the inspection path online for different target objects to meet the coverage requirement, with the presence of pose variation of the target object. In addition, the inspection cycle time reduction is observed to be 24% on average compared to the previous approaches during these test instances.","2169-3536","","10.1109/ACCESS.2018.2872693","Agent for Science, Technology and Research (A*STAR), Singapore, through the A*STAR Human-Centric Artificial Intelligence Programme(grant numbers:A1718g0048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8476296","Robotics;planning;artificial intelligence;reinforcement learning","Inspection;Robots;Planning;Surface treatment;Task analysis;Production facilities","cost reduction;factory automation;industrial manipulators;inspection;learning (artificial intelligence);Markov processes;optimisation;path planning;productivity;search problems","automatic online path generation;robotic inspection tasks;factory production line;robotized surface/shape inspection application;surface areas;novel computational framework;Markov decision process formulation;coverage planning problem;industrial surface inspection;robotic manipulator;reinforcement learning-based search algorithm;planning policy online;robotic inspection applications;inspection cycle time reduction;target objects;surface inspection;shape inspection;cost reduction;productivity improvement;near-optimal sequence","","27","","36","OAPA","28 Sep 2018","","","IEEE","IEEE Journals"
"REAL-TIME BATCHING IN JOB SHOPS BASED ON SIMULATION AND REINFORCEMENT LEARNING","T. Zhang; S. Xie; O. Rose","Universität der Bundeswehr München, Neubiberg, GERMANY; Universität der Bundeswehr München, Neubiberg, GERMANY; Universität der Bundeswehr München, Neubiberg, GERMANY","2018 Winter Simulation Conference (WSC)","3 Feb 2019","2018","","","3331","3339","Real-time batching in job shops decides 1) whether to start processing a batch or to wait for more jobs joining the batch, and 2) which batch should be processed first. It is addressed as a sequential decision-making problem and formalized based on Markov decision processes. By adding a dummy batch, which means no batches are selected and all batches wait for additional jobs, the first decision-making is generalized to the second. A simulation-based neural fitted Q learning is introduced to solve the Markov decision processes and build a decision maker. The well-trained decision maker decides which batch in a batch list should be processed first at each decision epoch. The experiment results show that the proposed approach outperforms some other decision rules.","1558-4305","978-1-5386-6572-5","10.1109/WSC.2018.8632524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8632524","","Markov processes;Real-time systems;Reinforcement learning;Sequential analysis;Mathematical model;Job shop scheduling;Space exploration","batch processing (industrial);decision making;job shop scheduling;learning (artificial intelligence);Markov processes;neural nets;production engineering computing","decision-making problem;reinforcement learning;simulation-based neural fitted Q learning;Markov decision processes;job shops;real-time batching","","9","","14","IEEE","3 Feb 2019","","","IEEE","IEEE Conferences"
"Dynamic Economic Optimization of a Continuously Stirred Tank Reactor Using Reinforcement Learning","D. Machalek; T. Quah; K. M. Powell","Department of Chemical Engineering, University of Utah, Salt Lake City, UT, USA; Department of Chemical Engineering, University of Utah, Salt Lake City, UT, USA; Department of Chemical Engineering, University of Utah, Salt Lake City, UT, USA","2020 American Control Conference (ACC)","27 Jul 2020","2020","","","2955","2960","Reinforcement learning (RL) algorithms are a set of goal-oriented machine learning algorithms that can perform control and optimization in a system. Most RL algorithms do not require any information about the underlying dynamics of the system, they only require input and output information. RL algorithms can therefore be applied to a wide range of systems. This paper explores the use of a custom environment to optimize a problem pertinent to process engineers. In this study the custom environment is a continuously stirred tank reactor (CSTR). The purpose of using a custom environment is to illustrate that any number of systems can readily become RL environments. Three RL algorithms are investigated: deep deterministic policy gradient (DDPG), twin-delayed DDPG (TD3), and proximal policy optimization. They are evaluated based on how they converge to a stable solution and how well they dynamically optimize the economics of the CSTR. All three algorithms perform 98% as well as a first principles model, coupled with a non-linear solver, but only TD3 demonstrates convergence to a stable solution. While itself limited in scope, this paper seeks to further open the door to a coupling between powerful RL algorithms and process systems engineering.","2378-5861","978-1-5386-8266-1","10.23919/ACC45564.2020.9147706","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9147706","","Optimization;Heuristic algorithms;Inductors;Chemical reactors;Steady-state;Modeling;Heating systems","chemical reactors;learning (artificial intelligence);optimisation;production engineering computing","dynamic economic optimization;continuously stirred tank reactor;reinforcement learning algorithms;output information;RL environments;proximal policy optimization;goal-oriented machine learning algorithm;CSTR;deep deterministic policy gradient;twin-delayed DDPG;TD3","","4","","18","","27 Jul 2020","","","IEEE","IEEE Conferences"
"Learning algorithm by reinforcement signals for the automatic recognition system","K. Ikuta; H. Tanaka; K. I. Tanaka; K. Kyuma","Advanced Technology R&D Center, Mitsubishi Electric Corporation, Hyogo, Japan; Dept. of Information Processing, Tokyo Institute of Technology, Tokyo, Japan; NA; Advanced Technology R&D Center, Mitsubishi Electric Corporation, Hyogo, Japan","2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)","7 Mar 2005","2004","5","","4844","4848 vol.5","The visual inspection of the industrial product copes with defects that have wide variety of features in the shape, size, and strength. Most of the learning algorithms of the recognition system require specific training patterns for learning of the feature extraction filters. However, there are many cases that the recognition tasks don't have specific training patterns. We propose a learning algorithm, which reconstructs feature extraction fillers on the basis of reinforcement signals. The recognition system constructed by the learning algorithm is robust against environmental variation.","1062-922X","0-7803-8566-7","10.1109/ICSMC.2004.1401298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1401298","","Inspection;Filters;Feature extraction;Machine vision;Research and development;Shape;Industrial training;Image resolution;Humans;Image processing","learning (artificial intelligence);automatic optical inspection;computer vision;feature extraction;image recognition;production engineering computing;inspection","learning algorithm;reinforcement signal;automatic recognition system;industrial product;visual inspection;feature extraction filter","","2","","3","IEEE","7 Mar 2005","","","IEEE","IEEE Conferences"
"A Structured Representation Framework for TRIZ-Based Chinese Patent Classification via Reinforcement Learning","J. Yu; L. Huang; Y. Hu; H. Chang","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Management, Guangzhou University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","2020 3rd International Conference on Artificial Intelligence and Big Data (ICAIBD)","9 Jul 2020","2020","","","6","10","Theory of Inventive Problem Solving (TRIZ) -based [5] patent classification is essential for patent management and industrial analysis. However, the most research of patent classification are just on English texts and their methods just consider word order but do not use any parse structure, which rely on the word segmentation. In this paper, we propose a new structured representation [11] framework for TRIZ-based Chinese patent classification, which can discover structures automatically and measure structures Our framework uses structured representation model based on reinforcement learning to discover structures. Furthermore, the structured representation model we use lack attention in important structures which may have information memory loss when dealing with long sentences. Therefore, we use attention mechanisms to measure structures that contribute to the patent classification. On 4 TRIZ based classification tasks, our framework significantly outperformed all models in terms of area under curve (AUC) [23] and outperforms Hierarchically Structured LSTM (HS-LSTM) [11]. Moreover, we achieved absolute improvements of 10.08% in performance on ""Innovation in product design"" classification task in AUC score compared with the state-of-the-art model, bidirectional encoder representation from transformers (BERT) [8].","","978-1-7281-9741-8","10.1109/ICAIBD49809.2020.9137486","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9137486","Chinese patent classification;text classification;TRIZ;structured representation;reinforcement learning","Patents;Technological innovation;Bit error rate;Text categorization;Reinforcement learning;Learning (artificial intelligence);Transformers","learning (artificial intelligence);patents;pattern classification;problem solving;product design;production engineering computing","structured representation framework;TRIZ-based chinese patent classification;reinforcement learning;patent classification;patent management;parse structure;TRIZ-based Chinese patent classification;structured representation model;product design classification task;hierarchically structured LSTM;TRIZ based classification tasks","","2","","25","IEEE","9 Jul 2020","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach for Integrated Scheduling in Automated Container Terminals","Z. Zhang; Z. Zhuang; W. Qin; H. Fang; S. Lan; C. Yang; Y. Tian","School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai International Port (Group) Co., Ltd., Shanghai, China; School of Economics and Management, University of Chinese Academy of Sciences, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; Shanghai International Port (Group) Co., Ltd., Shanghai, China","2022 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)","26 Dec 2022","2022","","","1182","1186","Automated container terminals are complex systems with multiple interactions and high dynamic characteristics. Integrated scheduling is expected to improve the overall efficiency. However, traditional optimization approaches such as mathematical models and meta-heuristic algorithms failed to tackle high dynamics. A reinforcement learning approach based on the scheduling network method is presented in this paper. Network-based heuristic rules are introduced into the action space, and a novel state definition that integrates local and global information about the scheduling problem is proposed. Group training and group validating strategies are adopted to test the generalization ability. Numerical experiment results reveal that the proposed approach converges to a high level and maintains good performance on unseen instances. Compared to the selected heuristic rules, the proposed method achieves 2.37% and 6.06% better results on training and test instances, respectively.","","978-1-6654-8687-3","10.1109/IEEM55944.2022.9989692","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9989692","Automated container terminal;integrated scheduling;network scheduling;reinforcement learning","Training;Job shop scheduling;Heuristic algorithms;Engineering management;Metaheuristics;Reinforcement learning;Containers","optimisation;production engineering computing;reinforcement learning;scheduling;sea ports","automated container terminals;complex systems;group validating strategies;high dynamic characteristics;integrated scheduling;mathematical models;meta-heuristic algorithms;multiple interactions;network-based heuristic rules;reinforcement learning approach;scheduling network method;scheduling problem;selected heuristic rules;traditional optimization approaches","","1","","12","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"Multi-Agent Reinforcement Learning for Dynamic Spare Parts Inventory Control","C. Yu; Y. Zhou; Z. Zhang","School of Mechanical Engineering, Southeast University, Nanjing, China; School of Mechanical Engineering, Southeast University, Nanjing, China; School of Mechanical Engineering, Southeast University, Nanjing, China","2020 Global Reliability and Prognostics and Health Management (PHM-Shanghai)","18 Dec 2020","2020","","","1","6","Spare parts inventory control is an important research topic in operational research, which aims to guarantee machine availability and reduce inventory costs. Existing inventory policy described by a limited number of parameters, is not flexible, not necessary the optimal. This paper developed a multi-agent reinforcement learning method based on the Dueling Double Deep Q-network framework to solve a two-echelon spare parts inventory control problem, where the fixed time window and lateral transshipment are considered. Numerical examples are demonstrated to investigate the performance of the proposed multi-agent reinforcement learning method. The result shows that the proposed method outperforms the genetic algorithm that derives the (s, S) policy.","","978-1-7281-5946-1","10.1109/PHM-Shanghai49105.2020.9280935","National Natural Science Foundation of China(grant numbers:71671041); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9280935","inventory optimization;deep reinforcement learning;multi-agent;spare parts","Optimization methods;Reinforcement learning;Games;Inventory control;Reliability;Prognostics and health management;Genetic algorithms","cost reduction;genetic algorithms;learning (artificial intelligence);maintenance engineering;multi-agent systems;production engineering computing;stock control","inventory costs reduction;two-echelon spare parts inventory control problem;multiagent reinforcement learning method;dueling double deep Q-network framework;genetic algorithm","","1","","15","IEEE","18 Dec 2020","","","IEEE","IEEE Conferences"
"Parallel and distributed multi-agent reinforcement learning","M. Kaya; A. Arslan","Department of Computer Engineering, Firat University, Elazig, Turkey; Department of Computer Engineering, Firat University, Elazig, Turkey","Proceedings. Eighth International Conference on Parallel and Distributed Systems. ICPADS 2001","7 Aug 2002","2001","","","437","441","The application of parallel and distributed systems to multi-agent environments has attracted recent attention. Multi-agent systems are a particular type of distributed artificial intelligence system. This paper presents an approach to learning in parallel and distributed systems. A variant of the job assignment problem is chosen as an evaluation task. This is an NP-hard problem, which is relevant to many industrial application domains. Experimental results show the effectiveness of the proposed approach.","1521-9097","0-7695-1153-8","10.1109/ICPADS.2001.934851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=934851","","Learning;Multiagent systems;Robustness;Concurrent computing;Intelligent robots;Parallel processing;Control systems;Distributed computing;Application software;Artificial intelligence","multi-agent systems;learning (artificial intelligence);distributed processing;computational complexity;scheduling;production control","parallel systems;distributed systems;multi-agent reinforcement learning;distributed artificial intelligence;job assignment problem;NP-hard problem;industrial applications","","1","","7","IEEE","7 Aug 2002","","","IEEE","IEEE Conferences"
"Analysis on Deep Reinforcement Learning in Industrial Robotic Arm","H. Guan","Shandong University, Shandong, Chian","2020 International Conference on Intelligent Computing and Human-Computer Interaction (ICHCI)","10 May 2021","2020","","","426","430","Deep reinforcement learning is a combination of reinforcement learning and deep learning. It allows the robot to learn new tasks on its own. In recent years, many studies have applied deep reinforcement learning algorithms to the manipulation of robotic arms, and have achieved excellent results. This article described the basic knowledge of deep reinforcement learning and analyzed the current problems faced by industrial robotic arms. By reviewing the main research that researchers have applied deep reinforcement learning algorithms to the field of manipulator operation in recent years and the development of related deep reinforcement learning algorithms. It concluded that how deep reinforcement learning can solve the problems faced by industrial robotic arms. Finally, this article referred to the challenges faced by the application of deep reinforcement learning and its application in the field of industrial robotic arms and then made a detailed analysis and explanation.","","978-1-6654-2316-8","10.1109/ICHCI51889.2020.00094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9424787","Deep reinforcement learning;robotic arm;manipulation;DQN;DDPG","Human computer interaction;Service robots;Shape;Virtual environments;Reinforcement learning;Programming;Manipulators","control engineering computing;deep learning (artificial intelligence);industrial manipulators;production engineering computing","industrial robotic arm;deep reinforcement learning algorithms;manipulator operation","","1","","33","IEEE","10 May 2021","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Approach for Crowdshipping Vehicle Routing Problem","H. Huang; Y. -S. Lin; J. -R. Kang; C. -C. Lin","Department of Industrial Engineering and Management, National Yang Ming Chiao Tung University, Taiwan; Department of Industrial Engineering and Management, National Yang Ming Chiao Tung University, Taiwan; Department of Information Management, Tatung University, Taipei, Taiwan; Department of Industrial Engineering and Management, National Yang Ming Chiao Tung University, Taiwan","2022 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)","26 Dec 2022","2022","","","0598","0599","Extending the vehicle routing problem (VRP), the crowdshipping VRP (CVRP) considers crowdsourcing logistics. Crowdsourcing is flexible and convenient to reduce transportation costs and carbon emissions. However, crowdshipping requires to adapt to real-time changes such as road conditions and customer demands, which heuristic algorithms are not suitable for addressing these issues. Therefore, this study proposes a deep reinforcement learning (DRL) approach to react to real-time environmental changes to solve the CVRP. The CVRP considers a single depot and multiple transfer points to serve multiple customers, in which cargos can be delivered by either the vehicle directly, or crowdworkers after the vehicle stores cargos at transfer points. In the proposed DRL, the agent explores feasible decisions, and revises the path that it should take based on feedbacks. The cost effectiveness that affects crowdshipping includes the vehicle routing, and whether the concerned customer is suitable for crowdshipping. The experimental results show the efficiency and accuracy of the trained model for medium-sized VRPs are much higher than classical heuristic algorithms.","","978-1-6654-8687-3","10.1109/IEEM55944.2022.9989773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9989773","Vehicle routing problem;crowdsourcing;machine learning","Crowdsourcing;Deep learning;Costs;Heuristic algorithms;Roads;Vehicle routing;Transportation","deep learning (artificial intelligence);goods distribution;logistics;optimisation;production engineering computing;reinforcement learning;search problems;transportation;vehicle routing;vehicles","carbon emissions;classical heuristic algorithms;concerned customer;cost effectiveness;crowdshipping vehicle routing problem;crowdshipping VRP;crowdsourcing logistics;customer demands;CVRP;deep reinforcement learning approach;DRL;multiple customers;multiple transfer points;real-time changes;real-time environmental changes;road conditions;single depot;transportation costs;vehicle stores cargos","","","","12","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"Solving the Traveling Saleman Problem Using Reinforcement Learning","J. Liang","New York University, New York, USA","2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)","10 Apr 2023","2023","","","79","83","Combinatorial optimization problems are heavily studied in recent decades due to its broad applications. The traveling salesman problem (TSP) is among the best-known combinatorial optimization problems, which is NP-hard. A variety of methods have been proposed to find optimal solutions to TSP. Recently, the development of reinforcement learning techniques brings a new approach to TSP, which finds the optimal iterations of all nodes in a graph with no requirement to analytically solving the intractable graph optimization problem. This paper demonstrates the algorithm of solving TSP using q-learning, an off-policy reinforcement learning method and show an example of optimal path finding among warehouses.","","978-1-6654-6253-2","10.1109/EEBDA56825.2023.10090654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10090654","traveling salesman problem;reinforcement learning;combinatorial optimization","Learning systems;Electrical engineering;Q-learning;Costs;Traveling salesman problems;Big Data;Data models","computational complexity;graph theory;optimisation;production engineering computing;reinforcement learning;travelling salesman problems","combinatorial optimization problems;intractable graph optimization problem;NP-hard problem;off-policy reinforcement learning;optimal iterations;optimal path finding;q-learning;traveling saleman problem;TSP","","","","14","IEEE","10 Apr 2023","","","IEEE","IEEE Conferences"
"Hybrid Disassembly Line Optimization with Reinforcement Learning","G. Xi; J. Wang; X. Guo; S. Liu; S. Qin; L. Qi","Liaoning Petrochemical University, Fushun, China; Monmouth University, USA; Liaoning Petrochemical University, Monmouth University, USA; College of Information Science and Engineering, Northeast University, Shenyang, China; Shangqiu Normal University, Shangqiu, China; Shandong University, Qingdao, China","2023 32nd Wireless and Optical Communications Conference (WOCC)","7 Jun 2023","2023","","","1","5","This paper explores the benefits of combining a U-shaped disassembly line with a single-row linear disassembly line for specific scenarios. To address the balancing problem that arises with such a hybrid disassembly line, the authors establish a mathe-matical model aimed at maximizing recovery profit. The Soft Actor-Critic (SAC) algorithm is proposed to find the solution, taking into account the characteristics of the problem. The performance of the SAC algorithm is compared to the Advantage Actor-Critic (A2C) algorithm, Deep Deterministic Policy Gradient (DDPG). The results demonstrate that the SAC algorithm is capable of achieving an approximately optimal result for small-scale cases and outperforms DDPG, A2C in solving large-scale disassembly cases.","2379-1276","979-8-3503-3715-0","10.1109/WOCC58016.2023.10139382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10139382","Hybrid disassembly line balancing problem;soft actor-critic algorithm;deep reinforcement learning;optimization","Wireless communication;Reinforcement learning;Approximation algorithms;Optical fiber communication;Optimization","assembling;deep learning (artificial intelligence);gradient methods;optimisation;production engineering computing;recycling;reinforcement learning","balancing problem;hybrid disassembly line optimization;large-scale disassembly cases;mathematical model;recovery profit;reinforcement learning;SAC algorithm;single-row linear disassembly line;soft actor-critic algorithm;U-shaped disassembly line","","","","30","IEEE","7 Jun 2023","","","IEEE","IEEE Conferences"
"A Noise-Insensitive Reinforcement Learning Control for a Nonlinear Bioreactor","L. L. Estrada-Rayme; P. Cárdenas-Lizana","Department of Bioengineering, Universidad de Ingenieria y Tecnologia - UTEC, Lima, Perú; Department of Bioengineering, Universidad de Ingenieria y Tecnologia - UTEC, Lima, Perú","2022 IEEE XXIX International Conference on Electronics, Electrical Engineering and Computing (INTERCON)","5 Sep 2022","2022","","","1","4","The capacity of the Model-Free Learning Control (MFLC) system is studied in the presence of noise signals for a highly nonlinear bioreactor. The bioreactor is a Multiple-Input Multiple-Output (MIMO) system based in a biochemical and physical model. A methodology is provided for designing the MFLC scheme and optimally selecting their elements and parameters. The scheme is tested for 3 different inputs (step, sinusoidal, and ramp) and it includes exogenous disturbances in the state variables. Noise signals are added to the output variables to model errors in sensor readout and uncertainties of the plant. The results show that the MFLC systems is very robust and insensitive to the presence of noise signal even when it is very high (50%). The presented methodology could be applied to control any nonlinear industrial process.","","978-1-6654-8636-1","10.1109/INTERCON55795.2022.9870085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9870085","model-free;learning;control;reinforcement;system;Q-learning;bioreactor;Klatt-Engell;temperature;component product;CSTR;nonlinear;noise;disturbance","Electrical engineering;Uncertainty;Q-learning;Biological system modeling;Computational modeling;Process control;Control systems","biochemistry;bioreactors;control engineering computing;learning systems;MIMO systems;nonlinear control systems;process control;production engineering computing;reinforcement learning;sensors","biochemical model;exogenous disturbances;MFLC;MFLC systems;MIMO;model-free learning control;multiple-input multiple-output system;noise signal;noise-insensitive reinforcement learning control;nonlinear bioreactor;nonlinear industrial process;output variables;physical model;plant uncertainties;sensor readout;state variables","","","","12","IEEE","5 Sep 2022","","","IEEE","IEEE Conferences"
"Research on Parallel Distribution Routing Optimization Based on Improved Reinforcement Learning Algorithm","J. Jiang; R. Ma; G. Shen","School of Information, Beijing Wuzi University, Beijing, China; School of Information, Beijing Wuzi University, Beijing, China; School of Information, Beijing Wuzi University, Beijing, China","2021 IEEE International Conference on Advances in Electrical Engineering and Computer Applications (AEECA)","2 Nov 2021","2021","","","183","186","With the rapid development of e-commerce, the efficiency requirements for logistics distribution are getting higher and higher, so distribution routing optimisation has important theoretical and practical significance. In this paper, a new coding mechanism is designed to solve the distribution task in the multi-distribution centre scenario and vehicle scheduling problem with capacity constraints in the distribution scenario. Simulation results show that this algorithm has the advantages of high solution efficiency, variable input and avoidance of re-referencing compared with two heuristics of Google OR-Tools, ant colony algorithm and simulated annealing algorithm, thus this algorithm can effectively solve the logistics distribution routing optimisation problem and improve the efficiency of logistics distribution.","","978-1-6654-3561-1","10.1109/AEECA52519.2021.9574338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9574338","Logistics distribution;vehicle routing;multi-depot;reinforcement learning algorithm;transformer","Heuristic algorithms;Simulation;Transportation;Reinforcement learning;Simulated annealing;Routing;Encoding","ant colony optimisation;learning (artificial intelligence);logistics;production engineering computing;scheduling;simulated annealing","parallel distribution routing optimization;improved reinforcement learning algorithm;coding mechanism;vehicle scheduling problem;ant colony algorithm;simulated annealing algorithm;logistics distribution routing optimisation problem;e-commerce","","","","12","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Cooperative Control for Multi-Intersection Traffic Signal Based on Deep Reinforcement Learning and Imitation Learning","Y. Huo; Q. Tao; J. Hu","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Access","9 Nov 2020","2020","8","","199573","199585","Traffic signal control has long been considered as a critical topic in intelligent transportation systems. Most existing related methods either suffer from inefficient training or mainly focus on isolated intersections. This article aims at the cooperative control for multi-intersection traffic signal, in which a novel end-to-end learning model is established and an efficient training method is proposed analogously, which is capable of adapting to large-scale scenarios. In the proposed method, the input traffic status in multi-intersection are expressed by a tensor without information loss, which significantly reduces model complexity than using a huge matrix, since additional convolutional layers can be required to extract features from a huge matrix. For the output, a multidimensional boolean vector is employed to simplify the control policy with abiding the practical phase changing rules, and then a multi-task learning structure is used to get the cooperative policy. Instead of only using the reinforcement learning to train the model, we employ imitation learning to integrate a rule based model to do the pre-training, which greatly accelerates the convergence. Afterwards, the reinforcement learning method is adopted to continue the fine training, where proximal policy optimization algorithm is incorporated to solve the policy collapse problem in multi-dimensional output situation. Numerical experiments demonstrate the distinctive advantages of the proposed method with comparison to the efficiency and accuracy of the related state-of-the-art methods.","2169-3536","","10.1109/ACCESS.2020.3034419","National Key R&D Program in China(grant numbers:2019YFF0303102); National Natural Science Foundation of China(grant numbers:61673232); Tsinghua University Initiative Scientific Research Program(grant numbers:2018Z05JDX005,20183080016)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9241814","Deep reinforcement learning;imitation learning;multi-intersection;proximal policy optimization;tensor","Training;Numerical models;Reinforcement learning;Feature extraction;Convergence;Optimization;Indexes","Boolean algebra;control engineering computing;convolutional neural nets;feature extraction;intelligent transportation systems;learning (artificial intelligence);optimisation;road traffic control;tensors;traffic engineering computing","policy collapse problem;proximal policy optimization;cooperative policy;phase changing rules;multidimensional Boolean vector;feature extraction;convolutional layers;tensor;imitation learning;cooperative control;end-to-end learning;intelligent transportation systems;traffic signal control;deep reinforcement learning;multiintersection traffic signal;multidimensional output situation;multitask learning structure;control policy;model complexity;traffic status","","13","","42","CCBYNCND","28 Oct 2020","","","IEEE","IEEE Journals"
"Proof of Concept: Using Reinforcement Learning agent as an adversary in Serious Games","D. Hornak; M. Jascur; N. Ferencik; M. Bundzel","Dept. of Cybernetics and Artificial Intelligence, Faculty of Electrical Engineering and Informatics, FEI TU, Košice, Slovak Republic; Dept. of Cybernetics and Artificial Intelligence, Faculty of Electrical Engineering and Informatics, FEI TU, Košice, Slovak Republic; Dept. of Cybernetics and Artificial Intelligence, Faculty of Electrical Engineering and Informatics, FEI TU, Košice, Slovak Republic; Dept. of Cybernetics and Artificial Intelligence, Faculty of Electrical Engineering and Informatics, FEI TU, Košice, Slovak Republic","2019 IEEE International Work Conference on Bioinspired Intelligence (IWOBI)","11 Jun 2020","2019","","","000111","000116","This article focuses on simple rehabilitation video-game called Flying with Friends. The rehabilitation industry has been experiencing a boom in recent years, coupled with the growing popularity of virtual reality technology, a drop in prices for these technologies and the expansion entertainment industry in the form of computer games. The goal of this experiment was to provide a proof that such systems are viable option when it comes to artificial intelligence systems in serious video-games, but not limited to only serious ones. The solution described in this article, in cooperation with experts, is going to be deployed in a real rehabilitation environment.","","978-1-7281-0968-8","10.1109/IWOBI47054.2019.9114431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9114431","Serious games;Reinforcement Learning;Q-Learning;Game Adaptation;Adaptive Game AI;Rehabilitation;Upper Limb","","entertainment;learning (artificial intelligence);patient rehabilitation;serious games (computing);virtual reality","reinforcement learning agent;simple rehabilitation video-game;rehabilitation industry;virtual reality technology;expansion entertainment industry;computer games;artificial intelligence systems;serious video-games;serious ones;rehabilitation environment;Flying with Friends","","1","","8","IEEE","11 Jun 2020","","","IEEE","IEEE Conferences"
"A reinforcement learning algorithm for neural networks with incremental learning ability","N. Shiraga; S. Ozawa; S. Abe","Graduate School of Science and Technology, Kobe University, Kobe, Japan; Graduate School of Science and Technology, Kobe University, Kobe, Japan; Graduate School of Science and Technology, Kobe University, Kobe, Japan","Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.","5 Jun 2003","2002","5","","2566","2570 vol.5","When neural networks are used for approximating action-values of Reinforcement Learning (RL) agents, the ""interference"" caused by incremental learning can be serious. To solve this problem, in this paper, a neural network model with incremental learning ability was applied to RL problems. In this model, correctly acquired input-output relations are stored into long-term memory, and the memorized data are effectively recalled in order to suppress the interference. In order to evaluate the incremental learning ability, the proposed model was applied to two problems: Extended Random-Walk Task and Extended Mountain-Car Task. In these tasks, the working space of agents is extended as the learning proceeds. In the simulations, we certified that the proposed model could acquire proper action-values as compared with the following three approaches to the approximation of action-value functions: tile coding, a conventional neural network model and the previously proposed neural network model.","","981-04-7524-1","10.1109/ICONIP.2002.1201958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201958","","Learning;Neural networks;Function approximation;Training data;Resource management;State estimation;Interference;Electronic mail;Tiles;Table lookup","learning (artificial intelligence);radial basis function networks","reinforcement learning algorithm;neural networks;incremental learning ability;input-output relations;interference suppression;extended random-walk task;extended mountain-car task;action-values;resource allocating network;long-term memory;normalized radial basis functions","","9","3","8","","5 Jun 2003","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Robot Arm Manipulation with Efficient Training Data through Simulation","X. Xing; D. E. Chang","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea","2019 19th International Conference on Control, Automation and Systems (ICCAS)","30 Jan 2020","2019","","","112","116","Deep reinforcement learning trains neural networks using experiences sampled from the replay buffer, which is commonly updated at each time step. In this paper, we propose a method to update the replay buffer adaptively and selectively to train a robot arm to accomplish a suction task in simulation. The response time of the agent is thoroughly taken into account. The state transitions that remain stuck at the boundary of constraint are not stored. The policy trained with our method works better than the one with the common replay buffer update method. The result is demonstrated both by simulation and by experiment with a real robot arm.","2642-3901","978-89-93215-17-5","10.23919/ICCAS47443.2019.8971637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8971637","Deep reinforcement learning;Replay buffer update;Robot arm","","image capture;learning (artificial intelligence);manipulators;neural nets","robot arm manipulation;training data;deep reinforcement learning;neural networks","","5","","17","","30 Jan 2020","","","IEEE","IEEE Conferences"
"optimization of dynamic multi-objective non-identical parallel machine scheduling with multi-stage reinforcement learning","L. Guo; Z. Zhuang; Z. Huang; W. Qin","Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Tong University, Shanghai, China","2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)","8 Oct 2020","2020","","","1215","1219","This study investigates the dynamic non-identical parallel machine scheduling problem, which is characterized by machine speed differences, uncertain mission arrival time and multiple optimization metrics. A multi-stage reinforcement learning approach is proposed to solve the problem through a multi-stage learning process which is trained repeatedly with different goals. Computational experiments have been conducted and the results demonstrate that the proposed multi-stage reinforcement learning approach can better balance the performance across the three different objectives for dynamic multi-objective non-identical parallel machine scheduling problem.","2161-8089","978-1-7281-6904-0","10.1109/CASE48305.2020.9216743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9216743","","Heuristic algorithms;Parallel machines;Dynamic scheduling;Job shop scheduling;Learning (artificial intelligence);Optimization","learning (artificial intelligence);optimisation;parallel machines;scheduling","multistage reinforcement learning approach;machine speed differences;uncertain mission arrival time;multiple optimization metrics;multistage learning process;dynamic multiobjective non-identical parallel machine scheduling","","4","","25","IEEE","8 Oct 2020","","","IEEE","IEEE Conferences"
"Weighted area coverage of maritime joint search and rescue based on multi-agent reinforcement learning","Y. Gao; G. Jin; Y. Guo; G. Zhu; Q. Yang; K. Yang","College of Systems Engineering, National University of Defense Technology, Changsha, China; College of Systems Engineering, National University of Defense Technology, Changsha, China; College of Systems Engineering, National University of Defense Technology, Changsha, China; College of Systems Engineering, National University of Defense Technology, Changsha, China; College of Systems Engineering, National University of Defense Technology, Changsha, China; College of Systems Engineering, National University of Defense Technology, Changsha, China","2019 IEEE 3rd Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)","6 Feb 2020","2019","","","593","597","Maritime search and rescue is the last line of defense to ensure the safety of life and property at sea. To solve the problem of cooperative planning of regional coverage task for multiple search and rescue equipment in the actual search and rescue process. Firstly, we propose a weighted region model mapped by the Monte Carlo drift prediction model. And then we use three reinforcement learning algorithms (Q-learning, Sarsa, Sarsa (lambda)) for comparative experiments. Finally, three algorithms are evaluated per the coverage and repetition rate, and the experimental results show that Sarsa coverage rate is higher and repetition rate is lower. We have solved the weighted area coverage problem in the real search and rescue process, greatly improving the decision-making efficiency of the system, and making the search with the highest probability, high search coverage and low repetition rate, which has extremely high practical value.","","978-1-7281-0513-0","10.1109/IMCEC46724.2019.8984116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8984116","joint search and rescue;Weighted area coverage;Multiple-agent reinforcement learning;Collaborative planning","","decision making;emergency management;learning (artificial intelligence);Monte Carlo methods;multi-agent systems;probability","multiagent reinforcement learning;regional coverage task;rescue equipment;weighted region model;Monte Carlo drift prediction model;reinforcement learning algorithms;Q-learning;Sarsa coverage rate;weighted area coverage problem;search coverage;low repetition rate;maritime joint search and rescue","","3","","17","IEEE","6 Feb 2020","","","IEEE","IEEE Conferences"
"A Dual Memory Structure for Efficient Use of Replay Memory in Deep Reinforcement Learning","W. Ko; D. E. Chang","School of Electrical Engineering, KAIST, Daejeon, Korea; School of Electrical Engineering, KAIST, Daejeon, Korea","2019 19th International Conference on Control, Automation and Systems (ICCAS)","30 Jan 2020","2019","","","1483","1486","In this paper, we propose a dual memory structure for reinforcement learning algorithms with replay memory. The dual memory consists of a main memory that stores various data and a cache memory that manages the data and trains the reinforcement learning agent efficiently. Experimental results show that the dual memory structure achieves higher training and test scores than the conventional single memory structure in three selected environments of OpenAI Gym. This implies that the dual memory structure enables better and more efficient training than the single memory structure.","2642-3901","978-89-93215-17-5","10.23919/ICCAS47443.2019.8971629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8971629","Reinforcement Learning;Replay Memory;Prioritized Experience Replay (PER);Deep Q-Network (DQN)","","learning (artificial intelligence);storage management","deep reinforcement learning;dual memory structure;reinforcement learning algorithms;replay memory;cache memory;conventional single memory structure;OpenAI Gym","","2","","12","","30 Jan 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Energy Management in Hybrid Electric Vehicle","T. Gole; A. Hange; R. Dhar; A. Bhurke; D. F. Kazi","VJTI, Mumbai, India; VJTI, Mumbai, India; University of Glasgow, Scotland, UK; VJTI, Mumbai, India; VJTI, Mumbai, India","2019 International Conference on Power Electronics, Control and Automation (ICPECA)","30 Jan 2020","2019","","","1","5","Electric vehicles are clean alternatives to traditional vehicles. Fuel cell and lead-acid batteries are the two sources considered for electric vehicle energy management in this paper. Designing of efficient control system algorithm for electric vehicles is the key to long lasting life of the sources. In this paper, simulation of reinforcement learning algorithm and also hardware implementation of energy management algorithm is implemented by nvidia's JETSON TX2 reinforcement algorithm.","","978-1-7281-3958-6","10.1109/ICPECA47973.2019.8975505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8975505","Fuel cell-battery hybrid Electric vehicle;Bi-directional converter;Boost converter;Reinforcement learning;Energy management","","battery powered vehicles;energy management systems;fuel cell vehicles;hybrid electric vehicles;lead acid batteries;learning (artificial intelligence);power engineering computing","clean alternatives;hybrid electric vehicle;energy management algorithm;reinforcement learning algorithm;efficient control system algorithm;electric vehicle energy management;lead-acid batteries;fuel cell;traditional vehicles","","2","","10","IEEE","30 Jan 2020","","","IEEE","IEEE Conferences"
"Joint Optimization Dispatching for Hybrid Power System Based on Deep Reinforcement Learning","L. Tie; C. Dai; J. Feng; F. Da; H. Wei; Q. Yuchen; W. Shuang","Electric Power Dispatching Center, State Grid Liaoning Electric Power Supply Co, Ltd, Shenyang, China; Electric Power Dispatching Center, State Grid Liaoning Electric Power Supply Co, Ltd, Shenyang, China; Electric Power Dispatching Center, State Grid Liaoning Electric Power Supply Co, Ltd, Shenyang, China; Electrical Engineering Department, Tsinghua University, Beijing, China; Electrical Engineering Department, Tsinghua University, Beijing, China; Electrical Engineering Department, Tsinghua University, Beijing, China; Electrical Engineering Department, Tsinghua University, Beijing, China","2019 IEEE 8th International Conference on Advanced Power System Automation and Protection (APAP)","15 Oct 2020","2019","","","1289","1293","With large scale renewable power integrating, hybrid power system needs joint optimization dispatching. Considering complementary characteristics of different power types, this paper first constructs a day ahead time scale optimized dispatching model. The objectives are minimizing the system operation cost and maximizing the renewable energy consumption. The startup-stop status of thermal units and power output of different type power stations are selected as optimization variables. The problem then is modeled as a multistep Markov decision process which is a sequential decision process problem. A reinforcement learning method, Deep Deterministic Policy Gradient algorithm, is introduced to solve the decision problem. Finally, simulations have been carried out to validate the effectiveness of the proposed method. Numerical results show that the proposed method obtains a satisfied result which can meet the power load demanded, ensure the consumption of renewable energy and minimize the system cost meanwhile.","","978-1-7281-1722-5","10.1109/APAP47170.2019.9224835","Science and Technology Project of State Grid; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9224835","Joint Optimization Dispatching;Hybrid Power System;Renewable Energy;Reinforcement Learning","Dispatching;Optimization;Hybrid power systems;Reservoirs;Renewable energy sources;Hydroelectric power generation;Wind power generation","cost reduction;energy consumption;gradient methods;hybrid power systems;learning (artificial intelligence);Markov processes;numerical analysis;power engineering computing;power generation dispatch;renewable energy sources;thermal power stations","power load demanded model;numerical analysis;deep deterministic policy gradient algorithm;thermal units;renewable energy consumption;operation cost reduction;deep reinforcement learning model;hybrid power system;joint optimization dispatching model;Markov decision process;power stations","","2","","17","IEEE","15 Oct 2020","","","IEEE","IEEE Conferences"
"Pattern classification using fuzzy adaptive learning control network and reinforcement learning","K. H. Quah; C. Quek; G. Leedham","Intelligent Systems Laboratory, School of Computer Engineering, Nanyang Technological University, Singapore; Intelligent Systems Laboratory, School of Computer Engineering, Nanyang Technological University, Singapore; Intelligent Systems Laboratory, School of Computer Engineering, Nanyang Technological University, Singapore","Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.","5 Jun 2003","2002","3","","1439","1443 vol.3","In this paper, we formulate a pattern classification problem as a reinforcement learning problem. The problem is realized with a temporal difference method in a fuzzy adaptive learning control network (FALCON-R). FALCON-R is constructed by integrating two basic FALCON-ART networks as function approximators, where one acts as a critic network (fuzzy predictor) and the other as an action network (fuzzy controller). Thorough performance evaluation using Fisher's Iris data is presented and compared against a novel FALCON-ART network. We show that the system can converge faster, is able to escape from local minima, and has excellent disturbance rejection capability and has strengths as a pattern classification technique.","","981-04-7524-1","10.1109/ICONIP.2002.1202858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1202858","","Pattern classification;Fuzzy control;Adaptive systems;Programmable control;Adaptive control;Supervised learning;Control systems;Fuzzy systems;Delay effects;Feedback","fuzzy neural nets;ART neural nets;learning (artificial intelligence);pattern classification;function approximation","pattern classification;reinforcement learning;fuzzy adaptive learning control network;FALCON-ART networks;function approximation;Fisher Iris data;local minima;disturbance rejection","","1","","6","","5 Jun 2003","","","IEEE","IEEE Conferences"
"Flight Attitude Simulator Control System Design based on Model-free Reinforcement Learning Method","Y. Zuo; K. Deng; Y. Yang; T. Huang","Harbin Institute of Technology, Harbin, China; Sichuan Academy of Aerospace Technology, Chengdu, China; Harbin Institute of Technology, Harbin, China; Harbin Institute of Technology, Harbin, China","2019 IEEE 3rd Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)","6 Feb 2020","2019","","","355","361","Two-degree-of-freedom flight attitude simulator is a commonly used experimental platform in the control theory research. Due to the unknown environment and complicated constructions, it is difficult to build an accurate dynamical model. Reinforcement learning (RL) has been considered as a new means of solving complex and unpredictable control problems in recent years compared with traditional control methods. This is because RL algorithm is independent on the system model, and the controller parameters can be optimized by the iteration of the algorithm itself. In this paper, three model-free RL methods including Deep Q Network (DQN), Policy-Gradient (PG) and Deep Deterministic Policy (DDPG) are used to design the controller of the 2-DOF flight attitude simulator, and the performances of the methods are analyzed respectively. The results of theoretical analysis and experimental verification show good robustness in the control system. Moreover, the contribution of the RL method to the control field is pointed out and the problems that still need to be solved are listed at the end of this paper.","","978-1-7281-0513-0","10.1109/IMCEC46724.2019.8983893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8983893","reinforcement learning;Deep Q Network;PolicyGradient;Deep Deterministic Policy;aircraft control","","aerospace simulation;aircraft control;attitude control;learning (artificial intelligence);neural nets","RL algorithm;system model;controller parameters;model-free RL methods;RL method;control field;flight attitude simulator control system design;model-free reinforcement learning;two-degree-of-freedom flight attitude simulator;experimental platform;control theory research;accurate dynamical model;reinforcement learning;complex control problems;unpredictable control problems;deep deterministic policy;deep Q network;policy-gradient;2-DOF flight attitude simulator","","","","7","IEEE","6 Feb 2020","","","IEEE","IEEE Conferences"
"Motion Planning using Reinforcement Learning for Electric Vehicle Battery optimization(EVBO)","H. Soni; V. Gupta; R. Kumar","Centre for Converging Technologies University of Rajasthan, Jaipur, India; Dept. of Electrical Engineering, Malaviya National Institute of Technology, Jaipur, India; Dept. of Electrical Engineering, Malaviya National Institute of Technology, Jaipur, India","2019 International Conference on Power Electronics, Control and Automation (ICPECA)","30 Jan 2020","2019","","","1","6","The increasing demand for electric vehicle and autonomous vehicle as the alternate to the combustion-driven vehicle has motivated the research in the area of motion planning. Motion planmng is a complicated problem as it requires the consideration of multiple entities, mainly human behaviour. In this paper, reinforcement learning techniques are explored for the motion planning of an electnc vehicle(EV) while optimizing battery consumption. The EV travel time has also been evaluated under different reinforcement learning schemes. A traffic simulation network is developed for a high-traffic zone of Jaipur city using Simulation for Urban Mobility(SUMO) software. Model-based and model-free method like value-iteration and q-learning are applied to the developed traffic network. The results show that value iteration and q-learning have shown improved battery consumption. However, value iteration gives greater efficiency in terms of travel time as well as battery consumption.","","978-1-7281-3958-6","10.1109/ICPECA47973.2019.8975684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8975684","Reinforcement Learning;Battery consumption;Electnc Vehicle;Motion Planning;Q-learning;Value-Iteration","","battery powered vehicles;iterative methods;learning (artificial intelligence);optimisation;path planning;power engineering computing;power system simulation","motion planning;autonomous vehicle;combustion-driven vehicle;reinforcement learning techniques;battery consumption;value iteration method;electric vehicle battery optimization;EVBO;traffic simulation network;Jaipur;simulation for urban mobility;SUMO","","","","22","IEEE","30 Jan 2020","","","IEEE","IEEE Conferences"
"Learn to Rotate: Part Orientation for Reducing Support Volume via Generalizable Reinforcement Learning","P. Shi; Q. Qi; Y. Qin; F. Meng; S. Lou; P. J. Scott; X. Jiang","EPSRC Future Advanced Metrology Hub, School of Computing and Engineering, University of Huddersfield, U.K.; EPSRC Future Advanced Metrology Hub, School of Computing and Engineering, University of Huddersfield, U.K.; EPSRC Future Advanced Metrology Hub, School of Computing and Engineering, University of Huddersfield, U.K.; Alliance Manchester Business School, University of Manchester, U.K.; EPSRC Future Advanced Metrology Hub, School of Computing and Engineering, University of Huddersfield, U.K.; EPSRC Future Advanced Metrology Hub, School of Computing and Engineering, University of Huddersfield, U.K.; EPSRC Future Advanced Metrology Hub, School of Computing and Engineering, University of Huddersfield, U.K.","IEEE Transactions on Industrial Informatics","19 Oct 2023","2023","19","12","11687","11700","In design for additive manufacturing, an essential task is to determine the optimal build orientation of a part according to one or multiple factors. Heuristic search is used by the most part orientation methods to select the optimal orientation from a large solution space. Search algorithms occasionally converge towards the local optimum and waste considerable time on trial and error. This issue could be addressed if there was an intelligent agent that knew the optimal search path for a given 3D model. A straightforward method to construct such an agent is reinforcement learning (RL). By adopting this idea, the time-consuming online searches in existing part orientation methods will be moved to the offline learning stage, potentially improving part orientation performance. This is a challenging problem because the goal is to build an agent capable of rotating arbitrary 3D models, whereas RL agents frequently struggle to generalize in new scenarios. Therefore, this paper suggests a generalizable reinforcement learning (GRL) framework to train the agent, and a GRL benchmark to support the training, testing, and comparison of part orientation approaches. Experimental results de- monstrate that the proposed method on average outperforms others in terms of effectiveness and efficiency. It is proved to have the potential to solve the local minima problems raised in the existing approaches, to swiftly discover the global (sub-)optimal solution (i.e., on average 2.62× to 229.00× faster than the random search algorithm), and to generalize beyond the environment in which it was trained.","1941-0050","","10.1109/TII.2023.3249751","EPSRC UKRI Innovation Fellowship(grant numbers:EP/S001328/1); EPSRC New Investigator(grant numbers:EP/S000453/1); EPSRC Future Advanced Metrology Hub(grant numbers:EP/P006930/1); EPSRC Fellowship in Manufacturing(grant numbers:EP/R024162/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054468","Additive manufacturing (AM);build orientation determination;generalizable reinforcement learning (GRL);support volume","Three-dimensional displays;Solid modeling;Task analysis;Search problems;Reinforcement learning;Training;Intelligent agents;Three-dimensional printing","","","","","","41","IEEE","27 Feb 2023","","","IEEE","IEEE Journals"
"Evaluating effectiveness and portability of reinforcement learned dialogue strategies with real users: the talk TownInfo evaluation","O. Lemon; K. Georgila; J. Henderson","School of Informatics, Edinburgh University, UK; School of Informatics, Edinburgh University, UK; School of Informatics, Edinburgh University, UK","2006 IEEE Spoken Language Technology Workshop","19 Mar 2007","2006","","","178","181","We report evaluation results for real users of a learnt dialogue management policy versus a hand-coded policy in the TALK project's ""Townlnfo"" tourist information system. The learnt policy, for filling and confirming information slots, was derived from COMMUNICATOR (flight-booking) data using reinforcement learning (RL) as described in [2], ported to the tourist information domain (using a general method that we propose here), and tested using 18 human users in 180 dialogues, who also used a state-of-the-art hand- coded dialogue policy embedded in an otherwise identical system. We found that users of the (ported) learned policy had an average gain in perceived task completion of 14.2% (from 67.6% to 81.8% at p < .03), that the hand-coded policy dialogues had on average 3.3 more system turns (p < .01), and that the user satisfaction results were comparable, even though the policy was learned for a different domain. Combining these in a dialogue reward score, we found a 14.4% increase for the learnt policy (a 23.8% relative increase, p < .03). These results are important because they show a) that results for real users are consistent with results for automatic evaluation [2] of learned policies using simulated users [3, 4], b) that a policy learned using linear function approximation over a very large policy space [2] is effective for real users, and c) that policies learned using data for one domain can be used successfully in other domains. We also present a qualitative discussion of the learnt policy.","","1-4244-0872-5","10.1109/SLT.2006.326774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123391","","Filling;Learning;System testing;Function approximation;Graphical user interfaces;Informatics;Project management;Management information systems;Humans;Natural languages","function approximation;information systems;interactive systems;learning (artificial intelligence);natural language interfaces;travel industry","reinforcement learned dialogue strategies;Talk Towninfo evaluation;learnt dialogue management policy;hand-coded policy;Townlnfo tourist information system;linear function approximation","","28","2","15","IEEE","19 Mar 2007","","","IEEE","IEEE Conferences"
"Waitress quadcopter explores how to serve drinks by reinforcement learning","E. Camci; E. Kayacan","Nanyang Technological University, School of Mechanical and Aerospace Engineering, Singapore; Nanyang Technological University, School of Mechanical and Aerospace Engineering, Singapore","2016 IEEE Region 10 Conference (TENCON)","9 Feb 2017","2016","","","28","32","Versatility of unmanned aerial vehicles has paved the way for drones to be employed in a broad class of tasks from aerial photography to search and rescue, or even emergency medical service such as delivering defibrillator for heart attack victims. Such multi-functionality has sparked an enormous research area which is full of eminently innovative ideas for the smart nations. In this work, a quadcopter drone is used as a waitress to deliver drinks from the bar top, right to customers' tables in a pub. As every inexperienced apprentice, it goes through an educational process in order to learn how to serve drinks to fulfill the expectations of the customers. In the proposed reinforcement learning algorithm, the agent uses the evaluative feedback from the customers as reward and adapts its actions accordingly. Simulation studies demonstrate the comparative results for different action-selection policies under fairly subjective feedback conditions for different types of customers.","2159-3450","978-1-5090-2597-8","10.1109/TENCON.2016.7847952","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847952","","Rotors;Robots;Aerodynamics;Learning (artificial intelligence);Bars;Vehicle dynamics;Mathematical model","autonomous aerial vehicles;catering industry;control engineering computing;helicopters;learning (artificial intelligence)","drink delivery;quadcopter drone;unmanned aerial vehicles;reinforcement learning;waitress quadcopter","","10","","11","IEEE","9 Feb 2017","","","IEEE","IEEE Conferences"
"Context-Aware Safe Reinforcement Learning for Non-Stationary Environments","B. Chen; Z. Liu; J. Zhu; M. Xu; W. Ding; L. Li; D. Zhao","State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing, China; Department of Mechanical Engineering, Carnegie Mellon University, USA; Department of Mechanical Engineering, Carnegie Mellon University, USA; Department of Mechanical Engineering, Carnegie Mellon University, USA; Department of Mechanical Engineering, Carnegie Mellon University, USA; State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing, China; Department of Mechanical Engineering, Carnegie Mellon University, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","10689","10695","Safety is a critical concern when deploying reinforcement learning agents for realistic tasks. Recently, safe reinforcement learning algorithms have been developed to optimize the agent’s performance while avoiding violations of safety constraints. However, few studies have addressed the nonstationary disturbances in the environments, which may cause catastrophic outcomes. In this paper, we propose the context-aware safe reinforcement learning (CASRL) method, a metal-earning framework to realize safe adaptation in non-stationary environments. We use a probabilistic latent variable model to achieve fast inference of the posterior environment transition distribution given the context data. Safety constraints are then evaluated with uncertainty-aware trajectory sampling. Prior safety constraints are formulated with domain knowledge to improve safety during exploration. The algorithm is evaluated in realistic safety-critical environments with non-stationary disturbances. Results show that the proposed algorithm significantly outperforms existing baselines in terms of safety and robustness.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561593","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561593","","Adaptation models;Uncertainty;Toy manufacturing industry;Reinforcement learning;Probabilistic logic;Inference algorithms;Robustness","multi-agent systems;probability;reinforcement learning;safety;sampling methods","safe adaptation;nonstationary environments;probabilistic latent variable model;posterior environment transition distribution;safety constraints;uncertainty-aware trajectory sampling;realistic safety-critical environments;nonstationary disturbances;reinforcement learning agents;context-aware safe reinforcement learning;CASRL;meta-learning;domain knowledge","","7","","35","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"The application of path planning algorithm based on deep reinforcement learning for mobile robots","S. Tian; S. Lei; Q. Huang; A. Huang","School of Mechanical and Engineering, Wuhan University of Technology, Wuhan, China; School of Mechanical and Engineering, Wuhan University of Technology, Wuhan, China; School of Mechanical and Engineering, Wuhan University of Technology, Wuhan, China; School of Mechanical and Engineering, Wuhan University of Technology, Wuhan, China","2022 International Conference on Culture-Oriented Science and Technology (CoST)","28 Sep 2022","2022","","","381","384","To meet the need for autonomous route planning for tour guide robots in tourist venues, this paper proposes a path planning algorithm based on deep reinforcement learning. The traditional Deep Q-learning Network (DQN) algorithm two defects - overfitting and overestimation. This paper adopts a method that discards the experience pool and treats behavioural values equally, which not only solves the shortcomings of the traditional method, but also satisfies the need for mobile robots to lead tourists on tours through autonomous learning. The paper analyses the principle and process of the method and compares it with the traditional method through experiments to verify that the method outperforms the traditional method in terms of accuracy and speed.","","978-1-6654-6248-8","10.1109/CoST57098.2022.00084","Wuhan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9898568","Deep neural networks;Path planning;Mobile robots;Equivalence principle;Experience pool","Q-learning;Costs;Neural networks;Path planning;Planning;Mobile robots","learning (artificial intelligence);mobile robots;path planning;travel industry","deep reinforcement learning;mobile robots;autonomous route planning;tour guide robots;tourist venues;traditional Deep Q-learning Network algorithm;autonomous learning","","1","","13","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning Methods on Optimization Problems of Natural Gas Pipeline Networks","D. Yang; S. Yan; D. Zhou; T. Shao; L. Zhang; T. Xing","The key Lab. of power machinery and engineering of education ministry, Shanghai Jiao Tong University, Shanghai, China; The key Lab. of power machinery and engineering of education ministry, Shanghai Jiao Tong University, Shanghai, China; The key Lab. of power machinery and engineering of education ministry, Shanghai Jiao Tong University, Shanghai, China; PetroChina Beijing Oil & Gas Pipeline Control Center, Beijing, China; PetroChina Beijing Oil & Gas Pipeline Control Center, Beijing, China; PetroChina Beijing Oil & Gas Pipeline Control Center, Beijing, China","2020 4th International Conference on Smart Grid and Smart Cities (ICSGSC)","9 Nov 2020","2020","","","29","34","Traditional optimization methods of transport and distribution of natural gas pipeline networks have been widely used up to now with some problems in efficiency, cost, and flexibility, which are hard to be solved in the framework of traditional methods. In order to find the optimal solution in the constraints of each target of this optimization problem, this paper establishes a simulation model based on a part of a natural gas pipeline networks, and utilizes the reinforcement learning (RL) algorithm to analyze the model. The challenge of sparse rewards will also be dealt with. Then the optimal strategy of transport and distribution of gas in this model is obtained with different demands and initial conditions. The parameters of the operation of the strategy can be displayed in the simulation model and its advantages can also be fully reflected. Therefore, the scheme proposed in this paper can be directly or indirectly applied to the practical natural gas transport process.","","978-1-7281-9404-2","10.1109/ICSGSC50906.2020.9248563","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9248563","natural gas pipeline networks;simulation model;optimization problem;reinforcement learning;DDPG;sparse rewards;HER","Training;Analytical models;Smart cities;Pipelines;Reinforcement learning;Smart grids;Natural gas","gas industry;learning (artificial intelligence);natural gas technology;optimisation;pipelines;supply and demand","optimization problem;simulation model;natural gas pipeline networks;reinforcement learning algorithm;optimal strategy;natural gas transport process","","","","10","IEEE","9 Nov 2020","","","IEEE","IEEE Conferences"
"Drug Discovery using Generative Adversarial Network with Reinforcement Learning","G. Ravindra Padalkar; S. Dinkar Patil; M. Mallikarjun Hegadi; N. Kailash Jaybhaye","Electronics and Telecommunication MKSSS's Cummins college of Engineering for women, Pune; Electronics and Telecommunication MKSSS's Cummins college of Engineering for women, Pune; Electronics and Telecommunication MKSSS's Cummins college of Engineering for women, Pune; Electronics and Telecommunication MKSSS's Cummins college of Engineering for women, Pune","2021 International Conference on Computer Communication and Informatics (ICCCI)","21 Apr 2021","2021","","","1","3","A large amount of medical data is available to many of us and along with well-established deep learning algorithms, so the design of automated drug development pipelines has increased. The pipeline speeds up the drug discovery process and helps us better understand the disease. They help in planning pre-clinical lab experiments. This reduces the low productivity rate that the pharmaceutical companies are facing currently. Accurate predictions and insights are obtained by using deep learning techniques. So, this increases the need for deep learning approaches that have the potential to speed up the process, decision making, and reduce failure rates in drug discovery and development. With the fast development of computing power and enormous medical data, the project involving drug discovery have been benefited from artificial intelligence. The deep learning model knows as Generative Adversarial Network (GAN) with reinforcement learning is used to solve the problem.","2329-7190","978-1-7281-5875-4","10.1109/ICCCI50826.2021.9402449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402449","Convolution Neural Network;Generative Adversarial Network;Recurrent Neural Network;Reinforcement Learning","Drugs;Deep learning;Pipelines;Reinforcement learning;Generative adversarial networks;Data models;Gallium nitride","decision making;diseases;drugs;learning (artificial intelligence);pharmaceutical industry","low productivity rate;deep learning techniques;deep learning approaches;enormous medical data;deep learning model;Generative Adversarial Network;reinforcement learning;well-established deep learning algorithms;automated drug development pipelines;drug discovery process;pre-clinical lab experiments","","","","8","IEEE","21 Apr 2021","","","IEEE","IEEE Conferences"
