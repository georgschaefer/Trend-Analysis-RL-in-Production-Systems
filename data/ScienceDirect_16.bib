@article{JUNG2021117239,
title = {Optimal planning of a rooftop PV system using GIS-based reinforcement learning},
journal = {Applied Energy},
volume = {298},
pages = {117239},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117239},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921006607},
author = {Seunghoon Jung and Jaewon Jeoung and Hyuna Kang and Taehoon Hong},
keywords = {Rooftop PV, PV planning, Geographic information system, Reinforcement learning, Economic profitability},
abstract = {This study aimed to develop a geographic information system (GIS)-based reinforcement learning (RL) model for optimal planning of a rooftop PV system, considering the uncertainty of future scenarios throughout the life cycle of buildings. To that end, GIS was used to establish the spatial data for the rooftop PV installation, and an RL model was developed to maximize the economic profit of the rooftop PV installation in various locations and future scenarios. The developed model was applied to residential buildings in Nonhyeon district, South Korea to evaluate their economic profitability and to compare the model with the existing planning methods. With the use of the developed GIS-based RL model, the rooftop PV system became economically feasible, achieving average economic profit of 539,197 USD over all scenarios for all target buildings which was higher than that of the existing models by 4.4% and 4.3%. Furthermore, the developed model outperformed the existing models especially in volatile scenarios with lower solar radiation. Therefore, the use of the proposed GIS-based RL model can optimize the economic feasibility of rooftop PV systems for buildings, which will benefit building owners and community-level energy business owners. In conclusion, the developed model can promote the adoption of rooftop PV systems, which have 91.8% lower global warming potential than the Korean mixed grid, without additional subsidies to achieve Korea’s national CO2 emission reduction plan.}
}
@article{LINKWONGCHON2022105022,
title = {Adaptive neural control of PEMFC system based on data-driven and reinforcement learning approaches},
journal = {Control Engineering Practice},
volume = {120},
pages = {105022},
year = {2022},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2021.105022},
url = {https://www.sciencedirect.com/science/article/pii/S0967066121002811},
author = {Christophe Lin-Kwong-Chon and Cédric Damour and Michel Benne and Jean-Jacques Amangoua Kadjo and Brigitte Grondin-Pérez},
keywords = {Proton exchange membrane fuel cell, Active fault tolerance, Adaptive dynamic programming, Reinforcement learning, Deep echo state network},
abstract = {Proton exchange membrane fuel cell systems are being increasingly put forward as hydrogen energy carrier converters. Recent advancements in reliability strategies have been stimulated through maintaining a healthy operating condition of the system and covering plant faults. However, it is observed that occurrence or even the mitigation of these faults cause multilateral effects that can potentially destabilize the normal operation of the system. In the active fault tolerant control strategy, two modules are designed to fault management. The diagnostic module identifies the apparent fault and identifies the corrective commands, then the re-design module adapts the controller to dynamic system changes. In order to improve the generic characteristics of the re-design module, this paper presents a data-driven neural controller capable to automatically adapt to system health states. The developed approach comes from the machine learning class and combines adaptive dynamic programming, deep echo-state neural network models and fuzzy logic learning. The proposed controller is evaluated under occurrence of channels flooding and membrane drying faults, but also actuators and water purging disturbances. Simulation and experimental results show the effectiveness of the proposed data-driven approach without prior neural model training, while guaranteeing the stability and learning convergence of the adaptive controller.}
}
@article{CHEN2023,
title = {Container cluster placement in edge computing based on reinforcement learning incorporating graph convolutional networks scheme},
journal = {Digital Communications and Networks},
year = {2023},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2023.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S2352864823000470},
author = {Zhuo Chen and Bowen Zhu and Chuan Zhou},
keywords = {Edge computing, Network virtualization, Container cluster, Deep reinforcement learning, Graph convolutional network},
abstract = {Container-based virtualization technology has been more widely used in edge computing environments recently due to its advantages of lighter resource occupation, faster startup capability, and better resource utilization efficiency. To meet the diverse needs of tasks, it is usually needs to instantiate multiple network functions in the form of containers interconnect various generated containers to build a Container Cluster (CC). Then CCs will be deployed on edge service nodes with relatively limited resources. However, the increasingly complex and time-varying nature of tasks brings great challenges to optimal placement of CC. This paper regards the charges for various resources occupied by providing services as revenue, the service efficiency and energy consumption as cost, thus formulates a Mixed Integer Programming (MIP) model to describe the optimal placement of CC on edge service nodes. Furthermore, an Actor-Critic based Deep Reinforcement Learning (DRL) incorporating Graph Convolutional Networks (GCN) framework named as RL-GCN is proposed to solve the optimization problem. The framework obtains an optimal placement strategy through self-learning according to the requirements and objectives of the placement of CC. Particularly, through the introduction of GCN, the features of the association relationship between multiple containers in CCs can be effectively extracted to improve the quality of placement. The experiment results show that under different scales of service nodes and task requests, the proposed method can obtain the improved system performance in terms of placement error ratio, time efficiency of solution output and cumulative system revenue compared with other representative baseline methods.}
}
@article{MOHAMMADI2022109459,
title = {Drops on surface optimization (DSO): A new reinforcement learning based metaheuristic algorithm for virtual network functions placement in distributed cloud architectures},
journal = {Computer Networks},
volume = {219},
pages = {109459},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109459},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622004935},
author = {Marzieh Mohammadi and S. Ahmad Motamedi and Saeed Sharifian},
keywords = {Cloud architecture, Metaheuristic algorithm, Network function virtualization (NFV), Optimization, Reinforcement learning, Resource allocation},
abstract = {With the goal of making the networks more agile, flexible and cost efficient, network function virtualization (NFV) is one of the latest and most promising technologies introduced. This technology is the key block of future internet and plays a critical role in modern networks. Using this paradigm, different network functions can be independently deployed on general purpose hardware instead of residing on dedicated hardware (network appliances in traditional networks). NFV enables faster development and lower costs. As any other new technology, NFV has some emerging challenges along with its benefits. The main research challenge of NFV is the necessity of optimally placing the virtual network functions (VNFs) in an NFV-enabled network. This problem is mainly known as the VNF placement (VNFP) problem in the literature. Due to the nature of network applications, this placement should be scalable for large network sizes. In this paper, the VNFP problem is addressed and a new metaheuristic algorithm is proposed based on the behavior of droplets on a surface, named the Drops on Surface Optimization (DSO) Algorithm. In order to enhance the proposed algorithm by dynamic selection strategy, a reinforcement learning approach (Q learning) is adopted. A comprehensive evaluation is carried out using standard networks, compared with several methods. Simulation results reveal that our proposed algorithm outperforms other state-of-the-art approaches in terms of total end-to-end propagation delay of the placed service chain, cost of placement, number of active servers involved in the placement and scalability.}
}
@article{WANG2023109989,
title = {Parameterized deep reinforcement learning with hybrid action space for energy efficient data center networks},
journal = {Computer Networks},
volume = {235},
pages = {109989},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109989},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623004346},
author = {Ting Wang and Xi Fan and Kai Cheng and Xiao Du and Haibin Cai and Yang Wang},
keywords = {Data center networks, Energy efficiency, Green data center},
abstract = {To ensure the delivery of high-performance and reliable services, data center networks (DCNs) are often over-provisioned for peak workload and traffic bursts. However, in real-world data centers, network traffic seldom reaches peak capacity of the network, resulting in significant energy waste. Traditional energy conservation approaches either suffer from high computational complexity and low solution quality, or their strategies cannot be dynamically adjusted to accommodate changes in data center network traffic. Deep reinforcement learning (DRL) provides an effective way to deal with these issues. However, most of the existing DRL-based schemes only consider either a continuous action space or a discrete action space, which greatly restricts the optimality of decisions. To solve these problems, this paper proposes a novel DRL-based DCN energy optimization framework, named SmartDCN. Specifically, SmartDCN consists of a traffic prediction module (TPM) and an energy optimization module (EOM). TPM incorporates an improved LSTM model JANET with an attention mechanism providing a high prediction accuracy, while EOM integrates our newly proposed parameterized DRL algorithm, named PAS-DQN, combining with the discrete-continuous hybrid action space. PAS-DQN implements a two-level control mechanism for the network, using TPM to predict future traffic in the data center as input. It is devoted to dynamically aggregating current traffic and makes tradeoffs between energy efficiency, performance, and robustness to optimize the network’s power consumption by dynamically calculating the minimum required network subset and turning off the non-involved network devices to achieve power savings. Experimental results show that SmartDCN significantly outperforms the existing state-of-the-art schemes in terms of energy savings under various network conditions.}
}
@article{YOO2021487,
title = {A Dynamic Penalty Function Approach for Constraint-Handling in Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {3},
pages = {487-491},
year = {2021},
note = {16th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.289},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321010624},
author = {Haeun Yoo and Victor M. Zavala and Jay H. Lee},
keywords = {Reinforcement Learning, Penalty approach, Dynamic Penalty, Constraints},
abstract = {Reinforcement learning (RL) is attracting attention as an effective way to solve sequential optimization problems that involve high dimensional state/action space and stochastic uncertainties. Many such problems involve constraints expressed by inequality constraints. This study focuses on using RL to solve constrained optimal control problems. Most RL application studies have dealt with inequality constraints by adding soft penalty terms for violating the constraints to the reward function. However, while training neural networks to learn the value (or Q) function, one can run into computational issues caused by the sharp change in the function value at the constraint boundary due to the large penalty imposed. This difficulty during training can lead to convergence problems and ultimately lead to poor closed-loop performance. To address this issue, this study proposes a dynamic penalty (DP) approach where the penalty factor is gradually and systematically increased during training as the iteration episodes proceed. We first examine the ability of a neural network to represent a value function when uniform, linear, or DP functions are added to prevent constraint violation. The agent trained by a Deep Q Network (DQN) algorithm with the DP function approach was compared with agents with other constant penalty functions in a simple vehicle control problem. Results show that the proposed approach can improve the neural network approximation accuracy and provide faster convergence when close to a solution.}
}
@article{HIRASHIMA2005318,
title = {A NEW METHOD FOR MARSHALING PLAN USING A REINFORCEMENT LEARNING CONSIDERING DESIRED LAYOUT OF CONTAINERS IN PORT TERMINALS},
journal = {IFAC Proceedings Volumes},
volume = {38},
number = {1},
pages = {318-323},
year = {2005},
note = {16th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20050703-6-CZ-1902.00274},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016362863},
author = {Yoichi HIRASHIMA and Osamu FURUYA and Kazuhiro TAKEDA and Mingcong DENG and Akira INOUE},
keywords = {Container marshaling, Block stacking problem, Q-learning, Reinforcement learning, Binary tree},
abstract = {In container yard terminals, containers brought by trucks in the random order. Containers have to be loaded into the ship in a certain order, since each container has its own shipping destination and it cannot be rearranged after loading. Therefore, containers have to be rearranged from the initial arrangement into the desired arrangement before shipping. In the problem, the number of container-arrangements increases by the exponential rate with increase of total count of containers, and the rearrangement process occupies large part of total run time of material handling operation at the terminal. Moreover, conventional methods require enormous time and cost to derive an admissible result for rearrangement process. In this paper, a Q-Learning algorithm considering the desired position of containers for a marshaling in the container yard terminal is proposed. In the proposed method, the learning process consists of two parts: rearrangement plan assuring explicit transfer of container to the desired position, and, removal plan for preparing the rearrange operation. Using the proposed method, the learning performance can be improved as compared to the conventional method. In order to show effectiveness of the proposed method, computer simulations for several examples are conducted.}
}
@article{CHARBONNIER2022118825,
title = {Scalable multi-agent reinforcement learning for distributed control of residential energy flexibility},
journal = {Applied Energy},
volume = {314},
pages = {118825},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.118825},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922002689},
author = {Flora Charbonnier and Thomas Morstyn and Malcolm D. McCulloch},
keywords = {Energy management system, Multi-agent reinforcement learning, Demand-side response, Peer-to-peer, Prosumer, Smart grid},
abstract = {This paper proposes a novel scalable type of multi-agent reinforcement learning-based coordination for distributed residential energy. Cooperating agents learn to control the flexibility offered by electric vehicles, space heating and flexible loads in a partially observable stochastic environment. In the standard independent Q-learning approach, the coordination performance of agents under partial observability drops at scale in stochastic environments. Here, the novel combination of learning from off-line convex optimisations on historical data and isolating marginal contributions to total rewards in reward signals increases stability and performance at scale. Using fixed-size Q-tables, prosumers are able to assess their marginal impact on total system objectives without sharing personal data either with each other or with a central coordinator. Case studies are used to assess the fitness of different combinations of exploration sources, reward definitions, and multi-agent learning frameworks. It is demonstrated that the proposed strategies create value at individual and system levels thanks to reductions in the costs of energy imports, losses, distribution network congestion, battery depreciation and greenhouse gas emissions.}
}
@article{MAGHAMI2022114385,
title = {Automated design of phononic crystals under thermoelastic wave propagation through deep reinforcement learning},
journal = {Engineering Structures},
volume = {263},
pages = {114385},
year = {2022},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2022.114385},
url = {https://www.sciencedirect.com/science/article/pii/S0141029622004990},
author = {Ali Maghami and Seyed Mahmoud Hosseini},
keywords = {Deep reinforcement learning, Phononic crystal, Bandgap, Thermoelastic wave propagation, Deep deterministic policy gradient, Automated design},
abstract = {This article presents a novel concept of deep reinforcement learning (DRL) to facilitate the reverse design of layered phononic crystal (PC) beams with anticipated band structures focusing on the band structure analysis of thermoelastic waves propagating. To this end, we define the reverse design of phononic crystals (PCs) as a game for the DRL agent. To achieve the desired band structure, the DRL agent needs to obtain the topological system of PC. We trained a DRL agent called deep deterministic policy gradient (DDPG). An environment is developed and used to simulate the reverse design of layered PCs with the acquisition of a reward function. The presented reward function encourages the agent to achieve the desired bandgaps. The trained DDPG agent can maximize the game’s score by attaining the desired bandgap. The presented concept allows the user to instantly generate the design parameters through the trained DDPG agent without unnecessary search over the design space. We demonstrated that the DRL agent could perform very well for the automated design of PCs with hundred design cases.}
}
@article{LIANG2023122164,
title = {An information entropy-driven evolutionary algorithm based on reinforcement learning for many-objective optimization},
journal = {Expert Systems with Applications},
pages = {122164},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122164},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423026660},
author = {Peng Liang and Yangtao Chen and Yafeng Sun and Ying Huang and Wei Li},
keywords = {Many-objective optimization, Evolutionary algorithm, Reinforcement learning, Irregular pareto fronts, Information entropy},
abstract = {Many-objective optimization problems (MaOPs) are challenging tasks involving optimizing many conflicting objectives simultaneously. Decomposition-based many-objective evolutionary algorithms have effectively maintained a balance between convergence and diversity in recent years. However, these algorithms face challenges in accurately approximating the complex geometric structure of irregular Pareto fronts (PFs). In this paper, an information entropy-driven evolutionary algorithm based on reinforcement learning (RL-RVEA) for many-objective optimization with irregular Pareto fronts is proposed. The proposed algorithm leverages reinforcement learning to guide the evolution process by interacting with the environment to learn the shape and features of PF, which adaptively adjusts the distribution of reference vectors to cover the PFs structure effectively. Moreover, an information entropy-driven adaptive scalarization approach is designed in this papaer to reflect the diversity of nondominated solutions, which facilitates the algorithm to balance multiple competing objectives adaptively and select solutions efficiently while maintaining individual diversity. To verify the effectiveness of the proposed algorithm, the RL-RVEA compared with seven state-of-the-art algorithms on the DTLZ, MaF, and WFG test suites and four real-world MaOPs. The results of the experiments demonstrate that the suggested algorithm provides a novel and practical method for addressing MaOPs with irregular PFs.}
}
@article{YANG2023107280,
title = {Reinforcement learning strategies in cancer chemotherapy treatments: A review},
journal = {Computer Methods and Programs in Biomedicine},
volume = {229},
pages = {107280},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.107280},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722006617},
author = {Chan-Yun Yang and Chamani Shiranthika and Chung-Yih Wang and Kuo-Wei Chen and Sagara Sumathipala},
keywords = {Dynamic treatment regimen, Chemotherapy, Reinforcement learning, Optimal drug schedule},
abstract = {Background and objective
Cancer is one of the major causes of death worldwide and chemotherapies are the most significant anti-cancer therapy, in spite of the emerging precision cancer medicines in the last 2 decades. The growing interest in developing the effective chemotherapy regimen with optimal drug dosing schedule to benefit the clinical cancer patients has spawned innovative solutions involving mathematical modeling since the chemotherapy regimens are administered cyclically until the futility or the occurrence of intolerable adverse events. Thus, in this present work, we reviewed the emerging trends involved in forming a computational solution from the aspect of reinforcement learning.
Methods
Initially, this survey in-depth focused on the details of the dynamic treatment regimens from a broad perspective and then narrowed down to inspirations from reinforcement learning that were advantageous to chemotherapy dosing, including both offline reinforcement learning and supervised reinforcement learning.
Results
The insights established in the chemotherapy-planning problem associated with the Reinforcement Learning (RL) has been discussed in this study. It showed that the researchers were able to widen their perspectives in comprehending the theoretical basis, dynamic treatment regimens (DTR), use of the adaptive control on DTR, and the associated RL techniques.
Conclusions
This study reviewed the recent researches relevant to the topic, and highlighted the challenges, open questions, possible solutions, and future steps in inventing a realistic solution for the aforementioned problem.}
}
@article{NGUYEN2021221,
title = {Reinforcement Learning and Adaptive Optimal Control of Congestion Pricing},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {2},
pages = {221-226},
year = {2021},
note = {16th IFAC Symposium on Control in Transportation Systems CTS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321004626},
author = {Tri Nguyen and Weinan Gao and Xiangnan Zhong and Shaurya Agarwal},
keywords = {Congestion pricing, reinforcement learning, adaptive optimal control},
abstract = {The increasing road traffic congestion has urged researchers to look for solutions to tackle the problem. Many different interventions reduce traffic jams including, optimizing traffic-lights, using video surveillance to monitor road conditions, strategic road network resilience, and congestion pricing. This paper uses a nonlinear model for dynamic congestion pricing, considering manual-toll and automatic toll lanes using wireless communication technologies. The model can adjust the traveling demand and improve traffic flow performance by charging more for entering express lanes. We linearize the model about the equilibrium states and propose a reinforcement learning-based adaptive optimal control approach to learn the optimal control gain of the linearized model. Further, we rigorously show that the developed optimal controller can ensure the stability of the original nonlinear closed-loop system by making its output asymptotically converge to zero. Finally, the proposed approach is validated by numerical simulations.}
}
@article{SALVI2022276,
title = {Stabilization of vertical motion of a vehicle on bumpy terrain using deep reinforcement learning*},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {37},
pages = {276-281},
year = {2022},
note = {2nd Modeling, Estimation and Control Conference MECC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.11.197},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322028403},
author = {Ameya Salvi and John Coleman and Jake Buzhardt and Venkat Krovi and Phanindra Tallapragada},
keywords = {Autonomous robotic systems, Intelligent robotics, Reinforcement Learning, Vehicle Dynamics, Cyber-Physical Systems},
abstract = {Stabilizing vertical dynamics for on-road and off-road vehicles is an important research area that has been looked at mostly from the point of view of ride comfort. The advent of autonomous vehicles now shifts the focus more towards developing stabilizing techniques from the point of view of onboard proprioceptive and exteroceptive sensors whose real-time measurements influence the performance of an autonomous vehicle. The current solutions to this problem of managing the vertical oscillations usually limit themselves to the realm of active suspension systems without much consideration to modulating the vehicle velocity, which plays an important role by the virtue of the fact that vertical and longitudinal dynamics of a ground vehicle are coupled. The task of stabilizing vertical oscillations for military ground vehicles becomes even more challenging due lack of structured environments, like city roads or highways, in offroad scenarios. Moreover, changes in structural parameters of the vehicle, such as mass (due to changes in vehicle loading), suspension stiffness and damping values can have significant effect on the controller's performance. This demands the need for developing deep learning based control policies, that can take into account an extremely large number of input features and approximate a near optimal control action. In this work, these problems are addressed by training a deep reinforcement learning agent to minimize the vertical acceleration of a scaled vehicle travelling over bumps by controlling its velocity.}
}
@article{WULFING201966,
title = {Adaptive long-term control of biological neural networks with Deep Reinforcement Learning},
journal = {Neurocomputing},
volume = {342},
pages = {66-74},
year = {2019},
note = {Advances in artificial neural networks, machine learning and computational intelligence},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.10.084},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219301468},
author = {Jan M. Wülfing and Sreedhar S. Kumar and Joschka Boedecker and Martin Riedmiller and Ulrich Egert},
keywords = {Reinforcement Learning, Biological neural networks, Artificial neural networks, Deep learning, Deep Reinforcement Learning, Neurostimulation, Closed-loop neurostimulation},
abstract = {Driving activity patterns in the brain to desired levels is an important therapeutic strategy for many neurological disorders. The highly dynamic nature of neuronal networks and changes with disease progression create an urgent need for closed-loop control. Without adequate mathematical models of such complex networks, it remains unclear how tractable control problems can be formulated and solved for neurobiological systems. Reinforcement Learning (RL) is a promising tool to address such challenges, but has rarely been used for the long-term control of live, plastic neural networks. This is a difficult problem since it requires the controller to adapt to poorly characterized non-stationary background processes that alter stimulus-response relations over time. We captured these challenges in a novel control task, defined as clamping response strengths to predefined levels over long durations in a living model system, namely generic BNNs in vitro grown on microelectrode arrays. We show that by defining appropriate state-action spaces and employing powerful nonlinear RL methods such as Deep RL, adaptivity to non-stationary background processes can be achieved in a series of experiments: In 27/29 networks, the learned controllers were able to improve performance compared to a random and a (linear) LSPI controller by a large margin.}
}
@article{QI2022121703,
title = {Hierarchical reinforcement learning based energy management strategy for hybrid electric vehicle},
journal = {Energy},
volume = {238},
pages = {121703},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121703},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221019514},
author = {Chunyang Qi and Yiwen Zhu and Chuanxue Song and Guangfu Yan and Feng Xiao and  {Da wang} and Xu Zhang and Jingwei Cao and Shixin Song},
keywords = {Deep reinforcement learning, Energy management, Hybrid electric vehicle, Hierarchical reinforcement learning},
abstract = {As the core technology of hybrid electric vehicles (HEVs), energy management strategy directly affects the fuel consumption of vehicles. This research proposes a novel reinforcement learning (RL)-based algorithm for energy management strategy of HEVs. Hierarchical structure is used in deep Q-learning algorithm (DQL-H) to get the optimal solution of energy management. Through this new RL method, we not only solve the problem of sparse reward in training process, but also achieve the optimal power distribution. In addition, as a kind of hierarchical algorithm, DQL-H can change the way of exploration of the vehicle environment and make it more effective. The experimental results show that the proposed DQL-H method realizes better training efficiency and lower fuel consumption, compared to other RL-based ones.}
}
@incollection{MOHAMMED2019187,
title = {Chapter 9 - Reinforcement learning and deep neural network for autonomous driving},
editor = {Guido Dartmann and Houbing Song and Anke Schmeink},
booktitle = {Big Data Analytics for Cyber-Physical Systems},
publisher = {Elsevier},
pages = {187-213},
year = {2019},
isbn = {978-0-12-816637-6},
doi = {https://doi.org/10.1016/B978-0-12-816637-6.00009-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166376000099},
author = {Shawan Taha Mohammed and Andreas Bytyn and Gerd Ascheid and Guido Dartmann},
keywords = {Artificial intelligence, Autonomous driving, Reinforcement learning, Deep learning, Deep deterministic policy gradient, SUMO simulation},
abstract = {This chapter deals with a behavioral decision model for autonomous driving. Such a model aims at an application in which reinforcement learning and deep neural networks are used for autonomous driving. The training and evaluation takes place in a simulation. For this purpose, SUMO Simulation will be used to create own scenarios. An own sensor modeling will also be developed. The Deep Deterministic Policy Gradient (DDPG) method by Lillicrap et al. (2015) is used as a reinforcement learning algorithm and is adapted to this usecase. The work shows that the learned model can react to different types of drivers and the surrounding traffic. It also supports a safe and fast response to driving reactions.}
}
@article{ALABI2023120633,
title = {Automated deep reinforcement learning for real-time scheduling strategy of multi-energy system integrated with post-carbon and direct-air carbon captured system},
journal = {Applied Energy},
volume = {333},
pages = {120633},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120633},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922018906},
author = {Tobi Michael Alabi and Nathan P. Lawrence and Lin Lu and Zaiyue Yang and R. {Bhushan Gopaluni}},
keywords = {Deep reinforcement learning, Carbon capture, Zero-emission, Carbon removal, Integrated energy system},
abstract = {The carbon-capturing process with the aid of CO2 removal technology (CDRT) has been recognised as an alternative and a prominent approach to deep decarbonisation. However, the main hindrance is the enormous energy demand and the economic implication of CDRT if not effectively managed. Hence, a novel deep reinforcement learning agent (DRL), integrated with an automated hyperparameter selection feature, is proposed in this study for the real-time scheduling of a multi-energy system (MES) coupled with CDRT. Post-carbon capture systems (PCCS) and direct-air capture systems (DACS) are considered CDRT. Various possible configurations are evaluated using real-time multi-energy data of a district in Arizona, the United States, and CDRT parameters from manufacturers' catalogues and pilot project documentation. The simulation results validate that an optimised soft-actor critic (SAC) DRL algorithm outperformed the Twin-delayed deep deterministic policy gradient (TD3) algorithm due to its maximum entropy feature. We then trained four (4) SAC DRL agents, equivalent to the number of considered case studies, using optimised hyperparameter values and deployed them in real time for evaluation. The results show that the proposed DRL agent can meet the prosumers' multi-energy demand and schedule the CDRT energy demand economically without specified constraints violation. Also, the proposed DRL agent outperformed rule-based scheduling by 23.65%. However, the configuration with PCCS and solid-sorbent DACS is considered the most suitable configuration with a high CO2 captured-released ratio (CCRR) of 38.54, low CO2 released indicator (CRI) value of 2.53, and a 36.5% reduction in CDR cost due to waste heat utilisation and high absorption capacity of the selected sorbent. However, the adoption of CDRT is not economically viable at the current carbon price. Finally, we showed that CDRT would be attractive at a carbon price of 400-450USD/ton with the provision of tax incentives by the policymakers.}
}
@article{PARK2022120111,
title = {Multi-agent deep reinforcement learning approach for EV charging scheduling in a smart grid},
journal = {Applied Energy},
volume = {328},
pages = {120111},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120111},
url = {https://www.sciencedirect.com/science/article/pii/S030626192201368X},
author = {Keonwoo Park and Ilkyeong Moon},
keywords = {Electric vehicles, Smart grid, Scheduling, Multi-agent deep reinforcement learning},
abstract = {As the competitive advantages of electric vehicles, both in terms of operating costs and eco-friendly characteristics have gained attention, the demand for electric vehicles has increased, and studies for efficiently charging electric vehicles are being actively conducted. Previous studies have mainly focused on scheduling one electric vehicle visiting a charging station or scheduling multiple electric vehicles in a centralized execution method. However, a decentralized execution method that can schedule multiple vehicles according to their status is more suitable in a realistic smart grid charging environment that requires quick decisions. Therefore, we propose a multi-agent deep reinforcement learning approach with a centralized training and decentralized execution method that can derive charging scheduling for each electric vehicle. Computational experiments show that the proposed approach shows desirable performance in minimizing the operating cost of electric vehicles.}
}
@article{KAYGISIZ2002481,
title = {SMOOTHING STABILITY ROUGHNESS OF FRACTAL BOUNDARIES USING REINFORCEMENT LEARNING},
journal = {IFAC Proceedings Volumes},
volume = {35},
number = {1},
pages = {481-485},
year = {2002},
note = {15th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20020721-6-ES-1901.01066},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015394878},
author = {Burak H. Kaygisiz and Aydan M. Erkmen and Ismet Erkmen},
keywords = {Fractals, Stability domains, Nonlinear systems, Intelligence, Learning algorithms rough sets},
abstract = {We describe in this paper a new approach to the identification of the stable regions of nonlinear systems, using cell mapping equipped with measures of fractal dimension and those from rough set theory. The proposed fractal-rough set approach divides the state space into cells, finds out the chaotic region using cell to cell mapping technique and classifies the cells according to the fractal dimension of each cell. Assigning the fractal dimension to each cell in the state space, cells are then classified as the members of lower approximation, upper approximation or boundary region of the stable region with the help of rough set theory. Rough sets with fractal dimension as their attributes are used to model the uncertainty on the stable region which is treated as a set of cells in this paper. This uncertainty is then smoothed by a reinforcement learning algorithm. Our approach is applied to the stability of a dynamical system with finger shaped boundary region.}
}
@article{ALMARRIDI2021108279,
title = {Reinforcement learning approaches for efficient and secure blockchain-powered smart health systems},
journal = {Computer Networks},
volume = {197},
pages = {108279},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108279},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621003005},
author = {Abeer Z. Al-Marridi and Amr Mohamed and Aiman Erbad},
keywords = {Reinforcement learning, Blockchain, e-Health, Multi-objective optimization, Healthchain-RL},
abstract = {Emerging technological innovation toward e-Health transition is a worldwide priority for ensuring people’s quality of life. Hence, secure exchange and analysis of medical data amongst diverse organizations would increase the efficiency of e-Health systems toward elevating medical phenomena such as outbreaks and acute patients’ disorders. However, medical data exchange is challenging since issues, such as privacy, security, and latency may arise. Thus, this paper introduces Healthchain-RL, an adaptive, intelligent, consortium, and secure Blockchain-powered health system employing artificial intelligence, especially Deep Reinforcement Learning (DRL). Blockchain and DRL technologies show their robust performance in different fields, including healthcare systems. The proposed Healthchain-RL framework aggregates heterogeneous healthcare organizations with different requirements using the power of Blockchain while maintaining an optimized framework via an online intelligent decision-making RL algorithm. Hence, an intelligent Blockchain Manager (BM) was proposed based on the DRL, mainly Deep Q-Learning and it is variations, to optimizes the Blockchain network’s behavior in real-time while considering medical data requirements, such as urgency and security levels. The proposed BM works toward intelligently changing the blockchain configuration while optimizing the trade-off between security, latency, and cost. The optimization model is formulated as a Markov Decision Process (MDP) and solved effectively using three RL-based techniques. These three techniques are Deep Q-Networks (DQN), Double Deep Q-Networks (DDQN), and Dueling Double Deep Q-Networks (D3QN). Finally, a comprehensive comparison is conducted between the proposed techniques and two heuristic approaches. The proposed strategies converge in real-time adaptivity to the system status while maintaining maximum security and minimum latency and cost.}
}
@article{MARTINSEN2018329,
title = {Straight-Path Following for Underactuated Marine Vessels using Deep Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {29},
pages = {329-334},
year = {2018},
note = {11th IFAC Conference on Control Applications in Marine Systems, Robotics, and Vehicles CAMS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.502},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318321918},
author = {Andreas B. Martinsen and Anastasios M. Lekkas},
keywords = {Deep reinforcement learning, path following, marine control systems, deep deterministic policy gradients},
abstract = {We propose a new framework, based on reinforcement learning, for solving the straight-path following problem for underactuated marine vessels under the influence of unknown ocean current. A dynamic model from the Marine Systems Simulator is employed to simulate the motion of a mariner-class vessel, however the policy search algorithm has no prior knowledge of the system it is assigned to control. A deep neural network is used as function approximator and the deep deterministic policy gradients method is employed to extract a suitable policy that minimizes the cross-track error. Two intuitive reward functions, which in addition prevent noisy rudder behavior, are proposed and compared. The simulation results demonstrate excellent performance, also in comparison with the line-of-sight guidance law.}
}
@article{TANG2024121880,
title = {A Systematic Literature Review of Reinforcement Learning-based Knowledge Graph Research},
journal = {Expert Systems with Applications},
volume = {238},
pages = {121880},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121880},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423023825},
author = {Zifang Tang and Tong Li and Di Wu and Junrui Liu and Zhen Yang},
keywords = {Knowledge graphs, Reinforcement learning, Systematic literature review, Markov decision processes},
abstract = {Knowledge graphs (KGs) model entities or concepts and their relations in a structural manner. The incompleteness has turned out to be the main challenge that hinders the application of KGs. Recently, reinforcement learning (RL) has been recognized as an effective method to deal with such a challenge, which models research tasks into a sequence decision problem without labels. Although an increasing number of studies investigate and analyze KGs using RL, there lacks a systematic literature review that comprehensively and quantitatively analyzes the landscape of RL-based KG research (RL-KG for short). As a result, researchers may have encountered difficulties in appropriately adopting RL techniques in KG research, even reinventing the wheels. In this paper, we follow the Systematic Literature Review (SLR) methodology to survey, screen, and investigate papers of RL-KG. Specifically, we identify 109 highly related papers from 1542, and systematically investigate them with regard to the following five research questions: (1) to what extent RL-KG have been investigated; (2) what application domains have been covered; (3) what RL techniques have been mainly considered; (4) whether there is a connection between the influence and reproducibility of these papers; (5) what specialized datasets, evaluation metrics, and publication venues have been applied. Through an in-depth analysis of the review results, we systematically and comprehensively identify some significant phenomena and analyze the reasons and difficulties of these phenomena. Based on such analysis, we tentatively propose promising future research topics to promote the RL-KG.}
}
@article{HE2022108406,
title = {Ensemble-based Deep Reinforcement Learning for robust cooperative wind farm control},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {143},
pages = {108406},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2022.108406},
url = {https://www.sciencedirect.com/science/article/pii/S0142061522004197},
author = {Binghao He and Huan Zhao and Gaoqi Liang and Junhua Zhao and Jing Qiu and Zhao Yang Dong},
keywords = {Wind farm control, Deep reinforcement learning, Deep deterministic policy gradient, Learning cost, Ensemble learning},
abstract = {The wake effect is the major obstacle to reaching the maximum power generation for wind farms, since choosing the suitable wake model that satisfies both computational cost and accuracy is a difficult task. Deep Reinforcement Learning (DRL) is a powerful data-driven method that can learn the optimal control policy without modeling the environment. However, the “trial and error” mechanism of DRL may cause high costs during the learning process. To address this issue, we propose an ensemble-based DRL wind farm control framework. Under this framework, a new algorithm called Actor Bagging Deep Deterministic Policy Gradient (AB-DDPG) is proposed, which combines the actor-network bagging method with the Deep Deterministic Policy Gradient. The gradient of the proposed method is proved to be consistent with the DDPG method. The experiment results in WFSim show that AB-DDPG can learn the optimal control policy with lower learning cost and a more robust learning process.}
}
@article{HE2023108962,
title = {Frequency regulation of multi-microgrid with shared energy storage based on deep reinforcement learning},
journal = {Electric Power Systems Research},
volume = {214},
pages = {108962},
year = {2023},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2022.108962},
url = {https://www.sciencedirect.com/science/article/pii/S0378779622010112},
author = {Xingtang He and Shaoyun Ge and Hong Liu and Zhengyang Xu and Yang Mi and Chengshan Wang},
keywords = {Microgrid, Frequency regulation, Share energy storage, Deep reinforcement learning},
abstract = {The microgrid is one of the fundamental ways to consume renewable energy, and the safety and economy of its frequency regulation are widely concerned and studied. For the microgrid with shared energy storage, a new frequency regulation method based on deep reinforcement learning (DRL) is proposed to cope with the uncertainty of source load, which considers both frequency performance and the operational economy of the microgrid. Firstly, a frequency regulation model for the microgrid is developed by sharing the frequency regulation potential of energy consumers. Secondly, a command allocation model for smart generation control (SGC) based on the integrated benefit is proposed, where frequency safety and economy are combined. Then, a DRL framework is designed, and the twin delayed deep deterministic policy gradient algorithm is used to implement the SGC of the microgrid in the continuous action space. Finally, the effectiveness of the proposed frequency regulation method is demonstrated by designing comparative simulations in the isolated multi-microgrid, and the cost weight and responsiveness of energy consumers are analyzed.}
}
@article{ZHAO2022119346,
title = {Deep reinforcement learning-based joint load scheduling for household multi-energy system},
journal = {Applied Energy},
volume = {324},
pages = {119346},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119346},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922006924},
author = {Liyuan Zhao and Ting Yang and Wei Li and Albert Y. Zomaya},
keywords = {Household multi-energy system, Joint load scheduling, Deep reinforcement learning, Energy management},
abstract = {Under the background of the popularization of renewable energy sources and gas-fired domestic devices in households, this paper proposes a joint load scheduling strategy for household multi-energy system (HMES) aiming at minimizing residents’ energy cost while maintaining the thermal comfort. Specifically, the studied HMES contains photovoltaic, gas-electric hybrid heating system, gas-electric kitchen stove and various types of conventional loads. Yet, it is challenging to develop an efficient energy scheduling strategy due to the uncertainties in energy price, photovoltaic generation, outdoor temperature, and residents’ hot water demand. To tackle this problem, we formulate the HMES scheduling problem as a Markov decision process with both continuous and discrete actions and propose a deep reinforcement learning-based HMES scheduling approach. A mixed distribution is used to approximate the scheduling strategies of different types of household devices, and proximal policy optimization is used to optimize the scheduling strategies without requiring any prediction information or distribution knowledge of system uncertainties. The proposed approach can handle continuous actions of power-shiftable devices and discrete actions of time-shiftable devices simultaneously, as well as the optimal management of electrical devices and gas-fired devices, so as to jointly optimize the operation of all household loads. The proposed approach is compared with a deep Q network (DQN)-based approach and a model predictive control (MPC)-based approach. Comparison results show that the average energy cost of the proposed approach is reduced by 12.17% compared to the DQN-based approach and 4.59% compared to the MPC-based approach.}
}
@article{FU2023110546,
title = {ED-DQN: An event-driven deep reinforcement learning control method for multi-zone residential buildings},
journal = {Building and Environment},
volume = {242},
pages = {110546},
year = {2023},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2023.110546},
url = {https://www.sciencedirect.com/science/article/pii/S0360132323005735},
author = {Qiming Fu and Zhu Li and Zhengkai Ding and Jianping Chen and Jun Luo and Yunzhe Wang and You Lu},
keywords = {Thermal comfort control, Deep reinforcement learning, Event-driven, Multi-zone residential buildings, HVAC systems},
abstract = {Residential Heating, Ventilation, and Air conditioning (HVAC) systems are responsible for a significant amount of energy consumption, but their management is challenging due to the complexities of building thermodynamics and human activities. Reinforcement learning (RL) has been adopted to tackle this issue, but traditional RL methods require massive training data, long learning periods, and frequent equipment adjustments. To address these issues, we construct a new event-driven Markov decision process (ED-MDP) framework, which enables adjustments of control policies triggered by events, reducing unnecessary operations. Moreover, we propose an event-driven deep Q network (ED-DQN) method, which optimizes the action selection based on the triggered events. In the HVAC control problem, the proposed ED-DQN can effectively capture dynamic non-linear features of thermal comfort, and reduce the equipment damage caused by frequent adjustments. Our experimental results show that compared to three benchmark methods and three RL methods, our ED-DQN achieved state-of-the-art performance in both energy saving and thermal comfort violations. Moreover, our method demonstrates promising performance when applied to new test thermal environments, indicating its robustness and adaptability for optimizing residential HVAC controls.}
}
@article{XIAO2023150,
title = {Explore deep reinforcement learning for efficient task processing based on federated optimization in big data},
journal = {Future Generation Computer Systems},
volume = {149},
pages = {150-161},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.06.027},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23002492},
author = {Shan Xiao and Chunyi Wu},
keywords = {Big data, Consumer electronics, Deep reinforcement learning, Federated optimization, Virtual network embedding},
abstract = {In recent years, along with the extensive application of consumer electronics, the task execution with cloud computing for big data has become one of the research focuses. Nevertheless, the traditional theories and algorithms are still employed by existing research work to explore the feasible solutions, which takes a beating from low generalization performance, system load imbalance, more response delay, etc. To solve the matter, a task execution method called DROP (Deep Reinforcement network aided Optimization method aiming at task Processing) has been put forward, which is capable of completing task request allocation through virtual network embedding. The prominence of this method is explained by its effect in reducing load balancing degree, minimizing bandwidth resource overhead, and preserving electric energy as well as meeting customer demands. It makes use of Deep Deterministic Policy Gradient (DDPG) instead of depending on tons of iterations for better path selection schemes in previous methods, through continuous environment interaction and trial-and-error evaluation to get better strategy selection for virtual link embedding. To realize the virtual node embedding in the federated optimization based system architecture, the intentional deep feature learning network is applied. Compared with the cutting edge approaches, the performance benefits of DROP can be verified by the experimental results in terms of bringing down the extra cost on resources and energy of the substrate network during the task execution for big data.}
}
@article{PERERA2021110618,
title = {Applications of reinforcement learning in energy systems},
journal = {Renewable and Sustainable Energy Reviews},
volume = {137},
pages = {110618},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2020.110618},
url = {https://www.sciencedirect.com/science/article/pii/S1364032120309023},
author = {A.T.D. Perera and Parameswaran Kamalaruban},
keywords = {Energy systems, Reinforcement learning, Renewable energy, Building energy, Machine learning},
abstract = {Energy systems undergo major transitions to facilitate the large-scale penetration of renewable energy technologies and improve efficiencies, leading to the integration of many sectors into the energy system domain. As the complexities in this domain increase, it becomes challenging to control energy flows using existing techniques based on physical models. Moreover, although data-driven models, such as reinforcement learning (RL), have gained considerable attention in many fields, a direct shift into RL is not feasible in the energy domain irrespective of the ongoing complexities. To this end, a top-down approach is used to understand this behavior by reviewing the current state of the art. We classified RL papers in the literature into seven categories based on their area of application. Subsequently, publications under each category were further examined relative to problem diversity, RL technique employed, performance improvement (compared with other white and gray box models), verification, and reproducibility; many of the articles reported a 10–20% performance improvement with the use of RL. In most studies, however, deep learning techniques and state-of-the-art actor-critic methods (e.g., twin delayed deep deterministic policy gradient and soft actor-critic) were not applied. This has remarkably hindered performance improvements and problems related to complex energy flows have not been considered. Approximately half of the publications reported the use of Q-learning. Furthermore, despite the availability of historical data in the energy system domain, batch RL algorithms have not been exploited. Emerging multi-agent RL applications may be considered as a positive development that can enable the management of complex interactions among multiple parties. Most studies lack proper benchmarking compared to model-based approaches or gray-box models, and a majority cover energy dispatch problems and building energy management. Although RL can adequately solve problems that are considerably integrated in several sectors, only a limited number of publications have discussed its broad application. The present study clearly demonstrates that even without the full utilization of RL capacity, this technique has a considerable potential in resolving the continuously increasing complexity within the energy system domain.}
}
@article{CUAYAHUITL2019118,
title = {Ensemble-based deep reinforcement learning for chatbots},
journal = {Neurocomputing},
volume = {366},
pages = {118-130},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219311269},
author = {Heriberto Cuayáhuitl and Donghyeon Lee and Seonghan Ryu and Yongjin Cho and Sungja Choi and Satish Indurthi and Seunghak Yu and Hyungtak Choi and Inchul Hwang and Jihie Kim},
keywords = {Deep supervised/unsupervised/reinforcement learning, Neural chatbots},
abstract = {Trainable chatbots that exhibit fluent and human-like conversations remain a big challenge in artificial intelligence. Deep Reinforcement Learning (DRL) is promising for addressing this challenge, but its successful application remains an open question. This article describes a novel ensemble-based approach applied to value-based DRL chatbots, which use finite action sets as a form of meaning representation. In our approach, while dialogue actions are derived from sentence clustering, the training datasets in our ensemble are derived from dialogue clustering. The latter aim to induce specialised agents that learn to interact in a particular style. In order to facilitate neural chatbot training using our proposed approach, we assume dialogue data in raw text only – without any manually-labelled data. Experimental results using chitchat data reveal that (1) near human-like dialogue policies can be induced, (2) generalisation to unseen data is a difficult problem, and (3) training an ensemble of chatbot agents is essential for improved performance over using a single agent. In addition to evaluations using held-out data, our results are further supported by a human evaluation that rated dialogues in terms of fluency, engagingness and consistency – which revealed that our proposed dialogue rewards strongly correlate with human judgements.}
}
@article{JIA2023100732,
title = {A simulation framework for telescope array and its application in distributed reinforcement learning-based scheduling of telescope arrays},
journal = {Astronomy and Computing},
volume = {44},
pages = {100732},
year = {2023},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2023.100732},
url = {https://www.sciencedirect.com/science/article/pii/S2213133723000471},
author = {P. Jia and Q. Jia and T. Jiang and Z. Yang},
keywords = {Telescope arrays, Reinforcement learning, Sky surveys, Time domain astronomy},
abstract = {Time-domain astronomy necessitates continuous observation of celestial objects across the entire sky, with specific observation depth and cadence requirements. Telescope arrays, comprised of numerous wide-field optical telescopes, have emerged as a novel observational instrument for time-domain astronomy. However, the observation capabilities of ground-based optical telescope arrays are often constrained by various dynamic factors such as clouds, satellites, and sky background. To meet the observation requirements, active scheduling of telescopes within a telescope array is essential, leveraging telemetry data from the environment. However, due to the complexity and cost of telescope arrays, it is impractical to directly design or test algorithms using a physical telescope array. In this paper, we propose a framework for simulating telescope arrays that incorporates the majority of real observation effects. Building upon this framework, we further introduce a scheduler based on a distributed reinforcement learning framework to optimize the observation strategy of telescopes within the array. The scheduler is trained and evaluated using the simulator. Results demonstrate that the distributed control framework-based scheduler enhances the observation efficiency of the telescope array.}
}
@article{WANG2023120430,
title = {Comparison of reinforcement learning and model predictive control for building energy system optimization},
journal = {Applied Thermal Engineering},
volume = {228},
pages = {120430},
year = {2023},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2023.120430},
url = {https://www.sciencedirect.com/science/article/pii/S1359431123004593},
author = {Dan Wang and Wanfu Zheng and Zhe Wang and Yaran Wang and Xiufeng Pang and Wei Wang},
keywords = {Building controls, Reinforcement learning, Model predictive control, BOPTEST},
abstract = {Advanced controls could enhance buildings’ energy efficiency and operational flexibility while guaranteeing the indoor comfort. The control performance of reinforcement learning (RL) and model predictive control (MPC) have been widely studied in the literature. However, in existing studies, the reinforcement learning and model predictive control are tested in separate environments, making it challenging to directly compare their performance. In this paper, RL and MPC controls are implemented and compared with traditional rule-based controls in an open-source virtual environment to control a heat pump system of a residential house. The RL controllers were developed with three widely-used algorithms: Deep Deterministic Policy Gradient (DDPG), Dueling Deep Q Networks (DDQN), and Soft Actor Critic (SAC), and the MPC controller was developed using reduced-order thermal resistance-capacity network model. The building optimization testing (BOPTEST) framework is employed as a standardized virtual building simulator to conduct this study. The test case BOPTEST Hydronic Heat Pump is selected for the assessment and benchmarking of the control performance, data efficiency, implementation efforts and computational demands of the RL and MPC controllers. The comparison results revealed that for the RL controllers, only the DDPG algorithm outperforms the baseline controller in both the typical and peak heating scenarios. The MPC controller is superior to the RL and baseline controllers in both two scenarios because it can take the best possible action based on the current system state even with a model that deviates to a certain degree from reality. The findings of this study shed light on the selection of advanced building controllers among two promising candidates: MPC and RL.}
}
@article{JIN2023115958,
title = {DEMRL: Dynamic estimation meta reinforcement learning for path following on unseen unmanned surface vehicle},
journal = {Ocean Engineering},
volume = {288},
pages = {115958},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.115958},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823023429},
author = {Kefan Jin and Hao Zhu and Rui Gao and Jian Wang and Hongdong Wang and Hong Yi and C.-J. {Richard Shi}},
keywords = {Meta reinforcement learning, Policy learning, Path following control, Unmanned surface vehicle},
abstract = {Reinforcement learning has been widely used for unmanned surface vehicle (USV) control tasks. However, the requirement of numerous training samples limits its transferability to new USVs. In this article, we propose a dynamic estimation meta reinforcement learning (DEMRL) approach that enables few-shot learning for the path following control policy. We first present a dynamic estimation method to learn a latent dynamic context feature. The learned context contains the hidden information of USV dynamics with only a few estimation samples. We then propose a meta reinforcement learning based training framework to learn the generalizable path following control policy. After that, given the prior knowledge from dynamic context, the well-trained policy can easily adapt to the target USV during the rapid adaptation process. This proposed method represents the initial effort in tackling the few-shot learning challenge associated with training reinforcement learning based USV path-following policies. Extensive experiments demonstrate that the proposed method can achieve promising path following performance for unseen USV with very few training data and training volume.}
}
@article{PIGOTT2022108521,
title = {GridLearn: Multiagent reinforcement learning for grid-aware building energy management},
journal = {Electric Power Systems Research},
volume = {213},
pages = {108521},
year = {2022},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2022.108521},
url = {https://www.sciencedirect.com/science/article/pii/S0378779622006320},
author = {Aisling Pigott and Constance Crozier and Kyri Baker and Zoltan Nagy},
keywords = {Demand-side management, Reactive power control, Unsupervised learning, Voltage regulation},
abstract = {Increasing amounts of distributed generation in distribution networks can provide both challenges and opportunities for voltage regulation across the network. Intelligent control of smart inverters and other smart building energy management systems can be leveraged to alleviate these issues. GridLearn is a multiagent reinforcement learning platform that incorporates both building energy models and power flow models to achieve grid level goals, by controlling behind-the-meter resources. This study demonstrates how multi-agent reinforcement learning can preserve building owner privacy and comfort while pursuing grid-level objectives. Building upon the CityLearn framework which considers RL for building-level goals, this work expands the framework to a network setting where grid-level goals are additionally considered. As a case study, we consider voltage regulation on the IEEE-33 bus network using controllable building loads, energy storage, and smart inverters. The results show that the RL agents nominally reduce instances of undervoltages and reduce instances of overvoltages by 34%.}
}
@article{NIU2023110782,
title = {Three-dimensional collaborative path planning for multiple UCAVs based on improved artificial ecosystem optimizer and reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {276},
pages = {110782},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110782},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123005324},
author = {Yanbiao Niu and Xuefeng Yan and Yongzhen Wang and Yanzhao Niu},
keywords = {Multiple unmanned combat aerial vehicles, Collaborative path planning, Artificial ecosystem optimizer, Learning framework, Multi-strategy database, Reinforcement learning},
abstract = {This study proposes a multi-strategy evolutionary artificial ecosystem optimizer based on reinforcement learning (MEAEO-RL) to tackle the collaborative path-planning problem of multiple unmanned combat aerial vehicles (UCAVs) in complex environments with multiple constraints. The objective is to generate optimal candidate paths for each UCAV, ensuring they reach the destination simultaneously while considering time variables and obstacle avoidance. To overcome the limitations of the standard artificial ecosystem optimizer (AEO), such as local optimality and slow convergence, a learning framework inspired by brain-like perception is constructed. This framework enhances swarm agents with greater intelligence by fusing swarm intelligence and human cognitive mechanisms. Meanwhile, a multi-strategy database is implemented within the evolutionary learning framework to replace the single-update method of the AEO during the consumption phase. To reduce the computational complexity of the algorithm, agents in the consumption stage utilize experience accumulation from reinforcement learning to select an effective update strategy for obtaining the latest consumer location. Path-planning simulation experiments are conducted in a series of complex three-dimensional environments, demonstrating the algorithm’s robustness, improved convergence accuracy, and ability to plan collaborative paths for multiple UCAVs while satisfying various constraints.}
}
@article{CHENG2022112058,
title = {Optimum condition-based maintenance policy with dynamic inspections based on reinforcement learning},
journal = {Ocean Engineering},
volume = {261},
pages = {112058},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.112058},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822013889},
author = {Jianda Cheng and Yan Liu and Minghui Cheng and Wei Li and Tianyun Li},
keywords = {Condition-based maintenance, Fatigue damage, Dynamic inspection, Reinforcement learning},
abstract = {During the service life, inspections and repairs should be applied timely to maintain the reliability level of deteriorating structures. Condition-based maintenance (CBM) is an effective maintenance policy to reduce the life cycle cost. When the number of inspections does not change regardless of the performance, the CBM is categorize as fixed inspection (FI), otherwise, the inspection policy is denoted as dynamic inspection (DI). Compared with FI policy, DI policy performs the inspections based on the actual state and can avoid the unnecessary or insufficient inspections. Reinforcement learning is an effective and advanced decision-making tool and provides a useful method to optimize DI policy. Meanwhile, reinforcement learning has two methods (model free and model based) distinguished by the interaction method of environment. Comparison of two methods can help select an appropriate method to derive DI policy. Here, model based dynamic inspection (MBDI) and model free dynamic inspection (MFDI) are investigated for their performances in integrity management of fatigue structures. A fatigue details of ship structure is applied to illustrate the proposed framework and comparison study between DI and FI is performed. Results show that dynamic inspections can effectively reduce the expected life cycle costs. Furthermore, MFDI has a better performance than MBDI under different deteriorating rate and cost conditions.}
}
@article{V2023110756,
title = {Deep reinforcement learning with reward shaping for tracking control and vibration suppression of flexible link manipulator},
journal = {Applied Soft Computing},
pages = {110756},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110756},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623007743},
author = {Joshi Kumar V. and Vinodh Kumar E. and Shivram S. and Sweta Shah and Dhruv Mahajan},
keywords = {Deep reinforcement learning, Deep deterministic policy gradient, Adaptive Kalman filter, Flexible link, Vibration control},
abstract = {This paper puts forward a novel deep reinforcement learning control using deep deterministic policy gradient (DRLC-DDPG) framework to address the reference tracking and vibration suppression problem of rotary flexible link (RFL) manipulator. Specifically, this study attempts to address the continuous action space DRLC problem through DDPG algorithm and presents a Lyapunov function based reward shaping approach for guaranteed deep reinforcement learning (DRL) convergence and enhanced speed of training. The proposed approach synthesizes the hard and soft constraints of the flexible manipulator as a constrained Markov decision problem (MDP) and evaluates the performance of DRLC-DDPG framework through hardware in loop (HIL) testing to realize precise servo tracking and suppressed vibration of the flexible manipulator. For identifying the dynamical model of the RFL, an empirical Auto-Regressive eXogenous (ARX) model using the closed loop identification technique is built. Moreover, to extract the true states (servo angle and deflection angle) from the actual measurements, which typically have the influence of sensor noise, an adaptive Kalman filter (AKF) is augmented with the DRLC scheme. The experimental results of DRLC-DDPG scheme compared with those of the model predictive control (MPC) for several test cases reveal that the proposed scheme is superior to MPC both in terms of trajectory tracking and robustness against the external disturbances and model uncertainty.}
}
@article{LI2022290,
title = {Data-driven coordinated control method for multiple systems in proton exchange membrane fuel cells using deep reinforcement learning},
journal = {Energy Reports},
volume = {8},
pages = {290-311},
year = {2022},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.11.250},
url = {https://www.sciencedirect.com/science/article/pii/S2352484721013962},
author = {Jiawen Li and Tiantian Qian and Tao Yu},
keywords = {Proton exchange membrane fuel cell, Distributed deep reinforcement learning, Gas supply system, Heat management system, Coordinated control method},
abstract = {To improve the stability and operating efficiency of a proton exchange membrane fuel cell (PEMFC) system, a distributed deep reinforcement learning-based data-driven coordinated control method is proposed for realizing the coordinated control of a PEMFC gas supply system and heat management system. In addition, a siphonophora multiagent double-delay deep deterministic policy gradient (SMA-4DPG) algorithm is proposed for this method. The design of the algorithm is based on bionics in that it imitates the feeding and survival strategies of a siphonophora, a jellyfish creature with a hydra-like structure. The algorithm utilizes different exploration principles for exploring the PEMFC environment to improve the robustness of the coordination strategy in a manner similar to the siphonophora with its different prey-seeking organs. With this algorithm, the gas supply system and the heat management system are treated as two agents. Through centralized training, agents with different objectives and time scales can coordinate with each other and improve the stability of the PEMFC output voltage and stack temperature. The effectiveness of the proposed algorithm is demonstrated in a series of experiments in which its performance is compared with that of a conventional controller.}
}
@article{FU2022104165,
title = {Applications of reinforcement learning for building energy efficiency control: A review},
journal = {Journal of Building Engineering},
volume = {50},
pages = {104165},
year = {2022},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2022.104165},
url = {https://www.sciencedirect.com/science/article/pii/S2352710222001784},
author = {Qiming Fu and Zhicong Han and Jianping Chen and You Lu and Hongjie Wu and Yunzhe Wang},
keywords = {Reinforcement learning, Intelligent buildings, Energy consumption},
abstract = {The wide variety of smart devices equipped in modern intelligent buildings and the increasing comfort requirements of occupants for the environment make the control of intelligent buildings important and complex. Reinforcement learning, as a class of control techniques in machine learning, has been explored for its potential in the field of intelligent building control. Reinforcement learning methods applied to intelligent buildings can effectively reduce energy consumption. In this paper, we classify reinforcement learning algorithms and analyze the control problems that each algorithm is suitable for solving. In addition, we review the reinforcement learning methods applied to control and manage buildings, outline the problems and future directions of reinforcement learning applications in intelligent buildings, and give our suggestions for researchers who want to use reinforcement learning methods to solve control problems in this field.}
}
@incollection{WHITEHEAD1990179,
title = {Active Perception and Reinforcement Learning},
editor = {Bruce Porter and Raymond Mooney},
booktitle = {Machine Learning Proceedings 1990},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {179-188},
year = {1990},
isbn = {978-1-55860-141-3},
doi = {https://doi.org/10.1016/B978-1-55860-141-3.50025-0},
url = {https://www.sciencedirect.com/science/article/pii/B9781558601413500250},
author = {Steven D. Whitehead and Dana H. Ballard},
abstract = {This paper considers adaptive control architectures that integrate active sensory-motor systems with decision systems based on reinforcement learning. One unavoidable consequence of active perception is that the agent's internal representation often confounds external world states. We call this phenomenon perceptual aliasing and show that it destabilizes existing reinforcement learning algorithms with respect to the optimal decision policy. A new decision system that overcomes these difficulties is described. The system incorporates a perceptual subcycle within the overall decision cycle and uses a modified learning algorithm to suppress the effects of perceptual aliasing. The result is a control architecture that learns not only how to solve a task but also where to focus its attention in order to collect necessary sensory information.}
}
@article{WU2022109150,
title = {Embedded draw-down constraint reward function for deep reinforcement learning},
journal = {Applied Soft Computing},
volume = {125},
pages = {109150},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109150},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622004082},
author = {Jimmy Ming-Tai Wu and Sheng-Hao Lin and Jia-Hao Syu and Mu-En Wu},
keywords = {Risk prediction model, Quadrupole exciton, Polariton, , },
abstract = {Money management, also known as asset allocation, is constantly at the forefront of research in the trading and investing fields. Since Markowitz established the current portfolio theory in 1952, it has drawn many experts to this intriguing topic. The Kelly criteria are one of the brightest stars among these new techniques. It provides an elegant solution for players and investors to get the best bidding fraction and maximize their logarithm worth over time. However, it ignores the fact that each investor has a different risk tolerance, and the proportion was calculated using the Kelly criterion without taking into account the downside risk. This paper attempts to develop a risk prediction model using a probability-based method and adjust the reward function of deep reinforcement learning to account for the downside risk. To summarize, rather than a naive reward function that solely optimizes the return, the improved deep reinforcement learning may consider an investor’s risk tolerance. Finally, we solely analyze the scenario of a single asset and employ DXY, GBP/USD, and EUR/USD as the underlying training and validation data sets. The outcome demonstrates that the adjustment to the reward mechanism produces an interesting performance. When the required MDD (Maximum draw-down) is greater than 3%, the likelihood is on average greater than 70%.}
}
@article{AHMED2023106593,
title = {Active control of flexible rotors using deep reinforcement learning with application of multi-actor-critic deep deterministic policy gradient},
journal = {Engineering Applications of Artificial Intelligence},
volume = {124},
pages = {106593},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106593},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623007777},
author = {Maheed H. Ahmed and Abdullah AboHussien and Aly El-Shafei and Ahmed M. Darwish and Ahmed H. Abdel-Gawad},
keywords = {Active vibration control, Automatic balancing, Deep reinforcement learning, Active magnetic bearing, Instability control},
abstract = {Vibration in rotating machinery is one of the main causes of machine failure. Active and passive control methods have been developed in order to reduce vibration levels and extend the operating speeds of machines. These controllers are either manually tuned or tuned during operation using adaptive control techniques and are tailored to a single vibration source. In the field of artificial intelligence, deep reinforcement learning has greatly impacted the field of continuous control, from mastering simple games to controlling multiple actuators in a robot doing complex tasks. Deep reinforcement learning agents are capable of finding optimal control policies without a model of the underlying system. This work proposes the multi-actor-critic deep deterministic policy gradient (MAC-DDPG) algorithm by integrating multiple criteria to train concurrent actors in a periodic system. Using the frequency footprint of each type of vibration, a cost function is designed to train the critics and actors. The proposed controller is evaluated on a test rig supported by two patented Smart Electro-Magnetic Actuator Journal Integrated Bearings (SEMAJIB). The proposed controller is capable of finding optimal control policies for reducing the synchronous vibration caused by the rotor’s unbalance and stabilizing a system with oil whip vibration. A derivative controller actor and a harmonic actor are used concurrently for controlling the vibrations. The proposed controller is able to reach unbalance vibration reduction up to 93%. In addition, the proposed controller is successful in completely eliminating oil whip instability with up to 99% reduction.}
}
@article{NASURUDEENAHAMED202092,
title = {A Reinforcement Learning Integrated in Heuristic search method for self-driving vehicle using blockchain in supply chain management},
journal = {International Journal of Intelligent Networks},
volume = {1},
pages = {92-101},
year = {2020},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2020.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666603020300129},
author = {N. {Nasurudeen Ahamed} and P. Karthikeyan},
keywords = {Blockchain, Artificial intelligence, Machine learning, Reinforcement learning, Public ledger, Self-driving},
abstract = {Blockchain is a distributed open (Public) ledger that is used to record the transaction across many computers. Blockchain technology can be applied in any domain such as banking, healthcare, real estate, travel, food, and supply chain. In supply chain management to train the self-driving vehicle in blockchain technology also integrate the Artificial Intelligence (AI) and Machine Learning (ML) Algorithms. In this paper we have proposed Reinforcement learning integrated heuristic search method (RLIH) for self-driving vehicle using blockchain in supply chain management by combining the advantage of reinforcement learning and heuristic search method. RLIH is developed using Decentralized app and result shows that proposed method outperform the existing heuristic search method in term of service time and data traffic.}
}
@article{XU2022214,
title = {Stock movement prediction via gated recurrent unit network based on reinforcement learning with incorporated attention mechanisms},
journal = {Neurocomputing},
volume = {467},
pages = {214-228},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.09.072},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221014508},
author = {Hongfeng Xu and Lei Chai and Zhiming Luo and Shaozi Li},
keywords = {Stock movement prediction, Gated recurrent unit, Attention, Reinforcement learning},
abstract = {The recent advances usually mine market information from the chaotic data to conduct a stock movement prediction task. However, the current stock price movement prediction approaches mainly compute attention weighted sum of the global contextual semantic embeddings, which fails to combine local word-level or char-level ones to jointly learn news-level representation. Moreover, for Chinese stock price movement prediction task, some collected news texts are chaotic even irrelevant to the target stock. It suggests that the models need filter some news-level representations (viewed as noises) to enhance the performance. To that aim, we develop a novel stock price movement prediction network via bidirectional gated recurrent unit (GRU) network based on reinforcement learning (RL) with incorporated attention mechanism. In specific, to reduce the noise of news texts and learn news-level representation with more abundant semantics, two novel attention mechanisms respectively based on add and dot operation were first proposed in this work. We then design a novel GRU structure based on RL to filter some irrelated news-level representations (i.e., news-level noises) and capture abundant long-term dependencies. Finally, the experimental results show that the proposed model far outperforms the recent advances and achieves state-of-the-art performances.}
}
@article{XIAO2022476,
title = {A cold-start-free reinforcement learning approach for traffic signal control},
journal = {Journal of Intelligent Transportation Systems},
volume = {26},
number = {4},
pages = {476-485},
year = {2022},
issn = {1547-2450},
doi = {https://doi.org/10.1080/15472450.2021.1934679},
url = {https://www.sciencedirect.com/science/article/pii/S1547245022003759},
author = {Nan Xiao and Liang Yu and Jinqiang Yu and Peng Chen and Yuehu Liu},
keywords = {cold start, deep learning, model-based control, reinforcement learning, traffic signal control},
abstract = {Typical reinforcement learning (RL) requires a huge amount of data before achieving an acceptable result, and its performance can be rather poor during initial interacting process. Sample inefficiency and cold-start phenomenon of RL limits its feasibility in a range of real-world applications such as traffic signal control (TSC). On the other hand, a large amount of data on TSC can be accumulated by various model-based controllers (MBCs) rooted in traffic engineering. In this context, we propose a new RL approach which can avoid the appearance of cold starts by taking advantage of MBC experiences. First, three frameworks of joint utilization of RL and MBC are summarized for TSC, and staged framework is considered to have the edge over the other two. Then, a staged noisy-net prioritized dueling double deep Q-network (NPDD-DQN) is described in detail for TSC, where MBC experiences are used in both pre-training and online training processes. Experimental evaluation demonstrates that staged NPDD-DQN can achieve a boost in initial performance as compared to pure NPDD-DQN that does not utilize any control experiences, and learn to improve final performance beyond the underlying MBC. The effectiveness of the proposed method opens up the possibility of real implementation of RL in TSC.}
}
@article{ZHANG2022101896,
title = {Deep reinforcement learning based IRS-assisted mobile edge computing under physical-layer security},
journal = {Physical Communication},
volume = {55},
pages = {101896},
year = {2022},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2022.101896},
url = {https://www.sciencedirect.com/science/article/pii/S1874490722001732},
author = {Lianhong Zhang and Shiwei Lai and Junjuan Xia and Chongzhi Gao and Dahua Fan and Jianghong Ou},
keywords = {Mobie edge computing, Intelligent reflecting surface, DRL, Physical-layer security},
abstract = {In this paper, we investigate an intelligent reflecting surface (IRS)-assisted mobile edge computing (MEC) network under physical-layer security, where users can partially offload confidential and compute-intensive tasks to a computing access point (CAP) with the help of the IRS. We consider an eavesdropping environment, where an eavesdropper steals information from the communication. For the considered MEC network, we firstly design a secure data transmission rate to ensure physical-layer security. Moreover, we formulate the optimization target as minimizing the system cost linearized by the latency and energy consumption (ENCP). In further, we employ a deep deterministic policy gradient (DDPG) to optimize the system performance by allocating the offloading ratio and wireless bandwidth and computational capability to users. Finally, considering the impacts from different resources, based on DDPG, seeing our optimization strategy as one criterion, we designed other criteria with different resource allocation schemes. And some simulation results are given to demonstrate that our proposed criterion outperforms other criteria.}
}
@article{CEUSTERS2021117634,
title = {Model-predictive control and reinforcement learning in multi-energy system case studies},
journal = {Applied Energy},
volume = {303},
pages = {117634},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117634},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921010011},
author = {Glenn Ceusters and Román Cantú Rodríguez and Alberte Bouso García and Rüdiger Franke and Geert Deconinck and Lieve Helsen and Ann Nowé and Maarten Messagie and Luis Ramirez Camargo},
keywords = {Model-predictive control, Reinforcement learning, Optimal control, Multi-energy systems},
abstract = {Model predictive control (MPC) offers an optimal control technique to establish and ensure that the total operation cost of multi-energy systems remains at a minimum while fulfilling all system constraints. However, this method presumes an adequate model of the underlying system dynamics, which is prone to modelling errors and is not necessarily adaptive. This has an associated initial and ongoing project-specific engineering cost. In this paper, we present an on- and off-policy multi-objective reinforcement learning (RL) approach that does not assume a model a priori, benchmarking this against a linear MPC (LMPC — to reflect current practice, though non-linear MPC performs better) - both derived from the general optimal control problem, highlighting their differences and similarities. In a simple multi-energy system (MES) configuration case study, we show that a twin delayed deep deterministic policy gradient (TD3) RL agent offers the potential to match and outperform the perfect foresight LMPC benchmark (101.5%). This while the realistic LMPC, i.e. imperfect predictions, only achieves 98%. While in a more complex MES system configuration, the RL agent’s performance is generally lower (94.6%), yet still better than the realistic LMPC (88.9%). In both case studies, the RL agents outperformed the realistic LMPC after a training period of 2 years using quarterly interactions with the environment. We conclude that reinforcement learning is a viable optimal control technique for multi-energy systems given adequate constraint handling and pre-training, to avoid unsafe interactions and long training periods, as is proposed in fundamental future work.}
}
@article{ALMAHAMID2022105321,
title = {Autonomous Unmanned Aerial Vehicle navigation using Reinforcement Learning: A systematic review},
journal = {Engineering Applications of Artificial Intelligence},
volume = {115},
pages = {105321},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105321},
url = {https://www.sciencedirect.com/science/article/pii/S095219762200358X},
author = {Fadi AlMahamid and Katarina Grolinger},
keywords = {Reinforcement Learning, Autonomous UAV navigation, UAV, Systematic review},
abstract = {There is an increasing demand for using Unmanned Aerial Vehicle (UAV), known as drones, in different applications such as packages delivery, traffic monitoring, search and rescue operations, and military combat engagements. In all of these applications, the UAV is used to navigate the environment autonomously — without human interaction, perform specific tasks and avoid obstacles. Autonomous UAV navigation is commonly accomplished using Reinforcement Learning (RL), where agents act as experts in a domain to navigate the environment while avoiding obstacles. Understanding the navigation environment and algorithmic limitations plays an essential role in choosing the appropriate RL algorithm to solve the navigation problem effectively. Consequently, this study first identifies the main UAV navigation tasks and discusses navigation frameworks and simulation software. Next, RL algorithms are classified and discussed based on the environment, algorithm characteristics, abilities, and applications in different UAV navigation problems, which will help the practitioners and researchers select the appropriate RL algorithms for their UAV navigation use cases. Moreover, identified gaps and opportunities will drive UAV navigation research.}
}
@article{LI2024121959,
title = {Managing mixed traffic at signalized intersections: An adaptive signal control and CAV coordination system based on deep reinforcement learning},
journal = {Expert Systems with Applications},
volume = {238},
pages = {121959},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121959},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423024612},
author = {Duowei Li and Feng Zhu and Jianping Wu and Yiik Diew Wong and Tianyi Chen},
keywords = {Connected and autonomous vehicle (CAV), Mixed traffic, Signalized intersection, Adaptive signal control, Deep reinforcement learning, Multi-agent coordination},
abstract = {Managing the mixed traffic involving connected and autonomous vehicles (CAVs) and human-driven vehicles (HVs) at a signalized intersection has become a concern of researchers. However, the performances of most existing control methods are limited, especially when CAV penetration rate is low, since they fail to make a better trade-off between safety and operational efficiency for both CAVs and HVs. To this end, this study proposes a deep reinforcement learning (DRL) powered control system for the mixed traffic at signalized intersections, which aims to optimize operational efficiency of both CAVs and HVs while assuring safety and reducing interference on HVs’ driving habits. The system adopts an adaptive traffic signal control strategy and an efficient CAV control policy with a passing rule proposed as a link in between. The traffic signal control strategy allows traffic light to adaptively adjust its phase and duration based on real-time traffic information, while the CAV control policy permits the CAVs meeting certain safety constraints to form platoons and pass through the intersection in a coordinated manner regardless of traffic signals. As an efficient DRL algorithm, Deep Q-Network (DQN) is adopted to adaptively control the signals and implement CAV coordination. The proposed system is examined on Simulation of Urban Mobility (SUMO), given different CAV penetration rates and traffic conditions. It is found that the proposed system not only outperforms the state-of-the-art control methods on reducing travel time and fuel consumption under low CAV penetration rate, but also enlarges its advantages with the increase of CAV penetration rate. In certain traffic scenarios, the proposed system can even achieve a maximum reduction of travel time by 37.33% and fuel consumption by 15.95%, in comparison to the existing method with the best performance. Besides, to some extent, the comparisons between the performances of CAVs and HVs demonstrate certain benefits of introducing CAVs into the mixed traffic.}
}
@article{DELIMAMUNGUBA2023106067,
title = {Condition-based maintenance with reinforcement learning for refrigeration systems with selected monitored features},
journal = {Engineering Applications of Artificial Intelligence},
volume = {122},
pages = {106067},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106067},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623002518},
author = {Caio Filipe {de Lima Munguba} and Gustavo {de Novaes Pires Leite} and Alvaro Antonio Villa Ochoa and Enrique {Lopez Droguett}},
keywords = {Refrigeration, Degradation, Energy, Condition-based maintenance, Reinforcement learning},
abstract = {Worldwide, buildings are responsible for almost 30% of energy consumption, and those buildings that intensively use refrigeration systems, such as supermarkets and grocery stores, are also among the most energy-intensive consumers. Refrigeration devices, either commercial or residential, are responsible for a significant part of net emissions. Based on careful measurements, it is possible to reduce energy consumption in these devices by up to 15% only by improving the fault detection and diagnosis techniques. Thus, improving maintenance programs has become a crucial area in energy management in recent years. Nowadays, the market has experienced a hike after smart systems and new network interfaces applied to smart buildings that have allowed previously isolated devices to become smart devices, interacting with control algorithms smartly and, to some extent, autonomously. Here, we propose a reinforcement learning framework to develop a maintenance policy for mechanical compression refrigeration devices. Firstly, a test bench is built in which each component is assigned to be individually repairable and individually degradable in parallel and interconnected processes. Then, the degradation of the components is combined to formulate the system degradation, and the optimal maintenance policy is modeled via Markov decision processes and solved by a reinforcement learning algorithm. The agent-proposed maintenance program if compared to corrective maintenance, managed to reduce energy use and emissions by around 6% while avoiding shortfalls, as well as about the preventive program, where the agent managed to accomplish the same level of energy efficiency while reducing the maintenance costs by 31% and the time under maintenance in 10%. It was found that the reinforcement learning frameworks applied to maintenance have a series of challenges but are innovative and can show promising results compared to traditional maintenance techniques, such as preventive and corrective ones.}
}
@article{LAGO2019488,
title = {Building day-ahead bidding functions for seasonal storage systems: A reinforcement learning approach},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {4},
pages = {488-493},
year = {2019},
note = {IFAC Workshop on Control of Smart Grid and Renewable Energy Systems CSGRES 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.08.258},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319305956},
author = {Jesus Lago and Ecem Sogancioglu and Gowri Suryanarayana and Fjo De Ridder and Bart De Schutter},
keywords = {Seasonal Storage, Bidding Functions, Reinforcement Learning, Energy Storage},
abstract = {Due to the increasing integration of renewable sources in the electrical grid, electricity generation is expected to become more uncertain. In this context, seasonal thermal energy storage systems (STESSs) are key to shift the delivery of renewable energy sources and tackle their uncertainty problems. In this paper, we propose an optimal controller for STESSs that, using reinforcement learning, builds bidding functions for the day-ahead market. In detail, considering that there is an uncertain energy demand that the STESS has to satisfy, the controller buys energy in the day-ahead market so that the uncertain demand is satisfied while the profits are maximized. Since prices are low during periods of large renewable energy generation (and vice versa), maximizing the profit of a STESS indirectly shifts the delivery of renewable energy to periods of high energy demand while reducing their uncertainty problems. To evaluate the proposed algorithm, we consider a real STESS providing different yearly-demand levels; then, we compare the performance of the controller to the theoretical upper bound, i.e. the optimal cost of buying energy given perfect knowledge of the demand and prices. The results indicate that the proposed controller performs reasonably well: despite the large uncertainty in prices and demand, the proposed controller obtains 70%-50% of the maximum gains given by the theoretical bound.}
}
@article{WANG2023105551,
title = {Deep reinforcement learning-PID based supervisor control method for indirect-contact heat transfer processes in energy systems},
journal = {Engineering Applications of Artificial Intelligence},
volume = {117},
pages = {105551},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105551},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622005413},
author = {Xuan Wang and Jinwen Cai and Rui Wang and Gequn Shu and Hua Tian and Mingtao Wang and Bowen Yan},
keywords = {Deep-reinforcement learning, Supervisor control, Indirect-contact heat transfer, Energy system control, Organic Rankine Cycle},
abstract = {Indirect-contact heat exchangers have been widely used in various energy systems, and the precise tracking control of important heat transfer parameters, such as temperature, is vital for safe and efficient operation. However, the high nonlinearity of heat transfer and large disturbance brings difficulty to optimal control. Considering the strong perception and decision-making capabilities of deep reinforcement learning (DRL), this study proposed a supervisor control method combined DRL and proportional–integral–derivative (PID). A set of the fewest conveniently measurable variables was derived as agent observations to describe the heat transfer process effectively and thereby improve the control efficiency under large disturbances. In addition, the local heat transfer process was used as a training environment to reduce training costs significantly. Finally, superheat temperature control in a complex organic Rankine cycle was simulated with SIMULINK to evaluate the effectiveness of the proposed observation variables and the training and control methods. The results showed that the proposed control method achieved satisfactory performance. The average absolute tracking error was only 0.246 K under trained and untrained disturbances, whereas that of the PID control was 4.645 K. Compared with the model predictive control, the DRL-PID-based supervisory control evidently performed better under a large disturbance; the average absolute tracking errors under DRL-PID control and MPC were 0.288 K and 0.509 K, respectively.}
}
@article{GUO2022108061,
title = {Energy management of intelligent solar parking lot with EV charging and FCEV refueling based on deep reinforcement learning},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {140},
pages = {108061},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2022.108061},
url = {https://www.sciencedirect.com/science/article/pii/S014206152200103X},
author = {Guodong Guo and Yanfeng Gong},
keywords = {Intelligent solar parking lot, Deep deterministic policy gradient, Fuzzy logic controller, Electric vehicles, Fuel cell electric vehicles},
abstract = {The design of optimal energy management systems of intelligent solar parking lot (ISPL) is crucial to facilitate the integration of solar energy generation, electric vehicle (EV) charging, and hydrogen refueling of fuel cell electric vehicles (FCEVs). However, uncertainties in the photovoltaic (PV) output, the arrival and departure times of vehicles and the users' preferences pose challenges to energy management. To improve the benefits, this paper proposes a real-time energy management strategy for power flow and mass flow control of ISPL based on a deep reinforcement learning (DRL) model. A fuzzy logic controller of EVs subsystem respond to retail price set by DRL agent according to the user's preference parameters. Moreover, vectors obtained through interaction, such as detection loads, are adopted to improve the agent's perception of the real-time state of ISPL. Then an ISPL system with refined modeling, real-time interactive perception, and intelligent decision-making is established. Additionally, some strategies are used to improve the stability of training. The simulation results demonstrate the effectiveness of the proposed method in increasing the system benefits and improving the matching degree between flexible loads and PV power.}
}
@article{CHEN2021107108,
title = {Distributed computation offloading method based on deep reinforcement learning in ICV},
journal = {Applied Soft Computing},
volume = {103},
pages = {107108},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107108},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621000314},
author = {Chen Chen and Yuru Zhang and Zheng Wang and Shaohua Wan and Qingqi Pei},
keywords = {Intelligent Connected Vehicles, Computing offloading, Vehicular edge computing, Deep Q-learning Network},
abstract = {With the rapid development of Intelligent Connected Vehicles (ICVs), more effective computation resources optimization schemes in task scheduling are exactly required for large-scale network implementation. We observe that an offloading scheme that almost all tasks are going to be executed in Multi-Access Edge Computing (MEC) servers, which lead to a lot of vehicle resources to be underutilized and put a great burden on severs, is not a good solution for resource utilization. So we first consider the scenario where MEC is not available or enough. We take surrounding vehicles as a Resource Pool (RP). And we propose a distributed computation offloading method to utilize all resources, in which a complex task can be split into many small sub-tasks. How to assign these minor tasks to get a better execution time in RP is a hard problem. The executing time of a complex computing task is a min–max problem. In this paper, a distributed computation offloading strategy based on Deep Q-learning Network (DQN) is proposed to find the best offloading method to minimize the execution time of a compound task. We can demonstrate that the model proposed in this paper can take full advantage of the computing resources of the surrounding vehicles and greatly reduce the execution time of the computation tasks.}
}
@article{LEE2022109556,
title = {A reinforcement learning approach for multi-fleet aircraft recovery under airline disruption},
journal = {Applied Soft Computing},
volume = {129},
pages = {109556},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109556},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622006226},
author = {Junhyeok Lee and Kyungsik Lee and Ilkyeong Moon},
keywords = {Airline disruption, Aircraft recovery, Reinforcement learning, Q-learning, Double Q-learning},
abstract = {An airline scheduler plans flight schedules with efficient resource utilization. However, unpredictable airline disruptions, such as temporary closures of an airports, cause schedule perturbations. Therefore, recovering disrupted flight schedules is essential for airlines. Many previous studies have relied on copies of flight arcs, which could affect the quality of solutions, and have not addressed the key measure of airlines’ on-time performance as their objective. To fill these research gaps, we propose Q-learning and Double Q-learning algorithms using the reinforcement learning approach for aircraft recovery to support airline operations. We present an artificial environment of daily flight schedules and the Markov decision process for aircraft recovery. The proposed approach is first compared with existing algorithms on the benchmark instance. In comparison with other algorithms, the developed Q-learning and Double Q-learning algorithms obtain high-quality solutions within the proper computation time. To verify that the proposed approach can be applicable to a real-world case and can adapt to realistic conditions, we employ a domestic flight schedule from one of the airlines in South Korea. We evaluate the reinforcement learning approach on a set of experiments carried out on real-world data. Computational experiments show that reinforcement learning algorithms recover disrupted flight schedules effectively, and that our approaches flexibly adapt to various objectives and realistic conditions.}
}
@article{LAWRENCE2020230,
title = {Reinforcement Learning based Design of Linear Fixed Structure Controllers},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {230-235},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.127},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320303839},
author = {Nathan P. Lawrence and Gregory E. Stewart and Philip D. Loewen and Michael G. Forbes and Johan U. Backstrom and R. Bhushan Gopaluni},
keywords = {reinforcement learning, process control, PID control, derivative-free optimization},
abstract = {Reinforcement learning has been successfully applied to the problem of tuning PID controllers in several applications. The existing methods often utilize function approximation, such as neural networks, to update the controller parameters at each time-step of the underlying process. In this work, we present a simple finite-difference approach, based on random search, to tuning linear fixed-structure controllers. For clarity and simplicity, we focus on PID controllers. Our algorithm operates on the entire closed-loop step response of the system and iteratively improves the PID gains towards a desired closed-loop response. This allows for embedding stability requirements into the reward function without any modeling procedures.}
}
@article{CORACI2023120598,
title = {Online transfer learning strategy for enhancing the scalability and deployment of deep reinforcement learning control in smart buildings},
journal = {Applied Energy},
volume = {333},
pages = {120598},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120598},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922018554},
author = {Davide Coraci and Silvio Brandi and Tianzhen Hong and Alfonso Capozzoli},
keywords = {Online transfer learning, Homogeneous transfer learning, Intra-agent transfer learning, Building adaptive control, Deep reinforcement learning, Energy efficiency},
abstract = {In recent years, advanced control strategies based on Deep Reinforcement Learning (DRL) proved to be effective in optimizing the management of integrated energy systems in buildings, reducing energy costs and improving indoor comfort conditions when compared to traditional reactive controllers. However, the scalability and implementation of DRL controllers are still limited since they require a considerable amount of time before converging to a near-optimal solution. This issue is currently addressed in literature through the offline pre-training of the DRL agent. However this solution results in two main critical issues: (1) the need to develop a building surrogate model to perform the training task, and (2) the need to perform a fine-tuning process over several training episodes to obtain a near-optimal control policy. In this context, this paper introduces an Online Transfer Learning (OTL) strategy that exploits two knowledge-sharing techniques, weight-initialization and imitation learning, to transfer a DRL control policy from a source office building to various target buildings in a simulation environment coupling EnergyPlus and Python. A DRL controller based on discrete Soft Actor–Critic (SAC) is trained on the source building to manage the operation of a cooling system consisting of a chiller and a thermal storage. Several target buildings are defined to benchmark the performance of the OTL strategy with that of a Rule-Based Controller (RBC) and two DRL-based control strategies, deployed in offline and online fashion. The strategy adopted for OTL emulates the real world implementation with a simulation process by implementing the transferred DRL agent for a single episode in the target buildings. Target buildings have the same geometrical features and are served by the same energy system as the source building, but differ in terms of weather conditions, electricity price schedules, occupancy patterns, and building envelope efficiency levels. The results show that the OTL strategy can reduce the cumulated sum of temperature violations on average by 50% and 80% respectively when compared to RBC and online DRL while enhancing the energy system operation with electricity cost savings ranging between 20% and 40%. The OTL agent performs slightly worse than the offline DRL controller but it does not require any modeling effort and can be implemented directly on target buildings emulating a real-world implementation.}
}
@article{ONILE2023104392,
title = {Energy efficient behavior modeling for demand side recommender system in solar microgrid applications using multi-agent reinforcement learning model},
journal = {Sustainable Cities and Society},
volume = {90},
pages = {104392},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2023.104392},
url = {https://www.sciencedirect.com/science/article/pii/S2210670723000033},
author = {Abiodun E. Onile and Juri Belikov and Yoash Levron and Eduard Petlenkov},
keywords = {Demand side recommender systems, Multi-agent reinforcement learning, Microgrid, Solar photovoltaic, BESS, Consumer comfort},
abstract = {Electricity consumers are often faced with challenges relating to the choice of an optimal energy saving plan. Increasing integration of transient renewable energy sources promises tantalizing solutions but also poses emerging stability challenges for the electricity grid. Demand side management using battery energy storage systems (BESSs) is crucial towards extending the physical limits of existing electricity grid. However, problems related to consumer behavior towards adoption of energy/battery efficiency measures and consumer comfort feedback exist. In this study, we present BESS technologies that are embedded into the grid and further enhanced with the use of reinforcement learning control and recommendation system technologies for improving the grid reliability, attaining self-consumption and demand response goals. The novelty of the proposed work is highlighted by using a separate class of active controller for BESS technologies, thereby separating it from loads which determine user comfort. Similarly, an adaptive demand side recommender scheme was used to provide recommendations targeting various microgrid entities. The result of the study shows that operating BESS using the multi-agent reinforcement learning control strategy achieved a maximum peak load reduction of about 24.5% alongside 94% comfort improvements in certain loads. The linear reduction in peak load was further enhanced by the BESS efficiency-related recommendations when compared to the baseline scenario.}
}
@article{KUTSCHINSKI20032207,
title = {Learning competitive pricing strategies by multi-agent reinforcement learning},
journal = {Journal of Economic Dynamics and Control},
volume = {27},
number = {11},
pages = {2207-2218},
year = {2003},
note = {Computing in economics and finance},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(02)00122-7},
url = {https://www.sciencedirect.com/science/article/pii/S0165188902001227},
author = {Erich Kutschinski and Thomas Uthmann and Daniel Polani},
keywords = {Distributed simulation, Agent-based computational economics, Dynamic pricing, Multi-agent reinforcement learning, Q-learning},
abstract = {In electronic marketplaces automated and dynamic pricing is becoming increasingly popular. Agents that perform this task can improve themselves by learning from past observations, possibly using reinforcement learning techniques. Co-learning of several adaptive agents against each other may lead to unforeseen results and increasingly dynamic behavior of the market. In this article we shed some light on price developments arising from a simple price adaptation strategy. Furthermore, we examine several adaptive pricing strategies and their learning behavior in a co-learning scenario with different levels of competition. Q-learning manages to learn best-reply strategies well, but is expensive to train.}
}
@incollection{MOWBRAY20211215,
title = {Generating an optimal control policy via apprenticeship and reinforcement learning},
editor = {Metin Türkay and Rafiqul Gani},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {50},
pages = {1215-1220},
year = {2021},
booktitle = {31st European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-88506-5.50187-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032388506550187X},
author = {Max Mowbray and Robin Smith and Antonio Del Río Chanona and Dongda Zhang},
keywords = {Apprenticeship learning, Reinforcement learning, Inverse reinforcement learning, Optimal control, Machine learning},
abstract = {The development of advanced process control schemes is continuing driver of research within process systems engineering. In this work, we propose a framework, which leverages existing process data to automatically learn and update a control policy. This framework is underpinned by machine learning methods, namely, apprenticeship (AL) and reinforcement learning (RL), which compose offline and online learning respectively. In offline learning, we synchronously identify a function descriptive of the control objective and a parameterisation of the existing control policy expressed in data. The parameterised control policy is then updated automatically online via data from the ongoing process and RL. Importantly, the parameterisation learned offline achieves similar performance under the identified control objective to the existing control policy. Ultimately, the framework proposed enables a reduction in the technical intensity of offline RL-based policy learning, and approximates existing controllers in a more robust fashion than by supervised learning. The performance and efficiency of the framework proposed is explored via case study. Future work should focus on increasing sample efficiency in policy learning, satisfaction of constraints and accounting for model uncertainty.}
}
@article{MENG202113,
title = {An advanced real-time dispatching strategy for a distributed energy system based on the reinforcement learning algorithm},
journal = {Renewable Energy},
volume = {178},
pages = {13-24},
year = {2021},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2021.06.032},
url = {https://www.sciencedirect.com/science/article/pii/S0960148121008983},
author = {Fanyi Meng and Yang Bai and Jingliang Jin},
keywords = {Distribution system, Economic dispatch, Coordinated dispatching strategy, Real-time control, Markov decision process, Reinforcement learning},
abstract = {A desirable dispatching strategy is essentially important for securely and economically operating of wind-thermal hybrid distribution systems. Existing dispatch strategies usually assume that wind power has priority of injection. For real-time control, such strategies are simple and easy to realize, but they lack flexibility and incur higher operation and maintenance (O&M) costs. This study analyzed the power dispatching process as a dynamic sequential control problem and established a Markov decision process model to explore the optimal coordinated dispatch strategy for coping with wind and demand disturbance. As a salient feature, the improved dispatch strategy minimizes the long-run expected operation and maintenance costs. To evaluate the model efficiently, a Monte Carlo method and the Q-learning algorithm were employed to the growing computational cost over the state space. Through a specified numerical case, we demonstrated the properties of the coordinated dispatch strategy and used it to address a 24-h real-time dispatching problem. The proposed algorithm shows high efficiency in solving real-time dispatching problems.}
}
@article{PAUDEL2023129097,
title = {A deep reinforcement learning approach for power management of battery-assisted fast-charging EV hubs participating in day-ahead and real-time electricity markets},
journal = {Energy},
volume = {283},
pages = {129097},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.129097},
url = {https://www.sciencedirect.com/science/article/pii/S036054422302491X},
author = {Diwas Paudel and Tapas K. Das},
keywords = {EV charging hubs, Day-ahead commitment, Battery storage system, Deep reinforcement learning, Fast charging, Power management},
abstract = {Publicly available electric vehicle charging hubs are expected to grow, to meet the increasing charging demand of EVs. A dominant class of these will be fast-charging hubs where the EVs will arrive for charging at all hours of the day, get the requested charge, and leave promptly. The profitability of these fast-charging hubs will be highly dependent on the variation of the day-ahead prices of electricity, volatility of the real-time power market, and the randomness of EV charging demand. The hubs can hedge against these uncertainties by committing power purchases in the day-ahead electricity market and by adopting dynamic real-time power management strategies. We develop a novel two-step methodology. The first step entails a mixed integer linear program (MILP) that assists the hubs in their day-ahead power commitment. The second step employs a Markov decision process (MDP) model that derives the real-time power management control actions. The MILP is solved using a commercial solver and the MDP is solved using a deep reinforcement learning algorithm. We demonstrate the effectiveness of our methodology for a fast-charging hub, housing 150 charging stations and a battery storage system, that operates in the Pennsylvania-New Jersey- Maryland interconnection (PJM) power grid.}
}
@article{REINKENSMEYER201260,
title = {A computational model of use-dependent motor recovery following a stroke: Optimizing corticospinal activations via reinforcement learning can explain residual capacity and other strength recovery dynamics},
journal = {Neural Networks},
volume = {29-30},
pages = {60-69},
year = {2012},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2012.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608012000317},
author = {David J. Reinkensmeyer and Emmanuel Guigon and Marc A. Maier},
keywords = {Stroke, Motor control, Reinforcement learning, Use-dependent plasticity},
abstract = {This paper describes a computational model of use-dependent recovery of movement strength following a stroke. The model frames the problem of strength recovery as that of learning appropriate activations of residual corticospinal neurons to their target motoneuronal pools. For example, for an agonist/antagonist muscle pair, we assume the motor system must learn to activate preserved agonist-exciting corticospinal neurons and deactivate preserved antagonist-exciting corticospinal neurons. The model incorporates a biologically plausible reinforcement learning algorithm for adjusting cell activation patterns–stochastic search–using generated limb force as the teaching signal to adjust the synaptic weights that determine cell activations. The model makes predictions consistent with clinical and brain imaging data, such as that patients can achieve an increase in strength after appearing to reach a recovery plateau (i.e., “residual capacity”), that the differential effect of a dose of movement practice will be greater earlier in recovery, and that force-related brain activation will increase in secondary motor areas following a stroke. An interesting prediction that could be explored clinically is that temporarily inhibiting subpopulations of more powerfully connected corticospinal neurons during late movement training will allow the motor system to optimize corticospinal neurons with a weaker influence, whose optimization was blocked by the rapid optimization of more strongly connected neurons early in training.}
}
@article{SASS2022107819,
title = {Multi-agent reinforcement learning-based exploration of optimal operation strategies of semi-batch reactors},
journal = {Computers & Chemical Engineering},
volume = {162},
pages = {107819},
year = {2022},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.107819},
url = {https://www.sciencedirect.com/science/article/pii/S0098135422001570},
author = {Ádám Sass and Alex Kummer and János Abonyi},
keywords = {Temperature control, RL-controller, Feeding trajectory, Cascade control},
abstract = {The operation of semi-batch reactors requires caution because the feeding reagents can accumulate, leading to hazardous situations due to the loss of control ability. This work aims to develop a method that explores the optimal operational strategy of semi-batch reactors. Since reinforcement learning (RL) is an efficient tool to find optimal strategies, we tested the applicability of this concept. We developed a problem-specific RL-based solution for the optimal control of semi-batch reactors in different operation phases. The RL-controller varies the feeding rate in the feeding phase directly, while in the mixing phase, it works as a master in a cascade control structure. The RL-controllers were trained with different neural network architectures to define the most suitable one. The developed RL-based controllers worked very well and were able to keep the temperature at the desired setpoint in the investigated system. The results confirm the benefit of the proposed problem-specific RL-controller.}
}
@article{SORANZO2022105182,
title = {A Reinforcement Learning approach to the location of the non-circular critical slip surface of slopes},
journal = {Computers & Geosciences},
volume = {166},
pages = {105182},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2022.105182},
url = {https://www.sciencedirect.com/science/article/pii/S0098300422001352},
author = {Enrico Soranzo and Carlotta Guardiani and Ahsan Saif and Wei Wu},
keywords = {Deep Q-Network, Factor of safety, Method of slices, Non-circular failure, Reinforcement Learning, Slip surface, Slope stability},
abstract = {We propose a numerical procedure to locate the critical slip surface of slopes with the method of slices. We employ the Deep-Q Learning algorithm with experience replay and target memory to determine a non-circular slip surface. The overall stability analysis is performed with Janbu’s simplified method. Our approach, however, is flexible and can accommodate other Limit Equilibrium Methods. The accuracy of the method is demonstrated by using typical verification examples and comparing the results to the search methods implemented in SLOPE/W and Slide2. It is demonstrated that the Deep-Q learning algorithm can be efficiently applied to layered slopes.}
}
@article{PARK2023107964,
title = {Optimal energy storage system control using a Markovian degradation model—Reinforcement learning approach},
journal = {Journal of Energy Storage},
volume = {71},
pages = {107964},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2023.107964},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X23013610},
author = {Jaemin Park and Taehyeon Kwon and Min K. Sim},
keywords = {Energy storage system, Battery degradation, Electricity cost, Time-of-use, Reinforcement learning},
abstract = {The degradation property of an energy storage system (ESS) has a decisive impact on the economic benefits of ESS operation. However, existing degradation models either do not fully reflect the effects of battery usage or require a full operation history of the battery. This study proposes an accurate Markovian model for battery degradation that reflects battery usage and requires only the current state. We demonstrate the use of the model by establishing a Markov decision process based on the proposed Markovian degradation model and solving it with deep reinforcement learning algorithms. Our intelligent agent, which aims to minimize the sum of electricity cost and degradation cost, generates cost savings of 5%–29% compared to baseline strategies. This framework offers an optimal ESS operation strategy considering battery degradation in a tractable and practical way.}
}
@article{CHOI2023105111,
title = {Reinforcement learning-based dynamic planning of cut and fill operations for earthwork optimization},
journal = {Automation in Construction},
volume = {156},
pages = {105111},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105111},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523003710},
author = {Gwan Choi and SangUk Han},
keywords = {Earthwork allocation planning, Dynamic environment, Reinforcement learning, Attention mechanism},
abstract = {Earthwork allocation planning aims to minimize earthwork costs by optimizing cut–fill pairs and their sequences. However, previous optimization approaches are limited in their ability to respond to dynamic changes in earthworks in computational processes. This paper proposes a reinforcement learning model with an attention mechanism that can address such dynamics by learning a strategy of selecting the cut–fill pairs with the shortest travel time in a given environment state through trial-and-error processes. For validation, the proposed model was compared with benchmarking models through four experiments: one-dimensional problems, two-dimensional problems, a constraint-change scenario, and a case study. The benchmarking results showed that the differences in total travel times were 4.845%, 5.183%, −0.068%, and 18.577%, respectively, implying that the travel time can be better optimized by incorporating the changing environments into the learning process. Hence, the proposed model can contribute to reducing earthwork costs by enhancing earthwork planning in practice.}
}
@incollection{KOOL2018153,
title = {Chapter 7 - Competition and Cooperation Between Multiple Reinforcement Learning Systems},
editor = {Richard Morris and Aaron Bornstein and Amitai Shenhav},
booktitle = {Goal-Directed Decision Making},
publisher = {Academic Press},
pages = {153-178},
year = {2018},
isbn = {978-0-12-812098-9},
doi = {https://doi.org/10.1016/B978-0-12-812098-9.00007-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128120989000073},
author = {Wouter Kool and Fiery A. Cushman and Samuel J. Gershman},
keywords = {Cognitive control, Decision-making, Model-based, Model-free, Reinforcement learning},
abstract = {Most psychological research on reinforcement learning has depicted two systems locked in battle for control of behavior: a flexible but computationally expensive “model-based” system and an inflexible but cheap “model-free” system. However, the complete picture is more complex, with the two systems cooperating in myriad ways. We focus on two issues at the frontier of this research program. First, how is the conflict between these systems adjudicated? Second, how the systems can be combined to harness the relative strengths of each? This chapter reviews recent work on competition and cooperation between the two systems, highlighting the computational principles that govern different forms of interaction.}
}
@article{LIN2022108894,
title = {Multiagent-based deep reinforcement learning for risk-shifting portfolio management},
journal = {Applied Soft Computing},
volume = {123},
pages = {108894},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108894},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622002678},
author = {Yu-Cen Lin and Chiao-Ting Chen and Chuan-Yun Sang and Szu-Hao Huang},
keywords = {Portfolio selection, Deep reinforcement learning, Multiagent},
abstract = {The growing popularity of quantitative trading in pursuit of a systematic and algorithmic approach to investment has drawn considerable attention among traders and investment firms. Consequently, an effective computational method for evaluating potential risk factors and returns is crucial for the development of algorithmic trading strategies. In traditional finance and financial engineering research, statistical approaches have been widely applied to quantitative analysis. Meanwhile, investor demand for quantitative hedge funds has surged worldwide. In the current study, the multiperiod portfolio selection problem was considered in terms of the realistic transaction cost model, which is a major concern for quantitative hedge fund managers. We developed a dedicated multiagent-based deep reinforcement learning framework with a two-level nested agent structure to determine effective portfolio management methods with different objectives. In addition, we proposed a specially-designed reward function for investment performance evaluation and a novel policy network structure for trading decision-making. To efficiently identify specific asset attributes in a portfolio, each agent is equipped with a refined deep policy network and a special training method that enables the proposed reinforcement learning agent to learn risk transfer behaviors. The results revealed the effectiveness of our proposed framework, which outperformed several established or representative portfolio selection strategies.}
}
@article{ZHAO20202465,
title = {Fault-Tolerant Control for the Formation of Multiple Unknown Nonlinear Quadrotors via Reinforcement Learning⁎⁎This work was supported by the National Natural Science Foundation of China under Grants 61873012, 61503012, and 61633007, and the Office of Naval Research under Grant N00014-17-1-2239.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {2465-2470},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.194},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320304596},
author = {Wanbing Zhao and Hao Liu and Frank L. Lewis},
keywords = {Fault-tolerant control, formation control, reinforcement learning, model-free, quadrotor system},
abstract = {In this paper, the fault-tolerant control problem for the formation of unknown quadrotor team with nonlinearities, couplings, and actuator faults in the dynamics is investigated. A distributed observer is designed to estimate the position references for each quadrotor. A hierarchical control scheme is constructed including a fault-tolerant position controller to achieve the desired formation and a fault-tolerant attitude controller to track the attitude references. Reinforcement learning algorithms are designed to learn the optimal control policies of the position and attitude controllers. Simulation results are given to illustrate the effectiveness of the proposed controller.}
}
@article{ZHANG2023100107,
title = {Tactical conflict resolution in urban airspace for unmanned aerial vehicles operations using attention-based deep reinforcement learning},
journal = {Green Energy and Intelligent Transportation},
volume = {2},
number = {4},
pages = {100107},
year = {2023},
issn = {2773-1537},
doi = {https://doi.org/10.1016/j.geits.2023.100107},
url = {https://www.sciencedirect.com/science/article/pii/S2773153723000439},
author = {Mingcheng Zhang and Chao Yan and Wei Dai and Xiaojia Xiang and Kin Huat Low},
keywords = {Unmanned aircraft system traffic management, Tactical conflict resolution, Double deep Q network, Attention mechanism, Secondary conflict},
abstract = {Unmanned aerial vehicles (UAVs) have gained much attention from academic and industrial areas due to the significant number of potential applications in urban airspace. A traffic management system for these UAVs is needed to manage this future traffic. Tactical conflict resolution for unmanned aerial systems (UASs) is an essential piece of the puzzle for the future UAS Traffic Management (UTM), especially in very low-level (VLL) urban airspace. Unlike conflict resolution in higher altitude airspace, the dense high-rise buildings are an essential source of potential conflict to be considered in VLL urban airspace. In this paper, we propose an attention-based deep reinforcement learning approach to solve the tactical conflict resolution problem. Specifically, we formulate this task as a sequential decision-making problem using Markov Decision Process (MDP). The double deep Q network (DDQN) framework is used as a learning framework for the host drone to learn to output conflict-free maneuvers at each time step. We use the attention mechanism to model the individual neighbor's effect on the host drone, endowing the learned conflict resolution policy to be adapted to an arbitrary number of neighboring drones. Lastly, we build a simulation environment with various scenarios covering different types of encounters to evaluate the proposed approach. The simulation results demonstrate that our proposed algorithm provides a reliable solution to minimize secondary conflict counts compared to learning and non-learning-based approaches under different traffic density scenarios.}
}
@article{HUANG2023110802,
title = {Algorithmic trading using combinational rule vector and deep reinforcement learning},
journal = {Applied Soft Computing},
volume = {147},
pages = {110802},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110802},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623008207},
author = {Zhen Huang and Ning Li and Wenliang Mei and Wenyong Gong},
keywords = {Algorithmic trading, Combinational rule vectors, Deep reinforcement learning},
abstract = {Algorithmic trading rules are widely used in financial markets as technical analysis tools for security trading. However, traditional trading rules are not sufficient to make a trading decision. In this paper, we propose a new algorithmic trading method called CR-DQN, which incorporates deep Q-learning with two popular trading rules: moving average (MA) and trading range break-out (TRB). The input of deep Q-learning is combinational rule vectors, whose component is a linear combination of 140 rules produced by MA and TRB with different parameters. Due to non-stationary characteristics, we devise a reward driven combination weight updating scheme to generate combinational rule vectors, which can capture intrinsic features of financial data. Since the sparse reward exists in CR-DQN, we design a piecewise reward function which shows great potential in the experiments. Taking combinational rule vectors as input, the LSTM based Deep Q-learning network is used to learn an optimal algorithmic trading strategy. We apply our model to both Chinese and non-Chinese stock markets, and CR-DQN exhibits the best performance on a variety of evaluation criteria compared to many other approaches, demonstrating the effectiveness of the proposed method.}
}
@article{LIU2023125890,
title = {Energy management for hybrid electric vehicles based on imitation reinforcement learning},
journal = {Energy},
volume = {263},
pages = {125890},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.125890},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222027761},
author = {Yonggang Liu and Yitao Wu and Xiangyu Wang and Liang Li and Yuanjian Zhang and Zheng Chen},
keywords = {Energy management, Dynamic programming, Hybrid electric vehicle, Imitation reinforcement learning},
abstract = {An effective energy management strategy (EMS) in hybrid electric vehicles (HEVs) is indispensable to promote consumption efficiency due to time-varying load conditions. Currently, learning based algorithms have been widely applied in energy controlling performance of HEVs. However, the enormous computation intensity, massive data training and rigid requirement of prediction of future operation state hinder their substantial exploitation. To mitigate these concerns, an imitation reinforcement learning-based algorithm with optimal guidance is proposed in this paper for energy control of hybrid vehicles to accelerate the solving process and meanwhile achieve preferable control performance. Firstly, offline global optimization is firstly conducted considering various driving conditions to search power allocation trajectories. Then, the battery depletion boundaries with respect to driving distance are introduced to generate a narrowed state space, in which the optimal trajectory is fused into the training process of reinforcement learning to guide the high-efficiency strategy production. The simulation validations reveal that the proposed method provides preferable energy reduction for HEVs in arbitrary driving scenarios, and suggests an efficient solution instruction for similar problems in mechanical and electrical systems with constraints and optimal information.}
}
@article{ELHAKI2022116714,
title = {Reinforcement learning-based saturated adaptive robust neural-network control of underactuated autonomous underwater vehicles},
journal = {Expert Systems with Applications},
volume = {197},
pages = {116714},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116714},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422001907},
author = {Omid Elhaki and Khoshnam Shojaei and Parisa Mehrmohammadi},
keywords = {Adaptive robust control, Reinforcement learning, Actor–Critic neural network, Actuator saturation, Actuator nonlinearity},
abstract = {This paper studies a high-performance intelligent online adaptive robust saturated dynamic surface control framework for underactuated autonomous underwater vehicles by engaging Actor–Critic neural networks in the presence of unmodeled dynamics, uncertainties, ocean disturbances and actuator saturation. The proposed controller is designed based on reinforcement learning method to compensate the effects of unmodeled dynamics and uncertainties more accurately that can lead to a better performance for the controller. The Actor–Critic neural networks are trained real-time by designing online training laws creatively and a new critic function is proposed to supervise the closed-loop performance with the aid of the Critic neural network. The proposed structure for the reinforcement learning method benefits from a model-free algorithm and only relies on the measurable variables of the closed-loop control system. This independency from system dynamics results in considerable low computational burden for the controller and, hence, the proposed control algorithm is efficient computationally. The hyperbolic tangent function is ingeniously used as a saturated stabilizer term to bound the control signals that results in low-amplitude control action and the probability of actuator saturation phenomenon is minimized by learning and compensating the actuator saturation nonlinearity as well. Finally, the stability of the proposed closed-loop system is investigated by the Lyapunov’s direct methodology and simulations along a comparative study with some quantitative assessments certify the contributions of this paper.}
}
@incollection{MARTINEZ2021419,
title = {Probabilistic Modeling for Optimization of Bioreactors using Reinforcement Learning with Active Inference},
editor = {Metin Türkay and Rafiqul Gani},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {50},
pages = {419-424},
year = {2021},
booktitle = {31st European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-88506-5.50066-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323885065500668},
author = {Ernesto C. Martínez and Jong Woo Kim and Tilman Barz and Mariano N. {Cruz Bournazou}},
keywords = {active inference, optimization, probabilistic models, reinforcement learning},
abstract = {The open-ended complexity of abiotic conditions in bioreactors means that it is generally infeasible to model its dynamic behaviour comprehensively. Learning optimization oriented probabilistic models encoding a parsimonious representation is far more efficient for bioprocess development and optimization. In this work, active inference is integrated with reinforcement learning to demonstrate that useful probabilistic models for bioreactor optimization can be learned by balancing optimization-oriented and information-seeking objectives. The baker’s yeast bioprocess is used as a case study. For online Bayesian update of model parameter distributions, simulation results demonstrate that highly informative data can be sampled by minimizing the variational free energy of the expected future. The resulting probabilistic model is thus biased towards bioreactor optimization.}
}
@article{ZHANG2019112199,
title = {Deep reinforcement learning–based approach for optimizing energy conversion in integrated electrical and heating system with renewable energy},
journal = {Energy Conversion and Management},
volume = {202},
pages = {112199},
year = {2019},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2019.112199},
url = {https://www.sciencedirect.com/science/article/pii/S0196890419312051},
author = {Bin Zhang and Weihao Hu and Di Cao and Qi Huang and Zhe Chen and Frede Blaabjerg},
keywords = {Integrated energy system, Dynamic energy conversion, Artificial intelligence, Deep reinforcement learning, Proximal policy optimization},
abstract = {With advanced information technologies applied in integrated energy systems (IESs), controlling the energy conversion has become an effective method for improving grid flexibility and reducing the operating cost of IESs. This study proposes a dynamic energy conversion strategy for the energy management of an IES with renewable energy, which considers the system operator’s (SO) operating cost. Deep reinforcement learning (DRL) is used to illustrate the hierarchical decision-making process, in which the dynamic energy conversion problem is formulated as a discrete finite Markov decision process, and proximal policy optimization (PPO) is adopted to solve the decision-making problem. Using DRL, the SO can adaptively decide the wind power conversion ratio during the online learning process, where the uncertainties of customers’ load demand profiles, flexibility of spot electricity prices, and wind power generation are addressed. Simulations show that the proposed PPO-based renewable energy conversion algorithm can effectively reduce the SO’s operating cost.}
}
@article{RADOVIC2021107537,
title = {Hardware implementation of the upper confidence-bound algorithm for reinforcement learning},
journal = {Computers & Electrical Engineering},
volume = {96},
pages = {107537},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107537},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621004821},
author = {Nevena Radović and Milena Erceg},
keywords = {FPGA, Hardware implementation, Machine learning, Multi-armed bandit problem, Upper confidence-bound algorithm},
abstract = {The upper confidence-bound algorithm has been identified as a popular and useful approach in reinforcement learning, suitable for solving diverse modern-day problems. In this paper, we have developed efficient, multiple-clock-cycle hardware for this algorithm to ensure its practical application in real-time. The real-life situation that belongs to a class of problems commonly known as multi-armed bandit problems has been observed. The developed design is tested and verified by a field-programmable gate array circuit design. The obtained results have the degree of accuracy of the ones achieved in software simulation, which proofs the robustness of the developed solution. In terms of execution time, the proposed hardware implementation significantly outperforms the software simulation. Finally, the calculation complexity of the implementation does not depend on the number of observed iterations, which guarantees the effective implementation of the developed design. All implementation details have been provided.}
}
@article{ZHOU2020106016,
title = {Combined heat and power system intelligent economic dispatch: A deep reinforcement learning approach},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {120},
pages = {106016},
year = {2020},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2020.106016},
url = {https://www.sciencedirect.com/science/article/pii/S0142061519336713},
author = {Suyang Zhou and Zijian Hu and Wei Gu and Meng Jiang and Meng Chen and Qiteng Hong and Campbell Booth},
keywords = {Combined heat and power economic dispatch, Deep reinforcement learning, Proximal policy optimization},
abstract = {This paper proposed a Deep Reinforcement learning (DRL) approach for Combined Heat and Power (CHP) system economic dispatch which obtain adaptability for different operating scenarios and significantly decrease the computational complexity without affecting accuracy. In the respect of problem description, a vast of Combined Heat and Power (CHP) economic dispatch problems are modeled as a high-dimensional and non-smooth objective function with a large number of non-linear constraints for which powerful optimization algorithms and considerable time are required to solve it. In order to reduce the solution time, most engineering applications choose to linearize the optimization target and devices model. To avoid complicated linearization process, this paper models CHP economic dispatch problems as Markov Decision Process (MDP) that making the model highly encapsulated to preserve the input and output characteristics of various devices. Furthermore, we improve an advanced deep reinforcement learning algorithm: distributed proximal policy optimization (DPPO), to make it applicable to CHP economic dispatch problem. Based on this algorithm, the agent will be trained to explore optimal dispatch strategies for different operation scenarios and respond to system emergencies efficiently. In the utility phase, the trained agent will generate optimal control strategy in real time based on current system state. Compared with existing optimization methods, advantages of DRL methods are mainly reflected in the following three aspects: 1) Adaptability: under the premise of the same network topology, the trained agent can handle the economic scheduling problem in various operating scenarios without recalculation. 2) High encapsulation: The user only needs to input the operating state to get the control strategy, while the optimization algorithm needs to re-write the constraints and other formulas for different situations. 3) Time scale flexibility: It can be applied to both the day-ahead optimized scheduling and the real-time control. The proposed method is applied to two test system with different characteristics. The results demonstrate that the DRL method could handle with varieties of operating situations while get better optimization performance than most of other algorithms.}
}
@article{LIU2023110680,
title = {NeuroCrossover: An intelligent genetic locus selection scheme for genetic algorithm using reinforcement learning},
journal = {Applied Soft Computing},
volume = {146},
pages = {110680},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110680},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623006981},
author = {Haoqiang Liu and Zefang Zong and Yong Li and Depeng Jin},
keywords = {Reinforcement learning, Genetic algorithms, Crossover, Intelligent genetic locus selection, Combinatorial optimization problems, Proximal policy optimization},
abstract = {Researchers have been studying genetic algorithms (GAs) extensively in recent decades and employing them to address extremely challenging combinatorial optimization problems (COPs). Although GAs achieve superior performance, they are less efficient because most GAs are designed manually without intelligent parameter configuration to support scalable problem-solving strategies and learnable evolutionary operators. To address this issue, machine learning (ML) techniques have been integrated with GAs for operator and parameter selection, however, few studies have focused on intelligent genetic locus selection for influential operators in GAs. To fill this gap, this paper proposes an intelligent genetic locus selection algorithm that serves as the foundation of parameter configuration for critical operators. With the established framework, the Cross Information Synergistic Attention (CISA) model and the n-step proximal policy optimization (PPO) have been utilized to intelligently select the appropriate genetic locus for the most influential phase, i.e., crossover, during the evolutionary process. The proposed NeuroCrossover algorithm is validated on extensive COPs, including the traveling salesman problem, capacitated vehicle routing problem, and bin packing problem. The results demonstrate the efficiency and effectiveness of our algorithm, which outperforms other methods in terms of solution quality, convergence speed, and generalization. For instance, with the CISA, the average percentage gaps of our algorithm are 3.64% and 6.38% for instances in TSPLIB and CVRPLIB, respectively, obtaining gains of about 0.47% and 1.20% compared to those of GA. The proposed algorithm provides a novel solution to lead GAs to an efficient search and improve their scalability and learnability.}
}
@article{YUNGAICELANAULA2022103444,
title = {A flexible SDN-based framework for slow-rate DDoS attack mitigation by using deep reinforcement learning},
journal = {Journal of Network and Computer Applications},
volume = {205},
pages = {103444},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103444},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522000960},
author = {Noe M. Yungaicela-Naula and Cesar Vargas-Rosales and Jesús Arturo Pérez-Díaz and Diego Fernando Carrera},
keywords = {Software defined networking, Deep learning, Deep reinforcement learning, Intrusion detection system, Slow rate DDoS, Mitigation},
abstract = {Distributed Denial-of-Service (DDoS) attacks are difficult to mitigate with existing defense tools. Fortunately, it has been demonstrated that Software-Defined Networking (SDN) with machine learning (ML) and deep learning (DL) techniques has a high potential to handle these threats effectively. However, although there are many SDN-based solutions for detecting DDoS attacks, only a few contain mitigation strategies. Additionally, most previous studies have focused on solving high-rate DDoS attacks. For the time being, recent slow-rate DDoS threats are hard to detect and mitigate. In this work, we propose a modular, flexible, and scalable SDN-based framework that integrates a DL-based intrusion detection system (IDS) and a deep reinforcement learning (DRL)-based intrusion prevention system (IPS) to address slow-rate DDoS threats. We incorporated scalability features into this framework, such as data-plane-based traffic monitoring and traffic flow sampling. Moreover, we have designed a lightweight DRL-based IPS to provide rapid mitigation responses. Furthermore, to evaluate the framework, we deployed a data center network using Mininet, Open Network Operating System (ONOS) controller, and Apache Web server. Next, we performed extensive experiments varying the number of attackers and the rate of attack connections. The proposed IDS achieved an average detection rate of 98%, with a flow sampling rate of 30%. In addition, IPS timely mitigated slow-rate DDoS with 100% of success for a few attackers. Taken together, these results show that the proposed framework provides effective responses to malicious and legitimate connections.}
}
@article{ZARRABIAN2016179,
title = {Reinforcement learning approach for congestion management and cascading failure prevention with experimental application},
journal = {Electric Power Systems Research},
volume = {141},
pages = {179-190},
year = {2016},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2016.06.041},
url = {https://www.sciencedirect.com/science/article/pii/S0378779616302504},
author = {Sina Zarrabian and Rabie Belkacemi and Adeniyi A. Babalola},
keywords = {Blackout, Cascading failure, Q-learning, Real-time, Reinforcement learning, Smart grids},
abstract = {This article proposes a method based on the reinforcement learning (RL) for preventing cascading failure (CF) and blackout in smart grids by acting on the output power of the generators in real-time. The proposed research work utilizes the Q-learning algorithm to train the system for the optimal action selection strategy during the state-action learning process by updating the action values based on the obtained rewards. The trained system then is able to relieve congestion of transmission lines in real-time by adjusting the output power of the generators (actions) to prevent consecutive line outages and blackout after N-1 and N-1-1 contingency conditions. The proposed RL-based control is validated through experimental implementation as well as simulation studies on the IEEE 118-bus test system for different contingency case studies. The results obtained from the experimental and simulation studies show the accuracy and robustness of the proposed approach in preventing cascading failure and blackout.}
}
@article{SUN2020227964,
title = {Data-driven reinforcement-learning-based hierarchical energy management strategy for fuel cell/battery/ultracapacitor hybrid electric vehicles},
journal = {Journal of Power Sources},
volume = {455},
pages = {227964},
year = {2020},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2020.227964},
url = {https://www.sciencedirect.com/science/article/pii/S0378775320302676},
author = {Haochen Sun and Zhumu Fu and Fazhan Tao and Longlong Zhu and Pengju Si},
keywords = {Fuel cell hybrid electric vehicle, Energy management strategy, Reinforcement learning, Data driven, Hierarchical power splitting},
abstract = {A reinforcement-learning-based energy management strategy is proposed in this paper for managing energy system of Fuel Cell Hybrid Electric Vehicles (FCHEV) equipped with three power sources. A hierarchical power splitting structure is employed to shrink large state-action space based on an adaptive fuzzy filter. Then, the reinforcement-learning-based algorithm using Equivalent Consumption Minimization Strategy (ECMS) is proposed for tackling high-dimensional state-action space, and finding a trade-off between global learning and real-time implementation. The power splitting policy based on experimental data is obtained by using reinforcement learning algorithm, which allows for many different driving cycles and traffic conditions. The proposed energy management strategy can achieve low computation cost, optimal fuel cell efficiency and energy consumption economy. Simulation results confirm that, compared with existing learning algorithms and optimization methods, the proposed reinforcement-learning-based energy management strategy using ECMS can achieve high computation efficiency, lower power fluctuation of fuel cell and optimal fuel economy of FCHEV.}
}
@article{LIM2023983,
title = {Reinforcement learning-based virtual network embedding: A comprehensive survey},
journal = {ICT Express},
volume = {9},
number = {5},
pages = {983-994},
year = {2023},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2023.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S2405959523000346},
author = {Hyun-Kyo Lim and Ihsan Ullah and Youn-Hee Han and Sang-Youn Kim},
keywords = {Virtual network embedding, Deep reinforcement learning, Graph neural network, Artificial intelligence},
abstract = {Virtual network embedding plays a vital role in network virtualization, as it determines the deployment and connection of virtual networks to the physical network in the 5G and beyond. An efficient virtual network embedding algorithm is essential to ensure that virtual networks are embedded in a way that meets the performance, security, and resource requirements of the virtual networks and their users. The integration of reinforcement learning with virtual network embedding can lead to more intelligent and efficient network management, which can enhance the performance of large-scale networked systems. Reinforcement learning has the potential to improve and overcome some limitations of traditional algorithms, such as the need for prior knowledge of network conditions and the difficulty in dealing with non-linear and dynamic network environments. Therefore, we conducted this survey to provide a comprehensive overview and examine potential future directions for the optimal reinforcement learning-based virtual network embedding solutions. However, applying reinforcement learning directly to virtual network embedding is a challenging task that requires further research and study. Additionally, it encourages researchers to examine the potential of reinforcement learning in virtual network embedding, identify the challenges for its application, and cover various factors related to the reinforcement learning application in virtual network embedding, including motivations, performance metrics, and challenges.}
}
@article{WELTZ2022139,
title = {Reinforcement Learning Methods in Public Health},
journal = {Clinical Therapeutics},
volume = {44},
number = {1},
pages = {139-154},
year = {2022},
issn = {0149-2918},
doi = {https://doi.org/10.1016/j.clinthera.2021.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0149291821004550},
author = {Justin Weltz and Alex Volfovsky and Eric B. Laber},
keywords = {decision making, machine learning, public health, reinforcement learning},
abstract = {Purpose
Reinforcement learning (RL) is the subfield of machine learning focused on optimal sequential decision making under uncertainty. An optimal RL strategy maximizes cumulative utility by experimenting only if and when the information generated by experimentation is likely to outweigh associated short-term costs. RL represents a holistic approach to decision making that evaluates the impact of every action (ie, data collection, allocation of resources, and treatment assignment) in terms of short-term and long-term utility to stakeholders. Thus, RL is an ideal model for a number of complex decision problems that arise in public health, including resource allocation in a pandemic, monitoring or testing, and adaptive sampling for hidden populations. Nevertheless, although RL has been applied successfully in a wide range of domains, including precision medicine, it has not been widely adopted in public health. The purposes of this review are to introduce key ideas in RL and to identify challenges and opportunities associated with the application of RL in public health.
Methods
We provide a nontechnical review of the theoretical and methodologic underpinnings of RL. A running example of RL for the management of an infectious disease is used to illustrate ideas.
Findings
RL has the potential to make a transformative impact in a range of sequential decision problems in public health. By allocating resources if, when, and where they are most impactful, RL can improve health outcomes while reducing resource consumption.
Implications
Public health researchers and stakeholders should consider RL as a means of efficiently using data to inform optimal evidence-based decision making.}
}
@article{SHI20222182,
title = {Satellite attitude tracking control of moving targets combining deep reinforcement learning and predefined-time stability considering energy optimization},
journal = {Advances in Space Research},
volume = {69},
number = {5},
pages = {2182-2196},
year = {2022},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2021.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S027311772100908X},
author = {Zhong Shi and Fanyu Zhao and Xin Wang and Zhonghe Jin},
keywords = {Moving target tracking, Attitude tracking control, Twin delayed deep deterministic policy gradient algorithm, Predefined-time stability},
abstract = {Space-based moving targets tracking and observation facilitates target recognition and analysis of target characteristics, but the ability of satellite attitude tracking control needs to be improved, especially considering the energy optimization for long-time tracking. An attitude tracking control method combining deep reinforcement learning and predefined-time stability is proposed, which not only improves the autonomous decision-making ability but also ensures the reliability of the satellite attitude control. The long short-term memory network is integrated into the twin delayed deep deterministic policy gradient algorithm to learn the moving state of the target from its image positions as the input to generate the desired attitude in real time, and energy optimization is considered in the design of the reward function. Then an adaptive backstepping controller is designed to achieve predefined-time stability in the presence of the external disturbance and uncertain inertia properties, which ensures that the satellite attitude is controlled to the desired value within a predefined decision period. Finally, a simulation system of moving target tracking is established, and the results indicate that our approach is superior in terms of tracking ability and energy consumption.}
}
@article{KUMAR2022107987,
title = {Traffic scheduling, network slicing and virtualization based on deep reinforcement learning},
journal = {Computers and Electrical Engineering},
volume = {100},
pages = {107987},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107987},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622002567},
author = {Priyan Malarvizhi Kumar and Shakila Basheer and Bharat S. Rawal and Fatemeh Afghah and Gokulnath Chandra Babu and Manimuthu Arunmozhi},
keywords = {Deep reinforcement learning, Network slicing, Traffic scheduling},
abstract = {The revolutionary paradigm of the 5 G network slicing introduces promising market possibilities through multi-tenancy support. Customized slices might be provided to other tenants at a different price as an emerging company to operators. Network slicing is difficult to deliver higher performance and cost-effective facilities through render resources utilisation in alignment with customer activity. Therefore, this paper, Deep Reinforcement Learning-based Traffic Scheduling Model (DRLTSM), has been proposed to interact with the environment by searching for new alternative actions and reinforcement patterns believed to encourage outcomes. The DRL for network slicing situations addresses power control and core network slicing and priority-based sizing involves radio resource. This paper aims to develop three main network slicing blocks i) traffic analysis and network slice forecasting, (ii) network slice admission management decisions, and (iii) adaptive load prediction corrections based on calculated deviations; Our findings suggest very significant possible improvements show that DRLTSM is dramatically improving its efficiency rate to 97.32%, scalability and compatibility in comparison with its baseline.}
}
@article{LOPEZGUEDE201895,
title = {Making physical proofs of concept of reinforcement learning control in single robot hose transport task complete},
journal = {Neurocomputing},
volume = {271},
pages = {95-103},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.01.110},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217312237},
author = {Jose Manuel Lopez-Guede and Julian Estevez and Asier Garmendia and Manuel Graña},
keywords = {Reinforcement learning, Linked multicomponent robotic systems, LMCRS, Hose transport, Proof of concept},
abstract = {This paper deals with the realization of physical proof of concept experiments in the paradigm of Linked Multi-Component Robotic Systems (LMCRS). The main objective is to demonstrate that the controllers learned through Reinforcement Learning (RL) algorithms with different state space formalizations and different spatial discretizations in a simulator are reliable in a real world configuration of the task of transporting a hose by a single robot. This one is a prototypical example of LMCRS task (extendable to much more complex tasks). We describe how the complete system has been designed and implemented. Two different previously learned RL controllers have been tested solving two different LMCRS control problems, using different state space modeling and discretization step in each case. The physical realizations validate previously published simulation based results, giving a strong argument in favor of the suitability of RL techniques to deal with LMCRS systems.}
}
@article{ZHAO2022377,
title = {Dynamic metasurface control using Deep Reinforcement Learning},
journal = {Mathematics and Computers in Simulation},
volume = {197},
pages = {377-395},
year = {2022},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2022.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0378475422000696},
author = {Ying Zhao and Liang Li and Stéphane Lanteri and Jonathan Viquerat},
keywords = {Dynamic metasurface, Artificial neural networks, Deep reinforcement learning, Proximal policy optimization},
abstract = {Dynamic metasurface is an emerging concept for achieving a flexible control of electromagnetic waves. Generalized sheet transition conditions (GSTCs) can be used to model the relationship between the electromagnetic response and surface susceptibility parameters characterizing a metasurface. However, when it comes to the inverse problem of designing and controlling a metasurface in a space–time varying context based on GSTCs, the dynamic synthesis of the susceptibility parameters is a difficult and non-intuitive task. In this paper, we transform the inverse problem of solving dynamic susceptibility parameters into a sequence of control problems. Based on FDTD numerical simulations, a Deep Reinforcement Learning (DRL) framework using a proximal policy optimization (PPO) algorithm and a fully connected neural network is designed to control the susceptibility parameters intelligently and efficiently, promoting the further expansion of the application range of metasurface and thus helping realize more flexible and effective control of electromagnetic waves. We provide numerical results in a one-dimensional setting to show the applicability, correctness and effectiveness of the proposed approach.}
}
@article{MENG2023310,
title = {Integrating safety constraints into adversarial training for robust deep reinforcement learning},
journal = {Information Sciences},
volume = {619},
pages = {310-323},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.051},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522013408},
author = {Jinling Meng and Fei Zhu and Yangyang Ge and Peiyao Zhao},
keywords = {Reinforcement learning, Adversarial training, Penalty function method, Robust deep reinforcement learning, Constrained Markov decision process},
abstract = {The ability to resist interference is the key to the widespread application of reinforcement learning. Although adversarial training is a promising method for robust promotion, standard adversarial training leads to unstable results or performance deterioration due to the presence of perturbation. To address the problem, a robust reinforcement learning method which integrates safety constraints that are modelled by environment termination conditions into adversarial training is proposed, where safety constraints are adopted to restrict agent’s actions and guide the training process. For better modelling the robust reinforcement learning problem, a modified constrained Markov Decision Process (MDP) that considers perturbation for robust reinforcement learning, named Constrained Markov Decision Process (CMDP) with Perturbation (CMDPP) is also introduced. The proposed safe robust reinforcement learning method based on CMDPP utilizes the penalty function to solve CMDP and generates perturbation from the gradient of state for adversarial training. Tests on the robustness of the proposed method under several attack methods and evaluation of generalization through changing environment dynamics were carried out on the OpenAI gym and Roboschool environments. The results demonstrate that our method not only has a better performance confronting the attack but also has a higher generalization capability with reference to the changing environment dynamics.}
}
@article{LI2023100280,
title = {Deep reinforcement learning for wind and energy storage coordination in wholesale energy and ancillary service markets},
journal = {Energy and AI},
volume = {14},
pages = {100280},
year = {2023},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2023.100280},
url = {https://www.sciencedirect.com/science/article/pii/S2666546823000526},
author = {Jinhao Li and Changlong Wang and Hao Wang},
keywords = {Wind-battery system, Wind curtailment, Electricity market, Deep reinforcement learning},
abstract = {Wind energy has been increasingly adopted to mitigate climate change. However, the variability of wind energy causes wind curtailment, resulting in considerable economic losses for wind farm owners. Wind curtailment can be reduced using battery energy storage systems (BESS) as onsite backup sources. Yet, this auxiliary role may significantly weaken the economic potential of BESS in energy trading. Ideal BESS scheduling should balance onsite wind curtailment reduction and market bidding, but practical implementation is challenging due to coordination complexity and the stochastic nature of energy prices and wind generation. We investigate the joint-market bidding strategy of a co-located wind-battery system in the spot and Regulation Frequency Control Ancillary Service markets. We propose a novel deep reinforcement learning-based approach that decouples the system’s market participation into two related Markov decision processes for each facility, enabling the BESS to absorb onsite wind curtailment while performing joint-market bidding to maximize overall operational revenues. Using realistic wind farm data, we validated the coordinated bidding strategy, with outcomes surpassing the optimization-based benchmark in terms of higher revenue by approximately 25% and more wind curtailment reduction by 2.3 times. Our results show that joint-market bidding can significantly improve the financial performance of wind-battery systems compared to participating in each market separately. Simulations also show that using curtailed wind generation as a power source for charging the BESS can lead to additional financial gains. The successful implementation of our algorithm would encourage co-location of generation and storage assets to unlock wider system benefits.}
}
@article{YANG2023105475,
title = {A new intelligent fault diagnosis framework for rotating machinery based on deep transfer reinforcement learning},
journal = {Control Engineering Practice},
volume = {134},
pages = {105475},
year = {2023},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2023.105475},
url = {https://www.sciencedirect.com/science/article/pii/S0967066123000448},
author = {Daoguang Yang and Hamid Reza Karimi and Marek Pawelczyk},
keywords = {Deep reinforcement learning, Convolutional auto-encoder, Fault diagnosis, Double deep Q network, Transfer learning},
abstract = {The advancement of artificial intelligence algorithms has gained growing interest in identifying the fault types in rotary machines, which is a high-efficiency but not a human-like module. Hence, in order to build a human-like fault identification module that could learn knowledge from the environment, in this paper, a deep reinforcement learning framework is proposed to provide an end-to-end training mode and a human-like learning process based on an improved Double Deep Q Network. In addition, to improve the convergence properties of the Deep Reinforcement Learning algorithm, the parameters of the former layers of the convolutional neural networks are transferred from a convolutional auto-encoder under an unsupervised learning process. The experiment results show that the proposed framework could efficiently extract the fault features from raw time-domain data and have higher accuracy than other deep learning models with balanced samples and better performance with imbalanced samples.}
}
@article{WAQAR20221,
title = {Deep multi-agent reinforcement learning for resource allocation in NOMA-enabled MEC},
journal = {Computer Communications},
volume = {196},
pages = {1-8},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.09.018},
url = {https://www.sciencedirect.com/science/article/pii/S014036642200367X},
author = {Noor Waqar and Syed Ali Hassan and Haris Pervaiz and Haejoon Jung and Kapal Dev},
keywords = {Non-orthogonal multiple access (NOMA), Mobile edge computing (MEC), Multi-agent reinforcement learning (MARL)},
abstract = {Non-orthogonal multiple access (NOMA) and mobile edge computing (MEC) are being considered as promising technologies to address the stringent demands of the emerging fifth generation (5G) networks. This paper investigates the resource allocation problem in NOMA-enabled MEC system for multiple users, by joint optimization of power and computation resources to enhance effective throughput of the system. Because of the severe non convexity of the problem, a decentralized multi-agent reinforcement learning (MARL) scheme is proposed, where each user to base station (U2B) link acts as an agent, and collectively interacts with the network environment, in order to maximize the objective function under limited power and computation resource constraints. Simulation results demonstrate that the proposed MARL scheme results in considerable improvement in the effective throughput of the system, which is comparable to the optimal results derived from the exhaustive optimal search, with significantly low overhead.}
}
@article{LI2021117541,
title = {A data-driven output voltage control of solid oxide fuel cell using multi-agent deep reinforcement learning},
journal = {Applied Energy},
volume = {304},
pages = {117541},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117541},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921009193},
author = {Jiawen Li and Tao Yu and Bo Yang},
keywords = {Large-scale multi-agent deep reinforcement learning, Discrete-continuous hybrid action space large-scale multi-agent twin delayed deep deterministic policy gradient, Solid oxide fuel cell, Output voltage control, Fuel utilization},
abstract = {To effectively control the output voltage of solid oxide fuel cells (SOFCs) and improve the operating efficiency of SOFC systems, an SOFC output voltage data-driven controller based on multi-agent large-scale deep reinforcement learning is proposed, whereby a discrete–continuous hybrid action space large-scale multi-agent twin delayed deep deterministic policy gradient (DHASL-MATD3) is used as the control algorithm for this controller. To solve the low robustness problem of deep reinforcement learning-based conventional controllers, this algorithm adopts a hybrid action space multi-agent policy that achieves parallel exploration by using double deep Q-learning (DDQN) agents with discrete space and deep deterministic policy gradient (DDPG) agents with continuous action space, thus improving exploration efficiency and realizing excellent robustness. In addition, many techniques are adopted by this algorithm to solve the problem of Q-value overestimation. Ultimately, an SOFC output voltage controller with stronger robustness is obtained. Simulation results show that this controller can effectively control the output voltage of a SOFC by regulating the fuel flux and maintaining its fuel utilization within a reasonable range.}
}
@article{ZHANG2023120972,
title = {Cost-aware scheduling systems for real-time workflows in cloud: An approach based on Genetic Algorithm and Deep Reinforcement Learning},
journal = {Expert Systems with Applications},
volume = {234},
pages = {120972},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120972},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423014744},
author = {Jingwei Zhang and Long Cheng and Cong Liu and Zhiming Zhao and Ying Mao},
keywords = {Workflow scheduling, Cloud computing, Deep Reinforcement Learning, Deep Q-learning, Genetic Algorithm},
abstract = {With the development of cloud computing, a growing number of applications are migrating to a cloud environment. In the process, the real-time scheduling of workflows has gradually become a technical challenge, due to the dynamic and uncertain nature of cloud environments and the complex dependencies between sub-tasks of the workflow. Although various methods have been reported up to now, these methods have their respective shortcomings, such as heuristic-based methods are hard to find optimal scheduling scheme and metaheuristic-based methods incur high computational overhead, which often lead to the violation of QoS (Quality of Service) requirements and increases service renting costs of executing workflows. Inspired by the successful application of Deep Reinforcement Learning (DRL) in cloud job scheduling, this paper proposes a real-time workflow scheduling method which combines Genetic Algorithm (GA) and DRL, aiming to reduce both execution cost and response time. To be specific, we design a real-time workflow scheduling algorithm named GA-DQN by combining the global search capability of GA and the environment awareness decision-making capability of DRL to divides scheduling process into two stages. First, the execution scheme of workflow in virtual machine is calculated when workflow arrives. Then, a DRL agent uses this scheme as the feature of workflow to assign workflow to a suitable virtual machine. In this way, the use of DRL to sense environment increases the computational efficiency of GA, and the execution scheme obtained by GA helps DRL to obtain the feature of workflow. On this basis of real world workflow, three groups of simulation experience are carried out to compare GA-DQN with four baseline method which consist of three traditional methods and a state-of-the-art method. The comparison results demonstrate that GA-DQN outperforms the other methods in terms of response time, execution cost, and success rate across different workloads and cloud instance configurations.}
}
@article{PARK2023106465,
title = {Distributional and hierarchical reinforcement learning for physical systems with noisy state observations and exogenous perturbations},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106465},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106465},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623006498},
author = {Jehyun Park and Jongeun Choi and Sungjae Nah and Dohee Kim},
keywords = {Hierarchical reinforcement learning, Distributional reinforcement learning, Noisy state observation, Exogenous perturbation},
abstract = {Reinforcement learning has shown remarkable success in various applications, and in some cases, even outperforms human performance. However, despite the potential of reinforcement learning, numerous challenges still exist. In this paper, we introduce a novel approach that exploits the synergies between hierarchical reinforcement learning and distributional reinforcement learning to address complex sparse-reward tasks, where noisy state observations or non-stationary exogenous perturbations are present. Our proposed method has a hierarchical policy structure, where random rewards are modeled as random variables that follow a value distribution. This approach enables the handling of complex tasks and increases robustness to uncertainties arising from measurement noise or exogenous perturbations, such as wind. To achieve this, we extend the distributional soft Bellman operator and temporal difference error to include the hierarchical structure, and we use quantile regression to approximate the reward distribution. We evaluate our method using a bipedal robot in the OpenAI Gym environment and an electric autonomous vehicle in the SUMO traffic simulator. The results demonstrate the effectiveness of our approach in solving complex tasks with the aforementioned uncertainties when compared to state-of-the-art methods. Our approach demonstrates promising results in handling uncertainties caused by noise and perturbations for challenging sparse-reward tasks, and could potentially pave the way for the development of more robust and effective reinforcement learning algorithms in real physical systems.}
}
@article{GUAN2023103232,
title = {Reinforcement learning-driven deep question generation with rich semantics},
journal = {Information Processing & Management},
volume = {60},
number = {2},
pages = {103232},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103232},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322003338},
author = {Menghong Guan and Subrota Kumar Mondal and Hong-Ning Dai and Haiyong Bao},
keywords = {Deep question generation, Reinforcement learning, Semantic graphs, Gated Graph Neural Network},
abstract = {Deep question generation (DQG) refers to generating a complex question from different sentences in context. Existing methods mainly focus on enhancing information extraction based on the encoder–decoder neural networks though they cannot perform well in DQG tasks. To address this issue, we consider combining reinforcement learning with semantic-rich information to generate deep questions in this paper. In particular, we propose a Semantic-Rich Reinforcement Learning Deep Question Generation (SRL-DQG) model, which better utilizes the semantic graphs of document representations based on the Gated Graph Neural Network (GGNN). In order to generate high-quality questions, we also optimize specific objectives via reinforcement learning with consideration of four evaluation factors including naturality, relevance, answerability, and difficulty. Empirical evaluations demonstrate that our SRL-DQG effectively improves the quality of generated questions and achieves superior performance than existing methods in terms of multiple performance metrics. Specifically, we show that several BLEU-n scores were improved by 3.5% to 10% after running SRL-DQG on 6072 samples of HotPotQA.}
}
@article{LI2023120702,
title = {ΔV-learning: An adaptive reinforcement learning algorithm for the optimal stopping problem},
journal = {Expert Systems with Applications},
volume = {231},
pages = {120702},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120702},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423012046},
author = {Xiying Li and Chi-Guhn Lee},
keywords = {Reinforcement learning, Optimal stopping, Markov decision process, Stochastic control, Secretary problem, Optimal replacement},
abstract = {The optimal stopping problem is concerned with finding an optimal policy to stop a stochastic process in order to maximize the expected return. This problem is critical in stochastic control and can be found in many different fields, such as operations research, finance, and healthcare. In this paper, we model the underlying stochastic process of the optimal stopping problem as a Markov decision process and propose a computationally efficient model-free value-based reinforcement learning approach, named ΔV-learning. The efficiency is improved by taking advantage of the unique structural properties of the optimal stopping problem into our algorithm design. We consider two types of the optimal stopping problems: the standard optimal stopping and the regenerative optimal stopping, which differ in their transition dynamics once the stopping action is executed. We conduct numerical experiments on our proposed method and compare its performance against existing reinforcement learning algorithms and rule-based policies. The results show that our ΔV-learning method is able to outperform the benchmark algorithms in all experiments.}
}
@article{JALALI2022100903,
title = {Solar irradiance forecasting using a novel hybrid deep ensemble reinforcement learning algorithm},
journal = {Sustainable Energy, Grids and Networks},
volume = {32},
pages = {100903},
year = {2022},
issn = {2352-4677},
doi = {https://doi.org/10.1016/j.segan.2022.100903},
url = {https://www.sciencedirect.com/science/article/pii/S2352467722001552},
author = {Seyed Mohammad Jafar Jalali and Sajad Ahmadian and Bahareh Nakisa and Mahdi Khodayar and Abbas Khosravi and Saeid Nahavandi and Syed Mohammed Shamsul Islam and Miadreza Shafie-khah and João P.S. Catalão},
keywords = {Solar irradiance forecasting, Deep neural networks, Evolutionary computation, Ensemble strategy, Deep reinforcement learning},
abstract = {Solar irradiance forecasting is a major priority for the power transmission systems in order to generate and incorporate the performance of massive photovoltaic plants efficiently. As such, prior forecasting techniques that use classical modelling and single deep learning models that undertake feature extraction procedures manually were unable to meet the output demands in specific situations with dynamic variability. Therefore, in this study, we propose an efficient novel hybrid solar irradiance forecasting model based on three steps. In the first step, we employ a powerful variable input selection strategy named as partial mutual information (PMI) to calculate the linear and non-linear correlations of the original solar irradiance data. In the second step, unlike the traditional deep learning models designing their architectures manually, we utilize several deep long short term memory-convolutional neural network (LSTM-CNN) models optimized by a novel modified whale optimization algorithm in order to compute the forecasting results of the solar irradiance datasets. Finally, in the third step, we deploy a deep reinforcement learning strategy for selecting the best subset of the combined deep optimized LSTM-CNN models. Through analysing the forecasting results over two real-world datasets gathered from the USA solar irradiance stations, it can be inferred that our proposed algorithm outperforms other powerful benchmarked algorithms in 1-step, 2-step, 12-step, and 24-step ahead forecasting.}
}
@article{FENG2022103611,
title = {Coordinating ride-sourcing and public transport services with a reinforcement learning approach},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {138},
pages = {103611},
year = {2022},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.103611},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X22000572},
author = {Siyuan Feng and Peibo Duan and Jintao Ke and Hai Yang},
keywords = {Ride-sourcing service, Multimodal transportation, Reinforcement learning, Order dispatching, Public transit},
abstract = {Combining ride-sourcing and public transit services (with ride-sourcing service to address the first/last-mile issues) can bring many benefits, such as saving passengers’ trip fares, improving drivers’ earnings, reducing gas emissions, and alleviating traffic congestion. However, it still remains a challenging issue to coordinate ride-sourcing and public transit services through real-time order dispatching. In this paper, we model the order dispatching in a multi-modal transportation system as a large-scale sequential decision-making problem. A centralized algorithm is then proposed to dispatch idle drivers to arriving passenger orders and determine whether to advise passengers to use a combined mode of ride-sourcing and public transit services (if yes, the algorithm also needs to recommend an appropriate transportation hub). In particular, our proposed algorithm contains a reinforcement learning approach that estimates the long-term expected rewards, and an Integer Linear Programming (ILP) that matches idle drivers and waiting passengers in real-time based on both immediate revenue and the estimated long-term rewards. By evaluation on the real-world on-demand data and metro system in Manhattan, the proposed method shows remarkable improvement on the system’s efficiency under different density of supply and demands.}
}
@article{KAMALAPURKAR2016247,
title = {Efficient model-based reinforcement learning for approximate online optimal control},
journal = {Automatica},
volume = {74},
pages = {247-258},
year = {2016},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2016.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0005109816303272},
author = {Rushikesh Kamalapurkar and Joel A. Rosenfeld and Warren E. Dixon},
keywords = {Model-based reinforcement learning, Data-based control, Adaptive control, Local approximation},
abstract = {An infinite horizon optimal regulation problem is solved online for a deterministic control-affine nonlinear dynamical system using a state following (StaF) kernel method to approximate the value function. Unlike traditional methods that aim to approximate a function over a large compact set, the StaF kernel method aims to approximate a function in a small neighborhood of a state that travels within a compact set. Simulation results demonstrate that stability and approximate optimality of the control system can be achieved with significantly fewer basis functions than may be required for global approximation methods.}
}
@article{RATHORE2023301511,
title = {Adversarial superiority in android malware detection: Lessons from reinforcement learning based evasion attacks and defenses},
journal = {Forensic Science International: Digital Investigation},
volume = {44},
pages = {301511},
year = {2023},
note = {Selected papers of the Tenth Annual DFRWS EU Conference},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2023.301511},
url = {https://www.sciencedirect.com/science/article/pii/S2666281723000124},
author = {Hemant Rathore and Adarsh Nandanwar and Sanjay K. Sahay and Mohit Sewak},
keywords = {Android, Adversarial robustness, Machine and deep learning, Malware detection, Reinforcement learning},
abstract = {Today, android smartphones are being used by billions of users and thus have become a lucrative target of malware designers. Therefore being one step ahead in this zero-sum game of malware detection between the anti-malware community and malware developers is more of a necessity than a desire. This work focuses on a proactive adversary-aware framework to develop adversarially superior android malware detection models. We first investigate the adversarial robustness of thirty-six distinct malware detection models constructed using two static features (permission and intent) and eighteen classification algorithms. We designed two Targeted Type-II Evasion Attacks (TRPO-MalEAttack and PPO-MalEAttack) based on reinforcement learning to exploit vulnerabilities in the above malware detection models. The attacks aim to add minimum perturbations in each malware application and convert it into an adversarial application that can fool the malware detection models. The TRPO-MalEAttack achieves an average fooling rate of 95.75% (with 2.02 mean perturbations), reducing the average accuracy from 86.01% to 49.11% in thirty-six malware detection models. On the other hand, The PPO-MalEAttack achieves a higher average fooling rate of 96.87% (with 2.08 mean perturbations), reducing the average accuracy from 86.01% to 48.65% in the same thirty-six detection models. We also develop a list of the TEN most vulnerable android permissions and intents that an adversary can use to generate more adversarial applications. Later, we propose a defense strategy (MalVPatch) to counter the adversarial attacks on malware detection models. The MalVPatch defense achieves higher detection accuracy along with a drastic improvement in the adversarial robustness of malware detection models. Finally, we conclude that investigating the adversarial robustness of models is necessary before their real-world deployment and helps achieve adversarial superiority in android malware detection.}
}