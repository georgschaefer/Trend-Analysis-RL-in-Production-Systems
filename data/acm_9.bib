@article{10.1145/3590154,
author = {Tong, Junbo and Shi, Daming and Liu, Yi and Fan, Wenhui},
title = {GLDAP: Global Dynamic Action Persistence Adaptation for Deep Reinforcement Learning},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3590154},
doi = {10.1145/3590154},
abstract = {In the implementation of deep reinforcement learning (DRL), action persistence strategies are often adopted so agents maintain their actions for a fixed or variable number of steps. The choice of the persistent duration for agent actions usually has notable effects on the performance of reinforcement learning algorithms. Aiming at the research gap of global dynamic optimal action persistence and its application in multi-agent systems, we propose a novel framework: global dynamic action persistence (GLDAP), which achieves global action persistence adaptation for deep reinforcement learning. We introduce a closed-loop method that is used to learn the estimated value and the corresponding policy of each candidate action persistence. Our experiment shows that GLDAP achieves an average of 2.5\%~90.7\% performance improvement and 3~20 times higher sampling efficiency over several baselines across various single-agent and multi-agent domains. We also validate the ability of GLDAP to determine the optimal action persistence through multiple experiments.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {may},
articleno = {7},
numpages = {18},
keywords = {temporal abstraction, action persistence, Deep reinforcement learning}
}

@inproceedings{10.1109/WIIAT.2008.88,
author = {Salkham, As'ad and Cunningham, Raymond and Garg, Anurag and Cahill, Vinny},
title = {A Collaborative Reinforcement Learning Approach to Urban Traffic Control Optimization},
year = {2008},
isbn = {9780769534961},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WIIAT.2008.88},
doi = {10.1109/WIIAT.2008.88},
abstract = {The high growth rate of vehicles per capita now poses a real challenge to efficient Urban Traffic Control (UTC). An efficient solution to UTC must be adaptive in order to deal with the highly-dynamic nature of urban traffic. In the near future, global positioning systems and vehicle-to-vehicle/infrastructure communication may provide a more detailed local view of the traffic situation that could be employed for better global UTC optimization. In this paper we describe the design of a next-generation UTC system that exploits such local knowledge about a junction's traffic in order to optimize traffic control. Global UTC optimization is achieved using a local Adaptive Round Robin (ARR) phase switching model optimized using Collaborative Reinforcement Learning (CRL). The design employs an ARR-CRL-based agent controller for each signalized junction that collaborates with neighbouring agents in order to learn appropriate phase timing based on the traffic pattern. We compare our approach to non-adaptive fixed-time UTC system and to a saturation balancing algorithm in a large-scale simulation of traffic in Dublin's inner city centre. We show that the ARR-CRL approach can provide significant improvement resulting in up to ~57\% lower average waiting time per vehicle compared to the saturation balancing algorithm.},
booktitle = {Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {560–566},
numpages = {7},
keywords = {Reinforcement Learning, Optimization, Collaborative Reinforcement Learning, Urban Traffic Control},
series = {WI-IAT '08}
}

@inproceedings{10.1145/3360901.3364417,
author = {Merkle, Nicole and Philipp, Patrick},
title = {Cooperative Web Agents by Combining Semantic Technologies with Reinforcement Learning},
year = {2019},
isbn = {9781450370080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360901.3364417},
doi = {10.1145/3360901.3364417},
abstract = {An increasing number of Web pages are enriched with semantic annotations using, for instance, RDFa and Schema.org in order to ease data interpretation for Web browsers and search engines. Lifted data representations might enable a new class of semantic agents for complex decision-making on the Web, reaching from filling out Web forms and booking flights to mastering any Web-based task. However, exclusively using semantic models has several drawbacks, such as extensive manual efforts for enabling semantic agents to solve Web tasks or limited capabilities to personalize task solutions for end users. While using semantics is a crucial component for guiding prospective semantic agents, we have to find a balance between modelling rich background knowledge and learning optimal agent behaviour through advanced Machine Learning (ML) methods. In this work, we propose a semantic agent framework for (i) modelling agent-related semantic annotations for Web tasks, (ii) using the latter to train statistical agents using Reinforcement Learning (RL) in an offline simulation as well as in real online use and (iii) feeding back the learned agent behaviour and its provenance information in terms of a semantic model that can be directly used by purely semantics-based agents. We evaluate our approach based on the MiniWob++ benchmark for automatically solving Web tasks. We show that our proposed semantic agent framework enables to (i) warm start agents with semantic background knowledge to faster learn task-dependent optimal behaviour in terms of the expected cumulative reward and (ii) directly reuse the learned behaviour by semantic-based agents that act by automatically derived Notation3 (N3) implication rules.},
booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
pages = {205–212},
numpages = {8},
keywords = {web automation, semantic agents, semantic web, simulation, reinforcement learning},
location = {Marina Del Rey, CA, USA},
series = {K-CAP '19}
}

@inproceedings{10.1145/3551349.3560507,
author = {Ferdous, Raihana and Kifetew, Fitsum and Prandi, Davide and Susi, Angelo},
title = {Towards Agent-Based Testing of 3D Games Using Reinforcement Learning},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560507},
doi = {10.1145/3551349.3560507},
abstract = {Computer game is a billion-dollar industry and is booming. Testing games has been recognized as a difficult task, which mainly relies on manual playing and scripting based testing. With the advances in technologies, computer games have become increasingly more interactive and complex, thus play-testing using human participants alone has become unfeasible. In recent days, play-testing of games via autonomous agents has shown great promise by accelerating and simplifying this process. Reinforcement Learning solutions have the potential of complementing current scripted and automated solutions by learning directly from playing the game without the need of human intervention. This paper presented an approach based on reinforcement learning for automated testing of 3D games. We make use of the notion of curiosity as a motivating factor to encourage an RL agent to explore its environment. The results from our exploratory study are promising and we have preliminary evidence that reinforcement learning can be adopted for automated testing of 3D games.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {211},
numpages = {8},
keywords = {reinforcement learning, functional coverage, game play testing},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.5555/3427510.3427538,
author = {Greasley, Andrew},
title = {Implementing Reinforcement Learning in Simio Discrete-Event Simulation Software},
year = {2020},
isbn = {9781713814290},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {A significant barrier to the combined use of simulation and AI techniques such as Machine Learning (ML) are that developers have differing backgrounds and use different tools. In particular simulation model developers may lack expertise in the software tools and coding abilities needed for the development of ML algorithms. In order to bridge this gap this article presents a discrete-event simulation (DES) that incorporates the use of a reinforcement learning (RL) algorithm which determines an approximate best route for robots in a factory moving from one physical location to another whilst avoiding collisions with fixed barriers. The study shows how the object modelling and graphical facilities of the Simio commercial off-the-shelf (COTS) DES software package enables an RL capability without the need to use program code or require an interface with external RL software.},
booktitle = {Proceedings of the 2020 Summer Simulation Conference},
articleno = {27},
numpages = {11},
keywords = {autonomous robot, SIMIO, machine learning, reinforcement learning},
location = {Virtual Event, Spain},
series = {SummerSim '20}
}

@inproceedings{10.1145/3510003.3510625,
author = {Tufano, Rosalia and Scalabrino, Simone and Pascarella, Luca and Aghajani, Emad and Oliveto, Rocco and Bavota, Gabriele},
title = {Using Reinforcement Learning for Load Testing of Video Games},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510625},
doi = {10.1145/3510003.3510625},
abstract = {Different from what happens for most types of software systems, testing video games has largely remained a manual activity performed by human testers. This is mostly due to the continuous and intelligent user interaction video games require. Recently, reinforcement learning (RL) has been exploited to partially automate functional testing. RL enables training smart agents that can even achieve super-human performance in playing games, thus being suitable to explore them looking for bugs. We investigate the possibility of using RL for load testing video games. Indeed, the goal of game testing is not only to identify functional bugs, but also to examine the game's performance, such as its ability to avoid lags and keep a minimum number of frames per second (FPS) when high-demanding 3D scenes are shown on screen. We define a methodology employing RL to train an agent able to play the game as a human while also trying to identify areas of the game resulting in a drop of FPS. We demonstrate the feasibility of our approach on three games. Two of them are used as proof-of-concept, by injecting artificial performance bugs. The third one is an open-source 3D game that we load test using the trained agent showing its potential to identify areas of the game resulting in lower FPS.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {2303–2314},
numpages = {12},
keywords = {load testing, reinforcement learning},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3351180.3351199,
author = {Guo, Zichang and Huang, Jin and Ren, Wenjie and Wang, Chundong},
title = {A Reinforcement Learning Approach for Inverse Kinematics of Arm Robot},
year = {2019},
isbn = {9781450371834},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351180.3351199},
doi = {10.1145/3351180.3351199},
abstract = {The inverse kinematics is the foundation and emphases of the industrial robot control. Traditional solutions of inverse kinematics cause many difficulties to the exploitation of many kinds of industrial robots because of the complexity derivation, difficulty of calculation, multiple solutions, lack of instantaneity. This paper proposes a new way to obtain the inverse kinematics of 5-DOF arm robot with a grip by using the method of deep deterministic policy gradient in reinforcement learning, the method combines the neural network and robotics knowledge through the continuing attempts to get the accuracy solution. The propose of the simulation by Tensorflow and Matplotlib is designed to verify the accuracy of the new way, the results of simulations show that comparing with the traditional way, the end grip joint of robot can arrive at the location we set with some more error, but the angle of every joint can be calculated and the error is in an acceptable range, the accuracy of the angle and posture is satisfied. This is a new way to solve inverse kinematics of robot which is easier than traditional way, but has more meaning on solutions of multi-degree of freedom robots.},
booktitle = {Proceedings of the 2019 4th International Conference on Robotics, Control and Automation},
pages = {95–99},
numpages = {5},
keywords = {Reinforcement learning, Inverse kinematics, DDPG, Arm robot},
location = {Guangzhou, China},
series = {ICRCA 2019}
}

@article{10.5555/2627435.2627456,
author = {Moore, Brett L. and Pyeatt, Larry D. and Kulkarni, Vivekanand and Panousis, Periklis and Padrez, Kevin and Doufas, Anthony G.},
title = {Reinforcement Learning for Closed-Loop Propofol Anesthesia: A Study in Human Volunteers},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Clinical research has demonstrated the efficacy of closed-loop control of anesthesia using the bispectral index of the electroencephalogram as the controlled variable. These controllers have evolved to yield patient-specific anesthesia, which is associated with improved patient outcomes. Despite progress, the problem of patient-specific anesthesia remains unsolved. A variety of factors confound good control, including variations in human physiology, imperfect measures of drug effect, and delayed, hysteretic response to drug delivery. Reinforcement learning (RL) appears to be uniquely equipped to overcome these challenges; however, the literature offers no precedent for RL in anesthesia. To begin exploring the role RL might play in improving anesthetic care, we investigated the method's application in the delivery of patient-specific, propofol-induced hypnosis in human volunteers. When compared to performance metrics reported in the anesthesia literature, RL demonstrated patient-specific control marked by improved accuracy and stability. Furthermore, these results suggest that RL may be considered a viable alternative for solving other difficult closed-loop control problems in medicine. More rigorous clinical study, beyond the confines of controlled human volunteer studies, is needed to substantiate these findings.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {655–696},
numpages = {42},
keywords = {propofol, closed-loop control, hypnosis, reinforcement learning, anesthesia, bispectral index}
}

@inproceedings{10.1145/3316781.3317768,
author = {Zheng, Hao and Louri, Ahmed},
title = {An Energy-Efficient Network-on-Chip Design Using Reinforcement Learning},
year = {2019},
isbn = {9781450367257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316781.3317768},
doi = {10.1145/3316781.3317768},
abstract = {The design space for energy-efficient Network-on-Chips (NoCs) has expanded significantly comprising a number of techniques. The simultaneous application of these techniques to yield maximum energy efficiency requires the monitoring of a large number of system parameters which often results in substantial engineering efforts and complicated control policies. This motivates us to explore the use of reinforcement learning (RL) approach that automatically learns an optimal control policy to improve NoC energy efficiency. First, we deploy power-gating (PG) and dynamic voltage and frequency scaling (DVFS) to simultaneously reduce both static and dynamic power. Second, we use RL to automatically explore the dynamic interactions among PG, DVFS, and system parameters, learn the critical system parameters contained in the router and cache, and eventually evolve optimal per-router control policies that significantly improve energy efficiency. Moreover, we introduce an artificial neural network (ANN) to efficiently implement the large state-action table required by RL. Simulation results using PARSEC benchmark show that the proposed RL approach improves power consumption by 26\%, while improving system performance by 7\%, as compared to a combined PG and DVFS design without RL. Additionally, the ANN design yields 67\% area reduction, as compared to a conventional RL implementation.},
booktitle = {Proceedings of the 56th Annual Design Automation Conference 2019},
articleno = {47},
numpages = {6},
location = {Las Vegas, NV, USA},
series = {DAC '19}
}

@article{10.5555/1390681.1442787,
author = {Cs\'{a}ji, Bal\'{a}zs Csan\'{a}d and Monostori, L\'{a}szl\'{o}},
title = {Value Function Based Reinforcement Learning in Changing Markovian Environments},
year = {2008},
issue_date = {6/1/2008},
publisher = {JMLR.org},
volume = {9},
issn = {1532-4435},
abstract = {The paper investigates the possibility of applying value function based reinforcement learning (RL) methods in cases when the environment may change over time. First, theorems are presented which show that the optimal value function of a discounted Markov decision process (MDP) Lipschitz continuously depends on the immediate-cost function and the transition-probability function. Dependence on the discount factor is also analyzed and shown to be non-Lipschitz. Afterwards, the concept of (ε,δ)-MDPs is introduced, which is a generalization of MDPs and ε-MDPs. In this model the environment may change over time, more precisely, the transition function and the cost function may vary from time to time, but the changes must be bounded in the limit. Then, learning algorithms in changing environments are analyzed. A general relaxed convergence theorem for stochastic iterative algorithms is presented. We also demonstrate the results through three classical RL methods: asynchronous value iteration, Q-learning and temporal difference learning. Finally, some numerical experiments concerning changing environments are presented.},
journal = {J. Mach. Learn. Res.},
month = {jun},
pages = {1679–1709},
numpages = {31}
}

@inproceedings{10.1145/3380446.3430622,
author = {Zhu, Keren and Liu, Mingjie and Chen, Hao and Zhao, Zheng and Pan, David Z.},
title = {Exploring Logic Optimizations with Reinforcement Learning and Graph Convolutional Network},
year = {2020},
isbn = {9781450375191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380446.3430622},
doi = {10.1145/3380446.3430622},
abstract = {Logic synthesis for combinational circuits is to find the minimum equivalent representation for Boolean logic functions. A well-adopted logic synthesis paradigm represents the Boolean logic with standardized logic networks, such as and-inverter graphs (AIG), and performs logic minimization operations over the graph iteratively. Although the research for different logic representation and operations is fruitful, the sequence of using the operations are often determined by heuristics. We propose a Markov decision process (MDP) formulation of the logic synthesis problem and a reinforcement learning (RL) algorithm incorporating with graph convolutional network to explore the solution search space. The experimental results show that the proposed method outperforms the well-known logic synthesis heuristics with the same sequence length and action space.},
booktitle = {Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD},
pages = {145–150},
numpages = {6},
keywords = {logic synthesis, reinforcement learning, graph neural network},
location = {Virtual Event, Iceland},
series = {MLCAD '20}
}

@inproceedings{10.1145/3587716.3587785,
author = {Liu, Ke and Gu, Jian and Gu, Hao and Zhu, Ziran},
title = {A Hybrid Reinforcement Learning and Genetic Algorithm for VLSI Floorplanning},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587716.3587785},
doi = {10.1145/3587716.3587785},
abstract = {Floorplanning plays an essential role in very large-scale integration (VLSI) design flow since its solution quality significantly affects the circuit’s power, performance, and area (PPA). Under practical manufacturing circumstances, the fixed boundary of a floorplan needs to be considered, and such a tight fixed-outline makes the floorplanning problem more complicated. In this paper, we develop a hybrid reinforcement learning and genetic algorithm for VLSI floorplanning. On the one hand, the crossover and mutation operations of the genetic algorithm are adopted to filter out individuals which are more likely to transform to the global optimal solution. On the other hand, we propose an off-policy based reinforcement learning method to further optimize the individuals, which trains an agent from scratch and makes the agent learn local search optimization strategy of the floorplanning problem. Experiment results show that the proposed algorithm can obtain a better area and wirelength of a floorplan.},
booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
pages = {412–418},
numpages = {7},
keywords = {VLSI, fixed-outline, reinforcement learning, genetic algorithm, floorplanning},
location = {Zhuhai, China},
series = {ICMLC '23}
}

@inproceedings{10.1145/3279755.3279760,
author = {Fraternali, Francesco and Balaji, Bharathan and Gupta, Rajesh},
title = {Scaling Configuration of Energy Harvesting Sensors with Reinforcement Learning},
year = {2018},
isbn = {9781450360470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3279755.3279760},
doi = {10.1145/3279755.3279760},
abstract = {With the advent of the Internet of Things (IoT), an increasing number of energy harvesting methods are being used to supplement or supplant battery based sensors. Energy harvesting sensors need to be configured according to the application, hardware, and environmental conditions to maximize their usefulness. As of today, the configuration of sensors is either manual or heuristics based, requiring valuable domain expertise. Reinforcement learning (RL) is a promising approach to automate configuration and efficiently scale IoT deployments, but it is not yet adopted in practice. We propose solutions to bridge this gap: reduce the training phase of RL so that nodes are operational within a short time after deployment and reduce the computational requirements to scale to large deployments. We focus on configuration of the sampling rate of indoor solar panel based energy harvesting sensors. We created a simulator based on 3 months of data collected from 5 sensor nodes subject to different lighting conditions. Our simulation results show that RL can effectively learn energy availability patterns and configure the sampling rate of the sensor nodes to maximize the sensing data while ensuring that energy storage is not depleted. The nodes can be operational within the first day by using our methods. We show that it is possible to reduce the number of RL policies by using a single policy for nodes that share similar lighting conditions.},
booktitle = {Proceedings of the 6th International Workshop on Energy Harvesting \&amp; Energy-Neutral Sensing Systems},
pages = {7–13},
numpages = {7},
keywords = {scaling, internet of things, reinforcement learning, battery-less},
location = {Shenzhen, China},
series = {ENSsys '18}
}

@inproceedings{10.1145/3378184.3378198,
author = {Schmidt, Alexander and Schellroth, Florian and Riedel, Oliver},
title = {Control Architecture for Embedding Reinforcement Learning Frameworks on Industrial Control Hardware},
year = {2020},
isbn = {9781450376303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378184.3378198},
doi = {10.1145/3378184.3378198},
abstract = {Using reinforcement learning to find new control strategies for manufacturing processes is a promising approach. However, in order to use reinforcement learning in a manufacturing environment, industrial requirements must be met. In this paper, a new control architecture is proposed that allows reinforcement learning frameworks to be executed on programmable logic controllers, which implies requirements regarding real-time execution. An agent exchange module is presented, that allows to automatically load agents from a non real-time learning environment to a control program running in a real-time environment of a programmable logic controller. The proposed architecture is evaluated on an experimental setup with a non-linear process and commercial off-the-shelf control hardware, to meet industrial requirements. A genetic algorithm is used to learn new control strategies without having prior knowledge of the system.},
booktitle = {Proceedings of the 3rd International Conference on Applications of Intelligent Systems},
articleno = {14},
numpages = {6},
keywords = {Real-Time Communication, Control Architecture, Reinforcement Learning, Genetic Algorithm},
location = {Las Palmas de Gran Canaria, Spain},
series = {APPIS 2020}
}

@inproceedings{10.1145/2662117.2662129,
author = {Mehta, Dhanvin and Yamparala, Devesh},
title = {Policy Gradient Reinforcement Learning for Solving Supply-Chain Management Problems},
year = {2014},
isbn = {9781450330374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2662117.2662129},
doi = {10.1145/2662117.2662129},
abstract = {Supply-chain management problems are quite common in various industries and it is becoming increasingly necessary to tackle uncertainties while making decisions due to the rapid rise in production and consumption levels, and shortening of product life cycles. In our work, we tackle this problem of general stochastic supply-chain management problem by formulating it as a multi-arm non-contextual bandit problem and then taking a policy gradient descent approach (a Reinforcement Learning approach) to find a robust policy. The gradient descent is guided by cost from a simulator which models the demand, lead times and other uncertainties. Our experiments demonstrate that it finds better solutions than naive worst-case linear programming solutions to such problems.},
booktitle = {Proceedings of the 6th IBM Collaborative Academia Research Exchange Conference (I-CARE) on I-CARE 2014},
pages = {1–4},
numpages = {4},
keywords = {Reinforcement learning, Supply-chain management},
location = {Bangalore, India},
series = {I-CARE 2014}
}

@article{10.1145/3585316,
author = {Fahim, Abdulrahman and Papalexakis, Evangelos and Krishnamurthy, Srikanth V. and K. Roy Chowdhury, Amit and Kaplan, Lance and Abdelzaher, Tarek},
title = {AcTrak: Controlling a Steerable Surveillance Camera Using Reinforcement Learning},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2378-962X},
url = {https://doi.org/10.1145/3585316},
doi = {10.1145/3585316},
abstract = {Steerable cameras that can be controlled via a network, to retrieve telemetries of interest have become popular. In this paper, we develop a framework called AcTrak, to automate a camera’s motion to appropriately switch between (a) zoom ins on existing targets in a scene to track their activities, and (b) zoom out to search for new targets arriving to the area of interest. Specifically, we seek to achieve a good trade-off between the two tasks, i.e., we want to ensure that new targets are observed by the camera before they leave the scene, while also zooming in on existing targets frequently enough to monitor their activities. There exist prior control algorithms for steering cameras to optimize certain objectives; however, to the best of our knowledge, none have considered this problem, and do not perform well when target activity tracking is required. AcTrak &nbsp;automatically controls the camera’s PTZ configurations using reinforcement learning (RL), to select the best camera position given the current state. Via simulations using real datasets, we show that AcTrak detects newly arriving targets 30\% faster than a non-adaptive baseline and rarely misses targets, unlike the baseline which can miss up to 5\% of the targets. We also implement AcTrak to control a real camera and demonstrate that in comparison with the baseline, it acquires about 2\texttimes{} more high resolution images of targets.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {apr},
articleno = {14},
numpages = {27},
keywords = {PTZ cameras, Steerable cameras, Reinforcement learning, Zoom-in tracking, camera control, surveillance systems}
}

@inproceedings{10.1145/2627585.2627591,
author = {Iacoboaiea, Ovidiu Constantin and Sayrac, Berna and Ben Jemaa, Sana and Bianchi, Pascal},
title = {SON Conflict Resolution Using Reinforcement Learning with State Aggregation},
year = {2014},
isbn = {9781450329903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627585.2627591},
doi = {10.1145/2627585.2627591},
abstract = {In future generation networks one of the main focuses is on automating the network optimization. This is done through so called Self Organizing Network (SON) functions. A SON instance is a realization of a SON function that governs one or several cells. Several independent SON instances of one or multiple SON functions are likely to generate conflicts. This raises the need for a SON COordinator (SONCO) meant to solve these conflicts. In this paper we consider that each SON function has one SON instance on every cell and we present the design of a SONCO function for coordinating all these instances. The SONCO solves the conflicts that appear on the update requests arbitrating (i.e. accepting/denying the requests) so that it minimizes a predefined regret. This regret takes into account the weights associated to the SON functions that rank their importance according to the operator policies. We solve the problem in a Reinforcement Learning (RL) framework as it offers the possibility to improve the decisions based on past experiences. We employ a state-aggregation technique to make the state space of our solution scale linearly with the number of cells. We provide a study case for two SON functions: Mobility Load Balancing (MLB) tuning the Cell Individual Offset(CIO) and Mobility Robustness Optimization (MRO) tuning the CIO together with the handover hysteresis. The proposed SONCO function solves the conflicts on the CIO update requests. Numerical results show how the proposed SONCO is able to favor either MLB or MRO requests according to their associated weights.},
booktitle = {Proceedings of the 4th Workshop on All Things Cellular: Operations, Applications, \&amp; Challenges},
pages = {15–20},
numpages = {6},
keywords = {MLB, MRO, LTE, reinforcement learning, state aggregation, SON coordination, SON instances},
location = {Chicago, Illinois, USA},
series = {AllThingsCellular '14}
}

@inproceedings{10.1145/3580305.3599777,
author = {Tomasi, Federico and Cauteruccio, Joseph and Kanoria, Surya and Ciosek, Kamil and Rinaldi, Matteo and Dai, Zhenwen},
title = {Automatic Music Playlist Generation via Simulation-Based Reinforcement Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599777},
doi = {10.1145/3580305.3599777},
abstract = {Personalization of playlists is a common feature in music streaming services, but conventional techniques, such as collaborative filtering, rely on explicit assumptions regarding content quality to learn how to make recommendations. Such assumptions often result in misalignment between offline model objectives and online user satisfaction metrics. In this paper, we present a reinforcement learning framework that solves for such limitations by directly optimizing for user satisfaction metrics via the use of a simulated playlist-generation environment. Using this simulator we develop and train a modified Deep Q-Network, the action head DQN (AH-DQN), in a manner that addresses the challenges imposed by the large state and action space of our RL formulation. The resulting policy is capable of making recommendations from large and dynamic sets of candidate items with the expectation of maximizing consumption metrics. We analyze and evaluate agents offline via simulations that use environment models trained on both public and proprietary streaming datasets. We show how these agents lead to better user-satisfaction metrics compared to baseline methods during online A/B tests. Finally, we demonstrate that performance assessments produced from our simulator are strongly correlated with observed online metric results.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4948–4957},
numpages = {10},
keywords = {reinforcement learning, music playlist generation, simulation, recommender systems},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3447928.3456653,
author = {Hunt, Nathan and Fulton, Nathan and Magliacane, Sara and Hoang, Trong Nghia and Das, Subhro and Solar-Lezama, Armando},
title = {Verifiably Safe Exploration for End-to-End Reinforcement Learning},
year = {2021},
isbn = {9781450383394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447928.3456653},
doi = {10.1145/3447928.3456653},
abstract = {Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We characterize safety constraints in terms of a refinement relation on Markov decision processes - rather than directly constraining the reinforcement learning algorithm so that it only takes safe actions, we instead refine the environment so that only safe actions are defined in the environment's transition structure. This has pragmatic system design benefits and, more importantly, provides a clean conceptual setting in which we are able to prove important safety and efficiency properties. These allow us to transform the constrained optimization problem of acting safely in the original environment into an unconstrained optimization in a refined environment.},
booktitle = {Proceedings of the 24th International Conference on Hybrid Systems: Computation and Control},
articleno = {14},
numpages = {11},
keywords = {formal verification, safe artificial intelligence, hybrid systems, neural networks, differential dynamic logic, reinforcement learning},
location = {Nashville, Tennessee},
series = {HSCC '21}
}

@inproceedings{10.1145/3329859.3329876,
author = {Hilprecht, Benjamin and Binnig, Carsten and R\"{o}hm, Uwe},
title = {Towards Learning a Partitioning Advisor with Deep Reinforcement Learning},
year = {2019},
isbn = {9781450368025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329859.3329876},
doi = {10.1145/3329859.3329876},
abstract = {In this paper we introduce a partitioning advisor for analytical workloads based on Deep Reinforcement Learning. In contrast to existing approaches for automated partitioning design, an RL agent learns its decisions based on experience by trying out different partitionings and monitoring the rewards for different workloads. In our experimental evaluation with a distributed database and various complex schemata, we show that our learned partitioning advisor is thus not only able to find partitionings that outperform existing approaches for automated data partitioning but is also able to find non-obvious partitionings.},
booktitle = {Proceedings of the Second International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
articleno = {6},
numpages = {4},
location = {Amsterdam, Netherlands},
series = {aiDM '19}
}

@inproceedings{10.1145/3368826.3377928,
author = {Haj-Ali, Ameer and Ahmed, Nesreen K. and Willke, Ted and Shao, Yakun Sophia and Asanovic, Krste and Stoica, Ion},
title = {NeuroVectorizer: End-to-End Vectorization with Deep Reinforcement Learning},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377928},
doi = {10.1145/3368826.3377928},
abstract = {One of the key challenges arising when compilers vectorize loops for today’s SIMD-compatible architectures is to decide if vectorization or interleaving is beneficial. Then, the compiler has to determine the number of instructions to pack together and the interleaving level (stride). Compilers are designed today to use fixed-cost models that are based on heuristics to make vectorization decisions on loops. However, these models are unable to capture the data dependency, the computation graph, or the organization of instructions. Alternatively, software engineers often hand-write the vectorization factors of every loop. This, however, places a huge burden on them, since it requires prior experience and significantly increases the development time. In this work, we explore a novel approach for handling loop vectorization and propose an end-to-end solution using deep reinforcement learning (RL). We conjecture that deep RL can capture different instructions, dependencies, and data structures to enable learning a sophisticated model that can better predict the actual performance cost and determine the optimal vectorization factors. We develop an end-to-end framework, from code to vectorization, that integrates deep RL in the LLVM compiler. Our proposed framework takes benchmark codes as input and extracts the loop codes. These loop codes are then fed to a loop embedding generator that learns an embedding for these loops. Finally, the learned embeddings are used as input to a Deep RL agent, which dynamically determines the vectorization factors for all the loops. We further extend our framework to support random search, decision trees, supervised neural networks, and nearest-neighbor search. We evaluate our approaches against the currently used LLVM vectorizer and loop polyhedral optimization techniques. Our experiments show 1.29\texttimes{}−4.73\texttimes{} performance speedup compared to baseline and only 3\% worse than the brute-force search on a wide range of benchmarks.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {242–255},
numpages = {14},
keywords = {LLVM, Code Optimization, Deep Reinforcement Learning, Automatic Vectorization},
location = {San Diego, CA, USA},
series = {CGO 2020}
}

@inproceedings{10.1145/3318464.3389779,
author = {Bar El, Ori and Milo, Tova and Somech, Amit},
title = {Automatically Generating Data Exploration Sessions Using Deep Reinforcement Learning},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389779},
doi = {10.1145/3318464.3389779},
abstract = {Exploratory Data Analysis (EDA) is an essential yet highly demanding task. To get a head start before exploring a new dataset, data scientists often prefer to view existing EDA notebooks -- illustrative, curated exploratory sessions, on the same dataset, that were created by fellow data scientists who shared them online. Unfortunately, such notebooks are not always available (e.g., if the dataset is new or confidential). To address this, we present ATENA, a system that takes an input dataset and auto-generates a compelling exploratory session, presented in an EDA notebook. We shape EDA into a control problem, and devise a novel Deep Reinforcement Learning (DRL) architecture to effectively optimize the notebook generation. Though ATENA uses a limited set of EDA operations, our experiments show that it generates useful EDA notebooks, allowing users to gain actual insights.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1527–1537},
numpages = {11},
keywords = {interactive data analysis, auto generated, auto EDA, data exploration, EDA, autogenerated, notebooks, EDA notebooks},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3534678.3539193,
author = {Prabhakar, Prakruthi and Yuan, Yiping and Yang, Guangyu and Sun, Wensheng and Muralidharan, Ajith},
title = {Multi-Objective Optimization of Notifications Using Offline Reinforcement Learning},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539193},
doi = {10.1145/3534678.3539193},
abstract = {Mobile notification systems play a major role in a variety of applications to communicate, send alerts and reminders to the users to inform them about news, events or messages. In this paper, we formulate the near-real-time notification decision problem as a Markov Decision Process where we optimize for multiple objectives in the rewards. We propose an end-to-end offline reinforcement learning framework to optimize sequential notification decisions. We address the challenge of offline learning using a Double Deep Q-network method based on Conservative Q-learning that mitigates the distributional shift problem and Q-value overestimation. We illustrate our fully-deployed system and demonstrate the performance and benefits of the proposed approach through both offline and online experiments.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3752–3760},
numpages = {9},
keywords = {reinforcement learning, offline evaluation, mobile notifications},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.5555/3545946.3598671,
author = {Li, Michelle and Dennis, Michael},
title = {The Benefits of Power Regularization in Cooperative Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Cooperative Multi-Agent Reinforcement Learning (MARL) algorithms, trained only to optimize task reward, can lead to a concentration of power where the failure or adversarial intent of a single agent could decimate the reward of every agent in the system. In the context of teams of people, it is often useful to explicitly consider how power is distributed to ensure no person becomes a single point of failure. Here, we argue that explicitly regularizing the concentration of power in cooperative RL systems can result in systems which are more robust to single agent failure, adversarial attacks, and incentive changes of co-players. To this end, we define a practical pairwise measure of power that captures the ability of any co-player to influence the ego agent's reward, and then propose a power-regularized objective which balances task reward and power concentration. Given this new objective, we show that there always exists an equilibrium where every agent is playing a power-regularized best-response balancing power and task reward. Moreover, we present two algorithms for training agents towards this power-regularized objective: Sample Based Power Regularization (SBPR), which injects adversarial data during training; and Power Regularization via Intrinsic Motivation (PRIM), which adds an intrinsic motivation to regulate power to the training objective. Our experiments demonstrate that both algorithms successfully balance task reward and power, leading to lower power behavior than the baseline of task-only reward and avoid catastrophic events in case an agent in the system goes off-policy.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {457–465},
numpages = {9},
keywords = {distribution of power, intrinsic motivation, game theory, adversarial robustness, cooperative multi-agent reinforcement learning, fault tolerance, multi-agent reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.1145/2629486,
author = {Pan, Gung-Yu and Jou, Jing-Yang and Lai, Bo-Cheng},
title = {Scalable Power Management Using Multilevel Reinforcement Learning for Multiprocessors},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1084-4309},
url = {https://doi.org/10.1145/2629486},
doi = {10.1145/2629486},
abstract = {Dynamic power management has become an imperative design factor to attain the energy efficiency in modern systems. Among various power management schemes, learning-based policies that are adaptive to different environments and applications have demonstrated superior performance to other approaches. However, they suffer the scalability problem for multiprocessors due to the increasing number of cores in a system. In this article, we propose a scalable and effective online policy called MultiLevel Reinforcement Learning (MLRL). By exploiting the hierarchical paradigm, the time complexity of MLRL is O(n lg n) for n cores and the convergence rate is greatly raised by compressing redundant searching space. Some advanced techniques, such as the function approximation and the action selection scheme, are included to enhance the generality and stability of the proposed policy. By simulating on the SPLASH-2 benchmarks, MLRL runs 53\% faster and outperforms the state-of-the-art work with 13.6\% energy saving and 2.7\% latency penalty on average. The generality and the scalability of MLRL are also validated through extensive simulations.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {aug},
articleno = {33},
numpages = {23},
keywords = {multiprocessors, reinforcement learning, Dynamic power management}
}

@inproceedings{10.5555/3523760.3523933,
author = {Singh, Saurav and Heard, Jamison},
title = {Human-Aware Reinforcement Learning for Adaptive Human Robot Teaming},
year = {2022},
publisher = {IEEE Press},
abstract = {Mistakes in high stress and critical multitasking environments, such as piloting an airplane and the NASA control room, can lead to catastrophic failures. The human's internal state (e.g., workload) may be used to facilitate a robot teammate's adaptations, such that the robot can interact with the human without negatively impacting overall team performance. Human performance has a direct correlation with workload states; thus, the human's internal workload state may be leveraged to adapt a robot's interactions with the human in order to improve team performance. A reinforcement learning-based paradigm that incorporates human workload states to determine appropriate robot adaptations is presented. Preliminary results using the proposed approach in a supervisory-based NASA MATB-II environment are presented.},
booktitle = {Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1049–1052},
numpages = {4},
keywords = {soft actor critic, workload, human robot interaction, reinforcement learning},
location = {Sapporo, Hokkaido, Japan},
series = {HRI '22}
}

@inproceedings{10.1145/3508352.3549388,
author = {Liu, Guan-Ting and Tai, Wei-Chen and Lin, Yi-Ting and Jiang, Iris Hui-Ru and Shiely, James P. and Cheng, Pu-Jen},
title = {Sub-Resolution Assist Feature Generation with Reinforcement Learning and Transfer Learning},
year = {2022},
isbn = {9781450392174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508352.3549388},
doi = {10.1145/3508352.3549388},
abstract = {As modern photolithography feature sizes continue to shrink, sub-resolution assist feature (SRAF) generation has become a key resolution enhancement technique to improve the manufacturing process window. State-of-the-art works resort to machine learning to overcome the deficiencies of model-based and rule-based approaches. Nevertheless, these machine learning-based methods do not consider or implicitly consider the optical interference between SRAFs, and highly rely on post-processing to satisfy SRAF mask manufacturing rules. In this paper, we are the first to generate SRAFs using reinforcement learning to address SRAF interference and produce mask-rule-compliant results directly. In this way, our two-phase learning enables us to emulate the style of model-based SRAFs while further improving the process variation (PV) band. A state alignment and action transformation mechanism is proposed to achieve orientation equivariance while expediting the training process. We also propose a transfer learning framework, allowing SRAF generation under different light sources without retraining the model. Compared with state-of-the-art works, our method improves the solution quality in terms of PV band and edge placement error (EPE) while reducing the overall runtime.},
booktitle = {Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
articleno = {29},
numpages = {9},
keywords = {transfer learning, Markov decision process, reinforcement learning, design for manufacturability, sub-resolution assist feature},
location = {San Diego, California},
series = {ICCAD '22}
}

@inproceedings{10.1145/3501774.3501786,
author = {V. Chernov, Andrey and K. Savvas, Ilias and A. Butakova, Maria and O. Kartashov, Oleg},
title = {Safe Reinforcement Learning in Simulated Environment of Self-Driving Laboratory},
year = {2022},
isbn = {9781450385060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501774.3501786},
doi = {10.1145/3501774.3501786},
abstract = {Today we see tremendous potential in applying artificial intelligence (AI), deep reinforcement learning, and agent-based simulation to complex real-world problems. AI helps people support and automate decision-making penetrating almost all daily life aspects and research areas. One of the reasons for this potential is that AI helps us solve problems at a lower cost of resources and time. Materials research acceleration often relies upon AI using and automation of laboratory experiments, bringing significant fruitful results and advances. Self-driving laboratories include closed-loop chemistry experimentation and assist in designing new functional nanomaterials and optimizing their known parameters with AI and machine learning approaches. Due to the possibility of involving in the nanomaterials design process and some hazardous components, routine experimentation under chemists' continuous monitoring is usually required. Shifting to new intelligent technologies in self-driving laboratories with automated closed-loop experimentation requires excluding risks and accidents because of improper AI applications. This paper discusses safe deep reinforcement learning and its application in a simulated environment in self-driving laboratories experimenting with new functional materials. We proposed an approach to solving the problem of safe reinforcement learning by learning the intelligent agent to find a hidden reward and implemented that approach by constructing and using the heatmap from observation of the hidden reward neighborhood.},
booktitle = {Proceedings of the 2021 European Symposium on Software Engineering},
pages = {78–84},
numpages = {7},
keywords = {Safe reinforcement learning, Hidden reward, Artificial intelligence safety gridworlds},
location = {Larissa, Greece},
series = {ESSE '21}
}

@inproceedings{10.1145/3395363.3397354,
author = {Pan, Minxue and Huang, An and Wang, Guoxin and Zhang, Tian and Li, Xuandong},
title = {Reinforcement Learning Based Curiosity-Driven Testing of Android Applications},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397354},
doi = {10.1145/3395363.3397354},
abstract = {Mobile applications play an important role in our daily life, while it still remains a challenge to guarantee their correctness. Model-based and systematic approaches have been applied to Android GUI testing. However, they do not show significant advantages over random approaches because of limitations such as imprecise models and poor scalability. In this paper, we propose Q-testing, a reinforcement learning based approach which benefits from both random and model-based approaches to automated testing of Android applications. Q-testing explores the Android apps with a curiosity-driven strategy that utilizes a memory set to record part of previously visited states and guides the testing towards unfamiliar functionalities. A state comparison module, which is a neural network trained by plenty of collected samples, is novelly employed to divide different states at the granularity of functional scenarios. It can determine the reinforcement learning reward in Q-testing and help the curiosity-driven strategy explore different functionalities efficiently. We conduct experiments on 50 open-source applications where Q-testing outperforms the state-of-the-art and state-of-practice Android GUI testing tools in terms of code coverage and fault detection. So far, 22 of our reported faults have been confirmed, among which 7 have been fixed.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {153–164},
numpages = {12},
keywords = {reinforcement learning, functional scenario division, Android app testing},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3342827.3342844,
author = {Chou, Tai-Liang and Hsueh, Yu-Ling},
title = {A Task-Oriented Chatbot Based on LSTM and Reinforcement Learning},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342844},
doi = {10.1145/3342827.3342844},
abstract = {Traditional conversational chatbots usually adopt a retrieved-based model. Developers have to provide a large amount of conversational data and classify those data to different intents. To avoid cumbersome development processes, we propose a method to build a chatbot by a sentence generation model which generates sequence sentences based on the generative adversarial network. The architecture of our model contains a generator that generates a diverse sentence, and a discriminator that judges the sentences between the generated and the raw data. In the generator, we combine the attention model that responses for tracking conversational states with the sequence-to-sequence model using hierarchical long-short term memory to extract sentence information. For the discriminator, we calculate twotypes of rewards to assign low rewards for repeated sentences and high rewards for diverse sentences. Extensive experiments are presented to demonstrate the utility of our model which generates more diverse and information-rich sentences than those of the existing approaches.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {87–91},
numpages = {5},
keywords = {Natural Language Processing, Sentence Generation, Deep Learning, Service Robots, Dialog Management, Chatbots},
location = {Tokushima, Japan},
series = {NLPIR '19}
}

@article{10.1613/jair.1.12594,
author = {Fu, Justin and Tacchetti, Andrea and Perolat, Julien and Bachrach, Yoram},
title = {Evaluating Strategic Structures in Multi-Agent Inverse Reinforcement Learning},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12594},
doi = {10.1613/jair.1.12594},
abstract = {A core question in multi-agent systems is understanding the motivations for an agent's actions based on their behavior. Inverse reinforcement learning provides a framework for extracting utility functions from observed agent behavior, casting the problem as finding domain parameters which induce such a behavior from rational decision makers.&nbsp; We show how to efficiently and scalably extend inverse reinforcement learning to multi-agent settings, by reducing the multi-agent problem to N single-agent problems while still satisfying rationality conditions such as strong rationality. However, we observe that rewards learned naively tend to lack insightful structure, which causes them to produce undesirable behavior when optimized in games with different players from those encountered during training. We further investigate conditions under which rewards or utility functions can be precisely identified, on problem domains such as normal-form and Markov games, as well as auctions, where we show we can learn reward functions that properly generalize to new settings.},
journal = {J. Artif. Int. Res.},
month = {sep},
pages = {925–951},
numpages = {27},
keywords = {reinforcement learning, multiagent systems, game theory}
}

@inproceedings{10.1145/3383313.3411536,
author = {Joachims, Thorsten and Raimond, Yves and Koch, Olivier and Dimakopoulou, Maria and Vasile, Flavian and Swaminathan, Adith},
title = {REVEAL 2020: Bandit and Reinforcement Learning from User Interactions},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3411536},
doi = {10.1145/3383313.3411536},
abstract = {The REVEAL workshop1 focuses on framing the recommendation problem as a one of making personalized interventions, e.g.&nbsp;deciding to recommend a particular item to a particular user. Moreover, these interventions sometimes depend on each other, where a stream of interactions occurs between the user and the system, and where each decision to recommend something will have an impact on future steps and long-term rewards. This framing creates a number of challenges we will discuss at the workshop. How can recommender systems be evaluated offline in such a context? How can we learn recommendation policies that are aware of these delayed consequences and outcomes?},
booktitle = {Proceedings of the 14th ACM Conference on Recommender Systems},
pages = {628–629},
numpages = {2},
keywords = {offline evaluation, multi-armed bandits, off-policy, causal inference, reinforcement learning, recommender systems},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}

@inproceedings{10.1145/3319619.3326843,
author = {Stapelberg, B. and Malan, K. M.},
title = {Global Structure of Policy Search Spaces for Reinforcement Learning},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3326843},
doi = {10.1145/3319619.3326843},
abstract = {Reinforcement learning is gaining prominence in the machine learning community. It dates back over three decades in areas such as cybernetics and psychology, but has more recently been applied widely in robotics, game playing and control systems. There are many approaches to reinforcement learning, most of which are based on the Markov decision process model. The goal of reinforcement learning is to learn the best strategy (referred to as a policy in reinforcement learning) of an agent interacting with its environment in order to reach a specified goal. Recently, evolutionary computation has been shown to be of benefit to reinforcement learning in some limited scenarios. Many studies have shown that the performance of evolutionary computation algorithms is influenced by the structure of the fitness landscapes of the problem being optimised. In this paper we investigate the global structure of the policy search spaces of simple reinforcement learning problems. The aim is to highlight structural characteristics that could influence the performance of evolutionary algorithms in a reinforcement learning context. Results indicate that the problems we investigated are characterised by enormous plateaus that form unimodal structures, resulting in a kind of needle-in-a-haystack global structure.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1773–1781},
numpages = {9},
keywords = {reinforcement learning, local optima networks, fitness landscapes},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.1145/3018661.3018702,
author = {Cai, Han and Ren, Kan and Zhang, Weinan and Malialis, Kleanthis and Wang, Jun and Yu, Yong and Guo, Defeng},
title = {Real-Time Bidding by Reinforcement Learning in Display Advertising},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018702},
doi = {10.1145/3018661.3018702},
abstract = {The majority of online display ads are served through real-time bidding (RTB) --- each ad display impression is auctioned off in real-time when it is just being generated from a user visit. To place an ad automatically and optimally, it is critical for advertisers to devise a learning algorithm to cleverly bid an ad impression in real-time. Most previous works consider the bid decision as a static optimization problem of either treating the value of each impression independently or setting a bid price to each segment of ad volume. However, the bidding for a given ad campaign would repeatedly happen during its life span before the budget runs out. As such, each bid is strategically correlated by the constrained budget and the overall effectiveness of the campaign (e.g., the rewards from generated clicks), which is only observed after the campaign has completed. Thus, it is of great interest to devise an optimal bidding strategy sequentially so that the campaign budget can be dynamically allocated across all the available impressions on the basis of both the immediate and future rewards. In this paper, we formulate the bid decision process as a reinforcement learning problem, where the state space is represented by the auction information and the campaign's real-time parameters, while an action is the bid price to set. By modeling the state transition via auction competition, we build a Markov Decision Process framework for learning the optimal bidding policy to optimize the advertising performance in the dynamic real-time bidding environment. Furthermore, the scalability problem from the large real-world auction volume and campaign budget is well handled by state value approximation using neural networks. The empirical study on two large-scale real-world datasets and the live A/B testing on a commercial platform have demonstrated the superior performance and high efficiency compared to state-of-the-art methods.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {661–670},
numpages = {10},
keywords = {bid optimization, display ads, reinforcement learning},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.1145/3343031.3350961,
author = {Luo, Yadan and Huang, Zi and Zhang, Zheng and Wang, Ziwei and Li, Jingjing and Yang, Yang},
title = {Curiosity-Driven Reinforcement Learning for Diverse Visual Paragraph Generation},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350961},
doi = {10.1145/3343031.3350961},
abstract = {Visual paragraph generation aims to automatically describe a given image from different perspectives and organize sentences in a coherent way. In this paper, we address three critical challenges for this task in a reinforcement learning setting: the mode collapse, the delayed feedback, and the time-consuming warm-up for policy networks. Generally, we propose a novel Curiosity-driven Reinforcement Learning (CRL) framework to jointly enhance the diversity and accuracy of the generated paragraphs. First, by modeling the paragraph captioning as a long-term decision-making process and measuring the prediction uncertainty of state transitions as intrinsic rewards, the model is incentivized to memorize precise but rarely spotted descriptions to context, rather than being biased towards frequent fragments and generic patterns. Second, since the extrinsic reward from evaluation is only available until the complete paragraph is generated, we estimate its expected value at each time step with temporal-difference learning, by considering the correlations between successive actions. Then the estimated extrinsic rewards are complemented by dense intrinsic rewards produced from the derived curiosity module, in order to encourage the policy to fully explore action space and find a global optimum. Third, discounted imitation learning is integrated for learning from human demonstrations, without separately performing the time-consuming warm-up in advance. Extensive experiments conducted on the Standford image-paragraph dataset demonstrate the effectiveness and efficiency of the proposed method, improving the performance by 38.4\% compared with state-of-the-art.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {2341–2350},
numpages = {10},
keywords = {visual paragraph generation, reinforcement learning},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.1145/3461778.3462029,
author = {Law, Matthew V and Li, Zhilong and Rajesh, Amit and Dhawan, Nikhil and Kwatra, Amritansh and Hoffman, Guy},
title = {Hammers for Robots: Designing Tools for Reinforcement Learning Agents},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462029},
doi = {10.1145/3461778.3462029},
abstract = {In this paper we explore what role humans might play in designing tools for reinforcement learning (RL) agents to interact with the world. Recent work has explored RL methods that optimize a robot’s morphology while learning to control it, effectively dividing an RL agent’s environment into the external world and the agent’s interface with the world. Taking a user-centered design (UCD) approach, we explore the potential of a human, instead of an algorithm, redesigning the agent’s tool. Using UCD to design for a machine learning agent brings up several research questions, including what it means to understand an RL agent’s experience, beliefs, tendencies, and goals. After discussing these questions, we then present a system we developed to study humans designing a 2D racecar for an RL autonomous driver. We conclude with findings and insights from exploratory pilots with twelve users using this system.},
booktitle = {Proceedings of the 2021 ACM Designing Interactive Systems Conference},
pages = {1638–1653},
numpages = {16},
keywords = {user-centered design, reinforcement learning, human-agent interaction},
location = {Virtual Event, USA},
series = {DIS '21}
}

@inproceedings{10.1145/3512290.3528844,
author = {Turner, Matthew J. and Hemberg, Erik and O'Reilly, Una-May},
title = {Analyzing Multi-Agent Reinforcement Learning and Coevolution in Cybersecurity},
year = {2022},
isbn = {9781450392372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512290.3528844},
doi = {10.1145/3512290.3528844},
abstract = {Cybersecurity simulations can offer deep insights into the behavior of agents in the battle to secure computer systems. We build on existing work modeling the competition between an attacker and defender on a network architecture in a zero-sum game using a graph database linking cybersecurity attack patterns, vulnerabilities, and software. We apply coevolution to this challenging environment, and in a novel modeling approach for this problem, interpret each population as a distribution over fixed strategies to form a mixed strategy Nash equilibrium. We compare the results to solutions generated by multi-agent reinforcement learning and show that evolutionary methods demonstrate a considerable degree of robustness to parameter misspecification in this environment.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1290–1298},
numpages = {9},
keywords = {cybersecurity, coevolution, nash equilibrium, evolutionary algorithms, machine learning},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.1145/3565472.3595611,
author = {Kraus, Matthias and Wagner, Nicolas and Riekenbrauck, Ron and Minker, Wolfgang},
title = {Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning},
year = {2023},
isbn = {9781450399326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565472.3595611},
doi = {10.1145/3565472.3595611},
abstract = {The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive. Well-defined proactive behavior may improve human-machine cooperation, as the agent takes a more active role during interaction and takes off responsibility from the user. However, proactivity is a double-edged sword because poorly executed pre-emptive actions may have a devastating effect on the task outcome and the relationship with the user. For designing adequate proactive dialog strategies, we propose a novel approach including both social and task-relevant features in the dialog. Here, the primary goal is to optimize proactive behavior so that it is task-oriented - this implies high task success and efficiency - while also being socially effective by fostering user trust. Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for more successful human-machine cooperation.},
booktitle = {Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization},
pages = {146–155},
numpages = {10},
keywords = {reinforcement learning, human-computer interaction, trust, proactive conversational AI, dialog system},
location = {Limassol, Cyprus},
series = {UMAP '23}
}

@inproceedings{10.1145/3377929.3389901,
author = {Schmidgall, Samuel},
title = {Adaptive Reinforcement Learning through Evolving Self-Modifying Neural Networks},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3389901},
doi = {10.1145/3377929.3389901},
abstract = {The adaptive learning capabilities seen in biological neural networks are largely a product of the self-modifying behavior emerging from online plastic changes in synaptic connectivity. Current methods in Reinforcement Learning (RL) only adjust to new interactions after reflection over a specified time interval, preventing the emergence of online adaptivity. Recent work addressing this by endowing artificial neural networks with neuromodulated plasticity have been shown to improve performance on simple RL tasks trained using backpropagation, but have yet to scale up to larger problems. Here we study the problem of meta-learning in a challenging quadruped domain, where each leg of the quadruped has a chance of becoming unusable, requiring the agent to adapt by continuing locomotion with the remaining limbs. Results demonstrate that agents evolved using self-modifying plastic networks are more capable of adapting to complex meta-learning learning tasks, even outperforming the same network updated using gradient-based algorithms while taking less time to train.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {89–90},
numpages = {2},
keywords = {reinforcement learning, adaptive, self-modifying, meta-learning},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3293353.3293400,
author = {Prasad, Vignesh and Yadav, Karmesh and Saurabh, Rohitashva Singh and Daga, Swapnil and Pareekutty, Nahas and Krishna, K. Madhava and Ravindran, Balaraman and Bhowmick, Brojeshwar},
title = {Learning to Prevent Monocular SLAM Failure Using Reinforcement Learning},
year = {2020},
isbn = {9781450366151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293353.3293400},
doi = {10.1145/3293353.3293400},
abstract = {Monocular SLAM refers to using a single camera to estimate robot ego motion while building a map of the environment. While Monocular SLAM is a well studied problem, automating Monocular SLAM by integrating it with trajectory planning frameworks is particularly challenging. This paper presents a novel formulation based on Reinforcement Learning (RL) that generates fail safe trajectories wherein the SLAM generated outputs do not deviate largely from their true values. Quintessentially, the RL framework successfully learns the otherwise complex relation between perceptual inputs and motor actions and uses this knowledge to generate trajectories that do not cause failure of SLAM. We show systematically in simulations how the quality of the SLAM dramatically improves when trajectories are computed using RL. Our method scales effectively across Monocular SLAM frameworks in both simulation and in real world experiments with a mobile robot.},
booktitle = {Proceedings of the 11th Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {47},
numpages = {9},
keywords = {Robot Navigation, Reinforcement Learning, Monocular SLAM},
location = {Hyderabad, India},
series = {ICVGIP '18}
}

@article{10.1145/2668130,
author = {Kraemer, Landon and Banerjee, Bikramjit},
title = {Reinforcement Learning of Informed Initial Policies for Decentralized Planning},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/2668130},
doi = {10.1145/2668130},
abstract = {Decentralized partially observable Markov decision processes (Dec-POMDPs) offer a formal model for planning in cooperative multiagent systems where agents operate with noisy sensors and actuators, as well as local information. Prevalent solution techniques are centralized and model based—limitations that we address by distributed reinforcement learning (RL). We particularly favor alternate learning, where agents alternately learn best responses to each other, which appears to outperform concurrent RL. However, alternate learning requires an initial policy. We propose two principled approaches to generating informed initial policies: a naive approach that lays the foundation for a more sophisticated approach. We empirically demonstrate that the refined approach produces near-optimal solutions in many challenging benchmark settings, staking a claim to being an efficient (and realistic) approximate solver in its own right. Furthermore, alternate best response learning seeded with such policies quickly learns high-quality policies as well.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {dec},
articleno = {18},
numpages = {32},
keywords = {multiagent reinforcement learning, Decentralized partially observable Markov decision processes}
}

@inproceedings{10.1145/1389095.1389100,
author = {Dries, Erik J. and Peterson, Gilbert L.},
title = {Scaling Ant Colony Optimization with Hierarchical Reinforcement Learning Partitioning},
year = {2008},
isbn = {9781605581309},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1389095.1389100},
doi = {10.1145/1389095.1389100},
abstract = {This paper merges hierarchical reinforcement learning (HRL) with ant colony optimization (ACO) to produce a HRL ACO algorithm capable of generating solutions for large domains. This paper describes two specific implementations of the new algorithm: the first a modification to Dietterich's MAXQ-Q HRL algorithm, the second a hierarchical ant colony system algorithm. These implementations generate faster results, with little to no significant change in the quality of solutions for the tested problem domains. The application of ACO to the MAXQ-Q algorithm replaces the reinforcement learning, Q-learning, with the modified ant colony optimization method, Ant-Q. This algorithm, MAXQ-AntQ, converges to solutions not significantly different from MAXQ-Q in 88\% of the time. This paper then transfers HRL techniques to the ACO domain and traveling salesman problem (TSP). To apply HRL to ACO, a hierarchy must be created for the TSP. A data clustering algorithm creates these subtasks, with an ACO algorithm to solve the individual and complete problems. This paper tests two clustering algorithms, k-means and G-means. The results demonstrate the algorithm with data clustering produces solutions 20 times faster with 5-10\% decrease in solution quality due to the effects of clustering.},
booktitle = {Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation},
pages = {25–32},
numpages = {8},
keywords = {swarm intelligence, ant colony optimization, hierarchical reinforcement learning},
location = {Atlanta, GA, USA},
series = {GECCO '08}
}

@article{10.5555/2503308.2503360,
author = {Lang, Tobias and Toussaint, Marc and Kersting, Kristian},
title = {Exploration in Relational Domains for Model-Based Reinforcement Learning},
year = {2012},
issue_date = {January 2012},
publisher = {JMLR.org},
volume = {13},
number = {1},
issn = {1532-4435},
abstract = {A fundamental problem in reinforcement learning is balancing exploration and exploitation. We address this problem in the context of model-based reinforcement learning in large stochastic relational domains by developing relational extensions of the concepts of the E3 and R-MAX algorithms. Efficient exploration in exponentially large state spaces needs to exploit the generalization of the learned model: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be a well-known context in which exploitation is promising. To address this we introduce relational count functions which generalize the classical notion of state and action visitation counts. We provide guarantees on the exploration efficiency of our framework using count functions under the assumption that we had a relational KWIK learner and a near-optimal planner. We propose a concrete exploration algorithm which integrates a practically efficient probabilistic rule learner and a relational planner (for which there are no guarantees, however) and employs the contexts of learned relational rules as features to model the novelty of states and actions. Our results in noisy 3D simulated robot manipulation problems and in domains of the international planning competition demonstrate that our approach is more effective than existing propositional and factored exploration techniques.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {3725–3768},
numpages = {44},
keywords = {relational transition models, robotics, reinforcement learning, statistical relational learning, exploration}
}

@article{10.1613/jair.1.12372,
author = {Furelos-Blanco, Daniel and Law, Mark and Jonsson, Anders and Broda, Krysia and Russo, Alessandra},
title = {Induction and Exploitation of Subgoal Automata for Reinforcement Learning},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12372},
doi = {10.1613/jair.1.12372},
abstract = {In this paper we present ISA, an approach for learning and exploiting subgoals in episodic reinforcement learning (RL) tasks. ISA interleaves reinforcement learning with the induction of a subgoal automaton, an automaton whose edges are labeled by the task’s subgoals expressed as propositional logic formulas over a set of high-level events. A subgoal automaton also consists of two special states: a state indicating the successful completion of the task, and a state indicating that the task has finished without succeeding. A state-of-the-art inductive logic programming system is used to learn a subgoal automaton that covers the traces of high-level events observed by the RL agent. When the currently exploited automaton does not correctly recognize a trace, the automaton learner induces a new automaton that covers that trace. The interleaving process guarantees the induction of automata with the minimum number of states, and applies a symmetry breaking mechanism to shrink the search space whilst remaining complete. We evaluate ISA in several gridworld and continuous state space problems using different RL algorithms that leverage the automaton structures. We provide an in-depth empirical analysis of the automaton learning performance in terms of the traces, the symmetry breaking and specific restrictions imposed on the final learnable automaton. For each class of RL problem, we show that the learned automata can be successfully exploited to learn policies that reach the goal, achieving an average reward comparable to the case where automata are not learned but handcrafted and given beforehand.},
journal = {J. Artif. Int. Res.},
month = {may},
pages = {1031–1116},
numpages = {86}
}

@article{10.5555/3291125.3309631,
author = {\v{S}o\v{s}i\'{c}, Adrian and Zoubir, Abdelhak M. and Rueckert, Elmar and Peters, Jan and Koeppl, Heinz},
title = {Inverse Reinforcement Learning via Nonparametric Spatio-Temporal Subgoal Modeling},
year = {2018},
issue_date = {January 2018},
publisher = {JMLR.org},
volume = {19},
number = {1},
issn = {1532-4435},
abstract = {Advances in the field of inverse reinforcement learning (IRL) have led to sophisticated inference frameworks that relax the original modeling assumption of observing an agent behavior that reflects only a single intention. Instead of learning a global behavioral model, recent IRL methods divide the demonstration data into parts, to account for the fact that different trajectories may correspond to different intentions, e.g., because they were generated by different domain experts. In this work, we go one step further: using the intuitive concept of subgoals, we build upon the premise that even a single trajectory can be explained more efficiently locally within a certain context than globally, enabling a more compact representation of the observed behavior. Based on this assumption, we build an implicit intentional model of the agent's goals to forecast its behavior in unobserved situations. The result is an integrated Bayesian prediction framework that significantly outperforms existing IRL solutions and provides smooth policy estimates consistent with the expert's plan. Most notably, our framework naturally handles situations where the intentions of the agent change over time and classical IRL algorithms fail. In addition, due to its probabilistic nature, the model can be straightforwardly applied in active learning scenarios to guide the demonstration process of the expert.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {2777–2821},
numpages = {45},
keywords = {Bayesian nonparametric modeling, graphical models, gibbs sampling, inverse reinforcement learning, learning from demonstration, subgoal inference}
}

@inproceedings{10.1145/3442381.3449893,
author = {Wang, Zhenduo and Ai, Qingyao},
title = {Controlling the Risk of Conversational Search via Reinforcement Learning},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449893},
doi = {10.1145/3442381.3449893},
abstract = {Users often formulate their search queries and questions with immature language without well-developed keywords and complete structures. Such queries are likely to fail to express their true information needs and raise ambiguity as fragmental language often yield various interpretations and aspects. This gives search engines a hard time processing and understanding the query, and eventually leads to unsatisfactory retrieval results. An alternative approach to direct answer while facing an ambiguous query is to proactively ask clarifying questions to the user. Recent years have seen many works and shared tasks from both NLP and IR community about identifying the need for asking clarifying question and methodology to generate them. An often neglected fact by these works is that although sometimes the need for clarifying questions is correctly recognized, the clarifying questions these system generate are still off-topic and dissatisfaction provoking to users and may just cause users to leave the conversation. In this work, we propose a risk-aware conversational search agent model to balance the risk of answering user’s query and asking clarifying questions. The agent is fully aware that asking clarifying questions can potentially collect more information from user, but it will compare all the choices it has and evaluate the risks. Only after then, it will make decision between answering or asking. To demonstrate that our system is able to retrieve better answers, we conduct experiments on the MSDialog dataset which contains real-world customer service conversations from Microsoft products community. We also purpose a reinforcement learning strategy which allows us to train our model on the original dataset directly and saves us from any further data annotation efforts. Our experiment results show that our risk-aware conversational search agent is able to significantly outperform strong non-risk-aware baselines.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {1968–1977},
numpages = {10},
keywords = {reinforcement learning, conversational search},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3427773.3427862,
author = {Christensen, Morten Herget and Ernewein, C\'{e}dric and Pinson, Pierre},
title = {Demand Response through Price-Setting Multi-Agent Reinforcement Learning},
year = {2020},
isbn = {9781450381932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427773.3427862},
doi = {10.1145/3427773.3427862},
abstract = {Price based demand response is a cost-effective way of obtaining flexibility needed in power systems with high penetration of intermittent renewable energy sources. Model-free deep reinforcement learning is proposed as a way to train autonomous agents for enabling buildings to participate in demand response programs as well as coordinating such programs though price setting in a multiagent setup. First, we show price responsive control of buildings with electric heat pumps using deep deterministic policy gradient. Then a coordinating agent is trained to manage a population of buildings by adjusting the price in order to keep the total load from exceeding the available capacity considering also the non-flexible base load.},
booktitle = {Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings \&amp; Cities},
pages = {1–5},
numpages = {5},
keywords = {demand response, neural networks, deep reinforcement learning, multi-agent systems},
location = {Virtual Event, Japan},
series = {RLEM'20}
}

@inproceedings{10.1145/3302509.3311053,
author = {Gao, Qitong and Hajinezhad, Davood and Zhang, Yan and Kantaros, Yiannis and Zavlanos, Michael M.},
title = {Reduced Variance Deep Reinforcement Learning with Temporal Logic Specifications},
year = {2019},
isbn = {9781450362856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302509.3311053},
doi = {10.1145/3302509.3311053},
abstract = {In this paper, we propose a model-free reinforcement learning method to synthesize control policies for mobile robots modeled as Markov Decision Process (MDP) with unknown transition probabilities that satisfy Linear Temporal Logic (LTL) specifications. Specifically, we develop a reduced variance deep Q-Learning technique that relies on Neural Networks (NN) to approximate the state-action values of the MDP and employs a reward function that depends on the accepting condition of the Deterministic Rabin Automaton (DRA) that captures the LTL specification. The key idea is to convert the deep Q-Learning problem into a nonconvex max-min optimization problem with a finite-sum structure, and develop an Arrow-Hurwicz-Uzawa type stochastic reduced variance algorithm with constant stepsize to solve it. Unlike Stochastic Gradient Descent (SGD) methods that are often used in deep reinforcement learning, our method can estimate the gradients of an unknown loss function more accurately and can improve the stability of the training process. Moreover, our method does not require learning the transition probabilities in the MDP, constructing a product MDP, or computing Accepting Maximal End Components (AMECs). This allows the robot to learn an optimal policy even if the environment cannot be modeled accurately or if AMECs do not exist. In the latter case, the resulting control policies minimize the frequency with which the system enters bad states in the DRA that violate the task specifications. To the best of our knowledge, this is the first model-free deep reinforcement learning algorithm that can synthesize policies that maximize the probability of satisfying an LTL specification even if AMECs do not exist. Rigorous convergence analysis and rate of convergence are provided for the proposed algorithm as well as numerical experiments that validate our method.},
booktitle = {Proceedings of the 10th ACM/IEEE International Conference on Cyber-Physical Systems},
pages = {237–248},
numpages = {12},
keywords = {reinforcement learning, linear temporal logic, reduced variance stochastic optimization},
location = {Montreal, Quebec, Canada},
series = {ICCPS '19}
}

@inproceedings{10.1145/3343031.3350935,
author = {Yu, Tong and Shen, Yilin and Zhang, Ruiyi and Zeng, Xiangyu and Jin, Hongxia},
title = {Vision-Language Recommendation via Attribute Augmented Multimodal Reinforcement Learning},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350935},
doi = {10.1145/3343031.3350935},
abstract = {Interactive recommenders have demonstrated the advantage over traditional recommenders with dynamic change of items. However, the traditional user feedback in the format of clicks or ratings, provides limited user preference information and limited history tracking capabilities. As a result, it takes a user many interactions to find a desired item. Data of other modalities, such as item visual appearance and user comments in natural language, may enable richer user feedback. However, there are several critical challenges to be addressed when utilizing these multimodal data: multimodal matching, user preference tracking, and adaptation to dynamic unseen items. Without properly handling these challenges, the recommendations can easily violate the users' preference from their past natural language feedback. In this paper, we introduce a novel approach, called vision-language recommendation, that enables users to provide natural language feedback on visual products to have more natural and effective interactions. To model more explicit and accurate multimodal matching, we propose a novel visual attribute augmented reinforcement learning approach that enhances the grounding of natural language to visual items. Furthermore, to effectively track the users' preference and overcome the performance deficiency on dynamic unseen items after deployment, we propose a novel history multimodal matching reward to continuously adapt the model on-the-fly. Empirical results show that, our system augmented by visual attribute and history multimodal matching can significantly increase the success rate, reduce the number of recommendations that violate the user's previous feedback, and need less number of user interactions to find the desired items.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {39–47},
numpages = {9},
keywords = {reinforcement learning, vision and language, interactive recommender system, multimodal},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.1145/2663474.2663481,
author = {Zhu, Minghui and Hu, Zhisheng and Liu, Peng},
title = {Reinforcement Learning Algorithms for Adaptive Cyber Defense against Heartbleed},
year = {2014},
isbn = {9781450331500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663474.2663481},
doi = {10.1145/2663474.2663481},
abstract = {In this paper, we investigate a model where a defender and an attacker simultaneously and repeatedly adjust the defenses and attacks. Under this model, we propose two iterative reinforcement learning algorithms which allow the defender to identify optimal defenses when the information about the attacker is limited. With probability one, the adaptive reinforcement learning algorithm converges to the best response with respect to the attacks when the attacker diminishingly explores the system. With a probability arbitrarily close to one, the robust reinforcement learning algorithm converges to the min-max strategy despite that the attacker persistently explores the system. The algorithm convergence is formally proven and the algorithm performance is verified via numerical simulations.},
booktitle = {Proceedings of the First ACM Workshop on Moving Target Defense},
pages = {51–58},
numpages = {8},
keywords = {security, algorithms},
location = {Scottsdale, Arizona, USA},
series = {MTD '14}
}

