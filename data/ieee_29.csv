"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"OmniDRL: Robust Pedestrian Detection using Deep Reinforcement Learning on Omnidirectional Cameras","G. Dias Pais; T. J. Dias; J. C. Nascimento; P. Miraldo","Institute for Systems and Robotics (LARSyS), Institute Superior Tecnico, Universidade de Lisboa, Portugal; Institute for Systems and Robotics (LARSyS), Institute Superior Tecnico, Universidade de Lisboa, Portugal; Institute for Systems and Robotics (LARSyS), Institute Superior Tecnico, Universidade de Lisboa, Portugal; Division of Decision and Control Systems, KTH Royal Institute of Technology, Stockholm, Sweden","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","4782","4789","Pedestrian detection is one of the most explored topics in computer vision and robotics. The use of deep learning methods allowed the development of new and highly competitive algorithms. Deep Reinforcement Learning has proved to be within the state-of-the-art in terms of both detection in perspective cameras and robotics applications. However, for detection in omnidirectional cameras, the literature is still scarce, mostly because of their high levels of distortion. This paper presents a novel and efficient technique for robust pedestrian detection in omnidirectional images. The proposed method uses deep Reinforcement Learning that takes advantage of the distortion in the image. By considering the 3D bounding boxes and their distorted projections into the image, our method is able to provide the pedestrian’s position in the world, in contrast to the image positions provided by most state-of-the-art methods for perspective cameras. Our method avoids the need of pre-processing steps to remove the distortion, which is computationally expensive. Beyond the novel solution, our method compares favorably with the state-of-the-art methodologies that do not consider the underlying distortion for the detection task.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794471","","Cameras;Three-dimensional displays;Distortion;Image segmentation;Robot vision systems;Object detection","cameras;computerised instrumentation;image processing;learning (artificial intelligence)","robust pedestrian detection;omnidirectional cameras;computer vision;robotics;deep learning methods;omnidirectional imaging;OmniDRL;deep reinforcement learning;3D bounding boxes","","3","","51","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"A reinforcement learning approach to lift generation in flapping MAVs: simulation results","M. Motamed; J. Yan","Electrical and Computer Engineering Department, University of British Columbia, Vancouver, BC, Canada; Electrical and Computer Engineering Department, University of British Columbia, Vancouver, BC, Canada","Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.","26 Jun 2006","2006","","","2150","2155","Flapping micro aerial vehicles are interesting in applications where maneuverability is needed in confined spaces. Yet aerodynamics of insect flapping flight is not completely known and the research in this area lacks a suitable aerodynamic model that can be used for control purposes. Reinforcement learning approach is proposed which is inspired from ""how"" the learning is achieved in real insects in nature. The reinforcement learning controller has been simulated using a quasi-steady aerodynamic model and shown to converge to a flapping motion. The learning capacity and advantages of this approach are also discussed","1050-4729","0-7803-9505-0","10.1109/ROBOT.2006.1642022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1642022","","Learning;Aerodynamics;Insects;Aerospace engineering;Application software;Aerospace control;Computational modeling;Computer simulation;Space vehicles;Force measurement","aerodynamics;aerospace control;learning (artificial intelligence)","lift generation;reinforcement learning approach;flapping micro aerial vehicles;quasisteady aerodynamic model","","3","","26","IEEE","26 Jun 2006","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Motion Control Strategy of a Multi-rotor UAV for Payload Transportation with Minimum Swing","F. Panetsos; G. C. Karras; K. J. Kyriakopoulos","School of Mechanical Engineering, Control Systems Lab, National Technical University of Athens, Athens, Greece; Dept. of Computer Science and Telecommunications, University of Thessaly, Lamia, Greece; School of Mechanical Engineering, Control Systems Lab, National Technical University of Athens, Athens, Greece","2022 30th Mediterranean Conference on Control and Automation (MED)","1 Aug 2022","2022","","","368","374","This paper addresses the problem of controlling a multirotor UAV with a cable-suspended load. In order to ensure the safe transportation of the load, the swinging motion, induced by the strongly coupled dynamics, has to be minimized. Specifically, using the Twin Delayed Deep Deterministic Policy Gradient (TD3) Reinforcement Learning algorithm, a policy Neural Network is trained in a model-free manner which navigates the vehicle to the desired waypoints while, simultaneously, compensating for the load oscillations. The learned policy network is incorporated into the cascaded control architecture of the autopilot by replacing the common PID position controller and, thus, communicating directly with the inner attitude one. The performance of the proposed policy is demonstrated through a comparative simulation and experimental study while using an octorotor UAV.","2473-3504","978-1-6654-0673-4","10.1109/MED54222.2022.9837220","PATH; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837220","","Navigation;Attitude control;Transportation;Reinforcement learning;Water pollution;Autopilot;Sensors","attitude control;autonomous aerial vehicles;cascade control;compensation;control system synthesis;helicopters;learning (artificial intelligence);mobile robots;motion control;position control;robot dynamics;three-term control","Deep Reinforcement;motion control strategy;multirotor UAV;payload transportation;minimum swing;cable-suspended load;safe transportation;swinging motion;strongly coupled dynamics;Twin Delayed Deep Deterministic Policy Gradient Reinforcement Learning algorithm;policy Neural Network;model-free manner;load oscillations;learned policy network;cascaded control architecture;common PID position controller;octorotor UAV","","3","","28","IEEE","1 Aug 2022","","","IEEE","IEEE Conferences"
"FRL-FI: Transient Fault Analysis for Federated Reinforcement Learning-Based Navigation Systems","Z. Wan; A. Anwar; A. Mahmoud; T. Jia; Y. -S. Hsiao; V. J. Reddi; A. Raychowdhury",Georgia Institute of Technology; Georgia Institute of Technology; Harvard University; Carnegie Mellon University; Harvard University; Harvard University; Georgia Institute of Technology,"2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)","19 May 2022","2022","","","430","435","Swarm intelligence is being increasingly deployed in autonomous systems, such as drones and unmanned vehicles. Federated reinforcement learning (FRL), a key swarm intelligence paradigm where agents interact with their own environments and cooperatively learn a consensus policy while preserving privacy, has recently shown potential advantages and gained popularity. However, transient faults are increasing in the hardware system with continuous technology node scaling and can pose threats to FRL systems. Meanwhile, conventional redundancy-based protection methods are challenging to deploy on resource-constrained edge applications. In this paper, we experimentally evaluate the fault tolerance of FRL navigation systems at various scales with respect to fault models, fault locations, learning algorithms, layer types, communication intervals, and data types at both training and inference stages. We further propose two cost-effective fault detection and recovery techniques that can achieve up to $3.3\times$ improvement in resilience with $<2.7\%$ overhead in FRL systems.","1558-1101","978-3-9819263-6-1","10.23919/DATE54114.2022.9774562","Semiconductor Research Corporation (SRC) program; DARPA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774562","","Training;Privacy;Navigation;Fault detection;Reinforcement learning;Inference algorithms;Servers","fault diagnosis;fault location;fault tolerant computing;learning (artificial intelligence);mobile robots;redundancy;remotely operated vehicles","fault tolerance;resource-constrained edge applications;continuous technology node scaling;hardware system;transient faults;consensus policy;key swarm intelligence paradigm;unmanned vehicles;autonomous systems;federated reinforcement learning-based navigation systems;transient fault analysis;FRL-FI;recovery techniques;cost-effective fault detection;fault locations;fault models;FRL navigation systems","","3","","19","","19 May 2022","","","IEEE","IEEE Conferences"
"Trajectory-Tracking Control of Robotic Systems via Deep Reinforcement Learning","S. Zhang; C. Sun; Z. Feng; G. Hu","School of Electrical & Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical & Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical & Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical & Electronic Engineering, Nanyang Technological University, Singapore","2019 IEEE International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM)","19 May 2020","2019","","","386","391","This paper studies the trajectory tracking problems for a robotic manipulator and a mobile robot by using deep reinforcement learning based methods. A medel-free deep reinforcement learning method based on Deep Deter-ministic Policy Gradient (DDPG) is designed for training. The priority replay memory is adopted to sample more significant transitions at each update. A distributed framework with multiple workers is proposed. Synchronous workers generate transitions and compute gradients for the global network, and collecting workers explore the environment with different policies and exploration noises. During the training, we adopt random reference state initialization to solve the exploration problem, which can make the robots learn from the reference trajectory effectively. Numerical simulations are provided to demonstrate the effectiveness and efficiency of the proposed methods. It can be seen from the simulation results that the agent trained by the proposed distributed DDPG could learn faster and achieve smaller tracking errors than DDPG.","2326-8239","978-1-7281-3458-1","10.1109/CIS-RAM47153.2019.9095802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9095802","Trajectory tracking;DDPG;distributed framework","Robot kinematics;Mobile robots;Machine learning;Training;Task analysis;Trajectory tracking","control engineering computing;distributed control;learning (artificial intelligence);manipulators;mobile robots;neural nets;random processes;trajectory control","random reference state initialization;exploration problem;reference trajectory;distributed DDPG;trajectory-tracking control;robotic systems;trajectory tracking problems;robotic manipulator;mobile robot;medel-free deep reinforcement learning method;deep deterministic policy gradient;priority replay memory;distributed framework;synchronous workers","","3","","16","IEEE","19 May 2020","","","IEEE","IEEE Conferences"
"AR3n: A Reinforcement Learning-Based Assist-as-Needed Controller for Robotic Rehabilitation","S. Pareek; H. J. Nisar; T. Kesavadas","Data Science Team at Cargill, Minneapolis, USA; Department of Industrial and Enterprise Systems Engineering, University of Illinois, Urbana-Champaign, Urbana, USA; Research and Economic Development Executive Council, State University of New York, Albany, Albany, USA","IEEE Robotics & Automation Magazine","","2023","PP","99","2","10","In this article, we present AR3n (pronounced as Aaron), an assist-as-needed (AAN) controller that utilizes reinforcement learning (RL), to supply adaptive assistance during a robot-assisted handwriting rehabilitation task. Unlike previous AAN controllers, our method does not rely on patient-specific controller parameters or physical models. We propose the use of a virtual patient model to generalize AR3n across multiple subjects. The system modulates robotic assistance in real time based on a subject’s tracking errors while minimizing the amount of robotic assistance. The controller is experimentally validated through a set of simulations and human subject experiments. Finally, a comparative study with a traditional rule-based controller is conducted to analyze differences in the assistance mechanisms of the two controllers.","1558-223X","","10.1109/MRA.2023.3282434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10158787","","Robots;Task analysis;Force;Behavioral sciences;Training;Adaptation models;Impedance","","","","3","","","IEEE","21 Jun 2023","","","IEEE","IEEE Early Access Articles"
"HMC: A Hybrid Reinforcement Learning Based Model Compression for Healthcare Applications","R. Soni; J. Guan; G. Avinash; V. R. Saripalli","GE Healthcare, San Ramon, CA, USA; GE Healthcare, San Ramon, CA, USA; GE Healthcare, San Ramon, CA, USA; GE Healthcare, San Ramon, CA, USA","2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)","19 Sep 2019","2019","","","146","151","Artificial intelligence (AI) healthcare applications to optimize workflows, reduce costs while focusing on patient care are on the rise. While deeper and wider neural networks are designed for complex healthcare applications, model compression is poised to be an effective way to deploy networks on medical devices that often have hardware and speed constraints. Most state-of-the-art model compression techniques require a resource centric manual process that explores a large space to find a trade-off solution between model size and accuracy. Recently, reinforcement learning (RL) approaches are proposed to automate such hand-crafted process. However, most RL model compression algorithms are model-free, meaning a very long time to train such RL agents due to the huge state space. In this work, we develop a hybridRL model compression (HMC) method that integrates model-based and model-free RL approaches. We demonstrate our method on a wide range of imaging data on healthcare related model architectures. Compared to model-free RL approaches, our results show that HMC method reduces the training time significantly, exhibits better generalization capabilities across different data sets, and preserves comparable model compression performance.","2161-8089","978-1-7281-0356-3","10.1109/COASE.2019.8843047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8843047","","Training;Medical services;Reinforcement learning;Data models;Neural networks;Medical devices;Network architecture","data compression;generalisation (artificial intelligence);health care;learning (artificial intelligence);patient care","complex healthcare applications;speed constraints;resource centric manual process;reinforcement learning approaches;hand-crafted process;RL model compression algorithms;RL agents;huge state space;hybridRL model compression method;model-free RL approaches;healthcare related model architectures;HMC method;hybrid reinforcement;artificial intelligence healthcare applications;model compression performance;neural networks;generalization capabilities;imaging data","","3","","24","IEEE","19 Sep 2019","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Flocking Control of UAVs in Complex Environments","M. Salimi; P. Pasquier","Simon Fraser University (SFU), Surrey, Canada; Simon Fraser University (SFU), Surrey, Canada","2021 6th International Conference on Robotics and Automation Engineering (ICRAE)","4 Jan 2022","2021","","","344","352","Flocking formation of unmanned aerial vehicles (UAVs) is an open challenge due to kinematics complexity and uncertainties in complex environments. In this paper, the UAV flocking control problem is formulated as a partially observable Markov decision process (POMDP) and solved by deep reinforcing learning. In particular, we consider a leader-follower configuration, where consensus among all UAVs is used to train a shared control policy, and each UAV performs actions based on the local information it collects. In addition, to avoid collision among UAVs and guarantee flocking and navigation, a reward function is added with the global flocking maintenance, mutual reward, and a collision penalty. We adapt deep deterministic policy gradient (DDPG) with centralized training and decentralized execution to obtain the flocking control policy using actor-critic networks and a global state space matrix. The simulation results demonstrate that the trained optimal policy converges to flocking formation without parameter tuning and has good generalization ability for different UAVs.","","978-1-6654-0697-0","10.1109/ICRAE53653.2021.9657767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9657767","Formation Control;Reinforcing Learning;Swarm Control;Flocking;Deep Deterministic Policy Gradient;UAV Swarm","Training;Uncertainty;Navigation;Simulation;Process control;Reinforcement learning;Maintenance engineering","aerospace computing;autonomous aerial vehicles;collision avoidance;control engineering computing;decision theory;deep learning (artificial intelligence);intelligent robots;learning (artificial intelligence);Markov processes;mobile robots;reinforcement learning;robot kinematics;robot programming;uncertain systems","deep reinforcement;complex environments;flocking formation;unmanned aerial vehicles;kinematics complexity;uncertainties;UAV flocking control problem;partially observable Markov decision process;shared control policy;navigation;global flocking maintenance;deep deterministic policy gradient;trained optimal policy;leader-follower configuration;centralized training;decentralized execution;actor-critic networks;global state space matrix;collision avoidance;collion penalty","","3","","68","IEEE","4 Jan 2022","","","IEEE","IEEE Conferences"
"Multi-Modal Legged Locomotion Framework With Automated Residual Reinforcement Learning","C. Yu; A. Rosendo","School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China","IEEE Robotics and Automation Letters","3 Aug 2022","2022","7","4","10312","10319","While quadruped robots usually have good stability and load capacity, bipedal robots offer a higher level of flexibility / adaptability to different tasks and environments. A multi-modal legged robot can take the best of both worlds. In this paper, we propose a multi-modal locomotion framework that is composed of a hand-crafted transition motion and a learning-based bipedal controller—learnt by a novel algorithm called Automated Residual Reinforcement Learning. This framework aims to endow arbitrary quadruped robots with the ability to walk bipedally. In particular, we 1) design an additional supporting structure for a quadruped robot and a sequential multi-modal transition strategy; 2) propose a novel class of Reinforcement Learning algorithms for bipedal control and evaluate their performances in both simulation and the real world. Experimental results show that our proposed algorithms have the best performance in simulation and maintain a good performance in a real-world robot. Overall, our multi-modal robot could successfully switch between biped and quadruped, and walk in both modes.","2377-3766","","10.1109/LRA.2022.3191071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9830825","Evolutionary robotics;legged robots;multi-modal locomotion;reinforcement learning","Robots;Legged locomotion;Quadrupedal robots;Hip;Reinforcement learning;Humanoid robots;Knee","learning (artificial intelligence);legged locomotion;motion control;robot dynamics","automated residual reinforcement learning;arbitrary quadruped robots;quadruped robot;multimodal transition strategy;Reinforcement Learning algorithms;bipedal control;real-world robot;multimodal robot;multimodal legged locomotion framework;load capacity;bipedal robots;multimodal legged robot;multimodal locomotion framework;hand-crafted transition motion;learning-based bipedal controller-learnt","","3","","56","IEEE","15 Jul 2022","","","IEEE","IEEE Journals"
"Applying Asynchronous Deep Classification Networks and Gaming Reinforcement Learning-Based Motion Planners to Mobile Robots","G. Ryou; Y. Sim; S. H. Yeon; S. Seok","Seoul National University, Seoul, South Korea; Seoul National University, Seoul, South Korea; NAVER LABS Research Internship Program; NAVER LABS, Gyeonggi-do, South Korea","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","6268","6275","In this paper, we propose a new methodology to embed deep learning-based algorithms in both visual recognition and motion planning for general mobile robotic platforms. A framework for an asynchronous deep classification network is introduced to integrate heavy deep classification networks into a mobile robot with no loss of system bandwidth. Moreover, a gaming reinforcement learning-based motion planner, a novel and convenient embodiment of reinforcement learning, is introduced for simple implementation and high applicability. The proposed approaches are implemented and evaluated on a developed robot, TT2-bot. The evaluation was based on a mission devised for a qualitative evaluation of the general purposes and performances of a mobile robotic platform. The robot was required to recognize targets with a deep classifier and plan the path effectively using a deep motion planner. As a result, the robot verified that the proposed approaches successfully integrate deep learning technologies on the stand-alone mobile robot. The embedded neural networks for recognition and path planning were critical components for the robot.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460798","","Mobile robots;Learning (artificial intelligence);Machine learning;Neural networks;Sensors;Training","control engineering computing;game theory;learning (artificial intelligence);mobile robots;neural nets;path planning;pattern classification","gaming reinforcement learning-based motion planner;mobile robotic platform;deep classifier;asynchronous deep classification network;visual recognition;motion planning;deep learning-based algorithms;TT2-bot;embedded neural networks","","3","2","22","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"A reinforcement learning path planning approach for range-only underwater target localization with autonomous vehicles","I. Masmitja; M. Martin; K. Katija; S. Gomariz; J. Navarro","Bioinspiration Lab, Mbari, CA, USA; Knowledge Engineering and Machine Learning Group, Universitat Politècnica de Catalunya, Barcelona Tech., Barcelona, Spain; Bioinspiration Lab, Mbari, CA, USA; Electronics Department Universitat Politècnica de Catalunya, Sarti Research Group, Barcelona Tech., Barcelona, Spain; Csic, Institut de Ciències del Mar, Barcelona, Spain","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","675","682","Underwater target localization using range-only and single-beacon (ROSB) techniques with autonomous vehicles has been used recently to improve the limitations of more complex methods, such as long baseline and ultra-short baseline systems. Nonetheless, in ROSB target localization methods, the trajectory of the tracking vehicle near the localized target plays an important role in obtaining the best accuracy of the predicted target position. Here, we investigate a Reinforcement Learning (RL) approach to find the optimal path that an autonomous vehicle should follow in order to increase and optimize the overall accuracy of the predicted target localization, while reducing time and power consumption. To accomplish this objective, different experimental tests have been designed using state-of-the-art deep RL algorithms. Our study also compares the results obtained with the analytical Fisher information matrix approach used in previous studies. The results revealed that the policy learned by the RL agent outperforms trajectories based on these analytical solutions, e.g. the median predicted error at the beginning of the target’s localisation is 17% less. These findings suggest that using deep RL for localizing acoustic targets could be successfully applied to in-water applications that include tracking of acoustically tagged marine animals by autonomous underwater vehicles. This is envisioned as a first necessary step to validate the use of RL to tackle such problems, which could be used later on in a more complex scenarios.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926499","Ministerio de Economía y Competitividad; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926499","","Location awareness;Autonomous underwater vehicles;Target tracking;Power demand;Computer aided software engineering;Navigation;Reinforcement learning","autonomous underwater vehicles;mobile robots;navigation;optimisation;path planning;reinforcement learning","target position prediction;optimal path;deep RL algorithms;analytical Fisher information matrix approach;RL agent;acoustic targets;autonomous underwater vehicles;range-only underwater target localization;ultra-short baseline systems;ROSB target localization methods;tracking vehicle;reinforcement learning path planning approach;range-only and single-beacon techniques;power consumption;target localization prediction","","3","","41","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"A Robotic Auto-Focus System based on Deep Reinforcement Learning","X. Yu; R. Yu; J. Yang; X. Duan","Center of Wireless Communication and Signal Processing, Peking University, Beijing, China; Center of Wireless Communication and Signal Processing, Peking University, Beijing, China; Center of Wireless Communication and Signal Processing, Peking University, Beijing, China; Center of Wireless Communication and Signal Processing, Peking University, Beijing, China","2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)","20 Dec 2018","2018","","","204","209","Considering its advantages in dealing with high-dimensional visual input and learning control policies in discrete domain, Deep Q Network (DQN) could be an alternative method of traditional auto-focus means in the future. In this paper, based on Deep Reinforcement Learning, we propose an end-to-end approach that can learn auto-focus policies from visual input and finish at a clear spot automatically. We demonstrate that our method - discretizing the action space with coarse to fine steps and applying DQN is not only a solution to auto-focus but also a general approach towards vision-based control problems. Separate phases of training in virtual and real environments are applied to obtain an effective model. Virtual experiments, which are carried out after the virtual training phase, indicates that our method could achieve 100% accuracy on a certain view with different focus range. Further training on real robots could eliminate the deviation between the simulator and real scenario, leading to reliable performances in real applications.","","978-1-5386-9582-1","10.1109/ICARCV.2018.8581213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8581213","","Training;Microscopy;Aerospace electronics;Focusing;Manipulators;Optical microscopy","learning (artificial intelligence);robot vision","robotic auto-focus system;high-dimensional visual input;discrete domain;end-to-end approach;auto-focus policies;vision-based control problems;virtual training phase;DQN;deep reinforcement learning;focus range;deep Q network","","3","","21","IEEE","20 Dec 2018","","","IEEE","IEEE Conferences"
"Data Driven Control of Interacting Two Tank Hybrid System using Deep Reinforcement Learning","D. M. Jones; S. Kanagalakshmi","Dept. of Electrical Engineering, National Institute of Technology Calicut, Kerala, India; Dept. of Electrical Engineering, National Institute of Technology Calicut, Kerala, India","2021 IEEE 6th International Conference on Computing, Communication and Automation (ICCCA)","10 Jan 2022","2021","","","297","303","This paper investigates the use of a Deep Neural Network based Reinforcement Learning(RL) algorithm applied to a non-linear system for the design of a controller. It aims to augment the large amounts of data that we possess along with the already known dynamics of the non-linear hybrid tank system for effective control of the liquid level. Control systems represent a non-linear optimization problem, and Machine Learning helps to achieve non-linear optimization using large amounts of data. This document demonstrates the use of Deep Deterministic Policy Gradient (DDPG), an off-policy based actor-critic methodology of reinforcement learning, which is efficient in solving problems where states and actions lie in continuous spaces instead of discrete spaces. The test bench on which RL is being applied is a Multi-Input Multi-Output (MIMO) system called the Interacting Two Tank Hybrid System, with the aim of controlling the liquid levels in the two tanks. In Deep Reinforcement Learning, we are implementing the policy of the agent by means of deep neural networks. The idea behind using the neural network architectures for reinforcement learning is that we want reward signals obtained to strengthen the connection that leads to a good policy. Moreover, these deep neural networks are unique in their ability to represent complex functions if we give them ample amounts of data.","2642-7354","978-1-6654-1473-9","10.1109/ICCCA52192.2021.9666405","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9666405","reinforcement learning;deep neural network;actor-critic method;interacting water tank system;DDPG","Deep learning;Training;Liquids;Neural networks;Reinforcement learning;Aerospace electronics;Control systems","continuous systems;control system synthesis;deep learning (artificial intelligence);level control;MIMO systems;nonlinear programming;reinforcement learning","deep neural networks;neural network architectures;data driven control;deep neural network;nonlinear hybrid tank system;liquid level;nonlinear optimization problem;machine learning;deep deterministic policy gradient;off-policy based actor-critic methodology;multiinput multioutput system;deep reinforcement learning;interacting two tank hybrid system;reward signals","","3","","14","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"Multi-player H∞ Differential Game using On-Policy and Off-Policy Reinforcement Learning","P. An; M. Liu; Y. Wan; F. L. Lewis","Department of Electrical Engineering, University of Texas at Arlington, Arlington, TX, USA; Department of Electrical Engineering, University of Texas at Arlington, Arlington, TX, USA; Department of Electrical Engineering, University of Texas at Arlington, Arlington, TX, USA; UTA Research Institute, University of Texas at Arlington, Fort Worth, Texas, USA","2020 IEEE 16th International Conference on Control & Automation (ICCA)","30 Nov 2020","2020","","","1137","1142","This paper studies a multi-player H∞ differential game for systems of general linear dynamics. In this game, multiple players design their control inputs to minimize their cost functions in the presence of worst-case disturbances. We first derive the optimal control and disturbance policies using the solutions to Hamilton-Jacobi-Isaacs (HJI) equations. We then prove that the derived optimal policies stabilize the system and constitute a Nash equilibrium solution. Two integral reinforcement learning (IRL) -based algorithms, including the policy iteration IRL and off-policy IRL, are developed to solve the differential game online. We show that the off-policy IRL can solve the multi-player H∞ differential game online without using any system dynamics information. Simulation studies are conducted to validate the theoretical analysis and demonstrate the effectiveness of the developed learning algorithms.","1948-3457","978-1-7281-9093-8","10.1109/ICCA51439.2020.9264554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264554","","Games;Nash equilibrium;Mathematical model;Heuristic algorithms;System dynamics;Optimal control;Attenuation","differential games;iterative methods;learning (artificial intelligence);linear systems;minimisation;optimal control;partial differential equations;stability","learning algorithms;Nash equilibrium solution;system stability;on-policy reinforcement learning;multiplayer H∞ differential game;cost function minimization;system dynamics information;off-policy IRL;integral reinforcement learning -based algorithms;optimal policies;Hamilton-Jacobi-Isaacs equations;disturbance policies;optimal control;worst-case disturbances;general linear dynamics;off-policy reinforcement learning","","3","","22","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Optimal Control of a Two-Wheeled Self-Balancing Robot by Reinforcement Q-learning","L. Guo; S. A. A. Rizvi; Z. Lin","The Charles L. Brown Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA, USA; The Charles L. Brown Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA, USA; The Charles L. Brown Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA, USA","2020 IEEE 16th International Conference on Control & Automation (ICCA)","30 Nov 2020","2020","","","955","960","This paper concerns optimal control of the linear motion, tilt motion, and yaw motion of a two-wheeled self-balancing robot (TWSBR). Traditional optimal control methods for the TWSBR usually require a precise model of the system, and other control methods exist that achieve stabilization in the face of parameter uncertainties. In practical applications, it is often desirable to realize optimal control in the absence of the precise knowledge of the system parameters. This paper proposes to use a new feedback-based reinforcement Q-learning method to solve the linear quadratic regulation (LQR) control problem for the TWSBR. The proposed control scheme is completely online and does not require any knowledge of the system parameters. Both state feedback optimal control and output feedback optimal control are presented. Numerical simulation shows that the proposed optimal control scheme is capable of stabilizing the system and converging to the LQR solution obtained through solving the algebraic Riccati equation.","1948-3457","978-1-7281-9093-8","10.1109/ICCA51439.2020.9264485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264485","","Optimal control;Robots;Wheels;Mobile robots;Mathematical model;Output feedback;State feedback","control engineering computing;learning (artificial intelligence);linear quadratic control;mobile robots;motion control;Riccati equations;self-balancing vehicles;stability;state feedback;uncertain systems;wheels","two-wheeled self-balancing robot;linear motion;yaw motion;TWSBR;parameter uncertainties;system parameters;linear quadratic regulation control;state feedback;output feedback;optimal control;feedback-based reinforcement Q-learning;tilt motion;stabilization;LQR control;numerical simulation;algebraic Riccati equation","","3","","17","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Reinforcement learning-based motion planning of a triangular floating platform under environmental disturbances","K. Tziortziotis; K. Vlachos; K. Blekas","Department of Computer Science and Engineering, University of Ioannina, Ioannina, Greece; Department of Computer Science and Engineering, University of Ioannina, Ioannina, Greece; Department of Computer Science and Engineering, University of Ioannina, Ioannina, Greece","2016 24th Mediterranean Conference on Control and Automation (MED)","8 Aug 2016","2016","","","1014","1019","This paper investigates the use of reinforcement learning for the motion planing of an autonomous triangular marine platform in unknown environments under various environmental disturbances. The marine platform is over-actuated, i.e. it has more control inputs than degrees of freedom. The proposed approach uses an online least-squared policy iteration scheme for value function approximation in order to estimate optimal policy. We evaluate our approach in simulation, taking under consideration the dynamics of the platform, the dynamics and limitations of the actuators, under the presence of wind, and sea current disturbances. We report simulation results concerning its performance on estimating optimal navigation policies to unknown environments. Despite the model dynamics, the actuation dynamics and limitations, and the environmental disturbances, the presented results are promising.","","978-1-4673-8345-5","10.1109/MED.2016.7536000","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7536000","","Learning (artificial intelligence);Vehicle dynamics;Dynamics;Navigation;Marine vehicles;Mathematical model;Force","actuators;autonomous underwater vehicles;iterative methods;learning (artificial intelligence);least squares approximations;path planning","actuation dynamics;optimal navigation policy estimation;sea current disturbances;actuator limitations;optimal policy estimation;value function approximation;online least-squared policy iteration scheme;unknown environments;autonomous triangular marine platform;triangular floating platform;reinforcement learning-based motion planning","","3","","20","IEEE","8 Aug 2016","","","IEEE","IEEE Conferences"
"Inverse reinforcement learning with evaluation","V. Freire da Silva; A. H. Reali Costa; P. Lima","Laboratório de Técnicas Inteligentes-Escola Politécnica, Universidade de Sáo Paulo, Sao Paulo, Brazil; Laboratório de Técnicas Inteligentes-Escola Politécnica, Universidade de Sáo Paulo, Sao Paulo, Brazil; Institute for System and Robotics, Instituto Superior Técnico, Lisboa, Portugal","Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.","26 Jun 2006","2006","","","4246","4251","Reinforcement learning (RL) is a method that helps programming an autonomous agent through human-like objectives as reinforcements, where the agent is responsible for discovering the best actions to fulfil the objectives. Nevertheless, it is not easy to disentangle human objectives in reinforcement like objectives. Inverse reinforcement learning (IRL) determines the reinforcements that a given agent behaviour is fulfilling from the observation of the desired behaviour. In this paper we present a variant of IRL, which is called IRL with evaluation (IRLE) where instead of observing the desired agent behaviour, the relative evaluation between different behaviours is known by the access to an evaluator. We present also a solution for this problem under the assumption that a relative linear function that preserves the order assumed by the evaluator exists and that the evaluator evaluates policies instead of behaviours. This is posed as a linear feasibility problem, whose solution is well known. Results of simulations of a set of heterogeneous robots in a search and rescue scenario are presented to illustrate the method and the possibility to transfer the learned reinforcement function among robots","1050-4729","0-7803-9505-0","10.1109/ROBOT.2006.1642355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1642355","","Humans;Autonomous agents;Robot programming;Learning systems","intelligent robots;learning (artificial intelligence);mobile robots;telerobotics","inverse reinforcement learning;autonomous agent;linear feasibility problem;heterogeneous robots;learned reinforcement function","","3","","11","IEEE","26 Jun 2006","","","IEEE","IEEE Conferences"
"Energy Harvesting Aware Multi-Hop Routing Policy in Distributed IoT System Based on Multi-Agent Reinforcement Learning","W. Zhang; T. Liu; M. Xie; L. Li; D. Kar; C. Pan","Department of Computer Science, Texas A&M University-Corpus Christi, Corpus Christi, USA; Department of Math and Computer Science, Lawrence Technological University, Southfield, USA; Department of Computer Science, University of Texas at San Antonio, San Antonio, USA; Department of Computer Science, Texas A&M University-Corpus Christi, Corpus Christi, USA; Department of Computer Science, Texas A&M University-Corpus Christi, Corpus Christi, USA; Department of Computer Science, Texas A&M University-Corpus Christi, Corpus Christi, USA","2022 27th Asia and South Pacific Design Automation Conference (ASP-DAC)","21 Feb 2022","2022","","","562","567","Energy harvesting technologies offer a promising solution to sustainably power an ever-growing number of Internet of Things (IoT) devices. However, due to the weak and transient natures of energy harvesting, IoT devices have to work intermittently rendering conventional routing policies and energy allocation strategies impractical. To this end, this paper, for the very first time, developed a distributed multi-agent reinforcement algorithm known as global actor-critic policy (GAP) to address the problem of routing policy and energy allocation together for the energy harvesting powered IoT system. At the training stage, each IoT device is treated as an agent and one universal model is trained for all agents to save computing resources. At the inference stage, packet delivery rate can be maximized. The experimental results show that the proposed GAP algorithm achieves ~ 1.28× and ~ 1.24× data transmission rate than that of the Q-table and ESDSRAA algorithm, respectively.","2153-697X","978-1-6654-2135-5","10.1109/ASP-DAC52403.2022.9712528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712528","","Training;Spread spectrum communication;Routing;Rendering (computer graphics);Inference algorithms;Energy harvesting;Resource management","energy harvesting;Internet of Things;learning (artificial intelligence);multi-agent systems;telecommunication network routing;wireless sensor networks","multiagent reinforcement algorithm;global actor-critic policy;IoT device;energy harvesting aware multihop routing policy;distributed IoT system;multiagent reinforcement learning;energy harvesting technologies;Things devices;weak natures;transient natures;conventional routing policies;energy allocation strategies","","3","","15","IEEE","21 Feb 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Robotic Safe Control with Force Sensing","N. Lin; L. Zhang; Y. Chen; Y. Zhu; R. Chen; P. Wu; X. Chen","School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Information Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Information Science and Technology of China, Hefei, China; School of Information Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China","2019 WRC Symposium on Advanced Robotics and Automation (WRC SARA)","16 Dec 2019","2019","","","148","153","For the task with complicated manipulation in unstructured environments, traditional hand-coded methods are ineffective, while reinforcement learning can provide more general and useful policy. Although the reinforcement learning is able to obtain impressive results, its stability and reliability is hard to guarantee, which would cause the potential safety threats. Besides, the transfer from simulation to real-world also will lead in unpredictable situations. To enhance the safety and reliability of robots, we introduce the force and haptic perception into reinforcement learning. Force and tactual sensation play key roles in robotic dynamic control and human-robot interaction. We demonstrate that the force-based reinforcement learning method can be more adaptive to environment, especially in sim-to-real transfer. Experimental results show in object pushing task, our strategy is safer and more efficient in both simulation and real world, thus it holds prospects for a wide variety of robotic applications.","","978-1-7281-5552-4","10.1109/WRC-SARA.2019.8931917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8931917","","Reinforcement learning;Task analysis;Safety;Force;Tactile sensors","control engineering computing;force sensors;haptic interfaces;human-robot interaction;learning (artificial intelligence);manipulators;mobile robots;motion control;tactile sensors","robotic safe control;force sensing;robotic dynamic control;human-robot interaction;force-based reinforcement learning method;hand-coded methods;haptic perception;tactual sensation;force sensation","","3","","33","IEEE","16 Dec 2019","","","IEEE","IEEE Conferences"
"Sample Efficient Reinforcement Learning via Model-Ensemble Exploration and Exploitation","Y. Yao; L. Xiao; Z. An; W. Zhang; D. Luo","TBSI, Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; TBSI, Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; TBSI, Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; TBSI, Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Tencent AI Lab, Shenzhen, China","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","4202","4208","Model-based deep reinforcement learning has achieved success in various domains that require high sample efficiencies, such as Go and robotics. However, there are some remaining issues, such as planning efficient explorations to learn more accurate dynamic models, evaluating the uncertainty of the learned models, and more rational utilization of models. To mitigate these issues, we present MEEE, a model-ensemble method that consists of optimistic exploration and weighted exploitation. During exploration, unlike prior methods directly selecting the optimal action that maximizes the expected accumulative return, our agent first generates a set of action candidates and then seeks out the optimal action that takes both expected return and future observation novelty into account. During exploitation, different discounted weights are assigned to imagined transition tuples according to their model uncertainty respectively, which will prevent model predictive error propagation in agent training. Experiments on several challenging continuous control benchmark tasks demonstrated that our approach outperforms other model-free and model-based state-of-the-art methods, especially in sample complexity.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561842","","Training;Uncertainty;Conferences;Estimation;Reinforcement learning;Predictive models;Benchmark testing","deep learning (artificial intelligence);multi-agent systems;reinforcement learning","model-ensemble exploration;model-based deep reinforcement learning;model-ensemble method;optimistic exploration;weighted exploitation;expected accumulative return;expected return;model uncertainty;model predictive error propagation;sample complexity;model-ensemble exploitation","","3","","46","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Zero-Shot Reinforcement Learning on Graphs for Autonomous Exploration Under Uncertainty","F. Chen; P. Szenher; Y. Huang; J. Wang; T. Shan; S. Bai; B. Englot","Department of Mechanical Engineering, Stevens Institute of Technology, USA; Department of Mechanical Engineering, Stevens Institute of Technology, USA; Department of Mechanical Engineering, Stevens Institute of Technology, USA; Department of Mechanical Engineering, Stevens Institute of Technology, USA; Computer Science & Artificial Intelligence Laboratory, Massachusetts Institute of Technology, USA; Wing, Alphabet Inc.; Department of Mechanical Engineering, Stevens Institute of Technology, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","5193","5199","This paper studies the problem of autonomous exploration under localization uncertainty for a mobile robot with 3D range sensing. We present a framework for self-learning a high-performance exploration policy in a single simulation environment, and transferring it to other environments, which may be physical or virtual. Recent work in transfer learning achieves encouraging performance by domain adaptation and domain randomization to expose an agent to scenarios that fill the inherent gaps in sim2sim and sim2real approaches. However, it is inefficient to train an agent in environments with randomized conditions to learn the important features of its current state. An agent can use domain knowledge provided by human experts to learn efficiently. We propose a novel approach that uses graph neural networks in conjunction with deep reinforcement learning, enabling decision-making over graphs containing relevant exploration information provided by human experts to predict a robot's optimal sensing action in belief space. The policy, which is trained only in a single simulation environment, offers a real-time, scalable, and transferable decision-making strategy, resulting in zero-shot transfer to other simulation environments and even real-world environments.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561917","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561917","","Training;Uncertainty;Three-dimensional displays;Transfer learning;Decision making;Virtual environments;Reinforcement learning","decision making;graph theory;mobile robots;neural nets;reinforcement learning","autonomous exploration;localization uncertainty;mobile robot;3D range sensing;high-performance exploration policy;single simulation environment;transfer learning;domain adaptation;domain randomization;randomized conditions;domain knowledge;graph neural networks;deep reinforcement learning;exploration information;transferable decision-making strategy;zero-shot transfer;simulation environments;real-world environments;zero-shot reinforcement learning;sim2sim approach;sim2real approach","","3","","34","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"An Adaptive Fuzzy Reinforcement Learning Cooperative Approach for the Autonomous Control of Flock Systems","S. Qu; M. Abouheaf; W. Gueaieb; D. Spinello","School of Electrical Engineering & Computer Science, University of Ottawa, Ottawa, Canada; School of Electrical Engineering & Computer Science, University of Ottawa, Ottawa, Canada; School of Electrical Engineering & Computer Science, University of Ottawa, Ottawa, Canada; Department of Mechanical Engineering, University of Ottawa, Ottawa, Canada","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","8927","8933","The flock-guidance problem enjoys a challenging structure where multiple optimization objectives are solved simultaneously. This usually necessitates different control approaches to tackle various objectives, such as guidance, collision avoidance, and cohesion. The guidance schemes, in particular, have long suffered from complex tracking-error dynamics. Furthermore, techniques that are based on linear feedback strategies obtained at equilibrium conditions either may not hold or degrade when applied to uncertain dynamic environments. Pre-tuned fuzzy inference architectures lack robustness under such unmodeled conditions. This work introduces an adaptive distributed technique for the autonomous control of flock systems. Its relatively flexible structure is based on online fuzzy reinforcement learning schemes which simultaneously target a number of objectives; namely, following a leader, avoiding collision, and reaching a flock velocity consensus. In addition to its resilience in the face of dynamic disturbances, the algorithm does not require more than the agent position as a feedback signal. The effectiveness of the proposed method is validated with two simulation scenarios and benchmarked against a similar technique from the literature.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561204","","Fuzzy logic;Adaptive systems;Heuristic algorithms;Reinforcement learning;Control systems;Inference algorithms;Robustness","adaptive control;aircraft control;collision avoidance;feedback;fuzzy control;fuzzy reasoning;learning (artificial intelligence);multi-robot systems;robust control","equilibrium conditions;uncertain dynamic environments;pre-tuned fuzzy inference architectures;unmodeled conditions;adaptive distributed technique;autonomous control;flock systems;relatively flexible structure;online fuzzy reinforcement learning schemes;flock velocity consensus;dynamic disturbances;feedback signal;similar technique;adaptive fuzzy reinforcement;flock-guidance problem;challenging structure;multiple optimization objectives;different control approaches;collision avoidance;guidance schemes;complex tracking-error dynamics;linear feedback strategies","","3","","43","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Approach for Long-term Short-term Planning on Frenet Frame","M. Moghadam; A. Alizadeh; E. Tekin; G. H. Elkaim","Faculty of Electrical and computer Engineering, University of California Santa Cruz (UCSC), California, U.S; Department of Mechatronics Engineering, Istanbul Technical University (ITU); Faculty of Electrical and computer Engineering, University of California Santa Cruz (UCSC), California, U.S; Faculty of Electrical and computer Engineering, University of California Santa Cruz (UCSC), California, U.S","2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)","5 Oct 2021","2021","","","1751","1756","Tactical decision-making and strategic motion planning for autonomous highway driving are challenging due to predicting other road users' behaviors, diversity of environments, and complexity of the traffic interactions. This paper presents a novel end-to-end continuous deep reinforcement learning approach towards autonomous cars' decision-making and motion planning. For the first time, we define both states and action spaces on the Frenet space to make the driving behavior less variant to the road curvatures than the surrounding actors' dynamics and traffic interactions. The agent receives time-series data of past trajectories of the surrounding vehicles and applies convolutional neural networks along the time channels to extract features in the backbone. The algorithm generates continuous spatiotemporal trajectories on the Frenet frame for the feedback controller to track. Extensive high-fidelity highway simulations on CARLA show the superiority of the presented approach compared with commonly used baselines and discrete reinforcement learning on various traffic scenarios. Furthermore, the proposed method's advantage is confirmed with a more comprehensive performance evaluation against 1000 randomly generated test scenarios. Code: https://github.com/MajidMoghadam2006/RL-frenet-trajectory-planning-in-CARLA","2161-8089","978-1-6654-1873-7","10.1109/CASE49439.2021.9551598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551598","","Performance evaluation;Roads;Heuristic algorithms;Decision making;Reinforcement learning;Feature extraction;Trajectory","convolutional neural nets;feedback;learning (artificial intelligence);path planning;time series;traffic engineering computing","short-term planning;frenet frame;tactical decision-making;autonomous highway driving;end-to-end continuous deep reinforcement learning approach;autonomous cars;motion planning;Frenet space;driving behavior;time-series data;convolutional neural networks;continuous spatiotemporal trajectories;high-fidelity highway simulations;discrete reinforcement","","3","","35","IEEE","5 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning of Depth Stabilization with a Micro Diving Agent","G. Brinkmann; W. M. Bessa; D. -A. Duecker; E. Kreuzer; E. Solowjow","Institute of Mechanics and Ocean Engineering, Hamburg University of Technology, Germany; Department of Mechanical Engineering, Federal University of Rio Grande do Norte, Brazil; Institute of Mechanics and Ocean Engineering, Hamburg University of Technology, Germany; Institute of Mechanics and Ocean Engineering, Hamburg University of Technology, Germany; Institute of Mechanics and Ocean Engineering, Hamburg University of Technology, Germany","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","6197","6203","Reinforcement learning (RL) allows robots to solve control tasks through interaction with their environment. In this paper we study a model-based value-function RL approach, which is suitable for computationally limited robots and light embedded systems. We develop a diving agent, which uses the RL algorithm for underwater depth stabilization. Simulations and experiments with the micro diving agent demonstrate its ability to learn the depth stabilization task.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8461137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461137","","Computational modeling;Learning (artificial intelligence);Task analysis;Robot kinematics;Heuristic algorithms;Force","embedded systems;learning (artificial intelligence);microrobots;multi-agent systems;robot programming;underwater vehicles","model-based value-function RL algorithm;micro underwater agents;underwater robotics;underwater depth stabilization;light embedded systems;control tasks;microdiving agent;reinforcement learning","","2","","17","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Multi-agent Collaborative Adaptive Cruise Control Based On Reinforcement Learning","Y. Liu; W. Sun; W. Xu; X. Xiong; L. Hao; L. Qu","Teaching Development and Studentinnovation Center of SEIEE, Shanghai Jiaotong Univeristy, Shanghai, China; Teaching Development and Studentinnovation Center of SEIEE, Shanghai Jiaotong Univeristy, Shanghai, China; Teaching Development and Studentinnovation Center of SEIEE, Shanghai Jiaotong Univeristy, Shanghai, China; Teaching Development and Studentinnovation Center of SEIEE, Shanghai Jiaotong Univeristy, Shanghai, China; Teaching Development and Studentinnovation Center of SEIEE, Shanghai Jiaotong Univeristy, Shanghai, China; Teaching Development and Studentinnovation Center of SEIEE, Shanghai Jiaotong Univeristy, Shanghai, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","3388","3393","With the development and application of intelligent networked vehicles, Adaptive Cruise Control (ACC) is gradually developed and upgraded to Cooperative Adaptive Cruise Control (CACC). However, in the CACC system, following error, relative speed and acceleration have a certain range of constraints. If the acceleration fluctuates and amplifies backwards along the queue and violates the saturation constraint boundary, the acceleration of the vehicle behind will increase continuously, and the system will lose the stability of the queue and cause traffic accidents. Therefore, based on the Model Predictive Control (MPC) theory, this paper takes the CACC system as the research object and adopts the form of weighted quadratic performance functional and linear matrix inequality constraints. The design problem of the ACC is finally transformed into an online convex quadratic programming problem with constraints. At the same time, the saturation control of the CACC system is studied based on the parametric Lyapunov equation to improve the stability of the system.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728518","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728518","Multi-vehicle scheduling;Path planning;Vehicle-to-Vehicle communication;Cooperative Adaptive Cruise Control","Adaptive systems;Software packages;Heuristic algorithms;Conferences;Optimal control;Reinforcement learning;Stability analysis","adaptive control;control system synthesis;convex programming;learning (artificial intelligence);linear matrix inequalities;Lyapunov methods;multi-agent systems;predictive control;quadratic programming;road safety;road vehicles;stability;velocity control","saturation constraint boundary;Model Predictive Control theory;CACC system;weighted quadratic performance;linear matrix inequality constraints;ACC;saturation control;multiagent collaborative Adaptive Cruise Control;reinforcement learning;intelligent networked vehicles;acceleration fluctuates;amplifies backwards","","2","","23","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Application of Deep Reinforcement Learning to Dynamic Verification of DRAM Designs","H. Choi; I. Huh; S. Kim; J. Ko; C. Jeong; H. Son; K. Kwon; J. Chai; Y. Park; J. Jeong; D. S. Kim; J. Y. Choi","Design Technology Team of Memory Business; Computational Science & Engineering Team, Samsung Electronics; Computational Science & Engineering Team, Samsung Electronics; Computational Science & Engineering Team, Samsung Electronics; Computational Science & Engineering Team, Samsung Electronics; Design Technology Team of Memory Business; Design Technology Team of Memory Business; Design Technology Team of Memory Business; Design Technology Team of Memory Business; Computational Science & Engineering Team, Samsung Electronics; Computational Science & Engineering Team, Samsung Electronics; Design Technology Team of Memory Business","2021 58th ACM/IEEE Design Automation Conference (DAC)","8 Nov 2021","2021","","","523","528","This paper presents a deep neural network based test vector generation method for dynamic verification of memory devices. The proposed method is built on reinforcement learning framework, where the action is input stimulus on device pins and the reward is coverage score of target circuitry. The developed agent efficiently explores high-dimensional and large action space by using policy gradient method with Å-nearest neighbor search, transfer learning, and replay buffer. The generated test vectors attained the coverage score of 100% for fifteen representative circuit blocks of modern DRAM design. The output vector length was only 7% of the human-created vector length.","0738-100X","978-1-6654-3274-0","10.1109/DAC18074.2021.9586282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586282","","Gradient methods;Transfer learning;Random access memory;Reinforcement learning;Linear programming;Timing;Pins","deep learning (artificial intelligence);DRAM chips;gradient methods;logic design;logic testing;nearest neighbour methods;reinforcement learning;search problems;vectors","dynamic verification;memory devices;coverage score;policy gradient method;transfer learning;output vector length;human-created vector length;deep reinforcement learning;DRAM designs;deep neural network;test vector generation method;Å-nearest neighbor search;replay buffer","","2","","17","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Model-Free Safe Reinforcement Learning Through Neural Barrier Certificate","Y. Yang; Y. Jiang; Y. Liu; J. Chen; S. E. Li","School of Vehicle and Mobility, Tsinghua University, Beijing, China; School of Vehicle and Mobility, Tsinghua University, Beijing, China; Institute of Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; Institute of Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; School of Vehicle and Mobility, Tsinghua University, Beijing, China","IEEE Robotics and Automation Letters","27 Jan 2023","2023","8","3","1295","1302","Safety is a critical concern when applying reinforcement learning (RL) to real-world control tasks. However, existing safe RL works either only consider expected safety constraint violations and fail to maintain safety guarantees, or use overly conservative safety certificate tools borrowed from safe control theory, which sacrifices reward optimization and relies on analytic system models. This letter proposes a model-free safe RL algorithm that achieves near-zero constraint violations with high rewards. Our key idea is to jointly learn a policy and a neural barrier certificate under stepwise state constraint setting. The barrier certificate is learned in a model-free manner by minimizing the violations of appropriate barrier properties on transition data collected by the policy. We extend the single-step invariant property of the barrier certificate to a multi-step version and construct the corresponding multi-step invariant loss. This loss balances the bias and variance of the barrier certificate and enhances both the safety and performance of the policy. The policy is optimized under the constraint of the multi-step invariant property using the Lagrangian method. We optimize the policy in a model-free manner by introducing an importance sampling weight in the constraint. We test our algorithm on multiple problems, including classic control tasks, robot collision avoidance, and autonomous driving. Results show that our algorithm achieves near-zero constraint violations and high performance compared to the baselines. Moreover, the learned barrier certificates successfully identify the feasible regions on multiple tasks.","2377-3766","","10.1109/LRA.2023.3238656","NSF China(grant numbers:52221005); Tsinghua University Initiative Scientific Research Program; Tsinghua University-Toyota Joint Research Center for AI Technology of Automated Vehicle; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023989","Robot safety;reinforcement learning;neural barrier certificate","Safety;Task analysis;Costs;Data models;Computational modeling;Analytical models;Robots","differential equations;neural nets;optimisation;reinforcement learning","analytic system models;autonomous driving;barrier certificate learning;barrier properties;Lagrangian method;model-free safe reinforcement learning;model-free safe RL algorithm;multistep invariant loss;multistep invariant property;near-zero constraint violations;neural barrier certificate;overly conservative safety certificate tools;policy optimization;real-world control tasks;reward optimization;robot collision avoidance;safe control theory;safe RL works;safety constraint violations;safety guarantees;single-step invariant property;stepwise state constraint setting","","2","","41","IEEE","20 Jan 2023","","","IEEE","IEEE Journals"
"Continuous Control of Autonomous Vehicles using Plan-assisted Deep Reinforcement Learning","T. Dwivedi; T. Betz; F. Sauerbeck; P. Manivannan; M. Lienkamp","Department of Mechanical Engineering, Indian Institute of Technology Madras, Chennai, India; Institute of Automotive Technology, School of Engineering and Design, Technical University of Munich, Garching, Germany; Institute of Automotive Technology, School of Engineering and Design, Technical University of Munich, Garching, Germany; Department of Mechanical Engineering, Indian Institute of Technology Madras, Chennai, India; Institute of Automotive Technology, School of Engineering and Design, Technical University of Munich, Garching, Germany","2022 22nd International Conference on Control, Automation and Systems (ICCAS)","9 Jan 2023","2022","","","244","250","End-to-end deep reinforcement learning (DRL) is emerging as a promising paradigm for autonomous driving. Although DRL provides an elegant framework to accomplish final goals without extensive manual engineering, capturing plans and behavior using deep neural networks is still an unsolved issue. End-to-end architectures, as a result, are currently limited to simple driving scenarios, often performing sub-optimally when rare, unique conditions are encountered. We propose a novel plan-assisted deep reinforcement learning framework that, along with the typical state-space, leverages a “trajectory-space” to learn optimal control. While the trajectory-space, generated by an external planner, intrinsically captures the agent’s high-level plans, world models are used to understand the dynamics of the environment for learning behavior in latent space. An actor-critic network, trained in imagination, uses these latent features to predict policy and state-value function. Based primarily on DreamerV2 and Racing Dreamer, the proposed model is first trained in a simulator and eventually tested on the FITENTH race car. We evaluate our model for best lap times against parameter-tuned and learning-based controllers on unseen race tracks and demonstrate that it generalizes to complex scenarios where other approaches perform sub-optimally. Furthermore, we show the model’s enhanced stability as a trajectory tracker and establish the improvement in interpretability achieved by the proposed framework.","2642-3901","978-89-93215-24-3","10.23919/ICCAS55662.2022.10003698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003698","Autonomous driving;intelligent vehicles;world models;deep reinforcement learning;artificial intelligence","Deep learning;Software algorithms;Reinforcement learning;Prediction algorithms;Stability analysis;Software;Behavioral sciences","automobiles;control engineering computing;deep learning (artificial intelligence);mobile robots;optimal control;reinforcement learning;trajectory control","actor-critic network;autonomous driving;autonomous vehicles;continuous control;deep neural networks;DreamerV2;DRL;elegant framework;end-to-end architectures;extensive manual engineering;FITENTH race car;high-level plans;latent space;plan-assisted deep reinforcement learning framework;Racing Dreamer;rare conditions;simple driving scenarios;state-space;state-value function;trajectory-space to learn optimal control;unique conditions","","2","","34","","9 Jan 2023","","","IEEE","IEEE Conferences"
"Reducing Vibration of A Rotating Machine with Deep Reinforcement Learning","Z. Tao; D. Yian; H. Fan; Z. Xiangqi; W. Yaoyao; L. Tianlin; P. Bolchover; Y. Tao; Z. Guishui; C. Rongbing; Z. Ping; L. Qing; L. Dingsheng","Key Lab of Machine Perception Department of Machine Intelligence, Peking University, Beijing, China; Key Lab of Machine Perception Department of Machine Intelligence, Peking University, Beijing, China; Key Lab of Machine Perception Department of Machine Intelligence, Peking University, Beijing, China; Key Lab of Machine Perception Department of Machine Intelligence, Peking University, Beijing, China; Key Lab of Machine Perception Department of Machine Intelligence, Peking University, Beijing, China; Key Lab of Machine Perception Department of Machine Intelligence, Peking University, Beijing, China; Schlumberger; Schlumberger; Schlumberger; Schlumberger; Schlumberger; Schlumberger; Key Lab of Machine Perception Department of Machine Intelligence, Peking University, Beijing, China","2020 IEEE International Conference on Mechatronics and Automation (ICMA)","26 Oct 2020","2020","","","932","937","Reducing vibration of a high-speed rotating machine is an important task as the vibration can damage equipment, increase costs and reduce quality. Faced with the problems, in this paper, we proposed a control tool with several pads and sensors to actively dampening the vibration of the machine. Using Deep Reinforcement Learning (RL) to learn a policy that determines whether to stretch out a pad and the speed of the pads according to the information collected by sensors, our approach can change the displacement of the rotating machine in space by generating an opposite force with a pad while it touches the wall so as to reduce the vibration. We evaluate our approach against two baselines, a control method with all the pads stretched out and a method without equipping the control tool. Our results indicate that simple RL algorithms in controlling the pad tool have effectiveness in reducing the vibration of the machine.","2152-744X","978-1-7281-6416-8","10.1109/ICMA49215.2020.9233736","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9233736","Rotating Machine;Balance Control;Reinforcement Learning","Vibrations;Mechatronics;Rotating machines;Force;Tactile sensors;Reinforcement learning;Aerospace electronics","control engineering computing;learning (artificial intelligence);neural nets;vibration control","vibration reduction;high-speed rotating machine;control tool;pad tool;deep reinforcement learning;opposite force;simple RL algorithms","","2","","20","IEEE","26 Oct 2020","","","IEEE","IEEE Conferences"
"FISAR: Forward Invariant Safe Reinforcement Learning with a Deep Neural Network-Based Optimizer","C. Sun; D. -K. Kim; J. P. How","Laboratory for Information & Decision Systems (LIDS), Massachusetts Institute of Technology, Cambridge, MA; Laboratory for Information & Decision Systems (LIDS), Massachusetts Institute of Technology, Cambridge, MA; Laboratory for Information & Decision Systems (LIDS), Massachusetts Institute of Technology, Cambridge, MA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","10617","10624","This paper investigates reinforcement learning with constraints, which are indispensable in safety-critical environments. To drive the constraint violation to decrease monotonically, we take the constraints as Lyapunov functions and impose new linear constraints on the policy parameters’ updating dynamics. As a result, the original safety set can be forward-invariant. However, because the new guaranteed-feasible constraints are imposed on the updating dynamics instead of the original policy parameters, classic optimization algorithms are no longer applicable. To address this, we propose to learn a generic deep neural network (DNN)-based optimizer to optimize the objective while satisfying the linear constraints. The constraint-satisfaction is achieved via projection onto a polytope formulated by multiple linear inequality constraints, which can be solved analytically with our newly designed metric. To the best of our knowledge, this is the first DNN-based optimizer for constrained optimization with the forward invariance guarantee. We show that our optimizer trains a policy to decrease the constraint violation and maximize the cumulative reward monotonically. Results on numerical constrained optimization and obstacle-avoidance navigation validate the theoretical findings.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561147","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561147","","Measurement;Deep learning;Navigation;Heuristic algorithms;Conferences;Reinforcement learning;Safety","collision avoidance;deep learning (artificial intelligence);Lyapunov methods;optimisation;reinforcement learning;safety-critical software","Lyapunov functions;constraint-satisfaction;multiple linear inequality constraints;DNN-based optimizer;numerical constrained optimization;forward invariant safe reinforcement learning;safety-critical environments;FISAR;generic deep neural network;obstacle-avoidance navigation","","2","","36","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Safe multi-agent motion planning via filtered reinforcement learning","A. P. Vinod; S. Safaoui; A. Chakrabarty; R. Quirynen; N. Yoshikawa; S. Di Cairano","Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Corporation, Japan; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","7270","7276","We study the problem of safe multi-agent motion planning in cluttered environments. Existing multi-agent reinforcement learning-based motion planners only provide approximate safety enforcement. We propose a safe reinforcement learning algorithm that leverages single-agent reinforcement learning for target regulation and a subsequent convex optimization-based filtering that ensures the collective safety of the system. Our approach yields a safe, real-time implementable multi-agent motion planner that is simpler to train and enforces safety as hard constraints. Our approach can handle state and control constraints on the agents, and enforce collision avoidance among themselves and with static obstacles in the environment. Numerical simulations and hardware experiments show the efficacy of the approach.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812259","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812259","","Filtering;Reinforcement learning;Numerical simulation;Real-time systems;Regulation;Hardware;Safety","collision avoidance;learning (artificial intelligence);mobile robots;multi-agent systems;path planning","train enforces safety;safe multiagent motion planning;filtered reinforcement learning;existing multiagent reinforcement learning-based motion planners;approximate safety enforcement;safe reinforcement learning algorithm;leverages single-agent reinforcement learning;safe time;multiagent motion planner","","2","","26","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Orientation Estimation Using Inertial Sensors with Performance Guarantee","L. Hu; Y. Tang; Z. Zhou; W. Pan","School of Computer Science and Electronic Engineering, University of Essex, UK; Department of Cognitive Robotics, Delft University of Technology, Netherlands; Department of Cognitive Robotics, Delft University of Technology, Netherlands; Department of Cognitive Robotics, Delft University of Technology, Netherlands","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","10243","10249","This paper presents a deep reinforcement learning (DRL) algorithm for orientation estimation using inertial sensors combined with a magnetometer. Lyapunov’s method in control theory is employed to prove the convergence of orientation estimation errors. The estimator gains and a Lyapunov function are parametrised by deep neural networks and learned from samples based on the theoretical results. The DRL estimator is compared with three well-known orientation estimation methods on both numerical simulations and real dataset collected from commercially available sensors. The results show that the proposed algorithm is superior for arbitrary estimation initialisation and can adapt to a drastic angular velocity profile for which other algorithms can be hardly applicable. To the best of our knowledge, this is the first DRL-based orientation estimation method with an estimation error boundedness guarantee.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561440","China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561440","","Estimation error;Magnetometers;Inertial sensors;Quaternions;Reinforcement learning;Solids;Numerical simulation","acceleration measurement;angular velocity measurement;computerised instrumentation;deep learning (artificial intelligence);gradient methods;Lyapunov methods;magnetometers;neural nets;position measurement;reinforcement learning;sensors;state estimation","DRL-based orientation estimation method;estimation error;inertial sensors;performance guarantee;orientation estimation errors;estimator gains;Lyapunov function;deep neural networks;DRL estimator;arbitrary estimation initialisation;deep reinforcement learning;magnetometer;angular velocity profile;numerical simulations","","2","","32","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement learning with misspecified model classes","J. Joseph; A. Geramifard; J. W. Roberts; J. P. How; N. Roy","Massachusetts Institute of Technology, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA","2013 IEEE International Conference on Robotics and Automation","17 Oct 2013","2013","","","939","946","Real-world robots commonly have to act in complex, poorly understood environments where the true world dynamics are unknown. To compensate for the unknown world dynamics, we often provide a class of models to a learner so it may select a model, typically using a minimum prediction error metric over a set of training data. Often in real-world domains the model class is unable to capture the true dynamics, due to either limited domain knowledge or a desire to use a small model. In these cases we call the model class misspecified, and an unfortunate consequence of misspecification is that even with unlimited data and computation there is no guarantee the model with minimum prediction error leads to the best performing policy. In this work, our approach improves upon the standard maximum likelihood model selection metric by explicitly selecting the model which achieves the highest expected reward, rather than the most likely model. We present an algorithm for which the highest performing model from the model class is guaranteed to be found given unlimited data and computation. Empirically, we demonstrate that our algorithm is often superior to the maximum likelihood learner in a batch learning setting for two common RL benchmark problems and a third real-world system, the hydrodynamic cart-pole, a domain whose complex dynamics cannot be known exactly.","1050-4729","978-1-4673-5643-5","10.1109/ICRA.2013.6630686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630686","","Mathematical model;Data models;Computational modeling;Equations;Standards;Measurement;Training data","learning (artificial intelligence);maximum likelihood estimation;robots","reinforcement learning;misspecified model classes;real world robots;world dynamics;maximum likelihood model;batch learning setting;hydrodynamic cart-pole","","2","","37","IEEE","17 Oct 2013","","","IEEE","IEEE Conferences"
"Satellite Attitude Control with Deep Reinforcement Learning","D. Gao; H. Zhang; C. Li; X. Gao","Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China; Beijing Institute of Control Engineering, Beijing, China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","4095","4101","Deep reinforcement learning has recently gained great interest in the field of intelligent control. In this paper, we develop a set of deep reinforcement learning algorithms on satellite attitude control. By improving Deep Deterministic Policy Gradient (DDPG) and Twin Delayed DDPG (TD3), we analyze three kinds of attitude control problems, including no state-constrained, state-constrained and online optimization problems. We compare Deep Reinforcement Learning Controller with traditional PD controller at the same time. Besides, we summarize a design process to apply deep reinforcement learning algorithms on satellite attitude control problems. It is shown in this paper that Deep Reinforcement Learning Controller has advantages on model-free control and online optimization.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9326605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326605","satellite attitude control;deep reinforcement learning;online optimization control","Reinforcement learning;Satellites;Quaternions;Attitude control;Process control;Optimization;Deep learning","artificial satellites;attitude control;control engineering computing;deep learning (artificial intelligence);gradient methods;intelligent control","twin delayed DDPG;deep reinforcement learning;satellite attitude control;deep deterministic policy gradient;intelligent control","","2","","9","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning for 4-Finger-Gripper Manipulation","M. Ojer De Andres; M. Mahdi Ghazaei Ardakani; A. Robertsson","Mahdi Ghazaei is with the Department of Advanced Robotics (ADVR), Italian Institute of Technology (IIT); Mahdi Ghazaei is with the Department of Advanced Robotics (ADVR), Italian Institute of Technology (IIT); Anders Robertsson is with the Department of Automatic Control, Lund University, Lund, Sweden","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","4257","4262","In the framework of robotics, Reinforcement Learning (RL) deals with the learning of a task by the robot itself. This paper presents a hierarchical-planning approach in which the robot learns the optimal behavior for different levels in a decoupled way. For high-level discrete actions, Q-learning was chosen, whereas for the low level we utilize Policy Improvement with Path Integrals (PI2) algorithm to learn the parameters of policies, represented by rhythmic Dynamic Movement Primitives (DMPs). The paper studies the case of a 4-finger-gripper manipulator, which performs the task of continuously spinning a ball around a desired axis. The results demonstrate the efficacy of the hierarchical planning and the increased performance of the task when PI2 is used in conjunction with rhythmic DMPs in a real environment.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8461153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461153","","Robots;Trajectory;Task analysis;Planning;Heuristic algorithms;Learning (artificial intelligence);Approximation algorithms","grippers;intelligent robots;learning (artificial intelligence);manipulator dynamics;motion control;path planning","high-level discrete actions;Q-learning;rhythmic Dynamic Movement Primitives;4-finger-gripper manipulator;hierarchical planning;Reinforcement Learning;4-finger-gripper manipulation;hierarchical-planning approach","","2","","19","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Leveraging Curriculum Reinforcement Learning for Rocket Powered Landing Guidance and Control","H. Yuan; Y. Zhao; Y. Mou; X. Wang","Beijing Institute of Astronautical Systems Engineering, Beijing, China; Beijing Institute of Astronautical Systems Engineering, Beijing, China; Beijing Institute of Astronautical Systems Engineering, Beijing, China; China Academy of Launch Vehicle Technology, Beijing, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","5765","5770","Future missions to recover the first stage of rocket require advanced guidance and control algorithms which can overcome stochastic disturbances, so as to achieve pinpoint landing. Recent studies have shown promising results of reinforcement learning (RL) based powered landing guidance and control method. However, many of them suffer from poor sample efficiency due to the sparse and complex reward, that trades off different goals in terms of attitude stability, terminal velocity, terminal attitude, terminal position and fuel usage. In order to address the above-mentioned problem, we propose a novel curriculum RL method to learn a fuel optimal pinpoint landing policy by maximizing the complex reward in this paper. The basic idea is to start with an easier task, and then increase the difficulty level by adding more goals. In this way, each task provides a good initial policy for the next task, making it easier to train the policy. The plane rigid dynamic model of rocket powered landing is adopted. The simulation result shows that the proposed RL method, which combined curriculum learning and Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm, can learn a better policy than the TD3 algorithm without curriculum under the same number of samples.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728176","Reusable rocket;Powered landing guidance and control;Reinforcement learning;Optimal control;Curriculum learning","Rockets;Simulation;Heuristic algorithms;Stochastic processes;Reinforcement learning;Aerodynamics;Stability analysis","aerospace control;attitude control;entry, descent and landing (spacecraft);learning (artificial intelligence);optimisation;rockets;space vehicles","curriculum reinforcement learning;rocket powered landing guidance;advanced guidance;stochastic disturbances;poor sample efficiency;sparse reward;complex reward;attitude stability;terminal velocity;terminal attitude;terminal position;fuel usage;above-mentioned problem;novel curriculum RL method;fuel optimal pinpoint landing policy;easier task;good initial policy;curriculum learning;Twin Delayed Deep Deterministic Policy Gradient algorithm","","2","","36","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Safe Reinforcement Learning Using Robust Control Barrier Functions","Y. Emam; G. Notomista; P. Glotfelter; Z. Kira; M. Egerstedt","Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; Outrider, Golden, CO, USA; Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA; Samueli School of Engineering, University of California, Irvine, CA, USA","IEEE Robotics and Automation Letters","","2022","PP","99","1","8","Reinforcement Learning (RL) has been shown to be effective in many scenarios. However, it typically requires the exploration of a sufficiently large number of state-action pairs, some of which may be unsafe. Consequently, its application to safety-critical systems remains a challenge. An increasingly common approach to address safety involves the addition of a safety layer that projects the RL actions onto a safe set of actions. In turn, a difficulty for such frameworks is how to effectively couple RL with the safety layer to improve the learning performance. In this paper, we frame safety as a differentiable robust-control-barrier-function layer in a model-based RL framework. Moreover, we also propose an approach to modularly learn the underlying reward-driven task, independent of safety constraints. We demonstrate that this approach both ensures safety and effectively guides exploration during training in a range of experiments, including zero-shot transfer when the reward is learned in a constraint-agnostic fashion.","2377-3766","","10.1109/LRA.2022.3216996","Army Research Lab(grant numbers:W911NF-17-2-0181); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9928337","Robust/Adaptive Control;Reinforcement Learning;Robot Safety","Safety;Training;Task analysis;Robots;Reinforcement learning;Dynamical systems;Control systems","","","","2","","","IEEE","25 Oct 2022","","","IEEE","IEEE Early Access Articles"
"ARiADNE: A Reinforcement learning approach using Attention-based Deep Networks for Exploration","Y. Cao; T. Hou; Y. Wang; X. Yi; G. Sartoretti","Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore; Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore; Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore; Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore; Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","10219","10225","In autonomous robot exploration tasks, a mobile robot needs to actively explore and map an unknown environment as fast as possible. Since the environment is being revealed during exploration, the robot needs to frequently re-plan its path online, as new information is acquired by onboard sensors and used to update its partial map. While state-of-the-art exploration planners are frontier- and sampling-based, encouraged by the recent development in deep reinforcement learning (DRL), we propose ARiADNE, an attention-based neural approach to obtain real-time, non-myopic path planning for autonomous exploration. ARiADNE is able to learn dependencies at multiple spatial scales between areas of the agent's partial map, and implicitly predict potential gains associated with exploring those areas. This allows the agent to sequence movement actions that balance the natural trade-off between exploitation/refinement of the map in known areas and exploration of new areas. We experimentally demonstrate that our method outperforms both learning and non-learning state-of-the-art baselines in terms of average trajectory length to complete exploration in hundreds of simplified 2D indoor scenarios. We further validate our approach in high-fidelity Robot Operating System (ROS) simulations, where we consider a real sensor model and a realistic low-level motion controller, toward deployment on real robots.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160565","","Deep learning;Solid modeling;Three-dimensional displays;Reinforcement learning;Robot sensing systems;Sensor systems;Real-time systems","deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;motion control;multi-robot systems;path planning;reinforcement learning","ARiADNE;attention-based deep networks;attention-based neural approach;autonomous exploration;autonomous robot exploration tasks;complete exploration;deep reinforcement learning;high-fidelity Robot Operating System simulations;known areas;mobile robot;nonmyopic path;partial map;reinforcement learning approach;sampling-based;state-of-the-art exploration planners;unknown environment","","2","","27","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Leader-Follower Optimal Bipartite Consensus Control for Multi-agent systems with Time-delay Using Reinforcement Learning Method","J. Zhang; H. Ma; W. Li; Y. Zhang","College of Information Science and Engineering, Ocean University of China, Qingdao, China; College of Information Science and Engineering, Ocean University of China, Qingdao, China; College of Information Science and Engineering, Ocean University of China, Qingdao, China; College of Information Science and Engineering, Ocean University of China, Qingdao, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","1587","1592","This paper deals with the optimal bipartite consensus control (OBCC) for heterogeneous multi-agent systems (MASs) with time-delay based on reinforcement learning (RL) scheme. MASs, denoted by communication network, is modeled as a signed graph, and the competitive (cooperative) relationship is represented by negative (positive) edges. Firstly, Heterogeneous MASs is depicted by cooperative and competitive (hereinafter referred to as coopetition) network, and local neighborhood bipartite consensus errors and their performance index function (PIF) are put forward to formulate OBCC problem. Secondly, a proper model reduction method is developed to tackle time-delay in coopetition network, and then MASs with time-delay can be transformed into a delay-free MASs. Thirdly, a RL algorithm is employed to solve the solution of Hamilton-Jacobi-Bellman (HJB) equations under OBCC laws, and substantially we adopt neural networks (NNs) to calculate the control policy and PIF online. Finally, the pivotal simulation validates availability of proposed algorithm.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327131","National Science Foundation; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327131","Optimal bipartite consensus control;coopetition network;reinforcement learning;model reduction method","Mathematical model;Delays;Consensus control;Reduced order systems;Optimal control;Oceans;Information science","control engineering computing;delays;graph theory;learning (artificial intelligence);multi-agent systems;multi-robot systems;neurocontrollers;optimal control;reduced order systems","heterogeneous MAS;local neighborhood bipartite consensus errors;model reduction method;time-delay;delay-free MAS;leader-follower optimal bipartite consensus control;reinforcement learning method;heterogeneous multiagent systems;communication network;neural networks;Hamilton-Jacobi-Bellman equations","","2","","28","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Decentralized Global Connectivity Maintenance for Multi-Robot Navigation: A Reinforcement Learning Approach","M. Li; Y. Jie; Y. Kong; H. Cheng","School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","8801","8807","The problem of multi-robot navigation of connectivity maintenance is challenging in multi-robot applications. This work investigates how to navigate a multi-robot team in unknown environments while maintaining connectivity. We propose a reinforcement learning (RL) approach to develop a decentralized policy, which is shared among multiple robots. Given range sensor measurements and the positions of other robots, the policy aims to generate control commands for navigation and preserve the global connectivity of the robot team. We incorporate connectivity concerns into the RL framework as constraints and introduce behavior cloning to reduce the exploration complexity of policy optimization. The policy is optimized with all transition data collected by multiple robots in random simulated scenarios. We validate the effectiveness of the proposed approach by comparing different combinations of connectivity constraints and behavior cloning. We also show that our policy can generalize to unseen scenarios in both simulation and holonomic robots experiments.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812163","","Training;Navigation;Cloning;Reinforcement learning;Maintenance engineering;Position measurement;Robot sensing systems","learning (artificial intelligence);mobile robots;multi-robot systems;path planning","decentralized global connectivity maintenance;multirobot navigation;reinforcement learning approach;multirobot applications;multirobot team;decentralized policy;multiple robots;given range sensor measurements;robot team;connectivity concerns;policy optimization;connectivity constraints;holonomic robots experiments","","2","","31","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Adaptive Waveform Selection Algorithm based on Reinforcement Learning for Cognitive Radar","X. Cao; Z. Zheng; D. An","Unit 91404 of the PLA, Qinhuangdao, China; School of Information and Electronic, Beijing Institute of Technology, Beijing, China; Unit 91404 of the PLA, Qinhuangdao, China","2019 IEEE 2nd International Conference on Automation, Electronics and Electrical Engineering (AUTEEE)","12 Mar 2020","2019","","","208","213","Cognitive radar is a newly emerging intelligent radar that can adaptively change the transmitted signal waveform according to changes in the target and environment to improve the accuracy of target state estimation. In this paper, the running process of cognitive radar adaptive transmission is analyzed, the tracking waveform parameter selection is correlated with the target state estimation and the reinforcement learning model is established. The problem of unknown target state space is solved by the “prioritized sweeping” method and the computational efficiency is improved by replacing “eligibility trace”. The simulation results show that the indirect reinforcement learning method is better than the fixed waveform and the waveform selection algorithm based on the minimum mean square error for the tracking accuracy and state estimation error of the target.","","978-1-7281-5030-7","10.1109/AUTEEE48671.2019.9033413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9033413","cognitive radar;transmit adaptive;waveform selection;indirect reinforcement learning","Mathematical model;Cognitive radar;Learning (artificial intelligence);Radar tracking;Noise measurement;Target tracking","cognitive radar;learning (artificial intelligence);least mean squares methods;parameter estimation;radar computing;radar tracking;radar transmitters;state estimation;waveform analysis","adaptive waveform selection algorithm;signal waveform transmission;target state estimation error;reinforcement learning model;unknown target state space;prioritized sweeping method;indirect reinforcement learning method;intelligent cognitive radar adaptive transmission;waveform parameter selection tracking algorithm;minimum mean square error","","2","","17","IEEE","12 Mar 2020","","","IEEE","IEEE Conferences"
"Android as a Receptionist in a Shopping Mall Using Inverse Reinforcement Learning","Z. Chen; Y. Nakamura; H. Ishiguro","Graduate School of Engineering Science, Osaka University, Osaka, Japan; Graduate School of Engineering Science, Osaka University, Osaka, Japan; Graduate School of Engineering Science, Osaka University, Osaka, Japan","IEEE Robotics and Automation Letters","14 Jun 2022","2022","7","3","7091","7098","For human-robot interaction (HRI), it is difficult to hand-craft all the rules for robots owing to diverse situations. Therefore, inverse reinforcement learning (IRL) is a potential solution that helps transfer human knowledge about interactions to robots. However, the feasibility of practically using IRL for HRI remains unknown. Here, we demonstrate a practical HRI application of IRL. An android was trained using IRL and acted as a receptionist to encourage visitors to practice hand hygiene in a shopping mall. We found that android learning through IRL has a competitive ability to a well-trained human operator on the reception task. Furthermore, we found that the android maintained high performance regardless of customer traffic. Our results demonstrate the potential of IRL in advancing the social HRI field. We anticipate that our work will be a starting point for using IRL in future HRI applications.","2377-3766","","10.1109/LRA.2022.3180042","JST Moonshot R&D(grant numbers:JPMJMS2011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9788067","Social HRI;imitation learning","Robots;Human-robot interaction;Behavioral sciences;Task analysis;Training;Cameras;Remote control","human-robot interaction;reinforcement learning;robot programming;service robots","receptionist;shopping mall;inverse reinforcement learning;human-robot interaction;IRL;human knowledge;Android learning;human operator;social HRI field;HRI applications","","2","","36","IEEE","3 Jun 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning as a Method for Tuning CPG Controllers for Underwater Multi-Fin Propulsion","A. Drago; G. Carryon; J. Tangorra",NA; NA; NA,"2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","11533","11539","CPG-Based oscillator networks are increasingly being used to drive multi-limbed robots. To produce effective gaits with these networks, the relationship between the CPG parameters and the characteristics of the gait must be determined. However, due to the nonlinear nature of the oscillators, this relationship is challenging to ascertain. In this work a reinforcement learning algorithm is used to determine the CPG parameters that produce propulsively beneficial kinematics in a multi-fin underwater robot. Due to the high computational cost in creating high fidelity simulations of underwater systems, an alternate method using a low fidelity simulation is explored. To better simulate the dynamics of a two-finned swimming robot a thorough force sweep is conducted on the subject robot in a controlled environment. The resulting force data is used as the dynamic information in a simple simulation. This method allows for the learning of CPG weight settings that produce desired kinematic operating conditions and their resulting forces in simulation. Using this method, when the learned CPG parameters were applied directly to the physical robot, the robot executed the same desired kinematics and forces as expected from simulation with no additional learning needed.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812128","","Computational modeling;Heuristic algorithms;Force;Dynamics;Kinematics;Reinforcement learning;Propulsion","biomechanics;mobile robots;propulsion;reinforcement learning;robot dynamics;underwater vehicles","low fidelity simulation;two-finned swimming robot;controlled environment;force data;kinematic operating conditions;tuning CPG controllers;underwater multifin propulsion;multilimbed robots;reinforcement learning algorithm;propulsively beneficial kinematics;high computational cost;high fidelity simulations;CPG-based oscillator networks","","2","","26","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Data-Driven Lateral Fault-tolerance Control of Autonomous Vehicle System Using Reinforcement Learning","Y. Li; H. Zhang; Z. Wang","Department of Control Science and Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, Tongji University, Shanghai, China","2020 IEEE 16th International Conference on Control & Automation (ICCA)","30 Nov 2020","2020","","","1410","1415","In this paper, a data driven lateral fault-tolerant control (LFTC) method is proposed for four wheels drive vehicles, which considering both vehicle velocity and trajectory tracking performance. The novel design of the lateral control employs with zero-sum game and adaptive dynamics programming technique to solve the Riccati equation without requiring the knowledge of system, only using online data. The LFTC consists of the data driven off-policy and adaptive control. The adaptive parameters adjusted online to compensate the actuator faults automatically. The tracking system is asymptotically stable with the disturbance attenuation level γ. Finally, simulation is provided to show the effectiveness of the proposed method.","1948-3457","978-1-7281-9093-8","10.1109/ICCA51439.2020.9264394","National Natural Science Foundation of China; Shanghai International Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264394","","Vehicle dynamics;Actuators;Mathematical model;Control systems;Heuristic algorithms;Fault tolerant systems;Fault tolerance","adaptive control;asymptotic stability;control system synthesis;dynamic programming;fault tolerant control;learning (artificial intelligence);mobile robots;Riccati equations;road vehicles;robot programming","adaptive dynamics programming;asymptotic stability;Riccati equation;zero-sum game;trajectory tracking;vehicle velocity;four wheel drive vehicles;reinforcement learning;autonomous vehicle system;data driven lateral fault tolerance control;adaptive control","","2","","29","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning with Converging Goal Space and Binary Reward Function","W. Ro; W. Jeon; H. Bamshad; H. Yang","Department of Mechanical Engineering, Yonsei University, Seodaemun-gu, Seoul, Korea; Department of Mechanical Engineering, Yonsei University, Seodaemun-gu, Seoul, Korea; Department of Mechanical Engineering, Yonsei University, Seodaemun-gu, Seoul, Korea; Department of Mechanical Engineering, Yonsei University, Seodaemun-gu, Seoul, Korea","2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)","12 Nov 2020","2020","","","921","927","Usage of a sparse and binary reward function remains one of the most challenging problems in reinforcement learning. In particular, when the environments wherein robotic agents learn are sufficiently vast, it is much more difficult to learn tasks because the probability of reaching the goal is minimal. A Hindsight Experience Replay algorithm was proposed to overcome these difficulties; however, problems persist that affect the learning speed and delay learning when a learning agent cannot receive proper rewards at the beginning of the learning process. In this paper, we present a simple method called Converging Goal Space and Binary Reward Function, which helps agents learn tasks easily and efficiently in large environments while providing a binary reward. At an early stage in training, a larger goal space margin facilitates the reward function for a more rapid policy learning. As the number of successes increases, the goal space is gradually reduced to the size used to the size used in the test. We apply this reward function to two different task experiments: sliding and throwing, which must be explored at a wider range than the reach of the robotic arms, and then compare the learning efficiency to that of experiments that only employ a sparse and binary reward function. We show that the proposed reward function performs better in large environments using physics simulation, and we demonstrate that the function is applicable to real world robotic arms.","2161-8089","978-1-7281-6904-0","10.1109/CASE48305.2020.9249227","Basic Science Research Program through the National Research Foundation of Korea (NRF; Ministry of Education(grant numbers:NRF-2018R1D1A1B07049267); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9249227","","Task analysis;Reinforcement learning;Training;Uncertainty;Standards;End effectors;Delays","learning (artificial intelligence);manipulators;software agents","Hindsight Experience Replay algorithm;learning speed;delay learning;learning agent;rapid policy learning;goal space margin;converging goal space;robotic agents;sparse reward function;binary reward function;reinforcement learning","","2","","29","IEEE","12 Nov 2020","","","IEEE","IEEE Conferences"
"Using Reinforcement Learning to Create Control Barrier Functions for Explicit Risk Mitigation in Adversarial Environments","E. Scukins; P. Ögren","Aeronautics Division, SAAB, Linköping, Sweden; Robotics, Perception and Learning Lab., School of Electrical Engineering and Computer Science, Royal Institute of Technology (KTH), Stockholm, Sweden","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","10734","10740","Air Combat is a high-risk activity carried out by trained professionals operating sophisticated equipment. During this activity, a number of trade-offs have to be made, such as the balance between risk and efficiency. A policy that minimizes risk could have very low efficiency, and one that maximizes efficiency may involve very high risk.In this study, we use Reinforcement Learning (RL) to create Control Barrier Functions (CBF) that captures the current risk, in terms of worst-case future separation between the aircraft and an enemy missile. CBFs are usually designed manually as closed-form expressions, but for a complex system such as a guided missile, this is not possible. Instead, we solve an RL problem using high fidelity simulation models to find value functions with CBF properties, that can then be used to guarantee safety in real air combat situations. We also provide a theoretical analysis of what family of RL problems result in value functions that can be used as CBFs in this way.The proposed approach allows the pilot in an air combat scenario to set the exposure level deemed acceptable and continuously monitor the risk related to his/her own safety. Given input regarding acceptable risk, the system limits the choices of the pilot to those that guarantee future satisfaction of the provided bound.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561853","Vinnova; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561853","Control Barrier Functions;Reinforcement Learning;Safe Exploration","Missiles;Atmospheric modeling;Reinforcement learning;Tools;Mathematical models;Safety;Risk management","aerospace computing;control engineering computing;military aircraft;military computing;missile guidance;optimisation;reinforcement learning;risk management","reinforcement learning;control barrier functions;explicit risk mitigation;adversarial environments;high-risk activity;worst-case future separation;enemy missile;CBFs;closed-form expressions;guided missile;high fidelity simulation models;value functions;air combat scenario","","2","","22","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Model for Virtual Machines Consolidation in Cloud Data Center","Q. Chou; W. Fan; J. Zhang","Artificial Intelligence Lab Lenovo Research, Beijing, China; Artificial Intelligence Lab Lenovo Research, Beijing, China; Enterprise Cloud Research & 5G Lenovo Research, Bejing, China","2021 6th International Conference on Automation, Control and Robotics Engineering (CACRE)","10 Aug 2021","2021","","","16","21","Energy consumption in data center is currently the main focus of many large-scale enterprises and cloud service providers. Dynamic virtual machine (VM) consolidation technologies are widely used to improve resource utilization of data centers and reduce energy. They try to identify the poorly utilized physical hosts and make the most of the resources. Then idle hosts can be switched to sleep or active mode, meanwhile considering the real-time fluctuation of service workload. In this paper, we propose a Reinforcement Learning (RL) based Virtual Machine Consolidation (RL-VMC) framework with an application to the cloud data center operation. The RL-VMC method uses an agent corresponding to a VM planner to interact with the environment, which encapsulates the data center running state, and learns the optimal policy to determine the migration mapping from VMs to physical hosts. Specifically, we adopt the on-policy method State-Action-Reward-State-Action (SARSA) in RL-VMC, which learns from experience to get the optimal VMs migration strategy and manage the host power mode as well. Additionally, we use the PBRS technology to speed up the convergence of the RL-VMC. Experimental results show that the RL-VMC method can adapt to the dynamic workload maintaining an improved balance between energy consumption and Service Level Agreements (SLA).","","978-1-6654-3576-5","10.1109/CACRE52464.2021.9501288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9501288","Virtual machines consolidation;Cloud computing;Reinforcement learning","Data centers;Cloud computing;Energy consumption;Reinforcement learning;Switches;Virtual machining;Data models","cloud computing;computer centres;learning (artificial intelligence);power aware computing;resource allocation;virtual machines","large-scale enterprises;cloud service providers;dynamic virtual machine consolidation technologies;resource utilization;poorly utilized physical hosts;idle hosts;active mode;service workload;Virtual Machine Consolidation framework;cloud data center operation;RL-VMC method;VM planner;data center running state;on-policy method State-Action-Reward-State-Action;optimal VMs migration strategy;host power mode;dynamic workload;energy consumption;Service Level Agreements;Reinforcement Learning model;Virtual machines Consolidation","","2","","21","IEEE","10 Aug 2021","","","IEEE","IEEE Conferences"
"Distributed safe reinforcement learning for multi-robot motion planning","Y. Lu; Y. Guo; G. Zhao; M. Zhu","School of Electrical Engineering and Computer Science, Pennsylvania State University, University Park, PA; School of Aerospace Engineering, Xi’an Jiaotong University, Xi’an, China; School of Electrical Engineering and Computer Science, Pennsylvania State University, University Park, PA; School of Electrical Engineering and Computer Science, Pennsylvania State University, University Park, PA","2021 29th Mediterranean Conference on Control and Automation (MED)","15 Jul 2021","2021","","","1209","1214","This paper studies optimal motion planning of multiple mobile robots with collision avoidance. We develop a distributed reinforcement learning algorithm which ensures suboptimal goal reaching and anytime collision avoidance simultaneously. Theoretical results on the convergence of neural network weights, the uniform and ultimate boundedness of system states of the closed-loop system, and anytime collision avoidance are established. Numerical simulations for single integrator and unicycle robots illustrate the effectiveness of our theoretical results.","2473-3504","978-1-6654-2258-1","10.1109/MED51440.2021.9480176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9480176","Safety;reinforcement learning;motion planning","Simulation;Neural networks;Reinforcement learning;Numerical simulation;Planning;Multi-robot systems;Mobile robots","closed loop systems;collision avoidance;learning (artificial intelligence);mobile robots;multi-robot systems;neural nets;numerical analysis;optimal control","single integrator robots;numerical simulations;system state ultimate boundedness;neural network weight convergence;anytime collision avoidance;distributed safe reinforcement learning algorithm;optimal multirobot motion planning;unicycle robots;closed-loop system;suboptimal goal reaching","","2","","21","IEEE","15 Jul 2021","","","IEEE","IEEE Conferences"
"Source Term Estimation Using Deep Reinforcement Learning With Gaussian Mixture Model Feature Extraction for Mobile Sensors","M. Park; P. Ladosz; H. Oh","Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Ulsan, South Korea; Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Ulsan, South Korea; Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Ulsan, South Korea","IEEE Robotics and Automation Letters","8 Jul 2022","2022","7","3","8323","8330","This paper proposes a deep reinforcement learning method for mobile sensors to estimate the properties of the source of the hazardous gas release. The problem of estimating the properties of the released gas is generally termed as the source term estimation (STE) problem. Since the sensor measurements from atmospheric gas dispersion are sparse, intermittent, and time-varying due to the turbulence and the sensor noise, STE is considered to be a challenging problem. The particle filter is adopted to estimate the source term under such stochastic noise conditions. The deep deterministic policy gradient (DDPG) is also employed to find the best source search policy in terms of successful estimation and traveled distance. Through ablation studies, we demonstrate that the use of the Gaussian mixture model, which clusters the potential source positions from the particle filter, as an input to the DDPG and the gated recurrent unit functioning as a memory in DDPG help to improve the STE performance. Besides, simulation results in randomized source term conditions and previously-unseen environments show the superior STE performance of the proposed algorithm compared with the existing information-theoretic STE algorithm.","2377-3766","","10.1109/LRA.2022.3184787","Basic Science Research Program; National Research Foundation of Korea; Ministry of Education(grant numbers:2020R1A6A1A03040570); National Research Foundation of Korea(grant numbers:2020R1F1A1049066); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9802788","Autonomous agents;machine learning for robot control;motion and path planning;reinforcement learning;robotics in hazardous fields","Sensors;Gas detectors;Atmospheric measurements;Particle filters;Feature extraction;Search problems;Robot sensing systems","deep learning (artificial intelligence);feature extraction;Gaussian processes;hazards;mixture models;particle filtering (numerical methods);recurrent neural nets;stochastic processes","Gaussian mixture model feature extraction;mobile sensors;deep reinforcement learning method;hazardous gas release;released gas;source term estimation problem;sensor measurements;atmospheric gas dispersion;sensor noise;particle filter;stochastic noise conditions;deep deterministic policy gradient;DDPG;source search policy;successful estimation;potential source positions;randomized source term conditions;superior STE performance;existing information-theoretic STE algorithm","","2","","20","IEEE","21 Jun 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Active Target Tracking","H. Jeong; H. Hassani; M. Morari; D. D. Lee; G. J. Pappas","Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, USA; Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, USA; Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, USA; Department of Electrical and Computer Engineering, Cornell University, Ithaca, NY, USA; Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","1825","1831","We solve active target tracking, one of the essential tasks in autonomous systems, using a deep reinforcement learning (RL) approach. In this problem, an autonomous agent is tasked with acquiring information about targets of interests using its on-board sensors. The classical challenges in this problem are system model dependence and the difficulty of computing information-theoretic cost functions for a long planning horizon. RL provides solutions for these challenges as the length of its effective planning horizon does not affect the computational complexity, and it drops the strong dependency of an algorithm on system models. In particular, we introduce Active Tracking Target Network (ATTN), a unified deep RL policy that is capable of solving major sub-tasks of active target tracking – in-sight tracking, navigation, and exploration. The policy shows robust behavior for tracking agile and anomalous targets with a partially known target model. Additionally, the same policy is able to navigate in obstacle environments to reach distant targets as well as explore the environment when targets are positioned in unexpected locations.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561258","","Target tracking;Navigation;Computational modeling;Conferences;Reinforcement learning;Cost function;Planning","control engineering computing;deep learning (artificial intelligence);mobile robots;multi-agent systems;target tracking","essential tasks;autonomous systems;deep reinforcement learning approach;autonomous agent;classical challenges;system model dependence;information-theoretic cost functions;long planning horizon;effective planning horizon;system models;unified deep RL policy;active target tracking;agile targets;anomalous targets;partially known target model;distant targets;active tracking target network","","2","","30","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Motion Planning Using Reinforcement Learning Method for Underactuated Ship Berthing","H. Zhang; C. Yin; Y. Zhang","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China","2020 IEEE 16th International Conference on Control & Automation (ICCA)","30 Nov 2020","2020","","","354","359","This paper proposes a novel motion planning method for underactuated ship berthing using reinforcement learning (RL) technique. The berthing motion planning problem is formulated as a Markov Decision Process, where a specified reward function is designed for the accurate berthing task. The problem is addressed by a state-of-art RL algorithm, Twin Delayed Deep Deterministic Policy Gradient (TD3). The generated trajectories are feasible for the ship to accomplish berthing task, since the system constraints are fully taken into consideration when the trained agent interacts with environment by RL. Simulation results verify the effectiveness of RL based motion planning method, and the advantage of TD3 is shown by comparison with Deep Deterministic Policy Gradient for the same task.","1948-3457","978-1-7281-9093-8","10.1109/ICCA51439.2020.9264562","Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264562","","Marine vehicles;Planning;Trajectory;Task analysis;Dynamics;Optimal control;Vehicle dynamics","gradient methods;learning (artificial intelligence);Markov processes;motion control;optimal control;path planning;ships","Twin Delayed Deep Deterministic Policy Gradient;RL based motion planning method;reinforcement learning method;underactuated ship berthing;reinforcement learning technique;berthing motion planning problem;Markov Decision Process;specified reward function;accurate berthing task;RL algorithm","","2","","25","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Concentric Tube Robot Control with a Goal-Based Curriculum","K. Iyengar; D. Stoyanov","Wellcome/ EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, UK; Wellcome/ EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, UK","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","1459","1465","Concentric Tube Robots (CTRs), a type of continuum robot, are a collection of concentric, pre-curved tubes composed of super elastic nickel titanium alloy. CTRs can bend and twist from the interactions between neighboring tubes causing the kinematics and therefore control of the end-effector to be very challenging to model. In this paper, we develop a control scheme for a CTR end-effector in Cartesian space with no prior kinematic model using a deep reinforcement learning (DRL) approach with a goal-based curriculum reward strategy. We explore the use of curricula by changing the goal tolerance through training with constant, linear and exponential decay functions. Also, relative and absolute joint representations as a way of improving training convergence are explored. Quantitative comparisons for combinations of curricula and joint representations are performed and the exponential decay relative approach is used for training a robust policy in a noise-induced simulation environment. Compared to a previous DRL approach, our new method reduces training time and employs a more complex simulation environment. We report mean Cartesian errors of 1.29 mm and a success rate of 0.93 with a relative decay curriculum. In path following, we report mean errors of 1.37 mm in a noise-induced path following task. Albeit in simulation, these results indicate the promise of using DRL in model free control of continuum robots and CTRs in particular.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561620","Royal Academy of Engineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561620","","Training;Titanium alloys;Shape;Reinforcement learning;Kinematics;End effectors;Robustness","bending;elasticity;learning (artificial intelligence);medical robotics;robot kinematics","concentric Tube robot control;CTRs;continuum robot;pre-curved tubes;superelastic nickel titanium alloy;CTR end-effector;Cartesian space;prior kinematic model;deep reinforcement learning approach;goal-based curriculum reward strategy;curricula;constant decay functions;linear decay functions;exponential decay functions;relative representations;absolute joint representations;training convergence;exponential decay relative approach;noise-induced simulation environment;complex simulation environment;Cartesian errors;relative decay curriculum","","2","","26","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Virtual Testing and Policy Deployment Framework for Autonomous Navigation of an Unmanned Ground Vehicle Using Reinforcement Learning","T. Lewis; P. Benavidez; M. Jamshidi","Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX 78249; Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX 78249; Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX 78249","2021 World Automation Congress (WAC)","8 Oct 2021","2021","","","317","322","The use of deep reinforcement learning (DRL) as a framework for training a mobile robot to perform optimal navigation in an unfamiliar environment is a suitable choice for implementing AI with real-time robotic systems. In this study, the environment and surrounding obstacles of an Ackermann-steered UGV are reconstructed into a virtual setting for training the UGV to centrally learn the optimal route (guidance actions to be taken at any given state) towards a desired goal position using Multi-Agent Virtual Exploration in Deep Q-Learning (MVEDQL) for various model configurations. The trained model policies are to be transferred to a physical vehicle and compared based on their individual effectiveness for performing autonomous waypoint navigation. Prior to incorporating the learned model with the physical UGV for testing, this paper outlines the development of a GUI application to provide an interface for remotely deploying the vehicle and a virtual reality framework reconstruction of the training environment to assist safely testing the system using the reinforcement learning model.","2154-4824","978-1-68524-111-7","10.23919/WAC50355.2021.9559586","Air Force Research Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9559586","Reinforcement Learning;Virtual Reality;Autonomous Car;Simulation;Multi-Agent Systems;Exploration","Training;Solid modeling;Navigation;Reinforcement learning;Virtual reality;Real-time systems;Land vehicles","graphical user interfaces;learning (artificial intelligence);mobile robots;multi-agent systems;navigation;remotely operated vehicles;virtual reality","Virtual testing;policy deployment framework;autonomous navigation;unmanned ground vehicle;deep reinforcement learning;DRL;mobile robot;optimal navigation;unfamiliar environment;AI;real-time robotic systems;surrounding obstacles;Ackermann-steered UGV;virtual setting;optimal route;guidance actions;desired goal position;MultiAgent Virtual Exploration;Deep Q-Learning;model configurations;trained model policies;physical vehicle;individual effectiveness;autonomous waypoint navigation;physical UGV;virtual reality framework reconstruction;training environment;reinforcement learning model","","2","","10","","8 Oct 2021","","","IEEE","IEEE Conferences"
"Integrating symmetry of environment by designing special basis functions for value function approximation in reinforcement learning","G. -f. Wang; Z. Fang; B. Li; P. Li","School of Aeronautics and Astronautics, Zhejiang University, Hangzhou, ZJU, China; School of Aeronautics and Astronautics, Zhejiang University, Hangzhou, ZJU, China; School of Aeronautics and Astronautics, Zhejiang University, Hangzhou, ZJU, China; Department of Control Science and Engineering, Zhejiang University, Hangzhou, ZJU, China","2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV)","2 Feb 2017","2016","","","1","6","Reinforcement learning (RL) is usually regarded as tabula rasa learning, and the agent needs to randomly explore the environment, so the time consuming and data inefficiency will hinder RL from the real application. In order to accelerate learning speed and improve data efficiency, in this paper we expand the symmetry definition from finite state space to infinite state space and then propose designing a special type of symmetric basis functions for value function approximation to integrate the prior knowledge of symmetry about the environment for large or even infinite state space. After that, as an example, this particular approximate structure is incorporated into the policy evaluation phase of Least-Square Policy Iteration (LSPI), which we call symmetric LSPI (S-LSPI) and the convergence property is analyzed. Simulation results of chain walk and inverted pendulum balancing demonstrate that in contrast with regular LSPI (R-LSPI), the convergence speed of S-LSPI increases greatly and the computational burden decreases significantly simultaneously. It can illustrate the use of symmetric basis functions to capture the property of symmetry very well, and as a case study, it shows the promise to integrate symmetry of environment into RL agent.","","978-1-5090-3549-6","10.1109/ICARCV.2016.7838691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838691","Reinforcement Learning;Symmetric basis functions;Value Function Approximation;S-LSPI","Aerospace electronics;Function approximation;Learning (artificial intelligence);Convergence;Complexity theory;Synthetic aperture sonar;Robots","convergence;function approximation;iterative methods;learning (artificial intelligence);least squares approximations;multi-agent systems;state-space methods","RL agent;symmetric basis functions;regular LSPI;R-LSPI;inverted pendulum balancing;chain walk;convergence property;S-LSPI;symmetric LSPI;least-square policy iteration;policy evaluation phase;infinite state space;finite state space;data efficiency;learning speed;data inefficiency;tabula rasa learning;reinforcement learning;value function approximation;environment symmetry","","2","","24","IEEE","2 Feb 2017","","","IEEE","IEEE Conferences"
"On Decentralizing Federated Reinforcement Learning in Multi-Robot Scenarios","J. S. Nair; D. D. Kulkarni; A. Joshi; S. Suresh","Dept. of Computer Science and Engg, Federal Institute of Science and Technology, Angamaly, India; Dept. of Computer Science and Engg, Indian Institute of Technology Guwahati, Guwahati, India; Dept. of Computer Science and Engg, Indian Institute of Technology Guwahati, Guwahati, India; Dept. of Computer Science and Engg, Federal Institute of Science and Technology, Angamaly, India","2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM)","1 Nov 2022","2022","","","1","8","Federated Learning (FL) allows for collaboratively aggregating learned information across several computing devices and sharing the same amongst them, thereby tackling issues of privacy and the need of huge bandwidth. FL techniques generally use a central server or cloud for aggregating the models received from the devices. Such centralized FL techniques suffer from inherent problems such as failure of the central node and bottlenecks in channel bandwidth. When FL is used in conjunction with connected robots serving as devices, a failure of the central controlling entity can lead to a chaotic situation. This paper describes a mobile agent based paradigm to decentralize FL in multi-robot scenarios. Using Webots, a popular free open-source robot simulator, and Tartarus, a mobile agent platform, we present a methodology to decentralize federated learning in a set of connected robots. With Webots running on different connected computing systems, we show how mobile agents can perform the task of Decentralized Federated Reinforcement Learning (dFRL). Results obtained from experiments carried out using Q-learning and SARSA by aggregating their corresponding Q-tables, show the viability of using decentralized FL in the domain of robotics. Since the proposed work can be used in conjunction with other learning algorithms and also real robots, it can act as a vital tool for the study of decentralized FL using heterogeneous learning algorithms concurrently in multi-robot scenarios.","","979-8-3503-9858-8","10.1109/SEEDA-CECNSM57760.2022.9932985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9932985","Decentralized Federated Learning;Reinforcement Learning;Networked Robots;Mobile Agents","Privacy;Q-learning;Federated learning;Social networking (online);Mobile agents;Europe;Bandwidth","learning (artificial intelligence);mobile agents;mobile robots;multi-agent systems;multi-robot systems","mobile agent platform;connected robots;connected computing systems;Q-learning;decentralized FL;heterogeneous learning algorithms;multirobot scenarios;learned information;computing devices;huge bandwidth;central server;centralized FL techniques;central node;channel bandwidth;central controlling entity;popular free open-source robot simulator;decentralized federated reinforcement learning","","2","","28","IEEE","1 Nov 2022","","","IEEE","IEEE Conferences"
"Gliding Control of Underwater Gliding Snake-Like Robot Based on Reinforcement Learning","X. -L. Zhang; B. Li; J. Chang; J. -G. Tang","College of Information Science and Engineering, Northeastern University, Shenyang, China; State Key Laboratory of Robotics, Chinese Academy of Sciences, Shenyang, China; State Key Laboratory of Robotics, Chinese Academy of Sciences, Shenyang, China; State Key Laboratory of Robotics, Chinese Academy of Sciences, Shenyang, China","2018 IEEE 8th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","11 Apr 2019","2018","","","323","328","The control of the gliding action of underwater gliding snake-like robot is mainly studied in this paper. In order to solve the problem that the hydrodynamic environment is hard to model, the method of reinforcement learning is used to make the underwater gliding snake-like robot adapt to the underwater environment and automatically learn the gliding actions. A modified Monte Carlo policy gradient algorithm using pre-processed neural network is proposed to solve the problem that the states of the robot are difficult to be observed completely due to its complex structure. The gliding control problem of the underwater gliding snake-like robot can be approximated as a Markov Decision Process, so as to obtain an effective gliding control policy. Simulation results show the effectiveness of the proposed method.","2379-7711","978-1-5386-7057-6","10.1109/CYBER.2018.8688282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8688282","","","approximation theory;control engineering computing;gradient methods;learning (artificial intelligence);Markov processes;mobile robots;Monte Carlo methods;neurocontrollers","reinforcement learning;gliding action;underwater environment;gliding control problem;underwater gliding snake-like robot;hydrodynamic environment;modified Monte Carlo policy gradient algorithm;Markov decision process","","2","","14","IEEE","11 Apr 2019","","","IEEE","IEEE Conferences"
"Flexible online adaptation of learning strategy using EEG-based reinforcement signals in real-world robotic applications","S. K. Kim; E. Andrea Kirchner; F. Kirchner","Robotics Innovation Center (RIC), German Research Center for Artificial Intelligence (DFKI), Bremen, Germany; Robotics Innovation Center (RIC), German Research Center for Artificial Intelligence (DFKI), Bremen, Germany; Robotics Innovation Center (RIC), German Research Center for Artificial Intelligence (DFKI), Bremen, Germany","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","4885","4891","Flexible adaptation of learning strategy depending on online changes of the user's current intents have a high relevance in human-robot collaboration. In our previous study, we proposed an intrinsic interactive reinforcement learning approach for human-robot interaction, in which a robot learns his/her action strategy based on intrinsic human feedback that is generated in the human's brain as neural signature of the human's implicit evaluation of the robot's actions. Our approach has an inherent property that allows robots to adapt their behavior depending on online changes of the human's current intents. Such flexible adaptation is possible, since robot learning is updated in real time by human's online feedback. In this paper, the adaptivity of robot learning is tested on eight subjects who change their current control strategy by adding a new gesture to the previous used gestures. This paper evaluates the learning progress by analyzing learning phases (before and after adding a new gesture for control). The results show that the robot can adapt the previously learned policy depending on online changes of the user's intents. Especially, learning progress is interrelated with the classification performance of electroencephalograms (EEGs), which are used to measure the human's implicit evaluation of the robot's actions.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197538","","Electroencephalography;Decoding;Electronic learning;Task analysis;Human-robot interaction;Robot learning","electroencephalography;human-robot interaction;learning (artificial intelligence);medical robotics;medical signal processing","learned policy;learning phases;current control strategy;robot learning;intrinsic human feedback;human-robot interaction;intrinsic interactive reinforcement learning approach;human-robot collaboration;flexible adaptation;real-world robotic applications;reinforcement signals;flexible online adaptation;learning progress","","2","","19","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Efficient, swarm-based path finding in unknown graphs using reinforcement learning","M. Aurangzeb; F. L. Lewis; M. Huber","Arlington Research Institute, University of Texas, Fort Worth, TX, USA; Arlington Research Institute, University of Texas, Fort Worth, TX, USA; Department of Computer Science and Engineering, University of Texas, Arlington, TX, USA","2013 10th IEEE International Conference on Control and Automation (ICCA)","22 Jul 2013","2013","","","870","877","This paper addresses the problem of steering a swarm of autonomous agents out of an unknown maze to some goal located at an unknown location. This is particularly the case in situations where no direct communication between the agents is possible and all information exchange between agents has to occur indirectly through information “deposited” in the environment. To address this task, an ε-greedy collaborative reinforcement learning method using only local information exchanges is introduced in this paper to balance exploitation and exploration in the unknown maze and to optimize the ability of the swarm to exit from the maze. The learning and routing algorithm given here provides a mechanism for storing data needed to represent the collaborative utility function based on the experiences of previous agents visiting a node that results in routing decisions that improve with time. Two theorems show the theoretical soundness of the proposed learning method and illustrate the importance of the stored information in improving decision-making for routing. Simulation examples show that the introduced simple rules of learning from past experience significantly improve performance over random search and search based on Ant Colony Optimization, a metaheuristic algorithm.","1948-3457","978-1-4673-4708-2","10.1109/ICCA.2013.6564940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564940","","Routing;Robots;Learning (artificial intelligence);Peer-to-peer computing;Vectors;Collaboration;Heuristic algorithms","ant colony optimisation;greedy algorithms;groupware;learning (artificial intelligence);multi-agent systems","swarm-based path finding;autonomous agents;ε-greedy collaborative reinforcement learning method;local information exchanges;algorithm;collaborative utility function;random search;ant colony optimization","","2","","46","IEEE","22 Jul 2013","","","IEEE","IEEE Conferences"
"Safe Reinforcement Learning Using Black-Box Reachability Analysis","M. Selim; A. Alanwar; S. Kousik; G. Gao; M. Pavone; K. H. Johansson","Ain Shams University, Cairo, Egypt; Jacobs University, Bremen, Germany; Stanford University, Stanford, CA, USA; Stanford University, Stanford, CA, USA; Stanford University, Stanford, CA, USA; KTH Royal Institute of Technology, Stockholm, Sweden","IEEE Robotics and Automation Letters","8 Aug 2022","2022","7","4","10665","10672","Reinforcement learning (RL) is capable of sophisticated motion planning and control for robots in uncertain environments. However, state-of-the-art deep RL approaches typically lack safety guarantees, especially when the robot and environment models are unknown. To justify widespread deployment, robots must respect safety constraints without sacrificing performance. Thus, we propose a Black-box Reachability-based Safety Layer (BRSL) with three main components: (1) data-driven reachability analysis for a black-box robot model, (2) a trajectory rollout planner that predicts future actions and observations using an ensemble of neural networks trained online, and (3) a differentiable polytope collision check between the reachable set and obstacles that enables correcting unsafe actions. In simulation, BRSL outperforms other state-of-the-art safe RL methods on a Turtlebot 3, a quadrotor, a trajectory-tracking point mass, and a hexarotor in wind with an unsafe set adjacent to the area of highest reward.","2377-3766","","10.1109/LRA.2022.3192205","Vetenskapsrådet; Knut och Alice Wallenbergs Stiftelse; Toyota Research Institute; NASA University Leadership Initiative(grant numbers:80NSSC20M0163); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833266","Reinforcement learning;robot safety;task and motion planning","Safety;Robots;Navigation;Collision avoidance;Trajectory;Planning;Training","collision avoidance;control engineering computing;learning (artificial intelligence);mobile robots;neural nets;reachability analysis;safety;trajectory control","differentiable polytope collision check;BRSL;safe RL methods;safe reinforcement learning;motion planning;uncertain environments;safety guarantees;environment models;safety constraints;Black-box Reachability-based Safety Layer;black-box robot model;trajectory rollout planner;black-box reachability analysis","","2","","55","IEEE","19 Jul 2022","","","IEEE","IEEE Journals"
"Uncertainty-Aware Model-Based Offline Reinforcement Learning for Automated Driving","C. Diehl; T. S. Sievernich; M. Krüger; F. Hoffmann; T. Bertram","Institute of Control Theory and Systems Engineering, TU Dortmund University, Dortmund, Germany; Institute of Control Theory and Systems Engineering, TU Dortmund University, Dortmund, Germany; Institute of Control Theory and Systems Engineering, TU Dortmund University, Dortmund, Germany; Institute of Control Theory and Systems Engineering, TU Dortmund University, Dortmund, Germany; Institute of Control Theory and Systems Engineering, TU Dortmund University, Dortmund, Germany","IEEE Robotics and Automation Letters","20 Jan 2023","2023","8","2","1167","1174","Offline reinforcement learning (RL) provides a framework for learning decision-making from offline data and therefore constitutes a promising approach for real-world applications such as automated driving (AD). Especially in safety-critical applications, interpretability and transferability are crucial to success. That motivates model-based offline RL approaches, which leverage planning. However, current state-of-the-art (SOTA) methods often neglect the influence of aleatoric uncertainty arising from the stochastic behavior of multi-agent systems. Further, while many algorithms state that they are suitable for AD, there is still a lack of evaluation in challenging scenarios. This work proposes a novel approach for Uncertainty-aware Model-Based Offline REinforcement Learning Leveraging plAnning (UMBRELLA), which jointly solves the prediction, planning, and control problem of the self-driving vehicle (SDV) in an interpretable learning-based fashion. A trained action-conditioned stochastic dynamics model captures distinctively different future evolutions of the traffic scene. The analysis provides empirical evidence for the effectiveness of our approach and SOTA performance in challenging AD simulations and using a real-world public dataset.","2377-3766","","10.1109/LRA.2023.3236579","Federal Ministry for Economic Affairs and Climate Action; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015868","Human-aware motion planning;intelligent transportation systems;machine learning for robot control;planning under uncertainty;reinforcement learning","Planning;Uncertainty;Behavioral sciences;Stochastic processes;Predictive models;Reinforcement learning;Vehicle dynamics","decision making;intelligent transportation systems;multi-agent systems;reinforcement learning;stochastic processes;uncertainty handling","aleatoric uncertainty;automated driving;interpretable learning-based fashion;motivates model-based;multiagent systems;offline data;real-world applications;RL approaches;safety-critical applications;self-driving vehicle;stochastic behavior;traffice scene;trained action-conditioned stochastic dynamics model;uncertainty-aware model-based offline reinforcement learning leveraging planning","","2","","33","IEEE","12 Jan 2023","","","IEEE","IEEE Journals"
"Hierarchical reinforcement learning and decision making for intelligent machines","P. Lima; G. Saridis","Electrical, Computer and Systems Engineering Department, Rensselaer Polytechnic Institute, Troy, NY, USA; Electrical, Computer and Systems Engineering Department, Rensselaer Polytechnic Institute, Troy, NY, USA","Proceedings of the 1994 IEEE International Conference on Robotics and Automation","6 Aug 2002","1994","","","33","38 vol.1","A methodology for performance improvement of intelligent machines based on hierarchical reinforcement learning is introduced. Machine decision making and learning are based on a cost function which includes reliability and a computational cost of algorithms at the three levels of the hierarchy proposed by Saridis. Despite this particular formalization, the methodology intends to be sufficiently general to encompass different types of architectures and applications. Novel contributions of this work include the definition of a cost function combining reliability and complexity, recursively improved through feedback, a hierarchical reinforcement learning and decision making algorithm which uses that cost function, and a methodology supported on information-based complexity for joint measure of algorithm cost and reliability. Results of simulations show the application of the formalism to intelligent robotic systems.<>","","0-8186-5330-2","10.1109/ROBOT.1994.351014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=351014","","Decision making;Cost function;Intelligent robots;Computational intelligence;Learning systems;Machine learning;Computational efficiency;Computer architecture;Feedback;Intelligent systems","unsupervised learning;inference mechanisms;feedback;reliability","hierarchical reinforcement learning;decision making;intelligent machines;performance improvement;cost function;reliability;feedback;information-based complexity;algorithm cost;intelligent robotic systems","","2","","10","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Deriving a near-optimal power management policy using model-free reinforcement learning and Bayesian classification","Y. Wang; Q. Xie; A. Ammari; M. Pedram","Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA; Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA; National Institute of the Applied Sciences and of the Technology (INSAT), Tunis, Tunisia; Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA","2011 48th ACM/EDAC/IEEE Design Automation Conference (DAC)","11 Aug 2011","2011","","","41","46","To cope with the variations and uncertainties that emanate from hardware and application characteristics, dynamic power management (DPM) frameworks must be able to learn about the system inputs and environment and adjust the power management policy on the fly. In this paper we present an online adaptive DPM technique based on model-free reinforcement learning (RL), which is commonly used to control stochastic dynamical systems. In particular, we employ temporal difference learning for semi-Markov decision process (SMDP) for the model-free RL. In addition a novel workload predictor based on an online Bayes classifier is presented to provide effective estimates of the workload states for the RL algorithm. In this DPM framework, power and latency tradeoffs can be precisely controlled based on a user-defined parameter. Experiments show that amount of average power saving (without any increase in the latency) is up to 16.7% compared to a reference expert-based approach. Alternatively, the per-request latency reduction without any power consumption increase is up to 28.6% compared to the expert-based approach.","85-644924","978-1-4503-0636-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5981919","Dynamic Power Management;Bayes Classification;Reinforcement Learning","Strontium;Markov processes;Power demand;Prediction algorithms;Classification algorithms;Learning","Bayes methods;learning (artificial intelligence);Markov processes;pattern classification;power aware computing","near-optimal power management policy;model-free reinforcement learning;Bayesian classification;dynamic power management frameworks;stochastic dynamical systems;temporal difference learning;semiMarkov decision process;online Bayes classifier","","1","","14","","11 Aug 2011","","","IEEE","IEEE Conferences"
"Proficiency Constrained Multi-Agent Reinforcement Learning for Environment-Adaptive Multi UAV-UGV Teaming","Q. Yu; Z. Shen; Y. Pang; R. Liu","Cognitive Robotics and AI Lab (CRAI), College of Aeronautics and Engineering, Kent State University, Kent, OH, USA; Cognitive Robotics and AI Lab (CRAI), College of Aeronautics and Engineering, Kent State University, Kent, OH, USA; Cognitive Robotics and AI Lab (CRAI), College of Aeronautics and Engineering, Kent State University, Kent, OH, USA; Cognitive Robotics and AI Lab (CRAI), College of Aeronautics and Engineering, Kent State University, Kent, OH, USA","2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)","5 Oct 2021","2021","","","2114","2118","A mixed aerial and ground robot team, which includes both unmanned ground vehicles (UGVs) and unmanned aerial vehicles (UAVs), is widely used for disaster rescue, social security, precision agriculture, and military missions. However, team capability and corresponding configuration vary since robots have different motion speeds, perceiving ranges, reaching areas, and resilient capabilities to the dynamic environment. Due to heterogeneous robots inside a team and the resilient capabilities of robots, it is challenging to perform a task with an optimal balance between reasonable task allocations and maximum utilization of robot capability. To address this challenge for effective mixed ground and aerial teaming, this paper developed a novel teaming method, proficiency aware multi-agent deep reinforcement learning (Mix-RL), to guide ground and aerial cooperation by considering the best alignments between robot capabilities, task requirements, and environment conditions. Mix-RL largely exploits robot capabilities while being aware of the adaption of robot capabilities to task requirements and environment conditions. Mix-RL's effectiveness in guiding mixed teaming was validated with the task “social security for criminal vehicle tracking”.","2161-8089","978-1-6654-1873-7","10.1109/CASE49439.2021.9551457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551457","","Target tracking;Scalability;Reinforcement learning;Unmanned aerial vehicles;Land vehicles;Security;Resource management","autonomous aerial vehicles;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;path planning;remotely operated vehicles","proficiency constrained multiagent reinforcement learning;environment-adaptive multiUAV-UGV teaming;mixed aerial;unmanned ground vehicles;unmanned aerial vehicles;social security;team capability;resilient capabilities;heterogeneous robots;robot capability;effective mixed ground;aerial teaming;novel teaming method;aware multiagent deep reinforcement learning;aerial cooperation;task requirements;environment conditions;Mix-RL's effectiveness;mixed teaming","","1","","30","IEEE","5 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Adaptive Biofeedback Engine for Overground Walking Speed Training","H. Zhang; S. Li; Q. Zhao; A. K. Rao; Y. Guo; D. Zanotto","Department of Mechanical Engineering, Stevens Institute of Technology, Hoboken, NJ, USA; Department of Mechanical Engineering, Stevens Institute of Technology, Hoboken, NJ, USA; Department of Mechanical Engineering, Stevens Institute of Technology, Hoboken, NJ, USA; Department of Rehabilitation and Regenerative Medicine, Columbia University, New York, NY, USA; Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA; Department of Mechanical Engineering, Stevens Institute of Technology, Hoboken, NJ, USA","IEEE Robotics and Automation Letters","12 Jul 2022","2022","7","3","8487","8494","Wearable biofeedback systems (WBS) have been proposed to aid physical rehabilitation of individuals with motor impairments. Due to significant inter- and intra-individual differences, the effectiveness of a given biofeedback strategy may vary for different users and across therapeutic sessions, as a patient’s functional recovery progresses. To date, only a paucity of research has investigated the use of biofeedback strategies that can self-adapt based on the user’s response. This letter introduces a novel reinforcement learning with fuzzy logic biofeedback engine (RLFLE) for personalized overground walking speed training. The method leverages reinforcement learning and a fuzzy inference strategy to continuously modulate underfoot vibrotactile stimuli that encourage users to achieve a target walking speed. This stimulation strategy also enables the determination of a user’s maximum steady-state walking speed during a gait training session overground. The RLFLE was implemented in a custom-engineered WBS and validated against two simpler biofeedback strategies during walking tests with healthy adults. Participants showed lower walking speed errors when training with the RLFLE. Additionally, results indicate that the new method is more effective in determining an individual’s maximum steady-state walking speed. Given the importance of walking speed as an indicator of health status and as an essential outcome of exercise-based interventions, these results show promise for implementation in future technology-enhanced gait rehabilitation protocols.","2377-3766","","10.1109/LRA.2022.3187616","National Science Foundation(grant numbers:IIS-1838799,IIS-1838725,CMMI-1944203); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811338","Wearable biofeedback system;instrumented footwear;human-in-the-loop;reinforcement learning;fuzzy logic;gait training","Legged locomotion;Biological control systems;Training;Engines;Fuzzy logic;Task analysis;Integrated circuits","feedback;fuzzy logic;gait analysis;learning (artificial intelligence);medical computing;patient rehabilitation;patient treatment","reinforcement learning-based adaptive biofeedback engine;wearable biofeedback systems;motor impairments;intra-individual differences;novel reinforcement;fuzzy logic biofeedback engine;RLFLE;personalized overground walking speed training;reinforcement learning;fuzzy inference strategy;underfoot vibrotactile stimuli;target walking speed;gait training session;custom-engineered WBS;exercise-based interventions;technology-enhanced gait rehabilitation protocols;walking speed errors;biofeedback strategies","","1","","42","IEEE","30 Jun 2022","","","IEEE","IEEE Journals"
"Robotic Table Wiping via Reinforcement Learning and Whole-body Trajectory Optimization","T. Lew; S. Singh; M. Prats; J. Bingham; J. Weisz; B. Holson; X. Zhang; V. Sindhwani; Y. Lu; F. Xia; P. Xu; T. Zhang; J. Tan; M. Gonzalez",Robotics at Google; Robotics at Google; Everyday Robots; Everyday Robots; Everyday Robots; Everyday Robots; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google,"2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","7184","7190","We propose a framework to enable multipurpose assistive mobile robots to autonomously wipe tables to clean spills and crumbs. This problem is challenging, as it requires planning wiping actions while reasoning over uncertain latent dynamics of crumbs and spills captured via high-dimensional visual observations. Simultaneously, we must guarantee constraints satisfaction to enable safe deployment in unstructured cluttered environments. To tackle this problem, we first propose a stochastic differential equation to model crumbs and spill dynamics and absorption with a robot wiper. Using this model, we train a vision-based policy for planning wiping actions in simulation using reinforcement learning (RL). To enable zero-shot sim-to-real deployment, we dovetail the RL policy with a whole-body trajectory optimization framework to compute base and arm joint trajectories that execute the desired wiping motions while guaranteeing constraints satisfaction. We extensively validate our approach in simulation and on hardware. Video of experiments: https://youtu.be/inORKP4F3EI","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161283","","Analytical models;Visualization;Computational modeling;Reinforcement learning;Mathematical models;Hardware;Data models","differential equations;learning (artificial intelligence);mobile robots;reinforcement learning","arm joint trajectories;constraints satisfaction;desired wiping motions;high-dimensional visual observations;model crumbs;multipurpose assistive mobile robots;reinforcement learning;RL policy;robot wiper;robotic table wiping;safe deployment;spills;stochastic differential equation;uncertain latent dynamics;unstructured cluttered environments;vision-based policy;whole-body trajectory optimization framework;wiping actions;zero-shot sim-to-real deployment","","1","","52","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Adaptive Formation Framework with Obstacle Avoidance Based on Reinforcement Learning","S. Liu; M. Wang; Y. Feng; J. Sun; Y. Ji","School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","236","241","Multi-robot formation control is a control technology that enables robots to maintain a specific formation and adapt to environmental constraints during task execution. In practical application, it is important to maintain structure and adaptively avoid obstacles. Therefore, a multi-robot adaptive formation obstacle avoidance framework is proposed in this paper. There are two pieces to this framework. An obstacle avoidance navigation algorithm built on reinforcement learning makes up the first section. Additionally, we optimize the system’s reward function using the artificial potential field method (APF). In the second part, we design a new formation control strategy based on the leader-follower model and propose the virtual obstacle avoidance point (VOAP) to solve the overlapping problem between virtual target points and obstacles in the leader-follower model. The results of the simulations demonstrate that the proposed formation obstacle avoidance framework can make unmanned ground vehicles (UGV) carry out formation keeping and adaptive obstacle avoidance independently.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055718","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055718","multi-robot systems;leader-follower formation control;reinforcement learning;unmanned ground vehicles","Training;Adaptation models;Navigation;Simulation;Reinforcement learning;Formation control;Land vehicles","collision avoidance;mobile robots;multi-robot systems;reinforcement learning;remotely operated vehicles","adaptive formation framework;adaptive obstacle avoidance;control technology;formation control strategy;leader-follower model;multirobot adaptive formation obstacle avoidance framework;multirobot formation control;obstacle avoidance navigation algorithm;reinforcement learning;specific formation;virtual obstacle avoidance point","","1","","24","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Online Area Covering Autonomous Robot","O. Saha; G. Ren; J. Heydari; V. Ganapathy; M. Shah","Advanced AI Team, LG America Research Lab, Santa Clara, USA; Advanced AI Team, LG America Research Lab, Santa Clara, USA; Advanced AI Team, LG America Research Lab, Santa Clara, USA; Advanced AI Team, LG America Research Lab, Santa Clara, USA; Advanced AI Team, LG America Research Lab, Santa Clara, USA","2021 7th International Conference on Automation, Robotics and Applications (ICARA)","17 Mar 2021","2021","","","21","25","Autonomous area covering robots have been increasingly adopted in for diverse applications. In this paper, we investigate the effectiveness of deep reinforcement learning (RL) algorithms for online area coverage while minimizing the overlap. Through simulation experiments in grid based environments and in the Gazebo simulator, we show that Deep Q-Network (DQN) based algorithms efficiently cover unknown indoor environments. Furthermore, through empirical evaluations and theoretical analysis, we demonstrate that DQN with prioritized experience replay (DQN-PER) significantly minimizes the sample complexity while achieving reduced overlap when compared with other DQN variants. In addition, through simulations we demonstrate the performance advantage of DQN-PER over the state-of-the-art area coverage algorithms, BA* and BSA. Our experiments also indicate that a pre-trained RL agent can efficiently cover new unseen environments with minimal additional sample complexity. Finally, we propose a novel way of formulating the state representation to arrive at an area-agnostic RL agent for efficiently covering unknown environments.","","978-1-6654-0469-3","10.1109/ICARA51699.2021.9376477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376477","Autonomous Agents;Motion and Path Planning;Deep Learning Methods","Training;Learning systems;Reinforcement learning;Complexity theory;Indoor environment;Task analysis;Autonomous robots","control engineering computing;learning (artificial intelligence);mobile robots;path planning","area-agnostic RL agent;minimal additional sample complexity;unseen environments;pre-trained RL agent;state-of-the-art area coverage algorithms;DQN-PER;DQN variants;prioritized experience replay;empirical evaluations;unknown indoor environments;Deep Q-Network based algorithms;Gazebo simulator;grid based environments;simulation experiments;online area coverage;deep reinforcement learning algorithms;autonomous area;online area covering autonomous robot","","1","","29","IEEE","17 Mar 2021","","","IEEE","IEEE Conferences"
"Unified Data Collection for Visual-Inertial Calibration via Deep Reinforcement Learning","Y. Ao; L. Chen; F. Tschopp; M. Breyer; R. Siegwart; A. Cramariuc","Autonomous Systems Lab, ETH Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Switzerland; Arrival Ltd, United Kingdom; Autonomous Systems Lab, ETH Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Switzerland","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","1646","1652","Visual-inertial sensors have a wide range of applications in robotics. However, good performance often requires different sophisticated motion routines to accurately calibrate camera intrinsics and inter-sensor extrinsics. This work presents a novel formulation to learn a motion policy to be executed on a robot arm for automatic data collection for calibrating intrinsics and extrinsics jointly. Our approach models the calibration process compactly using model-free deep reinforcement learning to derive a policy that guides the motions of a robotic arm holding the sensor to efficiently collect measurements that can be used for both camera intrinsic calibration and camera-IMU extrinsic calibration. Given the current pose and collected measurements, the learned policy generates the subsequent transformation that optimizes sensor calibration accuracy. The evaluations in simulation and on a real robotic system show that our learned policy generates favorable motion trajectories and collects enough measurements efficiently that yield the desired intrinsics and extrinsics with short path lengths. In simulation, we are able to perform calibrations 10× faster than hand-crafted policies, which transfers to a real-world speed up of 3× over a human expert. The code of this work is publicly available at: https://github.com/ethz-asl/Learn-to-Calibrate.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811629","ETH Mobility Initiative; European Union's Horizon 2020 research and innovation programme(grant numbers:101017008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811629","","Current measurement;Robot vision systems;Reinforcement learning;Data collection;Manipulators;Cameras;Throughput","calibration;cameras;learning (artificial intelligence);robot vision","robotic system show;learned policy;favorable motion trajectories;desired intrinsics;hand-crafted policies;unified data collection;visual-inertial calibration;visual-inertial sensors;robotics;different sophisticated motion routines;camera intrinsics;inter-sensor extrinsics;motion policy;robot arm;automatic data collection;approach models;calibration process;model-free deep reinforcement learning;robotic arm;camera intrinsic calibration;optimizes sensor;calibration accuracy","","1","","22","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Autonomous Two-Stage Object Retrieval Using Supervised and Reinforcement Learning","T. Rouillard; I. Howard; L. Cui","School of Civil and Mechanical Engineering, Curtin University Bentley, Perth, WA, Australia; School of Civil and Mechanical Engineering, Curtin University Bentley, Perth, WA, Australia; School of Civil and Mechanical Engineering, Curtin University Bentley, Perth, WA, Australia","2019 IEEE International Conference on Mechatronics and Automation (ICMA)","29 Aug 2019","2019","","","780","786","Humans have been sending tele-operated robots into hazardous areas in an attempt to preserve life for many years. The task they are presented with is often challenging and requires cognitive abilities, that is, the ability to process information in order to apply it to a different situation. In this work, we proposed an autonomous approach employing both supervised and reinforcement learning for hidden-object retrieval in two stages. Stage 1 used both learning methods to find a hidden object whereas stage 2 only used reinforcement learning to isolate it. The method is targeted towards field robots with reduced computational power hence we explored the viability of tabular reinforcement learning algorithms. We used a convolutional neural network (CNN) to interpret the state of the scene from images and a reinforcement learning agent for each stage of the task. The robot was presented with a workspace containing piles of rubble under one of which an object was buried. The robot must learn to find and isolate it. We compared the performance of four reinforcement learning algorithms over 500 episodes and found that Sarsa (λ) and RMax were most appropriate for stage 1 and 2 respectively. The approach allowed a robot to learn to complete a search and retrieval task by interpreting images. This could lead to the deployment of such robots in disaster areas eliminating the need for tele-operated platforms.","2152-744X","978-1-7281-1699-0","10.1109/ICMA.2019.8816290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816290","Intelligent robots;Reinforcement learning;Computer vision;Robotic Manipulation","Reinforcement learning;Task analysis;Search problems;Three-dimensional displays;Convolutional neural networks;Robot sensing systems","convolutional neural nets;information retrieval;mobile robots;robot vision;stereo image processing;supervised learning;telerobotics","autonomous two-stage object retrieval;cognitive abilities;hidden-object retrieval;reduced computational power;tabular reinforcement learning algorithms;convolutional neural network;supervised learning;robots tele-operation","","1","","20","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Meta-Adversarial Inverse Reinforcement Learning for Decision-making Tasks","P. Wang; H. Li; C. -Y. Chan","University of California, Berkeley; Google Research; University of California, Berkeley","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","12632","12638","Learning from demonstrations has made great progress over the past few years. However, it is generally data hungry and task specific. In other words, it requires a large amount of data to train a decent model on a particular task, and the model often fails to generalize to new tasks that have a different distribution. In practice, demonstrations from new tasks will be continuously observed and the data might be unlabeled or only partially labeled. Therefore, it is desirable for the trained model to adapt to new tasks that have limited data samples available. In this work, we build an adaptable imitation learning model based on the integration of Meta-learning and Adversarial Inverse Reinforcement Learning (Meta-AIRL). We exploit the adversarial learning and inverse reinforcement learning mechanisms to learn policies and reward functions simultaneously from available training tasks and then adapt them to new tasks with the meta-learning framework. Simulation results show that the adapted policy trained with Meta-AIRL can effectively learn from limited number of demonstrations, and quickly reach the performance comparable to that of the experts on unseen tasks.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561330","","Training;Adaptation models;Simulation;Frequency-domain analysis;Conferences;Decision making;Reinforcement learning","decision making;reinforcement learning","adapted policy;Meta-AIRL;meta-adversarial inverse reinforcement learning;decision making tasks;adaptable imitation learning model;learning from demonstrations;reward functions","","1","","23","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Robust Adaptive Ensemble Adversary Reinforcement Learning","P. Zhai; T. Hou; X. Ji; Z. Dong; L. Zhang","Academy for Engineering and Technology, Fudan University, Shanghai, China; Academy for Engineering and Technology, Fudan University, Shanghai, China; State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China; Academy for Engineering and Technology, Fudan University, Shanghai, China; Academy for Engineering and Technology, Fudan University, Shanghai, China","IEEE Robotics and Automation Letters","15 Nov 2022","2022","7","4","12562","12568","Reinforcement learning needs to learn policies through trial and error. The unstable policies in the early stage of training make it expensive (and time-consuming) to train directly in the real environment, which may cause disastrous consequences. The popular solution is to use the simulator to train the policy and deploy it in a real environment. However, the modeling error and external disturbance between the simulation and the real environment may fail the physical deployment, resulting in the sim2real transfer problem. In this letter, we propose a novel robust adversarial reinforcement learning framework, which uses the ensemble training of multi-adversarial agents that can adaptively adjust adversaries' strength to enhance RL policy's robustness. More specifically, we take the accumulative reward as feedback and construct a PID controller to adjust the adversary's output magnitude to perform the adversarial training well. Experiments in the simulated and the real environment show that our algorithm improves the generalization ability of the policy for the modeling error and the uncertain disturbance simultaneously, outperforming the next best prior methods across all domains. The algorithm was further proven to be effective in a sim2real transfer task through the load experiment of a real racing drone, and the tracking performance is better than the PID-based flight controller.","2377-3766","","10.1109/LRA.2022.3220531","China Postdoctoral Science Foundation(grant numbers:BX20220071); Shanghai Municipality Science and Technology Major Project(grant numbers:2021SHZDZX0103); National Key R&D Program of China(grant numbers:2021ZD0113502); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9942298","Machine learning for robot control;reinforcement learning;robust/adaptive control","Training;Mirrors;Task analysis;Reinforcement learning;Stability analysis;Robustness;Optimization","adaptive control;aerospace computing;aerospace control;autonomous aerial vehicles;control engineering computing;feedback;multi-robot systems;reinforcement learning;robust control;three-term control","accumulative reward;adversarial training;ensemble training;feedback;multiadversarial agents;physical deployment;PID-based flight controller;RL policy robustness;robust adaptive ensemble adversary reinforcement learning;robust adversarial reinforcement learning framework;sim2real transfer problem;sim2real transfer task","","1","","24","IEEE","8 Nov 2022","","","IEEE","IEEE Journals"
"End-to-end grasping policies for human-in-the-loop robots via deep reinforcement learning","M. Sharif; D. Erdogmus; C. Amato; T. Padir","Electrical and Computer Engineering Department, Northeastern University, Boston, MA, USA; Electrical and Computer Engineering Department, Northeastern University, Boston, MA, USA; Khoury College of Computer Sciences, Northeastern University, Boston, MA, USA; Electrical and Computer Engineering Department, Northeastern University, Boston, MA, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","2768","2774","State-of-the-art human-in-the-loop robot grasping is hugely suffered by Electromyography (EMG) inference robustness issues. As a workaround, researchers have been looking into integrating EMG with other signals, often in an ad hoc manner. In this paper, we are presenting a method for end-to-end training of a policy for human-in-the-loop robot grasping on real reaching trajectories. For this purpose we use Reinforcement Learning (RL) and Imitation Learning (IL) in DEXTRON (DEXTerity enviRONment), a stochastic simulation environment with real human trajectories that are augmented and selected using a Monte Carlo (MC) simulation method. We also offer a success model which once trained on the expert policy data and the RL policy roll-out transitions, can provide transparency to how the deep policy works and when it is probably going to fail.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561937","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561937","","Training;Monte Carlo methods;Stochastic processes;Grasping;Reinforcement learning;Electromyography;Robustness","control engineering computing;deep learning (artificial intelligence);dexterous manipulators;electromyography;human-robot interaction;Monte Carlo methods;reinforcement learning","EMG;ad hoc manner;end-to-end training;reaching trajectories;imitation learning;stochastic simulation environment;human trajectories;Monte Carlo simulation method;expert policy data;RL policy roll-out;deep policy works;end-to-end grasping policies;human-in-the-loop robots;deep reinforcement learning;state-of-the-art human-in-the-loop robot grasping;electromyography inference robustness issues","","1","","40","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Improve the Stability and Robustness of Power Management through Model-free Deep Reinforcement Learning","L. Chen; X. Li; J. Xu","Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology; Microelectronics Thrust, The Hong Kong University of Science and Technology","2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)","19 May 2022","2022","","","1371","1376","Achieving high performance with low energy consumption has become a primary design objective in multi-core systems. Recently, power management based on reinforcement learning has shown great potential in adapting to dynamic environments without much prior knowledge. However, conventional Q-learning (QL) algorithms adopted in most existing works encounter serious problems about scalability, instability, and overestimation. In this paper, we present a deep reinforcement learning-based approach to improve the stability and robustness of power management while reducing the energy-delay product (EDP) under user-specified performance requirements. The comprehensive status of the system is monitored periodically, making our controller sensitive to environmental change. To further improve the learning effectiveness, knowledge sharing among multiple devices is implemented in our approach. Experimental results on multiple realistic applications show that the proposed method can reduce the instability up to 68% compared with QL. Through knowledge sharing among multiple devices, our federated approach achieves around 4.8% EDP improvement over QL on average.","1558-1101","978-3-9819263-6-1","10.23919/DATE54114.2022.9774731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774731","power management;deep reinforcement learning;experience replay;federated learning;multicore system","Performance evaluation;Learning systems;Energy consumption;Q-learning;Power system management;Scalability;Heuristic algorithms","deep learning (artificial intelligence);multiprocessing systems;power aware computing;reinforcement learning","stability;robustness;power management;low energy consumption;multicore systems;dynamic environments;conventional Q-learning algorithms;QL;energy delay product;learning effectiveness;knowledge sharing;multiple devices;deep reinforcement learning;user specified performance requirements","","1","","20","","19 May 2022","","","IEEE","IEEE Conferences"
"Self-Improving Safety Performance of Reinforcement Learning Based Driving with Black-Box Verification Algorithms","R. Dagdanov; H. Durmus; N. K. Ure","Department of Aeronautical Engineering, ITU Artificial Intelligence and Data Science Research Center, Istanbul Technical University, Turkey; Department of Electronics and Communication Engineering, Eatron Technologies, Istanbul Technical University, Turkey; Department of Computer Engineering, ITU Artificial Intelligence and Data Science Application and Research Center, Istanbul Technical University, Turkey","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5631","5637","In this work, we propose a self-improving artificial intelligence system to enhance the safety performance of reinforcement learning (RL)-based autonomous driving (AD) agents using black-box verification methods. RL algorithms have become popular in AD applications in recent years. However, the performance of existing RL algorithms heavily depends on the diversity of training scenarios. A lack of safety-critical scenarios during the training phase could result in poor generalization performance in real-world driving applications. We propose a novel framework in which the weaknesses of the training set are explored through black-box verification methods. After discovering AD failure scenarios, the RL agent's training is re-initiated via transfer learning to improve the performance of previously unsafe scenarios. Simulation results demonstrate that our approach efficiently discovers safety failures of action decisions in RL-based adaptive cruise control (ACC) applications and significantly reduces the number of vehicle collisions through iterative applications of our method. The source code is publicly available at https://github.com/data-and-decision-lab/self-improving-RL.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160883","Istanbul Technical University BAP(grant numbers:MOA-2019-42321); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160883","Deep Reinforcement Learning;Autonomous Driving;Black-Box Verification","Training;Source coding;Simulation;Transfer learning;Closed box;Reinforcement learning;Safety","advanced driver assistance systems;collision avoidance;deep learning (artificial intelligence);mobile robots;reinforcement learning","ACC;AD failure scenarios;black-box verification algorithms;black-box verification methods;reinforcement learning-based autonomous driving agents;RL algorithms;RL-based adaptive cruise control applications;safety failures;self-improving artificial intelligence system;self-improving safety performance;transfer learning;vehicle collision","","1","","23","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Reinforcement learning call control in variable capacity links","A. Pietrabissa","Computer and System Science Department (DIS), University of Rome, Italy","18th Mediterranean Conference on Control and Automation, MED'10","12 Aug 2010","2010","","","933","938","This paper defines a Reinforcement Learning (RL) approach to call control algorithms in links with variable capacity supporting multiple classes of service. The novelties of the document are the following: i) the problem is modeled as a constrained Markov Decision Process (MDP); ii) the constrained MDP is solved via a RL algorithm by using the Lagrangian approach and state aggregation. The proposed approach is capable of controlling class-level quality of service in terms of both blocking and dropping probabilities. Numerical simulations show the effectiveness of the approach.","","978-1-4244-8092-0","10.1109/MED.2010.5547750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5547750","Reinforcement Learning;Markov Decision Processes;Call Control;Communication Networks","Markov processes;Sun;Load modeling;Aerospace electronics;Cost function;Time frequency analysis;Numerical simulation","learning (artificial intelligence);Markov processes;quality of service;telecommunication congestion control;telecommunication network management","reinforcement learning call control;variable capacity link;call control algorithm;constrained Markov decision process;state aggregation;quality of service;numerical simulation","","1","","24","IEEE","12 Aug 2010","","","IEEE","IEEE Conferences"
"Robotic Navigation with Human Brain Signals and Deep Reinforcement Learning","C. Lin; S. M. S. Hasan; O. Bai","Electrical and Computer Engineering, Florida International University, Miami, USA; Electrical and Computer Engineering, Florida International University, Miami, USA; Electrical and Computer Engineering, Florida International University, Miami, USA","2021 4th International Conference on Robotics, Control and Automation Engineering (RCAE)","14 Dec 2021","2021","","","278","283","Navigation under a grid world has been a classical and historical theme in reinforcement learning, which is viewed as a Markov decision process (MDP) in general. The literature to date has proven that the navigation task can be addressed that an agent achieves the target with a high success rate and avoids collision with obstacles. But essentially, they met with success in a specific environment while the agent cannot work in a new surrounding, meaning that it does not boast broad applicability. In state-of-the-art approaches, poor feedback and lack of adaptability to increasing state spaces remain a problem. In this paper, we propose a modified approach to solve a series of navigation problems under moderate and huge-sized surroundings. The problem is addressed with a deep reinforcement learning algorithm with a guided classifier. We address these issues by providing a reliable guided reward with a brain-guided classifier based on human brain signals (electroencephalography, EEG) and a convolutional neural network. This paper explores several experiments to show that our model with deep RL and the brain-guided classifier can solve these complex and significant practical challenges. Our method improves efficiency by about twice as much as traditional approaches such as DQN and Q-learning.","","978-1-6654-2730-2","10.1109/RCAE53607.2021.9638872","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9638872","Markov decision process;electroencephalography (EEG);deep Q learning;reinforcement learning","Deep learning;Q-learning;Navigation;Process control;Markov processes;Feature extraction;Electroencephalography","bioelectric potentials;convolutional neural nets;deep learning (artificial intelligence);electroencephalography;medical robotics;navigation;neurophysiology;path planning;reinforcement learning","deep reinforcement learning algorithm;brain-guided classifier;human brain signals;Q-learning;robotic navigation;EEG;electroencephalography;convolutional neural network","","1","","13","IEEE","14 Dec 2021","","","IEEE","IEEE Conferences"
"Implementing Robotic Pick and Place with Non-visual Sensing Using Reinforcement Learning","M. B. Imtiaz; Y. Qiao; B. Lee","Software Research Institute, Athlone Institute of Technology, Athlone, Ireland; Software Research Institute, Athlone Institute of Technology, Athlone, Ireland; Software Research Institute, Athlone Institute of Technology, Athlone, Ireland","2022 6th International Conference on Robotics, Control and Automation (ICRCA)","18 Jul 2022","2022","","","23","28","In this study, we focus on learning and carrying out pick and place operations on various objects moving on a conveyor belt in a non-visual environment, using proximity sensors. The problem under consideration is formulated as a Markov Decision Process. and solved by using Reinforcement Learning. Learning robotic manipulations using simple reward signals is still considered to be an unresolved problem. Our reinforcement learning algorithm is based on model-free off-policy training using Q-Learning. Training and testing are performed in a simulation-based testbed, proving our approach to be successful in pick and place operations in non-visual industrial setups.","","978-1-6654-8174-8","10.1109/ICRCA55033.2022.9828993","Science Foundation Ireland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9828993","robotic manipulations;non-visual;markov decision problem;reinforcemenet learning;q-learning","Training;Q-learning;Service robots;Random access memory;Vision sensors;Markov processes;Robot sensing systems","conveyors;learning (artificial intelligence);manipulators;Markov processes","proximity sensors;Markov Decision Process;Learning robotic manipulations;simple reward signals;unresolved problem;reinforcement learning algorithm;model-free off-policy training;nonvisual industrial setups;robotic pick;nonvisual sensing;conveyor belt;nonvisual environment","","1","","31","IEEE","18 Jul 2022","","","IEEE","IEEE Conferences"
"Online velocity fluctuation of off-road wheeled mobile robots: A reinforcement learning approach","F. Gauthier-Clerc; A. Hill; J. Laneurit; R. Lenain; É. Lucet","Université Paris-Saclay, CEA, Palaiseau, France; Université Paris-Saclay, CEA, Palaiseau, France; Inrae, UR TSCF, Centre de Clermont-Ferrand, Université Clermont Auvergne, Aubière, France; Inrae, UR TSCF, Centre de Clermont-Ferrand, Université Clermont Auvergne, Aubière, France; Université Paris-Saclay, CEA, Palaiseau, France","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","2421","2427","During the off-road path following of a wheeled mobile robot in presence of poor grip conditions, the longitudinal velocity should be limited in order to maintain safe navigation with limited tracking errors, while at the same time being high enough to minimize travel time. Thus, this paper presents a new approach of online speed fluctuation, capable of limiting the lateral error below a given threshold, while maximizing the longitudinal velocity. This is accomplished using a neural network trained with a reinforcement learning method. This speed modulation is done side-by-side with an existing model-based predictive steering control, using a state estimator and dynamic observers. Simulated and experimental results show a decrease in tracking error, while maintaining a consistent travel time when compared to a classical constant speed method and to a kinematic speed fluctuation method.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560816","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560816","","Fluctuations;Limiting;Navigation;Neural networks;Modulation;Reinforcement learning;Kinematics","learning (artificial intelligence);mobile robots;navigation;predictive control;steering systems;wheels","online velocity fluctuation;mobile robots;off-road path;wheeled mobile robot;poor grip conditions;longitudinal velocity;safe navigation;tracking errors;online speed fluctuation;lateral error;given threshold;neural network;reinforcement learning method;speed modulation;existing model-based predictive steering control;consistent travel time;classical constant speed method;kinematic speed fluctuation method","","1","","33","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Mesh Based Analysis of Low Fractal Dimension Reinforcement Learning Policies","S. Gillen; K. Byl","Electrical and Computer Engineering Department, University of California, Santa Barbara, CA; Electrical and Computer Engineering Department, University of California, Santa Barbara, CA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","3546","3552","In previous work, using a process we call meshing, the reachable state spaces for various continuous and hybrid systems were approximated as a discrete set of states which can then be synthesized into a Markov chain. One of the applications for this approach has been to analyze locomotion policies obtained by reinforcement learning, in a step towards making empirical guarantees about the stability properties of the resulting system. In a separate line of research, we introduced a modified reward function for on-policy reinforcement learning algorithms that utilizes a ""fractal dimension"" of rollout trajectories. This reward was shown to encourage policies that induce individual trajectories which can be more compactly represented as a discrete mesh. In this work, we combine these two threads of research by building meshes of the reachable state space of a system subject to disturbances and controlled by policies obtained with the modified reward. Our analysis shows that the modified policies do produce much smaller reachable meshes. This shows that agents trained with the fractal dimension reward transfer their desirable quality of having a more compact state space to a setting with external disturbances. The results also suggest that the previous work using mesh based tools to analyze RL policies may be extended to higher dimensional systems or to higher resolution meshes than would have otherwise been possible.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561874","","Training;Visualization;Monte Carlo methods;Reinforcement learning;Tools;Aerospace electronics;Fractals","fractals;learning (artificial intelligence);Markov processes","mesh based analysis;low fractal dimension reinforcement learning policies;meshing;reachable state space;continuous systems;hybrid systems;Markov chain;locomotion policies;empirical guarantees;stability properties;modified reward function;on-policy reinforcement;rollout trajectories;induce individual trajectories;discrete mesh;system subject;modified policies;smaller reachable meshes;fractal dimension reward transfer their desirable quality;compact state space;mesh based tools;RL policies;higher dimensional systems;higher resolution meshes","","1","","13","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Integrating Deep Reinforcement and Supervised Learning to Expedite Indoor Mapping","E. Zwecher; E. Iceland; S. R. Levy; S. Y. Hayoun; O. Gal; A. Barel","Computer Science Department, Hebrew University of Jerusalem, Jerusalem, Israel; Computer Science Department, Hebrew University of Jerusalem, Jerusalem, Israel; Faculty of Aerospace Engineering, Technion - Israel Institute of Technology, Haifa, Israel; Faculty of Aerospace Engineering, Technion - Israel Institute of Technology, Haifa, Israel; Geo-information Department, Technion - Israel Institute of Technology, Haifa, Israel; Ariel Barel is an academic visitor at the Computer Science Department, Technion - Israel Institute of Technology, Haifa, Israel","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","10542","10548","The challenge of mapping indoor environments is addressed. Typical heuristic algorithms for solving the motion planning problem are frontier-based methods, that are especially effective when the environment is completely unknown. However, in cases where prior statistical data on the environment's architectonic features is available, such algorithms can be far from optimal. Furthermore, their calculation time may increase substantially as more areas are exposed. In this paper we propose two means by which to overcome these shortcomings. One is the use of deep reinforcement learning to train the motion planner. The second is the inclusion of a pre-trained generative deep neural network, acting as a map predictor. Each one helps to improve the decision making through use of the learned structural statistics of the environment, and both, being realized as neural networks, ensure a constant calculation time. We show that combining the two methods can shorten the duration of the mapping process by up to 4 times, compared to frontier-based motion planning.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811861","","Training;Neural networks;Decision making;Supervised learning;Reinforcement learning;System recovery;Predictive models","decision making;learning (artificial intelligence);mobile robots;neural nets;path planning","deep reinforcement;supervised learning;mapping indoor environments;typical heuristic algorithms;motion planning problem;frontier-based methods;prior statistical data;architectonic features;motion planner;pre-trained generative deep neural network;map predictor;learned structural statistics;neural networks;constant calculation time;mapping process;frontier-based motion planning","","1","","16","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach for Adaptive Covariance Tuning in the Kalman Filter","J. Gu; J. Li; K. Tei","Waseda University, Tokyo, Japan; Waseda University, Tokyo, Japan; Waseda University, Tokyo, Japan","2022 IEEE 5th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)","26 Jan 2023","2022","5","","1569","1574","State estimation and localization for the autonomous vehicle are essential for accurate navigation and safe maneuvers. The commonly used method is Kalman filtering, but its performance is affected by the noise covariance. An inappropriate set value will decrease the estimation accuracy and even makes the filter diverge. The noise covariance estimation problem has long been considered a tough issue because there is too much uncertainty in where the noise comes from and therefore unable to model it systematically. In recent years, Deep Reinforcement Learning (DRL) has made astonishing progress and is an excellent choice for tackling the problem that cannot be solved by conventional techniques, such as parameter estimation. By finely abstracting the problem as an MDP, we can use the DRL methods to solve it without too many prior assumptions. We propose an adaptive covariance tuning method applied to the Error State Extend Kalman Filter by taking advantage of DRL, called Reinforcement Learning Aided Covariance Tuning. The preliminary experiment result indicates that our method achieves a 14.73% estimation accuracy improvement on average compared with the vanilla fixed-covariance method and bound the estimation error within 0.4 m.","2693-2776","978-1-6654-7968-4","10.1109/IMCEC55388.2022.10020019","JSPS KAKENHI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020019","Reinforcement learning;Kalman filter;Autonomous driving;State estimation","Uncertainty;Parameter estimation;Navigation;Roads;Neural networks;Estimation;Reinforcement learning","covariance analysis;deep learning (artificial intelligence);estimation theory;Kalman filters;mobile robots;navigation;nonlinear filters;parameter estimation;path planning;reinforcement learning;remotely operated vehicles;robot vision;SLAM (robots);state estimation","adaptive covariance tuning;autonomous vehicle;deep reinforcement learning;DRL;error state extended Kalman filter;Kalman filtering;localization;navigation;noise covariance estimation;state estimation","","1","","13","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"High-Speed Autonomous Racing Using Trajectory-Aided Deep Reinforcement Learning","B. D. Evans; H. A. Engelbrecht; H. W. Jordaan","Department of Electrical and Electronic Engineering, Stellenbosch University, Stellenbosch, South Africa; Department of Electrical and Electronic Engineering, Stellenbosch University, Stellenbosch, South Africa; Department of Electrical and Electronic Engineering, Stellenbosch University, Stellenbosch, South Africa","IEEE Robotics and Automation Letters","19 Jul 2023","2023","8","9","5353","5359","The classical method of autonomous racing uses real-time localisation to follow a precalculated optimal trajectory. In contrast, end-to-end deep reinforcement learning (DRL) can train agents to race using only raw LiDAR scans. While classical methods prioritise optimization for high-performance racing, DRL approaches have focused on low-performance contexts with little consideration of the speed profile. This work addresses the problem of using end-to-end DRL agents for high-speed autonomous racing. We present trajectory-aided learning (TAL) that trains DRL agents for high-performance racing by incorporating the optimal trajectory (racing line) into the learning formulation. Our method is evaluated using the TD3 algorithm on four maps in the open-source F1Tenth simulator. The results demonstrate that our method achieves a significantly higher lap completion rate at high speeds compared to the baseline. This is due to TAL training the agent to select a feasible speed profile of slowing down in the corners and roughly tracking the optimal trajectory.","2377-3766","","10.1109/LRA.2023.3295252","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10182327","Deep learning methods;machine learning for robot control;reinforcement learning","Trajectory;Reinforcement learning;Laser radar;Deep learning;Sensors;Accidents;Radar tracking","deep learning (artificial intelligence);learning (artificial intelligence);optical radar;reinforcement learning","classical method;classical methods prioritise optimization;deep reinforcement learning;DRL approaches;end-to-end DRL agents;feasible speed profile;high-performance racing;high-speed autonomous racing;learning formulation;low-performance contexts;precalculated optimal trajectory;racing line;trajectory-aided deep reinforcement","","1","","30","IEEE","13 Jul 2023","","","IEEE","IEEE Journals"
"Retro-RL: Reinforcing Nominal Controller With Deep Reinforcement Learning for Tilting-Rotor Drones","I. M. A. Nahrendra; C. Tirtawardhana; B. Yu; E. M. Lee; H. Myung","School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea","IEEE Robotics and Automation Letters","19 Jul 2022","2022","7","4","9004","9011","Studies that broaden drone applications into complex tasks require a stable control framework. Recently, deep reinforcement learning (RL) algorithms have been exploited in many studies for robot control to accomplish complex tasks. Unfortunately, deep RL algorithms might not be suitable for being deployed directly into a real-world robot platform due to the difficulty in interpreting the learned policy and lack of stability guarantee, especially for a complex task such as a wall-climbing drone. This letter proposes a novel hybrid architecture that reinforces a nominal controller with a robust policy learned using a model-free deep RL algorithm. The proposed architecture employs an uncertainty-aware control mixer to preserve guaranteed stability of a nominal controller while using the extended robust performance of the learned policy. The policy is trained in a simulated environment with thousands of domain randomizations to achieve robust performance over diverse uncertainties. The performance of the proposed method was verified through real-world experiments and then compared with a conventional controller and the state-of-the-art learning-based controller trained with a vanilla deep RL algorithm.","2377-3766","","10.1109/LRA.2022.3189446","National Research Foundation of Korea; Ministry of Science and ICT(grant numbers:NRF-2018M3C1B9088328); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9822205","Aerial systems;mechanics and control;machine learning for robot control;reinforcement learning","Uncertainty;Drones;Mixers;Gaussian distribution;Task analysis;Reinforcement learning;Mathematical models","learning (artificial intelligence);mobile robots;robust control;stability","conventional controller;state-of-the-art learning-based controller;vanilla deep RL algorithm;retro-RL;nominal controller;tilting-rotor drones;drone applications;stable control framework;deep reinforcement learning algorithms;robot control;deep RL algorithms;real-world robot platform;learned policy;stability guarantee;complex task;wall-climbing drone;robust policy;model-free deep RL algorithm;uncertainty-aware control mixer;guaranteed stability;extended robust performance","","1","","37","IEEE","8 Jul 2022","","","IEEE","IEEE Journals"
"RADARS: Memory Efficient Reinforcement Learning Aided Differentiable Neural Architecture Search","Z. Yan; W. Jiang; X. S. Hu; Y. Shi",University of Notre Dame; George Mason University; University of Notre Dame; University of Notre Dame,"2022 27th Asia and South Pacific Design Automation Conference (ASP-DAC)","21 Feb 2022","2022","","","128","133","Differentiable neural architecture search (DNAS) is known for its capacity in the automatic generation of superior neural networks. However, DNAS based methods suffer from memory usage explosion when the search space expands, which may prevent them from running successfully on even advanced GPU platforms. On the other hand, reinforcement learning (RL) based methods, while being memory efficient, are extremely time-consuming. Combining the advantages of both types of methods, this paper presents RADARS, a scalable RL aided DNAS framework that can explore large search spaces in a fast and memory-efficient manner. RADARS iteratively applies RL to prune undesired architecture candidates and identifies a promising subspace to carry out DNAS. Experiments using a workstation with 12 GB GPU memory show that on CIFAR-10 and ImageNet datasets, RADARS can achieve up to 3.41% higher accuracy with 2.5X search time reduction compared with a state-of-the-art RL-based method, while the two DNAS baselines cannot complete due to excessive memory usage or search time. To the best of the authors’ knowledge, this is the first DNAS framework that can handle large search spaces with bounded memory usage.","2153-697X","978-1-6654-2135-5","10.1109/ASP-DAC52403.2022.9712499","National Science Foundation(grant numbers:CNS-1822099,CNS-1919167); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712499","","Spaceborne radar;Memory management;Neural networks;DNA;Graphics processing units;Reinforcement learning;Radar imaging","iterative methods;neural nets;reinforcement learning;search problems","differentiable neural architecture search;neural networks;memory usage explosion;search space expands;reinforcement learning;DNAS framework;search spaces;fast memory-efficient manner;GPU memory;search time reduction;RL-based method;bounded memory usage;scalable RL aided DNAS framework;memory efficient reinforcement learning;ImageNet datasets;memory size 12.0 GByte","","1","","20","IEEE","21 Feb 2022","","","IEEE","IEEE Conferences"
"Inverse Reinforcement Learning of Interaction Dynamics from Demonstrations","M. Hussein; M. Begum; M. Petrik","Cognitive Assistive Robotics Lab, University of New Hampshire; Cognitive Assistive Robotics Lab, University of New Hampshire; Department of Computer Science, University of New Hampshire","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","2267","2274","This paper presents a framework to learn the reward function underlying high-level sequential tasks from demonstrations. The purpose of reward learning, in the context of learning from demonstration (LfD), is to generate policies that mimic the demonstrator's policies, thereby enabling imitation learning. We focus on a human-robot interaction (HRI) domain where the goal is to learn and model structured interactions between a human and a robot. Such interactions can be modeled as a partially observable Markov decision process (POMDP) where the partial observability is caused by uncertainties associated with the ways humans respond to different stimuli. The key challenge in finding a good policy in such a POMDP is determining the reward function that was observed by the demonstrator. Existing inverse reinforcement learning (IRL) methods for POMDPs are computationally very expensive and the problem is not well understood. In comparison, IRL algorithms for Markov decision process (MDP) are well defined and computationally efficient. We propose an approach of reward function learning for high-level sequential tasks from human demonstrations where the core idea is to reduce the underlying POMDP to an MDP and apply any efficient MDPIRL algorithm. Our extensive experiments suggest that the reward function learned this way generates POMDP policies that mimic the policies of the demonstrator well.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793867","","Robots;Task analysis;Education;Reinforcement learning;Uncertainty;Aircraft navigation;Human-robot interaction","decision theory;human-robot interaction;learning (artificial intelligence);Markov processes","inverse reinforcement learning methods;high-level sequential tasks;human demonstrations;POMDP policies;interaction dynamics;human-robot interaction domain;structured interactions;partially observable Markov decision process;learning from demonstration;IRL algorithms;reward function learning;MDPIRL algorithm","","1","","41","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Obstacle Avoidance Algorithm for Mobile Robot Based on Deep Reinforcement Learning in Dynamic Environments","S. Xiaoxian; Y. Chenpeng; Z. Haoran; L. Chengju; C. Qijun","National Natural Science Foundation of China, Beijing, China; National Natural Science Foundation of China, Beijing, China; National Natural Science Foundation of China, Beijing, China; National Natural Science Foundation of China, Beijing, China; National Natural Science Foundation of China, Beijing, China","2020 IEEE 16th International Conference on Control & Automation (ICCA)","30 Nov 2020","2020","","","366","372","Developing a friendly and efficient obstacle avoidance algorithm for mobile robot in dynamic environments is challenging in the scenarios where robot plans its paths without observing other obstacles' intents. Recent works have shown the power of deep reinforcement learning techniques to learn collision-free policies. However, many of them ignore the interactions between obstacles, which may cause unnatural trajectory. Meanwhile, the performance of these methods is not well in simulation environment. Combined with the interactions of obstacles, we propose a mobile robot obstacle avoidance algorithm based on deep reinforcement learning in dynamic environments. Firstly, we not only consider the direct impact of obstacles on robots, but also take the interaction between obstacles into account using angel pedestrian grid. Therefore, robot can extract more abundant environmental prior information. Subsequently, the temporal characteristics of obstacles are extracted through attention mechanism, then we can obtain the joint impact of obstacles on the obstacle avoidance strategy. Finally, the robot obtains the control output through value-based reinforcement learning. The feasibility and effectiveness of our proposed algorithm in dynamic environments is verified through simulation experiments.","1948-3457","978-1-7281-9093-8","10.1109/ICCA51439.2020.9264363","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264363","","Robots;Collision avoidance;Heuristic algorithms;Reinforcement learning;Prediction algorithms;Trajectory;Predictive models","collision avoidance;learning (artificial intelligence);mobile robots;neural nets","mobile robot obstacle avoidance algorithm;deep reinforcement;dynamic environments;value-based reinforcement learning","","1","","25","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Maximising Coefficiency of Human-Robot Handovers Through Reinforcement Learning","M. Lagomarsino; M. Lorenzini; M. D. Constable; E. De Momi; C. Becchio; A. Ajoudani","Human-Robot Interfaces and Interaction Laboratory, Istituto Italiano di Tecnologia, Genoa, Italy; Human-Robot Interfaces and Interaction Laboratory, Istituto Italiano di Tecnologia, Genoa, Italy; Department of Psychology, Northumbria University, Newcastle, U.K.; Department of Electronics, Information and Bioengineering, Politecnico di Milano, Milan, Italy; Department of Neurology, University Medical CenterHamburg-Eppendorf, Hamburg, Germany; Human-Robot Interfaces and Interaction Laboratory, Istituto Italiano di Tecnologia, Genoa, Italy","IEEE Robotics and Automation Letters","14 Jun 2023","2023","8","8","4378","4385","Handing objects to humans is an essential capability for collaborative robots. Previous research works on human-robot handovers focus on facilitating the performance of the human partner and possibly minimising the physical effort needed to grasp the object. However, altruistic robot behaviours may result in protracted and awkward robot motions, contributing to unpleasant sensations by the human partner and affecting perceived safety and social acceptance. This letter investigates whether transferring the cognitive science principle that “humans act coefficiently as a group” (i.e. simultaneously maximising the benefits of all agents involved) to human-robot cooperative tasks promotes a more seamless and natural interaction. Human-robot coefficiency is first modelled by identifying implicit indicators of human comfort and discomfort as well as calculating the robot energy consumption in performing the desired trajectory. We then present a reinforcement learning approach that uses the human-robot coefficiency score as reward to adapt and learn online the combination of robot interaction parameters that maximises such coefficiency. Results proved that by acting coefficiently the robot could meet the individual preferences of most subjects involved in the experiments, improve the human perceived comfort, and foster trust in the robotic partner.","2377-3766","","10.1109/LRA.2023.3280752","ERC-StG Ergo-Lean(grant numbers:850932); Royal Society(grant numbers:IES\R3\203086); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10137876","Human factors and human-in-the-loop;physical human-robot interaction;human-centered robotics","Robots;Robot kinematics;Handover;Ergonomics;Trajectory;Task analysis;Human-robot interaction","cognition;control engineering computing;energy consumption;human-robot interaction;learning (artificial intelligence);mobile robots;robots","actingcoefficientlythe robot;affecting perceived safety;altruistic robot behaviours;awkward robot motions;cognitive science principle;collaborative robots;essential capability;handing objects;human comfort;human partner;human perceived comfort;human-robot handovers focus;maximising coefficiency;protracted robot motions;reinforcement learning approach;robot energy consumption;robot interaction parameters;robotic partner;social acceptance","","1","","32","IEEE","29 May 2023","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Autonomous Driving using High-Level Heterogeneous Graph Representations","M. Schier; C. Reinders; B. Rosenhahn","Leibniz University Hannover, L3S / Institute for Information Processing; Leibniz University Hannover, L3S / Institute for Information Processing; Leibniz University Hannover, L3S / Institute for Information Processing","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","7147","7153","Graph networks have recently been used for decision making in automated driving tasks for their ability to capture a variable number of traffic participants. Current high-level graph-based approaches, however, do not model the entire road network and thus must rely on handcrafted features for vehicle-to-vehicle edges encompassing the road topology indirectly. We propose an entity-relation framework that intuitively models the road network and the traffic participants in a heterogeneous graph, representing all relevant information. Our novel architecture transforms the heterogeneous road-vehicle graph into a simpler graph of homogeneous node and edge types to allow effective training for deep reinforcement learning while introducing minimal prior knowledge. Unlike previous approaches, the vehicle-to-vehicle edges of this reduced graph are fully learnable and can therefore encode traffic rules without explicit feature design, an important step towards a holistic reinforcement learning model for automated driving. We show that our proposed method outperforms precomputed handcrafted features on intersection scenarios while also learning the semantics of right-of-way rules.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160762","Federal Ministry of Education and Research (BMBF)(grant numbers:01DD20003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160762","","Training;Deep learning;Network topology;Roads;Semantics;Vehicular ad hoc networks;Reinforcement learning","decision making;deep learning (artificial intelligence);graph theory;learning (artificial intelligence);reinforcement learning;road traffic;road vehicles;traffic engineering computing","automated driving tasks;autonomous driving;current high-level graph-based approaches;decision making;deep reinforcement learning;edge types;entire road network;entity-relation framework;explicit feature design;graph networks;handcrafted features;heterogeneous road-vehicle graph;high-level heterogeneous graph representations;holistic reinforcement learning model;homogeneous node;reduced graph;road topology;simpler graph;traffic participants;traffic rules;vehicle-to-vehicle edges","","1","","35","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Improving Safety in Deep Reinforcement Learning using Unsupervised Action Planning","H. -L. Hsu; Q. Huang; S. Ha","Georgia Institute of Technology, Atlanta, GA, USA; Pacific Northwest National Laboratory, Richland, WA, USA; Georgia Institute of Technology, Atlanta, GA, USA","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","5567","5573","One of the key challenges to deep reinforcement learning (deep RL) is to ensure safety at both training and testing phases. In this work, we propose a novel technique of unsupervised action planning to improve the safety of on-policy reinforcement learning algorithms, such as trust region policy optimization (TRPO) or proximal policy optimization (PPO). We design our safety-aware reinforcement learning by storing all the history of “recovery” actions that rescue the agent from dangerous situations into a separate “safety” buffer and finding the best recovery action when the agent encounters similar states. Because this functionality requires the algorithm to query similar states, we implement the proposed safety mechanism using an unsupervised learning algorithm, k-means clustering. We evaluate the proposed algorithm on six robotic control tasks that cover navigation and manipulation. Our results show that the proposed safe RL algorithm can achieve higher rewards compared with multiple baselines in both discrete and continuous control problems. The supplemental video can be found at: https://youtu.be/AFTeWSohILo.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812181","","Training;Clustering algorithms;Reinforcement learning;Streaming media;Safety;Planning;Task analysis","deep learning (artificial intelligence);pattern clustering;reinforcement learning;robot programming;unsupervised learning","safety mechanism;unsupervised learning algorithm;safe RL algorithm;deep reinforcement learning;unsupervised action planning;on-policy reinforcement learning;proximal policy optimization;safety-aware reinforcement learning;trust region policy optimization;TRPO;k-means clustering;robotic control tasks","","1","","40","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"EImprove - Optimizing Energy and Comfort in Buildings based on Formal Semantics and Reinforcement Learning","S. Verma; S. Agrawal; R. Venkatesh; U. Shrotri; S. Nagarathinam; R. Jayaprakash; A. Dutta","TCS Research, India; TCS Research, India; TCS Research, India; TCS Research, India; TCS Research, India; TCS Research, India; TCS Research, India","2021 58th ACM/IEEE Design Automation Conference (DAC)","8 Nov 2021","2021","","","157","162","Heating, ventilation, and air-conditioning (HVAC) system’s supervisory control is crucial for energy-efficient thermal comfort in buildings. The control logic is usually specified as ‘if-then-that-else’ rules that capture the domain expertise of HVAC operators, but they often have conflicts that may lead to sub-optimal HVAC performance. We propose EImprove, a reinforcement-learning (RL) based framework that exploits these conflicts to learn a resolution policy. We evaluate EImprove through a co-simulation strategy involving EnergyPlus simulations of a real-world office setting and a formal requirement specifier. Our experiments show that EImprove learns 75% faster than a pure RL framework.","0738-100X","978-1-6654-3274-0","10.1109/DAC18074.2021.9586313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586313","","Training;Heating systems;HVAC;Buildings;Semantics;Energy resolution;Reinforcement learning","building management systems;building simulation;control engineering computing;energy conservation;formal specification;HVAC;learning (artificial intelligence);power engineering computing","EnergyPlus simulations;formal requirement specifier;pure RL framework;buildings;formal semantics;energy-efficient thermal comfort;EImprove;reinforcement-learning based framework;resolution policy;co-simulation strategy;heating, ventilation, and air-conditioning system supervisory control;if-then-that-else rules;suboptimal HVAC performance;real-world office setting;energy optimisation","","1","","16","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Data Transmission Resilience to Cyber-attacks on Heterogeneous Multi-agent Deep Reinforcement Learning Systems","N. E. Fard; R. R. Selmic","Department of Electrical and Computer Engineering, Concordia University, Montreal, QC, Canada; Department of Electrical and Computer Engineering, Concordia University, Montreal, QC, Canada","2022 17th International Conference on Control, Automation, Robotics and Vision (ICARCV)","10 Jan 2023","2022","","","758","764","This paper investigates the data transmission resilience between agents of a cluster-based, heterogeneous, multi-agent deep reinforcement learning (MADRL) system under gradient-based adversarial attacks. We propose an algorithm using a deep Q-network (DQN) approach and a proportional feedback controller to defend against the fast gradient sign method (FGSM) attack and improve the DQN agent performance. The feedback control system is an auxiliary tool that helps the DQN algorithm reduce system deficiencies. In accordance with the achieved results and under FGSM adversarial attack, the resilience of the developed system is evaluated in three different ways termed robust, semi-robust, and non-robust based on average reward and DQN loss. The data transfer is carried out between agents of a MADRL system in timely and time-delayed manners, for both leaderless and leader-follower scenarios. Simulation results are included to verify the presented results.","","978-1-6654-7687-4","10.1109/ICARCV57592.2022.10004318","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10004318","","Deep learning;Simulation;System performance;Clustering algorithms;Process control;Reinforcement learning;Propagation losses","computer crime;deep learning (artificial intelligence);feedback;gradient methods;multi-agent systems;reinforcement learning","average reward;cluster-based multiagent deep reinforcement learning systems;cyber-attacks;data transfer;data transmission resilience;deep Q-network approach;DQN algorithm;fast gradient sign method attack;feedback control system;FGSM adversarial attack;gradient-based adversarial attacks;heterogeneous multiagent deep reinforcement learning systems;MADRL system;proportional feedback controller;system deficiencies","","1","","18","IEEE","10 Jan 2023","","","IEEE","IEEE Conferences"
"Criticality-Guided Deep Reinforcement Learning for Motion Planning","L. Xu; F. Wu; Y. Zhou; H. Hu; Z. Ding; Y. Liu","Zhejiang Sci-Tech University, Hangzhou, China; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Xidian University, Xi’an, China; Zhejiang Sci-Tech University, Hangzhou, China; Zhejiang Sci-Tech University, Hangzhou, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","3378","3383","Real-time and efficient collision avoidance is still challenging for mobile robots running in dynamic and crowded environments. Recent research shows that deep reinforcement learning (DRL) provides a framework to plan collision-free trajectories efficiently. However, most of the current DRL-based methods focus on a fixed number of obstacles in the environments, which limits their applications. In this paper, we propose a learning-based model, Crit-LSTM-DRL, for a robot moving in environments with a variable number of obstacles. It combines an LSTM (Long Short-Term Memory) model and a value-based DRL model. Given the states of a set of obstacles, Crit-LSTM-DRL first sorts the obstacles according to their possible collision time to the robot and then feeds to the LSTM model to generate a fixed-size hidden state. Then, the value-based DRL model takes the hidden state and robot state as input to compute the value. Hence, at any time step, an action is selected that maximizes the value function defined in the DRL framework. Finally, we compare the performance of Crit-LSTM-DRL with a state-of-the-art DRL-based planning method that aims to deal with a variable number of obstacles. The simulation results show that the three models of Crit-LSTM-DRL can improve the success rate by 4%, 20.1%, and 3.8%, and reduce the collision rate by 35.5%, 75%, and 66.7%, respectively.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728277","National Natural Science Foundation of China; Natural Science Foundation of Shaanxi Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728277","Motion Planning;Collision Avoidance;Obstacle Criticality;Deep Reinforcement Learning","Simulation;Computational modeling;Reinforcement learning;Transforms;Real-time systems;Planning;Trajectory","collision avoidance;control engineering computing;deep learning (artificial intelligence);mobile robots;motion control;recurrent neural nets;reinforcement learning;trajectory control","collision-free trajectories;Crit-LSTM-DRL;hidden state;motion planning;collision avoidance;mobile robots;dynamic environment;crowded environment;criticality-guided deep reinforcement learning;long short-term memory","","1","","18","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Efficient Off-Policy Safe Reinforcement Learning Using Trust Region Conditional Value At Risk","D. Kim; S. Oh","Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, Korea; Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, Korea","IEEE Robotics and Automation Letters","29 Jun 2022","2022","7","3","7644","7651","This letter aims to solve a safe reinforcement learning (RL) problem with risk measure-based constraints. As risk measures, such as conditional value at risk (CVaR), focus on the tail distribution of cost signals, constraining risk measures can effectively prevent a failure in the worst case. An on-policy safe RL method, called TRC, deals with a CVaR-constrained RL problem using a trust region method and can generate policies with almost zero constraint violations with high returns. However, to achieve outstanding performance in complex environments and satisfy safety constraints quickly, RL methods are required to be sample efficient. To this end, we propose an off-policy safe RL method with CVaR constraints, called off-policy TRC. If off-policy data from replay buffers is directly used to train TRC, the estimation error caused by the distributional shift results in performance degradation. To resolve this issue, we propose novel surrogate functions, in which the effect of the distributional shift can be reduced, and introduce an adaptive trust-region constraint to ensure a policy not to deviate far from replay buffers. The proposed method has been evaluated in simulation and real-world environments and satisfied safety constraints within a few steps while achieving high returns even in complex robotic tasks.","2377-3766","","10.1109/LRA.2022.3184793","Institute of Information and Communications Technology Planning and Evaluation(grant numbers:2019-0-01190); Robot Learning: Effici- ent, Safe, and Socially-Acceptable Machine Learning; National Research Foundation(grant numbers:NRF-2022R1A2C2008239); Ministry of Science and ICT, South Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9802647","Reinforcement learning;robot safety;collision avoidance","Safety;Costs;Trajectory;Task analysis;Legged locomotion;Estimation error;Adaptation models","collision avoidance;Gaussian distribution;legged locomotion;navigation;optimisation;reinforcement learning;risk management;safety","risk measure-based constraints;CVaR-constrained RL problem;trust region method;zero constraint violations;CVaR constraints;off-policy data;replay buffers;adaptive trust-region constraint;safety constraints;off-policy TRC;trust region conditional value at risk;off-policy safe reinforcement learning;cost signal tail distribution;estimation error;performance degradation;surrogate functions;legged robots;robotic applications;robot navigation;collision avoidance;Gaussian distribution","","1","","29","IEEE","21 Jun 2022","","","IEEE","IEEE Journals"
"Benchmarking Actor-Critic Deep Reinforcement Learning Algorithms for Robotics Control With Action Constraints","K. Kasaura; S. Miura; T. Kozuno; R. Yonetani; K. Hoshino; Y. Hosoe","OMRON SINIC X Corporation, Tokyo, Japan; Manning College of Information and Computer Sciences University of Massachusetts Amherst, Amherst, MA, USA; OMRON SINIC X Corporation, Tokyo, Japan; OMRON SINIC X Corporation, Tokyo, Japan; Graduate School of Informatics, Kyoto University, Kyoto, Japan; Graduate School of Engineering, Kyoto University, Kyoto, Japan","IEEE Robotics and Automation Letters","16 Jun 2023","2023","8","8","4449","4456","This study presents a benchmark for evaluating action-constrained reinforcement learning (RL) algorithms. In action-constrained RL, each action taken by the learning system must comply with certain constraints. These constraints are crucial for ensuring the feasibility and safety of actions in real-world systems. We evaluate existing algorithms and their novel variants across multiple robotics control environments, encompassing multiple action constraint types. Our evaluation provides the first in-depth perspective of the field, revealing surprising insights, including the effectiveness of a straightforward baseline approach. The benchmark problems and associated code utilized in our experiments are made available online at github.com/omron-sinicx/action-constrained-RL-benchmark for further research and development.","2377-3766","","10.1109/LRA.2023.3284378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10146410","Reinforcement learning;action constraints;safety","Robots;Training;Optimization;Benchmark testing;Task analysis;Reinforcement learning;Safety","control engineering computing;deep learning (artificial intelligence);learning (artificial intelligence);multi-robot systems;reinforcement learning","action constraints;action-constrained reinforcement learning algorithms;action-constrained RL;actor-critic deep reinforcement;benchmark problems;learning system;multiple action constraint types;multiple robotics control environments;real-world systems","","1","","30","IEEE","8 Jun 2023","","","IEEE","IEEE Journals"
"Survivable Robotic Control through Guided Bayesian Policy Search with Deep Reinforcement Learning","S. J. A. Raza; A. Dastider; M. Lin","NA; NA; Department of Electrical and Computer Engineering, Univ. of Central Florida, Orlando, FL, USA","2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)","5 Oct 2021","2021","","","1188","1193","Many robot manipulation skills can be represented with deterministic characteristics and there exist efficient techniques for learning parameterized motor plans for those skills. However, one of the active research challenge still remains to sustain manipulation capabilities in situation of a mechanical failure. Ideally, like biological creatures, a robotic agent should be able to reconfigure its control policy by adapting to dynamic adversaries. In this paper, we propose a method that allows an agent to survive in a situation of mechanical loss, and adaptively learn manipulation with compromised degrees of freedom-we call our method Survivable Robotic Learning (SRL). Our key idea is to leverage Bayesian policy gradient by encoding knowledge bias in posterior estimation, which in turn alleviates future policy search explorations, in terms of sample efficiency and when compared to random exploration based policy search methods. SRL represents policy priors as Gaussian process, which allows tractable computation of approximate posterior (when true gradient is intractable), by incorporating guided bias as proxy from prior replays. We evaluate our proposed method against off-the-shelf model free learning algorithm (DDPG), testing on a hexapod robot platform which encounters incremental failure emulation, and our experiments show that our method improves largely in terms of sample requirement and quantitative success ratio in all failure modes. A demonstration video of our experiments can be viewed at: https://sites.google.com/view/survivalrl","2161-8089","978-1-6654-1873-7","10.1109/CASE49439.2021.9551472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551472","Survivable Robotic Control;Bayesian Learning;Guided Policy Search","Heuristic algorithms;Search methods;Emulation;Estimation;Reinforcement learning;Gaussian processes;Encoding","Bayes methods;Gaussian processes;learning (artificial intelligence);mobile robots","failure modes;incremental failure emulation;hexapod robot platform;off-the-shelf model free learning algorithm;guided bias;approximate posterior;policy priors;random exploration based policy search methods;sample efficiency;future policy search explorations;posterior estimation;knowledge bias;leverage Bayesian policy gradient;method Survivable Robotic Learning;compromised degrees;mechanical loss;dynamic adversaries;control policy;robotic agent;biological creatures;mechanical failure;manipulation capabilities;active research challenge;parameterized motor plans;deterministic characteristics;robot manipulation skills;deep reinforcement Learning;guided Bayesian policy search;Survivable Robotic control","","1","","17","IEEE","5 Oct 2021","","","IEEE","IEEE Conferences"
"Variable Admittance Interaction Control of UAVs via Deep Reinforcement Learning","Y. Feng; C. Shi; J. Du; Y. Yu; F. Sun; Y. Song","School of Mechatronical Engineering, Beijing Institute of Technology, China; School of Mechatronical Engineering, Beijing Institute of Technology, China; School of Mechatronical Engineering, Beijing Institute of Technology, China; School of Mechatronical Engineering, Beijing Institute of Technology, China; Department of Computer Science and Technology, Tsinghua University, China; Department of Computer Science and Technology, Tsinghua University, China","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","1291","1297","A compliant control model based on reinforcement learning (RL) is proposed to allow robots to interact with the environment more effectively and autonomously execute force control tasks. The admittance model learns an optimal adjustment policy for interactions with the external environment using RL algorithms. The model combines energy consumption and trajectory tracking of the agent state using a cost function. Therein, an Unmanned Aerial Vehicle (UAV) can operate stably in unknown environments where interaction forces exist. Furthermore, the model ensures that the interaction process is safe, comfortable, and flexible while protecting the external structures of the UAV from damage. To evaluate the model performance, we verified the approach in a simulation environment using a UAV in three external force scenes. We also tested the model across different UAV platforms and various low-level control parameters, and the proposed approach provided the best results.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160558","National Natural Science Foundation of China(grant numbers:62173037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160558","","Deep learning;Energy consumption;Trajectory tracking;Force;Reinforcement learning;Autonomous aerial vehicles;Admittance","autonomous aerial vehicles;control engineering computing;deep learning (artificial intelligence);force control;learning (artificial intelligence);mobile robots;reinforcement learning","admittance model;agent state;compliant control model;cost function;deep reinforcement learning;different UAV platforms;energy consumption;external environment;external force scenes;external structures;force control tasks;interaction forces;interaction process;low-level control parameters;optimal adjustment policy;RL algorithms;simulation environment;trajectory tracking;Unmanned Aerial Vehicle","","1","","35","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Automated Reinforcement Learning Based on Parameter Sharing Network Architecture Search","Z. Wang; J. Zhang; Y. Li; Q. Gong; W. Luo; J. Zhao","Science and Technology on Aerospace Intelligent Control Laboratory, Beijing Aerospace Automatic Control Institute, Beijing, China; Science and Technology on Aerospace Intelligent Control Laboratory, Beijing Aerospace Automatic Control Institute, Beijing, China; Science and Technology on Aerospace Intelligent Control Laboratory, Beijing Aerospace Automatic Control Institute, Beijing, China; Science and Technology on Aerospace Intelligent Control Laboratory, Beijing Aerospace Automatic Control Institute, Beijing, China; Science and Technology on Aerospace Intelligent Control Laboratory, Beijing Aerospace Automatic Control Institute, Beijing, China; Beijing Aerospace Automatic Control Institute, China Academy of Launch Vehicle Technology, Beijing, China","2021 6th International Conference on Robotics and Automation Engineering (ICRAE)","4 Jan 2022","2021","","","358","363","The performance of machine learning depends on the choice of hyperparameters to a great extent. Only by choosing the appropriate hyperparameters can we learn the desired learning results. At present, the end-to-end learning algorithm is widely concerned in the academic circles, and realizes the agile design from the demand end to the execution end at the design task level, which can dramatically reduce the complexity of the design. However, there are still a large number of hyperparameters, which need to be tuned manually, increasing the difficulty of machine learning application. Thus, with the continuous development of high-performance parallel computing, automated machine learning method arises. In this paper, aiming at the automatic design of the hyperparameter, the neural network architecture of deep reinforcement learning in the field of motion control, LSTM recurrent neural network topology generation algorithm, parameter sharing based fast reinforcement learning and evaluation mechanism, and graph generator parameter learning algorithm based on policy gradient are combined. An automated search and optimization framework of neural network architecture in the deep reinforcement learning is proposed, realizing the automated generation of network architecture. Finally, the effectiveness of the proposed approach is verified by taking the lunar lander landing control problem as an example.","","978-1-6654-0697-0","10.1109/ICRAE53653.2021.9657793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9657793","automated reinforcement learning;network architecture search;automated machine learning","Space vehicles;Motion planning;Machine learning algorithms;Recurrent neural networks;Moon;Reinforcement learning;Computer architecture","deep learning (artificial intelligence);parallel processing;recurrent neural nets;reinforcement learning","parameter sharing network architecture search;end-to-end learning algorithm;agile design;high-performance parallel computing;automated machine learning method;hyperparameter;neural network architecture;deep reinforcement learning;neural network topology generation algorithm;graph generator parameter;motion control;parameter sharing","","1","","10","IEEE","4 Jan 2022","","","IEEE","IEEE Conferences"
"Self-Supervised Reinforcement Learning for Active Object Detection","F. Fang; W. Liang; Y. Wu; Q. Xu; J. -H. Lim","Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore","IEEE Robotics and Automation Letters","2 Aug 2022","2022","7","4","10224","10231","Active object detection (AOD) offers significant advantage in expanding the perceptual capacity of a robotics system. AOD is formulated as a sequential action decision process to determine optimal viewpoints to identify objects of interest in a visual scene. While reinforcement learning (RL) has been successfully used to solve many AOD problems, conventional RL methods suffer from (i) sample inefficiency, and (ii) unstable outcome due to inter-dependencies of action type (direction of view change) and action range (step size of view change). To address these issues, we propose a novel self-supervised RL method, which employs self-supervised representations of viewpoints to initialize the policy network, and a self-supervised loss on action range to enhance the network parameter optimization. The output and target pairs of self-supervised learning loss are automatically generated from the policy network online prediction and a range shrinkage algorithm (RSA), respectively. The proposed method is evaluated and benchmarked on two public datasets (T-LESS and AVD) using on-policy and off-policy RL algorithms. The results show that our method enhances detection accuracy and achieves faster convergence on both datasets. By evaluating on a more complex environment with a larger state space (where viewpoints are more densely sampled), our method achieves more robust and stable performance. Our experiment on real robot application scenario to disambiguate similar objects in a cluttered scene has also demonstrated the effectiveness of the proposed method.","2377-3766","","10.1109/LRA.2022.3193019","Agency for Science, Technology and Research(grant numbers:A18A2b0046); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9835012","Active perception;active object detection;path planing;self-supervised learning;reinforcement learning","Visualization;Robots;Task analysis;Self-supervised learning;Object recognition;Object detection;Prediction algorithms","object detection;reinforcement learning","robotics system;sequential action decision process;optimal viewpoints;visual scene;AOD problems;sample inefficiency;unstable outcome;action type;view change;action range;novel self-supervised RL method;self-supervised representations;policy network;self-supervised loss;network parameter optimization;self-supervised learning loss;range shrinkage algorithm;off-policy RL algorithms;detection accuracy;supervised reinforcement learning;active object detection;perceptual capacity","","1","","36","IEEE","21 Jul 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning with Omnidirectional Images: application to UAV Navigation in Forests","C. -O. Artizzu; G. Allibert; C. Demonceaux","Université Côte d’Azur, CNRS, I3S, France; Université Côte d’Azur, CNRS, I3S, France; ImViA, Université Bourgogne Franche-Comté, France","2022 17th International Conference on Control, Automation, Robotics and Vision (ICARCV)","10 Jan 2023","2022","","","229","234","Deep Reinforcement Learning (DRL) is highly efficient for solving complex tasks such as drone obstacle avoidance using cameras. However, these methods are often limited by the camera perception capabilities. In this paper, we demonstrate that point-goal navigation performances can be improved by using cameras with a wider Field-Of-View (FOV). To this end, we present a DRL solution based on equirectangular images and demonstrates its relevance, especially compared to its perspective version. Several visual modalities are compared: ground truth depth, RGB, and depth directly estimated from these $360^{\circ}$ RGB images using Deep Learning methods. Next, we propose a spherical adaptation to take into account the spherical distortions of omnidirectional images in the convolutional neural networks (CNNs) used in the actor-critic network and show a significant improvement in navigation performance. Finally, we modify the perspective depth estimation network using this spherical adaptation and demonstrate a further performance improvement.","","978-1-6654-7687-4","10.1109/ICARCV57592.2022.10004267","ANR CLARA(grant numbers:ANR-18-CE33-0004); GENCI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10004267","","Deep learning;Visualization;Navigation;Reinforcement learning;Forestry;Cameras;Distortion","autonomous aerial vehicles;cameras;computerised instrumentation;convolutional neural nets;deep learning (artificial intelligence);image colour analysis;navigation;reinforcement learning;spatial variables measurement","actor-critic network;camera perception capabilities;CNN;convolutional neural networks;deep reinforcement learning;depth estimation network;DRL solution;drone obstacle avoidance;equirectangular imaging;field-of-view;forests;FOV;ground truth depth;omnidirectional imaging;point-goal navigation performances;RGB imaging;spherical adaptation distortions;UAV navigation performance;visual modalities","","1","","23","IEEE","10 Jan 2023","","","IEEE","IEEE Conferences"
"Distilling a Hierarchical Policy for Planning and Control via Representation and Reinforcement Learning","J. -S. Ha; Y. -J. Park; H. -J. Chae; S. -S. Park; H. -L. Choi","TU Berlin; NAVER CLOVA, NAVER Corp.; KAIST; KAIST; KAIST","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","4459","4466","We present a hierarchical planning and control framework that enables an agent to perform various tasks and adapt to a new task flexibly. Rather than learning an individual policy for each particular task, the proposed framework, DISH, distills a hierarchical policy from a set of tasks by representation and reinforcement learning. The framework is based on the idea of latent variable models that represent high-dimensional observations using low-dimensional latent variables. The resulting policy consists of two levels of hierarchy: (i) a planning module that reasons a sequence of latent intentions that would lead to an optimistic future and (ii) a feedback control policy, shared across the tasks, that executes the inferred intention. Because the planning is performed in low-dimensional latent space, the learned policy can immediately be used to solve or adapt to new tasks without additional training. We demonstrate the proposed framework can learn compact representations (3- and 1-dimensional latent states and commands for a humanoid with 197- and 36-dimensional state features and actions) while solving a small number of imitation tasks, and the resulting policy is directly applicable to other types of tasks, i.e., navigation in cluttered environments.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561017","","Training;State feedback;Navigation;Conferences;Humanoid robots;Reinforcement learning;Data models","inference mechanisms;multi-agent systems;planning (artificial intelligence);reinforcement learning","hierarchical policy;hierarchical planning;reinforcement learning;latent variable models;high-dimensional observations;low-dimensional latent variables;planning module;latent intentions;feedback control policy;low-dimensional latent space;learned policy;imitation tasks","","1","","49","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
