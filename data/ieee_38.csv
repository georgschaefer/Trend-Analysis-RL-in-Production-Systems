"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Multi-Agent Reinforcement Learning Based Actuator Control for EV HVAC Systems","S. Joo; D. Lee; M. Kim; T. Lee; S. Choi; S. Kim; J. Lee; J. Kim; Y. Lim; J. Lee","MakinaRocks, Seoul, Republic of Korea; MakinaRocks, Seoul, Republic of Korea; MakinaRocks, Seoul, Republic of Korea; MakinaRocks, Seoul, Republic of Korea; MakinaRocks, Seoul, Republic of Korea; MakinaRocks, Seoul, Republic of Korea; MakinaRocks, Seoul, Republic of Korea; Hanon Systems, Deajeon, Republic of Korea; MakinaRocks, Seoul, Republic of Korea; Hanon Systems, Deajeon, Republic of Korea","IEEE Access","26 Jan 2023","2023","11","","7574","7587","While electric vehicles (EVs) continue to draw more attention as an alternative to traditional fossil fuel vehicles, the relatively short driving range of EVs is often pointed out as their biggest drawback. In terms of energy consumption, one of the most energy-intensive systems in EVs is the heating, ventilation, and air conditioning (HVAC) system. Most HVAC systems use On/Off or PID control for the actuators, but these control methods have low efficiency and are difficult to apply in multiple-input multiple-output systems. In this paper, we propose a novel multi-agent deep reinforcement learning (MADRL) method to efficiently control the low-level actuators of the EV HAVC systems. Through this method, multiple objectivs such as setpoint temperature, subcooling and efficiency can be considered simultaneously by giving independent rewards for each actuator agent. The proposed method is evaluated via a actual vehicle simulator, and experimental results show that the MADRL-based method consumes only 53% of the energy consumption of PID control on average in a transient phase.","2169-3536","","10.1109/ACCESS.2022.3227450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9973292","Multi-agent reinforcement learning;energy consumption efficiency;HVAC;EV;RL","HVAC;Actuators;PI control;PD control;Temperature control;Reinforcement learning;Convergence","actuators;deep learning (artificial intelligence);electric vehicles;energy conservation;energy consumption;HVAC;multi-agent systems;reinforcement learning;three-term control","actuator agent;actuator control;air conditioning system;electric vehicles;energy consumption;energy-intensive systems;EV HVAC systems;fossil fuel vehicles;heating-ventilation-and-air conditioning;low-level actuators;MADRL-based method;multiagent deep reinforcement learning method;multiple-input multiple-output systems;PID control;subcooling;vehicle simulator","","1","","42","CCBY","7 Dec 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Control Strategy for Multi-Agent Systems Subjected to Actuator Cyberattacks During Affine Formation Maneuvers","S. El-Ferik; M. Maaruf; F. M. Al-Sunni; A. A. Saif; M. M. Al Dhaifallah","Control and Instrumentation Engineering Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Control and Instrumentation Engineering Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Control and Instrumentation Engineering Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Control and Instrumentation Engineering Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Control and Instrumentation Engineering Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia","IEEE Access","31 Jul 2023","2023","11","","77656","77668","In this research, we investigate the reinforcement learning-based control strategy for second-order continuous-time multi-agent systems (MASs) subjected to actuator cyberattacks during affine formation maneuvers. In this case, a long-term performance index is created to track the MASs tracking faults using a leader-follower structure. In order to approximate the ideal solution, which is challenging to find for systems vulnerable to cyberattacks during time-varying maneuvers, a critical neural network is used. The distributed control protocol is obtained, and the long-term performance index is minimized, using an actor neural network strengthened with critic signals. The actor-critic neural networks calculate unknown dynamics and the severity of attacks on the MAS actuators. The Nussbaum functions are applied to address this issue since attacks can result in a loss of control direction. The stability of the closed-loop system has been emphasized with the use of a Lyapunov candidate function. The performance of the suggested strategy is then supported by a numerical simulation.","2169-3536","","10.1109/ACCESS.2023.3296741","Research Center for Smart Mobility and Logistics, King Fahd University of Petroleum and Minerals(grant numbers:INML2300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10187130","Reinforcement learning;actor-critic neural networks;multi-agent systems;affine formation maneuver;actuator attacks;Nussbaum functions","Actuators;Stress;Neural networks;Frequency modulation;Protocols;Cyberattack;Robot sensing systems;Reinforcement learning;Neural networks;Multi-agent systems","adaptive control;closed loop systems;control system synthesis;distributed control;Lyapunov methods;multi-agent systems;neurocontrollers;nonlinear control systems;reinforcement learning;stability","actor neural network;actor-critic neural networks;actuator cyberattacks;affine formation maneuvers;closed-loop system;control direction;critical neural network;distributed control protocol;long-term performance index;MAS actuators;MASs tracking faults;multiagent systems subjected;reinforcement learning-based control strategy;second-order continuous-time multiagent systems;time-varying maneuvers","","1","","62","CCBYNCND","19 Jul 2023","","","IEEE","IEEE Journals"
"Multiagent Reinforcement Learning for Strategic Decision Making and Control in Robotic Soccer Through Self-Play","B. Brandão; T. W. De Lima; A. Soares; L. Melo; M. R. O. A. Maximo","Deep Learning Brazil, Federal University of Goiás (UFG), Goiânia, Goiás, Brazil; Deep Learning Brazil, Federal University of Goiás (UFG), Goiânia, Goiás, Brazil; Deep Learning Brazil, Federal University of Goiás (UFG), Goiânia, Goiás, Brazil; Deep Learning Brazil, Federal University of Goiás (UFG), Goiânia, Goiás, Brazil; Autonomous Computational Systems Laboratory (LAB-SCA), Computer Science Division, Aeronautics Institute of Technology, São José dos Campos, São Paulo, Brazil","IEEE Access","15 Jul 2022","2022","10","","72628","72642","Reinforcement Learning (RL) has shown promising performance in environments for both robotic control and strategic decision making. However, they are usually treated as separate problems with different objectives. In this work, we propose the use of Reinforcement Learning to solve both control and strategic problems as one, in a multi-agent robotic soccer environment. We use the IEEE Very Small Size Soccer (VSSS) challenge from the Latin American Robotics Competition (LARC) as a study case. In the VSSS, two autonomous teams of wheeled robots compete by pushing the ball around to score goals. To unify both control and strategy problems, our approach gives full control of the actuators’ speed to the RL algorithm whilst keeping the broader objective of winning the game. Our method achieves win rates as high as 93% against hand-coded heuristic strategies. In this work we contribute by developing an RL agent that can learn from self-play and generalize against new opponents. Our methodology uses multi-agent Reinforcement Learning with self-play in order to build up the knowledge for complex tasks. We also developed a simulated environment for the robotic soccer game.","2169-3536","","10.1109/ACCESS.2022.3189021","Brazilian Coordination for the Improvement of Higher Education Personnel (CAPES)(grant numbers:88882.385785/2019-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9817118","Decision making;multi-agent;reinforcement learning;self-play;strategy","Robots;Robot kinematics;Reinforcement learning;Games;Sports;Decision making;Task analysis","actuators;control engineering computing;decision making;mobile robots;multi-agent systems;multi-robot systems;reinforcement learning;velocity control","wheeled robots;RL agent;multiagent reinforcement learning;robotic soccer game;strategic decision making;robotic control;IEEE Very Small Size Soccer challenge;VSSS;Latin American Robotics Competition;multiagent robotic soccer;self-play;actuators speed control","","1","","46","CCBY","7 Jul 2022","","","IEEE","IEEE Journals"
"Highlighted Map for Mobile Robot Localization and Its Generation Based on Reinforcement Learning","R. Yoshimura; I. Maruta; K. Fujimoto; K. Sato; Y. Kobayashi","Regional Technology Support Division, Tokyo Metropolitan Industrial Technology Research Institute, Tokyo, Japan; Graduate School of Engineering, Kyoto University, Kyoto, Japan; Graduate School of Engineering, Kyoto University, Kyoto, Japan; Business Development Section, Tokyo Metropolitan Industrial Technology Research Institute, Tokyo, Japan; Business Development Section, Tokyo Metropolitan Industrial Technology Research Institute, Tokyo, Japan","IEEE Access","18 Nov 2020","2020","8","","201527","201544","This article proposes a new kind of map for mobile robot localization and its generation method. We call the map a highlighted map, on which uniquely shaped objects (landmarks) in monotonous environments are highlighted. By using this map, robots can use such landmarks as clues for localization, and thus, their localization performance can be improved without having to update their sensors or online computation. Furthermore, this map can be easily combined with many other existing localization algorithms. We formulate the problem of making a highlighted map and propose a numerical optimization method based on reinforcement learning. This optimization method automatically identifies and emphasizes the important landmarks on the map. The generated highlighted map is adapted to situations such as the sensor characteristics and robot dynamics because this method uses the actual sensor measurement data. It is proven that the optimization converges under certain technical assumptions. We performed a numerical simulation and real-world experiment showing that the highlighted map provides better localization accuracy than a conventional map.","2169-3536","","10.1109/ACCESS.2020.3035725","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9247228","Highlighted map;mobile robots;Monte Carlo localization;particle filters;reinforcement learning","Robot sensing systems;Mobile robots;Sensor phenomena and characterization;Shape;Robot kinematics;Noise measurement;Reinforcement learning","learning (artificial intelligence);mobile robots;optimisation;path planning;robot dynamics;SLAM (robots)","reinforcement learning;highlighted map;conventional map;mobile robot localization;localization algorithms","","1","","43","CCBY","3 Nov 2020","","","IEEE","IEEE Journals"
"A Controllable Agent by Subgoals in Path Planning Using Goal-Conditioned Reinforcement Learning","G. T. Lee; K. Kim","Department of Industrial and Systems Engineering, Rutgers University, The State University of New Jersey, Piscataway, NJ, USA; Department of Medicine, Channing Division of Network Medicine, Brigham and Women's Hospital and Harvard Medical School, Boston, MA, USA","IEEE Access","10 Apr 2023","2023","11","","33812","33825","The aim of path planning is to search for a path from the starting point to the goal. Numerous studies, however, have dealt with a single predefined goal. That is, an agent who has completed learning cannot reach other goals that have not been visited in the training. In the present study, we propose a novel reinforcement learning (RL) framework for an agent reachable to any subgoal as well as the final goal in path planning. To do this, we utilize goal-conditioned RL and propose bidirectional memory editing to obtain various bidirectional trajectories of the agent. Bidirectional memory editing can generate various behavior and subgoals of the agent from the limited trajectory. Then, the generated subgoals and behaviors of the agent are trained on the policy network so that the agent can reach any subgoals from any starting point. In addition, we present reward shaping for the short path of the agent to reach the goal. In the experimental result, the agent was able to reach the various goals that had never been visited by the agent during the training. We confirmed that the agent could perform difficult missions, such as a round trip, and the agent used the shorter route with reward shaping.","2169-3536","","10.1109/ACCESS.2023.3264264","Center for Discrete Mathematics and Theoretical Computer Science (DIMACS) through NSF(grant numbers:CCF-1445755); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10091554","Controllable agent;path planning;goal-conditioned reinforcement learning;bidirectional memory editing","Trajectory;Training;Behavioral sciences;Robots;Reinforcement learning;Task analysis;Memory","learning (artificial intelligence);mobile robots;path planning;reinforcement learning","agent reachable;bidirectional memory editing;bidirectional trajectories;controllable agent;generated subgoals;goal-conditioned reinforcement learning;goal-conditioned RL;path planning;reinforcement learning framework;short path;single predefined goal;subgoal","","1","","76","CCBYNCND","3 Apr 2023","","","IEEE","IEEE Journals"
"Utilizing Skipped Frames in Action Repeats for Improving Sample Efficiency in Reinforcement Learning","T. M. Luu; T. Nguyen; T. Vu; C. D. Yoo","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea","IEEE Access","24 Jun 2022","2022","10","","64965","64975","Action repeat has become the de-facto mechanism in deep reinforcement learning (RL) for stabilizing training and enhancing exploration. Here, the action is taken at the action-decision point and is executed repeatedly for a designated number of times until the next decision point. Although showing several advantages, in this mechanism, the intermediate states which stem from repeated actions are discarded in training agents, causing sample inefficiency. To utilize the discarded states as training data is nontrivial as the action, which causes the transition between these states, is unavailable. This paper proposes to infer the action at the intermediate states via an inverse dynamic model. The proposed method is simple and easily incorporated into the existing off-policy RL algorithms - integrating the proposed method with SAC shows consistent improvement across various tasks.","2169-3536","","10.1109/ACCESS.2022.3182107","Institute for Information & communications Technology Promotion (IITP); Korea government (MSIT)(grant numbers:2021-0-01381); Institute of Information & communications Technology Planning & Evaluation (IITP); Korea government (MSIT)(grant numbers:2022-0-00951); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793636","Action repeat mechanism;off-policy reinforcement learning;reinforcement learning;sample efficiency","Task analysis;Training;Heuristic algorithms;Benchmark testing;Data models;Training data;Robots","deep learning (artificial intelligence);reinforcement learning","skipped frames;action repeat;de-facto mechanism;deep reinforcement learning;sample inefficiency;inverse dynamic model;off-policy RL algorithms;SAC","","1","","46","CCBY","10 Jun 2022","","","IEEE","IEEE Journals"
"Integrating Multiple Policies for Person-Following Robot Training Using Deep Reinforcement Learning","C. K. Dewa; J. Miura","Department of Computer Science and Engineering, Toyohashi University of Technology, Toyohashi, Japan; Department of Computer Science and Engineering, Toyohashi University of Technology, Toyohashi, Japan","IEEE Access","25 May 2021","2021","9","","75526","75541","Given a training environment which follows Markov decision process for a specific task, a deep reinforcement learning (DRL) agent is able to find possible optimal policies which map states of the environment to appropriate actions by repeatedly trying various actions to maximize training rewards. However, the learned policies cannot be reused directly in the training process for other new tasks resulting wasted precious time and resources. To solve this problem, we propose a DRL-based method for training an agent capable of selecting the appropriate policy for current state of the environment from a set of previously trained optimal policies for a given task which can be decomposed into other sub tasks. We implement our proposed method to a person-following robot task training that can be broken down into three sub tasks, namely: navigation, left attending, and right attending. Using the proposed method, the previously learned optimal navigation policy obtained from our previous work is integrated to attending policies which are trained in this study. We also introduce the use of weight-scheduled action smoothing which is able to stabilize actions generated by the agent in the attending task training. Our experiment results show that the proposed method is able to integrate all sub policies using the action smoothing method even though the navigation and the attending policies have dissimilar input structures, unalike output ranges, and are trained in different ways. Moreover, our proposed method shows better results compared to training from scratch and training using transfer learning strategy.","2169-3536","","10.1109/ACCESS.2021.3082136","Japan Society for the Promotion of Science (JSPS) KAKENHI(grant numbers:17H01799); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9437214","Deep reinforcement learning;policies integration;person-following robot;robot training","Task analysis;Robots;Training;Navigation;Smoothing methods;Reinforcement learning;Collision avoidance","learning (artificial intelligence);Markov processes;mobile robots;navigation","multiple policies;person-following robot training;training environment;Markov decision process;deep reinforcement learning agent;possible optimal policies;map states;training rewards;learned policies;training process;precious time;DRL-based method;appropriate policy;trained optimal policies;person-following robot task training;learned optimal navigation policy;attending policies;weight-scheduled action smoothing;attending task training;action smoothing method","","1","","51","CCBY","20 May 2021","","","IEEE","IEEE Journals"
"Sensitivity Adaptation of Lower-Limb Exoskeleton for Human Performance Augmentation Based on Deep Reinforcement Learning","R. Zheng; Z. Yu; H. Liu; Z. Zhao; J. Chen; L. Jia","School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China; Beijing Institute of Precision Mechatronics and Controls, Beijing, China; Beijing Institute of Precision Mechatronics and Controls, Beijing, China; Beijing Institute of Precision Mechatronics and Controls, Beijing, China; Beijing Institute of Precision Mechatronics and Controls, Beijing, China; Beijing Institute of Precision Mechatronics and Controls, Beijing, China","IEEE Access","13 Apr 2023","2023","11","","36029","36040","The lower-limb exoskeleton for human performance augmentation (LEHPA) in sensitivity amplification control (SAC) is vulnerable to model parameter uncertainties and unmodeled dynamics due to its large sensitivity to external disturbances resulting from the positive feedback by the inverse dynamic model of the exoskeleton. This paper firstly proposes to combine SAC with deep reinforcement learning (DRL) to reduce the dependence on the model accuracy and tackle the ever-changing human-exoskeleton interaction (HEI) dynamics. The sensitivity adjustment is interpreted as finding the optimal policy for a Markov Decision Process (MDP) and solved using deep reinforcement learning algorithms. To train the policy safely and efficiently, a multibody simulation environment is created to implement the training process, accompanied by a novel hybrid inverse-forward dynamics simulation method to carry out the simulation. For comparison purposes, the SAC controller is introduced as a benchmark. A novel performance evaluation method based on the HEI forces at the back, thighs, and shanks is proposed to evaluate the control effect of the trained SADRL controller quantitatively. The SADRL controller is compared with the SAC controller at five specified walking speeds, resulting in a lumped HEI force ratio as low as 0.54. The total decrease of HEI forces demonstrates the superior control effect of the SADRL strategy.","2169-3536","","10.1109/ACCESS.2023.3265895","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10097746","Lower-limb exoskeleton for human performance augmentation;sensitivity amplification control;sensitivity adaptation;deep reinforcement learning","Exoskeletons;Sensitivity;Legged locomotion;Mathematical models;Deep learning;Performance evaluation;Reinforcement learning","deep learning (artificial intelligence);gait analysis;learning (artificial intelligence);Markov processes;medical robotics;motion control;patient rehabilitation;reinforcement learning;robot dynamics;wearable robots","deep reinforcement learning algorithms;external disturbances;HEI forces;human performance augmentation;human-exoskeleton interaction dynamics;inverse dynamic model;lower-limb exoskeleton;lumped HEI force ratio;Markov Decision Process;model accuracy;model parameter uncertainties;multibody simulation environment;novel hybrid inverse-forward dynamics simulation method;novel performance evaluation method;optimal policy;positive feedback;SAC controller;sensitivity adaptation;sensitivity adjustment;sensitivity amplification control;superior control effect;trained SADRL controller;training process;unmodeled dynamics","","1","","51","CCBYNCND","10 Apr 2023","","","IEEE","IEEE Journals"
"A Biologically Constrained Cerebellar Model With Reinforcement Learning for Robotic Limb Control","R. Liu; Q. Zhang; Y. Chen; J. Wang; L. Yang","Liaoning Key Laboratory of Integrated Circuit and Biomedical Electronic System, School of Biomedical Engineering, Dalian University of Technology, Dalian, China; Liaoning Key Laboratory of Integrated Circuit and Biomedical Electronic System, School of Biomedical Engineering, Dalian University of Technology, Dalian, China; Liaoning Key Laboratory of Integrated Circuit and Biomedical Electronic System, School of Biomedical Engineering, Dalian University of Technology, Dalian, China; Liaoning Key Laboratory of Integrated Circuit and Biomedical Electronic System, School of Biomedical Engineering, Dalian University of Technology, Dalian, China; Department of Electrical and Computer Engineering, University of Canterbury, Christchurch, New Zealand","IEEE Access","21 Dec 2020","2020","8","","222199","222210","The cerebellum is known to be critical for accurate adaptive control and motor learning. It has long been recognized that the cerebellum acts as a supervised learning machine. However, recent evidence shows that cerebellum is integral to reinforcement learning. This paper proposes a biologically plausible cerebellar model with reinforcement learning based on the cerebellar neural circuitry to eliminate the need for explicit teacher signals. The learning capacity of cerebellar reinforcement learning is first demonstrated by constructing a simulated cerebellar neural network agent and a detailed model of the human arm and muscle system in the Emergent virtual environment. Next, the cerebellar model is incorporated in both a simulated arm and a Geomagic Touch device to further verify the effectiveness of the cerebellar model in reaching tasks. Results from these experiments indicate that the cerebellar simulation is capable of driving the “arm plant” to arrive at the target positions accurately. Moreover, by examining the effect of the number of basic units, we find the results are consistent with previous findings that the central nervous system may recruit the muscle synergies to realize motor control. The study described here prompts several hypotheses about the relationship between motor control and learning and may be useful in the development of general-purpose motor learning systems for machines.","2169-3536","","10.1109/ACCESS.2020.3042994","National Natural Science Foundation of China (NSFC)(grant numbers:81741137); Liaoning Provincial Natural Science Foundation of China(grant numbers:2020-KF-12-04); China Postdoctoral Science Foundation Funded Project(grant numbers:2020M670714); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9285238","Cerebellum;cerebellar model;reinforcement learning;robotic limb control","Cerebellum;Brain modeling;Computer architecture;Microprocessors;Integrated circuit modeling;Reinforcement learning;Adaptation models","adaptive control;biocontrol;brain models;learning (artificial intelligence);learning systems;neurocontrollers;neurophysiology;robots","cerebellar neural circuitry;cerebellar reinforcement learning;simulated cerebellar neural network agent;human arm;muscle system;cerebellar simulation;motor control;biologically constrained cerebellar model;robotic limb control;supervised learning machine;biologically plausible cerebellar model;adaptive control;Geomagic touch device;general-purpose motor learning systems","","1","","39","CCBY","7 Dec 2020","","","IEEE","IEEE Journals"
"Automatic Curriculum Design for Object Transportation Based on Deep Reinforcement Learning","G. Eoh; T. -H. Park","Industrial AI Research Center, Chungbuk National University, Cheongju, South Korea; Industrial AI Research Center, Chungbuk National University, Cheongju, South Korea","IEEE Access","12 Oct 2021","2021","9","","137281","137294","This paper presents an automatic curriculum learning (ACL) method for object transportation based on deep reinforcement learning (DRL). Previous studies on object transportation using DRL have a sparse reward problem that an agent receives a rare reward for only the transportation completion of an object. Generally, curriculum learning (CL) has been used to solve the sparse reward problem. However, the conventional CL methods should be manually designed by users, which is difficult and tedious work. Moreover, there were no standard CL methods for object transportation. Therefore, we propose an ACL method for object transportation in which human intervention is unnecessary at the training step. A robot automatically designs curricula itself and iteratively trains according to the curricula. First, we define the difficult level of object transportation using a map, which is determined by the predicted travelling distance of an object and the existence of obstacles and walls. In the beginning, a robot learns the object transportation at an easy level (i.e., travelling distance is short and there are less obstacles around), then learns a difficult task (i.e., the long travelling distance of an object is required and there are many obstacles around). Second, training time also affects the performance of object transportation, and thus, we suggest an adaptive determining method of the number of training episodes. The number of episodes for training is adaptively determined based on the current success rate of object transportation. We verified the proposed method in simulation environments, and the success rate of the proposed method was 14% higher than no-curriculum. Also, the proposed method showed 63% (maximum) and 14% (minimum) higher success rates compared with the manual curriculum methods. Additionally, we conducted real experiments to verify the gap between simulation and practical results.","2169-3536","","10.1109/ACCESS.2021.3118109","Institute for Information and Communications Technology Planning and Evaluation (IITP) through the Ministry of Science and ICT (MSIT), South Korea(grant numbers:IITP-2021-2020-0-01462); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9559919","Curriculum learning;object transportation;deep reinforcement learning;difficulty level","Transportation;Robots;Training;Grasping;Reinforcement learning;Tools;Task analysis","collision avoidance;computer based training;deep learning (artificial intelligence);educational courses;educational robots;mobile robots;transportation","object transportation;deep reinforcement learning;automatic curriculum learning;automatic curriculum design;ACL;DRL;sparse reward problem;transportation completion;travelling distance;training time;adaptive determining method;training episodes;robot learning","","1","","58","CCBYNCND","5 Oct 2021","","","IEEE","IEEE Journals"
"Generative Adversarial Inverse Reinforcement Learning With Deep Deterministic Policy Gradient","M. Zhan; J. Fan; J. Guo","School of Electrical and Control Engineering, North China University of Technology, Beijing, China; Intelligent Transportation Key Laboratory, North China University of Technology, Beijing, China; Tianjin Vocational Institute, Tianjin, China","IEEE Access","22 Aug 2023","2023","11","","87732","87746","Although the issue of sparse expert samples at the early stage of training in inverse reinforcement learning (IRL) is successfully resolved by the introduction of generative adversarial network (GAN), the inherent drawbacks of GAN result in ineffective generated samples. Therefore, we propose an algorithm for generative adversarial inverse reinforcement learning that is based on deep deterministic policy gradient (DDPG). We use the deterministic strategy to replace the random noise input of the initial GAN model and reconstruct the generator of the GAN based on the Actor-Critic mechanism in order to improve the quality of GAN-generated samples during adversarial training. Meanwhile, we mix the GAN-generated virtual samples with the original expert samples of IRL as the expert sample set of IRL. Our approach not only solves the problem of sparse expert samples at the early stage of training, but most importantly, it makes the decision-making process of IRL occurring under GAN more efficient. In the subsequent IRL decision-making process, we also analyze the differences between the mixed expert samples and the non-expert trajectory samples generated by the initial strategy to determine the best reward function. The learned reward function is used to drive the RL process positively for policy updating and optimization, on which further non-expert trajectory samples are generated. By comparing the differences between the new non-expert samples and the mixed expert sample set, we hope to iteratively arrive at the reward function and optimal policy. Performance tests in the MuJoCo physical simulation environment and trajectory prediction experiments in Grid World show that our model improves the quality of GAN-generated samples and reduces the computational cost of the network training by approximately 20% for each given environment, applying to decision planning for autonomous driving.","2169-3536","","10.1109/ACCESS.2023.3305453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10217826","Inverse reinforcement learning;generative adversarial networks;deep deterministic policy gradient","Generative adversarial networks;Reinforcement learning;Training;Trajectory;Task analysis;Generators;Autonomous vehicles;Inverse problems;Deep learning","decision making;deep learning (artificial intelligence);gradient methods;reinforcement learning","actor-critic mechanism;DDPG;deep deterministic policy gradient;GAN-generated virtual samples;generative adversarial inverse reinforcement learning;generative adversarial network;IRL decision-making process;mixed expert sample set;nonexpert trajectory samples;sparse expert samples","","","","43","CCBYNCND","15 Aug 2023","","","IEEE","IEEE Journals"
"CST-RL: Contrastive Spatio-Temporal Representations for Reinforcement Learning","C. -K. Ho; C. -T. King","Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan","IEEE Access","23 Mar 2023","2023","11","","26820","26831","Learning representations from high-dimensional observations is critical for training of pixel-based continuous control tasks with reinforcement learning (RL). Without proper representations, the training will be very inefficient, requiring long training time and huge training data to learn directly from low-level pixel observations. Yet, a lot of information in such observations may be redundant or irrelevant. A common approach to solving this problem is to train auxiliary objectives alongside the main RL objective. The additional objectives provide more signals to the model and reduce the training time, resulting in better sample efficiency. A representative work is Contrastive Unsupervised Representations for Reinforcement Learning (CURL), which leverages contrastive learning to assist RL to learn useful representations. Although CURL performs very well in extracting spatial information from pixel inputs, it is found to overlook potential temporal signals. In this paper, a contrastive spatio-temporal representation learning framework for RL, called CST-RL, is introduced, which leverages 3D Convolutional Neural Network (3D CNN) alongside contrastive learning for sample-efficient RL. It pays attention to both spatial and temporal signals in pixel observations. Experiments based on DMControl show that CST-RL outperforms CURL in all six environments after 500K environment steps and only needs half of the steps to achieve the standard score in the majority of cases.","2169-3536","","10.1109/ACCESS.2023.3258540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10075624","Reinforcement learning;3D CNNs;contrastive learning;spatio-temporal representation learning;sample efficiency","Spatiotemporal phenomena;Task analysis;Correlation;Three-dimensional displays;Representation learning;Reinforcement learning;Feature extraction","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image representation;image resolution;reinforcement learning;unsupervised learning","3D convolutional neural network;contrastive spatio-temporal representation;contrastive unsupervised representations for reinforcement learning;CST-RL;CURL;high-dimensional observations;low-level pixel observations;pixel-based continuous control tasks;sample-efficient RL;spatial information;spatial signals;temporal signals","","","","40","CCBYNCND","17 Mar 2023","","","IEEE","IEEE Journals"
"2D LiDAR Based Reinforcement Learning for Multi-Target Path Planning in Unknown Environment","N. Abdalmanan; K. Kamarudin; M. A. A. Bakar; M. H. F. Rahiman; A. Zakaria; S. M. Mamduh; L. M. Kamarudin","Faculty of Electrical Engineering and Technology, Universiti Malaysia Perlis (UniMAP), Arau, Malaysia; Faculty of Electrical Engineering and Technology, Universiti Malaysia Perlis (UniMAP), Arau, Malaysia; Faculty of Electrical Engineering and Technology, Universiti Malaysia Perlis (UniMAP), Arau, Malaysia; Faculty of Electrical Engineering and Technology, Universiti Malaysia Perlis (UniMAP), Arau, Malaysia; Faculty of Electrical Engineering and Technology, Universiti Malaysia Perlis (UniMAP), Arau, Malaysia; Centre of Excellence for Advanced Sensor Technology (CEASTech), Universiti Malaysia Perlis (UniMAP), Arau, Malaysia; Centre of Excellence for Advanced Sensor Technology (CEASTech), Universiti Malaysia Perlis (UniMAP), Arau, Malaysia","IEEE Access","13 Apr 2023","2023","11","","35541","35555","Global path planning techniques have been widely employed in solving path planning problems, however they have been found to be unsuitable for unknown environments. Contrarily, the traditional Q-learning method, which is a common reinforcement learning approach for local path planning, is unable to complete the task for multiple targets. To address these limitations, this paper proposes a modified Q-learning method, called Vector Field Histogram based Q-learning (VFH-QL) utilized the VFH information in state space representation and reward function, based on a 2D LiDAR sensor. We compared the performance of our proposed method with the classical Q-learning method (CQL) through training experiments that were conducted in a simulated environment with a size of 400 square pixels, representing a 20-meter square map. The environment contained static obstacles and a single mobile robot. Two experiments were conducted: experiment A involved path planning for a single target, while experiment B involved path planning for multiple targets. The results of experiment A showed that VFH-QL method had 87.06% less training time and 99.98% better obstacle avoidance compared to CQL. In experiment B, VFH-QL method was found to have an average training time that was 95.69% less than that of the CQL method and 83.99% better path quality. The VFH-QL method was then evaluated using a benchmark dataset. The results indicated that the VFH-QL exhibited superior path quality, with efficiency of 94.89% and improvements of 96.91% and 96.69% over CQL and SARSA in the task of path planning for multiple targets in unknown environments.","2169-3536","","10.1109/ACCESS.2023.3265207","Fundamental Research Grant Scheme (FRGS) through the Ministry of Higher Education of Malaysia (MOHE)(grant numbers:FRGS/1/2022/TK08/UNIMAP/03/13); Malaysian Technical University Network (MTUN) Research Grant by MOHE(grant numbers:9002-00094/9028-00002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10093847","RL;path planning;Q-learning;mobile robot","Path planning;Q-learning;Robot sensing systems;Training;Mobile robots;Convergence","collision avoidance;mobile robots;optical radar;reinforcement learning","2D LiDAR sensor;classical Q-learning method;global path planning techniques;local path planning;multitarget path planning;obstacle avoidance;reinforcement learning approach;single mobile robot;state space representation;unknown environment;vector field histogram based q-learning;VFH information;VFH-QL method","","","","38","CCBY","6 Apr 2023","","","IEEE","IEEE Journals"
"Procedural Content Generation Using Reinforcement Learning for Disaster Evacuation Training in a Virtual 3D Environment","J. Agarwal; S. Shridevi","School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, India; Centre for Advanced Data Science, Vellore Institute of Technology, Chennai, India","IEEE Access","14 Sep 2023","2023","11","","98607","98617","This research addresses the need for effective disaster evacuation training methods by proposing a virtual reality system that utilizes Reinforcement Learning Procedural Content Generation (RL-PCG) algorithms. The aim of this study is to provide a cost-effective and safe way to conduct disaster evacuation preparedness training, surpassing the limitations of traditional real-life drills. The paper’s objectives encompass the design of a novel 3-layer PCG architecture for generating realistic disaster simulations in virtual reality, the implementation of a working prototype for fire disaster scenarios, and the evaluation of the proposed system’s effectiveness through comparison with existing RL agents. Significant findings include the superiority of the RL-PCG agent in generating diverse and realistic disaster scenarios with faster training time and lesser number of steps, even with limited processor capabilities. In conclusion, this research establishes that the RL-PCG Scenario for Disaster Evacuation Training in VR is a more effective method, leading to improved disaster preparedness for individuals, and opens avenues for further advancements in disaster training using virtual reality and reinforcement learning technologies. For a video demo of this work, please visit https://youtu.be/3WZnQOfUP94.","2169-3536","","10.1109/ACCESS.2023.3313725","Vellore Institute of Technology, India; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10246257","Disaster evacuation training;procedural content generation;reinforcement learning;virtual reality","Training;Games;Solid modeling;Virtual reality;Reinforcement learning;Heuristic algorithms;Phonocardiography;Disaster management","","","","","","35","CCBY","11 Sep 2023","","","IEEE","IEEE Journals"
"The Optimization of RBFNN Gearshift Controller Parameters for Electric Vehicles Using PILCO Reinforcement Learning","Y. Liu; J. Zhang; Z. Lv; J. Ye","School of Electromechanical Engineering, Guangdong University of Technology, Guangzhou, China; School of Electromechanical Engineering, Guangdong University of Technology, Guangzhou, China; School of Electromechanical Engineering, Guangdong University of Technology, Guangzhou, China; School of Mechatronics Engineering, Foshan University, Foshan, China","IEEE Access","1 Sep 2023","2023","11","","92807","92821","In order to improve the efficiency of gearshift controller parameter optimization and obtain a good gearshift controller control effect, this article proposes an optimization method for electric vehicle transmission gearshift controller, and selects dual clutch transmission as the research object that establishes an 11-degree-of-freedom gearshift dynamics model and a feedforward-feedback gearshift control model. The feedforward control is chosen from the target trajectory given by the Legendre pseudo-spectral approach, and the feedback controller is a Gaussian kernel radial basis function neural network controller. The feedback controller performs parameter optimization by the Probabilistic Inference for Learning Control (PILCO) reinforcement learning algorithm to obtain a control strategy that matches the actual gearshift conditions. By comparing how well the main/secondary moving disk can follow the target trajectory during various optimization iterations, it is verified that the algorithm requires only a few experiments to complete the optimization, and the optimized Radial Basis Function Neural Network (RBFNN) control has a better control effect by comparing the results of different iterations. Applying the learned controller to various slope and load circumstances yields data that demonstrate that all can have an obvious optimization effect with good robustness. Additionally, the reinforcement learning technique suggested in this research can be used for various gearshift controller parameter optimization to assist engineers and technicians in increasing the effectiveness of their Research and Development.","2169-3536","","10.1109/ACCESS.2023.3307131","Natural Science Foundation of Guangdong Province(grant numbers:2022A1515012080); National Science Foundation of Guangdong Province(grant numbers:2019A1515110562); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10225519","Electric vehicle;dual clutch transmission;gearshift control;reinforcement learning","Optimization;Torque;Reinforcement learning;Heuristic algorithms;Adaptive control;Trajectory;Gears;Electric vehicles;Gears;Automotive engineering;Reinforcement learning","clutches;feedback;feedforward;learning (artificial intelligence);power transmission (mechanical);radial basis function networks;reinforcement learning","11-degree-of-freedom gearshift dynamics model;actual gearshift conditions;Control reinforcement learning algorithm;control strategy;electric vehicle transmission gearshift controller;feedback controller performs parameter optimization;feedforward control;feedforward-feedback gearshift control model;Gaussian kernel radial basis function neural network controller;gearshift controller parameter optimization;good gearshift controller control effect;learned controller;obvious optimization effect;optimization iterations;optimization method;optimized Radial Basis Function Neural Network control;PILCO reinforcement Learning;RBFNN gearshift controller parameters","","","","31","CCBYNCND","21 Aug 2023","","","IEEE","IEEE Journals"
"Offline Reinforcement Learning for Automated Stock Trading","N. Lee; J. Moon","Department of Artificial Intelligence, Hanyang University, Seoul, South Korea; Department of Artificial Intelligence, Hanyang University, Seoul, South Korea","IEEE Access","19 Oct 2023","2023","11","","112577","112589","Recently, with the increasing interest in investments in financial stock markets, several methods have been proposed to automatically trade stocks and/or predict future stock prices using machine learning techniques, such as reinforcement learning (RL), LSTM, and transformers. Among them, RL has been applied to manage portfolio assets with a sequence of optimal actions. The most important factor in investing in stocks is the utilization of past stock price data. However, existing RL algorithms applied to stock markets do not consider past stock data when taking optimal actions, as RL is formulated based on the Markov property. In other words, it means that the existing RL algorithm infers action based on the current state only. To resolve this limitation, we propose Transformer Actor-Critic with Regularization (TACR) using decision transformer to train the model with the correlation of past MDP (Markov Decision Process) elements using an attention network. In addition, a critic network is added to improve the performance by updating the parameters based on the evaluation of an action. For an efficient learning method, we train our model using an offline RL algorithm through suboptimal trajectories. To prevent overestimating the value of actions and reduce learning time, we train TACR through a regularization technique with an added behavior cloning. The experimental results using various stock market datasets show that TACR performs better than other state-of-the-art methods in terms of the Sharpe ratio and profit.","2169-3536","","10.1109/ACCESS.2023.3324458","National Research Foundation of Korea (NRF) Grant; Ministry of Science and ICT, South Korea(grant numbers:NRF-2021R1A2C2094350); Institute of Information & Communications Technology Planning and Evaluation (IITP) Grant; Korea Government (MSIT)(grant numbers:2020-0-01373); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10285085","Portfolio allocation;reinforcement learning;sequence modeling;Sharpe ratio;transformer;stock market","Transformers;Prediction algorithms;Investment;Resource management;Predictive models;Portfolios;Markov processes;Sequences","","","","","","46","CCBYNCND","13 Oct 2023","","","IEEE","IEEE Journals"
"Ad Hoc-Obstacle Avoidance-Based Navigation System Using Deep Reinforcement Learning for Self-Driving Vehicles","N. S. Manikandan; G. Kaliyaperumal; Y. Wang","TIFAC-CORE in Automotive Infotronics, Vellore Institute of Technology, Vellore, Tamil Nadu, India; School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu, India; Department of Systems Science and Industrial Engineering, Binghamton University, Binghamton, NY, USA","IEEE Access","1 Sep 2023","2023","11","","92285","92297","In this research, a novel navigation algorithm for self-driving vehicles that avoids collisions with pedestrians and ad hoc obstacles is described. The proposed algorithm predicts the locations of ad hoc obstacles and wandering pedestrians by using an RGB-D depth sensor. Unique ad hoc-obstacle-aware mobility rules are presented considering those environmental uncertainties. A Deep Reinforcement Learning (DRL) algorithm is proposed as a decision-making technique (to steer the self-driving vehicle to reach the target without incident). The deep Q-network (DQN), double deep Q-network (DDQN), and dueling double deep Q-network (D3DQN) algorithms were compared, and the D3DQN had the fewest negative rewards. We tested the algorithms using the Carla simulation environment to examine the input values from the RGB-D and RGB-Lidar. The series of algorithms that make up the convoluted neural network D3DQN was consequently selected as the optimum DRL algorithm. In the modeling of slow-moving urban traffic, RGB-D and RGB-Lidar generated essentially the same results. A self-driving version of an updated child-ride-on-car was modified to demonstrate the real-time effectiveness of the proposed algorithm.","2169-3536","","10.1109/ACCESS.2023.3297661","Technology Information, Forecasting and Assessment Council-Centre of Relevance and Excellence (TIFAC-CORE) in Automotive Infotronics Centre (sponsored by Department of Science and Technology, Government of India), Vellore Institute of Technology, Vellore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10189852","Self-driving vehicle;deep reinforcement learning;ad hoc-obstacles;obstacle avoidance;lane detection;object detection","Pedestrians;Navigation;Reinforcement learning;Deep learning;Cameras;Lane detection;Roads;Collision avoidance;Lane detection;Autonomous automobiles","collision avoidance;decision making;deep learning (artificial intelligence);image colour analysis;mobile robots;optical radar;pedestrians;reinforcement learning;robot vision;traffic engineering computing;vehicular ad hoc networks","ad hoc-obstacle avoidance-based navigation system;convoluted neural network D3DQN;DDQN;decision-making technique;deep reinforcement learning;dueling double deep Q-network algorithms;hoc-obstacle-aware mobility rules;novel navigation algorithm;optimum DRL algorithm;RGB-D depth sensor;RGB-Lidar;self-driving vehicles;wandering pedestrians","","","","18","CCBYNCND","21 Jul 2023","","","IEEE","IEEE Journals"
"Overcoming Obstacles With a Reconfigurable Robot Using Deep Reinforcement Learning Based on a Mechanical Work-Energy Reward Function","O. Simhon; Z. Karni; S. Berman; D. Zarrouk","Department of Mechanical Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel; Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Be’er Sheva, Israel; Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Be’er Sheva, Israel; Department of Mechanical Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel","IEEE Access","19 May 2023","2023","11","","47681","47689","This paper presents a Deep Reinforcement Learning (DRL) method based on a mechanical (work) Energy reward function applied to a reconfigurable RSTAR robot to overcome obstacles. The RSTAR is a crawling robot that can reconfigure its shape and shift the location of its center of mass via a sprawl and a four-bar extension mechanism. The DRL was applied in a simulated environment with a physical engine (UNITY $^{\mathrm {TM}}$ ). The robot was trained on a step obstacle and a two-stage narrow passage obstacle composed of a horizontal and a vertical channel. To evaluate the benefits of the proposed Energy reward function, it was compared to time-based and movement-based reward functions. The results showed that the Energy-based reward produced superior results in terms of obstacle height, energy requirements, and time to overcome the obstacle. The Energy-based reward method also converged faster to the solution compared to the other reward methods. The DRL’s results for all the methods (energy, time and movement- based rewards) were superior to the best results produced by the human experts (see attached video).","2169-3536","","10.1109/ACCESS.2023.3274675","Helmsley Charitable Trust through the Agricultural, Biological and Cognitive Robotics Initiative; Marcus Endowment Fund, both at Ben Gurion University of the Negev; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10121761","Obstacle negotiation;reinforcement learning (RL);reconfigurable robot;reward shaping","Robots;Legged locomotion;Wheels;Reinforcement learning;Torque;Friction;Reconfigurable devices","deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;reinforcement learning","crawling robot;Deep Reinforcement;energy requirements;Energy-based reward method;four-bar extension mechanism;mechanical Energy reward function;mechanical work-Energy reward function;movement-based reward functions;obstacle height;reconfigurable robot;reconfigurable RSTAR robot;reward methods;step obstacle;time-based;two-stage narrow passage obstacle","","","","27","CCBYNCND","9 May 2023","","","IEEE","IEEE Journals"
"Decoding Reinforcement Learning for Newcomers","F. S. Neves; G. A. Andrade; M. F. Reis; A. P. Aguiar; A. M. Pinto","Centre for Robotics and Autonomous Systems, INESC TEC, Porto, Portugal; Department of Electrical and Computer Engineering, University of Porto, Porto, Portugal; Department of Electrical and Computer Engineering, University of Porto, Porto, Portugal; Department of Electrical and Computer Engineering, University of Porto, Porto, Portugal; Centre for Robotics and Autonomous Systems, INESC TEC, Porto, Portugal","IEEE Access","5 Jun 2023","2023","11","","52778","52789","The Reinforcement Learning (RL) paradigm is showing promising results as a generic purpose framework for solving decision-making problems (e.g., robotics, games, finance). The aim of this work is to reduce the learning barriers and inspire young students, researchers and educators to use RL as an obvious tool to solve robotics problems. This paper provides an intelligible step-by-step RL problem formulation and the availability of an easy-to-use interactive simulator for students at various levels (e.g., undergraduate, bachelor, master, doctorate), researchers and educators. The interactive tool facilitates the familiarization with the key concepts of RL, its problem formulation and implementation. In this work, RL is used for solving a robotics 2D navigational problem where the robot needs to avoid collisions with obstacles while aiming to reach a goal point. A navigational problem is simple and convenient for educational purposes, since the outcome is unambiguous (e.g., the goal is reached or not, a collision happened or not). Due to a lack of open-source graphical interactive simulators concerning the field of RL, this paper combines theoretical exposition with an accessible practical tool to facilitate the apprehension. The results demonstrated are produced by a Python script that is released as open-source to reduce the learning barriers in such innovative research topic in robotics.","2169-3536","","10.1109/ACCESS.2023.3279729","European Union’s Horizon 2020 Research and Innovation Program, (ATLANTIS)(grant numbers:871571); FLY.PT-P2020 Mobilizador within Project(grant numbers:POCI-01-0247-FEDER-046079); DynamiCITY within Project(grant numbers:NORTE-01-0145-FEDER-000073); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10132462","Artificial intelligence;case study;computer science education;educational software;online repositories;reinforcement learning;robotics","Q-learning;Educational robots;Navigation;Markov processes;Decoding;Complexity theory;Uncertainty","decision making;learning (artificial intelligence);mobile robots;path planning;Python;reinforcement learning","decision-making problems;doctorate;easy-to-use interactive simulator;educational purposes;generic purpose framework;interactive tool;learning barriers;navigational problem;obvious tool;open-source graphical interactive simulators;Reinforcement Learning paradigm;robotics 2D;robotics problems;step-by-step RL problem formulation","","","","44","CCBYNCND","24 May 2023","","","IEEE","IEEE Journals"
"Dynamic Multitarget Assignment Based on Deep Reinforcement Learning","Y. Wu; Y. Lei; Z. Zhu; X. Yang; Q. Li","College of Systems Engineering, National University of Defense Technology (NUDT), Changsha, China; College of Systems Engineering, National University of Defense Technology (NUDT), Changsha, China; College of Systems Engineering, National University of Defense Technology (NUDT), Changsha, China; College of Systems Engineering, National University of Defense Technology (NUDT), Changsha, China; College of Systems Engineering, National University of Defense Technology (NUDT), Changsha, China","IEEE Access","26 Jul 2022","2022","10","","75998","76007","Dynamic multi-target assignment is a key technology that needs to be supported in order to improve the strike effectiveness during the coordinated attack of the missile swarm, and it is of great significance for improving the intelligence level of the new generation of strike weapon groups. Changes in ballistic trajectory during the penetration of multi-warhead missiles may cause the original target assignment scheme to no longer be optimal. Therefore, reassigning targets based on the real-time position of the warhead plays an important role in improving the effectiveness of the strike. In this paper, the dynamic multi-target assignment decision modeling method combining combat simulation and deep reinforcement learning was discussed, and an intelligent decision-making training framework for multi-target assignment was designed based on deep reinforcement learning. In conjunction with the typical combat cases, the warhead combat process was also divided into the penetration phase and the multi-target assignment phase, the model framework and reward function against the multi-target assignment of the missile were devised, and the SAC algorithm was employed to conduct application research on intelligent decision modeling for multi-target assignment. Preliminary test results suggest that the intelligent decision-making model based on deep reinforcement learning provides better combat effects than the traditional decision model based on knowledge engineering.","2169-3536","","10.1109/ACCESS.2022.3190972","National Natural Science Foundation of China(grant numbers:62003359); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829738","Deep reinforcement learning;combat simulation;intelligent decision-making;multi-target assignment","Reinforcement learning;Weapons;Missiles;Decision making;Heuristic algorithms;Training;Computational modeling","decision making;deep learning (artificial intelligence);military computing;missiles;reinforcement learning;target tracking;weapons","missile swarm;ballistic trajectory;knowledge engineering;combat simulation;dynamic multitarget assignment decision modeling method;intelligent decision-making training framework;target assignment scheme;multiwarhead missiles;strike weapon groups;deep reinforcement learning","","","","33","CCBY","14 Jul 2022","","","IEEE","IEEE Journals"
"End-to-End High-Level Control of Lower-Limb Exoskeleton for Human Performance Augmentation Based on Deep Reinforcement Learning","R. Zheng; Z. Yu; H. Liu; J. Chen; Z. Zhao; L. Jia","School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China; Beijing Institute of Precision Mechatronics and Controls, Beijing, China; Beijing Institute of Precision Mechatronics and Controls, Beijing, China; Beijing Institute of Precision Mechatronics and Controls, Beijing, China; Beijing Institute of Precision Mechatronics and Controls, Beijing, China; Beijing Institute of Precision Mechatronics and Controls, Beijing, China","IEEE Access","26 Sep 2023","2023","11","","102340","102351","This paper proposes a novel end-to-end controller for the lower-limb exoskeleton for human performance augmentation (LEHPA) systems based on deep reinforcement learning (E2EDRL). The model-free controller contains two control levels: the high-level control responsible for end-to-end human motion intention recognition based on the exoskeleton state signals and the human-exoskeleton interaction (HEI) force signals by deep neural network predictor, and the low-level control for motion tracking by joint PD controllers. The deep neural network predictor does not require complex kinematic calculations that are inevitable in conventional human motion intention recognition methods. We execute the learning process in simulation to learn the E2EDRL strategy efficiently and safely by constructing a novel multibody simulation environment and proposing its specific hybrid inverse-forward dynamics simulation method. The passive mode (all joints remain unpowered) is introduced as a benchmark for comparison purposes. A novel performance assessment method based on HEI forces is put forward to evaluate the E2EDRL strategy quantitatively. The global ratio of the HEI forces in the E2EDRL strategy relative to those in the passive mode is as low as 0.65. The global reduction of the HEI forces demonstrates the superior control performance of the E2EDRL strategy.","2169-3536","","10.1109/ACCESS.2023.3317183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255549","Lower-limb exoskeleton for human performance augmentation;end-to-end human motion intention recognition;deep reinforcement learning","Exoskeletons;Force;Legged locomotion;Torque;Reinforcement learning;Deep learning;Kinematics;Performance evaluation;Human activity recognition","deep learning (artificial intelligence);human-robot interaction;learning (artificial intelligence);medical robotics;motion control;patient rehabilitation;PD control;reinforcement learning;wearable robots","control levels;conventional human motion intention recognition methods;deep neural network predictor;deep reinforcement learning;E2EDRL strategy;end-to-end controller;end-to-end human motion intention recognition;exoskeleton state signals;HEI forces;high-level control;human performance augmentation systems;human-exoskeleton interaction force signals;joint PD controllers;learning process;low-level control;lower-limb exoskeleton;model-free controller;motion tracking;performance assessment method;specific hybrid inverse-forward dynamics simulation;superior control performance","","","","64","CCBYNCND","19 Sep 2023","","","IEEE","IEEE Journals"
"Model Predictive Control-Based Reinforcement Learning Using Expected Sarsa","H. Moradimaryamnegari; M. Frego; A. Peer","Human-Centered Technologies and Machine Intelligence Laboratory, Faculty of Science and Technology, Free University of Bozen-Bolzano, Bolzano, Italy; Human-Centered Technologies and Machine Intelligence Laboratory, Faculty of Science and Technology, Free University of Bozen-Bolzano, Bolzano, Italy; Human-Centered Technologies and Machine Intelligence Laboratory, Faculty of Science and Technology, Free University of Bozen-Bolzano, Bolzano, Italy","IEEE Access","9 Aug 2022","2022","10","","81177","81191","Recent studies have shown the potential of Reinforcement Learning (RL) algorithms in tuning the parameters of Model Predictive Controllers (MPC), including the weights of the cost function and unknown parameters of the MPC model. However, a framework for easy and straightforward implementation that allows training in just a few episodes and overcoming the need for imposing extra constraints as required by state-of-the-art methods, is still missing. In this study, we present two implementations to achieve these goals. In the first approach, a nonlinear MPC plays the role of a function approximator for an Expected Sarsa RL algorithm. In the second approach, only the MPC cost function is considered as the function approximator, while the unknown parameters of the MPC model are updated based on more classical system identification. In order to evaluate the performance of the proposed algorithms, first numerical simulations are performed on a coupled tanks system. Then, both algorithms are applied to the real system and their closed-loop performance and convergence speed are compared with each other. The results indicate that the proposed algorithms allow tuning of MPCs over very few episodes. Finally, also the disturbance rejection ability of the proposed methods is demonstrated.","2169-3536","","10.1109/ACCESS.2022.3195530","a university-sponsored Ph.D. fellowship of the ASE PhD programme; Open Access Publishing Fund through the Free University of Bozen-Bolzano; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9846979","Model predictive control;reinforcement learning;expected Sarsa algorithm;model-based learning method","Q-learning;Computational modeling;Training;Prediction algorithms;Mathematical models;Costs;Cost function","closed loop systems;function approximation;learning (artificial intelligence);nonlinear control systems;predictive control","coupled tanks system;episodes;Model Predictive control-based Reinforcement;Reinforcement Learning algorithms;Model Predictive Controllers;unknown parameters;MPC model;easy implementation;straightforward implementation;extra constraints;nonlinear MPC;function approximator;Expected Sarsa RL algorithm;MPC cost function;classical system identification","","","","29","CCBY","1 Aug 2022","","","IEEE","IEEE Journals"
"Adaptive Call Center Workforce Management With Deep Neural Network and Reinforcement Learning","W. Kumwilaisak; S. Phikulngoen; J. Piriyataravet; N. Thatphithakkul; C. Hansakunbuntheung","Department of Electronics and Telecommunication Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, Thailand; Department of Electronics and Telecommunication Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, Thailand; Department of Electronics and Telecommunication Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, Thailand; Assistive Technology and Medical Devices Research Center, National Science and Technology Development Agency, Pathum Thani, Thailand; Assistive Technology and Medical Devices Research Center, National Science and Technology Development Agency, Pathum Thani, Thailand","IEEE Access","8 Apr 2022","2022","10","","35712","35724","Workforce management is one of several critical issues in a call center. A call center supervisor must assign an adequate number of call agents to handle a high volume of time-variant incoming calls. Without effective staff allocation, improper workforce management can degrade service quality and reduce customer satisfaction. This paper presents a novel call center workforce management based on a deep neural network and reinforcement learning (RL). The proposed method first uses a deep neural network to learn and predict call center traffic characteristics. The deep neural network consists of a Long-Short Term Memory (LSTM) network and a Deep Neural Network (DNN) capturing non-linear call traffic behaviors. The expected traffic parameters are supplied into the Erlang A model, which calculates important service metrics, including a call abandonment probability and the average response time. This paper applies a reinforcement learning framework using the Q-learning algorithm to establish the optimal starting times of call agent shifts and their associated call agent numbers by maximizing a defined reward function to handle dynamic call center traffic. The objective of these findings is to maintain the quality of service of a call center throughout working hours. The proposed method surpasses experienced human supervisors and previous workforce management schemes in terms of achieved qualities of service and average waiting time from experimental results under actual call center data.","2169-3536","","10.1109/ACCESS.2022.3160452","King Mongkut’s University of Technology Thonburi; National Science and Technology Development Agency (NSTDA), Thailand; Call Center Data Dataset provided by the Thai Telecommunication Relay Service (TTRS) Call Center; Relay Service Call Center for Thai hearing impaired, developed by the NSTDA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9737504","Reinforcement learning;Q-learning algorithm;fully connected network;long short-term memory network;Erlang A;Thai telecommunication relay services (TTRS)","Relays;Neural networks;Deep learning;Streaming media;Mobile handsets;Measurement;Forecasting","call centres;customer satisfaction;deep learning (artificial intelligence);personnel;probability;quality of service;recurrent neural nets;reinforcement learning;telecommunication traffic","adaptive call center workforce management;reinforcement learning;call center supervisor;long-short term memory network;deep neural network;dynamic call center traffic;nonlinear call traffic behaviors;time-variant incoming calls;service quality;customer satisfaction;call center traffic characteristics;LSTM;DNN;Erlang A model;service metrics;call abandonment probability;Q-learning;call agent shifts;call agent numbers;reward function;quality of service","","","","40","CCBY","17 Mar 2022","","","IEEE","IEEE Journals"
"Geometric Reinforcement Learning for Robotic Manipulation","N. Alhousani; M. Saveriano; I. Sevinc; T. Abdulkuddus; H. Kose; F. J. Abu-Dakka","Faculty of Computer and Informatics Engineering, Istanbul Technical University, Maslak, Sarıyer, İstanbul, Turkey; Department of Industrial Engineering (DII), University of Trento, Trento, Italy; MCFLY Robot Teknolojileri A.Ş, Sarıyer, İstanbul, Turkey; ILITRON Enerji ve Bilgi Teknolojileri A.Ş, Kağıthane, İstanbul, Turkey; Faculty of Computer and Informatics Engineering, Istanbul Technical University, Maslak, Sarıyer, İstanbul, Turkey; Munich Institute of Robotics and Machine Intelligence (MIRMI), Technical University of Munich, Munich, Germany","IEEE Access","16 Oct 2023","2023","11","","111492","111505","Reinforcement learning (RL) is a popular technique that allows an agent to learn by trial and error while interacting with a dynamic environment. The traditional Reinforcement Learning (RL) approach has been successful in learning and predicting Euclidean robotic manipulation skills such as positions, velocities, and forces. However, in robotics, it is common to encounter non-Euclidean data such as orientation or stiffness, and failing to account for their geometric nature can negatively impact learning accuracy and performance. In this paper, to address this challenge, we propose a novel framework for RL that leverages Riemannian geometry, which we call Geometric Reinforcement Learning ( $\mathcal {G}$ -RL), to enable agents to learn robotic manipulation skills with non-Euclidean data. Specifically,  $\mathcal {G}$ -RL utilizes the tangent space in two ways: a tangent space for parameterization and a local tangent space for mapping to a non-Euclidean manifold. The policy is learned in the parameterization tangent space, which remains constant throughout the training. The policy is then transferred to the local tangent space via parallel transport and projected onto the non-Euclidean manifold. The local tangent space changes over time to remain within the neighborhood of the current manifold point, reducing the approximation error. Therefore, by introducing a geometrically grounded pre- and post-processing step into the traditional RL pipeline, our  $\mathcal {G}$ -RL framework enables several model-free algorithms designed for Euclidean space to learn from non-Euclidean data without modifications. Experimental results, obtained both in simulation and on a real robot, support our hypothesis that  $\mathcal {G}$ -RL is more accurate and converges to a better solution than approximating non-Euclidean data.","2169-3536","","10.1109/ACCESS.2023.3322654","Scientific and Technological Research Council of Turkey (TÜBİTAK)(grant numbers:3201141); European Union under NextGenerationEU Project Interconnected Nord-Est Innovation Ecosystem (iNEST)(grant numbers:ECS 00000043); European ROBotics and AI Network (euROBIN) Project(grant numbers:101070596); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10273391","Learning on manifolds;policy optimization;policy search;geometric reinforcement learning","Manifolds;Robots;Geometry;Reinforcement learning;Aerospace electronics;Optimization;Impedance;Robot control","","","","","","44","CCBYNCND","6 Oct 2023","","","IEEE","IEEE Journals"
"Effective Experiences Collection and State Aggregation in Reinforcement Learning","T. Zhang; Y. Liu; Y. -J. Chen; K. -S. Hwang","School of Software, Northwestern Polytechnical University, Xi’an, China; School of Software, Northwestern Polytechnical University, Xi’an, China; Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan","IEEE Access","5 Sep 2019","2019","7","","120917","120928","In reinforcement learning systems, learning agents cluster a large number of experiences by identifying similarities in terms of domain knowledge and replace the groups with a representative prototype. Our method addresses two key challenges in reinforcement learning: the difficulty of transferring continuous state domain into the discrete state space, and the need for a good compromise between exploration and exploitation. To tackle the former challenge, the adaptive state aggregation algorithm with a decision tree helps the agent to learn the optimal policy in continuous state space. For the latter challenge, this paper proposes an adaptive state aggregation algorithm, which uses information entropy to evaluate the probability of state-action-dependent exploration and the value for  $\varepsilon $  is determined using the information entropy instead of manual tuning. Meanwhile, a Tabu search is used to lift the efficiency of exploration. For the planning algorithm, the time required for global exploration depends only on the metric resolution, and not on the size of the state space. The simulation experiments demonstrate the effectiveness of the proposed method, which adaptively partitions the state space into exclusive subspaces and, meanwhile, obtains a good compromise between exploration and exploitation.","2169-3536","","10.1109/ACCESS.2019.2931884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8781772","Reinforcement learning;ε-greedy;exploration-exploitation;Q-learning;state aggregation","Estimation error;Reinforcement learning;Decision trees;Information entropy;Training;Tuning;Adaptation models","data aggregation;decision trees;entropy;learning (artificial intelligence);planning (artificial intelligence);search problems","data aggregation;planning algorithm;Tabu search;decision tree;learning agents;experiences collection;state-action-dependent exploration;information entropy;continuous state space;adaptive state aggregation algorithm;discrete state space;domain knowledge;reinforcement learning systems","","","","28","CCBY","30 Jul 2019","","","IEEE","IEEE Journals"
"Mobile Service Robot Path Planning Using Deep Reinforcement Learning","A. A. N. Kumaar; S. Kochuvila","Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Bengaluru Campus, Bengaluru, India; Department of Electronics and Communication Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Bengaluru Campus, Bengaluru, India","IEEE Access","19 Sep 2023","2023","11","","100083","100096","A mobile service robot operates in a constantly changing environment with other robots and humans. The service environment is usually vast and unknown, and the robot is expected to operate continuously for a long period. The environment can be dynamic, leading to the generation of new routes or the permanent blocking of old routes. The traditional path planner that relies on static maps will not suffice for a dynamic environment. This work is focused on developing a reinforcement learning-based path planner for a dynamic environment. The proposed system uses the deep Q-Learning algorithm to learn the initial paths using a topological map of the environment. In an environmental change, the proposed  $\pmb \beta $ -decay transfer learning algorithm trains the agent in the new environment. This algorithm uses experience vs. exploration vs. exploitation-based training depending on the similarity of the old and new environments. The system is implemented on the Robotic Operating System framework and tested using Turtlebot3 mobile robot in the Gazebo simulator. The experimental results show that the reinforcement learning system learns all the routes based on the initial topological map of different service environments with an accuracy of over 98%. A comparative analysis of the  $\pmb \beta $ -decay transfer learning and non-transfer learning agents is performed based on various evaluation metrics. The transfer learning agent converges twice faster than the non-transfer learning agent.","2169-3536","","10.1109/ACCESS.2023.3311519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10238717","Path planning;mobile robots;service robot;navigation systems;reinforcement learning;deep Q-Learning;Q-learning;transfer learning;dynamic environment;machine learning;artificial intelligence;incremental learning;lifelong learning","Artificial intelligence;Path planning;Robots;Heuristic algorithms;Training;Service robots;Mobile robots;Navigation;Deep learning;Q-learning;Transfer learning;Machine learning","control engineering computing;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;path planning;reinforcement learning;robot programming;service robots","constantly changing environment;deep Q-Learning algorithm;deep reinforcement Learning;different service environments;dynamic environment;environmental change;exploration vs. exploitation-based training depending;initial paths;initial topological map;mobile service robot path planning;new environments;nontransfer learning agent;old environments;old routes;permanent blocking;proposed\pmb β-decay transfer learning algorithm;reinforcement learning system;reinforcement learning-based path planner;Robotic Operating System framework;service environment;static maps;the\pmb β-decay transfer learning;traditional path planner;transfer learning agent;Turtlebot3 mobile robot","","","","51","CCBYNCND","4 Sep 2023","","","IEEE","IEEE Journals"
"Pursuing Benefits or Avoiding Threats: Realizing Regional Multi-Target Electronic Reconnaissance With Deep Reinforcement Learning","Y. Xu; M. Zhang; B. Jin","School of Computer, Jiangsu University of Science and Technology, Zhenjiang, China; School of Computer, Jiangsu University of Science and Technology, Zhenjiang, China; School of Computer, Jiangsu University of Science and Technology, Zhenjiang, China","IEEE Access","30 Jun 2023","2023","11","","63972","63984","Unmanned combat aerial vehicles (UCAVs) are preferred for regional electronic reconnaissance due to their versatility and stealth. This paper proposes a deep reinforcement learning (DRL) method to enable UCAVs to complete regional multi-target electronic reconnaissance (MER) tasks with continuous autonomous maneuvers. Distinguishing from traditional heuristic search algorithms, we first derive the objective function of MER and elucidate sufficient conditions to improve the success rate of reconnaissance recognition. Then, using the original cognitive electronic warfare framework, a three-dimensional MER simulator named Scouer-N is created to satisfy the requirements of dynamic environment training for DRL-based agents. To enable the processing of sequential situation awareness, a generative network is constructed by introducing a partially observable Markov decision process (POMDP) model, which assists the UCAV in filtering the observations from the sensor and predicting the actual states. Finally, we propose a priority-driven state reward shaping method that provides normalized state representation and dense rewards to the agent during training to improve the agent’s behavioral knowledge for MER. The experimental results demonstrate a considerable improvement in the task success rate of the trained UCAV relative to the benchmark, proving the efficacy of our approach in helping agents learn the optimal reconnaissance strategy from the potential state space.","2169-3536","","10.1109/ACCESS.2023.3289077","National Natural Science Foundation of China(grant numbers:62006099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160026","Multi-target electronic reconnaissance;cognitive electronic warfare;deep reinforcement learning;3D motion planning;POMDP model","Radar;Task analysis;Reconnaissance;Spaceborne radar;Mathematical models;Radar tracking;Radar detection","autonomous aerial vehicles;decision making;deep learning (artificial intelligence);electronic warfare;learning (artificial intelligence);Markov processes;military aircraft;military computing;mobile robots;multi-agent systems;reinforcement learning;telecommunication computing","complete regional multitarget electronic reconnaissance tasks;continuous autonomous maneuvers;deep reinforcement learning method;DRL-based agents;dynamic environment training;elucidate sufficient conditions;optimal reconnaissance strategy;original cognitive electronic warfare framework;partially observable Markov decision process model;priority-driven state reward;pursuing benefits;reconnaissance recognition;regional electronic reconnaissance;stealth;task success rate;three-dimensional MER simulator;traditional heuristic search algorithms;trained UCAV;unmanned combat aerial vehicles","","","","34","CCBYNCND","23 Jun 2023","","","IEEE","IEEE Journals"
"Acquisition of Inducing Policy in Collaborative Robot Navigation Based on Multiagent Deep Reinforcement Learning","M. Kamezaki; R. Ong; S. Sugano","Waseda Research Institute for Science and Engineering, Waseda University, Tokyo, Shinjuku-ku, Japan; Department of Modern Mechanical Engineering, Waseda University, Tokyo, Shinjuku-ku, Japan; Waseda Research Institute for Science and Engineering, Waseda University, Tokyo, Shinjuku-ku, Japan","IEEE Access","14 Mar 2023","2023","11","","23946","23955","To avoid inefficient movement or the freezing problem in crowded environments, we previously proposed a human-aware interactive navigation method that uses inducement, i.e., voice reminders or physical touch. However, the use of inducement largely depends on many factors, including human attributes, task contents, and environmental contexts. Thus, it is unrealistic to pre-design a set of parameters such as the coefficients in the cost function, personal space, and velocity in accordance with the situation. To understand and evaluate if inducement (voice reminder in this study) is effective and how and when it must be used, we propose to comprehend them through multiagent deep reinforcement learning in which the robot voluntarily acquires an inducing policy suitable for the situation. Specifically, we evaluate whether a voice reminder can improve the time to reach the goal by learning when the robot uses it. Results of simulation experiments with four different situations show that the robot could learn inducing policies suited for each situation, and the effectiveness of inducement is greatly improved in more congested and narrow situations.","2169-3536","","10.1109/ACCESS.2023.3253513","Japan Science and Technology Agency (JST) PRESTO(grant numbers:JPMJPR1754); Japan Society for the Promotion of Science (JSPS) KAKENHI(grant numbers:19H01130); Waseda Research Institute Science and Engineering, Waseda University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10061379","Autonomous mobile robot;multiagent deep reinforcement learning;inducing policy acquisition;collaborative robot navigation","Autonomous robots;Mobile robots;Reinforcement learning;Deep learning;Multi-agent systems;Robot motion","deep learning (artificial intelligence);human-robot interaction;learning (artificial intelligence);mobile robots;multi-agent systems;navigation;path planning;reinforcement learning;robot vision","collaborative robot navigation;crowded environments;freezing problem;human attributes;human-aware interactive navigation method;inducement;inducing policy;inefficient movement;multiagent deep reinforcement learning;physical touch;voice reminder","","","","32","CCBY","6 Mar 2023","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Composite Controller for Cable-Driven Parallel Suspension System at High Angles of Attack","W. Wang; X. Wang; C. Shen; Q. Lin","School of Aerospace Engineering, Xiamen University, Xiamen, Fujian, China; School of Aerospace Engineering, Xiamen University, Xiamen, Fujian, China; School of Aerospace Engineering, Xiamen University, Xiamen, Fujian, China; School of Aerospace Engineering, Xiamen University, Xiamen, Fujian, China","IEEE Access","11 Apr 2022","2022","10","","36373","36384","This paper investigates an intelligent method for the motion control of a cable-driven parallel suspension system (CDPSS) in wind tunnel tests, especially at high angles of attack, which is characterized by unsteady and nonlinear aerodynamics. Considering the modeling uncertainties and the complex aerodynamic interference, a composite controller that combines deep deterministic policy gradient (DDPG) and computed-torque is proposed to improve the control performance. The tasks at hand consist in the construction of the training environment based on the dynamic equations and the Markov Decision Process (MDP) design. The supplementary computed-torque control is used to enhance the learning rate of the agent. Then a well-trained agent is applied in the high angles of attack maneuvers control with different examples, including single-DOF and multi-DOF coupled motion. The simulation results show the controller could fulfill the training tasks efficiently, and it turns out to be robust and maintain strong generalization ability despite handling the unlearned tasks.","2169-3536","","10.1109/ACCESS.2022.3163296","National Natural Science Foundation of China(grant numbers:12172315,12072304,11702232); Natural Science Foundation of Fujian Province of China(grant numbers:2021J01050); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745136","Wind tunnel test;cable-driven parallel mechanism;high angle of attack;motion control;DDPG","Aerodynamics;Atmospheric modeling;Aircraft;Wind tunnels;Mathematical models;Heuristic algorithms;Kinematics","aerodynamics;cables (mechanical);learning (artificial intelligence);Markov processes;motion control;robot dynamics;suspensions (mechanical components);torque control;wind tunnels","deep deterministic policy gradient;control performance;Markov Decision Process design;supplementary computed-torque control;high angles;reinforcement learning-based composite controller;cable-driven parallel suspension system;motion control;unsteady aerodynamics;nonlinear aerodynamics;complex aerodynamic interference","","","","24","CCBY","30 Mar 2022","","","IEEE","IEEE Journals"
"Seg-CURL: Segmented Contrastive Unsupervised Reinforcement Learning for Sim-to-Real in Visual Robotic Manipulation","B. Xu; T. Hassan; I. Hussain","Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates; Department of Electrical and Computer Engineering, Abu Dhabi University, Abu Dhabi, United Arab Emirates; Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates","IEEE Access","31 May 2023","2023","11","","50195","50204","Training image-based reinforcement learning (RL) agents are sample-inefficient, limiting their effectiveness in real-world manipulation tasks. Sim2Real, which involves training in simulations and transferring to the real world, effectively reduces the dependence on real data. However, the performance of the transferred agent degrades due to the visual difference between the two environments. This research presents a low-cost segmentation-driven unsupervised RL framework (Seg-CURL) to solve the Sim2Real problem. We transform the input RGB views to the proposed semantic segmentation-based canonical domain. Our method incorporates two levels of Sim2Real: task-level Sim2Real, which transfers the RL agent to the real world, and observation-level Sim2Real, which transfers the simulated U-nets to segment real-world scenes. Specifically, we first train contrastive unsupervised RL(CURL) with segmented images in the simulation environment. Next, we employ two U-Nets to segment robotic hand-view and side-view images during real robot control. These U-Net are pre-trained with synthetic RGB and segmentation masks in the simulation environment and fine-tuned with only 20 real images. We evaluate the robustness of the proposed framework in both simulation and real environments. Seg-CURL is robust to the texture, lighting, shadow, and camera position gap. Finally, our algorithm is tested on a real Baxter robot with a dark hand-view in the cube lifting task with a success rate of 16/20 in zero-shot transfer.","2169-3536","","10.1109/ACCESS.2023.3278208","Khalifa University of Science and Technology(grant numbers:FSU-2021-019,RC1-2018-KUCARS); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10129935","Reinforcement learning;robotic manipulation;Sim2Real;zero-short learning","Robots;Reinforcement learning;Image segmentation;Training;Visualization;Robot vision systems;Grasping;Learning systems","control engineering computing;convolutional neural nets;image colour analysis;image segmentation;image texture;manipulators;multi-agent systems;reinforcement learning;robot vision;unsupervised learning","Baxter robot;camera position gap;contrastive unsupervised RL;cube lifting task;dark hand-view;image texture;image-based reinforcement learning agents;lighting;low-cost segmentation-driven unsupervised RL framework;observation-level Sim2Real;real-world manipulation tasks;RGB views;RL agent;robot control;robotic hand-view image segmentation;Seg-CURL;segmentation masks;segmented contrastive unsupervised reinforcement learning;semantic segmentation-based canonical domain;side-view image segmentation;Sim-to-Real;simulation environment;synthetic RGB;task-level Sim2Real;U-nets;visual difference;visual robotic manipulation;zero-shot transfer","","","","35","CCBYNCND","19 May 2023","","","IEEE","IEEE Journals"
"Objective Weight Interval Estimation Using Adversarial Inverse Reinforcement Learning","N. Takayama; S. Arai","Department of Urban Environment Systems, Division of Earth and Environmental Sciences, Graduate School of Science and Engineering, Chiba University, Chiba, Japan; Department of Urban Environment Systems, Division of Earth and Environmental Sciences, Graduate School of Science and Engineering, Chiba University, Chiba, Japan","IEEE Access","15 Jun 2023","2023","11","","58532","58538","Several real-world problems are modeled as multi-objective sequential decision-making problems with multiple competing objectives, and multi-objective reinforcement learning (MORL) has garnered attention as a solution to this problem. One of the challenges in obtaining the desired policy using MORL is that the priorities (hereafter, weights) for each objective must be designed in advance to scalarize the reward vector. Determining weights through trial-and-error burdens system designers, and methods to estimate weights are needed. The existing methods use inverse reinforcement learning (IRL), which is not scalable because it requires reinforcement learning several times until an optimal policy is obtained. This study proposes a weight interval estimation (WInter) method using adversarial IRL (AIRL). AIRL is a scalable framework that reduces the computational complexity of IRL by simultaneously estimating rewards and policies. WInter estimates the weight interval using the expert neighborhoods obtained during AIRL training. We successfully estimated the weight interval through experiments in a benchmark environment for multi-objective sequential decision-making problems in a continuous state space while reducing computational complexity compared to the existing methods.","2169-3536","","10.1109/ACCESS.2023.3281593","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138820","Deep reinforcement learning;inverse reinforcement learning;multi-objective planning;preference estimation;sequential decision-making","Estimation;Reinforcement learning;Decision making;Generators;Pareto optimization;Aerospace electronics;Deep learning","computational complexity;decision making;learning (artificial intelligence);Pareto optimisation;power engineering computing;reinforcement learning","adversarial inverse reinforcement learning;adversarial IRL;MORL;multiobjective reinforcement learning;multiobjective sequential decision-making problems;multiple competing objectives;objective weight interval estimation;weight interval estimation method","","","","18","CCBYNCND","31 May 2023","","","IEEE","IEEE Journals"
"Boosting Performance of Visual Servoing Using Deep Reinforcement Learning From Multiple Demonstrations","A. Aflakian; A. Rastegharpanah; R. Stolkin","Department of Metallurgy and Materials Science, University of Birmingham, Birmingham, U.K.; Department of Metallurgy and Materials Science, University of Birmingham, Birmingham, U.K.; Department of Metallurgy and Materials Science, University of Birmingham, Birmingham, U.K.","IEEE Access","21 Mar 2023","2023","11","","26512","26520","In this study, knowledge of multiple controllers was used and combined with deep reinforcement learning (RL) to train a visual servoing (VS) technique. Deep RL algorithms were successful in solving complicated control problems, however they generally require a large amount of data before they achieve an acceptable performance. We developed a method that generates online hyper-volume action bounds from demonstrations of multiple controllers (experts) to address the issue of insufficient data in RL. The agent then continues to explore the created bounds to find more optimized solutions and gain more rewards. By doing this, we cut out pointless agent explorations, which results in a reduction in training time as well as an improvement in performance of the trained policy. During the training process, we used domain randomization and domain adaptation to make the VS approach robust in the real world. As a result, we showed a 51% decrease in training time to achieve the desired level of performance, compared to the case when RL was used solely. The findings showed that the developed method outperformed other baseline VS methods (image-based VS, position-based VS, and hybrid-decoupled VS) in terms of VS error convergence speed and maintained higher manipulability.","2169-3536","","10.1109/ACCESS.2023.3256724","“Reuse and Recycling of Lithium-Ion Batteries (RELIB)”; The Faraday Institution(grant numbers:FIRG005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10068197","Visual servoing;reinforcement learning;online action bounding;reinforcement learning from demonstrations;manipulability","Cameras;Robots;Visual servoing;Training;Reinforcement learning;Visualization;Task analysis","deep learning (artificial intelligence);learning (artificial intelligence);reinforcement learning;visual servoing","complicated control problems;created bounds;deep reinforcement learning;deep RL algorithms;domain adaptation;domain randomization;gain more rewards;insufficient data;multiple controllers;multiple demonstrations;online hyper-volume action;optimized solutions;pointless agent explorations;trained policy;training process;training time;visual servoing technique","","","","36","CCBY","13 Mar 2023","","","IEEE","IEEE Journals"
"Packet Delivery Maximization Using Deep Reinforcement Learning-Based Transmission Scheduling for Industrial Cognitive Radio Systems","P. D. Thanh; T. N. K. Hoan; H. T. H. Giang; I. Koo","Department of Electrical, Electronic and Computer Engineering, University of Ulsan (UOU), Ulsan, South Korea; Department of Electronics and Telecommunication Engineering, College of Engineering Technology, Can Tho University, Can Tho, Vietnam; Department of Electrical, Electronic and Computer Engineering, University of Ulsan (UOU), Ulsan, South Korea; Department of Electrical, Electronic and Computer Engineering, University of Ulsan (UOU), Ulsan, South Korea","IEEE Access","4 Nov 2021","2021","9","","146492","146508","The performance of data aggregation in industrial wireless communications can be degraded by environmental interference on Industrial Scientific Medical (ISM) channels. In this paper, cognitive radio (CR) was applied to enable devices to share primary channels with the aim of enhancing the transmission performance of the WirelessHART network. We considered a linear convergecast system, where the packets generated at each device were routed to the gateway (GW) through the aid of neighboring devices. The solar-powered cognitive access points (CAPs) were deployed to improve the network performance by opportunistically allocating the primary channels to the devices for data transmissions. Firstly, we formulate the scheduling problem of long-term throughput maximization as a framework of a Markov decision process with the constraints of the minimum delay, the number of required ISM channels, and the harvested energy at the CAPs. Then, we propose a deep reinforcement learning-based scheduling scheme to optimally assign multiple ISM and primary channels to the field devices in each superframe. The simulation results confirmed the superiority of the proposed scheme compared to existing methods.","2169-3536","","10.1109/ACCESS.2021.3123213","National Research Foundation of Korea through the Korean Government Ministry of Science and ICT (MSIT)(grant numbers:2021R1A2B5B01001721); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585683","WirelessHART;cognitive radio;Markov decision process;industrial scientific medical","Job shop scheduling;Wireless sensor networks;Interference;Wireless communication;Protocols;Sensors;Performance evaluation","cognitive radio;deep learning (artificial intelligence);Markov processes;optimisation;reinforcement learning;telecommunication computing;telecommunication scheduling","primary channels;field devices;packet delivery maximization;deep reinforcement learning-based transmission scheduling;Industrial cognitive radio systems;data aggregation;industrial wireless communications;environmental interference;Industrial Scientific Medical channels;transmission performance;WirelessHART network;linear convergecast system;neighboring devices;solar-powered cognitive access points;CAPs;network performance;data transmissions;scheduling problem;long-term throughput maximization;required ISM channels;multiple ISM;Markov decision process;minimum delay","","","","49","CCBYNCND","26 Oct 2021","","","IEEE","IEEE Journals"
"Bridging the Reality Gap Between Virtual and Physical Environments Through Reinforcement Learning","M. Ranaweera; Q. H. Mahmoud","Department of Electrical,Computer, and Software Engineering, Ontario Tech University, Oshawa, Canada; Department of Electrical,Computer, and Software Engineering, Ontario Tech University, Oshawa, Canada","IEEE Access","2 Mar 2023","2023","11","","19914","19927","Creating Reinforcement learning(RL) agents that can perform tasks in the real-world robotic systems remains a challenging task due to inconsistencies between the virtual-and the real-world. This is known as the “reality-gap” which hinders the performance of a RL agent trained in a virtual environment. The research describes the techniques used to train the models, generate randomized environments, reward function, and techniques utilized to transfer the model to the physical environment for evaluation. For this investigation, a low-cost 3-degrees-of-freedom (DOF) Steward platform was 3D modeled and created virtually and physically. The goal of the 3D-Stewart platform was to guide and balance the marble towards the center. Custom end-to-end APIs were developed to interact with the Godot game engine, manipulate physics and dynamics, interact with the in-game lighting and perform environment randomizations. Two RL algorithms: Q-learning and Actor-Critic, were implemented to evaluate the performance by using domain randomization and induced noise to bridge the reality gap. For Q-learning, raw frames were used to make predictions while Actor-Critic utilized marble position, velocity vector and relative position by pre-processing captured frames. The experimental results show the effectiveness of domain randomization and introduction of noise during the training.","2169-3536","","10.1109/ACCESS.2023.3249572","Natural Sciences and Engineering Research Council of Canada (NSERC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054009","Actor-critic;deep Q-learning;reinforcement learning;transfer-learning;sim-to-real;robotics","Robots;Adaptation models;Virtual environments;Training;Q-learning;Transfer learning;Reinforcement learning","application program interfaces;control engineering computing;manipulators;mobile robots;position control;reinforcement learning;virtual reality","3D-Stewart platform;actor-critic;custom end-to-end APIs;domain randomization;Godot game engine;low-cost 3-degrees-of-freedom;physical environment;Q-learning;randomized environments;real-world robotic systems;reality gap;reinforcement learning;reward function;RL agent;virtual environment","","","","38","CCBYNCND","27 Feb 2023","","","IEEE","IEEE Journals"
"Confrontation and Obstacle-Avoidance of Unmanned Vehicles Based on Progressive Reinforcement Learning","C. Ma; J. Liu; S. He; W. Hong; J. Shi","Department of Chemical and Biochemical Engineering, College of Chemistry and Chemical Engineering, Xiamen University, Xiamen, China; Department of Chemical and Biochemical Engineering, College of Chemistry and Chemical Engineering, Xiamen University, Xiamen, China; Department of Chemical and Biochemical Engineering, College of Chemistry and Chemical Engineering, Xiamen University, Xiamen, China; Department of Chemical and Biochemical Engineering, College of Chemistry and Chemical Engineering, Xiamen University, Xiamen, China; Department of Chemical and Biochemical Engineering, College of Chemistry and Chemical Engineering, Xiamen University, Xiamen, China","IEEE Access","31 May 2023","2023","11","","50398","50411","The core technique of unmanned vehicle systems is the autonomous maneuvering decision, which not only determines the applications of unmanned vehicles but also is the critical technique many countries are competing to develop. Reinforcement Learning (RL) is the potential design method for autonomous maneuvering decision-making systems. Nevertheless, in the face of complex decision-making tasks, it is still challenging to master the optimal policy due to the low learning efficiency caused by the complex environment, high dimensional state, and sparse reward. Inspired by the human learning process from simple to complex, we propose a novel progressive deep RL algorithm for policy optimization in unmanned autonomous decision-making systems in this paper. The proposed algorithm divides the training of the autonomous maneuvering decision into a sequence of curricula with learning tasks from simple to complex. Finally, through the self-play stage, the iterative optimization of the policy is realized. Furthermore, the confrontation environment with two unmanned vehicles with obstacles is analyzed and modeled. Finally, the simulation leads to the one-to-one adversarial tasks demonstrate the effectiveness and applicability of the proposed design algorithm.","2169-3536","","10.1109/ACCESS.2023.3278597","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10130281","Unmanned systems;reinforcement learning;autonomous maneuvering decision-making;obstacle-avoidance","Autonomous vehicles;Collision avoidance;Task analysis;Optimization;Vehicle dynamics;Reinforcement learning;Decision making","collision avoidance;decision making;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;optimisation;reinforcement learning;remotely operated vehicles","autonomous maneuvering decision-making systems;complex decision-making tasks;complex environment;confrontation environment;core technique;critical technique many countries;human learning process;low learning efficiency;obstacle-avoidance;optimal policy;policy optimization;potential design method;progressive deep RL algorithm;progressive reinforcement Learning;unmanned autonomous decision-making systems;unmanned vehicle systems;unmanned vehicles","","","","40","CCBYNCND","22 May 2023","","","IEEE","IEEE Journals"
"Deep Adversarial Reinforcement Learning Method to Generate Control Policies Robust Against Worst-Case Value Predictions","K. Ohashi; K. Nakanishi; Y. Yasui; S. Ishii","Department of Systems Science, Graduate School of Informatics, Kyoto University, Kyoto, Japan; Department of Systems Science, Graduate School of Informatics, Kyoto University, Kyoto, Japan; Honda Research and Development Company Ltd., Saitama, Japan; Department of Systems Science, Graduate School of Informatics, Kyoto University, Kyoto, Japan","IEEE Access","22 Sep 2023","2023","11","","100798","100809","Over the last decade, methods for autonomous control by artificial intelligence have been extensively developed based on deep reinforcement learning (DRL) technologies. However, despite these advances, robustness to noise in observation data remains as an issue in autonomous control policies implemented using DRL in practical applications. In this study, we present a general robust adversarial learning technology applicable to DRL. During these adversarial learning processes, policies are trained to output consistent control actions through regularization learning, even for adversarial input examples. Importantly, these adversarial examples are produced to lead the current policy to predict the worst action at each state. Although a naive implementation of regularization learning may cause DRL model to learn a biased objective function, our methods were found to minimize bias. When implemented as a modification of a deep Q-network for discrete-action problems in Atari 2600 games and of a deep deterministic policy gradient for continuous-action tasks in Pybullet, our new adversarial learning frameworks showed significantly enhanced robustness against adversarial and random noise added to the input compared to several recently proposed methods.","2169-3536","","10.1109/ACCESS.2023.3314750","New Energy and Industrial Technology Development Organization (NEDO), Japan; Japan Society for the Promotion of Science (JSPS), Japan, through KAKENHI(grant numbers:19H04180,22H04998,23H04676); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10250423","Deep reinforcement learning;adversarial example;robustness;regularization","Perturbation methods;Training;Robustness;Reinforcement learning;Adversarial machine learning;Task analysis;Aerospace electronics;Deep learning;Adversarial machine learning","","","","","","36","CCBYNCND","13 Sep 2023","","","IEEE","IEEE Journals"
"Towards Energy-Efficient Autonomous Driving: A Multi-Objective Reinforcement Learning Approach","X. He; C. Lv","School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore, Singapore; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore, Singapore","IEEE/CAA Journal of Automatica Sinica","1 May 2023","2023","10","5","1329","1331","Dear Editor, With the development of automobile industry and artificial intelligence (AI) domains, autonomous vehicles (AVs) are becoming a reality and promise to revolutionize human mobility [1]–[3]. The decision-making system of AVs is crucial, which is typically required to trade off multiple competing objectives. For example, when determining driving policies, autonomous electric vehicles (AEVs) need to consider two conflicting objectives: transport efficiency and electricity consumption. As one of state-of-the-art AI technologies, reinforcement learning (RL) has demonstrated its potential in a series of challenging tasks. Accordingly, RL has attracted considerable attention from global researchers [4].","2329-9274","","10.1109/JAS.2023.123378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10113610","","","artificial intelligence;automobile industry;decision making;learning (artificial intelligence);mobile robots;reinforcement learning;traffic engineering computing","artificial intelligence;automobile industry;autonomous electric vehicles;autonomous vehicles;dear Editor;decision-making system;determining driving policies;electricity consumption;human mobility;multiobjective reinforcement;multiple competing objectives;reinforcement learning;state-of-the-art AI technologies;towards energy-efficient autonomous driving","","","","13","","1 May 2023","","","IEEE","IEEE Journals"
"Highway Lane Change Decision-Making via Attention-Based Deep Reinforcement Learning","J. Wang; Q. Zhang; D. Zhao","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE/CAA Journal of Automatica Sinica","28 Dec 2021","2022","9","3","567","569","Deep reinforcement learning (DRL), combining the perception capability of deep learning (DL) and the decision-making capability of reinforcement learning (RL) [1], has been widely investigated for autonomous driving decision-making tasks. In this letter, we would like to discuss the impact of different types of state input on the performance of DRL-based lane change decision-making.","2329-9274","","10.1109/JAS.2021.1004395","National Natural Science Foundation of China (NSFC)(grant numbers:62173325); Beijing Municipal Natural Science Foundation(grant numbers:L191002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664628","","","control engineering computing;decision making;deep learning (artificial intelligence);mobile robots;reinforcement learning;road traffic control;traffic engineering computing","DRL;highway lane change decision-making;attention-based deep reinforcement learning;perception capability;decision-making capability;autonomous driving","","23","","14","","28 Dec 2021","","","IEEE","IEEE Journals"
"Editorial Special Issue on Adaptive Dynamic Programming and Reinforcement Learning","D. Liu; F. L. Lewis; Q. Wei","School of Automation, Guangdong University of Technology, Guangzhou, China; UTA Research Institute University of Texas at Arlington, Fort Worth, TX, USA; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","14 Oct 2020","2020","50","11","3944","3947","The past decade has witnessed a surge in research activities related to adaptive dynamic programming (ADP) and reinforcement learning (RL), particularly for control applications. Several books [item 1)–5) in the Appendix] and survey papers [item 6)–10) in the Appendix] have been published on the subject. Both ADP and RL provide approximate solutions to dynamic programming problems. In a 1995 article by Barto et al. [item 11) in the Appendix], they introduced the so-called “adaptive real-time dynamic programming,” which was specifically to apply ADP for real-time control. Later, in 2002, Murray et al. [item 12) in the Appendix] developed an ADP algorithm for optimal control of continuous-time affine nonlinear systems. On the other hand, the most famous algorithms in RL are the temporal difference algorithm [item 13) in the Appendix] and the Q-learning algorithm [item 14) and 15) in the Appendix].","2168-2232","","10.1109/TSMC.2020.3025549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9224268","","Special issues and sections;Reinforcement learning;Learning systems;Control systems;Dynamic programming;Real-time systems;Optimal control","","","","4","","0","IEEE","14 Oct 2020","","","IEEE","IEEE Journals"
"Special Issue on Deep Reinforcement Learning for Emerging IoT Systems","J. Hu; P. Liu; H. Liu; O. Anya; Y. Zhang","Department of Computer Science, University of Exeter, Exeter, U.K.; School of Computer Science, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Software Engineering, East China Normal University, Shanghai, China; Cloud Artificial Intelligence and Industry Solutions Google, Mountain View, USA; Department of Informatics, University of Oslo, Oslo, Norway","IEEE Internet of Things Journal","10 Jul 2020","2020","7","7","6175","6179","Nowadays we are witnessing the formation of a massive Internet-of-Things (IoT) ecosystem that integrates a variety of wireless-enabled devices ranging from smartphones, wearables, and virtual reality facilities to sensors, drones, and connected vehicles. As IoT is penetrating every aspect of people’s life, work, and entertainment, an increasing number of IoT devices and the emerging IoT applications are driving exponential growth in wireless traffic in the foreseeable future. As a result, current IoT system architectures are facing significant challenges to handle millions of devices; thousands of servers; the transmission and processing of large volume of data, etc.","2327-4662","","10.1109/JIOT.2020.2998256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138549","","Special issues and sections;Reinforcement learning;Ecosystems;Internet of Things;Virtual reality;Big Data","","","","4","","0","IEEE","10 Jul 2020","","","IEEE","IEEE Journals"
"Comments and Corrections Corrections to “Application of Reinforcement Learning to Deep Brain Stimulation in a Computational Model of Parkinson’s Disease”","M. Lu; X. Wei; Y. Che; J. Wang; K. A. Loparo","School of Information Technology Engineering, Tianjin University of Technology and Education, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, USA","IEEE Transactions on Neural Systems and Rehabilitation Engineering","6 Mar 2020","2020","28","3","766","766","In the above article [1], financial support was incorrectly published. The correct information is as follows: This work was supported in part by the National Natural Science Foundation of China under Grant 61501330 and Grant 61771330, and in part by the Tianjin Municipal Special Program of Talents Development for Excellent Youth Scholars under Grant TJTZJH-QNBJRC-2-2.","1558-0210","","10.1109/TNSRE.2020.2970520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9027068","","Learning (artificial intelligence);Brain stimulation;Brain modeling;Computational modeling;Parkinson's disease;Education;Information technology","","","","3","","1","IEEE","6 Mar 2020","","","IEEE","IEEE Journals"
"Dynamic Voltage and Frequency Scaling in NoCs With Supervised and Reinforcement Learning Techniques","Q. Fettes; M. Clark; R. Bunescu; A. Karanth; A. Louri",Ohio University; Belcan; Ohio University; Ohio University; The George Washington University,"Computer","23 Aug 2019","2019","52","9","4","5","Computer architects often face the challenging task of balancing various design considerations, such as performance, power, cost, and reliability. With its unprecedented success in numerous domains and disciplines, machine learning could be a promising approach to solving complicated architecture design and optimization problems. IEEE Transactions on Computers continues to lead research in this area, recently publishing more than 10 papers that propose innovative, viable, and promising ways to take the advantage of machine learning in computer architecture. Discusses how the IEEE Transactions on Computers will closely follow these exciting developments in applying machine learning to computer architectures and provide the latest academic and industry research.","1558-0814","","10.1109/MC.2019.2923827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812167","","","","","","2","","0","IEEE","23 Aug 2019","","","IEEE","IEEE Magazines"
"Guest editorial for special issue on extensions of reinforcement learning and adaptive control","F. L. Lewis; W. Dixon; Z. Hou; T. Yucelen",NA; NA; NA; NA,"IEEE/CAA Journal of Automatica Sinica","15 Jan 2015","2014","1","3","225","226","Adaptive control is a proven method for learning feedback controllers for systems with unknown dynamic models, exogenous disturbances, nonzero setpoints, and un-modeled nonlinearities. Adaptive control has been applied for years in process control, industry, aerospace systems, vehicle systems, and elsewhere. Reinforcement learning refers to a broad class of methods for improving control policies based on observation of the performance or value of current policies. Reinforcement learning allows the learning of optimal controls in real-time using data measured along system trajectories.","2329-9274","","10.1109/JAS.2014.7004679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004679","","Special issues and sections;Learning (artificial intelligence);Adaptive control;Feedback","","","","","","","","15 Jan 2015","","","IEEE","IEEE Journals"
