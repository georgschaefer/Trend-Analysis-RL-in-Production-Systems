"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Deep Reinforcement Learning for Coordinated Voltage Regulation in Active Distribution Networks","L. Yu; Z. Chen; X. Jiang; T. Zhang; D. Yue","College of Auto. & AI, NUPT, Nanjing, China; College of Auto. & AI, NUPT, Nanjing, China; College of Auto. & AI, NUPT, Nanjing, China; College of Auto. & AI, NUPT, Nanjing, China; College of Auto. & AI, NUPT, Nanjing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","4005","4010","In active distribution networks (ADNs), voltage violation is aggravated by the massive expansion of renewable distributed energy, such as distributed photovoltaics. Traditional voltage control schemes need to know accurate ADN models or can not support the coordination among several kinds of resources. In this paper, we investigate a coordination voltage regulation problem in ADNs with various distributed energy resources (DERs), e.g., static var compensators, energy storage systems, photovoltaic inverters, and flexible loads. To ensure voltage security while minimizing active power curtailment, we formulate a cooperative Markov game related to different DERs. Then, a novel voltage regulation algorithm is proposed to solve the game based on attention-based multi-agent proximal policy optimization. Finally, numerical results show the effectiveness of the proposed algorithm, e.g., compared with existing schemes, it can reduce the average voltage deviation by 84.99%-99.78% without any active power curtailment.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10054794","Research and Development; National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054794","voltage regulation;active distribution networks;multi-agent deep reinforcement learning","Photovoltaic systems;Deep learning;Renewable energy sources;Reinforcement learning;Games;Markov processes;Active distribution networks","control engineering computing;deep learning (artificial intelligence);demand side management;distributed power generation;game theory;Markov processes;multi-agent systems;optimisation;power distribution control;power system security;reinforcement learning;voltage control","active distribution networks;active power curtailment;ADN models;attention-based multiagent proximal policy optimization;average voltage deviation;cooperative Markov game;coordinated voltage regulation algorithm;coordination voltage regulation problem;deep reinforcement;DER;distributed photovoltaics;energy storage systems;photovoltaic inverters;renewable distributed energy resources;static VAr compensators;voltage control schemes;voltage security","","1","","16","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning","E. Vollenweider; M. Bjelonic; V. Klemm; N. Rudin; J. Lee; M. Hutter","Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5120","5126","Reinforcement learning (RL) has emerged as a powerful approach for locomotion control of highly articulated robotic systems. However, one major challenge is the tedious process of tuning the reward function to achieve the desired motion style. To address this issue, imitation learning approaches such as adversarial motion priors have been proposed, which encourage a pre-defined motion style. In this work, we present an approach to enhance the concept of adversarial motion prior-based RL, allowing for multiple, discretely switchable motion styles. Our approach demonstrates that multiple styles and skills can be learned simultaneously without significant performance differences, even in combination with motion data-free skills. We conducted several real-world experiments using a wheeled-legged robot to validate our approach. The experiments involved learning skills from existing RL controllers and trajectory optimization, such as ducking and walking, as well as novel skills, such as switching between a quadrupedal and humanoid configuration. For the latter skill, the robot was required to stand up, navigate on two wheels, and sit down. Instead of manually tuning the sit-down motion, we found that a reverse playback of the stand-up movement helped the robot discover feasible sit-down behaviors and avoided the need for tedious reward function tuning.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160751","Swiss National Science Foundation (SNF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160751","","Legged locomotion;Training;Navigation;Wheels;Switches;Reinforcement learning;Quadrupedal robots","humanoid robots;learning (artificial intelligence);legged locomotion;mobile robots;motion control;reinforcement learning;robot dynamics","advanced skills;adversarial motion prior-based RL;desired motion style;discretely switchable motion styles;existing RL controllers;highly articulated robotic systems;imitation learning;locomotion control;motion data-free skills;multiple adversarial motion priors;multiple styles;novel skills;pre-defined motion style;reinforcement learning;significant performance differences;tedious reward function tuning;wheeled-legged robot","","1","","23","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Reaching Pruning Locations in a Vine Using a Deep Reinforcement Learning Policy","F. Yandun; T. Parhar; A. Silwal; D. Clifford; Z. Yuan; G. Levine; S. Yaroshenko; G. Kantor","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Mineral, X - The Moonshot Factory, Mountain View, CA, USA; Mineral, X - The Moonshot Factory, Mountain View, CA, USA; Mineral, X - The Moonshot Factory, Mountain View, CA, USA; Mineral, X - The Moonshot Factory, Mountain View, CA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","2400","2406","We outline a neural network-based pipeline for perception, control and planning of a 7 DoF robot for tasks that involve reaching into a dormant grapevine canopy. The proposed system consists of a 6 DoF industrial robot arm and a linear slider that can actuate on an entire grape vine. Our approach uses Convolutional Neural Networks to detect buds in dormant grape vines and a Reinforcement Learning based control strategy to reach desired cut-point locations for pruning tasks. Within this framework, three methodologies are developed and compared to reach the desired locations: the learned policy-based approach (RL), a hybrid method that uses the learned policy and an inverse kinematics solver (RL+IK), and lastly a classical approach commonly used in robotics. We first tested and validated the suitability of the proposed learning methodology in a simulated environment that resembled laboratory conditions. A reaching accuracy of up to 61.90% and 85.71% for the RL and RL+IK approaches respectively was obtained for a vine that the agent observed while learning. When testing in a new vine, the accuracy was up to 66.66% and 76.19% for RL and RL+IK, respectively. The same methods were then deployed on a real system in an end to end procedure: autonomously scan the vine using a vision system, create its model and finally use the learned policy to reach cutting points. The reaching accuracy obtained in these tests was 73.08%.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9562075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562075","","Training;Service robots;Pipelines;Laboratories;Reinforcement learning;Robot sensing systems;End effectors","agricultural robots;convolutional neural nets;deep learning (artificial intelligence);industrial manipulators;manipulator kinematics;path planning;reinforcement learning;robot vision","cut-point locations;reaching accuracy;pruning locations;deep reinforcement learning policy;7 DoF robot;grapevine canopy;6 DoF industrial robot arm;grape vine;convolutional neural networks;inverse kinematics solver;vision system;agricultural manipulation","","1","","38","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Attitude Control Based Autonomous Underwater Vehicle Multi-mission Motion Control with Deep Reinforcement Learning","Y. Fang; J. Pu; H. Zhou; S. Liu; Y. Cao; Y. Liang","College of Power Engineering, Naval University of Engineering, Wuhan, China; College of Power Engineering, Naval University of Engineering, Wuhan, China; College of Power Engineering, Naval University of Engineering, Wuhan, China; College of Power Engineering, Naval University of Engineering, Wuhan, China; College of Power Engineering, Naval University of Engineering, Wuhan, China; Ship Integrated Testing and Training, Center Naval University of Engineering, Wuhan, China","2021 5th International Conference on Automation, Control and Robots (ICACR)","11 Nov 2021","2021","","","120","129","Due to the limitations of classical control strategies for underwater vehicles, the artificial intelligent control technologies have attracted more attention than ever, especially the Deep Reinforcement Learning. However, it is usually confusing to choose a proper Deep Reinforcement Learning algorithm and to configurate an effective reward function according to different autonomous underwater vehicle assignments. To solve the problem, this research explores three different missions that autonomous underwater vehicle might perform, with two Deep Reinforcement Learning algorithms and three reward functions adopted respectively. Deep Reinforcement Learning controllers take the attitude sensor information as input, the control signals of X-rudder blades as output. The simulation experiments results are compared and analyzed, which has an important reference value for the reward function setting and Deep Reinforcement Learning algorithm selection for different autonomous underwater vehicle control tasks.","","978-1-6654-3849-0","10.1109/ICACR53472.2021.9605171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9605171","autonomous underwater vehicle;deep deterministic policy gradient;deep reinforcement learning;multi-mission;proximal policy optimization","Motion planning;Autonomous underwater vehicles;Attitude control;Blades;Reinforcement learning;Robot sensing systems;Task analysis","attitude control;autonomous underwater vehicles;control engineering computing;deep learning (artificial intelligence);intelligent control;motion control","control based autonomous underwater vehicle multimission motion control;classical control strategies;artificial intelligent control technologies;effective reward function;different missions;control signals;autonomous underwater vehicle control tasks;deep reinforcement learning;autonomous underwater vehicle assignments;deep reinforcement learning algorithm selection","","1","","25","IEEE","11 Nov 2021","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Method for Humanoid Robot Walking","Y. Liu; S. Bi; M. Dong; Y. Zhang; J. Huang; J. Zhang","School of Computer Science and Technology, South China University of Technology, Guangzhou, China; School of Computer Science and Technology, South China University of Technology, Guangzhou, China; School of Computer Science and Technology, South China University of Technology, Guangzhou, China; School of Computer Science and Technology, South China University of Technology, Guangzhou, China; Shenzhen Academy of Robotics, Shenzhen, China; Shenzhen Academy of Robotics, Shenzhen, China","2018 IEEE 8th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","11 Apr 2019","2018","","","623","628","In this paper, we describe a model-free reinforcement learning method for gait controlling of humanoid robots, which combines Q-learning with Radial Basis Function Network. With the help of RBF Network, this method can solve the approximation problem caused by continuous state space and action space. The approach is applied to the controllers on hip joints of humanoid robots that receives sensory data and constantly adjusts the outputs of steering engines on hip joints, finding an optimal policy that can guide humanoid robots to walk stably on different uneven terrains. We have tested the approach on Webots, a simulation platform, and experiment results have proven the validity of the proposed method.","2379-7711","978-1-5386-7057-6","10.1109/CYBER.2018.8688355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8688355","humanoid robot;reinforcement learning;gait planning;radial basis function network","","humanoid robots;learning (artificial intelligence);legged locomotion;motion control;radial basis function networks;stability","RBF Network;humanoid robot walking;model-free reinforcement learning method;gait controlling;Radial Basis Function Network;Q-learning;Webots simulation platform","","1","","14","IEEE","11 Apr 2019","","","IEEE","IEEE Conferences"
"M2M-Routing: Environmental Adaptive Multi-agent Reinforcement Learning based Multi-hop Routing Policy for Self-Powered IoT Systems","W. Zhang; J. Zhang; M. Xie; T. Liu; W. Wang; C. Pan","Department of Computer Science, Texas A&M University-Corpus Christi, Corpus Christi, USA; Department of Electrical Engineering, Harvard University, Cambridge, USA; Department of Computer Science, University of Texas at San Antonio, San Antonio, USA; Department of Math and Computer Science, Lawrence Technological University, Southfield, USA; Department of Computer Science, Texas A&M University-Corpus Christi, Corpus Christi, USA; Department of Computer Science, Texas A&M University-Corpus Christi, Corpus Christi, USA","2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)","19 May 2022","2022","","","316","321","Energy harvesting (EH) technologies facilitate the trending proliferation of IoT devices with sustainable power supplies. However, the intrinsic weak and unstable nature of EH results in frequent and unpredictable power interruptions in EH IoT devices, which further causes unpleasant packet loss or reconnection failures in IoT network. Therefore, conventional routing and energy allocation methods are inefficient in the EH environments. The complexity of the EH environment caused a stumbling block to an intelligent routing policy and energy allocation. To address the problems, this work proposes an environment adaptive Deep Reinforcement Learning (DRL)-based multi-hop routing policy, M2M-Routing, to jointly optimize energy allocation and routing policy and mitigate these challenges through leveraging the offline computation resources. We prepare multi-models for the complex energy harvesting environment offline. By searching a historically similar power trace to identify the model ID, the prepared DRL model is selected to manage energy allocation and routing policy on the query power traces. Simulation results indicate that M2M-Routing improves the amount of data delivery by ~ 3 × to ~ 4 × compared with baselines.","1558-1101","978-3-9819263-6-1","10.23919/DATE54114.2022.9774779","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774779","","Training;Adaptation models;Adaptive systems;Computational modeling;Simulation;Reinforcement learning;Routing","deep learning (artificial intelligence);energy harvesting;Internet of Things;multi-agent systems;reinforcement learning;telecommunication computing;telecommunication network routing;telecommunication power management","self-powered IoT systems;energy harvesting technologies;sustainable power supplies;intrinsic weak nature;unstable nature;EH results;frequent power interruptions;unpredictable power interruptions;EH IoT devices;unpleasant packet loss;reconnection failures;IoT network;conventional routing;energy allocation;EH environment;intelligent routing policy;complex energy harvesting environment offline;historically similar power trace;query power traces;environmental adaptive multiagent reinforcement learning;M2M-routing;multihop routing policy;energy allocation methods;adaptive deep reinforcement learning","","1","","14","","19 May 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Vision-Based Lateral Control of a Self-Driving Car","M. Huang; M. Zhao; P. Parikh; Y. Wang; K. Ozbay; Z. -P. Jiang","Department of Electrical and Computer Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, New York University, Brooklyn, NY, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; C2SMART Center, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, New York University, Brooklyn, NY, USA","2019 IEEE 15th International Conference on Control and Automation (ICCA)","14 Nov 2019","2019","","","1126","1131","Lateral control design is one of the fundamental components for self-driving cars. In this paper, we propose a learning-based control strategy that enables a mobile car equipped with a camera to perfectly perform lane keeping in a road on the ground. Using the method of adaptive dynamic programming, the proposed control algorithm exploits the structural knowledge of the car kinematics as well as the collected data (images) about the lane information. An adaptive optimal lateral controller is obtained through a data-driven learning algorithm. The effectiveness of the proposed method is demonstrated by theoretical stability proofs and experimental evaluations.","1948-3457","978-1-7281-1164-3","10.1109/ICCA.2019.8900002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900002","","Automobiles;Kinematics;Tracking;Wheels;Stability analysis;Mobile robots","automobiles;computer vision;control system synthesis;dynamic programming;learning (artificial intelligence);motion control;position control","reinforcement learning;vision-based lateral control;self-driving car;lateral control design;learning-based control strategy;mobile car;adaptive dynamic programming;control algorithm;car kinematics;lane information;adaptive optimal lateral controller","","1","","27","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Networked Control with Network Delays for Signal Temporal Logic Specifications","J. Ikemoto; T. Ushio","Engineering Science, Osaka University, Toyonaka, Japan; Engineering Science, Osaka University, Toyonaka, Japan","2022 IEEE 27th International Conference on Emerging Technologies and Factory Automation (ETFA)","25 Oct 2022","2022","","","1","8","We apply deep reinforcement learning (DRL) to design of a networked controller with network delays to complete a temporal control task that is described by a signal temporal logic (STL) formula. STL is useful to deal with a temporal specification with a bounded time interval for a dynamical system. In general, an agent needs not only the current system state but also the past behavior of the system to determine a desired control action for satisfying the given STL formula. Additionally, we need to consider the effect of network delays for data transmissions. Thus, we propose an extended Markov decision process (MDP) using past system states and control actions, which is called a τd-MDP, so that the agent can evaluate the satisfaction of the STL formula considering the network delays. Thereafter, we apply a DRL algorithm to design a networked controller using the τd-MDP. Through simulations, we also demonstrate the learning performance of the proposed algorithm.","","978-1-6654-9996-5","10.1109/ETFA52439.2022.9921505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9921505","deep reinforcement learning;signal temporal logic;network control systems;network delays","Process control;Reinforcement learning;Syntactics;Markov processes;Solids;Control systems;Numerical simulation","learning (artificial intelligence);Markov processes;temporal logic","temporal specification;dynamical system;current system state;desired control action;STL formula;network delays;system states;deep reinforcement learning-based networked control;signal temporal logic specifications;temporal control task;signal temporal logic formula;τd-MDP;DRL;extended Markov decision process;MDP","","1","","25","IEEE","25 Oct 2022","","","IEEE","IEEE Conferences"
"Near-optimal Deep Reinforcement Learning Policies from Data for Zone Temperature Control","L. D. Natale; B. Svetozarevic; P. Heer; C. N. Jones","Urban Energy Systems Laboratory, Swiss Federal Laboratories for Materials Science and Technology (Empa), Dübendorf, Switzerland; Urban Energy Systems Laboratory, Swiss Federal Laboratories for Materials Science and Technology (Empa), Dübendorf, Switzerland; Urban Energy Systems Laboratory, Swiss Federal Laboratories for Materials Science and Technology (Empa), Dübendorf, Switzerland; Laboratoire d’Automatique, Swiss Federal Institute of Technology Lausanne (EPFL), Lausanne, Switzerland","2022 IEEE 17th International Conference on Control & Automation (ICCA)","25 Jul 2022","2022","","","698","703","Replacing poorly performing existing controllers with smarter solutions will decrease the energy intensity of the building sector. Recently, controllers based on Deep Reinforcement Learning (DRL) have been shown to be more effective than conventional baselines. However, since the optimal solution is usually unknown, it is still unclear if DRL agents are attaining near-optimal performance in general or if there is still a large gap to bridge.In this paper, we investigate the performance of DRL agents compared to the theoretically optimal solution. To that end, we leverage Physically Consistent Neural Networks (PCNNs) as simulation environments, for which optimal control inputs are easy to compute. Furthermore, PCNNs solely rely on data to be trained, avoiding the difficult physics-based modeling phase, while retaining physical consistency. Our results hint that DRL agents not only clearly outperform conventional rule-based controllers, they furthermore attain near-optimal performance.","1948-3457","978-1-6654-9572-1","10.1109/ICCA54724.2022.9831914","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9831914","","Energy consumption;Analytical models;Temperature distribution;Computational modeling;Buildings;Pipelines;Optimal control","learning (artificial intelligence);neural nets;optimal control;temperature control","DRL agents;theoretically optimal solution;leverage Physically Consistent Neural Networks;PCNNs;optimal control inputs;difficult physics-based modeling phase;physical consistency;outperform conventional rule-based controllers;near-optimal performance;optimal Deep Reinforcement;zone temperature control;smarter solutions;energy intensity;building sector;conventional baselines","","1","","21","IEEE","25 Jul 2022","","","IEEE","IEEE Conferences"
"CRMRL: Collaborative Relationship Meta Reinforcement Learning for Effectively Adapting to Type Changes in Multi-Robotic System","H. Jia; Y. Zhao; Y. Zhai; B. Ding; H. Wang; Q. Wu","College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China","IEEE Robotics and Automation Letters","29 Aug 2022","2022","7","4","11362","11369","Multi-agent reinforcement learning methods have been widely used for multi-robotic systems, and meta-learning methods are also applied to help robots reuse prior experiences to guide new tasks learning. But in some multi-robotic tasks, the robot types cannot be determined in advance or may dynamically change in practical application, bringing urgent challenges. When the robot types change, the prior experiences may be outdated and useless for new robots, even leading to negative transfer. It significantly limits the performance of most existing meta reinforcement learning methods. We get inspiration from the structure of bees, ants, and neurons. These colonies can overcome the interference caused by individual replacement, and always maintain dynamic stability. We find the relationship among individuals is more important than individuals themselves in these colonies, and propose a collaborative relationship meta reinforcement learning method (CRMRL). It concentrates on the relationship among robots, and reuses the collective knowledge to alleviate the interference caused by robot changes, dealing with the challenge of different combinations and dynamic changes in multi-robotic systems. The experiments are carried out on the StarCraft II platform and Webots simulator. The extensive results indicate that our method has noticeable improvement on many indicators compared with the traditional meta reinforcement learning methods.","2377-3766","","10.1109/LRA.2022.3196782","Major Science and Technology Innovation 2030(grant numbers:2020AAA0104803); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851500","Distributed robot systems;multi-robot system;reinforcement learning","Task analysis;Robots;Collaboration;Reinforcement learning;Robustness;Training;Multi-agent systems","","","","1","","34","IEEE","5 Aug 2022","","","IEEE","IEEE Journals"
"Collision-Free Deep Reinforcement Learning for Mobile Robots using Crash-Prevention Policy","M. D. Kobelrausch; A. Jantsch","Institute of Computer Technology, TU Wien, Vienna, Austria; Institute of Computer Technology, TU Wien, Vienna, Austria","2021 7th International Conference on Control, Automation and Robotics (ICCAR)","25 Jun 2021","2021","","","52","59","In this paper, we propose a crash-prevention policy for an autonomous collision-free mobile robot based on deep reinforcement learning. The objective is to reach a random location in a limited workspace safely. We go beyond the well-treated navigation paradigm by introducing a crash-prevention policy derived from action-sensor-space characteristics to achieve collision-free learning. This approach enables efficient and safe exploration by guaranteeing continuous collision-free actions, especially for agents learning in physical systems. We use Deep Deterministic Policy Gradient as a base method to evaluate the proposed crash-prevention policy on a mobile robot environment. Experiments show that using our approach maintains or even slightly improves training results while collisions are entirely avoided.","2251-2454","978-1-6654-4986-1","10.1109/ICCAR52225.2021.9463474","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463474","Safe Reinforcement Learning;Deep Reinforcement Learning;Continuous Control;Collision-Free Learning;Mobile Robots","Training;Navigation;Reinforcement learning;Robot sensing systems;Computer crashes;Mobile robots;Task analysis","collision avoidance;control engineering computing;deep learning (artificial intelligence);mobile robots;multi-agent systems;navigation","mobile robots;crash-prevention policy;continuous collision-free actions;deep deterministic policy gradient;agents learning;collision-free deep reinforcement learning;autonomous collision-free mobile robot;collision avoidance;action-sensor-space characteristics","","","","24","IEEE","25 Jun 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Controller for Vision-Based Serial Flexible Link Manipulator","U. K. Sahu; D. Patra; B. Subudhi","Department of Mechatronics Engineering, Manipal Institute of Technology, Manipal, Karnataka, India; Department of Electrical Engineering, National Institute of Technology, Rourkela, Odisha, India; School of Electrical Sciences, Indian Institute of Technology Goa, Goa, India","2021 International Symposium of Asian Control Association on Intelligent Robotics and Industrial Automation (IRIA)","4 Nov 2021","2021","","","331","336","In recent years, Flexible Link Manipulators (FLMs) find a wide spectrum of applications including space exploration, defense and medical services owing to several advantages over the rigid manipulators. However, due to the flexible structure of the links in these manipulators, a number of control complexities arise. Owing to non-collocated sensors and actuators, FLM acts as a non-minimum phase system. Therefore, it is challenging to design a control scheme to achieve perfect tip tracking performance with a small tracking error. The objective of this study is to design adaptive intelligent tip-tracking control strategies for FLMs. A vision sensor is used along with a standard mechanical sensor to provide an indirect measurement of tip point deflection. The last decade witnessed a great deal of research interest in visual servoing (VS) based control of FLM. To deal with the Field-of-View (FOV) issue of conventional Image-based Visual Servoing (IBVS) control scheme an intelligent Vision-based (IVB) controller with Deep Reinforcement Learning (DRL) is developed for tip-tracking control of FLM. In this paper, the performance of the designed controller is investigated using simulation studies. It is found that the proposed controller is able to quickly correct the tip position to bring the object within FOV to complete the visual servoing task.","","978-1-6654-3323-5","10.1109/IRIA53009.2021.9588674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9588674","Deep Reinforcement Learning;Flexible Manipulator;Visual Servoing;Tip-tracking Control","Visualization;Service robots;Reinforcement learning;Vision sensors;Manipulators;Visual servoing;Sensor systems","adaptive control;deep learning (artificial intelligence);flexible manipulators;image sensors;manipulator dynamics;position control;reinforcement learning;robot vision;tracking;visual servoing","actuators;FLM;nonminimum phase system;vision sensor;standard mechanical sensor;tip point deflection;visual servoing based control;flexible link manipulators;medical services;flexible structure;deep reinforcement learning;adaptive intelligent tip-tracking control;conventional image-based visual servoing control scheme;space exploration;defense services;intelligent vision-based controller;Field-of-View;FOV;IBVS;DRL","","","","11","IEEE","4 Nov 2021","","","IEEE","IEEE Conferences"
"Hardware-in-the-Loop Soft Robotic Testing Framework Using an Actor-Critic Deep Reinforcement Learning Algorithm","J. Marquez; C. Sullivan; R. M. Price; R. C. Roberts","Department of Electrical and Computer Engineering, University of Texas at El Paso, El Paso, TX, USA; University of Texas at El Paso, El Paso, TX, USA; Department of Electrical and Computer Engineering, University of Texas at El Paso, El Paso, TX, USA; Department of Electrical and Computer Engineering, University of Texas at El Paso, El Paso, TX, USA","IEEE Robotics and Automation Letters","14 Aug 2023","2023","8","9","6076","6082","Polymer-based soft robots are difficult to characterize due to their non-linear nature. This difficulty is compounded by multiple additional degrees of movement freedom which adds complexity to any control strategy proposed. The following work proposes and demonstrates a modular framework to test, debug and characterize soft robots using the robot operating system (ROS), to enable modeless deep reinforcement learning control strategies through hardware-in-the-loop system training. The framework is demonstrated using an actor-critic algorithm to learn a locomotion policy for a two-actuator pneu-net soft robot with integrated resistive flex sensors. The result of convergent locomotion studies was an 89.5% increase in the likelihood of reaching the end of frame design goal versus random oracle actuation vectors.","2377-3766","","10.1109/LRA.2023.3301215","NASA GEM Fellowship; Intel Graduate SHPE scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10202193","AI-enabled robotics;hardware-software integration in robotics;machine learning for robot control;modeling;control;and learning for soft robots;soft robot materials and design;reinforcement learning","Soft robotics;Robots;Robot sensing systems;Sensors;Actuators;Pneumatic systems;Flexible printed circuits","control engineering computing;deep learning (artificial intelligence);hardware-in-the-loop simulation;operating systems (computers);reinforcement learning;robot programming;soft robotics","actor-critic deep reinforcement learning algorithm;hardware-in-the-loop soft robotic testing framework;hardware-in-the-loop system training;locomotion policy learning;modeless deep reinforcement learning control strategies;polymer-based soft robots;robot operating system;ROS;two-actuator pneu-net soft robot","","","","38","IEEE","2 Aug 2023","","","IEEE","IEEE Journals"
"A non-myopic approach based on reinforcement learning for multiple moving targets search","Y. Xu; Y. Tan; Z. Lian; R. He","College of Information System and Management, National University of Defense Technology, Changsha, Hunan, China; College of Information System and Management, National University of Defense Technology, Changsha, Hunan, China; College of Information System and Management, National University of Defense Technology, Changsha, Hunan, China; College of Information System and Management, National University of Defense Technology, Changsha, Hunan, China","The 2010 IEEE International Conference on Information and Automation","19 Jul 2010","2010","","","1672","1677","Myopic information-based approaches maximizing information gain for single one observation opportunity are effective to search for multiple moving targets in ocean surveillance by space-based sensors. A non-myopic approach based on reinforcement learning is developed in order to maximize information gain for the long term. Reinforcement learning adjusts optimal control policy and learns system behaviors through trial-and-error experience from interactions with a dynamic environment. System states are characterized by the expected information gain, action-value functions are estimated by online SARAR (lambda) algorithm and parameterized control policy is approximated by neural networks. Finally, simulations show that non-myopic approach after sufficient training can provide better performance than myopic approach.","","978-1-4244-5704-5","10.1109/ICINFA.2010.5512235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5512235","optimal search theory;reinforcement learning;multiple moving targets;maritime surveillance;satellite","Learning;Surveillance;Satellites;Oceans;Sensor systems;Uncertainty;Resource management;State estimation;Target tracking;Entropy","computer vision;learning (artificial intelligence);motion compensation;neural nets;object detection","nonmyopic approach;reinforcement learning;multiple moving targets search;myopic information based approach;ocean surveillance;space based sensors;optimal control policy;action value function;online SARAR algorithm;parameterized control policy;neural networks","","","","23","IEEE","19 Jul 2010","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Multi-objective Optimization for Broadband Newtonian Noise Cancellation in GW Detectors","R. Jose; R. K. Kalaimani","Electrical Engineering, Indian Institute of Technology, Madras, India; Faculty of Electrical Engineering, Indian Institute of Technology, Madras, India","2022 30th Mediterranean Conference on Control and Automation (MED)","1 Aug 2022","2022","","","56","61","The sensitivity of terrestrial Gravitational-Wave detectors can be improved in the low-frequency region by sub-tracting Newtonian Noise at the mirrors of the interferometer. An estimate of the Newtonian Noise is obtained by gathering information from an array of sensors monitoring the sources of noise. Efficient and maximal subtraction of Newtonian Noise is possible when the position of the sensors is optimized for a wide range of frequencies. This constitutes a multi-objective optimization problem which is solved by generating a Pareto optimal solution. Generally, multiple Pareto optimal solutions are generated, and further analysis is done to select the most suitable Pareto point for implementation. This paper proposes a method to obtain a smart Pareto optimal point by modifying the Normal Boundary Intersection method using reinforcement learning techniques. The proposed method will directly generate the smart Pareto point to be implemented in the Newtonian Noise cancellation system. The performance of our algorithm is compared with existing literature.","2473-3504","978-1-6654-0673-4","10.1109/MED54222.2022.9837284","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837284","","Sensitivity;Reinforcement learning;Detectors;Pareto optimization;Markov processes;Noise cancellation;Mirrors","gravitational wave detectors;learning (artificial intelligence);optimisation;Pareto optimisation","broadband Newtonian Noise cancellation;GW detectors;terrestrial Gravitational-Wave detectors;low-frequency region;sub-tracting Newtonian Noise;maximal subtraction;multiobjective optimization problem;Pareto optimal solution;multiple Pareto optimal solutions;suitable Pareto point;smart Pareto optimal point;reinforcement learning techniques;smart Pareto point;Newtonian Noise cancellation system","","","","16","IEEE","1 Aug 2022","","","IEEE","IEEE Conferences"
"Inverse Reinforcement Learning of Pedestrian–Robot Coordination","D. Gonon; A. Billard","School of Engineering, EPFL, Lausanne, Switzerland; School of Engineering, EPFL, Lausanne, Switzerland","IEEE Robotics and Automation Letters","3 Jul 2023","2023","8","8","4815","4822","We apply inverse reinforcement learning (IRL) with a novel cost feature to the problem of robot navigation in human crowds. Consistent with prior empirical work on pedestrian behavior, the feature anticipates collisions between agents. We efficiently learn cost functions in continuous space from high-dimensional examples of public crowd motion data, assuming locally optimal examples. We evaluate the accuracy and predictive power of the learned models on test examples that we attempt to reproduce by optimizing the learned cost functions. We show that the predictions of our models outperform a recent related approach from the literature. The learned cost functions are incorporated into an optimal controller for a robotic wheelchair. We evaluate its performance in qualitative experiments where it autonomously travels between pedestrians, which it perceives through an on-board tracking system. The results show that our approach often generates appropriate motion plans that efficiently complement the pedestrians' motions.","2377-3766","","10.1109/LRA.2023.3289770","Hasler Foundation, Switzerland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10164266","Human-aware motion planning;learning from demonstration;path planning for multiple mobile robots or agents","Navigation;Trajectory;Cost function;Pedestrians;Costs;Reinforcement learning;Collision avoidance","handicapped aids;learning (artificial intelligence);mobile robots;optimal control;path planning;pedestrians;wheelchairs","high-dimensional examples;human crowds;inverse reinforcement learning;learned cost functions;locally optimal examples;novel cost feature;optimal controller;pedestrian behavior;pedestrian-robot coordination;pedestrians;predictive power;prior empirical work;public crowd motion data;robot navigation;robotic wheelchair;test examples","","","","19","CCBY","27 Jun 2023","","","IEEE","IEEE Journals"
"Multiagent Reinforcement Learning for Autonomous Routing and Pickup Problem with Adaptation to Variable Demand","D. Garces; S. Bhattacharya; S. Gil; D. Bertsekas","REACT lab, Harvard University, Boston, MA, USA; REACT lab, Harvard University, Boston, MA, USA; REACT lab, Harvard University, Boston, MA, USA; Department of Electrical Engineering and Computer Science, Arizona State University, AZ, USA","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","3524","3531","We derive a learning framework to generate routing/pickup policies for a fleet of autonomous vehicles tasked with servicing stochastically appearing requests on a city map. We focus on policies that 1) give rise to coordination amongst the vehicles, thereby reducing wait times for servicing requests, 2) are non-myopic, and consider a-priori potential future requests, 3) can adapt to changes in the underlying demand distribution. Specifically, we are interested in policies that are adaptive to fluctuations of actual demand conditions in urban environments, such as on-peak vs. off-peak hours. We achieve this through a combination of (i) an online play algorithm that improves the performance of an offline-trained policy, and (ii) an offline approximation scheme that allows for adapting to changes in the underlying demand model. In particular, we achieve adaptivity of our learned policy to different demand distributions by quantifying a region of validity using the q-valid radius of a Wasserstein Ambiguity Set. We propose a mechanism for switching the originally trained offline approximation when the current demand is outside the original validity region. In this case, we propose to use an offline architecture, trained on a historical demand model that is closer to the current demand in terms of Wasserstein distance. We learn routing and pickup policies over real taxicab requests in San Francisco with high variability between on-peak and off-peak hours, demonstrating the ability of our method to adapt to real fluctuation in demand distributions. Our numerical results demonstrate that our method outperforms alternative rollout-based reinforcement learning schemes, as well as other classical methods from operations research.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161067","ONR YIP(grant numbers:N00014-21-1-2714); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161067","","Adaptation models;Fluctuations;Operations research;Urban areas;Switches;Reinforcement learning;Routing","mobile robots;multi-agent systems;reinforcement learning;vehicle routing","a-priori potential future requests;autonomous routing and pickup problem;autonomous vehicles;city map;demand distribution;multiagent reinforcement learning;off-peak hours;offline approximation scheme;offline architecture;offline-trained policy;online play algorithm;q-valid radius;routing pickup policies;San Francisco;servicing requests;taxicab requests;urban environments;variable demand;wait times;Wasserstein Ambiguity Set","","","","44","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Automatic Parameter Tuning via Reinforcement Learning for Crowd Simulation with Social Distancing","Y. Zhao; R. Geraerts","Department of Information and Computing Sciences, Utrecht University, Utrecht, the Netherlands; Department of Information and Computing Sciences, Utrecht University, Utrecht, the Netherlands","2022 26th International Conference on Methods and Models in Automation and Robotics (MMAR)","8 Sep 2022","2022","","","87","92","Reinforcement learning (RL) has been applied to a variety of fields such as gaming and robot navigation. We study the application of RL in crowd simulation by proposing an automatic parameter tuning system based on Proximal Policy Optimization (PPO). The system can be used with any crowd simulation software to improve the quality of the simulation by automatically assigning parameters to each agent during the simulation. Our experiments indicate that the automatic parameter tuning system can reduce unexpected congestions in counterflow scenarios. In addition, by utilizing the improved commonly used crowd simulation algorithms and our parame-ter tunning system, we can represent social distancing behavior of pedestrians under COVID-19, where pedestrians comply to the suggested social distance when they have enough space to move while they reduce their social distances to others when there is limited space.","","978-1-6654-6858-9","10.1109/MMAR55195.2022.9874284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9874284","","Training;Software algorithms;Human factors;Reinforcement learning;Social factors;Software;Behavioral sciences","behavioural sciences computing;optimisation;pedestrians;reinforcement learning;traffic engineering computing","COVID-19;pedestrians;PPO;gaming;social distancing behavior;crowd simulation software;proximal policy optimization;automatic parameter tuning system;robot navigation;RL application;reinforcement learning","","","","27","IEEE","8 Sep 2022","","","IEEE","IEEE Conferences"
"Model-based Reinforcement Learning with Provable Safety Guarantees via Control Barrier Functions","H. Zhang; Z. Li; A. Clark","Department of Electrical and Computer Engineering, Worcester Polytechnic Inistitute, Worcester, MA, USA; Department of Electrical and Computer Engineering, Worcester Polytechnic Inistitute, Worcester, MA, USA; Department of Electrical and Computer Engineering, Worcester Polytechnic Inistitute, Worcester, MA, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","792","798","Safety is a critical property in applications including robotics, transportation, and energy. Safety is especially challenging in reinforcement learning (RL) settings, in which uncertainty of the system dynamics may cause safety violations during exploration. Control Barrier Functions (CBFs), which enforce safety by constraining the control actions at each time step, are a promising approach for safety-critical control. This technique has been applied to ensure the safety of model-free RL, however, it has not been integrated into model-based RL. In this paper, we propose Uncertainty-Tolerant Control Barrier Functions (UTCBFs), a new class of CBFs to incorporate model uncertainty and provide provable safety guarantees with desired probability. Furthermore, we introduce an algorithm for model-based RL to guarantee safety by integrating CBFs with gradient-based policy search. Our approach is verified through a numerical study of a cart-pole system and an inverted pendulum system with comparison to state-of-the-art RL algorithms.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561253","","Uncertainty;System dynamics;Transportation;Reinforcement learning;Stability analysis;Safety;Numerical models","gradient methods;learning (artificial intelligence);optimal control;pendulums;probability","model-based reinforcement learning;provable safety guarantees;critical property;reinforcement learning settings;safety violations;CBFs;control actions;safety-critical control;model-free RL;model-based RL;Uncertainty-Tolerant Control Barrier Functions;model uncertainty;gradient-based policy search;state-of-the-art RL algorithms","","","","43","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement-Learning-Based Job-Shop Scheduling for Intelligent Intersection Management","S. -C. Huang; K. -E. Lin; C. -Y. Kuo; L. -H. Lin; M. O. Sayin; C. -W. Lin",National Taiwan University; National Taiwan University; National Taiwan University; National Taiwan University; Bilkent University; National Taiwan University,"2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)","2 Jun 2023","2023","","","1","6","The goal of intersection management is to organize vehicles to pass the intersection safely and efficiently. Due to the technical advance of connected and autonomous vehicles, intersection management becomes more intelligent and potentially unsignalized. In this paper, we propose a reinforcement-learning-based methodology to train a centralized intersection manager. We define the intersection scheduling problem with a graph-based model and transform it to the job-shop scheduling problem (JSSP) with additional constraints. To utilize reinforcement learning, we model the scheduling procedure as a Markov decision process (MDP) and train the agent with the proximal policy optimization (PPO). A grouping strategy is also developed to apply the trained model to streams of vehicles. Experimental results show that the learning-based intersection manager is especially effective with high traffic densities. This paper is the first work in the literature to apply reinforcement learning on the graph-based intersection model. The proposed methodology can flexibly deal with any conflicting scenario and indicate the applicability of reinforcement learning to Intelligent intersection management.","1558-1101","979-8-3503-9624-9","10.23919/DATE56975.2023.10137280","Ministry of Education (MOE) in Taiwan(grant numbers:NTU-111V1901-5); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10137280","Intelligent intersection management;job-shop scheduling;proximal policy optimization;reinforcement learning","Training;Measurement;Connected vehicles;Reinforcement learning;Transforms;Markov processes;Scheduling","deep learning (artificial intelligence);graph theory;job shop scheduling;learning (artificial intelligence);Markov processes;mobile robots;optimisation;reinforcement learning;road traffic control;road vehicles;scheduling","autonomous vehicles;centralized intersection manager;connected vehicles;graph-based intersection model;graph-based model;Intelligent intersection management;intersection scheduling problem;job-shop scheduling problem;learning-based intersection manager;reinforcement learning;reinforcement-learning-based job-shop scheduling;reinforcement-learning-based methodology;scheduling procedure;trained model","","","","17","","2 Jun 2023","","","IEEE","IEEE Conferences"
"Adaptive Image-based Visual Servoing with Reinforcement Learning for Wheeled Mobile Robots","H. Shi; G. Sun; R. Zhang; X. Chen","School of Computer Science, Northwestern Poly technical University, Xi ‘an, Shaanxi Province, China; School of Computer Science, Northwestern Poly technical University, Xi ‘an, Shaanxi Province, China; School of Computer Science, Northwestern Poly technical University, Xi ‘an, Shaanxi Province, China; China Electronic Product Reliability and Environmental, Testing Research Institute, Guangzhou, Guangdong Province, China","2018 IEEE International Conference on Mechatronics and Automation (ICMA)","7 Oct 2018","2018","","","954","959","Appropriate servoing gain are critical to good performance of image-based visual servoing (IBVS). Servoing gain affects the stability and the convergence rate for the robot to reach a desired position, but the servoing gains in many IBVS applications are heuristically set as a constant. A generic method for determining a series of the servoing gains is proposed, which adjusts adaptively the servoing gain by using Q-Iearning in order to realize more efficient control. The proposed method addresses problems associated with IBVS control, for instance, slow convergence and low stability. The complete IBVS control system is validated by several experiments on a WMRs that reaches a desired position. Simulation and experimental results demonstrate that the proposed IBVS method has better convergence and stability than the competing methods.","2152-744X","978-1-5386-6075-1","10.1109/ICMA.2018.8484637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8484637","Image-based visual servoing;Mobile robot;Reinforcement learning;Servoing gain","Cameras;Visual servoing;Mobile robots;Stability analysis;Convergence","learning (artificial intelligence);mobile robots;path planning;position control;robot vision;visual servoing","adaptive image-based visual servoing;servoing gain;IBVS control system;Q-Iearning","","","","16","IEEE","7 Oct 2018","","","IEEE","IEEE Conferences"
"Multi-Alpha Soft Actor-Critic: Overcoming Stochastic Biases in Maximum Entropy Reinforcement Learning","C. Igoe; S. Pande; S. Venkatraman; J. Schneider","Machine Learning Department, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, United States; Robotics Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, United States; Robotics Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, United States; Robotics Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, United States","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","7162","7168","The successful application of robotic control requires intelligent decision-making to handle the long tail of complex scenarios that arise in real-world environments. Recently, Deep Reinforcement Learning (DRL) has provided a data-driven framework to automatically learn effective policies in such complex settings. Since its introduction in 2018, Soft Actor-Critic (SAC) remains as one of the most popular off-policy DRL algorithms and has been used extensively to learn performant robotic control policies. However, in this paper we argue that by relying on the maximum entropy formalism to define learning objectives, previous work introduces a significant bias away from optimal decision making, which often requires near-deterministic behaviour for high-precision tasks. Moreover, we show that when training with the original variants of SAC, overcoming this bias by reducing entropy budgets or entropy coefficients introduces separate issues that lead to slow or unstable learning. We address these shortcomings by treating the entropy coefficient $\alpha$ as a random variable and introduce Multi-Alpha Soft Actor-Critic (MAS). We show how MAS overcomes the stochastic bias of SAC in a variety of robotic control tasks including the CARLA urban-driving simulator, while maintaining the stability and sample efficiency of the original algorithms.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161395","","Training;Deep learning;Decision making;Reinforcement learning;Tail;Entropy;Stability analysis","decision making;deep learning (artificial intelligence);entropy;learning (artificial intelligence);mobile robots;reinforcement learning;traffic engineering computing","Deep Reinforcement Learning;effective policies;entropy budgets;entropy coefficients;intelligent decision-making;maximum entropy formalism;maximum entropy reinforcement learning;MultiAlpha Soft Actor-Critic;Multialpha soft actor-critic;off-policy DRL algorithms;optimal decision making;performant robotic control policies;robotic control tasks;slow learning;stochastic bias;unstable learning","","","","29","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Research on Intelligent Dynamic Scheduling Algorithm for Automated Guided Vehicles in Container Terminal Based on Deep Reinforcement Learning","F. Wang; Z. Lu; Y. Zhang","College of Intelligent Systems Science and Engineering, Harbin Engineering University, Harbin, Heilongjiang, China; College of Intelligent Systems Science and Engineering, Harbin Engineering University, Harbin, Heilongjiang, China; College of Intelligent Systems Science and Engineering, Harbin Engineering University, Harbin, Heilongjiang, China","2023 IEEE International Conference on Mechatronics and Automation (ICMA)","22 Aug 2023","2023","","","2406","2412","As container terminals become more intelligent, the scheduling of their handling equipment becomes more complex. Therefore, a faster and more efficient scheduling algorithm has become a hot research topic. In this paper, a Deep Q-Network (DQN) method in deep reinforcement learning is proposed for the scheduling problem of Automated Guided Vehicles (AGV) in automated container terminals. The value function (Q-value) of each task is obtained by a neural network and processed by a mask. After a period of training, the optimal planning strategy of the AGV is achieved in each case. Finally, two problem scenarios have been developed as examples for reviewing. One is a single AGV scheduling, which optimizes the time taken to complete the task. The other is a multi-AGV scheduling, which maximizes the efficiency of the work, and their optimal scheduling strategies are obtained respectively. The results show that the algorithm has good applicability.","2152-744X","979-8-3503-2084-8","10.1109/ICMA57826.2023.10215866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10215866","Container terminal;Automated guided vehicle (AGV);Equipment scheduling;Deep Q-Network (DQN);Mask","Deep learning;Training;Remotely guided vehicles;Scheduling algorithms;Heuristic algorithms;Optimal scheduling;Reinforcement learning","automatic guided vehicles;deep learning (artificial intelligence);dynamic scheduling;optimisation;reinforcement learning;scheduling;sea ports","automated container terminals;Automated Guided Vehicles;container terminal;Deep Q-Network method;deep reinforcement learning;faster scheduling algorithm;handling equipment;intelligent dynamic scheduling algorithm;more efficient scheduling algorithm;multiAGV scheduling;neural network;optimal planning strategy;optimal scheduling strategies;problem scenarios;scheduling problem;single AGV scheduling;value function","","","","21","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"SAFER: Safe Collision Avoidance Using Focused and Efficient Trajectory Search with Reinforcement Learning","M. Srouji; H. Thomas; Y. -H. H. Tsai; A. Farhadi; J. Zhang",Apple Inc.; Apple Inc.; Apple Inc.; Apple Inc.; Apple Inc.,"2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","8","Collision avoidance is key for mobile robots and agents to operate safely in the real world. In this work we present SAFER, an efficient and effective collision avoidance system that is able to improve safety by correcting the control commands sent by an operator. It combines real-world reinforcement learning (RL), search-based online trajectory planning, and automatic emergency intervention, e.g. automatic emergency braking (AEB). The goal of the RL is to learn an effective corrective control action that is used in a focused search for collision-free trajectories, and to reduce the frequency of triggering automatic emergency braking. This novel setup enables the RL policy to learn safely and directly on mobile robots in a real-world indoor environment, minimizing actual crashes even during training. Our real-world experiments show that, when compared with several baselines, our approach enjoys a higher average speed, lower crash rate, less emergency intervention, smaller computation overhead, and smoother overall control.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260402","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260402","","Training;Trajectory planning;Search methods;Reinforcement learning;Computer crashes;Trajectory;Safety","collision avoidance;learning (artificial intelligence);mobile robots;reinforcement learning;road safety","automatic emergency braking;collision-free trajectories;control commands;effective collision avoidance system;effective corrective control action;efficient collision avoidance system;efficient trajectory search;emergency intervention;focused search;mobile robots;real-world experiments;real-world indoor environment;reinforcement learning;RL policy;safe collision avoidance;SAFER;smoother overall control;trajectory planning","","","","43","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Inverted Landing in a Small Aerial Robot via Deep Reinforcement Learning for Triggering and Control of Rotational Maneuvers","B. Habas; J. W. Langelaan; B. Cheng","Department of Mechanical Engineering, Biological and Robotic Intelligent Fluid Locomotion Lab, The Pennsylvania State University, University Park, PA, USA; Department of Aerospace Engineering, Air Vehicle Intelligence and Autonomy Lab, The Pennsylvania State University, PA, USA; Department of Mechanical Engineering, Biological and Robotic Intelligent Fluid Locomotion Lab, The Pennsylvania State University, University Park, PA, USA","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","3368","3375","Inverted landing in a rapid and robust manner is a challenging feat for aerial robots, especially while depending entirely on onboard sensing and computation. In spite of this, this feat is routinely performed by biological fliers such as bats, flies, and bees. Our previous work has identified a direct causal connection between a series of onboard visual cues and kinematic actions that allow for reliable execution of this challenging aerobatic maneuver in small aerial robots. In this work, we utilized Deep Reinforcement Learning and a physics-based simulation to obtain a general, optimal control policy for robust inverted landing starting from any arbitrary approach condition. This optimized control policy provides a computationally-efficient mapping from the system's emulated observational space to its motor command action space, including both triggering and control of rotational maneuvers. This was accomplished by training the system over a large range of approach flight velocities that varied with magnitude and direction. Next, we performed a sim-to-real transfer and experimental validation of the learned policy via domain randomization, by varying the robot's inertial parameters in the simulation. Through experimental trials, we identified several dominant factors which greatly improved landing robustness and the primary mechanisms that determined inverted landing success. We expect the reinforcement learning framework developed in this study can be generalized to solve more challenging tasks, such as utilizing noisy onboard sensory data, landing on surfaces of various orientations, or landing on dynamically-moving surfaces.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160376","National Science Foundation(grant numbers:IIS-1815519,CMMI-1554429); Department of Defense (DoD); National Defense Science & Engineering Graduate (NDSEG); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160376","","Deep learning;Training;Visualization;Reinforcement learning;Aerospace electronics;Autonomous aerial vehicles;Robot sensing systems","aerospace robotics;aircraft control;autonomous aerial vehicles;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;optimal control;reinforcement learning","aerial robot;approach flight velocities;arbitrary approach condition;biological fliers;challenging aerobatic maneuver;challenging feat;computationally-efficient mapping;Deep Reinforcement Learning;direct causal connection;general control policy;inverted landing success;kinematic actions;landing robustness;learned policy;motor command action space;onboard sensing;onboard visual cues;optimal control policy;optimized control policy;physics-based simulation;rapid manner;robust inverted landing;robust manner;rotational maneuvers;system;triggering control","","","","34","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Meta-Reinforcement Learning via Language Instructions","Z. Bing; A. Koch; X. Yao; K. Huang; A. Knoll","Department of Informatics, Technical University of Munich, Germany; Department of Informatics, Technical University of Munich, Germany; Department of Informatics, Technical University of Munich, Germany; School of Data and Computer Science, Sun Yat-sen University, China; Department of Informatics, Technical University of Munich, Germany","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5985","5991","Although deep reinforcement learning has recently been very successful at learning complex behaviors, it requires a tremendous amount of data to learn a task. One of the fundamental reasons causing this limitation lies in the nature of the trial-and-error learning paradigm of reinforcement learning, where the agent communicates with the environment and pro-gresses in the learning only relying on the reward signal. This is implicit and rather insufficient to learn a task well. On the con-trary, humans are usually taught new skills via natural language instructions. Utilizing language instructions for robotic motion control to improve the adaptability is a recently emerged topic and challenging. In this paper, we present a meta-RL algorithm that addresses the challenge of learning skills with language instructions in multiple manipulation tasks. On the one hand, our algorithm utilizes the language instructions to shape its in-terpretation of the task, on the other hand, it still learns to solve task in a trial-and-error process. We evaluate our algorithm on the robotic manipulation benchmark (Meta-World) and it significantly outperforms state-of-the-art methods in terms of training and testing task success rates. Codes are available at https://tumi6robot.wixsite.com/million.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160626","Regional Development and Energy (StMWi)(grant numbers:DIK0249); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160626","","Training;Robot motion;Deep learning;Shape;Natural languages;Reinforcement learning;Encoding","control engineering computing;deep learning (artificial intelligence);manipulators;motion control;reinforcement learning","deep reinforcement learning;learning complex behaviors;learning skills;manipulation tasks;meta-reinforcement learning;meta-RL algorithm;Meta-World;natural language instructions;robotic manipulation;task success rates;trial-and-error learning paradigm","","","","40","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"A Novel Hybrid Approach for Fault-Tolerant Control of UAVs based on Robust Reinforcement Learning","Y. Sohège; M. Quiñones-Grueiro; G. Provan","Insight Centre for Data Analytics, University College Cork, Cork, Ireland; ISIS, Vanderbilt University, Nashville, Tennessee, USA; Insight Centre for Data Analytics, University College Cork, Cork, Ireland","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","10719","10725","The control of complex autonomous systems has significantly improved in recent years and unmanned aerial vehicles (UAVs) have become popular in the research community. Although the use of UAVs is increasing, much work remains to guarantee fault- tolerant control (FTC) properties of these vehicles. Model-based controllers are the standard way to control UAVs, however obtaining models of the system and environment for every possible operating condition a UAV can experience in a real-world scenario is not feasible. Reinforcement Learning has shown promise in controlling complex systems but requires training in a simulator (requiring a model) of the system. Further, stability guarantees do not exist for learning-based controllers, which limits their large scale application in the real-world. We propose a novel hybrid FTC approach that uses a learned supervisory controller (together with low-level PID controllers) with key stability guarantees. We use a robust reinforcement learning approach to learn the supervisory control parameters and prove stability. We empirically validate our framework using trajectory-following experiments (in simulation) for a quadcopter subject to rotor faults, wind disturbances, and severe position and attitude noise.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9562097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562097","","Training;Fault tolerant systems;Rotors;Reinforcement learning;Aerospace electronics;Supervisory control;Stability analysis","aircraft control;autonomous aerial vehicles;control system synthesis;fault tolerant control;fuzzy control;helicopters;large-scale systems;learning systems;mobile robots;remotely operated vehicles;self-adjusting systems;stability;three-term control","UAV;complex autonomous systems;unmanned aerial vehicles;model-based controllers;complex systems;learning-based controllers;learned supervisory controller;PID controllers;key stability guarantees;robust reinforcement learning approach;supervisory control parameters;rotor faults;fault-tolerant control properties;hybrid FTC approach;quadcopter","","","","33","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Interactive Reinforcement Learning With Bayesian Fusion of Multimodal Advice","S. Trick; F. Herbert; C. A. Rothkopf; D. Koert","Centre for Cognitive Science and Psychology of Information Processing, Technical University of Darmstadt, Darmstadt, Germany; Centre for Cognitive Science, Technical University of Darmstadt, Darmstadt, Germany; Centre for Cognitive Science and Psychology of Information Processing, Technical University of Darmstadt, Darmstadt, Germany; Centre for Cognitive Science, Technical University of Darmstadt, Darmstadt, Germany","IEEE Robotics and Automation Letters","28 Jun 2022","2022","7","3","7558","7565","Interactive Reinforcement Learning (IRL) has shown promising results in decreasing the learning times of Reinforcement Learning algorithms by incorporating human feedback and advice. In particular, the integration of multimodal feedback channels such as speech and gestures into IRL systems can enable more versatile and natural interaction of everyday users. In this letter, we propose a novel approach to integrate human advice from multiple modalities into IRL algorithms. For each advice modality we assume an individual base classifier that outputs a categorical probability distribution and fuse these distributions using the Bayesian fusion method Independent Opinion Pool. While existing approaches rely on heuristic fusion, our Bayesian approach is theoretically founded and fully exploits the uncertainty represented in the distributions. Experimental evaluations in a simulated grid world scenario and on a real-world human-robot interaction task with a 7-DoF robot arm show that our method clearly outperforms the closest related approach for multimodal IRL. In particular, our novel approach is more robust against misclassifications of the modalities’ individual base classifiers.","2377-3766","","10.1109/LRA.2022.3182100","German Federal Ministry of Education and Research(grant numbers:01IS20045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9794595","Human factors and human-in-the-loop;multi-modal perception for HRI;reinforcement learning","Bayes methods;Robots;Reinforcement learning;Uncertainty;Task analysis;Probabilistic logic;Human-robot interaction","Bayes methods;belief networks;human-robot interaction;manipulators;reinforcement learning;statistical distributions","categorical probability distribution;Bayesian fusion;heuristic fusion;human-robot interaction task;multimodal IRL;modalities;interactive reinforcement learning;multimodal advice;human feedback;multimodal feedback channels;IRL systems;versatile interaction;natural interaction;human advice;IRL algorithms;advice modality;independent opinion pool;7-DoF robot arm;individual base classifier","","","","32","IEEE","13 Jun 2022","","","IEEE","IEEE Journals"
"ASHA: Assistive Teleoperation via Human-in-the-Loop Reinforcement Learning","S. Chen; J. Gao; S. Reddy; G. Berseth; A. D. Dragan; S. Levine","University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","7505","7512","Building assistive interfaces for controlling robots through arbitrary, high-dimensional, noisy inputs (e.g., webcam images of eye gaze) can be challenging, especially when it involves inferring the user's desired action in the absence of a natural ‘default’ interface. Reinforcement learning from online user feedback on the system's performance presents a natural solution to this problem, and enables the interface to adapt to individual users. However, this approach tends to require a large amount of human-in-the-loop training data, especially when feedback is sparse. We propose a hierarchical solution that learns efficiently from sparse user feedback: we use offline pre-training to acquire a latent embedding space of useful, high-level robot behaviors, which, in turn, enables the system to focus on using online user feedback to learn a mapping from user inputs to desired high-level behaviors. The key insight is that access to a pre-trained policy enables the system to learn more from sparse rewards than a naïve RL algorithm: using the pre-trained policy, the system can make use of successful task executions to relabel, in hindsight, what the user actually meant to do during unsuccessful executions. We evaluate our method primarily through a user study with 12 participants who perform tasks in three simulated robotic manipulation domains using a webcam and their eye gaze: flipping light switches, opening a shelf door to reach objects inside, and rotating a valve. The results show that our method successfully learns to map 128-dimensional gaze features to 7-dimensional joint torques from sparse rewards in under 10 minutes of online training, and seamlessly helps users who employ different gaze strategies, while adapting to distributional shift in webcam inputs, tasks, and environments","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812442","NSF(grant numbers:IIS-1651843); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812442","","Training;Webcams;System performance;Training data;Reinforcement learning;Valves;Human in the loop","feedback;human-robot interaction;intelligent robots;learning (artificial intelligence);manipulators;robot vision;telerobotics","assistive teleoperation;human-in-the-loop reinforcement learning;assistive interfaces;high-dimensional inputs;noisy inputs;Webcam images;eye gaze;desired action;natural default interface;online user feedback;natural solution;human-in-the-loop training data;hierarchical solution;sparse user feedback;offline pre-training;latent embedding space;useful level robot behaviors;high-level robot behaviors;user inputs;pre-trained policy;sparse rewards;simulated robotic manipulation;gaze features;online training;ASHA;Webcam inputs;task executions;time 10.0 min","","","","65","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Safe Robot Control using Control Lyapunov Barrier Functions","D. Du; S. Han; N. Qi; H. B. Ammar; J. Wang; W. Pan","School of Astronautics, Harbin Institute of Technology, China; Department of Cognitive Robotics, Delft University of Technology, Netherlands; School of Astronautics, Harbin Institute of Technology, China; Huawei Technologies, United Kingdom; Department of Computer Science, University College, London, United Kingdom; Department of Computer Science, University of Manchester, United Kingdom","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","9442","9448","Reinforcement learning (RL) exhibits impressive performance when managing complicated control tasks for robots. However, its wide application to physical robots is limited by the absence of strong safety guarantees. To overcome this challenge, this paper explores the control Lyapunov barrier function (CLBF) to analyze the safety and reachability solely based on data without explicitly employing a dynamic model. We also proposed the Lyapunov barrier actor-critic (LBAC), a model-free RL algorithm, to search for a controller that satisfies the data-based approximation of the safety and reachability conditions. The proposed approach is demonstrated through simulation and real-world robot control experiments, i.e., a 2D quadrotor navigation task. The experimental findings reveal this approach's effectiveness in reachability and safety, surpassing other model-free RL methods.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160991","","Training;Navigation;Scalability;Robot control;Stars;Reinforcement learning;Robustness","control engineering computing;helicopters;learning (artificial intelligence);Lyapunov methods;mobile robots;path planning;reinforcement learning;robots","control Lyapunov barrier function;data-based approximation;Lyapunov barrier actor-critic;managing complicated control tasks;model-free RL algorithm;model-free RL methods;physical robots;reachability conditions;real-world robot control experiments;reinforcement learning;safe robot control;strong safety guarantees","","","","42","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Network Parameter Control in Cellular Networks through Graph-Based Multi-Agent Constrained Reinforcement Learning","A. L. Forsberg; A. Nikou; A. V. Feljan; J. Tumova","Ericsson Research, Research Area Artificial Intelligence (AI), Stockholm, Sweden; Department of Electrical Engineering and Computer Science, Division of Robotics, Perception and Learning, KTH, Stockholm, Sweden; Department of Electrical Engineering and Computer Science, Division of Robotics, Perception and Learning, KTH, Stockholm, Sweden; Ericsson Research, Research Area Artificial Intelligence (AI), Stockholm, Sweden","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","7","Cellular networks are growing in complexity at increasing speed and the geographical locations in which they are deployed in are getting denser. Traditional control methods fall short in providing a scalable and dynamic way of adapting the network to new conditions. Distributed multiagent reinforcement learning successfully addresses scalability problems seen in centralized approaches. The question of achieving learning with constraint satisfaction in distributed systems is still left unanswered in the state-of-the-art. In this work, we aim to perform distributed multi-agent constrained reinforcement learning in order to learn a policy online while satisfying imposed constraints. We use a coordination graph to model the interactions between agents and decompose the global value function. A conservative safety critic is trained in parallel to evaluate the safety of proposed actions. Our method allows for separate training of both the critic and the value network independently of each other, and hence offers flexibility in how and when to train the different models. The results are compared to a baseline using no safety critic. Simulations show that the agents succeed in learning a policy that can satisfy the constraints, while still maximizing the objective.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260368","Swedish Foundation for Strategic Research; KTH Royal Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260368","","Training;Cellular networks;Measurement;Computer aided software engineering;Scalability;Semantics;Force","graph theory;learning (artificial intelligence);multi-agent systems;reinforcement learning","cellular networks;centralized approaches;conservative safety critic;constraint satisfaction;coordination graph;distributed multiagent reinforcement learning;distributed systems;geographical locations;global value function;graph-based multiagent constrained reinforcement learning;imposed constraints;network parameter control;scalability problems;traditional control methods;value network","","","","19","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Bi-Touch: Bimanual Tactile Manipulation With Sim-to-Real Deep Reinforcement Learning","Y. Lin; A. Church; M. Yang; H. Li; J. Lloyd; D. Zhang; N. F. Lepora","Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.","IEEE Robotics and Automation Letters","24 Jul 2023","2023","8","9","5472","5479","Bimanual manipulation with tactile feedback will be key to human-level robot dexterity. However, this topic is less explored than single-arm settings, partly due to the availability of suitable hardware along with the complexity of designing effective controllers for tasks with relatively large state-action spaces. Here we introduce a dual-arm tactile robotic system (Bi-Touch) based on the Tactile Gym 2.0 setup that integrates two affordable industrial-level robot arms with low-cost high-resolution tactile sensors (TacTips). We present a suite of bimanual manipulation tasks tailored towards tactile feedback: bi-pushing, bi-reorienting, and bi-gathering. To learn effective policies, we introduce appropriate reward functions for these tasks and propose a novel goal-update mechanism with deep reinforcement learning. We also apply these policies to real-world settings with a tactile sim-to-real approach. Our analysis highlights and addresses some challenges met during the sim-to-real application, e.g. the learned policy tended to squeeze an object in the bi-reorienting task due to the sim-to-real gap. Finally, we demonstrate the generalizability and robustness of this system by experimenting with different unseen objects with applied perturbations in the real world.","2377-3766","","10.1109/LRA.2023.3295991","UoB-CSC-joint scholarship; Leadership Award from the Leverhulme Trust on ‘A biomimetic forebrain for robot touch’(grant numbers:RL-2016-39); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10184426","Force and tactile sensing;reinforcement learning","Robots;Robot sensing systems;Task analysis;Sensors;Tactile sensors;Training;Service robots","computerised monitoring;control engineering computing;deep learning (artificial intelligence);dexterous manipulators;reinforcement learning;tactile sensors","appropriate reward functions;Bi-Touch;bimanual manipulation tasks;bimanual Tactile manipulation;bireorienting task;deep reinforcement learning;dual-arm tactile robotic system;effective controllers;effective policies;human-level robot dexterity;industrial-level robot arms;learned policy;low-cost high-resolution tactile sensors;sim-to-real gap;single-arm settings;tactile feedback;Tactile Gym 2;tactile sim-to-real approach","","","","28","IEEE","17 Jul 2023","","","IEEE","IEEE Journals"
"DriveIRL: Drive in Real Life with Inverse Reinforcement Learning","T. Phan-Minh; F. Howington; T. -S. Chu; M. S. Tomov; R. E. Beaudoin; S. U. Lee; N. Li; C. Dicle; S. Findler; F. Suarez-Ruiz; B. Yang; S. Omari; E. M. Wolff",Motional AD Inc.; Motional AD Inc.; Motional AD Inc.; Motional AD Inc.; Motional AD Inc.; Motional AD Inc.; Motional AD Inc.; Motional AD Inc.; Motional AD Inc.; Motional AD Inc.; Motional AD Inc.; Motional AD Inc.; Motional AD Inc.,"2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","1544","1550","In this paper, we introduce the first published planner to drive a car in dense, urban traffic using Inverse Reinforcement Learning (IRL). Our planner, DriveIRL, generates a diverse set of trajectory proposals and scores them with a learned model. The best trajectory is tracked by our self-driving vehicle's low-level controller. We train our trajectory scoring model on a 500+ hour real-world dataset of expert driving demonstrations in Las Vegas within the maximum entropy IRL framework. DriveIRL's benefits include: a simple design due to only learning the trajectory scoring function, a flexible and relatively interpretable feature engineering approach, and strong real-world performance. We validated DriveIRL on the Las Vegas Strip and demonstrated fully autonomous driving in heavy traffic, including scenarios involving cut-ins, abrupt braking by the lead vehicle, and hotel pickup/dropoff zones. Our dataset, a part of nuPlan, has been released to the public to help further research in this area.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160449","ML-based Planning;Inverse Reinforcement Learning;Real-World Deployment;Self-Driving;Autonomous Vehicles;Urban Driving;Learning from Human Driving","Strips;Reinforcement learning;Lead;Reliability engineering;Entropy;Trajectory;Safety","driver information systems;entropy;intelligent transportation systems;learning (artificial intelligence);reinforcement learning;road vehicles;traffic engineering computing","500+ hour real-world dataset;dense traffic;DriveIRL's benefits;expert driving demonstrations;flexible feature engineering approach;Inverse Reinforcement Learning;Las Vegas Strip;low-level controller;maximum entropy IRL framework;published planner;relatively interpretable feature engineering approach;trajectory proposals;trajectory scoring function;trajectory scoring model;urban traffic","","","","30","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Robust Guidance Control of Vertical Landing Aircraft with Deep Reinforcement Learning Algorithm","C. Jia; Q. Gong; J. Zhang; W. Luo; X. Huang","National Key Laboratory of Aerospace Intelligent Control Technology, Beijing Aerospace Automatic Control Institute, Beijing, China; National Key Laboratory of Aerospace Intelligent Control Technology, Beijing Aerospace Automatic Control Institute, Beijing, China; National Key Laboratory of Aerospace Intelligent Control Technology, Beijing Aerospace Automatic Control Institute, Beijing, China; National Key Laboratory of Aerospace Intelligent Control Technology, Beijing Aerospace Automatic Control Institute, Beijing, China; National Key Laboratory of Aerospace Intelligent Control Technology, Beijing Aerospace Automatic Control Institute, Beijing, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","560","565","The working environment of vertical landing aircraft may contain strong uncertain disturbances which may lead to the out of control of aircrafts. To address this problem, we proposed a reinforcement learning based algorithm to design controllers for the trajectory following control of vertical landing aircrafts. The simulations indicated that the algorithm is stable in the training process after the network models are converged and the controller generated by the algorithm is robust with uncertain noisy environments. This algorithm can be utilized as a potential method to control complex systems with unpredictable noisy environment.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727835","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727835","vertical landing aircraft;reinforcement learning;unpredictable noise","Training;Atmospheric modeling;Simulation;Reinforcement learning;Control systems;Trajectory;Noise measurement","aircraft control;large-scale systems;learning (artificial intelligence);robust control","uncertain noisy environments;robust guidance control;vertical landing aircraft;deep reinforcement learning algorithm;strong uncertain disturbances;trajectory following control","","","","14","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Optimal HVAC Control in Shared Office Spaces Based on Deep Reinforcement Learning","S. Qin; L. Yu; D. Yue; C. Shen","College of IoT, NUPT, Nanjing, China; College of Auto. & AI, NUPT, Nanjing, China; College of Auto. & AI, NUPT, Nanjing, China; School of Cyber Sci. and Engi., Xi’an Jiaotong Univ., Xi’an, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","1599","1604","In smart buildings, heating, ventilation, and air conditioning (HVAC) systems consume about 40% of total en-ergy. Although HVAC energy consumption is high, the achieved thermal comfort satisfaction ratio (TCSR) in shared spaces is still low, e.g., about 38%. An important reason for this phenomenon is that temperature set-point can not be properly adjusted according to thermal comfort requirements of all occupants. Therefore, it is necessary to implement the optimal tradeoff between HVAC energy consumption and TCSR by dynamically adjusting temperature set-point. In this paper, we investigate the problem of optimal tradeoff between HVAC energy consumption and TCSR in shared office spaces. To this end, we first formulate a multi-objective HVAC energy optimization problem. Due to the existence of uncertain parameters as well as unknown building thermal dynamics models, it is challenging to solve the formulated problem. To overcome the challenge, we propose an HVAC control algorithm based on multi-objective deep reinforcement learning, which can flexibly adjust temperature set-point according to the preset target TCSR. Moreover, the proposed algorithm does not need to choose appropriate objective weights beforehand and retrain a policy even if the environment is changed. Simulations results show the effectiveness of the proposed algorithm.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727481","National Key Research and Development Program of China; National Natural Science Foundation of China; China Postdoctoral Science Foundation; Nanjing University of Posts and Telecommunications; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727481","smart buildings;HVAC;shared spaces;thermal comfort;multi-objective deep reinforcement learning","Energy consumption;Temperature distribution;HVAC;Smart buildings;Heuristic algorithms;Simulation;Reinforcement learning","air conditioning;building management systems;control engineering computing;deep learning (artificial intelligence);energy conservation;energy consumption;HVAC;optimal control;power engineering computing;reinforcement learning","optimal HVAC control;shared office spaces;air conditioning systems;HVAC energy consumption;TCSR;temperature set-point;thermal comfort requirements;optimal tradeoff;multiobjective HVAC energy optimization problem;unknown building thermal dynamics models;HVAC control algorithm;multiobjective deep reinforcement learning;thermal comfort satisfaction ratio","","","","18","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Confidence-Based Robot Navigation Under Sensor Occlusion with Deep Reinforcement Learning","H. Ryu; M. Yoon; D. Park; S. -E. Yoon","School of Computing, Korea Advanced Institute of Science and Technology, South Korea; School of Computing, Korea Advanced Institute of Science and Technology, South Korea; School of Computing, Korea Advanced Institute of Science and Technology, South Korea; School of Computing, Korea Advanced Institute of Science and Technology, South Korea","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","8231","8237","This paper considers the problem of prolonged occlusions on navigation sensors due to dust, smudges, soils, etc. Such uncontrollable occlusions often cause lower visibility as well as higher uncertainty that require considerably sophisticated behavior. To secure visibility (i.e., confidence about the world), we propose a confidence-based navigation method that encourages the robot to explore the uncertain region around the robot maximizing its local confidence. To effectively extract features from the variable size of sensor occlusions, we adopt a point-cloud based representation network. Our method returns a resilient navigation policy via deep reinforcement learning, autonomously avoiding collisions under sensor occlusions while reaching a goal. We evaluate our method in simulated and real-world environments with either static or dynamic obstacles under various sensor-occlusion scenarios. The experimental result shows that our method outperforms baseline methods under the highly occurring sensor occlusion, and achieves maximum 90% and 80% success rates in the tested static and dynamic environments, respectively.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812090","Institute of Information & communications Technology Planning & Evaluation(grant numbers:IITP-2015-0-00199); National Research Foundation of Korea(grant numbers:NRF-2021R1A4A3032834); MSIT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812090","","Uncertainty;Navigation;Reinforcement learning;Soil;Sensor phenomena and characterization;Robot sensing systems;Feature extraction","collision avoidance;feature extraction;learning (artificial intelligence);mobile robots;navigation;object tracking;path planning;robot vision;sensors","confidence-based robot navigation;deep reinforcement learning;prolonged occlusions;navigation sensors;uncontrollable occlusions;lower visibility;considerably sophisticated behavior;confidence-based navigation method;local confidence;point-cloud based representation network;resilient navigation policy;sensor-occlusion scenarios;highly occurring sensor occlusion","","","","34","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Real World Offline Reinforcement Learning with Realistic Data Source","G. Zhou; L. Ke; S. Srinivasa; A. Gupta; A. Rajeswaran; V. Kumar",Carnegie Mellon University; Work conducted durinu an internship at Meta AI; University of Washington; Carnegie Mellon University; Meta AI; Meta AI,"2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","7176","7183","Offline reinforcement learning (ORL) holds great promise for robot learning due to its ability to learn from arbitrary pre-generated experience. However, current ORL benchmarks are almost entirely in simulation and utilize contrived datasets like replay buffers of online RL agents or sub-optimal trajectories, and thus hold limited relevance for real-world robotics. In this work (Real-ORL), we posit that data collected from safe operations of closely related tasks are more practical data sources for real-world robot learning. Under these settings, we perform an extensive (6500+ trajectories collected over 800+ robot hours and 270+ human labor hour) empirical study evaluating generalization and transfer capabilities of representative ORL methods on four real-world tabletop manipulation tasks. Our study finds that ORL and imitation learning prefer different action spaces, and that ORL algorithms can generalize from leveraging offline heterogeneous data sources and outperform imitation learning. We release our dataset and implementations at URL: https://sites.google.com/view/real-orl.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161474","CHS(grant numbers:2007011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161474","","Uniform resource locators;Soft sensors;Heuristic algorithms;Transfer learning;Reinforcement learning;Multitasking;Manipulators","learning (artificial intelligence);manipulators;reinforcement learning;robot programming","270+ human labor hour;800+ robot hours;arbitrary pre-generated experience;closely related tasks;contrived datasets;current ORL benchmarks;leveraging offline heterogeneous data sources;online RL agents;ORL algorithms;outperform imitation learning;practical data sources;real-world robot learning;real-world robotics;real-world tabletop manipulation tasks;realistic data source;replay buffers;representative ORL methods;safe operations;sub-optimal trajectories;world offline reinforcement learning","","","","55","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Curriculum Reinforcement Learning From Avoiding Collisions to Navigating Among Movable Obstacles in Diverse Environments","H. -C. Wang; S. -C. Huang; P. -J. Huang; K. -L. Wang; Y. -C. Teng; Y. -T. Ko; D. Jeon; I. -C. Wu","National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; Graduate School of Convergence Science and Technology, the Research Institute for Convergence Science, and the Inter-University Semiconductor Research Center, Seoul National University, Seoul, South Korea; National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan","IEEE Robotics and Automation Letters","31 Mar 2023","2023","8","5","2740","2747","Curriculum learning has proven highly effective to speed up training convergence with improved performance in a variety of tasks. Researchers have been studying how a curriculum can be constituted to train reinforcement learning (RL) agents in various application domains. However, discovering curriculum sequencing requires the ranking of sub-tasks or samples in order of difficulty, which is not yet sufficiently studied for robot navigation problems. It is still an open question what navigation strategies can be learned and transferred during multi-stage transfer learning from easy to hard. Furthermore, despite of some attempts of learning real robot manipulation tasks using curriculum, most of existing works are limited to toy or simulated settings rather than realistic scenarios. To address those issues, we first investigated how the model convergence in diverse environments relates to the navigation strategies and difficulty metrics. We found that only some of the environments can be trained from scratch, such as in a relatively open tunnel-like environment that only required wall following. We then carried out a two-stage transfer learning for more difficult environments. We found such approach effective for goal navigation, but failed for more complex tasks where movable obstacles may be on the navigation path. To facilitate more complex policies in the navigation among movable obstacles (NAMO) task, another curriculum with distance and pace functions appropriate to the difficulty of the environment was developed. The proposed scheme was proved effective and the strategies learned were discussed via comprehensive evaluations conducted in simulated and real environments.","2377-3766","","10.1109/LRA.2023.3251193","Taiwan's National Science and Technology Council(grant numbers:111-NU- E-A49-001-NU,111-2634-F-A49-013,111-2623-E-A49-007); Qualcomm through the Taiwan University Research Collaboration Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10057016","Collision avoidance;curriculum learning;deep reinforcement learning;movable obstacles;search and rescue robots","Navigation;Training;Task analysis;Convergence;Manipulators;Measurement;Collision avoidance","collision avoidance;learning (artificial intelligence);manipulators;mobile robots;navigation;path planning;reinforcement learning","application domains;curriculum learning;curriculum reinforcement learning;curriculum sequencing;difficult environments;difficulty metrics;diverse environments relates;goal navigation;model convergence;movable obstacles task;multistage transfer learning;navigating;navigation path;navigation strategies;reinforcement learning agents;relatively open tunnel-like;required wall;robot manipulation tasks;robot navigation problems;sub-tasks;toy;training convergence;two-stage transfer learning","","","","30","IEEE","1 Mar 2023","","","IEEE","IEEE Journals"
"Reinforcement Learning for Outdoor Balloon Navigation: A Successful Controller for an Autonomous Balloon","S. L. Jeger; N. Lawrance; F. Achermann; O. Pang; M. Kovac; R. Y. Siegwart","Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Aerial Robotics Lab, Imperial College London, London, U.K.; Aerial Robotics Lab, Imperial College London, London, U.K.; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland","IEEE Robotics & Automation Magazine","","2023","PP","99","2","14","Autonomous ballooning allows for energy-efficient long-range missions but introduces significant challenges for planning and control algorithms, due to their single degree of actuation: vertical rate control through either buoyancy or vertical thrust. Lateral motion is typically due to the wind; thus, balloon flight is both nonholonomic and often stochastic. Finally, wind is very challenging to sense remotely, and estimates are often available only via low-temporal-and-spatial-frequency predictions from large-scale weather models and direct in situ measurements. In this work, reinforcement learning (RL) is used to generate a control policy for an autonomous balloon navigating between 3D positions in a time- and spatially varying wind field. The agent uses its position and velocity, the relative position of the target, and an estimate of the surrounding wind field to command a target altitude. The wind information contains local measurements and an encoding of global wind predictions from a large-scale numerical weather prediction (NWP) model around the current balloon location. The RL algorithm used in this work, the soft actor–critic (SAC), is trained with a reward favoring paths that reach as close as possible to the target, with minimum time and actuation costs. We evaluate our approach first in simulation and then with a controlled indoor experiment, where we generate an artificial wind field and reach a median distance of 23.4 cm from the target within a volume of 3.5 × 3.5 × 3.5 m over 30 trials. Finally, using a fully autonomous custom designed outdoor prototype capable of controlling altitude, long-range communication, redundant localization, and onboard computation, we validate our approach in a real-world setting. Over six flights, the agent navigates to predefined target positions, with an average target distance error of 360 m after traveling approximately 10 km within a volume of 22 × 22 × 3.2 km.","1558-223X","","10.1109/MRA.2023.3271203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10146448","","Wind;Navigation;Buoyancy;Aerospace electronics;Encoding;Atmospheric modeling;Training","","","","","","","IEEE","8 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Barrier Function-based Safe Reinforcement Learning for Formation Control of Mobile Robots","X. Zhang; Y. Peng; W. Pan; X. Xu; H. Xie","College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; Department of Cognitive Robotics, Delft University of Technology, the Netherlands; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","5532","5538","Distributed model predictive control (DMPC) concerns how to online control multiple robotic systems with constraints effectively. However, the nonlinearity, nonconvexity, and strong interconnections of dynamic system models and constraints can make the real-time and real-world DMPC implementations nontrivial. Reinforcement learning (RL) algorithms are promising for control policy design. However, how to ensure safety in terms of state constraints in RL remains a significant issue. This paper proposes a barrier function-based safe reinforcement learning algorithm for DMPC of nonlinear multi-robot systems under state constraints. The proposed approach is composed of several local learning-based MPC regulators. Each regulator, associated with a local system, learns and deploys the local control policy using a safe reinforcement learning algorithm in a distributed manner, i.e., with state information only among the neighbor agents. As a prominent feature of the proposed algorithm, we present a novel barrier-based policy structure to ensure safety, which has a clear mechanistic interpretation. Both simulated and real-world experiments on the formation control of mobile robots with collision avoidance show the effectiveness of the proposed safe reinforcement learning algorithm for DMPC.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811604","National Natural Science Foundation of China(grant numbers:62003361,61825305,U21A20518); Postdoctoral Science Foundation(grant numbers:47680); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811604","","Regulators;Heuristic algorithms;Reinforcement learning;Prediction algorithms;Formation control;Safety;Mobile robots","collision avoidance;control engineering computing;distributed control;learning (artificial intelligence);mobile robots;multi-robot systems;predictive control;safety","local system;local control policy;novel barrier-based policy structure;formation control;mobile robots;DMPC;distributed model predictive control;control multiple robotic systems;dynamic system models;reinforcement learning algorithms;RL;control policy design;state constraints;barrier function-based safe reinforcement learning algorithm;nonlinear multirobot systems;local learning-based MPC regulators","","","","33","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"RL-Legalizer: Reinforcement Learning-based Cell Priority Optimization in Mixed-Height Standard Cell Legalization","S. -Y. Lee; S. Park; D. Kim; M. Kim; T. P. Le; S. Kang","Dept. of Electrical Engineering, POSTECH, Rep. of Korea; Dept. of Electrical Engineering, POSTECH, Rep. of Korea; Dept. of Electrical Engineering, POSTECH, Rep. of Korea; Dept. of Electrical Engineering, POSTECH, Rep. of Korea; Rep. of Korea, AgileSoDA Company; Dept. of Electrical Engineering, POSTECH, Rep. of Korea","2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)","2 Jun 2023","2023","","","1","6","Cell legalization order has a substantial effect on the quality of modern VLSI designs, which use mixed-height standard cells. In this paper, we propose a deep reinforcement learning framework to optimize cell priority in the legalization phase of various designs. We extract the selected features of movable cells and their surroundings, then embed them into cell-wise deep neural networks. We then determine cell priority and legalize them in order using a pixel-wise search algorithm. The proposed framework uses a policy gradient algorithm and several training techniques, including grid-cell subepisode, data normalization, reduced-dimensional state, and network optimization. We aim to resolve the suboptimality of existing sequential legalization algorithms with respect to displacement and wirelength. On average, our proposed framework achieved 34% lower legalization costs in various benchmarks compared to that of the state-of-the-art legalization algorithm.","1558-1101","979-8-3503-9624-9","10.23919/DATE56975.2023.10136947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10136947","deep reinforcement learning;cell-wise neural network;displacement;wirelength","Training;Deep learning;Neural networks;Voltage;Reinforcement learning;Implants;Very large scale integration","deep learning (artificial intelligence);gradient methods;learning (artificial intelligence);reinforcement learning;VLSI","cell legalization order;cell-wise deep neural networks;deep reinforcement learning framework;grid-cell subepisode;legalization phase;lower legalization costs;mixed-height standard cell legalization;mixed-height standard cells;modern VLSI designs;movable cells;network optimization;pixel-wise search algorithm;policy gradient algorithm;reinforcement learning-based cell priority optimization;RL-legalizer;sequential legalization algorithms;state-of-the-art legalization algorithm;substantial effect","","","","32","","2 Jun 2023","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Environment for Particle Robot Navigation and Object Manipulation","J. Shen; E. Xiao; Y. Liu; C. Feng","Stuyvesant High School, New York, NY, USA; New York University, Brooklyn, NY, USA; New York University, Brooklyn, NY, USA; New York University, Brooklyn, NY, USA","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","6232","6239","Particle robots are novel biologically-inspired robotic systems where locomotion can be achieved collectively and robustly, but not independently. While its control is currently limited to a hand-crafted policy for basic locomotion tasks, such a multi-robot system could be potentially controlled via Deep Reinforcement Learning (DRL) for different tasks more efficiently. However, the particle robot system presents a new set of challenges for DRL differing from existing swarm robotics systems: the low degrees of freedom of each robot and the increased necessity of coordination between robots. We present a 2D particle robot simulator using the OpenAI Gym interface and Pymunk as the physics engine, and introduce new tasks and challenges to research the underexplored applications of DRL in the particle robot system. Moreover, we use Stable-baselines3 to provide a set of benchmarks for the tasks. Current baseline DRL algorithms show signs of achieving the tasks but are yet unable to reach the performance of the hand-crafted policy. Further development of DRL algorithms is necessary in order to accomplish the proposed tasks.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811965","","Training;Three-dimensional displays;Navigation;Robot kinematics;Supervised learning;Swarm robotics;Reinforcement learning","control engineering computing;deep learning (artificial intelligence);manipulators;mobile robots;multi-robot systems;reinforcement learning","particle robot system;hand-crafted policy;particle robot navigation;robotic systems;basic locomotion tasks;multirobot system;swarm robotics systems;2D particle robot simulator;deep reinforcement learning;baseline DRL algorithms;Stable-baselines3;OpenAI Gym interface;Pymunk","","","","28","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"On Asymptotic Stability of Nonlinear Systems with Deep Reinforcement Learning Controllers","G. Manfredi; L. De Cicco; S. Mascolo","Dipartimento di Ingegneria Elettrica e dell’Informazione at Politecnico di Bari, Bari, Italy; Dipartimento di Ingegneria Elettrica e dell’Informazione at Politecnico di Bari, Bari, Italy; Dipartimento di Ingegneria Elettrica e dell’Informazione at Politecnico di Bari, Bari, Italy","2022 30th Mediterranean Conference on Control and Automation (MED)","1 Aug 2022","2022","","","306","311","Controlling systems with learning-based control strategies is attracting the interest of the research community due to the advantages that machine learning offers, such as the possibility of controlling nonlinear systems that would be hard to control with conventional techniques, the possibility of controlling systems whose model is not available and so on. Reinforcement Learning (RL) and Deep Neural Networks (DNN) can be merged to obtain Deep Reinforcement Learning (DRL) control strategies. Yet, such new approaches are implemented only in few real-world applications since classical DRL control policies cannot guarantee asymptotic stability, which is a key requirement to guarantee safety. In this work, we propose a framework that, after extracting the DRL control policy, tries to synthesise a Lyapunov function that certifies the asymptotic stability of the system controlled with such a policy. We also show that our framework paves the way for safety guarantees that are often necessary when deriving a control policy. Results show that Lyapunov functions can be synthesised for the considered benchmark systems, thus ensuring asymptotic stability. Furthermore, the corresponding regions of attraction prove the quality of DRL control policies wrt other state-of-the-art learning-based controls.","2473-3504","978-1-6654-0673-4","10.1109/MED54222.2022.9837155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837155","Lyapunov stability;Regions of attraction;Deep Reinforcement Learning;Actor-Critic","Training;Deep learning;Asymptotic stability;Neural networks;Reinforcement learning;Benchmark testing;Control systems","asymptotic stability;deep learning (artificial intelligence);learning systems;Lyapunov methods;nonlinear control systems","deep neural networks;classical DRL control policies;asymptotic stability;Lyapunov function;nonlinear systems;deep reinforcement learning controllers;learning-based control strategies;machine learning;DNN","","","","17","IEEE","1 Aug 2022","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Decentralized Multi-Agent Control Approach exploiting Cognitive Cooperation on Continuous Environments","G. Camacho-Gonzalez; S. D’Avella; C. A. Avizzano; P. Tripicchio","Department of Excellence in Robotics & AI, Perceptual Robotics Laboratory, Institute of Mechanical Intelligence, Scuola Superiore Sant’Anna; Department of Excellence in Robotics & AI, Perceptual Robotics Laboratory, Institute of Mechanical Intelligence, Scuola Superiore Sant’Anna; Department of Excellence in Robotics & AI, Perceptual Robotics Laboratory, Institute of Mechanical Intelligence, Scuola Superiore Sant’Anna; Department of Excellence in Robotics & AI, Perceptual Robotics Laboratory, Institute of Mechanical Intelligence, Scuola Superiore Sant’Anna","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","1557","1562","Multi-agent system control is a research topic that has broad applications ranging from multi-robot cooperation to distributed sensor networks. Reinforcement learning is shown to be promising as a control strategy in cases where the dynamics of the agents are non-linear, complex, and highly uncertain since it can learn policies from samples without using much model information. The presented manuscript proposes a multi-agent decentralized control approach based on a new multi-agent reinforcement learning setting in which two virtual agents, sharing the same environment, control a single avatar but have access to complementary details necessary to finish the task. Each of them is responsible for solving a portion of the problem, and in order to efficiently solve it, a collaboration should emerge among the virtual agents not to compete but to focus on the final goal. Each virtual agent, performing individually, is not fully autonomous since it does not have a complete vision of the scene and needs the other one to properly command the avatar. The proposed approach proved to be able to solve efficiently constrained navigation problems in two different simulated setups. An actor-critic architecture with a Proximal Policy Optimization (PPO) algorithm has been employed in continuous action and state spaces. The training and the testing have been done in a maze-like environment designed using the StarCraft II Learning Environment.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926587","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926587","","Training;Navigation;Avatars;Reinforcement learning;Aerospace electronics;Robot sensing systems;Task analysis","computer games;decentralised control;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems","continuous environments;multiagent system control;multirobot cooperation;distributed sensor networks;control strategy;multiagent decentralized control approach;multiagent reinforcement;virtual agent;StarCraft II Learning Environment;reinforcement Learning decentralized multiagent control approach;cognitive cooperation","","","","36","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"CSGP: Closed-Loop Safe Grasp Planning via Attention-Based Deep Reinforcement Learning From Demonstrations","Z. Tang; Y. Shi; X. Xu","College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China","IEEE Robotics and Automation Letters","17 Apr 2023","2023","8","6","3158","3165","Grasping is at the core of many robotic manipulation tasks. Despite the recent progress, closed-loop grasp planning in stacked scenes is still unsatisfactory, in terms of efficiency, stability, and most importantly, safety. In this letter, we present CSGP, a closed-loop safe grasp planning approach via attention-based deep reinforcement learning (DRL) from demonstrations, which is capable of learning safe grasping policies that make surrounding objects less disturbed or damaged during manipulation. In CSGP, a 6-DoF safe grasping policy network with a Next-Best-Region attention module is proposed to intrinsically identify the safe regions in the view, facilitating the learning of safe grasping actions. Moreover, we design a fully automatic pipeline in the simulator to collect safe grasping demonstrations, which are utilized to pre-train the policy with behavior cloning and fine-tune it with DRL. To effectively and stably improve the policy during fine-tuning, a DRL from demonstrations algorithm named Safe-DDPGfD is presented in CSGP with a truncated height-anneal exploration mechanism for safe exploration. Moreover, we provide a benchmark that contains scenes with multiple levels of stack layers for method evaluation. Simulation results demonstrate the state-of-the-art performance of our method, achieving the Overall score of 88% in our benchmark. Also, real-world robot grasping experiments also show the effectiveness of our method.","2377-3766","","10.1109/LRA.2023.3253023","National Natural Science Foundation of China(grant numbers:61825305,62002379); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10059127","Deep learning in grasping and manipulation;deep learning for visual perception;learning from demonstration","Grasping;Planning;6-DOF;Pipelines;Network architecture;Training;Reinforcement learning","control engineering computing;deep learning (artificial intelligence);dexterous manipulators;grippers;learning (artificial intelligence);manipulators;reinforcement learning;robot vision","6-DoF safe grasping policy network;attention-based deep reinforcement learning;closed-loop safe grasp planning approach;CSGP;DRL;learning safe grasping policies;named Safe-DDPGfD;Next-Best-Region attention module;real-world robot grasping experiments;robotic manipulation tasks;safe exploration;safe grasping actions;safe grasping demonstrations;safe regions;stacked scenes","","","","28","IEEE","6 Mar 2023","","","IEEE","IEEE Journals"
"FNPG-NH: A Reinforcement Learning Framework for Flexible Needle Path Generation With Nonholonomic Constraints","M. Shah; N. Patel","Department of Engineering Design, Indian Institute of Technology Madras, Chennai, India; Department of Engineering Design, Indian Institute of Technology Madras, Chennai, India","IEEE Robotics and Automation Letters","9 Aug 2023","2023","8","9","5854","5861","Path planning algorithms for minimally invasive neurosurgery involve avoiding critical structures such as blood vessels and ventricles while following needle kinematics. The majority of planning solutions proposed in the literature use sampling-based algorithms. This letter introduces a Flexible Needle Path Generation framework with Non-Holonomic constraints (FNPG-NH), an extension of our FNPG framework. FNPG-NH uses deep Reinforcement Learning (RL) based methods such as Deep Deterministic Policy Gradient (DDPG), Proximal Policy Optimization (PPO), and Soft Actor-Critic (SAC) to obtain a kinematically feasible path for a bevel-tipped flexible needle using a nonholonomic model. RL algorithms presented in this work generate the control input for needle rotation based on the rewards generated by the environment. The deep RL algorithms are trained on an environment that consists of 1) ventricles segmented from T1 images of the healthy volunteers using atlas-based segmentation, 2) blood vessels segmented from MRA volumes of the same volunteer using thresholding, and 3) tumor volume from labeled BraTS 2020 dataset and placed at an anatomically relevant location. The paths generated by the reinforcement learning algorithm and the traditional sampling-based algorithm RRT are compared for various performance metrics. The reinforcement learning model was trained on 20 volumes and validated on 68 volumes, and RRT was evaluated on the same 68 validation volumes. The results show that the trajectories generated by the FNPG-NH framework are safer, shorter, and take less time than RRT while avoiding critical structures such as ventricles and blood vessels.","2377-3766","","10.1109/LRA.2023.3300576","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10198298","Surgical robotics: planning;steerable catheters/needles;reinforcement learning","Needles;Planning;Tumors;Trajectory;Neurosurgery;Minimally invasive surgery;Reinforcement learning","biomedical MRI;blood vessels;brain;deep learning (artificial intelligence);gradient methods;image segmentation;medical image processing;mobile robots;needles;path planning;reinforcement learning;sampling methods;surgery;trajectory control;tumours","atlas-based segmentation;bevel-tipped flexible needle;blood vessels;critical structures;deep deterministic policy gradient;deep reinforcement learning-based methods;deep RL algorithms;flexible needle path generation framework;FNPG framework;FNPG-NH framework;kinematically feasible path;minimally invasive neurosurgery;needle kinematics;needle rotation;nonholonomic constraints;nonholonomic model;path planning algorithms;proximal policy optimization;reinforcement learning algorithm;reinforcement learning model;soft actor-critic;traditional sampling-based;ventricles;volunteer using thresholding","","","","32","IEEE","1 Aug 2023","","","IEEE","IEEE Journals"
"Fuzzy Action-Masked Reinforcement Learning Behavior Planning for Highly Automated Driving","T. Rudolf; M. Gao; T. Schürmann; S. Schwab; S. Hohmann","FZI Research Center for Information Technology, Karlsruhe, Germany; FZI Research Center for Information Technology, Karlsruhe, Germany; FZI Research Center for Information Technology, Karlsruhe, Germany; FZI Research Center for Information Technology, Karlsruhe, Germany; Institute of Control Systems Karlsruhe Institute of Technology, Karlsruhe, Germany","2022 8th International Conference on Control, Automation and Robotics (ICCAR)","31 May 2022","2022","","","264","270","Highly-automated driving relies on informed decision making and safe control in a dynamic environment which is regulated by traffic rules. This complex environment can be formulated as a map-based problem where the driving task is constrained by kinematics, obstacles, traffic, and road structure. Though, solving the problem with a monolithic approach additionally can lead to infeasible conditions which are difficult to bypass. Therefore, we expect a separate layer for sequential decision making to be a more suitable approach to derive a lateral and longitudinal behavior. In this area of decision making, reinforcement learning (RL) is a viable approach to complex and nonlinear problems. However, the learning convergence to a reasonable behavior in a critical and constrained environment can be slow. To overcome this challenge, we propose an action-masked RL agent utilizing fuzzy traffic rule descriptions. The agent outputs are hybrid lateral and longitudinal actions based on the environment observations. These driving decisions interface the subsequent trajectory planner and control for optimal execution. In exemplary merging scenarios, we show the effectiveness of the masked agent to increase convergence speed toward a reasonable behavior.","2251-2454","978-1-6654-8116-8","10.1109/ICCAR55106.2022.9782671","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782671","behavior planning;reinforcement learning;fuzzy logic;action masking;highly automated driving","Roads;Decision making;Merging;Reinforcement learning;Behavioral sciences;Trajectory;Planning","decision making;fuzzy set theory;intelligent transportation systems;mobile robots;multi-agent systems;multi-robot systems;reinforcement learning;road traffic control;traffic engineering computing","lateral actions;longitudinal actions;environment observations;driving decisions;masked agent;highly automated driving;fuzzy action-masked reinforcement learning behavior planning;dynamic environment;traffic rules;map-based problem;road structure;sequential decision making;lateral behavior;longitudinal behavior;action-masked RL agent utilizing fuzzy traffic rule descriptions","","","","17","IEEE","31 May 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning based Planning for Urban Self-driving with Demonstration and Depth Completion","C. Wang; N. Aouf","City, University of London; City, University of London","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","962","967","Research shows major interests in urban self-driving in recent years, both perception and motion planning considered to be significant topics. Current techniques of decision making for driving policy are modular and hand designed, which is expensive and inefficient. With the development of machine learning, learning-based approaches have become a mainstream research direction. However, the performance in urban driving scenarios is far from satisfaction due to the brittle convergence property of deep reinforcement learning and debased observation. To solve these problems, this paper proposed a learning-based method with deep reinforcement learning (DRL) and imitation learning (IL), and additionally a novel depth completion model for better perception. Our framework is built upon Soft Actor-Critic algorithm and introducing an update method that value function, Q-function and policy network all learn from the expert data. To tackle the observation problem, we proposed a reconstruction restraint deep fusion depth completion network which can predict the integrated and precise depth map of the environment with our own novel pre-processed datasets. In experiment, our autonomous driving agent transfer smooth from IL to DRL in training, and outperformed state-of-art methods in urban challenging scenes and still competing compared to our model with groundtruth input.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9650055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650055","Autonomous Driving;Depth Completion;Deep Reinforcement Learning;Convolutional Neural Networks","Training;Learning systems;Neural networks;Decision making;Reinforcement learning;Prediction algorithms;Control systems","decision making;learning (artificial intelligence);path planning;traffic engineering computing","deep reinforcement learning;urban self-driving;motion planning;driving policy;machine learning;learning-based approaches;mainstream research direction;urban driving scenarios;learning-based method;imitation learning;novel depth completion model;policy network;reconstruction restraint deep fusion depth completion network;integrated depth map;precise depth map;autonomous driving agent;urban challenging scenes","","","","31","","28 Dec 2021","","","IEEE","IEEE Conferences"
"BERRY: Bit Error Robustness for Energy-Efficient Reinforcement Learning-Based Autonomous Systems","Z. Wan; N. Chandramoorthy; K. Swaminathan; P. -Y. Chen; V. J. Reddi; A. Raychowdhury",Georgia Institute of Technology; IBM Research; IBM Research; IBM Research; Harvard University; Georgia Institute of Technology,"2023 60th ACM/IEEE Design Automation Conference (DAC)","15 Sep 2023","2023","","","1","6","Autonomous systems, such as Unmanned Aerial Vehicles (UAVs), are expected to run complex reinforcement learning (RL) models to execute fully autonomous position-navigation-time tasks within stringent onboard weight and power constraints. We observe that reducing onboard operating voltage can benefit the energy efficiency of both the computation and flight mission, however, it can also result in on-chip bit failures that are detrimental to mission safety and performance. To this end, we propose BERRY, a robust learning framework to improve bit error robustness and energy efficiency for RL-enabled autonomous systems. BERRY supports robust learning, both offline and on-board the UAV, and for the first time, demonstrates the practicality of robust low-voltage operation on UAVs that leads to high energy savings in both compute-level operation and system-level quality-of-flight. We perform extensive experiments on 72 autonomous navigation scenarios and demonstrate that BERRY generalizes well across environments, UAVs, autonomy policies, operating voltages and fault patterns, and consistently improves robustness, efficiency and mission performance, achieving up to 15.62% reduction in flight energy, 18.51% increase in the number of successful missions, and 3.43× processing energy reduction.","","979-8-3503-2348-1","10.1109/DAC56929.2023.10247999","Semiconductor Research Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247999","","Low voltage;Systematics;Voltage;Reinforcement learning;Autonomous aerial vehicles;Robustness;Energy efficiency","autonomous aerial vehicles;learning (artificial intelligence);mobile robots;reinforcement learning","72 autonomous navigation scenarios;BERRY;bit error robustness;complex reinforcement learning models;compute-level operation;energy efficiency;energy reduction;energy-efficient reinforcement learning-based autonomous systems;flight energy;flight mission;fully autonomous position-navigation-time tasks;high energy savings;low-voltage operation;mission performance;mission safety;on-chip bit failures;onboard operating voltage;operating voltages;power constraints;RL-enabled autonomous systems;robust learning framework;stringent onboard weight;system-level quality-of-flight;UAV;Unmanned Aerial Vehicles","","","","21","IEEE","15 Sep 2023","","","IEEE","IEEE Conferences"
"Parallel Reinforcement Learning Simulation for Visual Quadrotor Navigation","J. Saunders; S. Saeedi; W. Lil","Department of Computer Science, University of Bath, UK; Department of Mechanical and Industrial Engineering, Toronto Metropolitan University, Toronto, Canada; Department of Computer Science, University of Bath, UK","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","1357","1363","Reinforcement learning (RL) is an agent-based approach for teaching robots to navigate within the physical world. Gathering data for RL is known to be a laborious task, and real-world experiments can be risky. Simulators facilitate the collection of training data in a quicker and more cost-effective manner. However, RL frequently requires a significant number of simulation steps for an agent to become skilful at simple tasks. This is a prevalent issue within the field of RL-based visual quadrotor navigation where state dimensions are typically very large and dynamic models are complex. Furthermore, rendering images and obtaining physical properties of the agent can be computationally expensive. To solve this, we present a simulation framework, built on AirSim, which provides efficient parallel training. Building on this framework, Ape-X is modified to incorporate parallel training of AirSim environments to make use of numerous networked computers. Through experiments we were able to achieve a reduction in training time from 3.9 hours to 11 minutes, for a toy problem, using the aforementioned framework and a total of 74 agents and two networked computers. Further details including a github repo and videos about our project, PRL4AirSim, can be found at https://sites.google.com/view/prl4airsim/home","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160675","","Training;Computers;Visualization;Navigation;Computational modeling;Atmospheric modeling;Reinforcement learning","aerospace computing;aerospace robotics;control engineering computing;helicopters;navigation;parallel processing;reinforcement learning;robot vision;software agents","agent-based approach;AirSim environments;Ape-X;dynamic models;efficient parallel training;networked computers;parallel reinforcement learning simulation;parallel training;physical properties;physical world;rendering images;RL-based visual quadrotor navigation;simulation framework;simulation steps;state dimensions;time 3.9 hour to 11 min;training data collection;training time","","","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Deep reinforcement learning based green wave speed guidance for human-driven connected vehicles at signalized intersections","S. Yuan; S. Xu; S. Zheng","Research Institute of China Telecom Co., Ltd., Guangzhou, China; Research Institute of China Telecom Co., Ltd., Guangzhou, China; Research Institute of China Telecom Co., Ltd., Guangzhou, China","2022 14th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA)","7 Mar 2022","2022","","","331","339","Green wave speed guidance is able to reduce the vehicle delay caused by unnecessary starting and braking at signalized intersections and ease traffic congestion, especially with the development of vehicle-to-infrastructure communications. Several methods have been proposed for connected vehicles in simplified environment by frequent control, which cannot be applied in real traffic situations. In this paper, we propose a Deep Q-Network (DQN) based method to learn to select an efficiency-optimal speed change mode for human-driven connected vehicles passing through the multi-lane and multi-intersection road network mixed with non-connected vehicles and random pedestrians. In order to avoid frequent speed fluctuations, we convert the continuous speed change into a smooth speed profile control by introducing vehicle dynamics model to transform the action space of DQN model into a series of control commands of the controller. By specifying an effective reward function to minimize the travel time in this task, the proposed method can achieve an effective mapping between the state representation and the corresponding ideal action. Experimental results on single-vehicle guidance and multi-vehicles guidance scenarios show that the proposed method outperforms two benchmark methods, obtains higher improvement in several travel efficiency metrics and more adaptive performance for dynamic traffic situations.","2157-1481","978-1-6654-9978-1","10.1109/ICMTMA54903.2022.00070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9723880","deep reinforcement learning;green wave speed guidance;human-driven connected vehicle;mixed traffic flow;signalized intersection;vehicle-to-infrastructure technology","Space vehicles;Connected vehicles;Navigation;Roads;Green products;Reinforcement learning;Benchmark testing","deep learning (artificial intelligence);optimal control;road traffic control;road vehicles;traffic engineering computing;vehicle dynamics;velocity control","signalized intersections;green wave speed guidance;vehicle delay;traffic congestion;vehicle-to-infrastructure communications;frequent control;deep Q-network;efficiency-optimal speed change mode;human-driven connected vehicles;nonconnected vehicles;frequent speed fluctuations;continuous speed change;smooth speed profile control;vehicle dynamics model;single-vehicle guidance;multivehicles;dynamic traffic situations;deep reinforcement learning;DQN model","","","","24","IEEE","7 Mar 2022","","","IEEE","IEEE Conferences"
"Autonomous Blimp Control via $H_{\infty}$ Robust Deep Residual Reinforcement Learning","Y. Zuo; Y. T. Liu; A. Ahmad","Institute of Flight Mechanics and Controls, University of Stuttgart, Stuttgart, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","8","Due to their superior energy efficiency, blimps may replace quadcopters for long-duration aerial tasks. However, designing a controller for blimps to handle complex dynamics, modeling errors, and disturbances remains an unsolved challenge. One recent work combines reinforcement learning (RL) and a PID controller to address this challenge and demonstrates its effectiveness in real-world experiments. In the current work, we build on that using an $H_{\infty}$ robust controller to expand the stability margin and improve the RL agent's performance. Empirical analysis of different mixing methods reveals that the resulting $\mathrm{H}_{\infty}$. RL controller outperforms the prior PID-RL combination and can handle more complex tasks involving intensive thrust vectoring. We provide our code as open-source at https://github.com/robot-perception-group/robust_deep_residual_blimp.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260561","","Computer aided software engineering;Codes;Reinforcement learning;Buoyancy;Stability analysis;Robustness;Energy efficiency","autonomous aerial vehicles;control engineering computing;helicopters;learning (artificial intelligence);mobile robots;reinforcement learning;robust control;three-term control","$robust deep residual reinforcement learning;\infty;autonomous blimp control via$H;blimps;complex dynamics;different mixing methods;long-duration aerial tasks;modeling errors;PID controller;prior PID-RL combination;quadcopters;real-world experiments;RL agent;RL controller;robust controller;stability margin;superior energy efficiency;unsolved challenge","","","","22","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Decentralized Multi-Robot Formation Control Using Reinforcement Learning","J. Obradović; M. Križmančić; S. Bogdan","University of Zagreb, Faculty of Electrical Engineering and Computing, Zagreb, Croatia; University of Zagreb, Faculty of Electrical Engineering and Computing, Zagreb, Croatia; University of Zagreb, Faculty of Electrical Engineering and Computing, Zagreb, Croatia","2023 XXIX International Conference on Information, Communication and Automation Technologies (ICAT)","11 Jul 2023","2023","","","1","7","This paper presents a decentralized leader-follower multi-robot formation control based on a reinforcement learning (RL) algorithm applied to a swarm of small educational Sphero robots. Since the basic Q-learning method is known to require large memory resources for Q-tables, this work implements the Double Deep Q-Network (DDQN) algorithm, which has achieved excellent results in many robotic problems. To enhance the system behavior, we trained two different DDQN models, one for reaching the formation and the other for maintaining it. The models use a discrete set of robot motions (actions) to adapt the continuous nonlinear system to the discrete nature of RL. The presented approach has been tested in simulation and real experiments which show that the multi-robot system can achieve and maintain a stable formation without the need for complex mathematical models and nonlinear control laws.","2643-1858","979-8-3503-9983-7","10.1109/ICAT57854.2023.10171272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10171272","reinforcement learning;double deep Q-networks;leader-follower;formation control;multi-robot system","Robot motion;Adaptation models;Q-learning;Mathematical models;Formation control;Stability analysis;Multi-robot systems","decentralised control;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;multi-robot systems;nonlinear control systems;reinforcement learning","basic Q-learning method;continuous nonlinear system;decentralized leader-follower multirobot formation control;decentralized multirobot formation control;different DDQN models;Double Deep Q-Network algorithm;educational Sphero robots;multirobot system;nonlinear control laws;reinforcement learning algorithm;robot motions;robotic problems;stable formation;system behavior","","","","29","IEEE","11 Jul 2023","","","IEEE","IEEE Conferences"
"Life Guard: A Reinforcement Learning-Based Task Mapping Strategy for Performance-Centric Aging Management","V. Rathore; V. Chaturvedi; A. K. Singh; T. Srikanthan; M. Shafique","School of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore; Department of Computer Science and Engineering, Indian Institute of Technology Palakkad, India; School of Computer Science and Electronic Engineering, University of Essex, UK; Institute of Computer Engineering, Technische Universität Wien (TU Wien), Austria; School of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore","2019 56th ACM/IEEE Design Automation Conference (DAC)","22 Aug 2019","2019","","","1","6","Device scaling to subdeca nanometer has pushed device aging as a primary design concern. In manycore systems, inevitable process variation further adds to delay degradation and, coupled with the scalability issues in manycores, makes aging management, while meeting performance demands, a complex problem. Life-Guard is a performance-centric reinforcement learning-based task mapping strategy that leverages the different impact of applications on aging for improving system health. Experimental results, comparing LifeGuard with two state-of-the-art aging optimizing techniques, on a 256-core system, showed that LifeGuard led to improved health for, respectively, 57% and 74% of the cores, and also an enhanced aggregate core frequency.","0738-100X","978-1-4503-6725-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8806976","aging;negative-bias temperature instability (NBTI);mapping;many-core systems;reinforcement learning","Aging;Task analysis;Aggregates;Negative bias temperature instability;Thermal variables control;Scalability;Multicore processing","learning (artificial intelligence);microprocessor chips;multiprocessing systems;network-on-chip;optimisation;power aware computing","performance-centric aging management;manycore systems;scalability issues;manycores;performance demands;performance-centric reinforcement learning-based task mapping strategy;system health;LifeGuard;aging optimizing techniques","","","","31","","22 Aug 2019","","","IEEE","IEEE Conferences"
"Barrier Lyapunov Function-Based Safe Reinforcement Learning Algorithm for Autonomous Vehicles with System Uncertainty","Y. Zhang; X. Liang; S. S. Ge; B. Gao; T. H. Lee","State Key Laboratory of Automotive Simulation and Control, Jilin University, Changchun, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Clean Energy Automotive Engineering Center, Tongji University, Shanghai, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","1592","1598","Guaranteed safety and performance under various circumstances remain technically critical and practically challenging for the wide deployment of autonomous vehicles. For such safety-critical systems, it will certainly be a requirement that safe performance should be ensured even during the reinforcement learning period in the presence of system uncertainty. To address this issue, a Barrier Lyapunov Function-based safe reinforcement learning algorithm (BLF-SRL) is proposed here for the formulated nonlinear system in strict-feedback form. This approach appropriately arranges the Barrier Lyapunov Function item into the optimized backstepping control method to constrain the state-variables in the designed safety region during learning when unknown bounded system uncertainty exists. More specifically, the overall system control is optimized with the optimized backstepping technique under the framework of Actor-Critic, which optimizes the virtual control in every backstepping subsystem. Wherein, the optimal virtual control is decomposed into Barrier Lyapunov Function items; and also with an adaptive item to be learned with deep neural networks, which achieves safe exploration during the learning process. Eventually, the principle of Bellman optimality is satisfied through iteratively updating the independently approximated actor and critic to solve the Hamilton-Jacobi-Bellman equation in adaptive dynamic programming. More notably, the variance of control performance under uncertainty is also reduced with the proposed method. The effectiveness of the proposed method is verified with motion control problems for autonomous vehicles through appropriate comparison simulations.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649902","China Scholarship Council(grant numbers:202006170145); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649902","Safe Reinforcement Learning;Adaptive Dynamic Programming;Barrier Lyapunov Function;Autonomous Vehicles","Uncertainty;Backstepping;Process control;Reinforcement learning;Control systems;Mathematical models;Safety","control engineering computing;control nonlinearities;control system synthesis;deep learning (artificial intelligence);dynamic programming;feedback;iterative methods;Lyapunov methods;mobile robots;motion control;nonlinear control systems;optimal control;partial differential equations;reinforcement learning;remotely operated vehicles;uncertain systems","optimal virtual control;autonomous vehicles;system uncertainty;safety-critical systems;backstepping control;barrier Lyapunov function-based safe reinforcement learning;nonlinear system;BLF-SRL;Bellman optimality;Hamilton-Jacobi-Bellman equation;dynamic programming;strict-feedback form;deep neural networks;motion control","","","","22","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Control of a Reconfigurable Planar Cable Driven Parallel Manipulator","A. Raman; A. Salvi; M. Schmid; V. Krovi","Department of Automotive Engineering, Clemson University International Center for Automotive Research (CU-ICAR), Greenville, SC; Department of Automotive Engineering, Clemson University International Center for Automotive Research (CU-ICAR), Greenville, SC; Department of Automotive Engineering, Clemson University International Center for Automotive Research (CU-ICAR), Greenville, SC; Department of Automotive Engineering, Clemson University International Center for Automotive Research (CU-ICAR), Greenville, SC","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","9644","9650","Cable driven parallel robots (CDPRs) are often challenging to model and to dynamically control due to the inherent flexibility and elasticity of the cables. The additional inclusion of online geometric reconfigurability to a CDPR results in a complex underdetermined system with highly non-linear dynamics. The necessary (numerical) redundancy resolution requires multiple layers of optimization rendering its application computationally prohibitive for real-time control. Here, deep reinforcement learning approaches can offer a model-free framework to overcome these challenges and can provide a real-time capable dynamic control. This study discusses three settings for a model-free DRL implementation in dynamic trajectory tracking: (i) for a standard non-redundant CDPR with a fixed workspace; (ii) in an end-to-end setting with redundancy resolution on a reconfigurable CDPR; and (iii) in a decoupled approach resolving kinematic and actuation redundancies individually.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160498","","Training;Parallel robots;Trajectory tracking;Computational modeling;Redundancy;Reinforcement learning;Real-time systems","cables (mechanical);control engineering computing;deep learning (artificial intelligence);end effectors;manipulator dynamics;manipulator kinematics;optimisation;reinforcement learning;trajectory control","actuation redundancies;cable driven parallel robots;cable elasticity;cable flexibility;complex underdetermined system;deep reinforcement learning;dynamic trajectory tracking;kinematic redundancies;model-free DRL implementation;nonlinear dynamics;online geometric reconfigurability;optimization;real-time capable dynamic control;reconfigurable CDPR;reconfigurable planar cable driven parallel manipulator;redundancy resolution;reinforcement learning control;standard nonredundant CDPR","","","","20","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Visual Servoing Control Strategy for Target Tracking Using a Multirotor UAV","A. Mitakidis; S. N. Aspragkathos; F. Panetsos; G. C. Karras; K. J. Kyriakopoulos","Control Systems Lab, School of Mechanical Engineering National Technical University of Athens, Zografou, Athens, Greece; Control Systems Lab, School of Mechanical Engineering National Technical University of Athens, Zografou, Athens, Greece; Control Systems Lab, School of Mechanical Engineering National Technical University of Athens, Zografou, Athens, Greece; Dept. of Informatics and Telecommunications, University of Thessaly, Lamia, Greece; Canter for Artificial Intelligence and Robotics (CAIR), New York University - Abu Dhabi, Abu Dhabi, United Arab Emirates","2023 9th International Conference on Automation, Robotics and Applications (ICARA)","23 May 2023","2023","","","219","224","In this work, a deep Reinforcement Learning control scheme is developed in order to execute autonomous tracking of an unmanned ground vehicle (UGV) with a multirotor unmanned aerial vehicle (UAV). The UAV is equipped with a downward looking camera and the detection of the target UGV is achieved through a convolutional neural network (CNN). The deep RL control policy is deployed as a cascade position controller, which commands the inner attitude controller of the autopilot, and achieves the following of the UGV despite the aggressive maneuvers of the target. The efficacy of the proposed framework is demonstrated via a set of outdoor experiments using an octocopter flying above the ground vehicle.","2767-7745","978-1-6654-8921-8","10.1109/ICARA56516.2023.10125971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10125971","target tracking;visual servoing;reinforcement learning","Deep learning;Target tracking;Attitude control;Reinforcement learning;Autonomous aerial vehicles;Cameras;Land vehicles","attitude control;autonomous aerial vehicles;cameras;convolutional neural nets;deep learning (artificial intelligence);helicopters;mobile robots;position control;reinforcement learning;remotely operated vehicles;robot vision;target tracking;visual servoing","autonomous tracking;cascade position controller;control scheme;convolutional neural network;deep Reinforcement;deep RL control policy;downward looking camera;inner attitude controller;multirotor UAV;multirotor unmanned aerial vehicle;target tracking;target UGV;unmanned ground vehicle;visual servoing control strategy","","","","23","IEEE","23 May 2023","","","IEEE","IEEE Conferences"
"Efficient Virtual Power Plant Dispatch Model for Distributed Energy Resources Using Deep Reinforcement Learning","S. Zhang; F. Ye; J. Zhou; Q. Xiao; M. Zhang; H. Wang; Y. Ye","State Grid Chongqing Electric Power Company, Economic and Technological Research Institute, Chongqing, China; State Grid Chongqing Electric Power Company, Economic and Technological Research Institute, Chongqing, China; State Grid Chongqing Electric Power Company, Economic and Technological Research Institute, Chongqing, China; State Grid Chongqing Electric Power Company, Economic and Technological Research Institute, Chongqing, China; State Grid Chongqing Electric Power Company, Economic and Technological Research Institute, Chongqing, China; State Grid Chongqing Electric Power Company, Economic and Technological Research Institute, Chongqing, China; School of Electrical Engineering, Southeast University, Nanjing, China","2023 IEEE 13th International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","28 Sep 2023","2023","","","510","514","In the summer of 2022, the southwestern region of China experienced the longest-lasting extreme heat climate since 1961. During this period, hydropower, which serves as a significant energy source, suffered from a severe water shortage, while residents faced a sharp increase in cooling demand, resulting in a significant power shortage. As a modern load aggregation and regulation technology, virtual power plants (VPP) can effectively utilize distributed photovoltaic resources and implement demand response strategies to adjust loads. This paper proposes a VPP scheduling method based on the deep deterministic policy gradient (DDPG) reinforcement learning algorithm and conducts a case study based on the real data from City X in southwestern China. Test results indicate that the proposed DDPG-based VPP scheduling method in this paper has higher training efficiency compared to other similar algorithms, and it can achieve the intelligent control of air conditioners through Smart Home device. The comprehensive cost under this dispatch method has only a 2.03% gap with the theoretical optimum.","2642-6633","979-8-3503-1519-6","10.1109/CYBER59472.2023.10256633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256633","","Training;Photovoltaic systems;Urban areas;Reinforcement learning;Smart homes;Virtual power plants;Scheduling","deep learning (artificial intelligence);demand side management;gradient methods;intelligent control;learning (artificial intelligence);photovoltaic power systems;power engineering computing;power generation dispatch;power generation economics;power generation scheduling;reinforcement learning;virtual power plants","conducts;cooling demand;DDPG-based VPP scheduling method;deep reinforcement learning;demand response strategies;dispatch method;distributed energy resources;efficient virtual power plant dispatch model;extreme heat climate;higher training efficiency;modern load aggregation;photovoltaic resources;regulation technology;severe water shortage;significant energy source;significant power shortage;southwestern China;virtual power plants","","","","20","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Autonomous Multi-View Navigation via Deep Reinforcement Learning","X. Huang; W. Chen; W. Zhang; R. Song; J. Cheng; Y. Li","School of Control Science and Engineering, Shandong University, China; School of Control Science and Engineering, Shandong University, China; School of Control Science and Engineering, Shandong University, China; School of Control Science and Engineering, Shandong University, China; School of Control Science and Engineering, Shandong University, China; School of Control Science and Engineering, Shandong University, China","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","13798","13804","In this paper, we propose a novel deep reinforcement learning (DRL) system for the autonomous navigation of mobile robots that consists of three modules: map navigation, multi-view perception and multi-branch control. Our DRL system takes as the input a routed map provided by a global planner and three RGB images captured by a multi-camera setup to gather global and local information, respectively. In particular, we present a multi-view perception module based on an attention mechanism to filter out redundant information caused by multi-camera sensing. We also replace raw RGB images with low-dimensional representations via a specifically designed network, which benefits a more robust sim2real transfer learning. Extensive experiments in both simulated and real-world scenarios demonstrate that our system outperforms state-of-the-art approaches.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561631","Research and Development; National Natural Science Foundation of China; Natural Science Foundation of Shandong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561631","","Navigation;Transfer learning;Reinforcement learning;Robot sensing systems;Information filters;Robustness;Path planning","cameras;control engineering computing;image colour analysis;image sensors;mobile robots;path planning;reinforcement learning","RGB images;robust sim2real transfer learning;autonomous multiview navigation;deep reinforcement learning system;autonomous navigation;mobile robots;map navigation;multibranch control;multicamera setup;multiview perception module;DRL","","","","35","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning With Energy-Exchange Dynamics for Spring-Loaded Biped Robot Walking","C. -Y. Kuo; H. Shin; T. Matsubara","Graduate School of Science and Technology, Nara Institute of Science and Technology, Nara, Japan; Honda R&D, Ltd., Saitama, Japan; Graduate School of Science and Technology, Nara Institute of Science and Technology, Nara, Japan","IEEE Robotics and Automation Letters","21 Aug 2023","2023","8","10","6243","6250","This letter presents a probabilistic Model-based Reinforcement Learning (MBRL) approach for learning the Energy-exchange Dynamics (EED) of a spring-loaded biped robot. Our approach enables on-site walking acquisition with high sample efficiency, real-time planning capability, and generalizability across skill conditions. Specifically, we learn the data-driven state transition dynamics of the robot in the formulation of energy-states, with their interaction characterized as energy-exchange to reduce dimensionality. To improve planning reliability with the learned EED, we design a control space based on a walking trajectory that follows the law of conservation of energy and is formulated by energy-states. We evaluated our approach using a four-degree-of-freedom spring-loaded biped robot in simulation and hardware, and generalizability is validated by using the same learning framework for different walking speeds and terrains in simulation and walking acquisition with hardware. All results showed successful on-site walking acquisition with a compact nine-dimension dynamics model, 40 Hz real-time planning, and on-site learning within a few minutes.","2377-3766","","10.1109/LRA.2023.3303786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10214113","Humanoid and bipedal locomotion;model learning for control;reinforcement learning","Legged locomotion;Robots;Planning;Actuators;Trajectory;Task analysis;Training","data preparation;legged locomotion;path planning;probability;reinforcement learning;robot dynamics;springs (mechanical);trajectory control","data-driven state transition dynamics;dimensionality reduction;EED learning;energy-exchange dynamics;energy-states;four-degree-of-freedom spring-loaded biped robot walking;MBRL;nine-dimension dynamics model;on-site learning;on-site walking acquisition;planning reliability;probabilistic model-based reinforcement learning approach;real-time planning capability;walking trajectory","","","","24","CCBYNCND","9 Aug 2023","","","IEEE","IEEE Journals"
"Applications of Reinforcement Learning in Three-phase Grid-connected Inverter","C. Li","Control and Computer Engineering North China Electric Power University, Beijing, China","2023 IEEE 13th International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","28 Sep 2023","2023","","","580","585","The grid-connected inverter is a key energy conversion device for grid-connected new energy and is widely used in distributed power generation system. However, the traditional control strategy has many limitations in the aspects of stability, system voltage and frequency adjustment, a large amount of renewable energy connected to the grid may cause power and frequency oscillation that threaten the stable operation of the grid. To solve this problem, an adaptive and optimal control method based on reinforcement learning and adaptive dynamic programming (ADP) is implemented for VSG three-phase grid-connected inverter. The method establishes a mathematical model of a VSG-based grid-connected inverter, and transformed into a standard linear quadratic regulation (LQR) optimization problem. On the basis of VSG power-frequency control, the dynamic compensation term given by the ADP algorithm is introduced into the active power loop. During the grid connection, the VSG output is optimally adjusted through the proposed adaptive optimal control strategy to reduce the system frequency fluctuation. In the case system dynamics are not known in the complex environment of grid-connected inverters, deep deterministic strategy gradient (DDPG) is used to replace the ADP algorithm in this paper. Finally, the effectiveness of the method is verified in the Simulink platform.","2642-6633","979-8-3503-1519-6","10.1109/CYBER59472.2023.10256489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256489","adaptive dynamic programming;grid-connected;virtual synchronous motor;deep deterministic policy gradient;three phase grid-connected inverter","Adaptive systems;System dynamics;Heuristic algorithms;Power system dynamics;Optimal control;Reinforcement learning;Inverters","distributed power generation;dynamic programming;frequency control;invertors;linear quadratic control;optimal control;optimisation;power engineering computing;power generation control;power grids;power system stability;reinforcement learning","active power loop;adaptive control method;adaptive dynamic programming;adaptive optimal control strategy;ADP algorithm;case system dynamics;distributed power generation system;frequency adjustment;frequency oscillation;grid connection;grid-connected new energy;key energy conversion device;optimal control method;reinforcement learning;renewable energy;standard linear quadratic regulation optimization problem;system frequency fluctuation;system voltage;VSG power-frequency control;VSG three-phase grid-connected inverter;VSG-based grid-connected inverter","","","","23","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Tracking Control of an Autonomous Surface Vessel in Natural Waters","W. Wang; X. Cao; A. Gonzalez-Garcia; L. Yin; N. Hagemann; Y. Qiao; C. Ratti; D. Rus","Computer Science and Artificial Intelligence Lab (CSAIL), Massachusetts Institute of Technology, Cambridge, MA, USA; Intelligent Perception and Computing Research Center, School of Artificial Intelligence, Beijing University of Posts and Telecommunications (BUPT), Beijing, China; SENSEable City Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Computer Science and Artificial Intelligence Lab (CSAIL), Massachusetts Institute of Technology, Cambridge, MA, USA; SENSEable City Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; SENSEable City Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; SENSEable City Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Computer Science and Artificial Intelligence Lab (CSAIL), Massachusetts Institute of Technology, Cambridge, MA, USA","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","3109","3115","Accurate control of autonomous marine robots still poses challenges due to the complex dynamics of the environment. In this paper, we propose a Deep Reinforcement Learning (DRL) approach to train a controller for autonomous surface vessel (ASV) trajectory tracking and compare its performance with an advanced nonlinear model predictive controller (NMPC) in real environments. Taking into account environmental disturbances (e.g., wind, waves, and currents), noisy measurements, and non-ideal actuators presented in the physical ASV, several effective reward functions for DRL tracking control policies are carefully designed. The control policies were trained in a simulation environment with diverse tracking trajectories and disturbances. The performance of the DRL controller has been verified and compared with the NMPC in both simulations with model-based environmental disturbances and in natural waters. Simulations show that the DRL controller has 53.33% lower tracking error than that of NMPC. Experimental results further show that, compared to NMPC, the DRL controller has 35.51% lower tracking error, indicating that DRL controllers offer better disturbance rejection in river environments than NMPC.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160858","Knut and Alice Wallenberg Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160858","","Deep learning;Trajectory tracking;Surface waves;Current measurement;Reinforcement learning;Predictive models;Trajectory","actuators;control engineering computing;deep learning (artificial intelligence);learning (artificial intelligence);marine robots;mobile robots;nonlinear control systems;predictive control;reinforcement learning;tracking;trajectory control;unmanned surface vehicles","account environmental disturbances;advanced nonlinear model predictive controller;autonomous marine robots;autonomous surface vessel trajectory tracking;Deep Reinforcement;diverse tracking trajectories;DRL controller;DRL tracking control policies;lower tracking error;model-based environmental disturbances;natural waters;NMPC;simulation environment","","","","38","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Synergistic Task and Motion Planning With Reinforcement Learning-Based Non-Prehensile Actions","G. Liu; J. de Winter; D. Steckelmacher; R. K. Hota; A. Nowe; B. Vanderborght","Brubotics, Vrije Universiteit Brussel, Brussels, Belgium; Brubotics, Vrije Universiteit Brussel, Brussels, Belgium; Artificial Intelligence (AI) Lab, Vrije Universiteit Brussel, Brussels, Belgium; Brubotics, Vrije Universiteit Brussel, Brussels, Belgium; Artificial Intelligence (AI) Lab, Vrije Universiteit Brussel, Brussels, Belgium; Brubotics, Vrije Universiteit Brussel, Brussels, Belgium","IEEE Robotics and Automation Letters","31 Mar 2023","2023","8","5","2764","2771","Robotic manipulation in cluttered environments requires synergistic planning among prehensile and non-prehensile actions. Previous works on sampling-based Task and Motion Planning (TAMP) algorithms, e.g. PDDLStream, provide a fast and generalizable solution for multi-modal manipulation. However, they are likely to fail in cluttered scenarios where no collision-free grasping approaches can be sampled without preliminary manipulations. To extend the ability of sampling-based algorithms, we integrate a vision-based Reinforcement Learning (RL) non-prehensile procedure, pusher. The pushing actions generated by pusher can eliminate interlocked situations and make the grasping problem solvable. Also, the sampling-based algorithm evaluates the pushing actions by providing rewards in the training process, thus the pusher can learn to avoid situations leading to irreversible failures. The proposed hybrid planning method is validated on a cluttered bin-picking problem and implemented in both simulation and real world. Results show that the pusher can effectively improve the success ratio of the previous sampling-based algorithm, while the sampling-based algorithm can help the pusher learn pushing skills.","2377-3766","","10.1109/LRA.2023.3261708","Flemish Government through the Program; China Scholarship Council; EU-Project euROBIN(grant numbers:101070596); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10080986","Task and motion planning;reinforcement learning;manipulation planning","Planning;Task analysis;Grasping;Robots;Training;Uncertainty;Search problems","control engineering computing;learning (artificial intelligence);manipulators;mobile robots;path planning;reinforcement learning;robot vision","cluttered bin-picking problem;cluttered environments;cluttered scenarios;collision-free grasping approaches;hybrid planning method;multimodal manipulation;prehensile;preliminary manipulations;previous sampling-based algorithm;pusher;pushing actions;Reinforcement Learning-based nonprehensile actions;robotic manipulation;sampling-based Task;synergistic planning","","","","31","IEEE","24 Mar 2023","","","IEEE","IEEE Journals"
"Feature Extraction for Effective and Efficient Deep Reinforcement Learning on Real Robotic Platforms","P. Böhm; P. Pounds; A. C. Chapman","The University of Queensland, Australia; The University of Queensland, Australia; The University of Queensland, Australia","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","7126","7132","Deep reinforcement learning (DRL) methods can solve complex continuous control tasks in simulated environments by taking actions based solely on state observations at each decision point. Because of the dynamics involved, individual snapshots of real-world sensor measurements afford only partial state observability, so it is typical to use a history of observations to improve training and policy performance. Such intertemporal information can be further exploited using a recurrent neural network (RNN) to reduce the dimensionality of the dynamic state representation. However, using RNNs as an internal part of a DRL network presents challenges of its own; and even then, the improvements in resulting policies are usually limited. To address these shortcomings, we propose using gated feature extraction to improve DRL training of real-world robots. Specifically, we use an untrained gated recurrent unit (GRU) to encode a low-dimension representation of the state observation sequence before passing it to the DRL training procedure. In addition to dimensionality reduction, this allows us to unroll the RNN by encoding the observations cumulatively as they are collected, thereby avoiding same-length input requirements, and train the RL network on the raw observations at the current step combined with the GRU-encoding of the preceding steps. Our simulation experiments employ gated feature extraction with the TD3 algorithm. Our results show that the GRU-encoded state observations improve the training speed and execution performance of the TD3 algorithm, improving the learned policies in all 19 test cases, exceeding the maximum achieved reward by over 38% in 8 and doubling the maximum achieved reward in three, while also outperforming a baseline implementation of SAC in 17 out of 19 environments. Moreover, the greatest improvement is seen in real-world experiments, where our approach successfully learns to balance a pendulum as well as a complex quadrupedal locomotion task. In contrast, the standard TD3 algorithm not only does not show any learning progress at all, but also repeatedly damages the hardware.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160862","Advance Queensland Trusted Autonomous Systems Defence CRC Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160862","","Training;Deep learning;Recurrent neural networks;Reinforcement learning;Computer architecture;Logic gates;Feature extraction","control engineering computing;deep learning (artificial intelligence);feature extraction;legged locomotion;recurrent neural nets;reinforcement learning","complex continuous control tasks;complex quadrupedal locomotion task;decision point;deep reinforcement learning methods;dimensionality reduction;DRL network;DRL training procedure;dynamic state representation;efficient deep reinforcement learning;execution performance;gated feature extraction;GRU-encoded state observations;GRU-encoding;individual snapshots;internal part;intertemporal information;learned policies;learning progress;low-dimension representation;partial state observability;policy performance;preceding steps;raw observations;real robotic platforms;real-world experiments;real-world robots;real-world sensor measurements;recurrent neural network;resulting policies;RL network;RNN;same-length input requirements;simulated environments;simulation experiments;standard TD3 algorithm;state observation sequence;training speed;untrained gated recurrent unit","","","","31","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Fault Detection and Optimization for Flight Vehicles via Deep Reinforcement Learning","H. Cheng; P. Chen; L. Shi; Y. He; R. Hu; J. Zhang","School of Astronautics, Northwestern Polytechnical University, Xi’an, China; School of Astronautics, Northwestern Polytechnical University, Xi’an, China; School of Astronautics, Northwestern Polytechnical University, Xi’an, China; School of Astronautics, Northwestern Polytechnical University, Xi’an, China; School of Astronautics, Northwestern Polytechnical University, Xi’an, China; School of Astronautics, Northwestern Polytechnical University, Xi’an, China","2020 IEEE 16th International Conference on Control & Automation (ICCA)","30 Nov 2020","2020","","","1384","1389","The problem of fault detection for unmanned flight vehicle is investigated in the paper. Firstly, the linear switched model of flight vehicle is obtained via nonlinear dynamic model. The fault detection filter based on robust control theory is designed to generate the residual signal. To achieve better performance, deep reinforcement learning is applied to tune the parameters. The scheduling interval can be viewed as a neighborhood of parameters obtained by robust control theory. The stability of closed-loop system is guaranteed by the aid of average dwell time method and Lyapunov functional method. Simulation results in the end demonstrate the effectiveness and superiority of proposed method.","1948-3457","978-1-7281-9093-8","10.1109/ICCA51439.2020.9264575","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264575","","Fault detection;Switched systems;Filtering theory;Switches;Reinforcement learning;Stability analysis;Robust control","autonomous aerial vehicles;closed loop systems;control system synthesis;fault diagnosis;learning (artificial intelligence);Lyapunov methods;nonlinear control systems;robust control;time-varying systems","deep reinforcement learning;unmanned flight vehicle;nonlinear dynamic model;fault detection filter;robust control theory;residual signal;fault optimization;linear switched model;scheduling interval;closed-loop system stability;Lyapunov functional method","","","","12","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Collision-free Path Planning For Welding Manipulator Via Deep Reinforcement Learning","B. Hu; T. Wang; C. Chen; Y. Xu; L. Cheng","Guangdong Provincial Key Laboratory of Cyber-Physical System, Guangdong University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Cyber-Physical System, Guangdong University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Cyber-Physical System, Guangdong University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Cyber-Physical System, Guangdong University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Cyber-Physical System, Guangdong University of Technology, Guangzhou, China","2022 27th International Conference on Automation and Computing (ICAC)","10 Oct 2022","2022","","","1","6","In the narrow industrial welding scene, it is difficult for the 6DOF manipulator to realize intelligent obstacle avoidance planning. This paper proposes an adaptive reinforcement learning on the path planning method of welding manipulator to find a collision-free path in the limited scene. The sub-actor network is designed to conduct guided search on the main actor-network to achieve effective obstacle avoidance. The overestimation of Q value is alleviated by embedding the return distribution function into maximum entropy to replace the shear-double Q learning of SAC. We evaluate our approach on a group path planning experiment, the experiments demonstrate that our method increases learning efficiency and obtains safer policies.","","978-1-6654-9807-4","10.1109/ICAC55051.2022.9911177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9911177","Maximum entropy;Welding robot planning;Narrow space planning;Obstacle avoidance","Q-learning;Trajectory planning;Welding;Heuristic algorithms;Multitasking;Robustness;Planning","collision avoidance;manipulators;mobile robots;multi-robot systems;reinforcement learning;robotic welding","collision-free path planning;welding manipulator;deep reinforcement learning;narrow industrial welding scene;6DOF manipulator;intelligent obstacle avoidance planning;adaptive reinforcement learning;path planning method;sub-actor network;actor-network;return distribution function;shear-double Q learning;group path planning experiment;maximum entropy","","","","23","IEEE","10 Oct 2022","","","IEEE","IEEE Conferences"
"Image-Goal Navigation via Metric Mapping and Keypoint based Reinforcement Learning","J. Seok Heo; Y. Choi; S. Oh","Department of Electrical Engineering and Computer Science, Seoul National University, Seoul, Korea; Department of Electrical Engineering and Computer Science, Seoul National University, Seoul, Korea; Department of Electrical Engineering and Computer Science, Seoul National University, Seoul, Korea","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","319","324","In this paper, we tackle the problem of Image-Goal Navigation, where an agent is asked to find the viewpoint of a given goal image in an unseen environment. Past methods rely on depth cameras, require panoramic observations and a pre-built graph network, or is trained on limited situations where the image-goal viewpoint is close to the initial agent state. We propose using a hierarchical policy structure which uses a high-level policy to guide the agent more efficiently to the image-goal viewpoint. Our method uses metric maps built by RGB images by Active Neural SLAM network and image descriptors extracted from place recognition networks. We have trained a policy network whose input is a SLAM Map, along with the information about matched keypoints and global descriptors between the observation and goal image, and output is a global goal which guides the agent to the image goal. Our method is trained on a self-made image-goal dataset based on the Gibson datset which contains images of the best viewpoints of semantic objects in the environment and is shown that using this method gives improved performance compared to previous related methods.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9650026","IITP(grant numbers:2019-0-01309); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650026","Image-goal Navigation;Keypoint Detection;Visual Servoing;Place Recognition","Measurement;Simultaneous localization and mapping;Image recognition;Navigation;Semantics;Neural networks;Reinforcement learning","feature extraction;image colour analysis;learning (artificial intelligence);mobile robots;path planning;robot vision;SLAM (robots)","place recognition networks;policy network whose input;SLAM Map;global goal;image goal;image-goal dataset;Image-Goal Navigation;given goal image;pre-built graph network;image-goal viewpoint;initial agent state;hierarchical policy structure;high-level policy;metric maps;RGB images;image descriptors","","","","19","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Adaptive Risk-Tendency: Nano Drone Navigation in Cluttered Environments with Distributional Reinforcement Learning","C. Liu; E. -J. van Kampen; G. C. H. E. de Croon",Delft University of Technology; Delft University of Technology; Delft University of Technology,"2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","7198","7204","Enabling the capability of assessing risk and making risk-aware decisions is essential to applying reinforcement learning to safety-critical robots like drones. In this paper, we investigate a specific case where a nano quadcopter robot learns to navigate an apriori-unknown cluttered environment under partial observability. We present a distributional reinforcement learning framework to generate adaptive risk-tendency policies. Specifically, we propose to use lower tail conditional variance of the learnt return distribution as intrinsic uncertainty estimation, and use exponentially weighted average forecasting (EWAF) to adapt the risk-tendency in accordance with the estimated uncertainty. In simulation and real-world empirical results, we show that (1) the most effective risk-tendency varies across states, (2) the agent with adaptive risk-tendency achieves superior performance compared to risk-neutral policy or risk-averse policy baselines. Code and video can be found in this repository: https://github.com/tudelft/risk-sensitive-rl.git","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160324","","Adaptation models;Uncertainty;Navigation;Estimation;Reinforcement learning;Tail;Observability","autonomous aerial vehicles;control engineering computing;decision making;helicopters;learning (artificial intelligence);mobile robots;path planning;reinforcement learning;risk analysis;risk management;robot programming","adaptive risk-tendency policies;apriori-unknown cluttered environment;cluttered environments;distributional reinforcement learning;drones;effective risk-tendency varies;intrinsic uncertainty estimation;learnt return distribution;lower tail conditional variance;making risk-aware decisions;nanodrone navigation;nanoquadcopter robot;risk-averse policy baselines;risk-neutral policy;safety-critical robots","","","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Obstacle Avoidance Based on Deep Reinforcement Learning and Artificial Potential Field","H. Han; Z. Xi; J. Cheng; M. Lv","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Air Traffic Control and Navigation College, Air Force Engineering University, Xian, China","2023 9th International Conference on Control, Automation and Robotics (ICCAR)","21 Jun 2023","2023","","","215","220","Obstacle avoidance is an essential part of mobile robot path planning, since it ensures the safety of automatic control. This paper proposes an obstacle avoidance algorithm that combines artificial potential field with deep reinforcement learning (DRL). State regulation is presented so that the pre-defined velocity constraint could be satisfied. To guarantee the isotropy of the robot controller as well as reduce training complexity, coordinate transformation into normal direction and tangent direction is introduced, making it possible to use one-dimension controllers to work in a two-dimension task. Artificial potential field (APF) is modified such that the obstacle directly affects the intermediate target positions instead of the control commands, which can well be used to guide the previously trained one-dimension DRL controller. Experiment results show that the proposed algorithm successfully achieved obstacle avoidance tasks in single-agent and multi-agent scenarios.","2251-2454","979-8-3503-2251-4","10.1109/ICCAR57134.2023.10151771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10151771","obstacle avoidance;deep reinforcement learning (DRL);artificial potential field (APF)","Deep learning;Training;Robot kinematics;Reinforcement learning;Regulation;Path planning;Safety","collision avoidance;deep learning (artificial intelligence);mobile robots;multi-agent systems;path planning;reinforcement learning","artificial potential field;automatic control;control commands;deep reinforcement learning;mobile robot path planning;obstacle avoidance algorithm;obstacle avoidance tasks;one-dimension controllers;one-dimension DRL controller;pre-defined velocity constraint;robot controller","","","","17","IEEE","21 Jun 2023","","","IEEE","IEEE Conferences"
"Viewpoint Selection for DermDrone using Deep Reinforcement Learning","M. A. Arzati; S. Arzanpour","MechatronicSystems Engineering, Simon Fraser University, British Colombia, Canada; MechatronicSystems Engineering, Simon Fraser University, British Colombia, Canada","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","544","553","This paper presents an RL-based method to improve the performance of real-time 3D human pose estimation as a positioning feedback for DermDrone which is a micro sized quadrotor designed MetaOptima to capture high resolution full body images for dermatology application. The camera viewpoint is identified as the key parameter in the accuracy of monocular 3D human pose estimation. We present a deep reinforcement learning based method for determining the best viewpoint given the flight trajectory. Our goal is to present a reliable and accurate positioning feedback for DermDrone using a 3D human pose estimation algorithm. DQN and its variants (Double DQN, and Dueling DQN) were employed and their performances were investigated by conducting several simulations. The results confirm that RL-based viewpoint selection improve the performance of 3D human pose estimation.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649799","Mitacs; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649799","Viewpoint Selection;Next-Best-View;Drone;Reinforcement Learning;Human Pose Estimation;Autonomous Navigation","Training;Visualization;Three-dimensional displays;Navigation;Pose estimation;Reinforcement learning;Real-time systems","cameras;learning (artificial intelligence);pose estimation","dermdrone;deep reinforcement learning;real-time 3D human;microsized quadrotor;high resolution full body images;dermatology application;camera viewpoint;monocular 3D human;reliable positioning feedback;accurate positioning feedback;estimation algorithm;Double DQN;Dueling DQN;RL-based viewpoint selection","","","","51","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Adaptively Calibrated Critic Estimates for Deep Reinforcement Learning","N. Dorka; T. Welschehold; J. Bödecker; W. Burgard","University of Freiburg, Freiburg im Breisgau, Germany; University of Freiburg, Freiburg im Breisgau, Germany; University of Freiburg, Freiburg im Breisgau, Germany; University of Technology Nuremberg, Nürnberg, Germany","IEEE Robotics and Automation Letters","22 Dec 2022","2023","8","2","624","631","Accurate value estimates are important for off-policy reinforcement learning. Algorithms based on temporal difference learning typically are prone to an over- or underestimation bias building up over time. In this letter, we propose a general method called Adaptively Calibrated Critics (ACC) that uses the most recent high variance but unbiased on-policy rollouts to alleviate the bias of the low variance temporal difference targets. We apply ACC to Truncated Quantile Critics [1], which is an algorithm for continuous control that allows regulation of the bias with a hyperparameter tuned per environment. The resulting algorithm adaptively adjusts the parameter during training rendering hyperparameter search unnecessary and sets a new state of the art on the OpenAI gym continuous control benchmark among all algorithms that do not tune hyperparameters for each environment. ACC further achieves improved results on different tasks from the Meta-World robot benchmark. Additionally, we demonstrate the generality of ACC by applying it to TD3 [2] and showing an improved performance also in this setting.","2377-3766","","10.1109/LRA.2022.3229236","European Commission(grant numbers:871449-OpenDR); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9984837","Deep learning methods;reinforcement learning","Entropy;Task analysis;Robots;Training;Deep learning;Benchmark testing;Approximation algorithms","calibration;deep learning (artificial intelligence);reinforcement learning;search problems","ACC;adaptively calibrated critics;deep reinforcement learning;hyperparameter search;low variance temporal difference;meta-world robot benchmark;off-policy reinforcement learning;OpenAI gym continuous control benchmark;temporal difference learning;truncated quantile critics","","","","40","IEEE","14 Dec 2022","","","IEEE","IEEE Journals"
"Optimal set point generation based on deep reinforcement learning and transfer knowledge for Wastewater Treatment Plants","O. Aponte-Rengifo; R. Vilanova; M. Francisco; P. Vega; S. Revollar","Informatics and Automatics Department, University of Salamanca, Spain; Telecommunications and Systems Department, Autonoma University of Barcelona, Spain; Informatics and Automatics Department, University of Salamanca, Spain; Informatics and Automatics Department, University of Salamanca, Spain; Informatics and Automatics Department, University of Salamanca, Spain","2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)","12 Oct 2023","2023","","","1","7","In this work, a model-free Reinforcement Learning method is implemented to control the aeration in the nitrification process in wastewater treatment plants (WWTPs). Controlling real complex non-linear processes is challenging due to model mismatch between the model used in the controller and the real process. Model-based control strategies can fail in a real implementation due to modeling errors. WWTPs include multivariable, nonlinear, and complex biological processes, which makes it difficult to obtain adequate models to represent the dynamics of the process under all possible conditions. For this reason, a model-free reinforcement learning agent that takes advantage of Transfer Learning is selected. This data-driven procedure is trained to deal with the trade-off between effluent quality and operating costs. The results demonstrate that model-free RL can reach sub-optimal solutions through a data-driven procedure.","1946-0759","979-8-3503-3991-8","10.1109/ETFA54631.2023.10275522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275522","Deep Reinforcement Learning;Transfer Learning;Wastewater Treatment Plants;Intelligent Control","Deep learning;Training;Biological system modeling;Transfer learning;Process control;Optimal control;Reinforcement learning","","","","","","34","IEEE","12 Oct 2023","","","IEEE","IEEE Conferences"
"Safe Trajectory Sampling in Model-Based Reinforcement Learning","S. Zwane; D. Hadjivelichkov; Y. Luo; Y. Bekiroglu; D. Kanoulas; M. P. Deisenroth","UCL Centre for Artificial Intelligence, University College London, UK; UCL Centre for Artificial Intelligence, University College London, UK; UCL Centre for Artificial Intelligence, University College London, UK; UCL Centre for Artificial Intelligence, University College London, UK; UCL Centre for Artificial Intelligence, University College London, UK; UCL Centre for Artificial Intelligence, University College London, UK","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","6","Model-based reinforcement learning aims to learn a policy to solve a target task by leveraging a learned dynamics model. This approach, paired with principled handling of uncertainty allows for data-efficient policy learning in robotics. However, the physical environment has feasibility and safety constraints that need to be incorporated into the policy before it is safe to execute on a real robot. In this work, we study how to enforce the aforementioned constraints in the context of model-based reinforcement learning with probabilistic dynamics models. In particular, we investigate how trajectories sampled from the learned dynamics model can be used on a real robot, while fulfilling user-specified safety requirements. We present a model-based reinforcement learning approach using Gaussian processes where safety constraints are taken into account without simplifying Gaussian assumptions on the predictive state distributions. We evaluate the proposed approach on different continuous control tasks with varying complexity and demonstrate how our safe trajectory-sampling approach can be directly used on a real robot without violating safety constraints.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260496","Engineering and Physical Sciences Research Council(grant numbers:EP/S021566/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260496","","Visualization;Uncertainty;Propioception;Reinforcement learning;Probabilistic logic;Safety;Trajectory","control engineering computing;Gaussian processes;reinforcement learning;robot programming;trajectory control","data-efficient policy learning;Gaussian process;learned dynamics model;model-based reinforcement learning;probabilistic dynamics models;robotics;safe trajectory sampling;safety constraints;user-specified safety requirements","","","","24","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Reinforcement fuzzy-neural adaptive iterative learning control for nonlinear systems","Y. -C. Wang; C. -J. Chien; Der-Tsai Lee","Department of Electronic Engineering, National University of Tainan, Tainan, Taiwan; Department of Electronic Engineering, Huafan University, Taipei, Taiwan; Institute of Information Science, Academia Sinica, Taipei, Taiwan","2008 10th International Conference on Control, Automation, Robotics and Vision","27 Feb 2009","2008","","","733","738","This paper proposes a new fuzzy neural network based reinforcement adaptive iterative learning controller for a class of nonlinear systems. Different from some existing reinforcement learning schemes, the reinforcement adaptive iterative learning controller has the advantages of rigorous proofs without using an approximation of the plant Jacobian. The critic is appended into the reinforcement adaptive iterative learning controller to generate the reinforcement signal, which provides a degree of satisfaction about the tracking performance. In addition, the reinforcement signal can be further applied in the weight adaptation rules. Iterative learning components of the reinforcement adaptive iterative learning controller are designed to compensate for the uncertainties of plant nonlinearities. The overall adaptive scheme guarantees all adjustable parameters and the internal signals remain bounded for all iterations. Moreover, the norm of tracking error vector at each time instant will asymptotically converge to a tunable residual set as iteration goes to infinity even the initial state error exists. Finally, a simulation result is given to demonstrate the learning performance of the fuzzy neural network based reinforcement adaptive iterative learning controller.","","978-1-4244-2286-9","10.1109/ICARCV.2008.4795608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4795608","Iterative learning control;reinforcement learning control;adaptive control;nonlinear systems;fuzzy neural network","Programmable control;Adaptive control;Nonlinear control systems;Control systems;Nonlinear systems;Fuzzy neural networks;Fuzzy control;Adaptive systems;Learning;Jacobian matrices","adaptive control;control nonlinearities;fuzzy control;iterative methods;learning systems;neurocontrollers;nonlinear control systems","fuzzy neural network;reinforcement adaptive iterative learning controller;nonlinear systems;reinforcement signal;weight adaptation rules;plant nonlinearities;tracking error vector","","","2","29","IEEE","27 Feb 2009","","","IEEE","IEEE Conferences"
"Dextrous Tactile In-Hand Manipulation Using a Modular Reinforcement Learning Architecture","J. Pitz; L. Röstel; L. Sievers; B. Bäuml","DLR Institute of Robotics and Mechatronics, Technical University of Munich; DLR Institute of Robotics and Mechatronics, Technical University of Munich; DLR Institute of Robotics and Mechatronics, Technical University of Munich; DLR Institute of Robotics and Mechatronics, Technical University of Munich","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","1852","1858","Dextrous in-hand manipulation with a multi-fingered robotic hand is a challenging task, esp. when performed with the hand oriented upside down, demanding permanent force-closure, and when no external sensors are used. For the task of reorienting an object to a given goal orientation (vs. infinitely spinning it around an axis), the lack of external sensors is an additional fundamental challenge as the state of the object has to be estimated all the time, e.g., to detect when the goal is reached. In this paper, we show that the task of reorienting a cube to any of the 24 possible goal orientations in a π/2-raster using the torque-controlled DLR-Hand II is possible. The task is learned in simulation using a modular deep reinforcement learning architecture: the actual policy has only a small observation time window of 0.5 s but gets the cube state as an explicit input which is estimated via a deep differentiable particle filter trained on data generated by running the policy. In simulation, we reach a success rate of 92% while applying significant domain randomization. Via zero-shot Sim2Real-transfer on the real robotic system, all 24 goal orientations can be reached with a high success rate. (Web: dlr-alr.github.io/dlr-tactile-manipulation)","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160756","","Training;Deep learning;Uncertainty;Reinforcement learning;Particle filters;Sensors;Service-oriented architecture","control engineering computing;deep learning (artificial intelligence);dexterous manipulators;position control;reinforcement learning;torque control","cube state;deep differentiable particle filter;dextrous tactile in-hand manipulation;external sensors;modular deep reinforcement learning architecture;modular reinforcement learning architecture;observation time window;permanent force-closure;robotic hand;robotic system;torque-controlled DLR-Hand II;via zero-shot Sim2Real-transfer","","","","15","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"GEM-RL: Generalized Energy Management of Wearable Devices using Reinforcement Learning","T. Basaklar; Y. Tuncel; S. Gumussoy; U. Ogras","Department of Electrical and Computer Engineering, University of Wisconsin - Madison, Madison, WI, USA; Department of Electrical and Computer Engineering, University of Wisconsin - Madison, Madison, WI, USA; Siemens Technology, Princeton, NJ, USA; Department of Electrical and Computer Engineering, University of Wisconsin - Madison, Madison, WI, USA","2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)","2 Jun 2023","2023","","","1","6","Energy harvesting (EH) and management (EM) have emerged as enablers of self-sustained wearable devices. Since EH alone is not sufficient for self-sustainability due to uncertainties of ambient sources and user activities, there is a critical need for a user-independent EM approach that does not rely on expected EH predictions. We present a generalized energy management framework (GEM-RL) using multi-objective reinforcement learning. GEM-RL learns the trade-off between utilization and the battery energy level of the target device under dynamic EH patterns and battery conditions. It also uses a lightweight approximate dynamic programming (ADP) technique that utilizes the trained MORL agent to optimize the utilization of the device over a longer period. Thorough experiments show that, on average, GEM-RL achieves Pareto front solutions within 5.4% of the offline Oracle for a given day. For a 7-day horizon, it achieves utility up to 4% within the offline Oracle and up to 50% higher utility compared to baseline EM approaches. The hardware implementation on a wearable device shows negligible execution time (1.98 ms) and energy consumption (23.17 μJ) overhead.","1558-1101","979-8-3503-9624-9","10.23919/DATE56975.2023.10137228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10137228","Energy harvesting;multi-objective reinforcement learning;dynamic programming;energy management","Energy consumption;Uncertainty;Wearable computers;Reinforcement learning;Hardware;Time measurement;Batteries","dynamic programming;energy consumption;energy harvesting;energy management systems;learning (artificial intelligence);power engineering computing;reinforcement learning;telecommunication computing;telecommunication power management","ambient sources;battery energy level;dynamic EH patterns;energy 23.17 muJ;expected EH predictions;GEM-RL achieves;generalized energy management framework;higher utility;lightweight approximate dynamic programming technique;multiobjective reinforcement learning;offline Oracle;self-sustainability;self-sustained wearable devices;time 1.98 ms;user activities;user-independent EM approach;wearable device","","","","20","","2 Jun 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning based Personalized Locomotion Planning for Lower-Limb Exoskeletons","J. K. Mehr; E. Guo; M. Akbari; V. K. Mushahwar; M. Tavakoli","Department of Electrical and Computer Engineering, and the Department of Medicine, University of Alberta, Edmonton, Alberta, Canada; Cumming School of Medicine, University of Calgary, Calgary, Alberta, Canada; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Alberta, Canada; Department of Medicine, Division of Physical Medicine and Rehabilitation, University of Alberta, Edmonton, Alberta, Canada; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Alberta, Canada","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5127","5133","This paper introduces intelligent central pattern generators (iCPGs) that can plan personalized walking trajectories for lower-limb exoskeletons. This can make walking more comfortable for the users by resolving one of the significant shortcomings of most commercially available exoskeletons, which is the use of pre-defined fixed trajectories for all users. The proposed method combines reinforcement learning (RL) with previously introduced adaptable central pattern generators (ACPGs) to learn a user's physical interaction behaviour and refine the exoskeleton's walking trajectories. The ACPG method embeds physical human-robot interaction (pHRI) in CPGs to make changing gait trajectories in real-time, possible. However, to effectively refine gait trajectories based on pHRIs, the parameters must be precisely identified and updated as a user interacts with the exoskeleton. Our proposed method uses RL to modify (amplify/attenuate) the pHRI energy based on a user's interaction behaviour, and form an effective energy value which can facilitate reaching desired gait pattern for users via iCPG dynamics. The proposed method can resolve the aforementioned challenges with ACPGs and personalized trajectory generation. The simulation and experimental results provide evidence that the proposed method can effectively adapt to the user's behaviour in different walking scenarios with the Indego lower-limb exoskeleton.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161559","","Legged locomotion;Parameter estimation;Simulation;Exoskeletons;Human-robot interaction;Reinforcement learning;Real-time systems","deep learning (artificial intelligence);gait analysis;human-robot interaction;learning (artificial intelligence);legged locomotion;medical robotics;mobile robots;motion control;patient rehabilitation;reinforcement learning;robot dynamics","ACPG method embeds physical human-robot interaction;ACPGs;changing gait trajectories;commercially available exoskeletons;deep reinforcement;desired gait pattern;different walking scenarios;effective energy value;Indego lower-limb exoskeleton;intelligent central pattern generators;introduced adaptable central pattern generators;locomotion planning;lower-limb exoskeletons;physical interaction behaviour;pre-defined;reinforcement learning;RL;significant shortcomings;trajectory generation","","","","24","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Online Safety Property Collection and Refinement for Safe Deep Reinforcement Learning in Mapless Navigation","L. Marzari; E. Marchesini; A. Farinelli","Department of Computer Science, University of Verona, Italy; Khoury College of Computer Sciences, Northeastern University, US; Department of Computer Science, University of Verona, Italy","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","7133","7139","Safety is essential for deploying Deep Reinforcement Learning (DRL) algorithms in real-world scenarios. Recently, verification approaches have been proposed to allow quantifying the number of violations of a DRL policy over input-output relationships, called properties. However, such properties are hard-coded and require task-level knowledge, making their application intractable in challenging safety-critical tasks. To this end, we introduce the Collection and Refinement of Online Properties (CROP) framework to design properties at training time. CROP employs a cost signal to identify unsafe interactions and use them to shape safety properties. Hence, we propose a refinement strategy to combine properties that model similar unsafe interactions. Our evaluation compares the benefits of computing the number of violations using standard hard-coded properties and the ones generated with CROP. We evaluate our approach in several robotic mapless navigation tasks and demonstrate that the violation metric computed with CROP allows higher returns and lower violations over previous Safe DRL approaches.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161312","","Deep learning;Training;Navigation;Shape;Crops;Reinforcement learning;Safety","control engineering computing;crops;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;multi-robot systems;navigation;path planning;reinforcement learning;robot programming","application intractable;called properties;CROP;DRL policy;input-output relationships;model similar unsafe interactions;Online Properties framework;previous Safe DRL approaches;refinement strategy;robotic mapless navigation tasks;safe deep reinforcement learning;safety properties;safety-critical tasks;task-level knowledge;verification approaches","","","","34","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Trajectory Optimization for Urban Rail Transit Considering Regenerative Energy Utilization: A Reinforcement Learning Approach","W. Wei; X. Wang","School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","300","305","Energy saving in urban rail transit (URT) has recently been developing vigorously. Regenerative braking is critical for reducing consumption in URT operation, especially when regenerative braking energy (RBE) recycling with high efficiency. This paper proposes a trajectory optimization to reduce traction consumption by means of reusing RBE. First, an electric energy computation model is developed to evaluate traction power on substation level, in which electric characteristic of traction power system is taken into consideration. Then, the train operation process is discretized and turned into a multistep decision optimization problem by defining the objective as a trade-off of RBE reusing efficiency and given running time. A Deep Q Network approach contains dense reward is introduced to solve it from a Reinforcement Learning perspective. Finally, some numerical studies based on real-world data are implemented to verify the effectiveness of proposed approach and algorithm.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055605","Deep Q network;trajectory optimization;regenerative braking;energy efficient;urban rail transit","Rails;Substations;Computational modeling;Decision making;Reinforcement learning;Electric variables;Traction power supplies","energy conservation;learning (artificial intelligence);optimisation;rail traffic;railway electrification;railways;regenerative braking;reinforcement learning;substations;traction;traction power supplies","Deep Q Network approach;electric characteristic;electric energy computation model;multistep decision optimization problem;RBE reusing efficiency;regenerative braking energy;Reinforcement Learning approach;Reinforcement Learning perspective;reusing RBE;traction consumption;traction power system;train operation process;trajectory optimization;urban rail transit considering regenerative energy utilization;URT operation","","","","14","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Reinforcement learning and automatic categorization","J. M. Porta; E. Celaya","Institut de Robòtica i Informàtica Industrial (CSIC-UPC), Barcelona, Spain; Institut de Robòtica i Informàtica Industrial (CSIC-UPC), Barcelona, Spain","1999 7th IEEE International Conference on Emerging Technologies and Factory Automation. Proceedings ETFA '99 (Cat. No.99TH8467)","6 Aug 2002","1999","1","","159","166 vol.1","The categorization process defines sensor and action categories from elementary sensor readings and basic actions so that the necessary elements for solving a task are correctly perceived and manipulated. In reinforcement learning, a previous categorization process is needed to define sensor and action categories with special requirements that we analyze and that, in general, are difficult to achieve, especially in complex tasks such as those that arise when working with autonomous robots. We show how these special requirements should be relaxed and we sketch a reinforcement learning algorithm that uses a less restrictive form of sensory categorization than existing algorithms. Additionally, we show how a given sensory categorization can be improved so that it better fits the demands of the previous algorithm.","","0-7803-5670-5","10.1109/ETFA.1999.815351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=815351","","Learning;Artificial intelligence;Sensor phenomena and characterization;Robot sensing systems;Intelligent robots;Robot control;Robot programming;Programming profession;Mechanical engineering;Micromotors","mobile robots;learning (artificial intelligence);intelligent control;sensors;robot programming","reinforcement learning;automatic categorization;categorization process;action categories;elementary sensor readings;basic actions;previous categorization process;sensor categories;autonomous robots;special requirements;sensory categorization","","","","16","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"ROMAX: Certifiably Robust Deep Multiagent Reinforcement Learning via Convex Relaxation","C. Sun; D. -K. Kim; J. P. How","Department of Aerospace Engineering, Mississippi State University, MS; Laboratory for Information & Decision Systems, Massachusetts Institute of Technology, Cambridge, MA; Laboratory for Information & Decision Systems, Massachusetts Institute of Technology, Cambridge, MA","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","5503","5510","In a multirobot system, a number of cyber-physical attacks (e.g., communication hijack, observation per-turbations) can challenge the robustness of agents. This robust-ness issue worsens in multiagent reinforcement learning because there exists the non-stationarity of the environment caused by simultaneously learning agents whose changing policies affect the transition and reward functions. In this paper, we propose a minimax MARL approach to infer the worst-case policy update of other agents. As the minimax formulation is computationally intractable to solve, we apply the convex relaxation of neural networks to solve the inner minimization problem. Such convex relaxation enables robustness in interacting with peer agents that may have significantly different behaviors and also achieves a certified bound of the original optimization problem. We eval-uate our approach on multiple mixed cooperative-competitive tasks and show that our method outperforms the previous state of the art approaches on this topic.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812321","Scientific Systems Company(grant numbers:SC-1661-04); ARL(grant numbers:W911NF-17-2-0181); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812321","","Learning systems;Perturbation methods;Neural networks;Reinforcement learning;Minimization;Robustness;Behavioral sciences","concave programming;convex programming;learning (artificial intelligence);minimax techniques;minimisation;multi-agent systems;multi-robot systems","ROMAX;robust deep multiagent reinforcement learning;convex relaxation;multirobot system;cyber-physical attacks;communication hijack;observation per-turbations;robust-ness issue worsens;nonstationarity;changing policies;reward functions;minimax MARL approach;worst-case policy update;minimax formulation;inner minimization problem;peer agents","","","","42","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Benchmarking Reinforcement Learning Techniques for Autonomous Navigation","Z. Xu; B. Liu; X. Xiao; A. Nair; P. Stone","Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin; Department of Computer Science, George Mason University; Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","9224","9230","Deep reinforcement learning (RL) has brought many successes for autonomous robot navigation. However, there still exists important limitations that prevent real-world use of RL-based navigation systems. For example, most learning approaches lack safety guarantees; and learned navigation systems may not generalize well to unseen environments. Despite a variety of recent learning techniques to tackle these challenges in general, a lack of an open-source benchmark and reproducible learning methods specifically for autonomous navigation makes it difficult for roboticists to choose what learning methods to use for their mobile robots and for learning researchers to identify current shortcomings of general learning methods for autonomous navigation. In this paper, we identify four major desiderata of applying deep RL approaches for autonomous navigation: (D1) reasoning under uncertainty, (D2) safety, (D3) learning from limited trial-and-error data, and (D4) generalization to diverse and novel environments. Then, we explore four major classes of learning techniques with the purpose of achieving one or more of the four desiderata: memory-based neural network architectures (D1), safe RL (D2), model-based RL (D2, D3), and domain randomization (D4). By deploying these learning techniques in a new open-source large-scale navigation benchmark and real-world environments, we perform a comprehensive study aimed at establishing to what extent can these techniques achieve these desiderata for RL-based navigation systems.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160583","NSF(grant numbers:CPS-1739964,IIS-1724157,NRI-1925082); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160583","","Training;Learning systems;Uncertainty;Navigation;Reinforcement learning;Benchmark testing;Robustness","deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;navigation;path planning;reinforcement learning","autonomous navigation;autonomous robot navigation;deep reinforcement learning;deep RL approaches;general learning methods;learning approaches lack;open-source large-scale navigation benchmark;recent learning techniques;reinforcement learning techniques;RL-based navigation systems","","","","34","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach Based on Parameter Prediction for Multi-AUV Task Assignment in Ocean Current","C. Ding; Z. Zheng","College of Computer and Cyber Security, Fujian Normal University, Fuzhou, China; College of Computer and Cyber Security, Fujian Normal University, Fuzhou, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","6764","6769","In this paper, the reinforcement learning (RL) is used to study task assignment problem of multiple autonomous underwater vehicles (AUVs) in ocean current. To solve the problem that traditional RL is prone to trap into non-optimal policy in training, a RL algorithm based on parameter prediction is proposed. The algorithm consists of four parts: First, we design an objective function with four constraints, including ocean current, task emergency, energy consumption and collision detection. The AUVs need to balance these constraints to find optimal strategy. Second, the reward distribution matrix (RDM) is constructed by the team cumulative reward generated by multiple episodes. Third, the features in RDM are extracted by truncated singular value decomposition, and the features are used to predict the policy feature. Forth, the prediction feature is mapped into policy parameters by its surrounding features, and we design a recovery mechanism to avoid the prediction errors. Finally, we analyze the effectiveness of the algorithm, and the simulation results show that compared with DDQN, our algorithm have a better performance.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055966","National Natural Science Foundation of China; Natural Science Foundation of Fujian Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055966","multi-AUV;task assignment;SVD;DDQN","Training;Energy consumption;Oceans;Simulation;Reinforcement learning;Prediction methods;Prediction algorithms","autonomous underwater vehicles;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;multi-robot systems;optimisation;reinforcement learning;singular value decomposition;underwater vehicles","AUVs;collision detection;energy consumption;multiAUV task assignment;multiple autonomous underwater vehicles;multiple episodes;nonoptimal policy;objective function;ocean current;optimal strategy;parameter prediction;policy feature;policy parameters;prediction errors;prediction feature;RDM;reinforcement learning;reward distribution matrix;RL algorithm;surrounding features;task assignment problem;task emergency;team cumulative reward;traditional RL;truncated singular value decomposition","","","","15","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Asynchronous Reinforcement Learning for Real-Time Control of Physical Robots","Y. Yuan; A. R. Mahmood","Department of Computing Science, University of Alberta, Edmonton, Canada; Department of Computing Science, University of Alberta, Edmonton, Canada","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","5546","5552","An oft-ignored challenge of real-world reinforcement learning is that the real world does not pause when agents make learning updates. As standard simulated environments do not address this real-time aspect of learning, most available implementations of RL algorithms process environment interactions and learning updates sequentially. As a consequence, when such implementations are deployed in the real world, they may make decisions based on significantly delayed observations and not act responsively. Asynchronous learning has been proposed to solve this issue, but no systematic comparison between sequential and asynchronous reinforcement learning was conducted using real-world environments. In this work, we set up two vision-based tasks with a robotic arm, implement an asynchronous learning system that extends a previous architecture, and compare sequential and asynchronous reinforcement learning across different action cycle times, sensory data dimensions, and mini-batch sizes. Our experiments show that when the time cost of learning updates increases, the action cycle time in sequential implementation could grow excessively long, while the asynchronous implementation can always maintain an appropriate action cycle time. Consequently, when learning updates are expensive, the performance of sequential learning diminishes and is outperformed by asynchronous learning by a substantial margin. Our system learns in real-time to reach and track visual targets from pixels within two hours of experience and does so directly using real robots, learning completely from scratch. Our code is available at: https://github.com/YufengYuan/ur5_async_r1.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811771","Alberta Machine Intelligence Institute (Amii); Natural Sciences and Engineering Research Council (NSERC) of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811771","","Visualization;Systematics;Costs;Reinforcement learning;Computer architecture;Robot sensing systems;Real-time systems","learning (artificial intelligence);mobile robots;robot vision","asynchronous reinforcement learning;real-world reinforcement learning;learning updates;RL algorithms process environment interactions;real-world environments;asynchronous learning system;different action cycle times;sequential implementation;asynchronous implementation;appropriate action cycle time;sequential learning","","","","29","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Black-Box Adversarial Attack for Robustness Improvement","S. Sarkar; A. R. Babu; S. Mousavi; S. Ghorbanpour; V. Gundecha; R. L. Gutierrez; A. Guillen; A. Naug","Hewlett Packard Enterprise, USA; Hewlett Packard Enterprise, USA; Hewlett Packard Enterprise, USA; Hewlett Packard Enterprise, USA; Hewlett Packard Enterprise, USA; Hewlett Packard Enterprise, USA; Hewlett Packard Enterprise, USA; Hewlett Packard Enterprise, USA","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","8","We propose a Reinforcement Learning (RL) based adversarial Black-box attack (RLAB) that aims at adding minimum distortion to the input iteratively to deceive image classification models. The RL agent learns to identify highly sensitive regions in the input's feature space to add distortions to induce misclassification with minimum steps and L2 norm. The agent also selectively removes noises introduced at earlier steps in the iteration, which has less impact on the model at a given state. This novel dual-action method is equivalent to doing a deep tree search to add noises without an exhaustive search, leading to the faster generation of an optimum adversarial sample. This black-box method focuses on naturally occurring distortion to effectively measure the robustness of models, a key element of trustworthiness. The proposed method beats existing heuristic based state-of-the-art black-box adversarial attacks on metrics such as the number of queries, L2 norm, and success rate on ImageNet and CIFAR-10 datasets. For the ImageNet dataset, the average number of queries achieved by the proposed method for ResNet-50, Inception-V3, and VGG-16 models are 42%, 32%, and 31 % better than the popular “Square Attack”. Furthermore, retraining the model with adversarial samples significantly improved robustness when evaluated on benchmark datasets such as CIFAR-10-C with the metrics of adversarial error and mean corruption error (mCE). Demo: https://tinyurl.com/yr8f7x9t","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260607","","Training;Computer aided software engineering;Closed box;Reinforcement learning;Benchmark testing;Distortion;Robustness","convolutional neural nets;deep learning (artificial intelligence);image classification;learning (artificial intelligence);reinforcement learning;tree searching","adversarial Black-box attack;adversarial error;adversarial samples;black-box adversarial attack;black-box method;deep tree search;earlier steps;exhaustive search;feature space;heuristic based state-of-the-art black-box adversarial attacks;highly sensitive regions;image classification models;iteration;minimum distortion;minimum steps;novel dual-action method;optimum adversarial sample;popular Square Attack;Reinforcement learning;Reinforcement Learning;RL agent;robustness improvement;VGG-16 models","","","","44","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Safe DNN-type Controller Synthesis for Nonlinear Systems via Meta Reinforcement Learning","H. Zhao; X. Zeng; N. Qi; Z. Yang; Z. Zeng","Shanghai Key Lab of Trustworthy Computing, East China Normal University, Shanghai, China; School of Computer and Information Science, Southwest University, Chongqing, China; Shanghai Key Lab of Trustworthy Computing, East China Normal University, Shanghai, China; Shanghai Key Lab of Trustworthy Computing, East China Normal University, Shanghai, China; Chengdu Institute of Computer Applications, Chinese Academy of Sciences, Chengdu, China","2023 60th ACM/IEEE Design Automation Conference (DAC)","15 Sep 2023","2023","","","1","6","There is a pressing need to synthesize provable safety controllers for nonlinear systems as they are embedded in many safety-critical applications. In this paper, we propose a safe Meta Reinforcement Learning (Meta-RL) approach to synthesize deep neural network (DNN) controllers for nonlinear systems subject to safety constraints. Our approach incorporates two phases: Meta-RL for training the controller network, and formal safety verification based on polynomial optimization solving. In the training phase, we provide a training framework which pre-trains a unified meta-initial controller for control systems by meta-learning. An important benefit of the proposed Meta-RL approach lies in that it is much more effective and succeeds in more controller training tasks compared with existing typical RL methods, e.g., Deep Deterministic Policy Gradient (DDPG). To formally verify the safety properties of the closed-loop system with the learned controller, we develop a verification procedure by using polynomial inclusion computation in combination with barrier certificate generation. Experiments on a set of benchmarks, including systems with dimension up to 12, demonstrate the effectiveness and applicability of our method.","","979-8-3503-2348-1","10.1109/DAC56929.2023.10247837","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247837","formal verification;controller synthesis;reinforcement learning;meta learning","Training;Metalearning;Reinforcement learning;Artificial neural networks;Pressing;Benchmark testing;Safety","deep learning (artificial intelligence);gradient methods;polynomials;reinforcement learning","closed-loop system;control systems;controller network;controller training tasks;deep deterministic policy gradient;deep neural network controllers;formal safety verification;learned controller;meta-learning;Meta-RL approach;nonlinear systems;provable safety controllers;safe DNN-type controller synthesis;safe meta reinforcement learning approach;safety constraints;safety properties;safety-critical applications;training framework;training phase;unified meta-initial controller","","","","16","IEEE","15 Sep 2023","","","IEEE","IEEE Conferences"
"An Effective Method for Satellite Mission Scheduling Based on Reinforcement Learning","X. Bao; S. Zhang; X. Zhang","School of Electrical and Information Engineering, Tianjin University, Tianjin, china; School of Electrical and Information Engineering, Tianjin University, Tianjin, china; School of Electrical and Information Engineering, Tianjin University, Tianjin, china","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","4037","4042","With the increasing number of satellites in orbit and the growing observation missions, how to make an allocation scheme with the maximization of total profit effectively has become increasingly important. In this paper, an effective method based on Reinforcement Learning is proposed to solve satellite mission scheduling problem, in which the arrival missions are arranged immediately without waiting all missions collected. Firstly, a mathematical model based on Markov Decision Process is established, whose goal is to find an optimal policy to maximize the accumulated reward. Then, Asynchronous Advantage Actor-Critic algorithm with neural network is used to assign missions to different satellites. The simulation experiments with comparison to first come first service algorithm and genetic algorithm are conducted, which demonstrates that the proposed method performs well with respect to real-time speed and solution quality.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327581","satellite mission scheduling;Markov Decision Process;Reinforcement Learning","Satellites;Task analysis;Scheduling;Reinforcement learning;Markov processes;Heuristic algorithms;Resource management","aerospace computing;artificial satellites;genetic algorithms;learning (artificial intelligence);Markov processes;scheduling","reinforcement learning;total profit;satellite mission scheduling problem;arrival missions;mathematical model;Markov decision process;asynchronous advantage actor-critic algorithm;first come first service algorithm;genetic algorithm","","","","15","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Sim-to-Real Model-Based and Model-Free Deep Reinforcement Learning for Tactile Pushing","M. Yang; Y. Lin; A. Church; J. Lloyd; D. Zhang; D. A. W. Barton; N. F. Lepora","Department of Engineering Mathematics, Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics, Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics, Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics, Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics, Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics, Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics, Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.","IEEE Robotics and Automation Letters","24 Jul 2023","2023","8","9","5480","5487","Object pushing presents a key non-prehensile manipulation problem that is illustrative of more complex robotic manipulation tasks. While deep reinforcement learning (RL) methods have demonstrated impressive learning capabilities using visual input, a lack of tactile sensing limits their capability for fine and reliable control during manipulation. Here we propose a deep RL approach to object pushing using tactile sensing without visual input, namely tactile pushing. We present a goal-conditioned formulation that allows both model-free and model-based RL to obtain accurate policies for pushing an object to a goal. To achieve real-world performance, we adopt a sim-to-real approach. Our results demonstrate that it is possible to train on a single object and a limited sample of goals to produce precise and reliable policies that can generalize to a variety of unseen objects and pushing scenarios without domain randomization. We experiment with the trained agents in harsh pushing conditions, and show that with significantly more training samples, a model-free policy can outperform a model-based planner, generating shorter and more reliable pushing trajectories despite large disturbances. The simplicity of our training environment and effective real-world performance highlights the value of rich tactile information for fine manipulation.","2377-3766","","10.1109/LRA.2023.3295236","Award from the Leverhulme Trust on ‘A biomimetic forebrain for robot touch’(grant numbers:RL-2016-39); EPSRC Doctoral Training Partnership; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10182274","Force and tactile sensing;dexterous manipulation;reinforcement learning","Robot sensing systems;Data models;Training;Task analysis;Robots;Reliability;Reinforcement learning","control engineering computing;deep learning (artificial intelligence);manipulators;reinforcement learning;tactile sensors","accurate policies;deep RL approach;fine control;fine manipulation;goal-conditioned formulation;harsh pushing conditions;model-based planner;model-free deep reinforcement learning;model-free policy;nonprehensile manipulation problem;object pushing;pushing trajectories;reliable control;robotic manipulation tasks;sim-to-real approach;sim-to-real model-based;tactile information;tactile pushing;tactile sensing;visual input","","","","36","IEEE","13 Jul 2023","","","IEEE","IEEE Journals"
"Optimal Motion Planning in Unknown Workspaces Using Integral Reinforcement Learning","P. Rousseas; C. P. Bechlioulis; K. J. Kyriakopoulos","School of Mechanical Engineering, Control Systems Laboratory, National Technical University of Athens, Athens, Greece; Department of Electrical and Computer Engineering, University of Patras, Patra, Greece; School of Mechanical Engineering, Control Systems Laboratory, National Technical University of Athens, Athens, Greece","IEEE Robotics and Automation Letters","9 Jun 2022","2022","7","3","6926","6933","A novel motion planning scheme for optimal navigation in unknown workspaces is proposed in this letter. Based upon the Artificial Harmonic Potential Fields (AHPFs) theory, a robust framework for provably correct (i.e., safe and globally convergent) navigation is enhanced through Integral Reinforcement Learning (IRL)1 to obtain a provably complete solution for optimal motion planning in unknown workspaces. Our method aims at bridging the gap between the control theoretic framework of mathematical rigor, with the data-driven Reinforcement Learning (RL) paradigm, while preserving the strong traits of each approach. Finally, it is compared against an RRT$^\star$ method to asses the optimality of the final results in a multiply connected synthetic workspace.","2377-3766","","10.1109/LRA.2022.3178788","European Union’s Horizon 2020 Research and Innovation Program(grant numbers:883484); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785456","Motion and path planning;optimization and optimal control;reinforcement learning","Robots;Navigation;Planning;Task analysis;Costs;Reinforcement learning;Minimization","approximation theory;collision avoidance;learning (artificial intelligence);mobile robots;path planning","optimal motion planning;unknown workspaces;Integral Reinforcement Learning;motion planning scheme;optimal navigation;Artificial Harmonic Potential Fields theory;provably complete solution;control theoretic framework;data-driven Reinforcement Learning paradigm;synthetic workspace","","","","42","IEEE","30 May 2022","","","IEEE","IEEE Journals"
"An Energy-Efficient Network-on-Chip Design using Reinforcement Learning","H. Zheng; A. Louri",George Washington University; George Washington University,"2019 56th ACM/IEEE Design Automation Conference (DAC)","22 Aug 2019","2019","","","1","6","The design space for energy-efficient Network-on-Chips (NoCs) has expanded significantly comprising a number of techniques. The simultaneous application of these techniques to yield maximum energy efficiency requires the monitoring of a large number of system parameters which often results in substantial engineering efforts and complicated control policies. This motivates us to explore the use of reinforcement learning (RL) approach that automatically learns an optimal control policy to improve NoC energy efficiency. First, we deploy power-gating (PG) and dynamic voltage and frequency scaling (DVFS) to simultaneously reduce both static and dynamic power. Second, we use RL to automatically explore the dynamic interactions among PG, DVFS, and system parameters, learn the critical system parameters contained in the router and cache, and eventually evolve optimal per-router control policies that significantly improve energy efficiency. Moreover, we introduce an artificial neural network (ANN) to efficiently implement the large state-action table required by RL. simulation results using parsec benchmark show that the proposed rl approach improves power consumption by 26%, while improving system performance by 7%, as compared to a combined PG and DVFS design without RL. additionally, the ann design yields 67% area reduction, as compared to a conventional RL implementation.","0738-100X","978-1-4503-6725-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807084","","Energy efficiency;Reinforcement learning;Monitoring;Artificial neural networks;Network-on-chip;Voltage control;Routing","energy conservation;learning (artificial intelligence);network-on-chip;neural nets;optimisation;power aware computing","maximum energy efficiency;reinforcement learning approach;NoC energy efficiency;artificial neural network;rl approach;power consumption;system performance;DVFS design;dynamic voltage and frequency scaling;RL implementation;energy-efficient network-on-chip design;power-gating;optimal per-router control policies","","","","25","","22 Aug 2019","","","IEEE","IEEE Conferences"
"Model-Based Reinforcement Learning for Position Control of Continuum Manipulators Actuated by Pneumatic Artificial Muscles","Z. Yan; X. Gao; Y. Li; P. Zhao; Z. Deng","Center of Ultra-Precision Optoelectronic Instrumentation Engineering Harbin Institute of Technology, Harbin, China; Center of Ultra-Precision Optoelectronic Instrumentation Engineering Harbin Institute of Technology, Harbin, China; Center of Ultra-Precision Optoelectronic Instrumentation Engineering Harbin Institute of Technology, Harbin, China; Center of Ultra-Precision Optoelectronic Instrumentation Engineering Harbin Institute of Technology, Harbin, China; Center of Ultra-Precision Optoelectronic Instrumentation Engineering Harbin Institute of Technology, Harbin, China","2023 IEEE International Conference on Mechatronics and Automation (ICMA)","22 Aug 2023","2023","","","1032","1037","The topic of position control for continuum manipulators (CMs) remains open and yet to be well explored and developed. Current applications of CMs focus on employing static or quasi-dynamic controllers built upon kinematic models or linearity in the joint space, resulting in a loss of the rich dynamics of a system. This paper presents a model-based reinforcement learning scheme for position control of a class of CMs with strong nonlinearity and input coupling, which includes a probabilistic dynamics model as the dynamic forward model and a policy update approach for the closed-loop policy. The effectiveness of the scheme is verified on a dual-segment CM actuated by pneumatic artificial muscles, and the experimental results confirm that such scheme can obtain good results with only a limited number of samples and interactions.","2152-744X","979-8-3503-2084-8","10.1109/ICMA57826.2023.10215794","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Natural Science Foundation of Heilongjiang Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10215794","Dynamic control;Position control;Reinforcement learning;Continuum manipulators","Couplings;Artificial muscles;Analytical models;Mechatronics;Position control;Linearity;Reinforcement learning","closed loop systems;learning (artificial intelligence);manipulator dynamics;manipulators;pneumatic actuators;position control;reinforcement learning","CMs focus;continuum manipulators actuated;dynamic forward model;input coupling;kinematic models;model-based reinforcement learning scheme;pneumatic artificial muscles;position control;probabilistic dynamics model;quasidynamic controllers;rich dynamics","","","","19","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Dynamic Management of Key States for Reinforcement Learning-assisted Garbage Collection to Reduce Long Tail Latency in SSD","W. Kang; S. Yoo","Computer Science and Engineering, Seoul National University, Republic of Korea; Computer Science and Engineering, Seoul National University, Republic of Korea","2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)","20 Sep 2018","2018","","","1","6","Garbage collection (GC) is one of main causes of the long-tail latency problem in storage systems. Long-tail latency due to GC is more than 100 times greater than the average latency at the 99th percentile. Therefore, due to such a long tail latency, real-time systems and quality-critical systems cannot meet the system requirements. In this study, we propose a novel key state management technique of reinforcement learning-assisted garbage collection. The purpose of this study is to dynamically manage key states from a significant number of state candidates. Dynamic management enables us to utilize suitable and frequently recurring key states at a small area cost since the full states do not have to be managed. The experimental results show that the proposed technique reduces by 22-25% the long-tail latency compared to a state-of-the-art scheme with real-world workloads.","","978-1-5386-4114-9","10.1109/DAC.2018.8465934","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8465934","Flash storage system;SSD;garbage collection;long-tail latency","Flash memories;Real-time systems;Embedded systems;Memory management;Computer science;Learning (artificial intelligence);Organizations","learning (artificial intelligence);storage management;systems analysis","dynamic management;reinforcement learning-assisted garbage collection;long-tail latency problem;storage systems;real-time systems;quality-critical systems;system requirements","","","","14","IEEE","20 Sep 2018","","","IEEE","IEEE Conferences"
"Robust Attitude Controller Designation of Rocket Vertical Flight Control via Deep Reinforcement Learning Algorithm","C. Jia; X. Huang; Y. Lin; M. Feng; W. Zhai; X. Liu","National Key Laboratory of Aerospace Intelligent Control Technology, Beijing Aerospace Automatic Control Institute, Beijing, China; National Key Laboratory of Aerospace Intelligent Control Technology, Beijing Aerospace Automatic Control Institute, Beijing, China; National Key Laboratory of Aerospace Intelligent Control Technology, Beijing Aerospace Automatic Control Institute, Beijing, China; National Key Laboratory of Aerospace Intelligent Control Technology, Beijing Aerospace Automatic Control Institute, Beijing, China; National Key Laboratory of Aerospace Intelligent Control Technology, Beijing Aerospace Automatic Control Institute, Beijing, China; National Key Laboratory of Aerospace Intelligent Control Technology, Beijing Aerospace Automatic Control Institute, Beijing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","4654","4660","The designation of attitude controller of the aircrafts during the flight process needs to take a variety of factors into consideration, which is difficult to deal with the traditional control algorithms. Reinforcement learning algorithm is considered as a potential method, but the high cost of flight failure and the difference between the simulation environment and the real environment are two obstacles which is hard to deal with. To solve these problems, in this paper, we build a training environment and algorithm to train a network attitude controller which is robust to the environment noise, parameter disturbances and flight task changes. The simulation experiment and a real flight experiment are carried out to prove the effectiveness of our method.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055795","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055795","robust attitude control;reinforcement learning;noise and task-changes","Training;Rockets;Attitude control;Atmospheric modeling;Simulation;Process control;Reinforcement learning","aircraft control;attitude control;control system synthesis;deep learning (artificial intelligence);reinforcement learning;robust control","deep reinforcement learning algorithm;environment noise;flight experiment;flight failure;flight process;flight task changes;network attitude controller;robust attitude controller designation;rocket vertical flight control;simulation environment;traditional control algorithms;training environment","","","","15","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"DAGA: Dynamics Aware Reinforcement Learning With Graph-Based Rapid Adaptation","J. Ji; B. Nie; Y. Gao","ParisTech Elite Institute of Technology, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; MoE Key Lab of Artificial Intelligence and AI Institute of Shanghai Jiao Tong University, Shanghai, China","IEEE Robotics and Automation Letters","6 Mar 2023","2023","8","4","2189","2196","Reinforcement learning (RL) has achieved some notable accomplishments in robotics. However, one of the major challenges when applying RL in the real world is that unexpected disturbances in local dynamics could lead to failures of policies at test time. It is necessary to learn a model capable of adapting to various environments with different dynamics. In this paper, a Dynamic-Aware reinforcement learning model with graph-based rapid adaptation (DAGA) is proposed to address these challenges. DAGA encodes the dynamic features from a few interactions and guides the policy with an environment embedding. To encourage the embedding to capture variations in dynamics, we present an objective function based on forward prediction and environment similarity. The proposed model enables the robot to generalize with a wide range of transition dynamics resulted from different hardware parameters. Experiments in robot locomotion and manipulation tasks show that DAGA outperforms existing baselines in both better sample efficiency and generalization. DAGA has the potential to deploy the RL policy in realistic and changing environments.","2377-3766","","10.1109/LRA.2023.3249632","National Natural Science Foundation of China(grant numbers:92248303); Shanghai Municipal Science and Technology Major Project(grant numbers:2021SHZDZX0102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054096","Reinforcement learning;transfer learning","Robots;Task analysis;Adaptation models;Hardware;Data models;Reinforcement learning;Vehicle dynamics","graph theory;manipulators;mobile robots;reinforcement learning","DAGA;dynamic features;dynamic-aware reinforcement learning model;environment embedding;graph-based rapid adaptation;local dynamics;manipulation tasks;realistic changing environments;RL policy;robot locomotion;transition dynamics;unexpected disturbances","","","","37","IEEE","27 Feb 2023","","","IEEE","IEEE Journals"
"Reinforcement Learning of Action and Query Policies With LTL Instructions Under Uncertain Event Detector","W. Hatanaka; R. Yamashina; T. Matsubara","Digital Strategy Division, RICOH Company, Ltd., Tokyo, Japan; Digital Strategy Division, RICOH Company, Ltd., Tokyo, Japan; Division of Information Science, Graduate School of Science and Technology, Nara Institute of Science and Technology (NAIST), Nara, Japan","IEEE Robotics and Automation Letters","19 Sep 2023","2023","8","11","7010","7017","Reinforcement learning (RL) with linear temporal logic (LTL) objectives can allow robots to carry out symbolic event plans in unknown environments. Most existing methods assume that the event detector can accurately map environmental states to symbolic events; however, uncertainty is inevitable for real-world event detectors. Such uncertainty in an event detector generates multiple branching possibilities on LTL instructions, confusing action decisions. Moreover, the queries to the uncertain event detector, necessary for the task's progress, may increase the uncertainty further. To cope with those issues, we propose an RL framework, Learning Action and Query over Belief LTL (LAQBL), to learn an agent that can consider the diversity of LTL instructions due to uncertain event detection while avoiding task failure due to the unnecessary event-detection query. Our framework simultaneously learns 1) an embedding of belief LTL, which is multiple branching possibilities on LTL instructions using a graph neural network, 2) an action policy, and 3) a query policy that decides whether or not to query for the event detector. Simulations in a 2D grid world and image-input robotic inspection environments show that our method successfully learns actions to follow LTL instructions even with uncertain event detectors.","2377-3766","","10.1109/LRA.2023.3313969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10246967","Reinforcement learning;planning under uncertainty","Task analysis;Detectors;Uncertainty;Robots;Event detection;Reinforcement learning;Proposals","graph neural networks;graph theory;learning (artificial intelligence);reinforcement learning;temporal logic","action policy;Belief LTL;belief LTL;linear temporal logic objectives;LTL instructions;multiple branching possibilities;query policy;real-world event detectors;reinforcement learning;symbolic event plans;symbolic events;uncertain event detection;uncertain event detector;unnecessary event-detection query","","","","27","IEEE","11 Sep 2023","","","IEEE","IEEE Journals"
"Distributed Dynamic Economic Dispatch of Multi-Microgrid System Based on Multi-Agent Deep Reinforcement Learning","W. Xu; X. Wang; K. Chen; H. Tan","School of Information Science and Engineering, Harbin Institude of Technology, Weihai, China; School of Information Science and Engineering, Harbin Institude of Technology, Weihai, China; School of Information Science and Engineering, Harbin Institude of Technology, Weihai, China; School of Information Science and Engineering, Harbin Institude of Technology, Weihai, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","5301","5306","The distributed economic dispatch of multi-microgrid (MMG) is an essential aspect of the operational planning of microgrids (MGs). We propose an approach to maximize economic benefit among MGs through dynamic dispatch based on multi-agent deep reinforcement learning (MADRL). First, a dynamic economic dispatch model of the MMG system in off-grid operation is formulated. The operating cost of equipment, the losses caused by load shedding and renewable energy curtailment are considered. Next, to cope with the high dimension of dynamic economic dispatch problem of MMG due to a quantity of dispatchable equipment, we propose an algorithm based on mean-field multi-agent reinforcement learning (MFMARL), which can flexibly adjust the uncertainty of the generation and load. The communication cost is reduced through the communication of mean action, and the surplus energy is utilized. Simulation results show the effectiveness of the proposed algorithm.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10054776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054776","multi-microgrid system;dynamic economic dispatch;mean-field theory;multi-agent deep reinforcement learning","Deep learning;Renewable energy sources;Costs;Uncertainty;Heuristic algorithms;Simulation;Reinforcement learning","deep learning (artificial intelligence);distributed power generation;load shedding;multi-agent systems;power generation dispatch;power generation economics;reinforcement learning","dispatchable equipment;distributed dynamic economic dispatch;distributed economic dispatch;dynamic dispatch;dynamic economic dispatch model;dynamic economic dispatch problem;economic benefit;mean-field multiagent reinforcement learning;microgrids;MMG system;multiagent deep reinforcement learning;multimicrogrid system;off-grid operation;operational planning;renewable energy curtailment","","","","14","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Augmenting Vision-Based Grasp Plans for Soft Robotic Grippers using Reinforcement Learning","V. Vatsal; N. George","TCS Research & Innovation, Tata Consultancy Services, Bengaluru, India; TCS Research & Innovation, Tata Consultancy Services, Bengaluru, India","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","1904","1909","Vision-based techniques for grasp planning of robotic end-effectors have been successfully deployed in pick-and-place tasks. However, for computing the optimal grasp, they assume the gripper to be of a rigid parallel-jaw type or a single-point vacuum suction-based design. Planners for soft robotic grippers have used learning from demonstration or heuristics that rely on the compliance of the gripper to achieve a grasp. We demonstrate a model-free reinforcement learning (RL) approach that modifies vision-based grasp plans generated for parallel-jaw grippers and adapts them to grasping with a four-fingered soft gripper. The observed state of the RL model is comprised of the grasp plans from the vision module, the deformation of the fingers, and the pose of the end-effector. The RL model controls each finger separately, discovering grasp synergies during training. This approach is compared to a baseline grasp synergy where all four fingers simultaneously enclose the object. In simulation, we achieve a pick-and-place success rate of 58.4% with the RL model for top-down grasping, compared to 43.2% with the baseline grasp synergy.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926580","","Training;Adaptation models;Visualization;Computational modeling;Fingers;Grasping;Reinforcement learning","dexterous manipulators;end effectors;grippers;reinforcement learning;robot vision","RL model;vision-based grasp plans;soft robotic grippers;robotic end-effectors;parallel-jaw type;single-point vacuum suction-based design;model-free reinforcement learning approach;parallel-jaw grippers;pick-and-place success rate;four-fingered soft gripper","","","","28","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"Interactive Reinforcement Learning based Assistive Robot for the Emotional Support of Children","E. Gamborino; L. -C. Fu","NTU Center for Artificial Intelligence & Advanced Robotics, National Taiwan University, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan","2018 18th International Conference on Control, Automation and Systems (ICCAS)","13 Dec 2018","2018","","","708","713","In this work, we challenge the Interactive Reinforcement Learning paradigm by implementing an interactive action-planning module developed with the goal of exploring the feasibility of using a robot to socially engage with children and improve their mood. Facial features of the child are captured and processed, determining their emotional reaction to a behavior performed by the robot. Then, these emotions are classified as affective states in a multi-dimensional model. Leveraging the expertise of a human trainer, the action-planning module interactively learns those actions that are the most appropriate to perform when the child subject is in a specific affective state. To validate the usefulness of the proposed methodology, we evaluated the impact of the robot on elementary school aged children. Our findings show that using this methodology, the robot is able not only to learn in real time from the human trainer through interactions, but also that performing these social actions a robot can improve the mood of children.","","978-89-93215-16-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8571998","Human-Robot Interaction;Socially Assistive Robot;Interactive Reinforcement Learning","Robots;Mood;Pediatrics;Hospitals;Tools","assisted living;interactive systems;learning (artificial intelligence);robots","social actions;assistive robot;emotional support;interactive action-planning module;facial features;emotional reaction;affective states;multidimensional model;human trainer;child subject;specific affective state;elementary school aged children;interactive reinforcement learning paradigm","","","","20","","13 Dec 2018","","","IEEE","IEEE Conferences"
"Economic Battery Storage Dispatch with Deep Reinforcement Learning from Rule-Based Demonstrations","M. Sage; M. Staniszewski; Y. F. Zhao","Department of Mechanical Engineering, McGill University, Montreal, Canada; Siemens Energy Canada Limited, Montreal, Canada; Department of Mechanical Engineering, McGill University, Montreal, Canada","2023 International Conference on Control, Automation and Diagnosis (ICCAD)","23 Jun 2023","2023","","","1","6","The application of deep reinforcement learning algorithms to economic battery dispatch problems has significantly increased recently. However, optimizing battery dispatch over long horizons can be challenging due to delayed rewards. In our experiments we observe poor performance of popular actor-critic algorithms when trained on yearly episodes with hourly resolution. To address this, we propose an approach extending soft actor-critic (SAC) with learning from demonstrations. The special feature of our approach is that, due to the absence of expert demonstrations, the demonstration data is generated through simple, rule-based policies. We conduct a case study on a grid-connected microgrid and use if-then-else statements based on the wholesale price of electricity to collect demonstrations. These are stored in a separate replay buffer and sampled with linearly decaying probability along with the agent's own experiences. Despite these minimal modifications and the imperfections in the demonstration data, the results show a drastic performance improvement regarding both sample efficiency and final rewards. We further show that the proposed method reliably outperforms the demonstrator and is robust to the choice of rule, as long as the rule is sufficient to guide early training into the right direction.","2767-9896","979-8-3503-4707-4","10.1109/ICCAD57653.2023.10152299","McGill Engineering Doctoral Award (MEDA) and MITACS(grant numbers:IT13369); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10152299","","Training;Economics;Deep learning;Reinforcement learning;Microgrids;Benchmark testing;Batteries","deep learning (artificial intelligence);distributed power generation;learning (artificial intelligence);probability;reinforcement learning","agent;deep reinforcement learning algorithms;delayed rewards;demonstration data;drastic performance improvement;economic battery dispatch problems;economic battery storage dispatch;expert demonstrations;final rewards;grid-connected microgrid;hourly resolution;long horizons;popular actor-critic algorithms;rule-based demonstrations;rule-based policies;separate replay buffer;soft actor-critic;yearly episodes","","","","26","IEEE","23 Jun 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Control for Uncertain Robotic Manipulator Trajectory Tracking","A. Liu; B. Zhang; W. Chen; Y. Luo; S. Fang; O. Zhang; Z. Liu; Z. Wang; J. Liu","Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, P. R. China; The Suihua Power Supply Company, State Grid Heilongjiang Electric Power Company Limited, Suihua, China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, P. R. China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, P. R. China; The Suihua Power Supply Company, State Grid Heilongjiang Electric Power Company Limited, Suihua, China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, P. R. China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, P. R. China; Space Control and Inertial Technology Research Center, Harbin Institute of Technology, Harbin, China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, P. R. China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","2740","2745","This paper investigates the trajectory tracking control method for a robotic manipulator with uncertainties. A compound controller combining a traditional control law with deep reinforcement learning is developed to improve tracking accuracy and adaptability. The model-based control method is able to increase sampling efficiency for learning control strategy. The introduction of deep reinforcement learning based on soft actor-critic structure and Lyapunov function constrain enables the system to compensate unknown uncertainties and remain stable. Eventually, a 3-DOF manipulator is used to show the effectiveness of the proposed controller. Comparative simulation results demonstrate that the compound controller acquires higher tracking accuracy than the pure model-based control method.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055583","Harbin Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055583","Deep reinforcement learning;trajectory tracking control;uncertain robotic manipulators;3-DOF","Deep learning;Uncertainty;Trajectory tracking;Simulation;Reinforcement learning;Manipulators;Stability analysis","adaptive control;control system synthesis;deep learning (artificial intelligence);learning (artificial intelligence);learning systems;Lyapunov methods;manipulators;nonlinear control systems;position control;reinforcement learning;trajectory control","adaptability;compound controller;control strategy;deep reinforcement learning;higher tracking accuracy;pure model-based control method;soft actor-critic structure;traditional control law;trajectory tracking control method;uncertain robotic manipulator trajectory tracking;unknown uncertainties","","","","18","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
