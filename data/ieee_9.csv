"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Reinforcement Learning for Service Restoration Algorithms in Distribution Networks","P. A. Parra; D. Celeita; G. Ramos; W. Martínez; G. Chaffey","Department of Electrical, and Electronics Engineering, Universidad de los Andes, Bogotá, Colombia; School of Engineering, Science and Technology, Universidad del Rosario, Bogotá, Colombia; Department of Electrical, and Electronics Engineering, Universidad de los Andes, Bogotá, Colombia; Department of Electrical Engineering (ESAT), KU Leuven, EnergyVille, Belgium; Department of Electrical Engineering (ESAT), KU Leuven, EnergyVille, Belgium","2022 IEEE Industry Applications Society Annual Meeting (IAS)","17 Nov 2022","2022","","","1","6","Modern Distribution Networks (DNs) are highly susceptible to faults, which affects their dependability and reliability. The operation complexity is crucial when DNs include critical infrastructure such as distributed energy resources, storage systems, charging stations and decentralized supply. FLISR (Fault Location, Isolation and Service Restoration) relies on advanced methodologies which aim to improve the quality of service with automated algorithms. This paper proposes a novel Service Restoration approach to automatically assist DNs resupply the out-of-service unfaulted customers after an event. The approach integrates Reinforcement Learning techniques in a co-simulation environment with OpenDSS. The results and contribution of this study could improve power supply quality and reliability of DNs throughout advanced Service Restoration (SR) methodologies. The idea is validated in real-time simulation to offer a performance assessment after training with co-simulated data.","2576-702X","978-1-6654-7815-1","10.1109/IAS54023.2022.9939804","KU Leuven; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9939804","Distribution automation;Distribution networks;Fault Location;Service restoration;Reinforcement Learning;Q-Learning","Training;Scalability;Power quality;Reinforcement learning;Distribution networks;Quality of service;Production","fault location;power distribution faults;power distribution reliability;power engineering computing;power supply quality;power system restoration;reinforcement learning","advanced Service Restoration methodologies;critical infrastructure;decentralized supply;distributed energy resources;Fault Location;Modern Distribution Networks;operation complexity;out-of-service unfaulted customers;power supply quality;quality of service;Reinforcement Learning techniques;Service Restoration algorithms;storage systems","","","","21","IEEE","17 Nov 2022","","","IEEE","IEEE Conferences"
"Predictive Maintenance for Edge-Based Sensor Networks: A Deep Reinforcement Learning Approach","K. S. Hoong Ong; D. Niyato; C. Yuen","School of Computer Science and Engineering, Nanyang Technological University Singapore; School of Computer Science and Engineering, Nanyang Technological University Singapore; Engineering Product Development, Singapore University of Technology and Design","2020 IEEE 6th World Forum on Internet of Things (WF-IoT)","13 Oct 2020","2020","","","1","6","Failure of mission-critical equipment interrupts production and results in monetary loss. The risk of unplanned equipment downtime can be minimized through Predictive Maintenance of revenue generating assets to ensure optimal performance and safe operation of equipment. However, the increased sensorization of the equipment generates a data deluge, and existing machine-learning based predictive model alone becomes inadequate for timely equipment condition predictions. In this paper, a model-free Deep Reinforcement Learning algorithm is proposed for predictive equipment maintenance from an equipment-based sensor network context. Within each equipment, a sensor device aggregates raw sensor data, and the equipment health status is analyzed for anomalous events. Unlike traditional black-box regression models, the proposed algorithm self-learns an optimal maintenance policy and provides actionable recommendation for each equipment. Our experimental results demonstrate the potential for broader range of equipment maintenance applications as an automatic learning framework.","","978-1-7281-5503-6","10.1109/WF-IoT48130.2020.9221098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9221098","","Productivity;Industries;Schedules;Mission critical systems;Reinforcement learning;Predictive models;Prediction algorithms","condition monitoring;data aggregation;learning (artificial intelligence);maintenance engineering;neural nets;optimisation;production engineering computing;production equipment;sensor fusion","edge-based sensor networks;deep reinforcement learning algorithm;predictive equipment maintenance;sensor data aggregation;equipment health status;optimal maintenance policy;equipment-based sensor network","","8","","15","IEEE","13 Oct 2020","","","IEEE","IEEE Conferences"
"CPG-Based Hierarchical Locomotion Control for Modular Quadrupedal Robots Using Deep Reinforcement Learning","J. Wang; C. Hu; Y. Zhu","State Key Laboratory of Tribology, Department of Mechanical Engineering, Tsinghua University, Beijing, China; State Key Laboratory of Tribology, Department of Mechanical Engineering, Tsinghua University, Beijing, China; State Key Laboratory of Tribology, Department of Mechanical Engineering, Tsinghua University, Beijing, China","IEEE Robotics and Automation Letters","28 Jul 2021","2021","6","4","7193","7200","Modular robots have the potential for an unmatched ability to perform versatile and robust locomotion. However, designing effective and adaptive locomotion controllers for modular robots is challenging, resulting in a number of model-based methods that typically require various forms of prior knowledge. Deep reinforcement learning (DRL) provides a promising model-free approach for locomotion control by trial-and-error. However, current DRL methods often require extensive interaction data, hindering many possible applications. In this letter, a novel two-level hierarchical locomotion framework for modular quadrupedal robots is proposed. The approach combines a low-level central pattern generator (CPG)-based controller with a high-level neural network to learn a variety of locomotion tasks using DRL. The low-level CPG controller is pre-optimized to generate stable rhythmic walking gaits, while the high-level network is trained to modulate the CPG parameters for achieving task goals based on high-dimensional inputs, including the robot states and user commands. The proposed approach is employed on a simulated modular quadruped. With a limited amount of prior knowledge, the proposed method is demonstrated to be capable of learning a variety of locomotion skills such as velocity tracking, path following, and navigating to a target. Simulation results show that the proposed method can achieve higher sample efficiency than the model-free DRL method and are substantially more robust than the baseline methods to external disturbances and irregular terrain.","2377-3766","","10.1109/LRA.2021.3092647","National Natural Science Foundation of China(grant numbers:51922059,51775305); Beijing Natural Science Foundation(grant numbers:JQ19010); State Key Laboratory of Tribology(grant numbers:SKLT2020D22); National Youth Talent Support Program; State Key Laboratory of Mechanical System and Vibration(grant numbers:MSV202007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9465716","Modular robots;deep reinforcement learning;locomotion control","Robots;Task analysis;Legged locomotion;Reinforcement learning;Target tracking;Morphology;Solid modeling","control engineering computing;control system synthesis;deep learning (artificial intelligence);gait analysis;humanoid robots;legged locomotion;motion control;neurocontrollers","CPG-based hierarchical locomotion control;modular quadrupedal robots;deep reinforcement learning;adaptive locomotion controllers;two-level hierarchical locomotion framework;low-level central pattern generator-based controller;high-level neural network;high-dimensional inputs;model-free DRL method;velocity tracking;trial-and-error","","6","","27","IEEE","25 Jun 2021","","","IEEE","IEEE Journals"
"Improved deep reinforcement learning based convergence adjustment method for power flow calculation","H. Xu; Z. Yu; Q. Zheng; J. Hou; Y. Wei","China Electric Power Research Institute, Beijing, 100192, China; China Electric Power Research Institute, Beijing, 100192, China; Beijing University of Posts and Telecommunications, Beijing, China; China Electric Power Research Institute, Beijing, 100192, China; China Electric Power Research Institute, Beijing, 100192, China","The 16th IET International Conference on AC and DC Power Transmission (ACDC 2020)","15 Sep 2021","2020","2020","","1898","1903","The Power Flow Convergence (PFC) adjustment is an essential issue in the study of power system Operation State Calculation (OSC). Currently, as the grid structure expands markedly, the PFC adjustment is realized by adjusting the regional active and reactive power manually in most dispatching centres, which is tedious and personnel-experience oriented. Therefore, it is crucial to automate the PFC adjustment for efficiency and quality improvement. In this paper, an improved deep reinforcement learning (IDRL) based method and an action-executing strategy are proposed for the automation of the PFC adjustment. The optimal configuration scheme of the adjustable generators is derived from the trained online Q network for multi-load levels. Finally, the testing results on the IEEE-118 bus system demonstrate that the proposed method can obtain the convergent power flow state according to the given load level effectively.","","","10.1049/icp.2020.0175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537970","","","reactive power;load flow;deep learning (artificial intelligence);power engineering computing","power flow calculation;Power Flow Convergence adjustment;power system Operation State Calculation;PFC adjustment;regional active power;reactive power;improved deep reinforcement learning based method;convergent power flow state;IDRL;action-executing strategy;IEEE-118 bus system;online Q network","","","","","","15 Sep 2021","","","IET","IET Conferences"
"Adaptive Interleaved Reinforcement Learning: Robust Stability of Affine Nonlinear Systems With Unknown Uncertainty","J. Li; J. Ding; T. Chai; F. L. Lewis; S. Jagannathan","School of Information and Control Engineering, Liaoning Shihua University, Fushun, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; UTA Research Institute, The University of Texas at Arlington, Arlington, TX, USA; Department of Electrical and Computer Engineering, Missouri University of Science and Technology, Rolla, MO, USA","IEEE Transactions on Neural Networks and Learning Systems","5 Jan 2022","2022","33","1","270","280","This article investigates adaptive robust controller design for discrete-time (DT) affine nonlinear systems using an adaptive dynamic programming. A novel adaptive interleaved reinforcement learning algorithm is developed for finding a robust controller of DT affine nonlinear systems subject to matched or unmatched uncertainties. To this end, the robust control problem is converted into the optimal control problem for nominal systems by selecting an appropriate utility function. The performance evaluation and control policy update combined with neural networks approximation are alternately implemented at each time step for solving a simplified Hamilton–Jacobi–Bellman (HJB) equation such that the uniformly ultimately bounded (UUB) stability of DT affine nonlinear systems can be guaranteed, allowing for all realization of unknown bounded uncertainties. The rigorously theoretical proofs of convergence of the proposed interleaved RL algorithm and UUB stability of uncertain systems are provided. Simulation results are given to verify the effectiveness of the proposed method.","2162-2388","","10.1109/TNNLS.2020.3027653","National Key Research and Development Program of China(grant numbers:2018YFB1701104); National Natural Science Foundation of China(grant numbers:61525302,61833004,61673280,62073158); Xingliao Plan of Liaoning Province(grant numbers:XLYC1808001); Science and Technology Program of Liaoning Province(grant numbers:2020JH2/10500001); Open Project of Key Field Alliance of Liaoning Province(grant numbers:2019-KF-03-06); Project of Liaoning Shihua University(grant numbers:2018XJJ-005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9241753","Interleaved reinforcement learning;neural networks (NNs);robust control;uncertain systems","Uncertainty;Nonlinear systems;Robust control;Optimal control;Adaptive systems;Robust stability;Linear systems","adaptive control;closed loop systems;control system synthesis;dynamic programming;learning (artificial intelligence);Lyapunov methods;neurocontrollers;nonlinear control systems;nonlinear systems;optimal control;robust control;stability;uncertain systems","reinforcement learning algorithm;DT affine nonlinear systems subject;matched unmatched uncertainties;robust control problem;optimal control problem;nominal systems;appropriate utility function;performance evaluation;control policy update;neural networks approximation;simplified Hamilton-Jacobi-Bellman equation;unknown bounded uncertainties;uncertain systems;adaptive interleaved reinforcement learning;robust stability;unknown uncertainty;adaptive robust controller design;discrete-time affine nonlinear systems;adaptive dynamic programming","","22","","29","IEEE","28 Oct 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning-Enabled Resampling Particle Swarm Optimization for Sensor Relocation in Reconfigurable WSNs","M. Wang; X. Wang; K. Jiang; B. Fan","Hunan Province Key Laboratory for Ultra-Fast Micro/Nano Technology and Advanced Laser Manufacture, School of Electrical Engineering, University of South China, Hengyang, China; Hunan Province Key Laboratory for Ultra-Fast Micro/Nano Technology and Advanced Laser Manufacture, School of Electrical Engineering, University of South China, Hengyang, China; Hunan Province Key Laboratory for Ultra-Fast Micro/Nano Technology and Advanced Laser Manufacture, School of Electrical Engineering, University of South China, Hengyang, China; Hunan Province Key Laboratory for Ultra-Fast Micro/Nano Technology and Advanced Laser Manufacture, School of Electrical Engineering, University of South China, Hengyang, China","IEEE Sensors Journal","14 Apr 2022","2022","22","8","8257","8267","Aiming to maximize coverage performance and reduce the number of sensors deployed in the reconfigurable wireless sensor networks (RWSNs), in this paper, we first formulate a new cooperative sensing coverage control problem based on the confident information coverage model. Then, inspired by the reinforcement learning and resampling technology, a novel learning automata-based resampling particle swarm optimization (RPSOLA) algorithm is proposed to solve complex multi-peak optimization problem and optimize the cooperative sensing coverage control problem of RWSNs. Experimental results demonstrate that the RPSOLA considerably outperforms other three peer schemes, the RPSO, BASPSO and PSO, in terms of the convergence, coverage rate and sensor redundancy.","1558-1748","","10.1109/JSEN.2022.3160487","National Natural Science Foundation of China(grant numbers:61971215); Scientific Research Fund of Hunan Provincial Education Department(grant numbers:21A0276); Natural Science Foundation of Hunan Province(grant numbers:2020JJ4526); Hunan Province Key Laboratory for Ultra-Fast Micro/Nano Technology and Advanced Laser Manufacture(grant numbers:2018TP1041); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9737506","Reconfigurable wireless sensor networks;sensor relocation;resampling particle swarm optimization;learning automata","Sensors;Wireless sensor networks;Particle swarm optimization;Optimization;Sensor phenomena and characterization;Search problems;Learning automata","learning (artificial intelligence);learning automata;optimisation;particle swarm optimisation;sensor placement;telecommunication control;wireless sensor networks","reinforcement learning-enabled;sensor relocation;reconfigurable wsns;coverage performance;reconfigurable wireless sensor networks;RWSNs;sensing coverage control problem;confident information coverage model;resampling technology;novel learning automata-based;particle swarm optimization algorithm;RPSOLA;complex multipeak optimization problem;coverage rate;sensor redundancy","","8","","28","IEEE","17 Mar 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Rumor Blocking Approach in Directed Social Networks","Q. He; Y. Lv; X. Wang; M. Huang; Y. Cai","College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; Northeastern University, the State Key Laboratory of Synthetical Automation for Process Industries, College of Computer Science and Engineering, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, College of Information Science and Engineering, Northeastern University, Shenyang, China; School of Mathematics and Statistics, Liaoning University, Shenyang, China","IEEE Systems Journal","9 Dec 2022","2022","16","4","6457","6467","Social network platforms (such as Facebook, Wechat, and Weibo) can help people build relationships, transmit information, and make daily communication more convenient. However, in recent times, the rapid spread of misinformation and rumors has been causing public panic. Especially, during the epidemic, the severity of the crisis has been further exacerbated. Therefore, in this article, we study the influence minimization problem and propose a practical framework to address the rumor propagation problem. At first, we formulate the influence minimization problem as the mathematical optimization model. Then, we leverage the multistage competitive linear threshold model to reflect the activation status of network nodes. We propose a practical framework, called CCSQ, to select the seed nodes, which consists of community detection, candidate seed nodes, and the seeding algorithm with the Q-learning method. In particular, we construct the action, reward, and state of the Q-learning-based seeding algorithm to adaptively generate the seed nodes. Experimental results show that the proposed approach achieves smaller rumor propagation than the baseline algorithms.","1937-9234","","10.1109/JSYST.2022.3159840","LiaoNing Revitalization Talents Program(grant numbers:XLYC1902010); National Natural Science Foundation of China(grant numbers:61872073); Key Project of Natural Science Foundation of China(grant numbers:62032013); National Natural Science Foundation of China; Major International (Regional) Joint Research Project(grant numbers:71620107003); Science and Technology Planning Project of Shenyang City(grant numbers:21-108-9-19); National Natural Science Foundation of China(grant numbers:62002055); Doctor Startup Foundation of Liaoning Province(grant numbers:2021-BS-055); Fundamental Research Funds for the Central Universities(grant numbers:N2119004); China Postdoctoral Science Foundation(grant numbers:2021M693318); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9748868","Influence minimization;Q-learning method;rumor blocking;social networks","Minimization;Social networking (online);Q-learning;Fake news;Optimization;Mathematical models;Integrated circuit modeling","minimisation;reinforcement learning;social networking (online)","candidate seed nodes;community detection;Facebook;influence minimization problem;mathematical optimization model;multistage competitive linear threshold model;public panic;Q-learning method;rapid spread;reinforcement learning-based rumor blocking approach;rumor propagation;rumor propagation problem;seeding algorithm;social network platforms;Weibo","","5","","49","IEEE","4 Apr 2022","","","IEEE","IEEE Journals"
"Reinforcement-Learning-Based Dynamic Opinion Maximization Framework in Signed Social Networks","Q. He; Y. Lv; X. Wang; J. Li; M. Huang; L. Ma; Y. Cai","College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, College of Computer Science and Engineering, Northeastern University, Shenyang, China; College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Software Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China","IEEE Transactions on Cognitive and Developmental Systems","7 Mar 2023","2023","15","1","54","64","Dynamic opinion maximization (DOM) is a significant optimization issue, whose target is to select some nodes in the network and prorogate the opinions of network nodes, and produce the optimum node opinions. Until now, the node opinions of related researches are unchanged and seldom focus on social relationships. In the real scenario, the dynamic process of network nodes over time and user preference have existed. Therefore, this article proposes the  ${Q}$ -learning-based DOM (QDOM) framework in signed social networks to solve the OM problem, which is made up of two phases: 1) the activated dynamic opinion model and 2) the  ${Q}$ -learning-based seeding process. We propose the activated dynamic opinion model based on stateless  ${Q}$ -learning theory to derive the opinion propagation process. Moreover, we design the  ${Q}$ -learning-based seeding algorithm to obtain the seed nodes. The experimental results on the four signed social network data sets demonstrate that the proposed framework outperforms the state-of-the-art approaches on positive opinions, the ratio of positive opinions, and activated nodes.","2379-8939","","10.1109/TCDS.2022.3141952","National Natural Science Foundation of China(grant numbers:61872073); Liaoning Revitalization Talents Program(grant numbers:XLYC1902010); Key Project of Natural Science Foundation of China(grant numbers:62032013); NFSC through the Major International (Regional) Joint Research Project(grant numbers:71620107003); Science and Technology Planning Project of Shenyang City(grant numbers:21-108-9-19); Doctor Startup Foundation of Liaoning Province(grant numbers:2021-BS-055); Fundamental Research Funds for the Central Universities(grant numbers:N2119004,N2119007); National Natural Science Foundation of China(grant numbers:62002055); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9676712","Activated dynamic opinion model;Q-learning;seeding algorithm;social trust","Heuristic algorithms;Social networking (online);Q-learning;Integrated circuit modeling;Greedy algorithms;Adaptation models;Optimization","learning (artificial intelligence);network theory (graphs);optimisation;social networking (online)","activated dynamic opinion model;activated nodes;DOM framework;dynamic process;network nodes;opinion propagation process;optimum node opinions;positive opinions;reinforcement-learning-based dynamic opinion maximization framework;seed nodes;signed social networks;significant optimization issue;social network data sets;social relationships;stateless Q-learning theory","","4","","61","IEEE","11 Jan 2022","","","IEEE","IEEE Journals"
"Passivity-Based Online Reinforcement Learning for Real Time Model-Free Overhead Crane System Control","H. Zhang; C. Zhao; J. Ding","State Key Laboratory of Industrial Control Technology, College of Control Science and Engineering, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, College of Control Science and Engineering, Zhejiang University, Hangzhou, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China","2022 34th Chinese Control and Decision Conference (CCDC)","14 Feb 2023","2022","","","4116","4121","In this paper, a novel model-free online Reinforcement Learning (RL) control method is proposed for the real-time overhead crane control problem. The crane control problem is first formulated as an optimal regulation problem with a user-specified objective function. Two neural-networks, namely Actor-Critic networks, are employed to approximate the objective function and the optimal control policy respectively. Then, an improved network updating rule with an additional passivity-based stabilization term is developed to remove the requirement of the initial stabilizing control policy. Unlike other crane control approaches, the proposed online RL algorithm does not rely on prior knowledge of the overhead crane mathematical model. Finally, simulation studies are carried out to demonstrate the effectiveness of the proposed method in the presence of system parameter variations when compared to LQR control.","1948-9447","978-1-6654-7896-0","10.1109/CCDC55256.2022.10034123","National Science Fund for Distinguished Young Scholars; State Key Laboratory of Industrial Control Technology; Zhejiang University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10034123","Overhead cranes;Reinforcement Learning;Q-learning;Data-driven Control","Adaptation models;Cranes;System dynamics;Reinforcement learning;Linear programming;Mathematical models;Real-time systems","control system synthesis;cranes;learning (artificial intelligence);linear quadratic control;nonlinear control systems;optimal control;optimisation;reinforcement learning;stability","Actor-Critic networks;additional passivity-based stabilization term;approximate the objective function;crane control approaches;improved network updating rule;initial stabilizing control policy;neural-networks;novel model-free online Reinforcement;online RL algorithm;optimal control policy;optimal regulation problem;overhead crane mathematical model;passivity-based online Reinforcement Learning;real-time overhead crane control problem;time model-free overhead crane system control;user-specified objective function","","","","26","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Effective Management for Blockchain-Based Agri-Food Supply Chains Using Deep Reinforcement Learning","H. Chen; Z. Chen; F. Lin; P. Zhuang","College of Economics, Fujian Agriculture and Forestry University, Fuzhou, China; College of Engineering, Mathematics, and Physical Sciences, University of Exeter, Exeter, U.K.; College of Economics, Fujian Agriculture and Forestry University, Fuzhou, China; College of Economics, Fujian Agriculture and Forestry University, Fuzhou, China","IEEE Access","4 Mar 2021","2021","9","","36008","36018","In agri-food supply chains (ASCs), consumers pay for agri-food products produced by farmers. During this process, consumers emphasize the importance of agri-food safety while farmers expect to increase their profits. Due to the complexity and dynamics of ASCs, the effective traceability and management for agri-food products face huge challenges. However, most of the existing solutions cannot well meet the requirements of traceability and management in ASCs. To address these challenges, we first design a blockchain-based ASC framework to provide product traceability, which guarantees decentralized security for the agri-food tracing data in ASCs. Next, a Deep Reinforcement learning based Supply Chain Management (DR-SCM) method is proposed to make effective decisions on the production and storage of agri-food products for profit optimization. The extensive simulation experiments are conducted to demonstrate the effectiveness of the proposed blockchain-based framework and the DR-SCM method under different ASC environments. The results show that reliable product traceability is well guaranteed by using the proposed blockchain-based ASC framework. Moreover, the DR-SCM can achieve higher product profits than heuristic and Q-learning methods.","2169-3536","","10.1109/ACCESS.2021.3062410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363883","Agri-food supply chains;agri-food safety;product traceability;profit optimization;blockchain;deep reinforcement learning","Blockchain;Supply chains;Bitcoin;Safety;Reinforcement learning;Production facilities;Security","agricultural safety;agriculture;blockchains;deep learning (artificial intelligence);food processing industry;food products;food safety;production engineering computing;profitability;supply chain management","agrifood products;supply chain management;product traceability;agrifood tracing data;blockchain-based agrifood supply chains;deep reinforcement learning;agrifood safety;blockchain-based ASC;DR-SCM;profit optimization;heuristic;Q-learning","","36","","42","CCBY","26 Feb 2021","","","IEEE","IEEE Journals"
"Safe trajectory design for indoor drones using reinforcement-learning-based methods","D. Tompos; B. Németh","Eötvös Loránd Research Network (ELKH) Kende utca 13-17., Institute for Computer Science and Control (SZTAKI), Budapest, Hungary; Eötvös Loránd Research Network (ELKH) Kende utca 13-17., Institute for Computer Science and Control (SZTAKI), Budapest, Hungary","2023 IEEE 17th International Symposium on Applied Computational Intelligence and Informatics (SACI)","27 Jun 2023","2023","","","000027","000032","This paper proposes a design method for achieving safe trajectory of indoor drones. The trajectory design with a reinforcement-learning-based (RL) agent is facilitated, which can result in efficient and collision-free motion. The method is developed for motion in indoor area with moving mobile robots, and thus, the collision with these obstacles must be avoided. Through RL-based design the fast motion of the drones can be achieved, which must perform a mission between workstations in a manufacturing system. The effectiveness of the design process through a simulation example on a real laboratory environment is illustrated.","2765-818X","979-8-3503-2110-4","10.1109/SACI58269.2023.10158591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10158591","safe drone motion;reinforcement learning;manufacturing systems","Robot kinematics;Design methodology;Laboratories;Reinforcement learning;Trajectory;Workstations;Mobile robots","collision avoidance;learning (artificial intelligence);mobile robots","collision-free motion;design method;design process;fast motion;indoor area;indoor drones;mobile robots;reinforcement-learning-based agent;reinforcement-learning-based methods;RL-based design;safe trajectory design","","","","12","IEEE","27 Jun 2023","","","IEEE","IEEE Conferences"
"Utility-Based Reinforcement Learning for Reactive Grids","J. Perez; C. Germain-Renaud; B. Kégl; C. Loomis","CNRS, Universite Paris-Sud, France; CNRS, Universite Paris-Sud, France; CNRS, Universite Paris-Sud, France; CNRS, Universite Paris-Sud, France","2008 International Conference on Autonomic Computing","27 Jun 2008","2008","","","205","206","Large scale production grids are an important case for autonomic computing. They follow a mutualization paradigm: decision-making (human or automatic) is distributed and largely independent, and, at the same time, it must implement the highlevel goals of the grid management. This paper deals with the scheduling problem with two partially conflicting goals: fairshare and Quality of Service (QoS). Fair sharing is a wellknown issue motivated by return on investment for participating institutions. Differentiated QoS has emerged as an important and unexpected requirement in the current usage of production grids. In the framework of the EGEE grid (one of the largest existing grids), applications from diverse scientific communities require a pseudo-interactive response time. More generally, seamless integration of the grid power into everyday use calls for unplanned and interactive access to grid resources, which defines reactive grids. The major result of this paper is that the combination of utility functions and reinforcement learning (RL) provides a general and efficient method for dynamically allocating grid resources in order to satisfy both end users with differentiated requirements and participating institutions. Combining RL methods and utility functions for resource allocation was pioneered by Tesauro and Vengerov. While the application contexts are different, the resource allocation issues are very similar. The main difference in our work is that we consider a multi-criteria optimization problem that includes a fair-share objective. A first contribution of our work is the definition of a set of variables describing states and actions that allows us to formulate the grid scheduling problem as a continuous action-state space reinforcement learning problem. To capture the immediate goals of end users and the long-term objectives of administrators, we propose automatically derived utility functions. Finally, our experimental results on a synthetic workload and a real EGEE trace show that RL clearly outperforms the classical schedulers, so it is a realistic alternative to empirical scheduler design.","","978-0-7695-3175-5","10.1109/ICAC.2008.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4550845","grid scheduling;reinforcement learning;utility function","Learning;Grid computing;Processor scheduling;Production;Resource management;Large-scale systems;Humans;Optimization methods;Vehicle dynamics;Steady-state","grid computing;interpolation;learning (artificial intelligence);optimisation;quality of service;resource allocation;scheduling;utility programs","reactive grids;reinforcement learning;scheduling;QoS;utility functions;multicriteria optimization problem;interpolation;resource allocation","","9","","6","IEEE","27 Jun 2008","","","IEEE","IEEE Conferences"
"A Reinforcement-Learning-Based Evolutionary Algorithm Using Solution Space Clustering For Multimodal Optimization Problems","H. Xia; C. Li; S. Zeng; Q. Tan; J. Wang; S. Yang","School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; School of Mechanical Engineering and Electronic Information, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; School of Computer Science and Informatics, De Montfort University, Leicester, United Kingdom","2021 IEEE Congress on Evolutionary Computation (CEC)","9 Aug 2021","2021","","","1938","1945","In evolutionary algorithms, how to effectively select interactive solutions for generating offspring is a challenging problem. Though many operators are proposed, most of them select interactive solutions (parents) randomly, having no specificity for the features of landscapes in various problems. To address this issue, this paper proposes a reinforcement-learning-based evolutionary algorithm to select solutions within the approximated basin of attraction. In the algorithm, the solution space is partitioned by the k-dimensional tree, and features of subspaces are approximated with respect to two aspects: objective values and uncertainties. Accordingly, two reinforcement learning (RL) systems are constructed to determine where to search: the objective-based RL exploits basins of attraction (clustered subspaces) and the uncertainty-based RL explores subspaces that have been searched comparatively less. Experiments are conducted on widely used benchmark functions, demonstrating that the algorithm outperforms three other popular multimodal optimization algorithms.","","978-1-7281-8393-0","10.1109/CEC45853.2021.9504896","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9504896","Evolutionary algorithm;reinforcement learning;landscape approximation;basin of attraction","Uncertainty;Clustering algorithms;Evolutionary computation;Reinforcement learning;Benchmark testing;Search problems;Feature extraction","evolutionary computation;feature extraction;learning (artificial intelligence);optimisation;pattern clustering;search problems;trees (mathematics)","reinforcement-learning-based evolutionary algorithm;multimodal optimization algorithms;solution space clustering;multimodal optimization problems;evolutionary algorithms;uncertainty-based reinforcement learning;objective-based reinforcement learning;solution space partitionion;k-dimensional tree;subspace feature approximation;clustered subspaces","","1","","22","IEEE","9 Aug 2021","","","IEEE","IEEE Conferences"
"Local Navigation and Docking of an Autonomous Robot Mower Using Reinforcement Learning and Computer Vision","A. Taghibakhshi; N. Ogden; M. West","Department of Mechanical Sciencea nd Engineering, The University of Illinois at Urbana-Champaign, Urbana, Illinois, USA; John Deere Technology Innovation Center, Champaign, Illinois, USA; Department of Mechanical Sciencea nd Engineering, The University of Illinois at Urbana-Champaign, Urbana, Illinois, USA","2021 13th International Conference on Computer and Automation Engineering (ICCAE)","10 May 2021","2021","","","10","14","We demonstrate a successful navigation and docking control system for the John Deere Tango autonomous mower, using only a single camera as the input. This vision-only system is of interest because it is inexpensive, simple for production, and requires no external sensing. This is in contrast to existing systems that rely on integrated position sensors and global positioning system (GPS) technologies. To produce our system we combined a state-of-the-art object detection architecture, You Look Only Once (YOLO), with a reinforcement learning (RL) architecture, Double Deep Q-Networks (Double DQN). The object detection network identifies features on the mower and passes its output to the RL network, providing it with a low-dimensional representation that enables rapid and robust training. Finally, the RL network learns how to navigate the machine to the desired spot in a custom simulation environment. When tested on mower hardware, the system is able to dock with centimeter-level accuracy from arbitrary initial locations and orientations.","","978-1-6654-1295-7","10.1109/ICCAE51876.2021.9426091","John Deere; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9426091","Reinforcement Learning;Deep Q-Learning;Object Detection;YOLO;Mower","Training;Navigation;Object detection;Reinforcement learning;Computer architecture;Control systems;Sensor systems","cameras;computer vision;Global Positioning System;lawnmowers;learning (artificial intelligence);mobile robots;object detection;position control;robot vision","autonomous robot mower;computer vision;successful navigation;docking control system;John Deere Tango autonomous mower;single camera;external sensing;integrated position sensors;global positioning system technologies;state-of-the-art object detection architecture;reinforcement learning architecture;Double Deep Q-Networks;Double DQN;object detection network;RL network;mower hardware;dock","","3","","17","IEEE","10 May 2021","","","IEEE","IEEE Conferences"
"Laser Engraver Control System based on Reinforcement Adversarial Learning","E. Nikolaev","Institute of Information Technologies and Telecommunications, North-Caucasus Fedral University, Stavropol, Russia","2019 International Russian Automation Conference (RusAutoCon)","14 Oct 2019","2019","","","1","5","In recent years, deep neural networks have demonstrated high efficiency in solving the problems associated with image processing and analysis. The latest studies in generative adversarial neural networks open up broad prospects for solving problems in image style transfer, cross-domain adaptation, and the generation of new data for a target probability distribution. The ability of generative networks to implement complex and weakly formalized transformations allows the information systems based on generative adversarial technologies to solve problems in machine learning in a manner similar to human creative thinking. This paper presents the results of an experimental study on the design, training and deployment of an intelligent control system for a CNC laser engraver based on the generative adversarial network. In order to train deep models, large amounts of data are required. Therefore, one of the main challenges in this experimental study is a lack of labelled training dataset. The proposed control system for the laser engraver based on the deep generative model allows decreasing the time of the manufacturing process.","","978-1-7281-0265-8","10.1109/RUSAUTOCON.2019.8867762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8867762","generative adversarial network;reinforcement learning;intelligent control system;deep learning","Training;Generators;Laser modes;Neural networks;Computer numerical control;Generative adversarial networks;Image synthesis","image processing;intelligent control;learning (artificial intelligence);neural nets;probability;statistical distributions","reinforcement adversarial learning;laser engraver control system;deep generative model;labelled training dataset;deep models;generative adversarial network;CNC laser engraver;intelligent control system;human creative thinking;machine learning;generative adversarial technologies;information systems;weakly formalized transformations;complex transformations;generative networks;target probability distribution;cross-domain adaptation;image style transfer;broad prospects;generative adversarial neural networks;image processing;deep neural networks","","2","","28","IEEE","14 Oct 2019","","","IEEE","IEEE Conferences"
"Integrating Reinforcement Learning and Learning From Demonstrations to Learn Nonprehensile Manipulation","X. Sun; J. Li; A. V. Kovalenko; W. Feng; Y. Ou","Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Department of Data Analysis and Artificial Intelligence, Kuban State University, Krasnodar, Russia; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institute of Advanced Technology, the Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, and the Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Chinese Academy of Sciences, Shenzhen, China","IEEE Transactions on Automation Science and Engineering","30 Jun 2023","2023","20","3","1735","1744","Motor skills are essential for robots to accomplish complicated and dexterous manipulation tasks, which are difficult to be mastered through traditional controller designs. Currently, robots learning from demonstrations enable them to learn control policies automatically from human motor demonstrations. However, the nonlinearity and instantaneousness of the demonstrated forces prohibit robots from fully mastering the motor skill features by simply exploiting force examples. Therefore, a self-improvement learning scheme is required to refine the control policy further until satisfactory motor skills are acquired. Hence, this paper combines learning from demonstrations and reinforcement learning to learn a controller for complex motor skills. The proposed method is validated on an IIWA KUKA robot, performing a specified nonprehensile manipulation task. Note to Practitioners—The motivation of this paper originates from the requirement to develop an efficient and fast learning algorithm that improves the robot skill learning efficiency. Specifically, our research focuses on the nonprehensile manipulation task, easily subject to environmental changes. Therefore, the robot must continuously interact with the environment to master the skill. To accelerate the skill learning process, learning from demonstrations initializes the control policies, and then the robot starts to practice the demonstrated skill. After each practice round, the robot receives a reward from the environment, and based on the reinforcement learning algorithm limited up to 100 trials, the robot masters the nonprehensile manipulation skill.","1558-3783","","10.1109/TASE.2022.3185071","National Key Research and Development Program of China(grant numbers:2018AAA0103001); National Natural Science Foundation of China(grant numbers:U1813208,62173319,62063006); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020B1515120054); Shenzhen Fundamental Research Program(grant numbers:JCYJ20200109115610172); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810199","Nonprehensile manipulation;reinforcement learning;learning from demonstrations","Robots;Task analysis;Trajectory;Reinforcement learning;Sports;Force;Education","control engineering computing;dexterous manipulators;reinforcement learning;robot programming","control policy;dexterous manipulation;human motor demonstrations;IIWA KUKA robot;learning from demonstrations;motor skill features;nonprehensile manipulation skill;reinforcement learning;robot skill learning efficiency;self-improvement learning","","1","","36","IEEE","29 Jun 2022","","","IEEE","IEEE Journals"
"Lifelong Federated Reinforcement Learning: A Learning Architecture for Navigation in Cloud Robotic Systems","B. Liu; L. Wang; M. Liu","Cloud Computing Lab, Shenzhen Institutes of Advanced Technology, Shenzhen, China; Cloud Computing Lab, Shenzhen Institutes of Advanced Technology, Shenzhen, China; Department of Electronics and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China","IEEE Robotics and Automation Letters","25 Oct 2019","2019","4","4","4555","4562","This letter was motivated by the problem of how to make robots fuse and transfer their experience so that they can effectively use prior knowledge and quickly adapt to new environments. To address the problem, we present a learning architecture for navigation in cloud robotic systems: Lifelong Federated Reinforcement Learning (LFRL). In the letter, we propose a knowledge fusion algorithm for upgrading a shared model deployed on the cloud. Then, effective transfer learning methods in LFRL are introduced. LFRL is consistent with human cognitive science and fits well in cloud robotic systems. Experiments show that LFRL greatly improves the efficiency of reinforcement learning for robot navigation. The cloud robotic system deployment also shows that LFRL is capable of fusing prior knowledge. In addition, we release a cloud robotic navigation-learning website to provide the service based on LFRL: www.shared-robotics.com.","2377-3766","","10.1109/LRA.2019.2931179","Shenzhen Science and Technology Innovation Commission(grant numbers:JCYJ2017081853518789); Guangdong Science and Technology Plan Guangdong-Hong Kong Cooperation Innovation Platform(grant numbers:2018B050502009); National Natural Science Foundation of China(grant numbers:61603376); National Natural Science Foundation of China(grant numbers:U1713211); Shenzhen Science, Technology and Innovation Commission(grant numbers:JCYJ20160428154842603); Basic Research Project of Shanghai Science and Technology Commission(grant numbers:16JC1401200); Chengzhong Xu from University of Macau; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8772088","Deep learning in robotics and automation;autonomous vehicle navigation;AI-based methods","Navigation;Reinforcement learning;Task analysis;Cloud computing;Training;Robot kinematics","cloud computing;learning (artificial intelligence);mobile robots;path planning;Web sites","transfer learning methods;LFRL;robot navigation;cloud robotic system deployment;learning architecture;knowledge fusion algorithm;lifelong federated reinforcement learning;human cognitive science;cloud robotic navigation-learning Website","","96","","30","IEEE","25 Jul 2019","","","IEEE","IEEE Journals"
"Integrating State Representation Learning Into Deep Reinforcement Learning","T. de Bruin; J. Kober; K. Tuyls; R. Babuška","Cognitive Robotics Department, Delft University of Technology, Delft, The Netherlands; Cognitive Robotics Department, Delft University of Technology, Delft, The Netherlands; Google Deepmind, London, U.K.; Cognitive Robotics Department, Delft University of Technology, Delft, The Netherlands","IEEE Robotics and Automation Letters","23 Feb 2018","2018","3","3","1394","1401","Most deep reinforcement learning techniques are unsuitable for robotics, as they require too much interaction time to learn useful, general control policies. This problem can be largely attributed to the fact that a state representation needs to be learned as a part of learning control policies, which can only be done through fitting expected returns based on observed rewards. While the reward function provides information on the desirability of the state of the world, it does not necessarily provide information on how to distill a good, general representation of that state from the sensory observations. State representation learning objectives can be used to help learn such a representation. While many of these objectives have been proposed, they are typically not directly combined with reinforcement learning algorithms. We investigate several methods for integrating state representation learning into reinforcement learning. In these methods, the state representation learning objectives help regularize the state representation during the reinforcement learning, and the reinforcement learning itself is viewed as a crucial state representation learning objective and allowed to help shape the representation. Using autonomous racing tests in the TORCS simulator, we show how the integrated methods quickly learn policies that generalize to new environments much better than deep reinforcement learning without state representation learning.","2377-3766","","10.1109/LRA.2018.2800101","Robust Robot Control (DL-Force)(grant numbers:656.000.003); Netherlands Organisation for Scientific Research (NWO); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8276247","Deep learning in robotics and automation;learning and adaptive systems;sensor fusion","Robot sensing systems;Learning (artificial intelligence);Task analysis;Machine learning;Shape;Training","learning (artificial intelligence)","deep reinforcement learning techniques;robotics;state representation learning integration;learning control policies;desirability;autonomous racing tests;TORCS simulator;sensory observations","","61","","32","IEEE","31 Jan 2018","","","IEEE","IEEE Journals"
"Learn to Adapt to Human Walking: A Model-Based Reinforcement Learning Approach for a Robotic Assistant Rollator","G. Chalvatzaki; X. S. Papageorgiou; P. Maragos; C. S. Tzafestas","School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Institute for Language and Speech Processing, Athens, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece","IEEE Robotics and Automation Letters","1 Aug 2019","2019","4","4","3774","3781","In this letter, we tackle the problem of adapting the motion of a robotic assistant rollator to patients with different mobility status. The goal is to achieve a coupled human–robot motion in a front-following setting as if the patient was pushing the rollator himself/herself. To this end, we propose a novel approach using model-based reinforcement learning (MBRL) for adapting the control policy of the robotic assistant. This approach encapsulates our previous work on human tracking and gait analysis from RGB-D and laser streams into a human-in-the-loop decision making strategy. We use long short-term memory (LSTM) networks for designing a human motion intention model and a coupling parameters forecast model, leveraging on the outcome of human gait analysis. An initial LSTM-based policy network was trained via imitation learning from human demonstrations in a motion capture setup. This policy is then fine-tuned with the MBRL framework using tracking data from real patients. A thorough evaluation analysis proves the efficiency of the MBRL approach as a user-adaptive controller.","2377-3766","","10.1109/LRA.2019.2929996","European Union and Greek national funds through the Operational Program Competitiveness, Entrepreneurship and Innovation; RESEARCH—CREATE—INNOVATE(grant numbers:T1EDK-01248/MIS: 5030856); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8767993","Human-centered robotics;learning and adaptive systems;automation in life sciences: biotechnology;pharmaceutical and health care","Legged locomotion;Adaptive systems;Robot kinematics;Robot sensing systems;Navigation;Predictive models;Learning systems;Biotechnology","","","","14","","43","IEEE","22 Jul 2019","","","IEEE","IEEE Journals"
"Experiments Focused on Exploration in Deep Reinforcement Learning","M. Kaloev; G. Krastev","Department of Computer systems and technologies, University of Ruse, Ruse, Bulgaria; Department of Computer systems and technologies, University of Ruse, Ruse, Bulgaria","2021 5th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)","19 Nov 2021","2021","","","351","355","Automation with deep neural networks is popular topic. Personal artificial assistance, self driving cars and recommendations algorithms, they all use reinforcement learning. However, there are many question about this technology still unanswered. Can the learning process be accelerated, why sometimes the automatons are learning wrong behavior, is it possible for machines teach each others. This papers try to answer those questions with experiments.","","978-1-6654-4930-4","10.1109/ISMSIT52890.2021.9604690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9604690","exploration and exploitation;reinforcement learning;actor critic algorithms;overestimation in Q-values","Training;Deep learning;Automation;Learning automata;Neural networks;Reinforcement learning;Automobiles","deep learning (artificial intelligence);reinforcement learning","deep reinforcement learning;deep neural networks;personal artificial assistance;self driving cars;recommendations algorithms;learning process;exploration","","3","","13","EU","19 Nov 2021","","","IEEE","IEEE Conferences"
"Planning of interactive information retrieval by means of reinforcement learning","V. Vitsentiy; A. Spink; A. Sachenko","Institute of Computer Information Technologies, Temopil Academy of National Economy, Ternopil, Ukraine; School of Information Sciences and Technology, Pennsylvania State University, University Park, PA, USA; Institute of Computer Information Technologies, Temopil Academy of National Economy, Ternopil, Ukraine","Second IEEE International Workshop on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications, 2003. Proceedings","8 Dec 2003","2003","","","396","399","The theoretical base of a new approach to automation of interactive information retrieval on the basis of reinforcement learning methods is considered. Advantages and problem questions of the use of reinforcement learning for this task are analyzed and the peculiarities of the task are defined","","0-7803-8138-6","10.1109/IDAACS.2003.1249594","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1249594","","Information retrieval;Learning;Information technology;Paper technology;Automation;Information analysis;Neural networks;Delay;Pattern recognition;Feedback","human computer interaction;information retrieval;learning (artificial intelligence)","interactive information retrieval;reinforcement learning;information retrieval system","","1","","7","IEEE","8 Dec 2003","","","IEEE","IEEE Conferences"
"Stimuli Generation for IC Design Verification using Reinforcement Learning with an Actor-Critic Model","S. L. Tweehuysen; G. L. A. Adriaans; M. Gomony","Prodrive Technologies, Eindhoven, The Netherlands; Prodrive Technologies, Eindhoven, The Netherlands; Dept. of Electrical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands","2023 IEEE European Test Symposium (ETS)","12 Jul 2023","2023","","","1","4","With Integrated Circuit (IC) designs becoming larger and more complex, there is a growing risk of errors in the Register-Transfer Layer (RTL) implementation. Stimuli generation to achieve high coverage in functional verification is paramount for finding these errors and preventing them from ending up in the final design. Several custom methods have been proposed for stimuli generation to reduce functional testing duration of RTL designs, while more flexible or generic methods could reduce verification time significantly by supporting larger range of RTL designs. This paper proposes a novel flexible stimuli generation technique by using reinforcement learning with an Actor-Critic model. Our benchmarking results showed that the proposed method achieves a higher coverage than baseline solution for a diverse range of RTL designs, making it a valuable addition to test automation tool-flow.","1558-1780","979-8-3503-3634-4","10.1109/ETS56758.2023.10174129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174129","","Training;Automation;Europe;Reinforcement learning;Benchmark testing;Integrated circuit modeling","electronic engineering computing;formal verification;integrated circuit design;program testing;reinforcement learning","actor-critic model;flexible stimuli generation technique;functional testing duration;functional verification;IC design verification;integrated circuit designs;register-transfer layer implementation;reinforcement learning;RTL designs","","","","14","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Reliable and Accurate Autonomous Flow Operation based on Off-line Trained Reinforcement Learning","S. Barzegar; M. Ruiz; L. Velasco","Optical Communications Group (GCO), Universitat Politècnica de Catalunya (UPC), Barcelona, Spain; Optical Communications Group (GCO), Universitat Politècnica de Catalunya (UPC), Barcelona, Spain; Optical Communications Group (GCO), Universitat Politècnica de Catalunya (UPC), Barcelona, Spain","2021 Optical Fiber Communications Conference and Exhibition (OFC)","26 Jul 2021","2021","","","1","3","A RL agent trained offline for reliability and able to refine its policies during online operation is proposed. Results for three illustrative flow automation use cases show remarkable performance with extraordinary adaptability to changes.","","978-1-943580-86-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9489915","","Image motion analysis;Computer vision;Automation;Reinforcement learning;Optical fiber communication;Reliability","","","","","","6","","26 Jul 2021","","","IEEE","IEEE Conferences"
"Whole Campaign Emulation with Reinforcement Learning for Cyber Test","T. Cody; E. Meno; P. Beling; L. Freeman","Intelligent Systems Division, Virginia Tech National Security Institute, Arlington, Virginia, USA; Intelligent Systems Division, Virginia Tech National Security Institute, Arlington, Virginia, USA; Intelligent Systems Division, Virginia Tech National Security Institute, Arlington, Virginia, USA; Virginia Tech National Security Institute, Arlington, Virginia, USA","IEEE Instrumentation & Measurement Magazine","4 Aug 2023","2023","26","5","25","30","Cyber-attacks pose existential, nation-level threats and directly challenge societal stability. The breadth of targets (small businesses to nation-states) and continuous nature of cyber-attacks make automated cyber test and evaluation (T&E) crucial to national security and domestic prosperity. Importantly, automation lowers the cost and increases the frequency of cyber T&E, thereby simultaneously increasing cyber test availability and coverage. Spurred by market demand as well as advancements in artificial intelligence (AI), automated approaches to penetration testing have seen a resurgence of interest in the academic literature. Yet to date, this burgeoning research community lacks a shared, long-term vision. Recently, we proposed a concept of whole campaign emulation (WCE) as a challenge problem and framework for automated penetration testing with reinforcement learning (RL) [1]. In this article, we review the state-of-the-art in RL-based automated penetration testing, assess its relation to WCE, and provide a case study using the open-source Network Attack Simulator (NASim) [2].","1941-0123","","10.1109/MIM.2023.10208253","U.S. Department of Defense(grant numbers:HQ003419D0003); Systems Engineering Research Center (SERC); Stevens Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10208253","","Costs;Automation;Emulation;Reinforcement learning;Organizations;Stability analysis;Artificial intelligence;Social factors","computer crime;national security;program testing;public domain software;reinforcement learning","automated cyber test and evaluation;automated penetration testing;cyber T&E;cyber-attacks;NASim;nation-level threats;national security;open-source network attack simulator;reinforcement learning;societal stability;whole campaign emulation","","","","12","IEEE","4 Aug 2023","","","IEEE","IEEE Magazines"
"Development of Applicable Reinforcement Learning Compensator Using Ranking Information for AUV","Y. Yashiro; K. Eguchi; R. Imamura; S. Arai","ICT Solution Headquarters, Mitsubishi Heavy Industries Ltd., Yokohama, Japan; ICT Solution Headquarters, Mitsubishi Heavy Industries Ltd., Nagasaki, Japan; Graduate School of Engineering, Chiba University, Chiba, Japan; Graduate School of Engineering, Chiba University, Chiba, Japan","OCEANS 2022 - Chennai","19 May 2022","2022","","","1","5","In nonlinear multi input multi output systems such as Autonomous Underwater Vehicle (AUV), a physical dynamical model is usually used to design a control system. To satisfy trade-off between plural control targets, design and adjustment of the control system are complicated, and which lead to struggle by control design expert. In recent years, applying data-driven techniques such as deep reinforcement learning have been tried to improve control performance by automatic adjustment in the deep neural network. However, we have difficulty to applying data-driven methods in sight of designing reward functions, and how to combine learning results into a controller so as not to lose prior information such as models. Then, in this study, we developed compensation control technology by deep reinforcement learning (DRL) for efficient control under multi-purpose trade-off, by using selector with ranking to several options of control target trajectory, and to verify its effectiveness by simulation using obstacle avoidance as an example.","","978-1-6654-1821-8","10.1109/OCEANSChennai45887.2022.9775527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9775527","Autonomous Underwater Vehicle;control compensation;deep reinforcement learning;obstacle avoidance","Sensitivity;Target recognition;Shape;Atmospheric modeling;Simulation;Oceans;Neural networks","autonomous underwater vehicles;collision avoidance;control system synthesis;learning (artificial intelligence);learning systems;MIMO systems;neural nets;nonlinear control systems","AUV;physical dynamical model;control system;control design expert;data-driven techniques;deep reinforcement learning;control performance;automatic adjustment;deep neural network;compensation control technology;control target trajectory;applicable reinforcement learning compensator;ranking information;nonlinear multiinput multioutput systems;plural control targets","","","","9","IEEE","19 May 2022","","","IEEE","IEEE Conferences"
"A Task Scheduling for Digital Array Radar Based on Reinforcement learning","C. Xue; D. Li; D. Wang; L. Wang","Key Laboratory of Radar Imaging and Microwave Photonics Ministry of Education, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Leihua Electronic Technology Research Institute, Aviation Industry Corporation of China, Wuxi, China; College of Economics and Management, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Key Laboratory of Radar Imaging and Microwave Photonics Ministry of Education, Nanjing University of Aeronautics and Astronautics, Nanjing, China","2023 8th International Conference on Computer and Communication Systems (ICCCS)","26 Jun 2023","2023","","","120","125","Target tracking is the key technology of the Digital Array Radar (DAR). The adaptive radar task scheduling interval can exert the advantage of digital array beams, significantly improving the target tracking performance. This article introduces the adaptive task scheduling interval technology in the task scheduling algorithm. This technology is an adaptive task scheduling interval scheme based on Reinforcement Learning (RL), realizing better resource scheduling through priority competition mechanisms. The adaptive task scheduling interval can not only timely compensate for the target movement status information during the tracking process but also increase the measurement quantity to improve the stability of tracking maneuverability. Due to the limited radar resources, reducing the tracking task scheduling interval restricts the system tracking capacity and the ability to complete other functions. Adaptive scheduling interval technologies should maintain a certain degree of balance between search and tracking. Finally, the adaptive scheduling interval control technology is simulated and verified.","","978-1-6654-5612-8","10.1109/ICCCS57501.2023.10151157","National Key Research and Development Program of China(grant numbers:2017YFB0502700); National Natural Science Foundation of China(grant numbers:61871217); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10151157","Digital Array Radar(DAR);Reinforcement Learning(RL);Task Scheduling Interval","Adaptive scheduling;Target tracking;Scheduling algorithms;Radar;Reinforcement learning;Radar tracking;Dynamic scheduling","adaptive radar;adaptive scheduling;phased array radar;radar tracking;scheduling;target tracking","adaptive radar task scheduling interval;adaptive scheduling interval control technology;adaptive scheduling interval technologies;adaptive task scheduling interval scheme;adaptive task scheduling interval technology;Digital Array Radar;radar resources;Reinforcement learning;Reinforcement Learning;system tracking capacity;target movement status information;target tracking performance;task scheduling algorithm;tracking maneuverability;tracking process;tracking task scheduling interval","","","","24","IEEE","26 Jun 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Using Genetic Algorithm for Parameter Optimization","A. Sehgal; H. La; S. Louis; H. Nguyen","Advanced Robotics and Automation (ARA) Laboratory, Reno, NV, USA; Advanced Robotics and Automation (ARA) Laboratory, Reno, NV, USA; Department of Computer Science and Engineering, University of Nevada, Reno, NV, USA; Advanced Robotics and Automation (ARA) Laboratory, Reno, NV, USA","2019 Third IEEE International Conference on Robotic Computing (IRC)","28 Mar 2019","2019","","","596","601","Reinforcement learning (RL) enables agents to take decision based on a reward function. However, in the process of learning, the choice of values for learning algorithm parameters can significantly impact the overall learning process. In this paper, we use a genetic algorithm (GA) to find the values of parameters used in Deep Deterministic Policy Gradient (DDPG) combined with Hindsight Experience Replay (HER), to help speed up the learning agent. We used this method on fetch-reach, slide, push, pick and place, and door opening in robotic manipulation tasks. Our experimental evaluation shows that our method leads to better performance, faster than the original algorithm.","","978-1-5386-9245-5","10.1109/IRC.2019.00121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675632","Reinforcement learning;Deep learning;Robot manipulation;Genetic algorithm","Genetic algorithms;Task analysis;Robots;Neural networks;Biological cells;Training;Reinforcement learning","genetic algorithms;learning (artificial intelligence);manipulators","original algorithm;genetic algorithm;parameter optimization;RL;reward function;learning process;learning agent;deep deterministic policy gradient;hindsight experience replay;deep reinforcement learning;learning algorithm parameters;DDPG;HER;robotic manipulation tasks","","49","","41","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"Reinforcement learning based overtaking decision-making for highway autonomous driving","X. Li; X. Xu; L. Zuo","The College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; The College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; The College of Mechatronics and Automation, National University of Defense Technology, Changsha, China","2015 Sixth International Conference on Intelligent Control and Information Processing (ICICIP)","21 Jan 2016","2015","","","336","342","In this paper, we develop an intelligent overtaking decision-making method for highway autonomous driving. The key idea is to use reinforcement learning algorithms to learn an optimized policy via a series of simulated driving scenarios. A vehicle model based on data fitting of real vehicles as well as a traffic model is established to simulate driving scenarios and validation tests of obtained policies. Human driving experiences are considered in designing the reward function. A reinforcement learning method called the Q-learning algorithm is used to learn overtaking decision-making policies. Simulations show that our method can learn feasible overtaking policies in different traffic environments and the performance is comparable or even better than manually designed decision rules.","","978-1-4799-1717-4","10.1109/ICICIP.2015.7388193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7388193","reinforcement learning;Q-learning;overtaking decision-making;vehicle model;highway road","Frequency modulation;Decision support systems;Vehicles;Decision making;Roads;Yttrium","intelligent transportation systems;learning (artificial intelligence)","intelligent overtaking decision-making method;highway autonomous driving;reinforcement learning algorithms;vehicle model;traffic model;reward function;Q-learning algorithm;intelligent transportation systems","","39","","14","IEEE","21 Jan 2016","","","IEEE","IEEE Conferences"
"Explainable AI in Deep Reinforcement Learning Models: A SHAP Method Applied in Power System Emergency Control","K. Zhang; P. Xu; J. Zhang","Electrical engineering and automation, Wuhan university, Wuhan, China; Electrical engineering and automation, Wuhan university, Wuhan, China; Electrical engineering and automation, Wuhan university, Wuhan, China","2020 IEEE 4th Conference on Energy Internet and Energy System Integration (EI2)","15 Feb 2021","2020","","","711","716","The application of artificial intelligence (AI) system is more and more extensive, using the explainable AI (XAI) technology to explain why machine learning (ML) models make certain predictions as important as the accuracy of the predictions, because it ensures the trust and transparency in the model decision-making process. For deep reinforcement learning (DRL) model, although some outstanding progress based on DRL has been made in many fields, it is difficult to explain and cannot be used in safety related occasions. Especially in power system, for the power system emergency control based on DRL, how to provide an intuitive and reliable XAI technology is urgent and necessary. The Shapley additive explanations (SHAP) method has been adopted to provide a reasonable interpretable model for an open-source platform named Reinforcement Learning for Grid Control (RLGC). Through a series of summary plots, force plots and probability of SHAP value, the under-voltage load shedding of power system based on DRL can be interpreted much easier and clearer. More importantly, this work is unique in the power system field, presenting the first use of the SHAP method and the probability of SHAP value to give explanations for emergency control based on DRL in power system.","","978-1-7281-9606-0","10.1109/EI250167.2020.9347147","State Grid Corporation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9347147","Deep reinforcement learning;explainable artificial intelligence;power system;under-voltage load shedding;Shapley additive explanations","Decision making;Reinforcement learning;Power systems;Power system reliability;Reliability;Artificial intelligence;Load modeling","cooperative systems;decision making;deep learning (artificial intelligence);human computer interaction;load shedding;neural nets;power system control","under-voltage load shedding;Reinforcement Learning for Grid Control;Shapley additive explanations method;power system field;SHAP value;reasonable interpretable model;reliable XAI technology;intuitive XAI technology;DRL;deep reinforcement learning model;model decision-making process;machine learning models;explainable AI technology;artificial intelligence system;power system emergency control;SHAP method applied;deep Reinforcement Learning models","","15","","18","IEEE","15 Feb 2021","","","IEEE","IEEE Conferences"
"Packet Routing Against Network Congestion: A Deep Multi-agent Reinforcement Learning Approach","R. Ding; Y. Yang; J. Liu; H. Li; F. Gao","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xian, China; Department of Automation, Tsinghua University, Beijing, China","2020 International Conference on Computing, Networking and Communications (ICNC)","30 Mar 2020","2020","","","932","937","The continuous growth of the network data would lead to the increased network congestion and the throughput decline. In this paper, we investigate the packet routing problem based on deep multi-agent reinforcement learning, where each router chooses the next hop router by itself intelligently. We design the modified deep Q-network in each router to evaluate the neighbor routers. The routers, each acting as an agent, choose the next hop router based on their local observation. Then they transfer the packets to the chosen routers and receive the reward and the observation of the next hop routers. Using their experience, the routers learn to improve the packet routing strategy by updating their Q-networks. We demonstrate that with proper reward set and training mechanism, the routers in the network can work in a distributed way to reduce the computational complexity compared with the single-agent reinforcement learning based algorithm. And the proposed algorithm can further reduce the congestion probability and improve the network performance.","2325-2626","978-1-7281-4905-9","10.1109/ICNC47757.2020.9049759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9049759","","Training;Reinforcement learning;Routing;Throughput;Computational complexity","intelligent networks;learning (artificial intelligence);multi-agent systems;telecommunication congestion control;telecommunication network routing","next hop router;network performance;single-agent reinforcement learning based algorithm;Q-networks;packet routing strategy;deep Q-network;packet routing problem;network congestion;deep multiagent reinforcement learning approach","","11","","16","IEEE","30 Mar 2020","","","IEEE","IEEE Conferences"
"Monocular Vision based Autonomous Landing of Quadrotor through Deep Reinforcement Learning","Y. Xu; Z. Liu; X. Wang","College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha; College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha; College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha","2018 37th Chinese Control Conference (CCC)","7 Oct 2018","2018","","","10014","10019","An improved deep reinforcement learning (DRL) method is proposed to solve autonomous landing problem of quadrotor. Autonomous landing is a significant function for unmanned aerial vehicle (UAV) such as quadrotor. Previous solutions are mainly based on relative position calculation or the landmark detection, which either needs massive additional sensors or lacks intelligence. In this paper, we focus on realizing autonomous landing through DRL method. Whole landing process is implemented by an improved deep Q-learning network (DQN) based end-to-end control scheme. Only one down-looking camera is used to capture raw images directly as input states. An Aruco tag is placed at the landing region for feature extraction. Double network and the dueling architecture are applied to improve DQN algorithm. Besides, the reward function is well designed to fit the auto-landing scenario. The experiments show that the improved DQN can make the quadrotor land on the landmark successfully and achieve better performance while comparing to the original deep Q-learning solution.","1934-1768","978-988-15639-5-8","10.23919/ChiCC.2018.8482830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8482830","Unmanned Aerial Vehicle (UAV);Autonomous Landing;Deep Reinforcement Learning (DRL);DQN","Training;Computer architecture;Machine learning;Cameras;Feature extraction;Neural networks;Unmanned aerial vehicles","autonomous aerial vehicles;cameras;feature extraction;learning (artificial intelligence);mobile robots;multi-robot systems;pose estimation;robot vision;SLAM (robots)","landing process;landing region;auto-landing scenario;original deep Q-learning solution;improved deep reinforcement learning method;autonomous landing problem;unmanned aerial vehicle;DRL method;deep Q-learning network;quadrotor landing;DQN based end-to-end control scheme","","10","","12","","7 Oct 2018","","","IEEE","IEEE Conferences"
"Intercept Strategy for Maneuvering Target Based on Deep Reinforcement Learning","X. Wang; Y. Cai; Y. Fang; Y. Deng","School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an; School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an; National Key Laboratory of Science Technology on Test Physics & Numerical Mathematics, Beijing; School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","3547","3552","A novel interception strategy for high-speed maneuvering target based on reinforcement learning (RL) algorithm is proposed. With reshaped reward function and observation consisting solely of relative position information and path angle, the interception policy is trained by Deep Deterministic Policy Gradient (DDPG) algorithm. The results of training and test indicate that the proposed guidance law can approximate the optimal control model of the maneuvering target interception and show the superiority over traditional method.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9549458","China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9549458","Exoatmospheric Interception;Maneuvering Target;Deep Reinforcement Learning;DDPG","Training;Navigation;Decision making;Optimal control;Reinforcement learning;Markov processes;Approximation algorithms","learning (artificial intelligence);missile guidance;optimal control;target tracking","high-speed maneuvering target;reinforcement learning algorithm;RL;reshaped reward function;relative position information;path angle;interception policy;Deep Deterministic Policy Gradient algorithm;maneuvering target interception;intercept strategy;Deep reinforcement learning;novel interception strategy","","4","","10","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Improved Performance for PMSM Control Based on Robust Controller and Reinforcement Learning","M. Nicola; C. -I. Nicola; C. Ionete; D. şendrescu; M. Roman","Research Department, National Institute for Research, Development and Testing in Electrical Engineering - ICMET Craiova, Craiova, Romania; Research Department, National Institute for Research, Development and Testing in Electrical Engineering - ICMET Craiova, Craiova, Romania; Department of Automation and Electronics, University of Craiova, Craiova, Romania; Department of Automation and Electronics, University of Craiova, Craiova, Romania; Department of Automation and Electronics, University of Craiova, Craiova, Romania","2022 26th International Conference on System Theory, Control and Computing (ICSTCC)","7 Nov 2022","2022","","","207","212","This article presents a Permanent Magnet Synchronous Motor (PMSM) control system which retains its performance for a significant variation of the parameters and load torque which represent disturbance for the control system. Classically, the PMSM control system is built in the form of a Field Oriented Control (FOC) control strategy structure built around PI speed (outer loop) and current (inner loop) controllers. We present the design stages and the numerical simulations performed in Matlab/Simulink, which prove the superiority of the robust control, by comparison with the classic FOC-type control structure. Because the Reinforcement Learning Twin-Delayed Deep Deterministic Policy Gradient (RL-TD3) agent is the most suitable for machine learning for process control, we synthesize a robust controller whose control quantities $\boldsymbol{u}_{d}$ and $\boldsymbol{u}_{q}$ are adjusted by a properly created and trained RL-TD3 agent. Using this robust combined controller plus RL-TD3 agent, superior performance is achieved in terms of response time and speed ripple.","2372-1618","978-1-6654-6746-9","10.1109/ICSTCC55426.2022.9931844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931844","control systems design;robust control;linear systems;reinforcement learning","Robust control;Torque;Process control;Reinforcement learning;Control systems;Numerical simulation;Robustness","angular velocity control;control system synthesis;machine vector control;permanent magnet motors;PI control;reinforcement learning;robust control;synchronous motors;torque control","classic FOC-type control structure;process control;robust controller;permanent magnet synchronous motor control system;PMSM control system;field oriented control control strategy;reinforcement learning twin-delayed deep deterministic policy gradient;RL-TD3 agent","","4","","20","IEEE","7 Nov 2022","","","IEEE","IEEE Conferences"
"Continuous-time Markov decision process with average reward: Using reinforcement learning method","S. Jia; L. Shen; H. Xue","College of Mechantronic Engineering and Automation, National University of Defense Technology, Changsha, Hunan, CN; College of Mechantronic Engineering and Automation, National University of Defense Technology, Changsha, Hunan, CN; College of Mechantronic Engineering and Automation, National University of Defense Technology, Changsha, Hunan, CN","2015 34th Chinese Control Conference (CCC)","14 Sep 2015","2015","","","3097","3100","Markov decision process (MDP) is a foundational framework of reinforcement learning advanced in sequential decision problems. Continuous-time Markov decision process (CTMDP) extends the discrete time MDP model by allowing actions to take place at any time. Prior work has little consideration on the reinforcement learning methods for solving CTMDPs. The aim of our article was to present a reinforcement learning approach based on the path of samples. For the key concept of performance potential function, a policy iteration algorithm with average reward was presented. Then, through the Robbins-Monro method, a temporal difference formula for evaluating the performance potential function was also proposed. Simulation results indicated that the presented algorithms could converge to the solution of the CTMDP problem at a proper speed.","1934-1768","978-9-8815-6389-7","10.1109/ChiCC.2015.7260117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7260117","Continuous-time Markov Decision Process;Reinforcement Learning;performance potential function","Markov processes;Learning (artificial intelligence);Mathematical model;Poisson equations;Random variables;Steady-state;Process control","decision making;iterative methods;learning (artificial intelligence);Markov processes","continuous-time Markov decision process;average reward;reinforcement learning method;CTMDP;sequential decision problems;discrete time MDP model;performance potential function;policy iteration algorithm;Robbins-Monro method;temporal difference formula","","3","","8","","14 Sep 2015","","","IEEE","IEEE Conferences"
"Reinforcement learning of dual-arm cooperation for a mobile manipulator with sequences of dynamical movement primitives","M. Deng; Y. Hu; Z. Li","College of Automation Science and Engineering, South China University of Technology, Guangzhou, China; College of Automation Science and Engineering, South China University of Technology, Guangzhou, China; College of Automation Science and Engineering, South China University of Technology, Guangzhou, China","2017 2nd International Conference on Advanced Robotics and Mechatronics (ICARM)","1 Feb 2018","2017","","","196","201","This paper presents a state-of-art reinforcement learning strategy to enable a human-like dual arm mobile robot to deal with some complicated tasks with dual arm cooperation. A complex movement task of robot can be divided into action phases and subgoals, each of which corresponds to a dynamic motion primitive (DMP). During control of the humanoid-like dual arm mobile robot, there are some noises that affect the precision of the movement of the robot. To deal with those uncertain perturbations while handling varying manipulation dynamics for grasping motion, a reinforcement learning (RL) algorithm with sequences of dynamical motion primitives strategy is proposed in this paper. To verify the effectiveness of the proposed strategy, a two-phase planning has been considered in the experiment, namely, the online redundancy resolution based on the neural-dynamic optimization algorithm to obtain the initial joint trajectories on the first trial, and the reinforcement learning of DMP in the learning process, where DMP is used to model the joint trajectories, and then reinforcement learning is employed to adjust the model to suppress uncertain perturbations.","","978-1-5386-3260-4","10.1109/ICARM.2017.8273159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8273159","Reinforcement Learning;Dual-arm Mobile Manipulations;Sequences of Dynamic Movement Primitive (SDMP);Redundancy resolution","Trajectory;Learning (artificial intelligence);Mobile robots;Robot sensing systems;Manipulators;Task analysis","humanoid robots;learning (artificial intelligence);manipulator dynamics;mobile robots;motion control;path planning","action phases;dynamic motion primitive;DMP;dual arm mobile robot;manipulation dynamics;reinforcement learning algorithm;dynamical motion primitives strategy;neural-dynamic optimization algorithm;learning process;dual-arm cooperation;mobile manipulator;dynamical movement primitives;complicated tasks;dual arm cooperation","","3","","9","IEEE","1 Feb 2018","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Approach to the Flexible Flowshop Scheduling Problem with Makespan Minimization","J. Zhu; H. Wang; T. Zhang","Department of Automation, Tsinghua University, Beijing, P. R. China; Department of Automation, Tsinghua University, Beijing, P. R. China; Department of Automation, Tsinghua University, Beijing, P. R. China","2020 IEEE 9th Data Driven Control and Learning Systems Conference (DDCLS)","7 Dec 2020","2020","","","1220","1225","Recent work has demonstrated the efficiency of deep reinforcement learning (DRL) in making optimization decisions in complex systems. Compared with other DRL algorithms, the proximal policy optimization (PPO) has higher stability and lower complexity. The typical flexible flowshop scheduling problem (FFSP) with identical parallel machines is an NP-hard problem. This paper is the first case to utilize PPO to solve the problem with makespan minimization. The particular state, action and reward function are designed for the FFSP to follow the Markov property. The efficiency of PPO is evaluated on the wafer pickling instance and random instances with different scales. The results show that PPO can always provide satisfactory solutions within a reasonable computational time.","","978-1-7281-5922-5","10.1109/DDCLS49620.2020.9275080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9275080","Deep Reinforcement Learning;Proximal Policy Optimization;Flexible Flowshop Scheduling;Makespan","Heuristic algorithms;Pickling;Job shop scheduling;Reinforcement learning;Optimization;Processor scheduling;Parallel machines","flow shop scheduling;learning (artificial intelligence);Markov processes;minimisation;parallel machines","makespan minimization;deep reinforcement learning approach;typical flexible flowshop scheduling problem;NP-hard problem;proximal policy optimization decisions;identical parallel machine;Markov property","","3","","18","IEEE","7 Dec 2020","","","IEEE","IEEE Conferences"
"A Multi-agent Traffic Signal Control System Using Reinforcement Learning","W. Wu; G. Haifei; J. An","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China","2009 Fifth International Conference on Natural Computation","28 Dec 2009","2009","4","","553","557","This paper presents a control method based on multi-agent for traffic signals. A reinforcement learning algorithm is used to optimize traffic flow in the intersection. The genetic algorithm intends to introduce a global optimization criterion to each of the local learning processes that optimize the cycle of traffic signals and green-ratio. Area-wide coordination is achieved by game theory. We combine local optimization with global optimization to optimize traffic signal in multi-intersection. Simulation results indicate that our presented method is superior than traditional control one.","2157-9563","978-0-7695-3736-8","10.1109/ICNC.2009.66","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362982","multi-agent;genetic algorithm;reinforcement learning;game theory;optimization and coordination","Control systems;Learning;Traffic control;Communication system traffic control;Game theory;Bismuth;Genetic algorithms;Signal processing;Centralized control;Automatic control","game theory;genetic algorithms;learning (artificial intelligence);road traffic","multiagent traffic signal control system;reinforcement learning;genetic algorithm;global optimization criterion;game theory","","3","","10","IEEE","28 Dec 2009","","","IEEE","IEEE Conferences"
"Model-Based Reinforcement Learning For Robot Control","X. Li; W. Shang; S. Cong","Department of Automation, University of Science and Technology of China, Hefei, P. R. China; Department of Automation, University of Science and Technology of China, Hefei, P. R. China; Department of Automation, University of Science and Technology of China, Hefei, P. R. China","2020 5th International Conference on Advanced Robotics and Mechatronics (ICARM)","14 Sep 2020","2020","","","300","305","Model-free deep reinforcement learning (MFRL) algorithms have achieved many impressive results. But they are generally stricken with high sample complexity, which puts forward a critical challenge for their application to real-world robots. Dynamic models are essential for robot control laws, but it is often hard to obtain accurate analytical dynamic models. Therefore a data-driven approach to learning models becomes significant for reinforcement learning to increase data efficiency. Model-based algorithms are effective methods to reduce sample complexity by learning the system dynamic model. However, in certain environments, it has been proven that learning an accurate system dynamic model is a formidable problem, and their asymptotic performance cannot achieve to the same level as model-free algorithms. In our work, we use an ensemble of deep neural networks to learn system dynamics and incorporate model uncertainty. Then in order to merge the high asymptotic performance of the advanced model-free methods, the deep deterministic policy gradient (DDPG) algorithm is adopted to optimize robot control policy. Furthermore, it has been implemented within ROS for controlling a Baxter robot in the simulation environment.","","978-1-7281-6479-3","10.1109/ICARM49381.2020.9195341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9195341","","Robots;Heuristic algorithms;Training;Neural networks;Learning (artificial intelligence);Data models;System dynamics","dexterous manipulators;learning (artificial intelligence);learning systems;neural nets","data-driven approach;learning models;model-based algorithms;system dynamic model;model-free algorithms;deep neural networks;system dynamics;model uncertainty;model-free methods;deep deterministic policy gradient algorithm;robot control policy;Baxter robot;model-based reinforcement;model-free deep reinforcement learning algorithms;real-world robots;robot control laws;analytical dynamic models;MFRL;DDPG;ROS","","2","","24","IEEE","14 Sep 2020","","","IEEE","IEEE Conferences"
"Optimal Energy Management of Energy Internet: A Distributed Actor-Critic Reinforcement Learning Method","Y. Cheng; J. Peng; X. Gu; F. Jiang; H. Li; W. Liu; Z. Huang","School of Automation, Central South University; School of Computer Science and Engineering, Central South University; School of Automation, Central South University; School of Computer Science and Engineering, Central South University; School of Computer Science and Engineering, Central South University; School of Computer Science and Engineering, Central South University; School of Automation, Central South University","2020 American Control Conference (ACC)","27 Jul 2020","2020","","","521","526","Owning to the capacity constraints and the uneven distribution of resources, energy management problem in energy internet is a major concern. To cope with the variations and complexity of large scale energy management, a distributed actor-critic reinforcement learning based method is proposed for optimal energy management. First, the intelligent action is decided in the distributed agent to alleviate the pressure on centralized intelligent computing. The distributed action of each agent is based on its neighbour information, and an actor-critic reinforcement learning algorithm is applied for dealing with the continuous action space. Then, aiming at the supply-demand balance, the action is adjusted based on global information exchange. After action adjustment, the corresponding rewards are sent to each agent. Finally, the modified action is executed in each agent under the condition of the supply-demand balance. And received rewards are utilized to update each agent. Simulation driven by Pecan Street Inc.s Dataport demonstrates that the proposed intelligent distributed method is effective.","2378-5861","978-1-5386-8266-1","10.23919/ACC45564.2020.9148019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9148019","","Energy management;Learning (artificial intelligence);Information exchange;Generators;Power generation;Load modeling;Erbium","energy management systems;learning (artificial intelligence);power engineering computing","intelligent distributed method;modified action;action adjustment;supply-demand balance;continuous action space;actor-critic reinforcement learning algorithm;distributed action;centralized intelligent computing;distributed agent;intelligent action;distributed actor-critic reinforcement learning based method;large scale energy management;energy management problem;distributed actor-critic reinforcement learning method;energy internet;optimal energy management","","2","","18","","27 Jul 2020","","","IEEE","IEEE Conferences"
"Autonomous Maneuver Decision of UAV Based on Deep Reinforcement Learning: Comparison of DQN and DDPG","Y. Wang; T. Ren; Z. Fan","School of Automation, Shenyang Aerospace University, Shenyang; School of Automation, Shenyang Aerospace University, Shenyang; School of Automation, Shenyang Aerospace University, Shenyang","2022 34th Chinese Control and Decision Conference (CCDC)","14 Feb 2023","2022","","","4857","4860","In this paper, the advantages and disadvantages of two kinds of deep reinforcement learning algorithms DQN and DDPG applying in UAV autonomous decision-making are compared. Firstly, DQN and DDPG models are designed to realize UAV maneuver decision-making in air combat. Secondly, the two models are trained to pursue the target UAV making uniform rectilinear motion respectively. Finally, the confrontation experiments between the two models are carried out. The results show that DDPG has the advantages of action continuity and shorter decision time. But DQN has better decision-making ability under the same model complexity.","1948-9447","978-1-6654-7896-0","10.1109/CCDC55256.2022.10033863","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10033863","UAV;Decision-making;Air combat;Deep reinforcement learning;comparison of DQN and DDPG","Deep learning;Atmospheric modeling;Decision making;Reinforcement learning;Complexity theory;Convergence","autonomous aerial vehicles;decision making;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;reinforcement learning","air combat;DDPG;decision-making ability;deep reinforcement learning algorithms DQN;model complexity;shorter decision time;UAV autonomous decision-making;UAV maneuver decision-making;uniform rectilinear motion","","2","","11","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Hybrid Reinforcement Learning for Optimal Control of Non-Linear Switching System","X. Li; L. Dong; L. Xue; C. Sun","School of Artificial Intelligence, Anhui University, Hefei 230601, China.; School of Cyber Science and Engineering, Southeast University, Nanjing 211189, China.; School of Automation, Southeast University, Nanjing 210096, China, and also with the Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing 210096, China.; School of Automation, Southeast University, Nanjing 210096, China, and also with the Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing 210096, China.","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","10","Based on the reinforcement learning mechanism, a data-based scheme is proposed to address the optimal control problem of discrete-time non-linear switching systems. In contrast to conventional systems, in the switching systems, the control signal consists of the active mode (discrete) and the control inputs (continuous). First, the Hamilton-Jacobi-Bellman equation of the hybrid action space is derived, and a two-stage value iteration method is proposed to learn the optimal solution. In addition, a neural network structure is designed by decomposing the Q-function into the value function and the normalized advantage value function, which is quadratic with respect to the continuous control of subsystems. In this way, the Q-function and the continuous policy can be simultaneously updated at each iteration step so that the training of hybrid policies is simplified to a one-step manner. Moreover, the convergence analysis of the proposed algorithm with consideration of approximation error is provided. Finally, the algorithm is applied evaluated on three different simulation examples. Compared to the related work, the results demonstrate the potential of our method.","2162-2388","","10.1109/TNNLS.2022.3156287","National Key Research and Development Program of China(grant numbers:2018AAA0101400); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20202006); National Natural Science Foundation of China(grant numbers:61921004,62173251); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9756440","Adaptive dynamic programming (DP);hybrid action space;normalized advantage value function (NAF);reinforcement learning (RL);switching system.","Switching systems;Switches;Aerospace electronics;Optimal control;Heuristic algorithms;Artificial neural networks;Approximation algorithms","","","","2","","","IEEE","13 Apr 2022","","","IEEE","IEEE Early Access Articles"
"A Cooperative Guidance Law for Multiple Missiles based on Reinforcement Learning","H. Chen; J. Yu; X. Dong","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","2022 IEEE International Conference on Unmanned Systems (ICUS)","29 Dec 2022","2022","","","1473","1478","The traditional proportional guidance law lacks the limitation of time and field of view. In order to realize the coordinated attack of multiple missiles on targets and improve the attack efficiency, a reinforcement learning cooperative guidance law based on deep deterministic policy gradient descent neural network is proposed. According to the particularity of guidance process, the reinforcement learning agent is obtained by constructing state space, action space and reward function training. The simulation results show that the enhanced learning guidance law can strike maneuvering targets simultaneously and satisfy the field of view constraint, which is superior to the traditional cooperative proportional guidance law.","2771-7372","978-1-6654-8456-5","10.1109/ICUS55513.2022.9986718","National Natural Science Foundation of China(grant numbers:62103016,61922008,62103023,61973013,61873011,62103016); CAST(grant numbers:2021QNRC001); China Postdoctoral Science Foundation(grant numbers:2020M680297); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9986718","missile;cooperative guidance law;time constraint;field of view constraint;deep deterministic policy gradient algorithm","Training;Missiles;Simulation;Neural networks;Process control;Clustering algorithms;Reinforcement learning","cooperative systems;deep learning (artificial intelligence);gradient methods;military computing;missile guidance;reinforcement learning;target tracking","action space;attack efficiency;cooperative guidance law;coordinated attack;deep deterministic policy gradient descent neural network;enhanced learning guidance law;missiles;proportional guidance law;reinforcement learning agent;reward function training;state space","","1","","11","IEEE","29 Dec 2022","","","IEEE","IEEE Conferences"
"Model-Free Load Frequency Control Based on Multi-Agent Deep Reinforcement Learning","G. -X. Liu; Z. -W. Liu; G. -X. Wei","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","2021 IEEE International Conference on Unmanned Systems (ICUS)","22 Dec 2021","2021","","","815","819","This paper proposes a model-free method for load frequency control (LFC) problem of power system with multiple generators based on multi-agent deep deterministic policy gradient (MADDPG), which can deal with the continuous state and action control problem such as LFC problem. A centralized learning is used by the proposed MADDPG method based on a global action-value function to restore the frequency and power to setting values of the whole system while load fluctuating. In order to demonstrate the effectiveness of the proposed method, a numerical simulation of MADDPG is included.","","978-1-6654-3885-8","10.1109/ICUS52573.2021.9641432","State Grid Corporation Science and Technology(grant numbers:5100-202099522A-0-0-00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9641432","load frequency control;deep reinforcement learning;automatic control generation;deep deterministic policy gradient","Training;Recurrent neural networks;Reinforcement learning;Numerical simulation;Feature extraction;Generators;Numerical models","frequency control;learning (artificial intelligence);load regulation;multi-agent systems;power system control","model-free load frequency control;multiagent deep reinforcement learning;model-free method;load frequency control problem;power system;multiple generators;multiagent deep deterministic policy gradient;continuous state;action control problem;LFC problem;centralized learning;MADDPG method;global action-value function;load fluctuating","","1","","13","IEEE","22 Dec 2021","","","IEEE","IEEE Conferences"
"Optimization of PV Energy Conversion System Using Reinforcement Learning Algorithm","M. A. Zeddini; M. Turki; M. F. Mimoun","Research Laboratory of Automation, Electrical Systems and Environment (LAS2E), National Engineering School of Monastir (ENIM), Monastir, Tunisia; Research Laboratory of Automation, Electrical Systems and Environment (LAS2E), National Engineering School of Monastir (ENIM), Monastir, Tunisia; Research Laboratory of Automation, Electrical Systems and Environment (LAS2E), National Engineering School of Monastir (ENIM), Monastir, Tunisia","2020 20th International Conference on Sciences and Techniques of Automatic Control and Computer Engineering (STA)","26 Jan 2021","2020","","","249","254","This paper proposes a novel MPPT algorithm using a reinforcement learning (RL) to track the Global Maximum Power Point (GMPP) for photovoltaic (PV) applications. The RL MPPT algorithm was validated by simulation studies under Matlab-simulink for a 2.5 kW PV conversion system based on 5*4 PV modules, a DC/DC converter and a resistive Load. In order to enhance the searching ability of proposed MPPT algorithm, a load and irradiation variations are introduced on simulations tests. In particular, a changing of partial shading condition (PSC) is undertaken to change the position and the value of the GMPP a lot of time for improving the efficiency of the algorithm.","2573-539X","978-1-7281-8815-7","10.1109/STA50679.2020.9329331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9329331","MPPT;Reinforcement Learning;Partial Shading","Photovoltaic systems;Radiation effects;Perturbation methods;Reinforcement learning;Energy conversion;Optimization;Load modeling","learning (artificial intelligence);load (electric);maximum power point trackers;optimisation;photovoltaic power systems;power engineering computing;solar cell arrays","PV energy conversion system;reinforcement learning algorithm;GMPP;photovoltaic applications;RL MPPT algorithm;irradiation variations;optimization method;global maximum power point tracking;Matlab-simulink simulation;PV modules;DC-DC converter;resistive load;load variations;partial shading condition;PSC;power 2.5 kW","","1","","15","IEEE","26 Jan 2021","","","IEEE","IEEE Conferences"
"Network Selection in 5G Networks Based on Markov Games and Friend-or-Foe Reinforcement Learning","A. Giuseppi; E. De Santis; F. Delli Priscoli; S. H. Won; T. Choi; A. Pietrabissa","Department of Computer, Control, and Management Engineering “Antonio Ruberti”, University of Rome La Sapienza, Rome, Italy; Department of Computer, Control, and Management Engineering “Antonio Ruberti”, University of Rome La Sapienza, Rome, Italy; Department of Computer, Control, and Management Engineering“Antonio Ruberti”, University of Rome La Sapienza, Rome, Italy; Future Mobile Communication Lab and the Intelligent Network Research Lab of ETRI, Daejoen, Republic of Korea; Future Mobile Communication Lab and the Intelligent Network Research Lab of ETRI, Daejoen, Republic of Korea; Department of Computer, Control, and Management Engineering “Antonio Ruberti”, University of Rome La Sapienza, Rome, Italy","2020 IEEE Wireless Communications and Networking Conference Workshops (WCNCW)","25 Jun 2020","2020","","","1","5","This paper presents a control solution for the optimal network selection problem in 5G heterogeneous networks. The control logic proposed is based on multi-agent Friend-or-Foe Q-Learning, allowing the design of a distributed control architecture that sees the various access points compete for the allocation of the connection requests. Numerical simulations validate conceptually the approach, developed in the scope of the EU-Korea project 5G-ALLSTAR","","978-1-7281-5178-6","10.1109/WCNCW48565.2020.9124723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9124723","Multi-Agent Reinforcement Learning;5G;Network Selection;Markov Games","Q-learning;5G mobile communication;Conferences;Decentralized control;Games;Markov processes;Numerical simulation","5G mobile communication;distributed control;game theory;learning (artificial intelligence);Markov processes;telecommunication control","5G networks;control solution;optimal network selection problem;5G heterogeneous networks;control logic;distributed control architecture;EU-Korea project 5G-ALLSTAR;Markov games;friend-or-foe reinforcement learning","","1","","14","IEEE","25 Jun 2020","","","IEEE","IEEE Conferences"
"Dynamic Load Shedding Strategy Using Distributional Deep Reinforcement Learning in Power System Emergency Control","S. Chen; Y. Bai; Z. Jun","School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China","2020 IEEE 4th Conference on Energy Internet and Energy System Integration (EI2)","15 Feb 2021","2020","","","248","253","With the uncertainty and complexity of power system control improved, emergency control strategies are facing significant challenge on adaptiveness and robustness. This paper applies a distributional deep reinforcement learning method in dynamic load shedding, which allow agents at different buses take collaborative actions in a distributed way. These agents are centrally trained and separately executed, which can have mutual collaboration with others. To validate the effectiveness of DDRL, our simulations are implemented on an open-source platform named Reinforcement Learning for Grid Control. Furthermore, we make comparisons and analysis in the IEEE 39-bus system to evaluate the performance of distributional deep reinforcement learning, and the results have demonstrated that the proposed method have satisfied adaptiveness and robustness.","","978-1-7281-9606-0","10.1109/EI250167.2020.9346749","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9346749","distributional deep reinforcement learning;deep Q-network;transient voltage recovery;emergency control;load shedding","Uncertainty;Power system dynamics;Collaboration;Reinforcement learning;Load shedding;Robustness;Voltage control","cooperative systems;fuzzy systems;learning (artificial intelligence);load shedding;power system control","IEEE 39-bus system;Grid Control;distributional deep reinforcement learning method;robustness;adaptiveness;emergency control strategies;power system control;uncertainty;power system emergency control;dynamic load shedding strategy","","1","","16","IEEE","15 Feb 2021","","","IEEE","IEEE Conferences"
"Maximum Power Point Tracking Based on Reinforcement Learning in Photovoltaic System","D. Lin; X. Li; S. Ding","School of Electrical and Automation Engineering, Nanjing Normal University, China; School of Electrical and Automation Engineering, Nanjing Normal University, China; School of Electrical and Automation Engineering, Nanjing Normal University, China","2020 IEEE International Conference on Power Electronics, Drives and Energy Systems (PEDES)","24 Mar 2021","2020","","","1","6","Maximum power point tracking (MPPT) technology is usually used in photovoltaic (PV) systems to extract the maximum power. Although the conventional MPPT techniques are easy to be implemented, they have to tune their control parameters by using trial-and-error method, which is not adaptive to different working conditions. Unlike the conventional MPPT techniques, the reinforcement learning-based MPPT (RL-MPPT) method has advantages of self-learning ability, which is better applicable performance under different weather conditions. To evaluate the RL-MPPT method, the simulations of Standard Test Conditions (STC) and varying irradiance conditions are performed.","","978-1-7281-5672-9","10.1109/PEDES49360.2020.9379644","National Natural Science Foundation of China(grant numbers:51977112); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9379644","Maximum power point tracking;Reinforcement Learning (RL);PV system","Maximum power point trackers;Photovoltaic systems;Reinforcement learning;Power electronics;Steady-state;Standards;Oscillators","control system synthesis;learning (artificial intelligence);maximum power point trackers;photovoltaic power systems;power engineering computing;power generation control","photovoltaic system;maximum power point tracking technology;trial-and-error method;reinforcement learning-based MPPT method;RL-MPPT method;control parameters;self-learning ability;weather conditions;standard test conditions;irradiance conditions","","1","","15","IEEE","24 Mar 2021","","","IEEE","IEEE Conferences"
"Steady state controller design for aero-engine based on reinforcement learning NNs","H. Zhang; S. Wei; G. Xu","School of Automation, Shenyang Aerospace University, Shenyang, China; School of Automation, Shenyang Aerospace University, Shenyang, China; School of Automation, Shenyang Aerospace University, Shenyang, China","2017 29th Chinese Control And Decision Conference (CCDC)","17 Jul 2017","2017","","","2168","2173","An aero-engine optimal steady state controller based on reinforcement learning neural networks (NNs) was proposed in this paper. The presented reinforcement learning NNs can achieve the optimal control objective by constructing two interconnected modules (i.e. action module and critic module). For the state variable models of small perturbation on steady operating points, the double-variable control of an aero-engine is accomplished by two similar backing propagation (BP) NNs. The simulation results show that the presented controller has the perfect performance with the smooth transition process. It not only has strong anti-interference ability and adaptability, but also has excellent robustness to the change of aero-engine model parameters.","1948-9447","978-1-5090-4657-7","10.1109/CCDC.2017.7978874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7978874","Aero-engine;Reinforcement learning neural networks;Robustness","Cost function;Steady-state;Learning (artificial intelligence);Artificial neural networks;Adaptation models;Optimal control","aerospace engines;backpropagation;control system synthesis;interconnected systems;neurocontrollers;optimal control;perturbation techniques","steady state controller design;aeroengine optimal steady state controller;reinforcement learning neural networks;reinforcement learning NN;interconnected modules;action module;critic module;state variable models;perturbation models;steady operating points;double-variable control;backpropagation NN;BP NN;smooth transition process;antiinterference ability;adaptability;aeroengine model parameters","","1","","12","IEEE","17 Jul 2017","","","IEEE","IEEE Conferences"
"Walking parameters design of biped robots based on reinforcement learning","Z. Liang; S. Zhu; X. Jin","College of Automation, Nanjing University of Posts and Telecommunications, 210046, China; College of Automation, Nanjing University of Posts and Telecommunications, 210046, China; Key Lab of MCCSE, Ministry of Education, Southeast University, Nanjing 210096, China","Proceedings of the 30th Chinese Control Conference","25 Aug 2011","2011","","","4017","4022","Biped walking pattern is one of the most difficult problems in the humanoid robot area, and there exists no ideal algorithm for a generalized walking scenario. Recently, some methods have been proposed, including trajectory planning, passive dynamic walk. In this paper, based on the solution of inverse kinematics of a leg by combining analysis method with numerical method, trajectory planning method is used to implement the humanoid robot walking skill in a 3D simulation environment. In order to get the walking parameters automately, reinforcement learning is studied and implemented by the train system of Apollo3D program, and the training algorithm is well tested in the RoboCup3D simulation platform.","2161-2927","978-988-17255-9-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6000935","Biped Robots;Gait Planning;Reinforcement Learning;Robocup3D","Legged locomotion;Electronic mail;Planning;Trajectory;Humanoid robots;Algorithm design and analysis","humanoid robots;learning (artificial intelligence);legged locomotion;multi-robot systems;numerical analysis;path planning;robot kinematics;solid modelling","walking parameter design;biped robots;reinforcement learning;humanoid robot;passive dynamic walk;trajectory planning method;Apollo3D program;RoboCup3D simulation platform","","","","10","","25 Aug 2011","","","IEEE","IEEE Conferences"
"Pursuit-Evasion Game of Unmanded Surface Vehicles Based on Deep Reinforcement Learning","X. Wang; Y. Wang; W. Zhou; J. Zhang","School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; College of Information Engineering, Shanghai Maritime University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China","2023 4th International Conference on Electronic Communication and Artificial Intelligence (ICECAI)","13 Jul 2023","2023","","","358","363","Making decisions during the pursuit-evasion game of unmanned surface vehicles (USVs) in a restricted environment with obstacles is a challenging problem. Specifically, in the pursuit game, the pursuer needs to consider how to approach the evader quickly and how to surround the evader and safely avoid obstacles in an environment containing obstacles. This paper proposes a distributed algorithm based on deep reinforcement learning to help USV solve the pursuit problem in a restricted environment. The proposed algorithm can deal with the game problem of multiple USVs on the ocean’s surface. In particular, composite reward function with guiding characteristics are designed based on the artificial potential field (APF) method for pursuit, encirclement, and obstacle avoidance, which can help the USV improve pursuit performance. Then, curriculum learning is used to help the USV improve learning efficiency in the early stage. The simulation results show that the algorithm is effective in different initial conditions and performs well.","","979-8-3503-1449-6","10.1109/ICECAI58670.2023.10176487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10176487","USVs;deep reinforcement learning;pursuit-evasion game;APF","Deep learning;Training;Sea surface;Simulation;Games;Reinforcement learning;Distributed algorithms","autonomous aerial vehicles;collision avoidance;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;multi-robot systems;reinforcement learning;unmanned surface vehicles","curriculum learning;deep reinforcement learning;environment containing obstacles;evader;game problem;learning efficiency;multiple USVs;obstacle avoidance;ocean;pursuit game;pursuit performance;pursuit problem;pursuit-evasion game;restricted environment;unmanded surface vehicles;unmanned surface vehicles;USV","","","","14","IEEE","13 Jul 2023","","","IEEE","IEEE Conferences"
"A DDPG Algorithm Based Reinforcement Learning Controller for Three-Phase DC-AC Inverters","J. Ye; S. Mei; H. Guo; Y. Hu; X. Zhang","School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, Shenzhen, China; School of Electrical, Electronic and Computer Engineering, University of Western Australia, Perth, Australia; School of Electrical, Electronic and Computer Engineering, University of Western Australia, Perth, Australia","2023 International Conference on Power Energy Systems and Applications (ICoPESA)","5 Jun 2023","2023","","","429","434","This paper proposes a DC-AC inverter controller based on reinforcement learning (RL) algorithm. Compared with the traditional PID control method, the structure of the RL algorithm based controller is simpler. The deep deterministic policy gradient (DDPG) algorithm in RL algorithm is used to realize model-free control of inverters, so that the control algorithm has adaptive ability to different types of DC-AC inverters. It can avoid the dependence of the control strategy on the system model. Through simulation, the control strategy of three-phase two-level DC-AC inverter based on DDPG algorithm is compared with the traditional PID control. The simulation results show that the total harmonic distortion (THD) is reduced by 12% and the current tracking performance is improved by at least 75%.","","979-8-3503-4560-5","10.1109/ICoPESA56898.2023.10141128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10141128","reinforcement learning;deep deterministic policy gradient algorithm;neural network;model-free control;DC-AC inverter","Training;Adaptation models;Total harmonic distortion;PI control;Heuristic algorithms;Simulation;Reinforcement learning","control engineering computing;control system synthesis;DC-AC power convertors;gradient methods;harmonic distortion;reinforcement learning;three-term control","DDPG algorithm;deep deterministic policy gradient algorithm;model-free control;PID control method;reinforcement learning controller;RL algorithm based controller;THD;three-phase two-level DC-AC inverter controller;total harmonic distortion","","","","10","IEEE","5 Jun 2023","","","IEEE","IEEE Conferences"
"Optimal Scheduling for a Multi-Energy Microgrid by a Soft Actor-Critic Deep Reinforcement Learning","Y. Luo; C. Liu; Q. Lai","School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China","2022 IEEE Power & Energy Society General Meeting (PESGM)","27 Oct 2022","2022","","","1","5","The uncertainties and intermittency of renewable generations and random fluctuating loads have brought great challenges to the operation of multi-energy microgrids (MEMGs). To deal with the uncertainties, the traditional meth-ods, such as the stochastic optimization and model predictive control, require accurate parametric model to describe the uncertainties. In this paper, a deep reinforcement learning meth-od based on the soft actor-critic (SAC) algorithm is proposed to achieve the optimal scheduling of MEMGs without need of the exact parameters about the uncertainties. Firstly, a deterministic optimal scheduling model is established without considering the uncertainties in an MEMG. Then, the established model is trans-ferred into as a Markov Decision Process (MDP) problem, where the uncertainties are involved in the state space, and the action space and reward function are designed based on the prior knowledge. Next, a SAC-based deep reinforcement learning method is proposed to solve the MDP problem. Finally, the simu-lation results on a demonstrative MEMG validate the effective-ness of the proposed method.","1944-9933","978-1-6654-0823-3","10.1109/PESGM48719.2022.9916785","National Natural Science Foundation of China(grant numbers:52007133); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9916785","―Multi-energy microgrids;soft actor-critic;deep reinforcement learning;Markov Decision Process","Renewable energy sources;Uncertainty;Simulation;Optimal scheduling;Reinforcement learning;Microgrids;Markov processes","","","","","","14","IEEE","27 Oct 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Image Understanding","F. Xiang; Z. Wang; X. Yuan","College of Electromechanical Engineering and Automation, National University of Defense Technology, Changsha, Hunan, China; College of Electromechanical Engineering and Automation, National University of Defense Technology, Changsha, Hunan, China; College of Electromechanical Engineering and Automation, National University of Defense Technology, Changsha, Hunan, China","2013 Third International Conference on Intelligent System Design and Engineering Applications","7 Feb 2013","2013","","","1102","1105","Reinforcement Learning is one of the hottest issues in current AI research fields. It's a effective method in solving some machine learning problems. It's high efficiency, simpler programming, easier understanding, and better performance. Here I will share my understanding. If there are something wrong, thanks for correct. In reinforcement learning, the learner is a decision-making agent that takes actions in an environment and receives reward (or penalty) for its actions in trying to solve a problem. After a set of trial-and-error runs, it should learn the best policy, which is the sequence of actions that maximize the total reward.","","978-1-4673-4893-5","10.1109/ISDEA.2012.261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6456071","Reinforcement Learning;Image Understanding;Machine Learning;Artificial Intelligence","Learning;Robots;Machine learning;Markov processes;Learning systems;Hidden Markov models;Image segmentation","image processing;learning (artificial intelligence)","reinforcement learning;image understanding;machine learning;decision-making agent","","","","6","IEEE","7 Feb 2013","","","IEEE","IEEE Conferences"
"Research on Cooperative Control of Traffic Signals based on Deep Reinforcement Learning","L. Fan; Y. Yang; H. Ji; S. Xiong","School of Automation, Beijing Information Science & Technology University, Beijing, China; School of Automation, Beijing Information Science & Technology University, Beijing, China; School of Electronics & Control Engineering, North China University of Technology, Beijing, China; School of Automation, Beijing Information Science & Technology University, Beijing, China","2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)","7 Jul 2023","2023","","","1608","1612","With the increasing traffic pressure, the problem of road congestion needs to be solved urgently. In recent years, many researchers have used deep reinforcement learning to solve traffic signal control problems, but most of them are based on the situation of single intersection or adjacent intersection, with limitations. This paper proposes an index to evaluate the regional traffic performance, and uses this index to design a traffic signal coordination control algorithm based on deep Q network. The experimental results on the SUMO simulation platform show that the proposed algorithm has better performance in terms of average waiting time and average time loss compared with the other three signal control algorithms.","2767-9861","979-8-3503-2105-0","10.1109/DDCLS58216.2023.10167232","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10167232","multi-agent;deep reinforcement learning;deep Q-network;traffic signal control;SUMO","Deep learning;Measurement;Learning systems;Simulation;Roads;Buildings;Reinforcement learning","deep learning (artificial intelligence);reinforcement learning;road traffic;road traffic control;road vehicles;traffic control;traffic engineering computing","adjacent intersection;deep Q network;deep reinforcement learning;increasing traffic pressure;regional traffic performance;road congestion;signal control algorithms;single intersection;traffic signal control problems;traffic signal coordination control algorithm;traffic signals","","","","12","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"Nash Tracking Controls of Multi-input Nonzero-Sum Game System with Reinforcement Learning","Y. o. Lv; X. Ren; L. Li; J. Na","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; Faculty of Mechanical & Electrical Engineering, Kunming University of Science & Technology, Kunming, China","2018 37th Chinese Control Conference (CCC)","7 Oct 2018","2018","","","2765","2769","This paper addresses Nash optimal tracking controls of the unknown multi-player game system with a reinforcement learning scheme. We propose a single-layer neural network to approximate the unknown multi-player system, where the system can be accurately identified. It is then used to calculate the Nash tracking controls, which are designed by two parts: the steady-state control and the optimal feedback tracking control. The optimal feedback tracking controls are obtained by using the HJB equation with the reinforcement learning scheme. The convergences of the NN weights and the approximated optimal controls are analyzed. Finally, a simulation is provided to illustrate the effectiveness of the methods in this paper.","1934-1768","978-988-15639-5-8","10.23919/ChiCC.2018.8483384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8483384","Reinforcement learning;nonzero-sum game;neural networks;Nash equilibrium;multi-input system","Artificial neural networks;Games;Learning (artificial intelligence);Mathematical model;Nash equilibrium;Convergence","approximation theory;feedback;game theory;learning (artificial intelligence);neurocontrollers;optimal control","reinforcement learning scheme;approximated optimal controls;Nash tracking controls;multiinput nonzero-sum game system;unknown multiplayer game system;single-layer neural network;steady-state control;optimal feedback tracking control;Nash optimal tracking controls;HJB equation;NN weights","","","","23","","7 Oct 2018","","","IEEE","IEEE Conferences"
"Air Combat Maneuver Decision Based on Deep Reinforcement Learning and Game Theory","S. Yin; Y. Kang; Y. Zhao; J. Xue","Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; College of Engineering and Information Technology, University of Chinese Academy of Sciences, Beijing, China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","6939","6943","The autonomous maneuver decision of UA V plays an important role in future air combat. However, the strong competitiveness of the air combat environment and the uncertainty of the opponent make it difficult to solve the optimal strategy. For these problems, we propose the algorithm based on deep reinforcement learning and game theory, which settles the matter that the existing methods cannot solve Nash equilibrium strategy in highly competitive environment. Specifically, 1 vl air combat is modeled as a two-player zero-sum Markov game, and a simplified two-dimensional simulation environment is constructed. We prove that the algorithm has good convergence through the simulation test. Compared with the opponent's strategy using DQN, our algorithm has better air combat performance and is more suitable for the air combat game environment.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9901992","National Key Research and Development Program of China(grant numbers:2018AAA0100801); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9901992","Air Combat;Reinforcement Learning;Game Theory;Maneuver Decision","Uncertainty;Atmospheric modeling;Simulation;Reinforcement learning;Games;Markov processes;Nash equilibrium","autonomous aerial vehicles;control engineering computing;deep learning (artificial intelligence);game theory;Markov processes;reinforcement learning","1 vl air combat;air combat game environment;air combat maneuver decision;air combat performance;deep reinforcement learning;DQN;future air combat;game theory;highly competitive environment;Nash equilibrium strategy;optimal strategy;two-dimensional simulation environment;two-player zero-sum Markov game;UA V autonomous maneuver decision","","","","18","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Analysis on the locomotion of cable tunnel inspection quadruped robot based on deep reinforcement learning","C. Wu; Y. Zhou; Y. Zhang; H. Li; X. Wang; Z. Li; Y. Xu; P. Ni","People's Republic of China, State Grid Shanghai Cable Company, Shanghai, China; People's Republic of China, State Grid Shanghai Cable Company, Shanghai, China; People's Republic of China, State Grid Shanghai Cable Company, Shanghai, China; People's Republic of China, State Grid Shanghai Cable Company, Shanghai, China; People's Republic of China, State Grid Shanghai Cable Company, Shanghai, China; Shuoneng (Shanghai) Automation Technology Co. Ltd.; Shuoneng (Shanghai) Automation Technology Co. Ltd.; Shuoneng (Shanghai) Automation Technology Co. Ltd.","22nd International Symposium on High Voltage Engineering (ISH 2021)","20 Jun 2022","2021","2021","","1995","1999","The quadruped robot applying the expert skill learning system can learn and generate adaptive skills from a group of representative expert skills, screen and master these combined skills through deep reinforcement learning, so as to select different skills in different environments to move in stranger environment. This approach leverages the advantages of trained expert skills and the fast online synthesis of adaptive policies to generate responsive motor skills during the changing tasks. The cable tunnel unmanned inspection quadruped robot is equipped with a 5-DOF mechanical arm with a sensor module on the top to realize the cable condition detection. The combination of the intelligent sensors can make the robot obtain the ability of environment perception similar to human beings, so that the quadruped robot can complete the instructions safely and accurately in the cable tunnel environment, collect and analyse the environment and cable condition parameters, find problems and give feedback in time.","","978-1-83953-605-2","10.1049/icp.2022.0457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9800408","","","inspection;intelligent sensors;learning (artificial intelligence);legged locomotion;mobile robots;motion control;position control;robot kinematics;service robots;tunnels","adaptive policies;adaptive skills;cable condition detection;cable condition parameters;cable tunnel environment;cable tunnel inspection;deep reinforcement learning;different skills;environment perception;expert skill learning system;fast online synthesis;inspection quadruped robot;master these combined skills;representative expert skills;responsive motor skills;stranger environment;trained expert skills","","","","","","20 Jun 2022","","","IET","IET Conferences"
"Deep Reinforcement Learning Based Mobile Robot Navigation Using Sensor Fusion","K. Yan; J. Gao; Y. Li","School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, Shenzhen, P. R. China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, Shenzhen, P. R. China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, Shenzhen, P. R. China","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","4125","4130","At present, mobile robot navigation usually uses the traditional SLAM method, which can accurately obtain the environment information, but it is difficult to effectively model the environment if the environment changes greatly in the later stage or in dynamic and complex scenes. In order to make the mobile robot better adaptable to the environment, a reinforcement learning method is used to control the mobile robot. This paper mainly introduces the application of reinforcement learning algorithms and data fusion to mobile robot navigation obstacle avoidance. We use the deep deterministic policy gradient(DDPG) algorithm in reinforcement learning to train neural networks. Gazebo simulation platform is adopted as the simulation environment of this paper, which can effectively simulate indoor and dynamic pedestrian environments. Considering that the depth value detected by a binocular camera may be affected by light in a real environment, we supposed the method of sensor fusion which is added to fuse the data of the laser sensor and vision sensor for 3D reconstruction and generate corresponding point cloud data. It is verified that this method performs better than the vision sensor as well as the laser sensor, which can improve the success rate of navigation by 10% in a dynamic environment. And this method can make the trajectory of the agent smoother and reach the target point faster.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10240555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240555","Reinforcement learning;deep deterministic policy gradient;sensor fusion","Three-dimensional displays;Simultaneous localization and mapping;Navigation;Heuristic algorithms;Reinforcement learning;Vision sensors;Sensor fusion","","","","","","18","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Intelligent Guidance Law for Air-to-Air Missiles","Y. Gu; W. Tang; S. Du","School of Automation, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Automation, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Automation, Northwestern Polytechnical University, Xi'an, Shaanxi, China","2023 4th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)","21 Aug 2023","2023","","","63","69","In the majority of research on guidance laws, missiles are commonly simplified as point masses. This approach may have adverse effects on the practical application of research outcomes. Additionally, conventional neural network-trained reinforcement learning (RL) guidance laws suffer from acceleration command oscillations (ACO) when the missile approaches the target, which can detrimentally affect the guidance performance. In order to tackle these issues, this study introduces a three-degree-of-freedom model that incorporates the missile body and controller in the training environment. Aiming to simulate the flight dynamics of missiles as closely to reality as possible. Furthermore, a gated recurrent unit layer is introduced in the neural network to allow the missile to leverage previous information and reduce chattering in output commands. Finally, the performance of the designed guidance law is validated through multiple repeated simulation experiments.","","979-8-3503-1452-6","10.1109/AINIT59027.2023.10212976","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10212976","missile guidance law;deep reinforcement learning;gated recurrent unit;missile three-degree-of-freedom model","Training;Seminars;Adaptive systems;Navigation;Neural networks;Reinforcement learning;Logic gates","control engineering computing;military computing;missile guidance;neurocontrollers;recurrent neural nets;reinforcement learning;trajectory control","acceleration command oscillations;air-to-air missiles;chattering reduction;controller;gated recurrent unit layer;missile body;missile flight dynamics;neural network-training;reinforcement learning-based intelligent guidance law;three-degree-of-freedom model","","","","8","IEEE","21 Aug 2023","","","IEEE","IEEE Conferences"
"A Novel Reinforcement Learning Control for a class of Strict-feedback Discrete-time Systems via Multi-Gradient Recursive","W. Bai; T. Li; Y. Long","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China","2021 International Conference on Security, Pattern Analysis, and Cybernetics（SPAC)","27 Sep 2021","2021","","","19","24","This paper investigates the reinforcement learning control design problem via multi-gradient recursive (MGR) for a general class of strict feedback systems. The system with unknown control gain is studied, which is more general than the one with known control gain. The long-term strategic utility function is estimated by constructing a critic neural network (NN), and an actuator NN is constructed to estimate the unknown function in the controller. A novel learning strategy, the socalled MGR, is proposed to learning the weight vector, which can not only eliminate the local optimal problem that inherent in the gradient descent method but also improve the rate of convergency. According to the Lyapunov theory, all signals in the control system are ensured to be semiglobal uniformly ultimately bounded (SGUUB). At last, the comparison simulation examples are given to validate this strategy.","","978-1-6654-4322-7","10.1109/SPAC53836.2021.9540001","National Natural Science Foundation of China(grant numbers:61903092,51939001,61976033,61773187,U1813203); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9540001","Reinforcement learning;multi-gradient recursive (MGR);strict-feedback system;neural networks (NN)","Discrete-time systems;Analytical models;Actuators;Control design;Reinforcement learning;Artificial neural networks;Security","adaptive control;closed loop systems;control system synthesis;discrete time systems;feedback;gradient methods;learning (artificial intelligence);learning systems;Lyapunov methods;neurocontrollers;nonlinear control systems;uncertain systems","strict-feedback discrete-time systems;multigradient recursive;reinforcement learning control design problem;strict feedback systems;unknown control gain;known control gain;long-term strategic utility function;critic neural network;actuator NN;novel learning strategy;socalled MGR;local optimal problem;gradient descent method;control system","","","","9","IEEE","27 Sep 2021","","","IEEE","IEEE Conferences"
"Miniaturized Hypoxic Generator Based on Reinforcement Learning for Continuous Altitude Simulation","Z. Liu; Y. Lu; Y. Liu","School of Automation, Beijing Institute of Technology, Beijing, P.R. China; School of Automation, Beijing Institute of Technology, Beijing, P.R. China; School of Automation, Beijing Institute of Technology, Beijing, P.R. China","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","6742","6747","Appropriate altitude pre-acclimatization training can effectively reduce the risk of acute altitude sickness among personnel in plateau areas. However, the size of the current mainstream plateau hypoxic generator system is too large, which is not conducive to widespread application in large areas. This paper proposes a deep reinforcement learning model that integrates an anti-time variation training method, which can effectively simplify the structural composition of the plateau hypoxic generator system and improve its miniaturization, and only need to install the sensor detection device at the output end of the system. Meanwhile, the algorithm proposed in this paper improves the average steady-state error index by 68.3% compared with the traditional deep reinforcement learning algorithm and PID classic control algorithm. Finally, we apply our proposed algorithm to design a practically functioning miniaturized hypoxia generation system.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10240848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240848","Deep Reinforcement Learning;Plateau Hypoxic Generator;Miniaturized Design","Training;Deep learning;Medical conditions;Stability criteria;Reinforcement learning;Generators;Steady-state","control system synthesis;deep learning (artificial intelligence);learning (artificial intelligence);reinforcement learning;three-term control","acute altitude sickness;anti-time variation training method;appropriate altitude pre-acclimatization training;average steady-state error index;continuous altitude simulation;current mainstream plateau hypoxic generator system;deep reinforcement learning model;miniaturization;miniaturized hypoxia generation system;miniaturized hypoxic generator;PID classic control algorithm;plateau areas;sensor detection device;structural composition;traditional deep reinforcement learning algorithm;widespread application","","","","16","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Reinforcement learning adaptive control for upper limb rehabilitation robot based on fuzzy neural network","M. Fan-Cheng; D. Ya-Ping","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China","Proceedings of the 31st Chinese Control Conference","24 Dec 2012","2012","","","5157","5161","Aiming to how to coordinate and control the patient's upper limb to trace the set train motion trajectory and position which are purposed base on the statues of the sick upper limb, the paper purposed a novel reinforcement leaning controller. In the continuous-time RL scheme, a fuzzy actor is employed to approximate the plant(which includes rehabilitation robot and the sick upper-limb), and a critic NN is designed to evaluate the performance of the actor At the same time, the critic NN generates some rewards back to the fuzzy actor for tuning weight of rules. The weight tuning law is given based on Lyapunov stability analysis. The purposed RL was finally simulated and analyzed, experiment and simulation results showed that the control strategy not only effectively provided the robot's tracking requirements, but also had strong robustness and flexibility.","2161-2927","978-988-15638-1-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6390836","Reinforcement Learning;Rehabilitation Robot;Fuzzy Neural Network;Adaptive Control","Electronic mail;Learning;Robot kinematics;Control systems;Adaptive control;Fuzzy neural networks","adaptive control;fuzzy neural nets;learning (artificial intelligence);Lyapunov methods;medical robotics;motion control;patient rehabilitation;robust control","reinforcement learning adaptive control;upper limb rehabilitation robot;fuzzy neural network;patient upper limb;robustness;robot tracking requirements;Lyapunov stability analysis;weight tuning law;critic NN;fuzzy actor;continuous-time RL scheme;sick upper limb;set train motion position tracing;set train motion trajectory tracing","","","","11","","24 Dec 2012","","","IEEE","IEEE Conferences"
"Reinforcement Learning Control Based on TWO-CMAC Structure","L. Xin; C. Wei; C. Mei","Department of Automation, Hefei University of Technology, Hefei, China; Department of Automation, Hefei University of Technology, Hefei, China; Department of Automation, Hefei University of Technology, Hefei, China","2009 International Conference on Intelligent Human-Machine Systems and Cybernetics","17 Nov 2009","2009","1","","116","121","To improve the effect of the on-line control, a reinforcement learning algorithm presents which is based on CMAC (Cerebella Model Articulation Controller, CMAC) in this article. The algorithm includes two parts, the related search unit and the self-adaptive comment unit. The related search unit CMAC adopts the learning algorithm with instructors, that is, the P gain of fixed-line regulator provides online learning samples data, through which CMAC could take part in controlling step by step; while the self-adaptive comment unit CMAC judges the controlling error, it also produces enhanced or punishment signal. Such an algorithm is implied in controlling the speed of small wheeled robots, and analysis the control performance of the single neuron adaptive PID in the parameters system changes by contrast, Matlab simulation datum shows the rapidly and validity of the reinforcement learning control.","","978-0-7695-3752-8","10.1109/IHMSC.2009.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5335931","Reinforcement Learning;CMAC;Robot","Learning;Mathematical model;Regulators;Error correction;Mobile robots;Control system analysis;Performance analysis;Algorithm design and analysis;Neurons;Programmable control","adaptive control;cerebellar model arithmetic computers;learning (artificial intelligence);mobile robots;three-term control","reinforcement learning control;two-CMAC structure;on-line control;CMAC;cerebella model articulation controller;self-adaptive comment unit;fixed-line regulator;online learning samples data;single neuron adaptive PID;wheeled robots","","","","10","IEEE","17 Nov 2009","","","IEEE","IEEE Conferences"
"Parameter Optimization via Reinforcement Learning for the Regulation of Swarms","Q. Wu; G. Liu; K. Liu; L. Chen","School of Automation Science and Electrical Engineering Beihang University, Beijing, China; School of Automation Science and Electrical Engineering Beihang University, Beijing, China; The State Key Laboratory of Software Development, Environment School of Automation Science and Electrical Engineering Beihang University, Beijing, China; Advanced Research Institute of Multidisciplinary Sciences Beijing Institute of Technology, Beijing, China","2023 9th International Conference on Information, Cybernetics, and Computational Social Systems (ICCSS)","9 Oct 2023","2023","","","62","67","The bird-oid object (Boids) model proposes a control algorithm to make the positions between agents achieve cooperative stability. By changing the parameters of cohesion and repulsion in the algorithm, the agents in the swarm can be made to converge to different positions, causing expansion and contraction of the formation. But it is often more difficult to select the appropriate parameters to form the ideal formation. Therefore, this paper proposes a method to improve the cohesive and repulsive parameters in the Boids model based on Q-learning network to achieve a simulation scenario with continuous obstacle avoidance and maximum coverage of space.","2639-4235","979-8-3503-4223-9","10.1109/ICCSS58421.2023.10270800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10270800","q-learning;boids model;obstacle avoidance;maximum coverage","Training;Q-learning;Computational modeling;Stability analysis;Regulation;Collision avoidance;Thermal stability","","","","","","22","IEEE","9 Oct 2023","","","IEEE","IEEE Conferences"
"A TD3 Algorithm Based Reinforcement Learning Controller for DC-DC Switching Converters","J. Ye; H. Guo; S. Mei; Y. Hu; X. Zhang","School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, Shenzhen, China; School of Electrical, Electronic and Computer Engineering, University of Western Australia, Perth, Australia; School of Electrical, Electronic and Computer Engineering, University of Western Australia, Perth, Australia","2023 International Conference on Power Energy Systems and Applications (ICoPESA)","5 Jun 2023","2023","","","358","363","Various linear and nonlinear controllers have been developed to improve the dynamic performance of DC-DC converters. Most controllers can only be designed on the basis of understanding the mathematical model of DC-DC converter, but the inherent nonlinear and time-varying characteristics of DC-DC switching converter make it difficult to complete the precise modeling, so the model-based control design is complex and the control performance is limited. In order to overcome the problem, this paper proposes a reinforcement learning (RL) controller based on the twin-delayed deep deterministic policy gradient (TD3) algorithm. This controller does not need the model of the switching converter. The converter will be regarded as a black box model, the policy approximation function (policy neural network) can be trained and learned by constructing a Markov decision process interacting with the black box model in the control system, and the optimal control action can be output. The RL controller is developed based on actor critic architecture, and a TD3 algorithm with higher learning efficiency is proposed to improve the control performance of the RL controller. The proposed RL controller based on TD3 algorithm is compared with the traditional PI controller. The simulation results show that the RL controller has better dynamic performance when the converter starts and the load step changes.","","979-8-3503-4560-5","10.1109/ICoPESA56898.2023.10141314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10141314","reinforcement learning;twin-delayed deep deterministic policy gradient algorithm;neural network;model-free control;DC-DC converter","Training;Switching converters;Heuristic algorithms;Process control;DC-DC power converters;Reinforcement learning;Control systems","control engineering computing;control system synthesis;DC-DC power convertors;gradient methods;Markov processes;nonlinear control systems;optimal control;PI control;power engineering computing;power system control;reinforcement learning","black box model;control system;DC-DC switching converter;Markov decision process;mathematical model;model-based control design;nonlinear controllers;optimal control action;PI controller;policy approximation function;reinforcement learning controller;RL controller;TD3 algorithm;time-varying characteristics;twin-delayed deep deterministic policy gradient algorithm","","","","11","IEEE","5 Jun 2023","","","IEEE","IEEE Conferences"
"UAV Landing Control Disturbed by Carrier Air-Wake Based on Deep Reinforcement Learning","G. Xu; L. Liu; H. Liu","School of Automation, Shenyang Aerospace University, Shenyang, China; School of Automation, Shenyang Aerospace University, Shenyang, China; School of Automation, Shenyang Aerospace University, Shenyang, China","2022 34th Chinese Control and Decision Conference (CCDC)","14 Feb 2023","2022","","","4719","4724","A method based on deep reinforcement learning (DRL) is proposed to solve the autonomous landing control problem of unmanned aerial vehicle (UAV) under the interference of carrier air-wake. Firstly, a simulation model of UAV landing on a carrier is built, and a reference trajectory to ensure its safe landing is designed. Secondly, a UAV autonomous landing control method based on Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm is proposed, and the reward function is designed that takes into account tracking error and trajectory stability. Finally, through offline training, an autonomous landing controller is obtained. The simulation results show that the controller designed in this paper can not only accurately control the landing trajectory of UAVs, but also cope with complex effects of nonlinear air-wake in nonlinear dynamic inverse (NDI) control, and greatly improve the safety of landing.","1948-9447","978-1-6654-7896-0","10.1109/CCDC55256.2022.10033898","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10033898","Deep Reinforcement Learning(DRL);Autonomous Landing Controller;UAV;Carrier Air-Wake","Deep learning;Training;Analytical models;Atmospheric modeling;Simulation;Reinforcement learning;Autonomous aerial vehicles","aircraft landing guidance;autonomous aerial vehicles;control engineering computing;control system synthesis;deep learning (artificial intelligence);gradient methods;mobile robots;reinforcement learning;remotely operated vehicles;trajectory control","account tracking error;autonomous landing control problem;autonomous landing controller;carrier air-wake;deep reinforcement learning;nonlinear air-wake;nonlinear dynamic inverse control;reference trajectory;safe landing;simulation model;trajectory stability;Twin Delayed Deep Deterministic Policy Gradient algorithm;UAV autonomous landing control method;UAV landing control disturbed;unmanned aerial vehicle","","","","12","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Multi-USV Deep Reinforcement Learning for Distributed Cooperative Target Tracking","C. -C. Wang; Y. -L. Wang; C. Peng","School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China","2022 IEEE International Conference on Unmanned Systems (ICUS)","29 Dec 2022","2022","","","242","247","The purpose of this paper is to discuss distributed cooperative target tracking for a multi-unmanned surface vehicle (multi-USV) system. The cooperative target tracking problem is formulated as a multi-USV learning problem. Based on this formulation, a multi-USV distributed cooperative target tracking (MUTT) algorithm is proposed. To avoid the collisions between USVs during the tracking process, an additional safety layer is introduced. Some safety signals are constructed based on USVs' states. By correcting actions through the trained safety layer, USVs can avoid collisions reasonably. Moreover, for the sake of demonstrating the effectiveness of the proposed MUTT algorithm in target tracking, reward functions and mission scenarios are well constructed. Furthermore, a comparison of the MUTT algorithm and Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm is given. The obtained results manifest that the proposed MUTT algorithm provides safe policies for multi-USV cooperative target tracking tasks.","2771-7372","978-1-6654-8456-5","10.1109/ICUS55513.2022.9986900","National Science Foundation of China(grant numbers:61873335,61833011); Science and Technology Commission of Shanghai Municipality(grant numbers:20ZR1420200,21SQBS01600,22JC1401400,19510750300,21190780300); 111 Project(grant numbers:D18003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9986900","Multi-USV systems;deep reinforcement learning;cooperative target tracking","Deep learning;Target tracking;Smoothing methods;Heuristic algorithms;Reinforcement learning;Safety;Task analysis","Kalman filters;learning (artificial intelligence);multi-agent systems;remotely operated vehicles;target tracking","MultiAgent Deep Deterministic Policy Gradient algorithm;multiunmanned surface vehicle;MultiUSV deep reinforcement learning;multiUSV distributed;MUTT algorithm;target tracking problem;tracking process;USVs' states","","","","14","IEEE","29 Dec 2022","","","IEEE","IEEE Conferences"
"UUV Target Tracking Path Planning Algorithm Based on Deep Reinforcement Learning","Y. Yue; W. Hao; H. Guanjie; Y. Yao","Naval Research Institute, Beijing, China; Jiangsu Automation research Institute, Lianyungang, China; Jiangsu Automation research Institute, Lianyungang, China; Jiangsu Automation research Institute, Lianyungang, China","2023 8th Asia-Pacific Conference on Intelligent Robot Systems (ACIRS)","7 Sep 2023","2023","","","65","71","Path planning is one of the basic key problems in UUV task planning research. This paper studies the UUV path planning method in target tracking task scenario. The target is in a moving state, the moving elements are uncertain, and the traditional path planning algorithm is not applicable or easy to fall into the local optimal solution. In this paper, a tracing path planning algorithm based on deep reinforcement learning is presented, and a network parameter update method combining soft update with optimal sample training is proposed in the target network update link. The simulation results show that the algorithm can accelerate the network convergence speed while guaranteeing the stability of the learning process, and can quickly plan the optimal trajectory and maximize the time to track the target after UUV finds the target.","","979-8-3503-0285-1","10.1109/ACIRS58671.2023.10240259","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240259","UUV;path planning;deep reinforcement learning","Training;Deep learning;Target tracking;Simulation;Reinforcement learning;Path planning;Stability analysis","autonomous underwater vehicles;learning (artificial intelligence);mobile robots;optimisation;path planning;reinforcement learning;target tracking","basic key problems;deep reinforcement learning;local optimal solution;moving elements;moving state;network parameter update method;target network update link;target tracking task scenario;tracing path planning algorithm;traditional path planning algorithm;UUV path planning method;UUV target tracking path planning algorithm;UUV task planning research","","","","12","IEEE","7 Sep 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Adaptive Intelligent Control for State-Constrained Nonlinear Multi-agent Systems","J. Liu; L. Yan; Y. Peng","School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China","2023 9th International Conference on Control Science and Systems Engineering (ICCSSE)","20 Sep 2023","2023","","","44","49","This article investigates the adaptive optimal consensus tracking problem of state-constrained nonlinear multiagent systems based on the reinforcement learning algorithm. Existing consensus control works with state constraints and pays little attention to achieving optimal performance. Firstly, a state-dependent mapping function is proposed, with which the original constrained dynamic system is equivalently mapped into a free-constrained one. Then, the reinforcement learning algorithm is integrated into the traditional backstepping design procedures to achieve optimal consensus. With the developed identifier-critic-actor framework, the optimal control inputs are generated using RBFNN to implement the proposed algorithm. Updating laws of the identifier, critic, and actor networks are driven by the HJB residual errors. Then, it is proved that consensus tracking is achieved and constraints are not violated. An illustrative example validates the effectiveness of the proposed approach.","2832-1871","979-8-3503-3905-5","10.1109/ICCSSE59359.2023.10245355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10245355","Reinforcement learning;adaptive optimal consensus;state constraints;multi-agent systems","Backstepping;Heuristic algorithms;Optimal control;Reinforcement learning;Consensus control;Dynamical systems;Task analysis","adaptive control;control nonlinearities;control system synthesis;intelligent control;multi-agent systems;nonlinear control systems;optimal control;radial basis function networks;reinforcement learning","adaptive intelligent control;adaptive optimal consensus tracking problem;consensus control;developed identifier-critic-actor framework;optimal control inputs;optimal performance;original constrained dynamic system;reinforcement learning algorithm;state constraints;state-constrained nonlinear multiagent systems;state-dependent mapping function;traditional backstepping design procedures","","","","21","IEEE","20 Sep 2023","","","IEEE","IEEE Conferences"
"The State-space Design Research of MPPT based on Reinforcement Learning in PV System","D. Lin; X. Li; S. Ding","School of Electrical and Automation Engineering, Nanjing Normal University, China; School of Electrical and Automation Engineering, Nanjing Normal University, China; School of Electrical and Automation Engineering, Nanjing Normal University, China","2021 IEEE 1st International Power Electronics and Application Symposium (PEAS)","6 Dec 2021","2021","","","1","6","Photovoltaic power (PV) generation is considered as a renewable energy technology. Due to the nonlinear behav-ior of photovoltaic modules, Maximum power point tracking (MPPT) technology is essential for high-efficiency photovoltaic systems. Although traditional MPPT technologies are simple to implement, they should use trial and error to tune their control parameters, which is the result of unsatisfactory performance in a changing environment. Different from the traditional MPPT technology, the MPPT based on reinforcement learning (RL-MPPT) method has self-learning ability and better applicability in the changing environment. However, the state space size of the method greatly affects the tracking ability. Therefore, this paper compares the tracking performance of the three state spaces through the results of simulation and experiment under En50530 test procedure.","","978-1-6654-1360-2","10.1109/PEAS53589.2021.9628732","National Natural Science Foundation of China; Nanjing Normal University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9628732","Photovoltaic power;MPPT;RL;State space design","Maximum power point trackers;Photovoltaic systems;Renewable energy sources;Reinforcement learning;Aerospace electronics;Power electronics","learning (artificial intelligence);maximum power point trackers;photovoltaic power systems;power generation control","traditional MPPT technology;reinforcement learning method;RL-MPPT;changing environment;state space size;tracking ability;tracking performance;state spaces;state-space design research;PV system;photovoltaic power generation;renewable energy technology;nonlinear behav-ior;photovoltaic modules;Maximum power point tracking technology;high-efficiency photovoltaic systems","","","","7","IEEE","6 Dec 2021","","","IEEE","IEEE Conferences"
"Comparison and Implementation of High Cited Inverse Reinforcement Learning Algorithms in Object World","X. Wang; W. Zhang; J. Chen","College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China; College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China; College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China","2017 9th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)","21 Sep 2017","2017","2","","374","377","Inverse reinforcement learning (inverse RL), an inverse process of reinforcement learning (RL), is mainly used to solve problems in extracting a reward function from observed (nearly) optimal behavior of an expert acting in an environment. The extract of a reward function is one of the biggest obstacles of the deployment of reinforcement learning agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual tuning. In this paper, we analyze the principles of three high cited inverse RL algorithm, Maximum Margin Planning (MMP), MMPBoost, and Maximum Entropy (MaxEnt), and implement them in the object world. Experiments show that MMP is not sensitive with environment changes and the effect of boosting is remarkable when comparing the results of MMP and MMPBoost. MaxEnt as an energy-based method is of enormous potential.","","978-1-5386-3022-8","10.1109/IHMSC.2017.195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8048179","inverse reinforcement learning;MMPBOOST;MAXENT;MMP","Learning (artificial intelligence);Entropy;Algorithm design and analysis;Color;Boosting;Feature extraction","inverse problems;learning (artificial intelligence);maximum entropy methods;optimisation;planning (artificial intelligence)","high cited inverse reinforcement learning algorithm;object world;reward function extraction;reinforcement learning agents;high cited inverse RL algorithm;maximum margin planning;MMPBoost;maximum entropy;energy-based method","","","","11","IEEE","21 Sep 2017","","","IEEE","IEEE Conferences"
"A 2D UAV Path Planning Method Based on Reinforcement Learning in the Presence of Dense Obstacles and Kinematic Constraints","X. Tang; Y. Chai; Q. Liu","School of Automation, Chongqing University, Chongqing, P. R. China; School of Automation, Chongqing University, Chongqing, P. R. China; School of Automation, Chongqing University, Chongqing, P. R. China","2022 IEEE 11th Data Driven Control and Learning Systems Conference (DDCLS)","26 Aug 2022","2022","","","306","311","The complex kinematic constraints and dense obstacles are always the huge challenge in the UAV path planning. To effectively deal with dense obstacles and kinematic constraints, a novel two-level optimization algorithm for unmanned aerial vehicles (UAVs) in 2D maps, called as Spherical Expansion-Proximal Policy Optimization (SE-PPO), is proposed in this paper. This method is a combination of SE and PPO algorithms. In the first level, SE algorithm is used to generate the initial path, and sub-goals are selected from this path in the first level. These sub-goals are optimized by the local path optimizer based on PPO algorithm to obtain the final path. The effectiveness of this method to deal with the kinematic constraints and dense obstacles is demonstrated by the results of the simulation experiments.","2767-9861","978-1-6654-9675-9","10.1109/DDCLS55054.2022.9858514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858514","hierarchical path planning;spherical expansion;reinforcement learning;proximal policy optimization;unmanned aerial vehicle;kinematic constraints","Training;Learning systems;Simulation;Kinematics;Reinforcement learning;Autonomous aerial vehicles;Control systems","autonomous aerial vehicles;collision avoidance;mobile robots;optimisation;reinforcement learning","dense obstacles;complex kinematic constraints;two-level optimization algorithm;SE-PPO;SE algorithm;local path optimizer;2D UAV path planning;reinforcement learning;unmanned aerial vehicles;spherical expansion-proximal policy optimization","","","","11","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"Hierarchical Multi-Agent Reinforcement Learning with Intrinsic Reward Rectification","Z. Liu; Z. Xu; G. Fan","Fusion Innovation Center, Institute of Automation, Chinese Academy of Sciences; Fusion Innovation Center, Institute of Automation, Chinese Academy of Sciences; Fusion Innovation Center, Institute of Automation, Chinese Academy of Sciences","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Hierarchical reinforcement learning (HRL) is a promising approach to solving long-term decision problems and complex tasks, as high-level policy can guide the training procedure of low-level policy with macro actions and intrinsic rewards. However, the amount that macro actions influence decision-making, which affects how much internal rewards should be given to low-level policy, is disregarded by current HRL algorithms. It may be reasonable to provide low-level policy with less intrinsic rewards if macro actions are less important in decision-making. In this paper, we propose a value decomposition based hierarchical multi-agent reinforcement learning method with intrinsic reward rectification, which can determine the effectiveness of macro actions and correct the intrinsic rewards. We show that our proposed method significantly outperforms the state-of-the-art value decomposition approaches on the StarCraft Multi-Agent Challenge platform.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10095374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095374","Hierarchical reinforcement learning;multi-agent reinforcement learning;intrinsic reward rectification","Training;Decision making;Signal processing algorithms;Reinforcement learning;Signal processing;Acoustics;Task analysis","","","","","","20","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Guided Deep Reinforcement Learning based on RBF-ARX Pseudo LQR in Single Stage Inverted Pendulum","T. Peng; H. Peng; F. Liu","School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China","2022 International Conference on Intelligent Systems and Computational Intelligence (ICISCI)","11 Nov 2022","2022","","","62","67","The Guided Deep Reinforcement Learning (GDRL) method is proposed to train an optimal controller to stabilize a Single Stage Inverted Pendulum (SSIP). Firstly, the RBF (Radial Basis Function)-ARX (Auto Regressive Exogenous) without offset term model is applied to model a SSIP offline, using the data sampled from a simulated SSIP. Then, a Pseudo LQR (PLQR) is designed to provide guiding signals to the agent during the training process. Together with the policy gradient in deep reinforcement learning, the policy network is trained by the softly-combined gradient from both the action-value network and the model based PLQR. To improve the policy monotonically, a searching scheme with adjusted step size is applied during the training. All of these above consist of the proposed method. The simulation results demonstrate the superiority of training speed and controlling performance of the GDRL.","","978-1-6654-5619-7","10.1109/ICISCI53188.2022.9941450","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9941450","RBF-ARX model;Pseudo LQR;Guided deep reinforcement learning;Adaptive gradient","Training;Simulation;Computational modeling;Process control;Reinforcement learning;Search problems;Data models","autoregressive processes;gradient methods;learning (artificial intelligence);linear quadratic control;nonlinear control systems;optimal control;pendulums;radial basis function networks","deep reinforcement learning;policy network;PLQR;training speed;controlling performance;Guided Deep Reinforcement;RBF-ARX PseudoLQR;Pendulum;optimal controller;Radial Basis Function;offset term model;SSIP offline;simulated SSIP;training process;policy gradient","","","","13","IEEE","11 Nov 2022","","","IEEE","IEEE Conferences"
"Acquisition of Automated Guided Vehicle Route Planning Policy Using Deep Reinforcement Learning","R. Kamoshida; Y. Kazama","Hitachi, Ltd. Research & Development Group Center for Technology Innovation - Systems Engineering Kokubunji-shi, Tokyo, Japan; Hitachi, Ltd. Research & Development Group Center for Technology Innovation - Systems Engineering Kokubunji-shi, Tokyo, Japan","2017 6th IEEE International Conference on Advanced Logistics and Transport (ICALT)","29 Nov 2018","2017","","","1","6","Automated guided vehicle (AGV) systems have been widely used in warehouses to improve productivity and reduce costs. For almost every warehouse, order picking is the most costly activity. In an order picking activity, the picker's travel time is the dominant component. To eliminate the travel time, we have developed a picking system in which AGVs transport the entire shelves including the required items to the pickers instead of the pickers moving to the shelves, which improves the efficiency of the picking activities. To minimize the shelf waiting time for the pickers, an intelligent AGV control method such as route planning is required. While there are already some existing approaches using reinforcement learning for this, reinforcement learning often requires hand-engineered low-dimensional state representation, which results in the loss of some state information. In this paper, we present an AGV route planning method for an AGV picking system using deep reinforcement learning. This method uses raw high-dimensional map information as input instead of hand-engineered low-dimensional state representation and it enables the acquisition of a successful AGV route planning policy. We evaluated the validity of the proposed method using an AGV picking system simulator and found that the proposed method outperforms other route planning strategies including our previous method.","","978-1-5386-1623-9","10.1109/ICAdLT.2017.8547000","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8547000","Automated guided vehicle;order picking;warehouse;reinforcement learning;deep learning","Planning;Productivity;Mathematical model;Logistics;Conferences;Large-scale systems","automatic guided vehicles;learning (artificial intelligence);order picking;path planning;vehicle routing;warehouse automation","map information;state representation;AGV route planning policy;warehouses;order picking;intelligent AGV control method;shelf waiting time;automated guided vehicle route planning policy;AGV picking system simulator;deep reinforcement learning","","15","","16","IEEE","29 Nov 2018","","","IEEE","IEEE Conferences"
"A comparison of reinforcement learning based approaches to appliance scheduling","N. Chauhan; N. Choudhary; K. George","PES Centre for Intelligent Systems, PES University Campus, Bangalore, India; PES Centre for Intelligent Systems, PES University Campus, Bangalore, India; Dept. of Telecommunication Engineering PES, Institute of Technology (now PES University), Bangalore, India","2016 2nd International Conference on Contemporary Computing and Informatics (IC3I)","4 May 2017","2016","","","253","258","Reinforcement learning is often proposed as a technique for intelligent control in a smart home setup with dynamic real-time energy pricing and advanced sub-metering infrastructure. In this paper, we introduce a variation of State Action Reward State Action (SARSA) as an optimization algorithm for appliance scheduling in smart homes with multiple appliances and compare it with the popular reinforcement learning method Q-learning. A simple, intuitive and unique treelike Markov decision process (MDP) structure of appliances is proposed which takes into account the states, such as on/off/runtime status, of all schedulable appliances but does not require the knowledge of the state to state transition probabilities.","","978-1-5090-5256-1","10.1109/IC3I.2016.7917970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917970","Reinforcement Learning;SARSA;Q-learning;Smart Grid;Demand Side Management;Demand Response","Home appliances;Pricing;Learning (artificial intelligence);Delays;Real-time systems;Markov processes;Optimization","decision theory;demand side management;domestic appliances;home automation;intelligent control;learning (artificial intelligence);Markov processes;optimisation;scheduling","reinforcement learning based approach;appliance scheduling;reinforcement learning;intelligent control;smart home setup;dynamic real-time energy pricing;advanced submetering infrastructure;State Action Reward State Action;SARSA;Q-learning reinforcement learning method;tree-like Markov decision process structure;tree-like MDP structure;state transition probabilities","","11","","23","IEEE","4 May 2017","","","IEEE","IEEE Conferences"
"Applying affective feedback to reinforcement learning in ZOEI, a comic humanoid robot","I. D. Addo; S. I. Ahamed","Marquette University, Milwaukee, WI, USA; Marquette University, Milwaukee, WI, USA","The 23rd IEEE International Symposium on Robot and Human Interactive Communication","20 Oct 2014","2014","","","423","428","As robotic technologies of varying shapes and forms continue to make their way into our everyday lives, the significance of a humanoid robot's ability to make a human interaction feel natural, engaging and entertaining becomes an area of keen interest in sociable robotics. In this paper, we present our findings on how affective feedback can be used to drive reinforcement learning in human-robot interactions (HRI) and other dialogue systems. We implemented a system where a humanoid robot, named ZOEI, acts as a standup comedian by entertaining a human audience in a bid to generate humor and positively influence the emotional state of the humans. The mood rating of the audience is recorded prior to the interaction session. Using a survey, the eventual emotional state of the human participant is captured after the HRI session. For each audience member, we capture feedback regarding how funny each joke was. We present the implementation of the content selection framework. We share our findings to substantiate the idea that by using expressive behaviors of the humanoid to influence the delivery of content (in this case, jokes) as well as employing reinforcement learning techniques for driving targeted content selection, the robot was able to improve the human mood score progressively across the 16 people who engaged in the study.","1944-9437","978-1-4799-6765-0","10.1109/ROMAN.2014.6926289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926289","HRI;Affective Content;Affective Computing;Humanoids;Affect Generation;Ubiquitous Computing;Collective Intelligence;Internet of Things","Humanoid robots;Learning (artificial intelligence);Senior citizens;Robot sensing systems;Emotion recognition","feedback;humanoid robots;human-robot interaction;learning (artificial intelligence);psychology;social aspects of automation","ZOEI;comic humanoid robot;affective feedback;reinforcement learning;human-robot interactions;dialogue systems;standup comedian;mood rating;human participant emotional state;content selection framework","","8","1","22","IEEE","20 Oct 2014","","","IEEE","IEEE Conferences"
"Autonomous Navigation of an AMR using Deep Reinforcement Learning in a Warehouse Environment","A. Balachandran; A. Lal S; P. Sreedharan","Department of Mechanical Engineering, Amrita Vishwa Vidyapeetham, Amritapuri; Department of Mechanical Engineering, Amrita Vishwa Vidyapeetham, Amritapuri; Department of Mechanical Engineering, Amrita Vishwa Vidyapeetham, Amritapuri","2022 IEEE 2nd Mysore Sub Section International Conference (MysuruCon)","13 Dec 2022","2022","","","1","5","Mobile robots have been used in warehouses worldwide as a means for distribution of goods and gained demand after the Covid19 labor issue. This paper proposes an Autonomous Mobile Robot (AMR) to navigate in a warehouse environment to its target location using LIDAR. The method used to solve this problem is a deep reinforcement learning algorithm called deep Q-network (DQN) to detect and avoid obstacles and reach the target location. DQN is used as it is desired for solving complex tasks. Training of the DQN algorithm is carried out in ROS Gazebo environment using LIDAR-based robot model. The LIDAR sensor detects the obstacles and the odometer sensor helps to find the distance between the target location are used as inputs for training the algorithm and optimal actions are taken based on the two inputs. A reward policy is awarded when an obstacle is avoided and reaches the target location. The results show that mobile robot can successfully navigate in an unknown environment through simulation and real life.","","978-1-6654-9790-9","10.1109/MysuruCon55714.2022.9971804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9971804","Warehouse;navigation;AM","Training;Deep learning;COVID-19;Laser radar;Navigation;Robot vision systems;Reinforcement learning","collision avoidance;control engineering computing;deep learning (artificial intelligence);distance measurement;mobile robots;optical radar;reinforcement learning;warehouse automation","AMR;Autonomous Mobile Robot;autonomous navigation;Covid19 labor issue;deep Q-network;deep reinforcement learning algorithm;DQN algorithm;LIDAR sensor;LIDAR-based robot model;mobile robots;odometer sensor;reward policy;ROS Gazebo environment;target location;unknown environment;warehouse environment","","4","","17","IEEE","13 Dec 2022","","","IEEE","IEEE Conferences"
"Multi-agent reinforcement learning for intelligent resource allocation in IIoT networks","J. Rosenberger; M. Urlaub; D. Schramm","Automation and Electrification Solutions, Bosch Rexroth AG, Lohr am Main, Germany; Automation and Electrification Solutions, Bosch Rexroth AG, Lohr am Main, Germany; Chair of Mechatronics, University of Duisburg-Essen, Duisburg, Germany","2021 IEEE Global Conference on Artificial Intelligence and Internet of Things (GCAIoT)","31 Jan 2022","2021","","","118","119","In the industrial Internet of Things (IIoT), a high number of devices with limited resources, like computational power, memory, bandwidth and, in case of wireless sensor networks, also energy, communicate. At the same time, the amount of data as well as the demand for data processing in the edge is rapidly increasing. To enable Industry 4.0 (I4.0) and the IIoT, an intelligent resource allocation is required to make optimal use of the available resources. For this purpose, a multi-agent system (MAS) based on deep reinforcement learning (DRL) is proposed. Multi-agent reinforcement learning (MARL) is already taken into account in different communication networks, e.g. for intelligent routing. Despite its great potential, little attention is paid to these methods in industry so far. In this work, DRL is applied for resource allocation and load balancing for industrial edge computing. An optimal usage of the available resources of the IIoT devices should be achieved. Due to the structure of IIoT systems as well as for security reasons, a MAS is preferred for decentralized decision making. In subsequent steps, it is planned to add and remove devices during runtime, to change the number of tasks to be executed as well as evaluations on single- and multi-policy-approaches. The following aspects will be considered for evaluation: (1) improvement of the resource usage of the devices and (2) overhead due to the MAS.","","978-1-6654-3841-4","10.1109/GCAIoT53516.2021.9692913","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9692913","multi-agent-system;deep reinforcement learning;resource allocation;load balancing;industrial internet of things;streaming data","Wireless sensor networks;Runtime;Reinforcement learning;Load management;Routing;Resource management;Security","decision making;deep learning (artificial intelligence);Internet of Things;multi-agent systems;reinforcement learning;resource allocation","deep reinforcement learning;multiagent system;data processing;wireless sensor networks;computational power;IIoT networks;intelligent resource allocation;resource usage;MAS;IIoT systems;IIoT devices;industrial edge computing;load balancing;DRL;intelligent routing;multiagent reinforcement learning","","3","","14","IEEE","31 Jan 2022","","","IEEE","IEEE Conferences"
"Storehouse: a Reinforcement Learning Environment for Optimizing Warehouse Management","J. Cestero; M. Quartulli; A. M. Metelli; M. Restelli","Data Intelligence for Energy and Industrial Processes, Vicomtech, San Sebastian, Spain; Data Intelligence for Energy and Industrial Processes, Vicomtech, San Sebastian, Spain; Dipartimento di Electronica. Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy; Dipartimento di Electronica. Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","1","9","Warehouse Management Systems have been evolving and improving thanks to new Data Intelligence techniques. However, many current optimizations have been applied to specific cases or are in great need of manual interaction. Here is where Reinforcement Learning techniques come into play, providing automatization and adaptability to current optimization policies. In this paper, we present Storehouse, a customizable environment that generalizes the definition of warehouse simulations for Reinforcement Learning. We also validate this environment against state-of-the-art reinforcement learning algorithms and compare these results to human and random policies.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9891985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9891985","","Training;Industries;Adaptation models;Neural networks;Layout;Reinforcement learning;Manuals","optimisation;production engineering computing;reinforcement learning;warehouse automation","warehouse management systems;optimization policies;storehouse;customizable environment;state-of-the-art reinforcement learning algorithms;automatization process;data intelligence techniques","","1","","27","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Flexible Load Scheduling of Intelligent Buildings Based on Deep Reinforcement Learning","Y. Zhu; Z. Cen; Z. Ren","Kaili Power Supply Bureau of Guizhou Power Grid Co. Ltd, Kaili, China; Kaili Power Supply Bureau of Guizhou Power Grid Co. Ltd, Kaili, China; Kaili Power Supply Bureau of Guizhou Power Grid Co. Ltd, Kaili, China","2023 IEEE International Conference on Image Processing and Computer Applications (ICIPCA)","27 Sep 2023","2023","","","1840","1844","Intelligent buildings include scattered and large amount of flexible load, facing a large number of air conditioning equipment, the model mechanism is complex, and the traditional optimizer takes a long time to solve. In order to solve the above problems, this paper proposes the deep reinforcement learning method to optimize the running state of load. Firstly, the framework of the building load control system in the current popular stage is proposed, and the optimization process is introduced. Secondly, a reinforcement learning model applicable to the building is built, and the equipment status and real-time electricity price information are obtained in real time. The efficient processing capacity of the reinforcement learning algorithm is used to optimize the equipment status in the next period. Finally, a building is simulated and compared with the traditional optimizer to verify the effectiveness of the proposed method.","","979-8-3503-1467-0","10.1109/ICIPCA59209.2023.10257958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10257958","Intelligent buildings;deep reinforcement learning;realtime electricity price","Deep learning;Temperature distribution;Buildings;Water heating;Reinforcement learning;Prediction algorithms;Scheduling","air conditioning;building management systems;deep learning (artificial intelligence);home automation;load management;optimisation;power engineering computing;pricing;reinforcement learning","air conditioning equipment;building load control system;current popular stage;deep reinforcement learning method;efficient processing capacity;equipment status;flexible load scheduling;intelligent buildings;optimization process;real-time electricity price information;reinforcement learning algorithm;reinforcement learning model","","","","15","IEEE","27 Sep 2023","","","IEEE","IEEE Conferences"
"A study for Motion-Planning Method Resident-tracking Robot based on Reinforcement Learning","M. Sugimoto; T. Yoshioka; K. Ishii; S. Nonaka; M. Deguchi; S. Tsuzuki; M. Hiran","Division of Electrical and Electronic Engineering and Computer Science, Electrical and Electronic Engineering Course, Ehime University, 3 Bunkyo-cho, Matsuyama City, Ehime Pref., Japan; Department of Electrical and Computer Engineering, National Institute of Technology, Kagawa College, 355 Chokushi-cho, Takamatsu City, Kagawa Pref., Japan; Department of Electro-Mechanical Systems Engineering, National Institute of Technology, Kagawa College., 355 Chokushi-cho, Takamatsu City, Kagawa Pref., Japan; Department of Integrated Science and Technology, National Institute of Technology, Tsuyama College, 624–1 Numa, Tsuyama City, Okayama Pref., Japan; Department of Electronics and Control Engineering, National Institute of Technology, Niihama College, 7–1 Yakumo-cho, Niihama City, Ehime Pref., Japan; Division of Electrical and Electronic Engineering and Computer Science, Electrical and Electronic Engineering Course, Ehime University, 3 Bunkyo-cho, Matsuyama City, Ehime Pref., Japan; Department of Electrical Engineering and Information Science, National Institute of Technology, Niihama College, 7–1 Yakumo-cho, Niihama City, Ehime Pref., Japan","2019 International Symposium on Micro-NanoMechatronics and Human Science (MHS)","13 Nov 2020","2019","","","1","5","Among the advanced countries, especially in Japan, we are facing on the decreasing birthrate and aging population ahead of the world. In recent, population of single-living elder, and elderly people have been taking care by same-aged person, have been actualized. Unfortunately, the population of caregivers is decreasing according to year. In this situation, the Elderly Person Watching System has been focused on. Especially, Robotics, Internet of Things, and Artificial Intelligence technology have been having high affinity with this system. In this study, firstly, the person watching robot system will be developed. In detail, in this paper, the flexible-analyze system will be focused on. In the algorithm, a resident-tracking task and an energy-saving task are holding. The whole tasks are working based on Reinforcement Learning. Thus, the behavior algorithm of the person watching robot system will be developed. In detail, in this paper, the flexible-analyze system will be focused on. In the algorithm, a resident-tracking task and an energy-saving task are holding. From the verification experiment, the simulation results showed the proposed method has been acquired actions to switch the desired task.","2474-3771","978-1-7281-2675-3","10.1109/MHS48134.2019.9249075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9249075","","Simulation;Sociology;Senior citizens;Switches;Task analysis;Statistics;Robots","geriatrics;home automation;learning (artificial intelligence);mobile robots;path planning","motion-planning method resident-tracking robot;reinforcement learning;elderly people;same-aged person;Internet of things;artificial intelligence technology;energy-saving task;behavior algorithm;elderly person watching system","","","","8","IEEE","13 Nov 2020","","","IEEE","IEEE Conferences"
"Cloud Computing Based Demand Response Management Using Deep Reinforcement Learning","C. Song; G. Han; P. Zeng","Key Laboratory of Networked Control Systems, Shenyang Institute of Automation, Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences, Shenyang, Liaoning, China; Changzhou Key Laboratory of Internet of Things Technology for Intelligent River and Lake, Hohai University, Changzhou, Jiangsu, China; Key Laboratory of Networked Control Systems, Shenyang Institute of Automation, Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences, Shenyang, Liaoning, China","IEEE Transactions on Cloud Computing","8 Mar 2022","2022","10","1","72","81","Demand response is an effective way for ensuring safety and stabilization of power grid by maintaining the balance between the supply and the demand of power grid, and this article focuses on using electric water heaters for demand response. In addition to considering comfort and price factors as did in previous works, this article considers the overshoot temperature and its influence on demand response. First, a theoretical model of the heating and cooling processes of the electric water heater is established; second, the demand response process using electric water heaters is analyzed, including the influences of the physical parameters and the settings of electric water heaters on the demand response process; third, a model is established considering the demand response requirement, the comfort of owners of electric water heaters, and the electricity price, simultaneously; fourth, an optimization method based on deep reinforcement learning is proposed for demand response using electric water heaters. Meanwhile, the influence of parameters on the results of demand response is discussed in details. Experimental results show the effectiveness of the proposed method.","2168-7161","","10.1109/TCC.2021.3117604","National Key Research and Development Program of China(grant numbers:2017YFE0125300); Jiangsu Key Research and Development Program(grant numbers:BE2019648); National Natural Science Foundation of China(grant numbers:U1801264); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9557850","Demand response;cloud computing;load aggregator;water heater;deep reinforcement learning","Water heating;Resistance heating;Demand response;Temperature distribution;Space heating;Cloud computing;Mathematical models","cloud computing;deep learning (artificial intelligence);demand side management;optimisation;power engineering computing;power grids;pricing;reinforcement learning","demand response process;electric water heater;demand response requirement;deep reinforcement learning;demand response management;power grid;cloud computing;comfort factors;price factors;cooling process;heating process","","19","","26","IEEE","4 Oct 2021","","","IEEE","IEEE Journals"
"Multi-Agent Confrontation Game Based on Multi-Agent Reinforcement Learning","S. Han; L. Ke; Z. Wang","The State Key Laboratory for Manufacturing Systems Engineering, School of Automation Science and Engineering, Xi'anJiaotong University, Xi'an, China; The State Key Laboratory for Manufacturing Systems Engineering, School of Automation Science and Engineering, Xi'anJiaotong University, Xi'an, China; CETC Key Laboratory of Data Link Technology, Xi'an, China","2021 IEEE International Conference on Unmanned Systems (ICUS)","22 Dec 2021","2021","","","157","162","This paper studies the multi-agent confrontation game problem, and takes unmanned aerial vehicle (UAV) offense-defense confrontation as the research object. Deep Deterministic Policy Gradient (DDPG) and Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm are used for policy optimization. The experimental results show that MADDPG can acquire good policy in multi-agent confrontation game environment. In order to make MADDPG suitable for large-scale multi-agent game problems and obtain robust policy, this paper improves MADDPG by introducing Mean Field Theory (MFT) and ‘Minimax’ idea. The experimental results show that the improved algorithms can deal with large-scale multi-agent game problem and obtain robust policy.","","978-1-6654-3885-8","10.1109/ICUS52573.2021.9641171","National Natural Science Foundation of China(grant numbers:61973244,61573277); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9641171","multi-agent confrontation game;policy gradient;mean field theory;minimax","Training;Degradation;Conferences;Games;Reinforcement learning;Autonomous aerial vehicles;Robustness","autonomous aerial vehicles;game theory;learning (artificial intelligence);multi-agent systems;remotely operated vehicles","MultiAgent reinforcement learning;multiagent confrontation game problem;unmanned aerial vehicle offense-defense confrontation;MultiAgent Deep Deterministic Policy Gradient algorithm;MADDPG;policy optimization;good policy;multiagent confrontation game environment;large-scale multiagent game problem;robust policy","","","","19","IEEE","22 Dec 2021","","","IEEE","IEEE Conferences"
"Real-Time Virtual Machine Scheduling in Industry IoT Network: A Reinforcement Learning Method","X. Ma; H. Xu; H. Gao; M. Bian; W. Hussain","School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer Engineering and Science, Shanghai University, Shanghai, China; Shanghai Shangda Hairun Information System Company, Ltd., Shanghai, China; Victoria University Business School, Victoria University, Melbourne, VIC, Australia","IEEE Transactions on Industrial Informatics","15 Dec 2022","2023","19","2","2129","2139","The widespread adoption of Industrial Internet of Things (IIoT)-based applications has driven the emergence and development of cloud-related computing paradigms with the ability to seamlessly leverage cloud resources. Heterogeneous resources, mobility factors in IoT, and dynamic behavior make it challenging for the corresponding virtual machine (VM) scheduling problem to address the processing effectiveness of application requests in these kinds of cloud environments. Based on reinforcement learning theory, this article proposes an online VM scheduling scheme (OSEC) for joint energy consumption and cost optimization that divides the scheduling process into two parts: VM allocation and VM migration. First, all the VMs and the physical machines (PMs) are regarded as a set of states and actions in the cloud environment, and the Q-learning feedback is used to achieve the iterative computation of Q-values to obtain the optimal parallel allocation sequence for multiple VMs. Then, VMs are migrated among the active PMs according to a grouping policy and the best-fit principle to achieve dynamic consolidation of the resources in the data center. Finally, experimental results show that compared with state-of-the-art algorithms under different conditions, the proposed method reduces energy consumption by approximately 18.25%, VM execution costs by approximately 21.34%, and service level agreement (SLA) violations by approximately 90.51%.","1941-0050","","10.1109/TII.2022.3211622","National Key R&D Program of China(grant numbers:2020YFB1006003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910009","Energy consumption;execution cost;online consolidation;quality of service;reinforcement learning;virtual machine (VM) scheduling","Job shop scheduling;Resource management;Dynamic scheduling;Optimization;Cloud computing;Real-time systems;Data centers","cloud computing;computer centres;Internet of Things;learning (artificial intelligence);power aware computing;scheduling;virtual machines","active PMs;application requests;cloud environment;cloud-related computing paradigms;corresponding virtual machine;cost optimization;dynamic behavior;dynamic consolidation;heterogeneous resources;IIoT;industry iot network;iterative computation;joint energy consumption;mobility factors;multiple VMs;online VM scheduling scheme;optimal parallel allocation sequence;physical machines;processing effectiveness;Q-learning feedback;reinforcement learning method;reinforcement learning theory;scheduling problem;scheduling process;Things-based applications;Time virtual machine scheduling;VM execution costs","","12","","31","IEEE","4 Oct 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning of Impedance Policies for Peg-in-Hole Tasks: Role of Asymmetric Matrices","S. Kozlovsky; E. Newman; M. Zacksenhouse","Autonomous Systems Program, Technion, Haifa, Israel; Autonomous Systems Program, Technion, Haifa, Israel; Faculty of Mechanical Engineering, Technion, Haifa, Israel","IEEE Robotics and Automation Letters","22 Aug 2022","2022","7","4","10898","10905","Robotic manipulators are playing an increasing role in a wide range of industries. However, their application to assembly tasks is hampered by the need for precise control over the environment and for task-specific coding. Cartesian impedance control is a well-established method for interacting with the environment and handling uncertainties. With the advance of Reinforcement Learning (RL) it has been suggested to learn the impedance matrices. However, most of the current work is limited to learning diagonal impedance matrices in addition to the trajectory itself. We argue that asymmetric impedance matrices enhance the ability to properly correct reference trajectories generated by a baseline planner, alleviating the need for learning the trajectory. Moreover, a task-specific set of asymmetric impedance matrices can be sufficient for simple tasks, alleviating the need for learning variable impedance control. We learn impedance policies for small (few mm) peg-in-hole using model-free RL, and investigate the advantage of using asymmetric impedance matrices and their space-invariance. Finally, we demonstrate zero-shot policy transfer from the simulation to a real robot, and generalization to new real-world environments, with larger parts and semi-flexible pegs.","2377-3766","","10.1109/LRA.2022.3191070","Israel Innovation Authority(grant numbers:74885); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9830834","Compliance and impedance control;machine learning for robot control;force and tactile sensing;reinforcement learning","Impedance;Trajectory;Task analysis;Symmetric matrices;Aerospace electronics;Robots;Damping","assembling;control engineering computing;industrial manipulators;matrix algebra;production engineering computing;reinforcement learning;trajectory control","asymmetric impedance matrices;impedance policies;reinforcement learning;peg-in-hole tasks;assembly tasks;Cartesian impedance control;diagonal impedance matrices;robotic manipulators;RL;zero-shot policy transfer;reference trajectories","","4","","28","CCBYNCND","15 Jul 2022","","","IEEE","IEEE Journals"
"Humanoid Muscle-Skeleton Robot Arm Design and Control Based on Reinforcement Learning","J. Fan; J. Jin; Q. Wang","Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China","2020 15th IEEE Conference on Industrial Electronics and Applications (ICIEA)","9 Nov 2020","2020","","","541","546","Muscle-skeleton robots share similar appearances and functions with humans, making these robots more adaptive in human interaction scenarios. In this paper, a new muscle-skeleton robot arm driven by artificial muscles is proposed. First, we design a new multifilament McKibben muscle and measure its properties. Then a humanoid robot arm referred to the anatomy of the human arm is presented, while the configuration of muscle is adjusted to reduce the complexity of manufacturing and controlling. Muscle-skeleton robot arms with different muscle configurations are controlled using the reinforcement learning method in the simulation environment, and different arm models' movement ranges are obtained to find the best muscle configuration. The experimental results show that the model with the best muscle configuration achieves 79.8% of the whole movement range.","2158-2297","978-1-7281-5169-4","10.1109/ICIEA48937.2020.9248350","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9248350","humanoid robot arm;multifilament McKibben muscle;reinforcement learning","Training;Industrial electronics;Humanoid robots;Reinforcement learning;Muscles;Manipulators;Manufacturing","control engineering computing;control system synthesis;humanoid robots;human-robot interaction;learning (artificial intelligence);learning systems;muscle","humanoid muscle-skeleton robot arm design;human interaction scenarios;artificial muscles;multifilament McKibben muscle;humanoid robot arm;human arm;arm models;muscle configuration;reinforcement learning","","4","","14","IEEE","9 Nov 2020","","","IEEE","IEEE Conferences"
"Control of Flexible Manipulator Based on Reinforcement Learning","L. Cui; W. Chen; H. Wang; J. Wang","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2018 Chinese Automation Congress (CAC)","24 Jan 2019","2018","","","2744","2749","Most researches about control of flexible manipulators are all based on the dynamic model, which is difficult to establish because of their flexibility and the tedious process of measuring flexible link's parameters. In this paper, the goal is to design a controller which is able to control the flexible manipulator to track a given position in joint space and suppress vibration without knowing the dynamic model. For the problem of tracking a given position, a tracking controller is designed based on sliding mode control, and for the purpose of vibration suppression, a vibration suppression controller is designed as a deep neural network. Because the input of the flexible manipulator, torques at each joint, is a high dimensional and continuous space, Deep Deterministic Policy Gradient Algorithm (DDPG) is adopted to train the neural network in the vibration suppression controller. The effectiveness of the proposed controller to track a given position and suppress vibration is demonstrated by numerical simulation.","","978-1-7281-1312-8","10.1109/CAC.2018.8623788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8623788","flexible manipulator;vibration suppression;reinforcement learning","Manipulator dynamics;Vibrations;Neural networks;Cameras;Gravity;Aerospace electronics","flexible manipulators;gradient methods;learning (artificial intelligence);manipulator dynamics;neurocontrollers;position control;variable structure systems;vibration control","reinforcement learning;numerical simulation;deep deterministic policy gradient algorithm;deep neural network;sliding mode control;vibration suppression controller;tracking controller;joint space;flexible link;dynamic model;flexible manipulator","","5","","21","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Obtaining Robust Control and Navigation Policies for Multi-robot Navigation via Deep Reinforcement Learning","C. Jestel; H. Surmann; J. Stenzel; O. Urbann; M. Brehler","Automation and Embedded Systems, Fraunhofer IML, Dortmund, Germany; Computer Science and Communication, Westfälische Hochschule Gelsenkirchen, Gelsenkirchen, Germany; Automation and Embedded Systems, Fraunhofer IML, Dortmund, Germany; Automation and Embedded Systems, Fraunhofer IML, Dortmund, Germany; Automation and Embedded Systems, Fraunhofer IML, Dortmund, Germany","2021 7th International Conference on Automation, Robotics and Applications (ICARA)","17 Mar 2021","2021","","","48","54","Multi-robot navigation is a challenging task in which multiple robots must be coordinated simultaneously within dynamic environments. We apply deep reinforcement learning (DRL) to learn a decentralized end-to-end policy which maps raw sensor data to the command velocities of the agent. In order to enable the policy to generalize, the training is performed in different environments and scenarios. The learned policy is tested and evaluated in common multi-robot scenarios like switching a place, an intersection and a bottleneck situation. This policy allows the agent to recover from dead ends and to navigate through complex environments.","","978-1-6654-0469-3","10.1109/ICARA51699.2021.9376457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376457","Deep Reinforcement Learning;Navigation;Multi-Agent","Training;Robust control;Navigation;Reinforcement learning;Switches;Robot sensing systems;Task analysis","deep learning (artificial intelligence);mobile robots;multi-robot systems;robust control","raw sensor data mapping;learned policy;decentralized end-to-end policy;dynamic environments;multiple robots;deep reinforcement learning;multirobot navigation","","5","","21","IEEE","17 Mar 2021","","","IEEE","IEEE Conferences"
"Multi-Target Encirclement with Collision Avoidance via Deep Reinforcement Learning using Relational Graphs","T. Zhang; Z. Liu; Z. Pu; J. Yi","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","8794","8800","In this paper, we propose a novel decentralized method based on deep reinforcement learning using robot-level and target-level relational graphs, to solve the problem of multi-target encirclement with collision avoidance (MECA). Specifically, the robot-level relational graphs, composed of three heterogeneous relational graphs between each robot and other robots, targets and obstacles, are modeled and learned through using graph attention networks (GATs) for extracting different spatial relational representations. Moreover, for each target within the observation of each robot, a target-level relational graph is built with GAT to construct spatial relations from the robot. Furthermore, the movement of each target is modeled by the target-level relational graph and learned through supervised learning for predicting the trajectory of the target. In addition, a knowledge-embedded compound reward function is defined to solve the multi-objective problem in MECA, and guide the policy learning for deriving the behavior of MECA. An actor-critic training algorithm based on the centralized training and decentralized execution framework is adopted to train the policy network. Simulation and real-world experiment results demonstrate the effectiveness and generalization of our method.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812151","National Key Research and Development Program of China(grant numbers:2018AAA0102402,2018AAA0101005); Chinese Academy of Sciences(grant numbers:XDA27030403); National Natural Science Foundation of China(grant numbers:62073323); Science and Technology Development Fund(grant numbers:0025/2019/AKP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812151","","Training;Supervised learning;Reinforcement learning;Predictive models;Prediction algorithms;Trajectory;Compounds","collision avoidance;graph theory;learning (artificial intelligence);mobile robots;multi-robot systems","multitarget encirclement;collision avoidance;robot-level relational graphs;heterogeneous relational graphs;graph attention networks;different spatial relational representations;target-level relational graph;spatial relations;deep reinforcement learning","","3","","29","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Active Pushing for Better Grasping in Dense Clutter with Deep Reinforcement Learning","N. Lu; T. Lu; Y. Cai; S. Wang","Chinese Academy of Sciences, Research Center on Intelligent Robotic Systems, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Research Center on Intelligent Robotic Systems, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Research Center on Intelligent Robotic Systems, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Research Center on Intelligent Robotic Systems, Institute of Automation, Beijing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","1657","1663","Robotic grasping in unstructured dense clutter remains a challenging task and has always been a key research direction in the field of robotics. In this paper, we propose a novel robotic grasping system that could use the synergies between pushing and grasping actions to automatically grasp the objects in dense clutter. Our method involves using fully convolutional action-value functions (FCAVF) to map from visual observations to two action-value tables in a Q-learning framework. These two value tables infer the utility of pushing and grasping actions, and the highest value with the corresponding location and orientation means the best place to execute action for the end effector. For better grasping, we introduce an active pushing mechanism based on a new metric, called Dispersion Degree, which describes how spread out the objects are in the environment. Then we design a coordination mechanism to apply the synergies of different actions based on the action-values and dispersion degree of the objects and make the grasps more effective. Experimental results show that our proposed robotic grasping system can greatly improve the robotic grasping success rate in dense clutter and also has the capability to be generalized to the new scenarios.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327270","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327270","robotic grasp;active push;synergies;deep reinforcement learning","Grasping;Robots;Robot kinematics;Reinforcement learning;Clutter;Measurement;Training","convolutional neural nets;dexterous manipulators;end effectors;grippers;learning (artificial intelligence);robot vision","deep reinforcement learning;unstructured dense clutter;robotic grasping system;synergies;fully convolutional action-value functions;action-value tables;Q-learning framework;active pushing mechanism;robotic grasping success rate;dispersion degree;visual observations;FCAVF;end effector","","3","","31","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Setpoint Tracking for the Suspension System of Medium-Speed Maglev Trains via Reinforcement Learning","F. Zhao; P. Jiang; K. You; S. Song; W. Zhang; L. Tong","Department of Automation and BNRist, Tsinghua University, Beijing, China; Department of Automation and BNRist, Tsinghua University, Beijing, China; Department of Automation and BNRist, Tsinghua University, Beijing, China; Department of Automation and BNRist, Tsinghua University, Beijing, China; CRRC Zhuzhou Locomotive Co. Ltd.; CRRC Zhuzhou Locomotive Co. Ltd.","2019 IEEE 15th International Conference on Control and Automation (ICCA)","14 Nov 2019","2019","","","1620","1625","In this paper, we consider setpoint tracking for the suspension system of medium-speed maglev trains for keeping constant distance above the track. Due to complicated environment, unknown disturbances and strong nonlinear coupling of model, the problem can not be effectively solved by most of the model-based controllers. To this purpose, we formulate the setpoint tracking problem as continuous-state, continuous-action Markov decision processes under unknown transition probabilities. Based on the deterministic policy gradient and neural network approximation, we propose a model-free reinforcement learning (RL) algorithm that learns a state-feedback controller from sampled data of the suspension system. We illustrate with simulations that our model-free method has high performance.","1948-3457","978-1-7281-1164-3","10.1109/ICCA.2019.8900006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900006","","","approximation theory;decision theory;gradient methods;learning (artificial intelligence);magnetic levitation;Markov processes;neurocontrollers;probability;railways;sampled data systems;state feedback;suspensions (mechanical components)","suspension system;medium-speed maglev trains;setpoint tracking problem;continuous-action Markov decision processes;unknown transition probabilities;model-free reinforcement learning algorithm;state-feedback controller;RL algorithm;strong nonlinear coupling;deterministic policy gradient;neural network approximation;sampled data;continuous-state processes","","2","","20","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Underwater Navigation Subject to Infrequent Position Measurements","K. Cheng; W. Lu; H. Xiong; H. Liu","School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","1984","1990","This paper explores the Reinforcement Learning (RL) approach in an underwater navigation problem, subject to infrequent position measurements and other unsynchronized observations in the absence of a Doppler velocity log equipment. In particular, an end-to-end History-Window(HW)-RL approach and a classical RL approach are studied and their performances are compared against an anti-windup PID controller. The state used in classical RL and PID is estimated from an Extend-Kalman-Filter (EKF), while the HW-RL utilizes a history of previous measurements as the input to the neural network controller, for the purpose of jointly training an (encoded) state estimator and a controller. Preliminary results have been obtained through numerical simulations conducted in the Webots simulator, showing the HW-RL and the classical RL (with EKF) are able to track given waypoints when the position measurements are infrequent.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727505","National Natural Science Foundation of China; Harbin Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727505","Autonomous Underwater Vehicle;Reinforcement Learning;Underwater Navigation;End-to-end Learning","Training;Recurrent neural networks;Navigation;Tracking;Simulation;Reinforcement learning;Position measurement","Kalman filters;learning (artificial intelligence);mobile robots;neurocontrollers;position measurement;state estimation;three-term control","underwater navigation subject;infrequent position measurements;Reinforcement Learning approach;underwater navigation problem;unsynchronized observations;Doppler velocity log equipment;classical RL approach;anti-windup PID controller;Extend-Kalman-Filter;HW-RL;neural network controller;state estimator","","1","","19","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Distributed Optimal Power Scheduling for Microgrid System via Deep Reinforcement Learning with Privacy Preserving","T. He; X. Wu; H. Dong; F. Guo; W. Yu","Department of Automation, Zhejiang University of Technology, Hangzhou, China; Department of Automation, Zhejiang University of Technology, Hangzhou, China; Department of Automation, Zhejiang University of Technology, Hangzhou, China; Department of Automation, Zhejiang University of Technology, Hangzhou, China; Green Rooftop Inc, Hangzhou, China","2022 IEEE 17th International Conference on Control & Automation (ICCA)","25 Jul 2022","2022","","","820","825","This paper tries to solve the optimal power scheduling problem for microgrid system by considering data privacy-preserving. Conventionally, such a problem is addressed by various numerical optimization approaches. However, these approaches usually suffer from high computational costs and result in a slow convergence speed. In order to tackle these, a novel data-driven approach, i.e., distributed deep reinforcement learning algorithm, is developed in this paper. A deep deterministic policy gradient algorithm learning model for each distributed power source is built first. Then in a distributed framework, each local power source establishes a state space and an action space respectively. Finally, the gradient clipping strategy is used to update the local model, and the differential privacy method is introduced to protect data privacy. Simulation results show that the individual difference rate of data protected by this method reaches 40%, and the accuracy rate of aggregated data reaches more than 85%. At the same time, compared with the centralized deep reinforcement learning algorithm, the training speed is increased by 15%, and the cost function value is reduced by 60%. These results indicate that a good trade-off is achieved between accurate optimal solution and privacy preserving.","1948-3457","978-1-6654-9572-1","10.1109/ICCA54724.2022.9831947","Natural Science Foundation of Zhejiang Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9831947","","Training;Privacy;Differential privacy;Processor scheduling;Simulation;Reinforcement learning;Microgrids","data protection;deep learning (artificial intelligence);distributed power generation;gradient methods;optimisation;power engineering computing;reinforcement learning;scheduling;state-space methods","gradient clipping strategy;data privacy protection;distributed optimal power scheduling;microgrid system;data privacy-preserving;distributed deep reinforcement learning;deep deterministic policy gradient;distributed power source;state space;action space;differential privacy","","1","","21","IEEE","25 Jul 2022","","","IEEE","IEEE Conferences"
"Conflict-constrained Multi-agent Reinforcement Learning Method for Parking Trajectory Planning","S. Chen; M. Wang; Y. Yang; W. Song","School of Automation, Beijing Institute of Technology, Beijing, P.R. China; School of Automation, Beijing Institute of Technology, Beijing, P.R. China; School of Automation, Beijing Institute of Technology, Beijing, P.R. China; School of Automation, Beijing Institute of Technology, Beijing, P.R. China","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","9421","9427","Automated Valet Parking (AVP) has been exten-sively researched as an important application of autonomous driving. Considering the high dynamics and density of real parking lots, a system that considers multiple vehicles simultaneously is more robust and efficient than a single vehicle setting as in most studies. In this paper, we propose a dis-tributed Multi-agent Reinforcement Learning(MARL) method for coordinating multiple vehicles in the framework of an AVP system. This method utilizes traditional trajectory planning to accelerate the learning process and introduces collision conflict constraints for policy optimization to mitigate the path conflict problem. In contrast to other centralized multi-agent path finding methods, the proposed approach is scalable, distributed, and adapts to dynamic stochastic scenarios. We train the models in random scenarios and validate in several artificially designed complex parking scenarios where vehicles are always disturbed by dynamic and static obstacles. Experimental results show that our approach mitigates path conflicts and excels in terms of success rate and efficiency.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160698","","Training;Trajectory planning;Stochastic processes;Feature extraction;Trajectory;Timing;Safety","collision avoidance;learning (artificial intelligence);mobile robots;multi-agent systems;reinforcement learning;road vehicles;trajectory control","artificially designed complex parking scenarios;autonomous driving;AVP system;centralized multiagent path finding methods;collision conflict constraints;conflict-constrained Multiagent Reinforcement Learning method;dynamic obstacles;dynamic stochastic scenarios;learning process;multiple vehicles;parking lots;Parking trajectory planning;path conflict problem;path conflicts;single vehicle;static obstacles;traditional trajectory planning;Valet Parking","","","","28","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Model-free Based Reinforcement Learning Control Strategy of Aircraft Attitude Systems","D. Huang; J. Hu; Z. Peng; B. Chen; M. Hao; B. K. Ghosh","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; Science and Technology on Complex, System Control and Intelligent Agent Cooperation Laboratory, Beijing, China; Department of Mathematics and Statistics, Texas Tech University, Lubbock, USA","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","743","748","Traditional aircraft control algorithms have a strong dependence on system models, and are difficult to cope with the increasingly complex battlefield environment for intelligent aircrafts. In this paper, a model-free reinforcement learning is proposed to solve an attitude stabilization problem of an aircraft based online intelligent control strategy. The attitude control problem is firstly formulated as an optimal control problem, and then an adaptive dynamic programming (ADP) technology is applied to compute the corresponding nonlinear Hamilton-Jacobi-Bellman (HJB) equation. Then, an actor-critic neural network structure is established to learn the optimal controller online not requiring the information of the aircraft dynamics. The proposed intelligent control strategy enables the aircraft to adjust its attitude according to the actual mission targets and environments under the proposed online control strategy, so that autonomous learning and intelligent operation can be realized. Finally, simulation examples are presented to validate the proposed model-free based control strategy.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9326707","National Natural Science Foundation of China; Program for New Century Excellent Talents in University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326707","Model-Free Control;Reinforcement Learning;Aircraft Attitude System;Actor-Critic Neural Network","Atmospheric modeling;Aircraft;Adaptation models;Mathematical model;Aerodynamics;Attitude control;Vehicle dynamics","adaptive control;aircraft control;attitude control;control system synthesis;dynamic programming;intelligent control;learning systems;nonlinear control systems;optimal control;stability","model-free based reinforcement learning control strategy;aircraft attitude systems;traditional aircraft control algorithms;system models;battlefield environment;intelligent aircrafts;model-free reinforcement learning;attitude stabilization problem;intelligent control strategy;attitude control problem;optimal control problem;adaptive dynamic programming technology;nonlinear Hamilton-Jacobi-Bellman equation;actor-critic neural network structure;optimal controller;aircraft dynamics;intelligent operation;model-free based control strategy;ADP;HJB equation","","","","34","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"A Mapless Navigation Method Based on Reinforcement Learning and Local Obstacle Map","X. Pang; Y. Li; Q. Liu; K. Deng","School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","5754","5759","Recently, research on mapless navigation based on Reinforcement Learning (RL) shows great potential. However, for indoor scenes with ""local minimal areas"", the results of end- to-end training based solely on RL are not ideal. This paper presents an encoding method for navigation input information, which encodes the single-line lidar information into a Local Obstacle Map (LOMap), and combines the robot speed and target point position information into the neural network. We modify a Safe RL algorithm Constrained Policy Optimization (CPO) for training, and propose a new navigation evaluation metric that considers time-consuming. Simulation experiments on Gazebo show that our proposed method can better adapt to indoor scenes with local minimal areas, and outperforms the CPO method with raw lidar information input in common metrics.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055983","mapless navigation;reinforcement learning","Training;Measurement;Laser radar;Navigation;Neural networks;Reinforcement learning;Robot sensing systems","collision avoidance;control engineering computing;mobile robots;navigation;neural nets;optimisation;reinforcement learning","constrained policy optimization;CPO method;end-to-end training;indoor scenes;local minimal areas;local obstacle map;LOMap;mapless navigation method;navigation evaluation metric;navigation input information;neural network;point position information;raw lidar information input;reinforcement learning;safe RL algorithm;single-line lidar information","","","","18","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Safe Exploration of Reinforcement Learning with Data-Driven Control Barrier Function","C. Zhang; S. Wang; S. Meng; Z. Kan","Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","1008","1013","Reinforcement learning relies on exploration and exploitation to find optimal policies. However, unconstrained exploration might lead to unsafe actions that jeopardize the system safety. To address this issue, this work presents a RL-based framework that integrates model-based CBF to ensure safe exploration during learning. Rather than synthesizing CBF by hand for complex dynamic systems, we exploit data-driven methods to learn CBFs from collected demonstrations of safe and desirable behavior. Unlike prior works that restrict on off-line collected expert demonstrations to train CBF, the CBF in this work is learned not only from preliminary expert demonstrations, but also from the on-line generated data at runtime, resulting in improved adaptation to complex environments. Numerical simulations and physical experiments using Crazyflie quadrotors are carried out to demonstrate the effectiveness of the developed safe RL framework. The experiment video is available at https://youtu.be/uscl-BQsLRo.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055848","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055848","Control barrier function;reinforcement learning;shield;data-driven","Adaptation models;Runtime;System dynamics;Heuristic algorithms;Reinforcement learning;Numerical simulation;Safety","autonomous aerial vehicles;control engineering computing;helicopters;learning (artificial intelligence);mobile robots;reinforcement learning;trajectory control","collected demonstrations;complex dynamic systems;data-driven control barrier function;data-driven methods;desirable behavior;developed safe RL framework;model-based CBF;off-line collected expert demonstrations;on-line generated data;optimal policies;preliminary expert demonstrations;reinforcement learning;RL-based framework;safe behavior;safe exploration;system safety;unconstrained exploration;unsafe actions","","","","19","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"A Virtual Reinforcement Learning Method for Aero-engine Intelligent Control","J. Zhu; W. Tang; J. Dong; P. Li","College of Automation, Northwestern Polytechnical University, Xi’an, China; College of Automation, Northwestern Polytechnical University, Xi’an, China; College of Automation, Northwestern Polytechnical University, Xi’an, China; College of Automation, Northwestern Polytechnical University, Xi’an, China","2023 8th International Conference on Automation, Control and Robotics Engineering (CACRE)","8 Aug 2023","2023","","","138","143","The aero-engine is a highly intricate thermal mechanical system characterized by significant nonlinearity, uncertainty, and time-varying behavior. As aerospace technology continues to advance, there is an increasing demand for aero-engines to deliver higher levels of performance. Against this background, traditional control methods have shown limitations in achieving optimal outcomes. Intelligent aero-engine technology has emerged as a significant and promising research area. Therefore, a virtual reinforcement learning method for aero-engine intelligent control is proposed in this paper. Firstly, this research establishes a data-driven virtual simulation environment for the aero-engine employing long short-term memory (LSTM) neural networks. Subsequently, the intelligent controller is trained within this environment utilizing the deep deterministic policy gradient (DDPG) algorithm. Finally, we verify the intelligent controller performance with JT9D engine model. Compared with traditional PID control, the intelligent controller has smaller overshoot and shorter setting time.","","979-8-3503-0277-6","10.1109/CACRE58689.2023.10208404","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10208404","intelligent control;aero-engine;virtual reinforcement learning;LSTM;DDPG algorithm","PI control;Uncertainty;Simulation;Neural networks;Reinforcement learning;PD control;Aircraft propulsion","aerospace engines;control engineering computing;deep learning (artificial intelligence);gradient methods;intelligent control;learning (artificial intelligence);recurrent neural nets;reinforcement learning;three-term control","aero-engine intelligent control;data-driven virtual simulation environment;highly intricate thermal mechanical system;intelligent aero-engine technology;intelligent controller performance;JT9D engine model;promising research area;significant research area;traditional control methods;traditional PID control;virtual reinforcement learning method","","","","18","IEEE","8 Aug 2023","","","IEEE","IEEE Conferences"
"Human-Robot Interaction System Design for Manipulator Control Using Reinforcement Learning","Z. Ding; C. Song; J. Xu; Y. Dou","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China","2021 36th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","26 Jul 2021","2021","","","660","665","In this article, a novel human-robot interaction (HRI) system is presented and applied in the robotic arm coordinated operation control task. The presented HRI system includes two parts, the impedance model controller and the robotic arm controller, which allows the operator to manipulate the robotic arm to accomplish the given task with minimal human effort. First, the model-based reinforcement learning (RL) method is applied in the impedance model for operator adaptation. The impedance model controller can transform human input into the specific signal for the manipulator. Second, a novel adaptive manipulator controller is designed. In contrast to existing controllers, a velocity-free filter is implemented in our controller, which is developed to replace the manipulator actuator's speed signal. The effectiveness of the presented HRI system is verified by the simulation based on real manipulator parameters.","","978-1-6654-3712-7","10.1109/YAC53711.2021.9486647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9486647","Adaptive impedance control;Human-robot interaction;Reinforcement learning","Adaptation models;Adaptive systems;Robot kinematics;Human-robot interaction;Reinforcement learning;Transforms;Manipulators","actuators;adaptive control;control engineering computing;control system synthesis;filtering theory;human-robot interaction;learning (artificial intelligence);manipulators","manipulator actuator speed signal;velocity-free filter;novel adaptive manipulator controller design;RL method;robotic arm coordinated operation control task;operator adaptation;model-based reinforcement learning method;impedance model controller;human-robot interaction system design;manipulator parameters;HRI system","","","","23","IEEE","26 Jul 2021","","","IEEE","IEEE Conferences"
