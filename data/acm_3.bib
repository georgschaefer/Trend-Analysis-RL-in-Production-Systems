@inproceedings{10.1145/3491102.3501992,
author = {Liao, Yi-Chi and Todi, Kashyap and Acharya, Aditya and Keurulainen, Antti and Howes, Andrew and Oulasvirta, Antti},
title = {Rediscovering Affordance: A Reinforcement Learning Perspective},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501992},
doi = {10.1145/3491102.3501992},
abstract = {Affordance refers to the perception of possible actions allowed by an object. Despite its relevance to human–computer interaction, no existing theory explains the mechanisms that underpin affordance-formation; that is, how affordances are discovered and adapted via interaction. We propose an integrative theory of affordance-formation based on the theory of reinforcement learning in cognitive sciences. The key assumption is that users learn to associate promising motor actions to percepts via experience when reinforcement signals (success/failure) are present. They also learn to categorize actions (e.g., “rotating” a dial), giving them the ability to name and reason about affordance. Upon encountering novel widgets, their ability to generalize these actions determines their ability to perceive affordances. We implement this theory in a virtual robot model, which demonstrates human-like adaptation of affordance in interactive widgets tasks. While its predictions align with trends in human data, humans are able to adapt affordances faster, suggesting the existence of additional mechanisms.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {362},
numpages = {15},
keywords = {Perception, Motion Planning, Affordance, Adaptation, Modeling, Action, Machine Learning, Theory, Interaction, Robotics, Design, Reinforcement Learning},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{10.5555/3586589.3586810,
author = {Tirumala, Dhruva and Galashov, Alexandre and Noh, Hyeonwoo and Hasenclever, Leonard and Pascanu, Razvan and Schwarz, Jonathan and Desjardins, Guillaume and Czarnecki, Wojciech Marian and Ahuja, Arun and Teh, Yee Whye and Heess, Nicolas},
title = {Behavior Priors for Efficient Reinforcement Learning},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {As we deploy reinforcement learning agents to solve increasingly challenging problems, methods that allow us to inject prior knowledge about the structure of the world and effective solution strategies becomes increasingly important. In this work we consider how information and architectural constraints can be combined with ideas from the probabilistic modeling literature to learn behavior priors that capture the common movement and interaction patterns that are shared across a set of related tasks or contexts. For example the day-to day behavior of humans comprises distinctive locomotion and manipulation patterns that recur across many different situations and goals. We discuss how such behavior patterns can be captured using probabilistic trajectory models and how these can be integrated effectively into reinforcement learning schemes, e.g. to facilitate multi-task and transfer learning. We then extend these ideas to latent variable models and consider a formulation to learn hierarchical priors that capture different aspects of the behavior in reusable modules. We discuss how such latent variable formulations connect to related work on hierarchical reinforcement learning (HRL) and mutual information and curiosity based objectives, thereby offering an alternative perspective on existing ideas. We demonstrate the effectiveness of our framework by applying it to a range of simulated continuous control domains, videos of which can be found at the following},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {221},
numpages = {68},
keywords = {control as inference, hierarchical reinforcement learning, reinforcement learning, probabilistic graphical models, transfer learning}
}

@article{10.5555/944919.944955,
author = {Perkins, Theodore J. and Barto, Andrew G.},
title = {Lyapunov Design for Safe Reinforcement Learning},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {Lyapunov design methods are used widely in control engineering to design controllers that achieve qualitative objectives, such as stabilizing a system or maintaining a system's state in a desired operating range. We propose a method for constructing safe, reliable reinforcement learning agents based on Lyapunov design principles. In our approach, an agent learns to control a system by switching among a number of given, base-level controllers. These controllers are designed using Lyapunov domain knowledge so that any switching policy is safe and enjoys basic performance guarantees. Our approach thus ensures qualitatively satisfactory agent behavior for virtually any reinforcement learning algorithm and at all times, including while the agent is learning and taking exploratory actions. We demonstrate the process of designing safe agents for four different control problems. In simulation experiments, we find that our theoretically motivated designs also enjoy a number of practical benefits, including reasonable performance initially and throughout learning, and accelerated learning.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {803–832},
numpages = {30}
}

@inproceedings{10.1145/2576768.2598360,
author = {Karafotias, Giorgos and Eiben, Agoston Endre and Hoogendoorn, Mark},
title = {Generic Parameter Control with Reinforcement Learning},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598360},
doi = {10.1145/2576768.2598360},
abstract = {Parameter control in Evolutionary Computing stands for an approach to parameter setting that changes the parameters of an Evolutionary Algorithm (EA) on-the-fly during the run. In this paper we address the issue of a generic and parameter-independent controller that can be readily plugged into an existing EA and offer performance improvements by varying the EA parameters during the problem solution process. Our approach is based on a careful study of Reinforcement Learning (RL) theory and the use of existing RL techniques. We present experiments using various state-of-the-art EAs solving different difficult problems. Results show that our RL control method has very good potential in improving the quality of the solution found without requiring additional resources or time and with minimal effort from the designer of the application.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1319–1326},
numpages = {8},
keywords = {evolutionary algorithms, parameter control, reinforcement learning},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@inproceedings{10.5555/3306127.3331670,
author = {Narvekar, Sanmit and Stone, Peter},
title = {Learning Curriculum Policies for Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Curriculum learning in reinforcement learning is a training methodology that seeks to speed up learning of a difficult target task, by first training on a series of simpler tasks and transferring the knowledge acquired to the target task. Automatically choosing a sequence of such tasks (i.e., a curriculum) is an open problem that has been the subject of much recent work in this area. In this paper, we build upon a recent method for curriculum design, which formulates the curriculum sequencing problem as a Markov Decision Process. We extend this model to handle multiple transfer learning algorithms, and show for the first time that a curriculum policy over this MDP can be learned from experience. We explore various representations that make this possible, and evaluate our approach by learning curriculum policies for multiple agents in two different domains. The results show that our method produces curricula that can train agents to perform on a target task as fast or faster than existing methods.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {25–33},
numpages = {9},
keywords = {curriculum learning, transfer learning, reinforcement learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1109/IAT.2007.75,
author = {Ribeiro, Richardson and Koerich, Alessandro L. and Enembreck, Fabricio},
title = {Noise Tolerance in Reinforcement Learning Algorithms},
year = {2007},
isbn = {0769530273},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IAT.2007.75},
doi = {10.1109/IAT.2007.75},
abstract = {This paper proposes a mechanism of noise tolerance for reinforcement learning algorithms. An adaptive agent that employs reinforcement learning algorithms may receive and accumulate many rewards for its actions. However, the amount of rewards received by the agent is not a guarantee of convergence to an optimal policy of action due to the noises produced by the environment. Therefore, we propose a noise tolerance mechanism which is able to estimate convergent policies without causing delays or an unexpected speedup in the agent's learning. Experimental results have shown that the proposed mechanism is able to speed up the convergence of the agent achieving good action policies very fast even in dynamic and noisy environments.},
booktitle = {Proceedings of the 2007 IEEE/WIC/ACM International Conference on Intelligent Agent Technology},
pages = {265–268},
numpages = {4},
keywords = {Adaptive Autonomous Agents, Reinforcement Learning and Noise Tolerant Learning.},
series = {IAT '07}
}

@inproceedings{10.1145/860575.860689,
author = {Chalkiadakis, Georgios and Boutilier, Craig},
title = {Coordination in Multiagent Reinforcement Learning: A Bayesian Approach},
year = {2003},
isbn = {1581136838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/860575.860689},
doi = {10.1145/860575.860689},
abstract = {Much emphasis in multiagent reinforcement learning (MARL) research is placed on ensuring that MARL algorithms (eventually) converge to desirable equilibria. As in standard reinforcement learning, convergence generally requires sufficient exploration of strategy space. However, exploration often comes at a price in the form of penalties or foregone opportunities. In multiagent settings, the problem is exacerbated by the need for agents to "coordinate" their policies on equilibria. We propose a Bayesian model for optimal exploration in MARL problems that allows these exploration costs to be weighed against their expected benefits using the notion of value of information. Unlike standard RL models, this model requires reasoning about how one's actions will influence the behavior of other agents. We develop tractable approximations to optimal Bayesian exploration, and report on experiments illustrating the benefits of this approach in identical interest games.},
booktitle = {Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent Systems},
pages = {709–716},
numpages = {8},
keywords = {multiagent learning, reinforcement learning, Bayesian methods},
location = {Melbourne, Australia},
series = {AAMAS '03}
}

@inproceedings{10.1145/1015330.1015430,
author = {Abbeel, Pieter and Ng, Andrew Y.},
title = {Apprenticeship Learning via Inverse Reinforcement Learning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015430},
doi = {10.1145/1015330.1015430},
abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {1},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/3543873.3587367,
author = {Luo, Jiyun and Hazra, Kurchi Subhra and Huo, Wenyu and Li, Rui and Mahabal, Abhijit},
title = {Personalized Style Recommendation via Reinforcement Learning},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587367},
doi = {10.1145/3543873.3587367},
abstract = {Pinterest fashion and home decor searchers often have different style tastes. Some existing work adopts users’ past engagement to infer style preference. These methods cannot help users discover new styles. Other work requires users to provide text or visual signals to describe their style preference, but users often are not familiar with style terms and do not have the right image to start with. In this paper, we propose a reinforcement learning (RL) method to help users explore and exploit style space without requiring extra user input. Experimental results show that our method improves the success rate of Pinterest fashion and home decor searches by 34.8\%.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {290–293},
numpages = {4},
keywords = {reinforcement learning, information retrieval, query recommendation, image search},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3005745.3005750,
author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
title = {Resource Management with Deep Reinforcement Learning},
year = {2016},
isbn = {9781450346610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3005745.3005750},
doi = {10.1145/3005745.3005750},
abstract = {Resource management problems in systems and networking often manifest as difficult online decision making tasks where appropriate solutions depend on understanding the workload and environment. Inspired by recent advances in deep reinforcement learning for AI problems, we consider building systems that learn to manage resources directly from experience. We present DeepRM, an example solution that translates the problem of packing tasks with multiple resource demands into a learning problem. Our initial results show that DeepRM performs comparably to state-of-the-art heuristics, adapts to different conditions, converges quickly, and learns strategies that are sensible in hindsight.},
booktitle = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks},
pages = {50–56},
numpages = {7},
location = {Atlanta, GA, USA},
series = {HotNets '16}
}

@inproceedings{10.5555/3535850.3535963,
author = {Pan, Xinlei and Xiao, Chaowei and He, Warren and Yang, Shuang and Peng, Jian and Sun, Mingjie and Liu, Mingyan and Li, Bo and Song, Dawn},
title = {Characterizing Attacks on Deep Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recent studies show that Deep Reinforcement Learning (DRL) models are vulnerable to adversarial attacks, which attack DRL models by adding small perturbations to the observations. However, some attacks assume full availability of the victim model, and some require a huge amount of computation, making them less feasible for real world applications. In this work, we make further explorations of the vulnerabilities of DRL by studying other aspects of attacks on DRL using realistic and efficient attacks. First, we adapt and propose efficient black-box attacks when we do not have access to DRL model parameters. Second, to address the high computational demands of existing attacks, we introduce efficient online sequential attacks that exploit temporal consistency across consecutive steps. Third, we explore the possibility of an attacker perturbing other aspects in the DRL setting, such as the environment dynamics. Finally, to account for imperfections in how an attacker would inject perturbations in the physical world, we devise a method for generating a robust physical perturbations to be printed. The attack is evaluated on a real-world robot under various conditions. We conduct extensive experiments both in simulation such as Atari games, robotics and autonomous driving, and on real-world robotics, to compare the effectiveness of the proposed attacks with baseline approaches. To the best of our knowledge, we are the first to apply adversarial attacks on DRL systems to physical robots.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1010–1018},
numpages = {9},
keywords = {robotics, adversarial machine learning, reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.5555/1516744.1516984,
author = {Paternina-Arboleda, Carlos D. and Montoya-Torres, Jairo R. and F\'{a}bregas-Ariza, Aldo},
title = {Simulation-Optimization Using a Reinforcement Learning Approach},
year = {2008},
isbn = {9781424427086},
publisher = {Winter Simulation Conference},
abstract = {The global optimization of complex systems such as industrial systems often necessitates the use of computer simulation. In this paper, we suggest the use of reinforcement learning (RL) algorithms and artificial neural networks for the optimization of simulation models. Several types of variables are taken into account in order to find global optimum values. After a first evaluation through mathematical functions with known optima, the benefits of our approach are illustrated through the example of an inventory control problem frequently found in manufacturing systems. Single-item and multi-item inventory cases are considered. The efficiency of the proposed procedure is compared against a commercial tool.},
booktitle = {Proceedings of the 40th Conference on Winter Simulation},
pages = {1376–1383},
numpages = {8},
location = {Miami, Florida},
series = {WSC '08}
}

@inproceedings{10.1145/1143844.1143929,
author = {Nevmyvaka, Yuriy and Feng, Yi and Kearns, Michael},
title = {Reinforcement Learning for Optimized Trade Execution},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143929},
doi = {10.1145/1143844.1143929},
abstract = {We present the first large-scale empirical application of reinforcement learning to the important problem of optimized trade execution in modern financial markets. Our experiments are based on 1.5 years of millisecond time-scale limit order data from NASDAQ, and demonstrate the promise of reinforcement learning methods to market microstructure problems. Our learning algorithm introduces and exploits a natural "low-impact" factorization of the state space.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {673–680},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}

@inproceedings{10.5555/3237383.3237849,
author = {Ramakrishnan, Ramya and Kamar, Ece and Dey, Debadeepta and Shah, Julie and Horvitz, Eric},
title = {Discovering Blind Spots in Reinforcement Learning},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Agents trained in simulation may make errors in the real world due to mismatches between training and execution environments. These mistakes can be dangerous and difficult to discover because the agent cannot predict them a priori. We propose using oracle feedback to learn a predictive model of these blind spots to reduce costly errors in real-world applications. We focus on blind spots in reinforcement learning (RL) that occur due to incomplete state representation: The agent does not have the appropriate features to represent the true state of the world and thus cannot distinguish among numerous states. We formalize the problem of discovering blind spots in RL as a noisy supervised learning problem with class imbalance. We learn models to predict blind spots in unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. The models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections. We evaluate our approach on two domains and show that it achieves higher predictive performance than baseline methods, and that the learned model can be used to selectively query an oracle at execution time to prevent errors. We also empirically analyze the biases of various feedback types and how they influence the discovery of blind spots.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1017–1025},
numpages = {9},
keywords = {safe reinforcement learning, transfer learning, interactive reinforcement learning},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.5555/3586210.3586486,
author = {Yedidsion, Harel and Dawadi, Prafulla and Norman, David and Zarifoglu, Emrah},
title = {Deep Reinforcement Learning for Queue-Time Management in Semiconductor Manufacturing},
year = {2023},
publisher = {IEEE Press},
abstract = {Queue-time constraints (QTC) define a limit on the time that a lot can wait between two process steps in its flow. In semiconductor manufacturing, lots that exceed that time limit experience yield loss, need rework, or get scraped. QTCs are difficult to schedule, since a lot needs to wait to be released to the first process step until there is available capacity to process the final step. However, exactly calculating if there is enough capacity is computationally expensive. In this work we propose a deep Reinforcement Learning (RL) method to manage releasing lots into the queue time constraint. We analyze the performance of our RL method and compare it to seven baseline solutions. Our empirical evaluation shows that the RL method outperforms the baselines in five performance metrics including the number of queue-time violations and makespan, while requiring negligible online compute time.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3275–3284},
numpages = {10},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3377929.3389859,
author = {Ha, David},
title = {Neuroevolution for Deep Reinforcement Learning Problems},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3389859},
doi = {10.1145/3377929.3389859},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {404–427},
numpages = {24},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.5555/3306127.3331668,
author = {Katt, Sammie and Oliehoek, Frans A. and Amato, Christopher},
title = {Bayesian Reinforcement Learning in Factored POMDPs},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Model-based Bayesian Reinforcement Learning (BRL) provides a principled solution to dealing with the exploration-exploitation trade-off, but such methods typically assume a fully observable environments. The few Bayesian RL methods that are applicable in partially observable domains, such as the Bayes-Adaptive POMDP (BA-POMDP), scale poorly. To address this issue, we introduce the Factored BA-POMDP model (FBA-POMDP), a framework that is able to learn a compact model of the dynamics by exploiting the underlying structure of a POMDP. The FBA-POMDP framework casts the problem as a planning task, for which we adapt the Monte-Carlo Tree Search planning algorithm and develop a belief tracking method to approximate the joint posterior over the state and model variables. Our empirical results show that this method outperforms a number of BRL baselines and is able to learn efficiently when the factorization is known, as well as learn both the factorization and the model parameters simultaneously.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {7–15},
numpages = {9},
keywords = {bayes networks, monte-carlo tree search, pomdps, bayesian reinforcement learning, monte-chain monte-carlo},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3497776.3517769,
author = {Wang, Huanting and Tang, Zhanyong and Zhang, Cheng and Zhao, Jiaqi and Cummins, Chris and Leather, Hugh and Wang, Zheng},
title = {Automating Reinforcement Learning Architecture Design for Code Optimization},
year = {2022},
isbn = {9781450391832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3497776.3517769},
doi = {10.1145/3497776.3517769},
abstract = {Reinforcement learning (RL) is emerging as a powerful technique for solving complex code optimization tasks with an ample search space. While promising, existing solutions require a painstaking manual process to tune the right task-specific RL architecture, for which compiler developers need to determine the composition of the RL exploration algorithm, its supporting components like state, reward, and transition functions, and the hyperparameters of these models. This paper introduces SuperSonic, a new open-source framework to allow compiler developers to integrate RL into compilers easily, regardless of their RL expertise. SuperSonic supports customizable RL architecture compositions to target a wide range of optimization tasks. A key feature of SuperSonic is the use of deep RL and multi-task learning techniques to develop a meta-optimizer to automatically find and tune the right RL architecture from training benchmarks. The tuned RL can then be deployed to optimize new programs. We demonstrate the efficacy and generality of SuperSonic by applying it to four code optimization problems and comparing it against eight auto-tuning frameworks. Experimental results show that SuperSonic consistently improves hand-tuned methods by delivering better overall performance, accelerating the deployment-stage search by 1.75x on average (up to 100x).},
booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
pages = {129–143},
numpages = {15},
keywords = {Compiler optimization, reinforcement learning, code optimization},
location = {Seoul, South Korea},
series = {CC 2022}
}

@inproceedings{10.5555/2343576.2343631,
author = {Ammar, Haitham B. and Tuyls, Karl and Taylor, Matthew E. and Driessens, Kurt and Weiss, Gerhard},
title = {Reinforcement Learning Transfer via Sparse Coding},
year = {2012},
isbn = {0981738117},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Although reinforcement learning (RL) has been successfully deployed in a variety of tasks, learning speed remains a fundamental problem for applying RL in complex environments. Transfer learning aims to ameliorate this shortcoming by speeding up learning through the adaptation of previously learned behaviors in similar tasks. Transfer techniques often use an inter-task mapping, which determines how a pair of tasks are related. Instead of relying on a hand-coded inter-task mapping, this paper proposes a novel transfer learning method capable of autonomously creating an inter-task mapping by using a novel combination of sparse coding, sparse projection learning and sparse Gaussian processes. We also propose two new transfer algorithms (TrLSPI and TrFQI) based on least squares policy iteration and fitted-Q-iteration. Experiments not only show successful transfer of information between similar tasks, inverted pendulum to cart pole, but also between two very different domains: mountain car to cart pole. This paper empirically shows that the learned inter-task mapping can be successfully used to (1) improve the performance of a learned policy on a fixed number of environmental samples, (2) reduce the learning times needed by the algorithms to converge to a policy on a fixed number of samples, and (3) converge faster to a near-optimal policy given a large number of samples.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {383–390},
numpages = {8},
keywords = {reinforcement learning, transfer learning, sparse Gaussian processes, sparse coding, optimization, inter-task mapping},
location = {Valencia, Spain},
series = {AAMAS '12}
}

@inproceedings{10.1145/1143844.1143845,
author = {Abbeel, Pieter and Quigley, Morgan and Ng, Andrew Y.},
title = {Using Inaccurate Models in Reinforcement Learning},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143845},
doi = {10.1145/1143844.1143845},
abstract = {In the model-based policy search approach to reinforcement learning (RL), policies are found using a model (or "simulator") of the Markov decision process. However, for high-dimensional continuous-state tasks, it can be extremely difficult to build an accurate model, and thus often the algorithm returns a policy that works in simulation but not in real-life. The other extreme, model-free RL, tends to require infeasibly large numbers of real-life trials. In this paper, we present a hybrid algorithm that requires only an approximate model, and only a small number of real-life trials. The key idea is to successively "ground" the policy evaluations using real-life trials, but to rely on the approximate model to suggest local changes. Our theoretical results show that this algorithm achieves near-optimal performance in the real system, even when the model is only approximate. Empirical results also demonstrate that---when given only a crude model and a small number of real-life trials---our algorithm can obtain near-optimal performance in the real system.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {1–8},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}

@article{10.1613/jair.1.12360,
author = {Whittlestone, Jess and Arulkumaran, Kai and Crosby, Matthew},
title = {The Societal Implications of Deep Reinforcement Learning},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12360},
doi = {10.1613/jair.1.12360},
abstract = {Deep Reinforcement Learning (DRL) is an avenue of research in Artificial Intelligence (AI) that has received increasing attention within the research community in recent years, and is beginning to show potential for real-world application. DRL is one of the most promising routes towards developing more autonomous AI systems that interact with and take actions in complex real-world environments, and can more flexibly solve a range of problems for which we may not be able to precisely specify a correct ‘answer’. This could have substantial implications for people’s lives: for example by speeding up automation in various sectors, changing the nature and potential harms of online influence, or introducing new safety risks in physical infrastructure. In this paper, we review recent progress in DRL, discuss how this may introduce novel and pressing issues for society, ethics, and governance, and highlight important avenues for future research to better understand DRL’s societal implications. This article appears in the special track on AI and Society.},
journal = {J. Artif. Int. Res.},
month = {may},
pages = {1003–1030},
numpages = {28}
}

@inproceedings{10.1145/3372278.3390727,
author = {Chen, Shengxin and Chen, Bo-Hao and Chen, Zhaojiong and Wu, YunBing},
title = {Itinerary Planning via Deep Reinforcement Learning},
year = {2020},
isbn = {9781450370875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372278.3390727},
doi = {10.1145/3372278.3390727},
abstract = {Itinerary planning that provides tailor-made tours for each traveler is a fundamental yet inefficient task in route recommendation. In this paper, we propose an automatic route recommendation approach with deep reinforcement learning to solve the itinerary planning problem. We formulate automatic generation of route recommendation as Markov Decision Process (MDP) and then solve it by our variational agent optimized through deep Q-learning algorithm. We train our agent using open data over various cities and show that the agent accomplishes notable improvement in comparison with other state-of-the-art methods.},
booktitle = {Proceedings of the 2020 International Conference on Multimedia Retrieval},
pages = {286–290},
numpages = {5},
keywords = {route recommendation, itinerary planning, reinforcement learning},
location = {Dublin, Ireland},
series = {ICMR '20}
}

@inproceedings{10.1145/301136.301195,
author = {Stone, Peter and Veloso, Manuela},
title = {Team-Partitioned, Opaque-Transition Reinforcement Learning},
year = {1999},
isbn = {158113066X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/301136.301195},
doi = {10.1145/301136.301195},
booktitle = {Proceedings of the Third Annual Conference on Autonomous Agents},
pages = {206–212},
numpages = {7},
location = {Seattle, Washington, USA},
series = {AGENTS '99}
}

@inproceedings{10.5555/3398761.3398813,
author = {Ganapathi Subramanian, Sriram and Poupart, Pascal and Taylor, Matthew E. and Hegde, Nidhi},
title = {Multi Type Mean Field Reinforcement Learning},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Mean field theory provides an effective way of scaling multiagent reinforcement learning algorithms to environments with many agents that can be abstracted by a virtual mean agent. In this paper, we extend mean field multiagent algorithms to multiple types. The types enable the relaxation of a core assumption in mean field games, which is that all agents in the environment are playing almost similar strategies and have the same goal. We conduct experiments on three different testbeds for the field of many agent reinforcement learning, based on the standard MAgents framework. We consider two different kinds of mean field games: a) Games where agents belong to predefined types that are known a priori and b) Games where the type of each agent is unknown and therefore must be learned based on observations. We introduce new algorithms for each type of game and demonstrate their superior performance over state of the art algorithms that assume that all agents belong to the same type and other baseline algorithms in the MAgent framework.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {411–419},
numpages = {9},
keywords = {multiagent systems, reinforcement learning, many-agent learning, mean field methods},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3356464.3357706,
author = {Hong, Weijun and Zhu, Menghui and Liu, Minghuan and Zhang, Weinan and Zhou, Ming and Yu, Yong and Sun, Peng},
title = {Generative Adversarial Exploration for Reinforcement Learning},
year = {2019},
isbn = {9781450376563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356464.3357706},
doi = {10.1145/3356464.3357706},
abstract = {Exploration is crucial for training the optimal reinforcement learning (RL) policy, where the key is to discriminate whether a state visiting is novel. Most previous work focuses on designing heuristic rules or distance metrics to check whether a state is novel without considering such a discrimination process that can be learned. In this paper, we propose a novel method called generative adversarial exploration (GAEX) to encourage exploration in RL via introducing an intrinsic reward output from a generative adversarial network, where the generator provides fake samples of states that help discriminator identify those less frequently visited states. Thus the agent is encouraged to visit those states which the discriminator is less confident to judge as visited. GAEX is easy to implement and of high training efficiency. In our experiments, we apply GAEX into DQN and the DQN-GAEX algorithm achieves convincing performance on challenging exploration problems, including the game Venture, Montezuma's Revenge and Super Mario Bros, without further fine-tuning on complicate learning algorithms. To our knowledge, this is the first work to employ GAN in RL exploration problems.},
booktitle = {Proceedings of the First International Conference on Distributed Artificial Intelligence},
articleno = {6},
numpages = {10},
keywords = {generative adversarial network, exploration, reinforcement learning},
location = {Beijing, China},
series = {DAI '19}
}

@inproceedings{10.5555/2031678.2031728,
author = {Taylor, Matthew E. and Kulis, Brian and Sha, Fei},
title = {Metric Learning for Reinforcement Learning Agents},
year = {2011},
isbn = {0982657161},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {A key component of any reinforcement learning algorithm is the underlying representation used by the agent. While reinforcement learning (RL) agents have typically relied on hand-coded state representations, there has been a growing interest in learning this representation. While inputs to an agent are typically fixed (i.e., state variables represent sensors on a robot), it is desirable to automatically determine the optimal relative scaling of such inputs, as well as to diminish the impact of irrelevant features. This work introduces Holler, a novel distance metric learning algorithm, and combines it with an existing instance-based RL algorithm to achieve precisely these goals. The algorithms' success is highlighted via empirical measurements on a set of six tasks within the mountain car domain.},
booktitle = {The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 2},
pages = {777–784},
numpages = {8},
keywords = {learning state representations, autonomous feature selection, distance metric learning, reinforcement learning},
location = {Taipei, Taiwan},
series = {AAMAS '11}
}

@inproceedings{10.5555/3398761.3398821,
author = {Hasanbeig, Mohammadhosein and Abate, Alessandro and Kroening, Daniel},
title = {Cautious Reinforcement Learning with Logical Constraints},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper presents the concept of an adaptive safe padding that forces Reinforcement Learning (RL) to synthesise optimal control policies while ensuring safety during the learning process. Policies are synthesised to satisfy a goal, expressed as a temporal logic formula, with maximal probability. Enforcing the RL agent to stay safe during learning might limit the exploration, however we show that the proposed architecture is able to automatically handle the trade-off between efficient progress in exploration (towards goal satisfaction) and ensuring safety. Theoretical guarantees are available on the optimality of the synthesised policies and on the convergence of the learning algorithm. Experimental results are provided to showcase the performance of the proposed method.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {483–491},
numpages = {9},
keywords = {safe exploration, model-free, linear temporal logic, reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.5555/3463952.3464080,
author = {Saphal, Rohan and Ravindran, Balaraman and Mudigere, Dheevatsa and Avancha, Sasikant and Kaul, Bharat},
title = {SEERL: Sample Efficient Ensemble Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Ensemble learning is a very prevalent method employed in machine learning. The relative success of ensemble methods is attributed to their ability to tackle a wide range of instances and complex problems that require different low-level approaches. However, ensemble methods are relatively less popular in reinforcement learning owing to the high sample complexity and computational expense involved in obtaining a diverse ensemble. We present a novel training and model selection framework for model-free reinforcement algorithms that use ensembles of policies obtained from a single training run. These policies are diverse in nature and are learned through directed perturbation of the model parameters at regular intervals. We show that learning and selecting an adequately diverse set of policies is required for a good ensemble while extreme diversity can prove detrimental to overall performance. Selection of an adequately diverse set of policies is done through our novel policy selection framework. We evaluate our approach on challenging discrete and continuous control tasks and also discuss various ensembling strategies. Our framework is substantially sample efficient, computationally inexpensive and is seen to outperform state-of-the-art (SOTA) scores in Atari 2600 and Mujoco.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1100–1108},
numpages = {9},
keywords = {combining policies, ensemble methods, deep reinforcement learning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3396851.3402363,
author = {Biswas, Debmalya},
title = {Reinforcement Learning Based HVAC Optimization in Factories},
year = {2020},
isbn = {9781450380096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396851.3402363},
doi = {10.1145/3396851.3402363},
abstract = {Heating, Ventilation and Air Conditioning (HVAC) units are responsible for maintaining the temperature and humidity settings in a building. Studies have shown that HVAC accounts for almost 50\% energy consumption in a building and 10\% of global electricity usage. HVAC optimization thus has the potential to contribute significantly towards our sustainability goals, reducing energy consumption and CO2 emissions. In this work, we explore ways to optimize the HVAC controls in factories. Unfortunately, this is a complex problem as it requires computing an optimal state considering multiple variable factors, e.g. the occupancy, manufacturing schedule, temperature requirements of operating machines, air flow dynamics within the building, external weather conditions, energy savings, etc. We present a Reinforcement Learning (RL) based energy optimization model that has been applied in our factories. We show that RL is a good fit as it is able to learn and adapt to multi-parameterized system dynamics in real-time. It provides around 25\% energy savings on top of the previously used Proportional-Integral-Derivative (PID) controllers.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Future Energy Systems},
pages = {428–433},
numpages = {6},
keywords = {Sustainability, HVAC, Energy Optimization, Reinforcement Learning, Machine Learning},
location = {Virtual Event, Australia},
series = {e-Energy '20}
}

@inproceedings{10.1145/3477495.3531714,
author = {Xin, Xin and Pimentel, Tiago and Karatzoglou, Alexandros and Ren, Pengjie and Christakopoulou, Konstantina and Ren, Zhaochun},
title = {Rethinking Reinforcement Learning for Recommendation: A Prompt Perspective},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531714},
doi = {10.1145/3477495.3531714},
abstract = {Modern recommender systems aim to improve user experience. As reinforcement learning (RL) naturally fits this objective---maximizing an user's reward per session---it has become an emerging topic in recommender systems. Developing RL-based recommendation methods, however, is not trivial due to the offline training challenge. Specifically, the keystone of traditional RL is to train an agent with large amounts of online exploration making lots of 'errors' in the process. In the recommendation setting, though, we cannot afford the price of making 'errors' online. As a result, the agent needs to be trained through offline historical implicit feedback, collected under different recommendation policies; traditional RL algorithms may lead to sub-optimal policies under these offline training settings.Here we propose a new learning paradigm---namely Prompt-Based Reinforcement Learning (PRL)---for the offline training of RL-based recommendation agents. While traditional RL algorithms attempt to map state-action input pairs to their expected rewards (e.g., Q-values), PRL directly infers actions (i.e., recommended items) from state-reward inputs. In short, the agents are trained to predict a recommended item given the prior interactions and an observed reward value---with simple supervised learning. At deployment time, this historical (training) data acts as a knowledge base, while the state-reward pairs are used as a prompt. The agents are thus used to answer the question: Which item should be recommended given the prior interactions \&amp; the prompted reward value? We implement PRL with four notable recommendation models and conduct experiments on two real-world e-commerce datasets. Experimental results demonstrate the superior performance of our proposed methods.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1347–1357},
numpages = {11},
keywords = {reinforcement learning, next item recommendation, recommender systems, session-based recommendation},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3489517.3530617,
author = {Chang, Fu-Chieh and Tseng, Yu-Wei and Yu, Ya-Wen and Lee, Ssu-Rui and Cioba, Alexandru and Tseng, I-Lun and Shiu, Da-shan and Hsu, Jhih-Wei and Wang, Cheng-Yuan and Yang, Chien-Yi and Wang, Ren-Chu and Chang, Yao-Wen and Chen, Tai-Chen and Chen, Tung-Chieh},
title = {Flexible Chip Placement via Reinforcement Learning: Late Breaking Results},
year = {2022},
isbn = {9781450391429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489517.3530617},
doi = {10.1145/3489517.3530617},
abstract = {Recently, successful applications of reinforcement learning to chip placement have emerged. Pretrained models are necessary to improve efficiency and effectiveness. Currently, the weights of objective metrics (e.g., wirelength, congestion, and timing) are fixed during pretraining. However, fixed-weighed models cannot generate the diversity of placements required for engineers to accommodate changing requirements as they arise. This paper proposes flexible multiple-objective reinforcement learning (MORL) to support objective functions with inference-time variable weights using just a single pretrained model. Our macro placement results show that MORL can generate the Pareto frontier of multiple objectives effectively.},
booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference},
pages = {1392–1393},
numpages = {2},
location = {San Francisco, California},
series = {DAC '22}
}

@inproceedings{10.1145/1390156.1390211,
author = {Jong, Nicholas K. and Stone, Peter},
title = {Hierarchical Model-Based Reinforcement Learning: R-Max + MAXQ},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390211},
doi = {10.1145/1390156.1390211},
abstract = {Hierarchical decomposition promises to help scale reinforcement learning algorithms naturally to real-world problems by exploiting their underlying structure. Model-based algorithms, which provided the first finite-time convergence guarantees for reinforcement learning, may also play an important role in coping with the relative scarcity of data in large environments. In this paper, we introduce an algorithm that fully integrates modern hierarchical and model-learning methods in the standard reinforcement learning setting. Our algorithm, R-maxq, inherits the efficient model-based exploration of the R-max algorithm and the opportunities for abstraction provided by the MAXQ framework. We analyze the sample complexity of our algorithm, and our experiments in a standard simulation environment illustrate the advantages of combining hierarchies and models.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {432–439},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/3383455.3422540,
author = {Yang, Hongyang and Liu, Xiao-Yang and Zhong, Shan and Walid, Anwar},
title = {Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy},
year = {2021},
isbn = {9781450375849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383455.3422540},
doi = {10.1145/3383455.3422540},
abstract = {Stock trading strategies play a critical role in investment. However, it is challenging to design a profitable strategy in a complex and dynamic stock market. In this paper, we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations. In order to avoid the large memory consumption in training networks with continuous action space, we employ a load-on-demand technique for processing very large data. We test our algorithms on the 30 Dow Jones stocks that have adequate liquidity. The performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy. The proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio.},
booktitle = {Proceedings of the First ACM International Conference on AI in Finance},
articleno = {31},
numpages = {8},
keywords = {actor-critic framework, automated stock trading, deep reinforcement learning, ensemble strategy, markov decision process},
location = {New York, New York},
series = {ICAIF '20}
}

@inproceedings{10.1145/3522749.3523066,
author = {Liu, Hang and Hyodo, Akihiko and Suzuki, Shintaro},
title = {Reinforcement Learning Based Indoor, Collaborative Autonomous Mobility　},
year = {2022},
isbn = {9781450385916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522749.3523066},
doi = {10.1145/3522749.3523066},
abstract = {By connecting building operations, building automation can be realized using mobile devices and AI processor. Aiming for improving living condition and reducing workloads, we designed a cyber-physical system to operate multiple infrastructure efficiently, which enables an indoor autonomous mobility. We customize a navigation map owing the 3D space information of the building, then an optimal driving route is calculated by an intelligent path planner to calculate. Two technical novelties are proposed: (1) intra-building sensors connecting to the central system are deployed to monitor certain vehicle's surroundings change at which spatial heights. The central with building's 3D model draws several customized maps at those heights. (2) based on the customized map, we use two-stage training scheme for path planner, which first and second stages utilize DQN and NEAT, respectively. It uses a refined network model for better, faster updating its structure and parameter. The proposed scheme is proven to reduce driving time consumption more than $20\%$ and accelerates training period more than $30\%$ compared to conventional algorithms.},
booktitle = {Proceedings of the 6th International Conference on Control Engineering and Artificial Intelligence},
pages = {53–56},
numpages = {4},
keywords = {genetic algorithm, RL, smart building, autonomous mobility},
location = {Virtual Event, Japan},
series = {CCEAI '22}
}

@inproceedings{10.1145/3514221.3526128,
author = {Wu, Wentao and Wang, Chi and Siddiqui, Tarique and Wang, Junxiong and Narasayya, Vivek and Chaudhuri, Surajit and Bernstein, Philip A.},
title = {Budget-Aware Index Tuning with Reinforcement Learning},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3526128},
doi = {10.1145/3514221.3526128},
abstract = {Index tuning aims to find the optimal index configuration for an input workload. It is a resource-intensive task since it requires making multiple expensive "what-if" calls to the query optimizer to estimate the cost of a query given an index configuration without actually building the indexes. In this paper, we study the problem of budget-aware index tuning where the number of what-if calls allowed when searching for the optimal configuration during tuning is constrained. This problem is challenging as it requires addressing the trade-off between investing what-if calls on exploring new configurations versus exploiting a known promising configuration. We formulate budget-aware index tuning as a Markov decision process, and propose a solution based on Monte Carlo tree search, a classic reinforcement learning technology. Experimental evaluation on both standard industry benchmarks and real workloads shows that our solution can significantly outperform alternative budget-aware solutions in terms of the quality of the index configuration.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {1528–1541},
numpages = {14},
keywords = {reinforcement learning, budget allocation, index tuning},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.5555/1402821.1402873,
author = {Zhang, Chongjie and Abdallah, Sherief and Lesser, Victor},
title = {Efficient Multi-Agent Reinforcement Learning through Automated Supervision},
year = {2008},
isbn = {9780981738123},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-Agent Reinforcement Learning (MARL) algorithms suffer from slow convergence and even divergence, especially in large-scale systems. In this work, we develop a supervision framework to speed up the convergence of MARL algorithms in a network of agents. The framework defines an organizational structure for automated supervision and a communication protocol for exchanging information between lower-level agents and higher-level supervising agents. The abstracted states of lower-level agents travel upwards so that higher-level supervising agents generate a broader view of the state of the network. This broader view is used in creating supervisory information which is passed down the hierarchy. We present a generic extension to MARL algorithms that integrates supervisory information into the learning process, guiding agents' exploration of their state-action space.},
booktitle = {Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3},
pages = {1365–1370},
numpages = {6},
keywords = {heuristics, supervision, reinforcement learning, multiagent systems},
location = {Estoril, Portugal},
series = {AAMAS '08}
}

@inproceedings{10.1145/3566097.3567894,
author = {Zhou, Guanglei and Anderson, Jason H.},
title = {Area-Driven FPGA Logic Synthesis Using Reinforcement Learning},
year = {2023},
isbn = {9781450397834},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3566097.3567894},
doi = {10.1145/3566097.3567894},
abstract = {Logic synthesis involves a rich set of optimization algorithms applied in a specific sequence to a circuit netlist prior to technology mapping. A conventional approach is to apply a fixed "recipe" of such algorithms deemed to work well for a wide range of different circuits. We apply reinforcement learning (RL) to determine a unique recipe of algorithms for each circuit. Feature-importance analysis is conducted using a random-forest classifier to prune the set of features visible to the RL agent. We demonstrate conclusive learning by the RL agent and show significant FPGA area reductions vs. the conventional approach (resyn2). In addition to circuit-by-circuit training and inference, we also train an RL agent on multiple circuits, and then apply the agent to optimize: 1) the same set of circuits on which it was trained, and 2) an alternative set of "unseen" circuits. In both scenarios, we observe that the RL agent produces higher-quality implementations than the conventional approach. This shows that the RL agent is able to generalize, and perform beneficial logic synthesis optimizations across a variety of circuits.},
booktitle = {Proceedings of the 28th Asia and South Pacific Design Automation Conference},
pages = {159–165},
numpages = {7},
keywords = {circuit optimization, reinforcement learning, logic synthesis},
location = {Tokyo, Japan},
series = {ASPDAC '23}
}

@inproceedings{10.1145/3511808.3557078,
author = {Yun, Won Joon and Mohaisen, David and Jung, Soyi and Kim, Jong-Kook and Kim, Joongheon},
title = {Hierarchical Reinforcement Learning Using Gaussian Random Trajectory Generation in Autonomous Furniture Assembly},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557078},
doi = {10.1145/3511808.3557078},
abstract = {In this paper, we propose a Gaussian Random Trajectory guided Hierarchical Reinforcement Learning (GRT-HL) method for autonomous furniture assembly. The furniture assembly problem is formulated as a comprehensive human-like long-horizon manipulation task that requires a long-term planning and a sophisticated control. Our proposed model, GRT-HL, draws inspirations from the semi-supervised adversarial autoencoders, and learns latent representations of the position trajectories of the end-effector. The high-level policy generates an optimal trajectory for furniture assembly, considering the structural limitations of the robotic agents. Given the trajectory drawn from the high-level policy, the low-level policy makes a plan and controls the end-effector. We first evaluate the performance of GRT-HL compared to the state-of-the-art reinforcement learning methods in furniture assembly tasks. We demonstrate that GRT-HL successfully solves the long-horizon problem with extremely sparse rewards by generating the trajectory for planning.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {3624–3633},
numpages = {10},
keywords = {hierarchical reinforcement learning, assembly control, robotics, reinforcement learning},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.5555/1838206.1838217,
author = {Amato, Christopher and Shani, Guy},
title = {High-Level Reinforcement Learning in Strategy Games},
year = {2010},
isbn = {9780982657119},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Video games provide a rich testbed for artificial intelligence methods. In particular, creating automated opponents that perform well in strategy games is a difficult task. For instance, human players rapidly discover and exploit the weaknesses of hard coded strategies. To build better strategies, we suggest a reinforcement learning approach for learning a policy that switches between high-level strategies. These strategies are chosen based on different game situations and a fixed opponent strategy. Our learning agents are able to rapidly adapt to fixed opponents and improve deficiencies in the hard coded strategies, as the results demonstrate.},
booktitle = {Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1 - Volume 1},
pages = {75–82},
numpages = {8},
keywords = {video games, virtual agents, reinforcement learning},
location = {Toronto, Canada},
series = {AAMAS '10}
}

@inproceedings{10.1145/3474085.3475604,
author = {Nie, Weizhi and Li, Jiesi and Xu, Ning and Liu, An-An and Li, Xuanya and Zhang, Yongdong},
title = {Triangle-Reward Reinforcement Learning: A Visual-Linguistic Semantic Alignment for Image Captioning},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475604},
doi = {10.1145/3474085.3475604},
abstract = {Image captioning aims to generate a sentence consisting of sequential linguistic words, to describe visual units (i.e., objects, relationships, and attributes) in a given image. Most of existing methods rely on the prevalent supervised learning with cross-entropy (XE) function to transfer visual units into a sequence of linguistic words. However, we argue that the XE objective is not sensitive to visual-linguistic alignment, which cannot discriminately penalize the semantic inconsistency and shrink the context gap. To solve these problems, we propose the Triangle-Reward Reinforcement Learning (TRRL) method. TRRL uses the scene graph (G)---objects as nodes and relationships as edges---to represent images, generated sentences, and ground truth sentences individually, and mutually align them during the training process. Specifically, TRRL formulates the image captioning into cooperative agents, where the first agent aims to extract visual scene graph (Gimg) from image (I) and the second agent translates this graph into sentence (S). To discriminately penalize the visual-linguistic inconsistency, TRRL proposes the novel triangle-reward function: 1) the generated sentence and its corresponding ground truth are decomposed into the linguistic scene graph (Gsen) and ground-truth scene graph (Ggt), respectively; 2) Gimg, Gsen, and Ggt are paired to calculate the semantic similarity scores which are proportionally assigned to reward each agent. Meanwhile, to make the training objective sensitive to context changes, we propose the node-level and triplet-level scoring methods to jointly measure the visual-linguistic graph correlations. Extensive experiments on the MSCOCO dataset demonstrate the superiority of TRRL. Additional ablation studies further validate its effectiveness.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {4510–4518},
numpages = {9},
keywords = {scene graph, visual-linguistic alignment, reinforcement learning, image captioning},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/1014052.1016912,
author = {Abe, Naoki and Verma, Naval and Apte, Chid and Schroko, Robert},
title = {Cross Channel Optimized Marketing by Reinforcement Learning},
year = {2004},
isbn = {1581138881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1014052.1016912},
doi = {10.1145/1014052.1016912},
abstract = {The issues of cross channel integration and customer life time value modeling are two of the most important topics surrounding customer relationship management (CRM) today. In the present paper, we describe and evaluate a novel solution that treats these two important issues in a unified framework of Markov Decision Processes (MDP). In particular, we report on the results of a joint project between IBM Research and Saks Fifth Avenue to investigate the applicability of this technology to real world problems. The business problem we use as a testbed for our evaluation is that of optimizing direct mail campaign mailings for maximization of profits in the store channel. We identify a problem common to cross-channel CRM, which we call the Cross-Channel Challenge, due to the lack of explicit linking between the marketing actions taken in one channel and the customer responses obtained in another. We provide a solution for this problem based on old and new techniques in reinforcement learning. Our in-laboratory experimental evaluation using actual customer interaction data show that as much as 7 to 8 per cent increase in the store profits can be expected, by employing a mailing policy automatically generated by our methodology. These results confirm that our approach is valid in dealing with the cross channel CRM scenarios in the real world.},
booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {767–772},
numpages = {6},
keywords = {cost sensitive learning, reinforcement learning, CRM, customer life time value, targeted marketing},
location = {Seattle, WA, USA},
series = {KDD '04}
}

@inproceedings{10.1145/2001858.2001957,
author = {Niekum, Scott and Spector, Lee and Barto, Andrew},
title = {Evolution of Reward Functions for Reinforcement Learning},
year = {2011},
isbn = {9781450306904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2001858.2001957},
doi = {10.1145/2001858.2001957},
abstract = {The reward functions that drive reinforcement learning systems are generally derived directly from the descriptions of the problems that the systems are being used to solve. In some problem domains, however, alternative reward functions may allow systems to learn more quickly or more effectively. Here we describe work on the use of genetic programming to find novel reward functions that improve learning system performance. We briefly present the core concepts of our approach, our motivations in developing it, and reasons to believe that the approach has promise for the production of highly successful adaptive technologies. Experimental results are presented and analyzed in our full report [3].},
booktitle = {Proceedings of the 13th Annual Conference Companion on Genetic and Evolutionary Computation},
pages = {177–178},
numpages = {2},
keywords = {push, pushgp, genetic programming, reinforcement learning, hungry-thirsty problem},
location = {Dublin, Ireland},
series = {GECCO '11}
}

@inproceedings{10.5555/1687878.1687892,
author = {Branavan, S. R. K. and Chen, Harr and Zettlemoyer, Luke S. and Barzilay, Regina},
title = {Reinforcement Learning for Mapping Instructions to Actions},
year = {2009},
isbn = {9781932432459},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains --- Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.},
booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1},
pages = {82–90},
numpages = {9},
location = {Suntec, Singapore},
series = {ACL '09}
}

@inproceedings{10.5555/3398761.3399125,
author = {Padakandla, Sindhu},
title = {Reinforcement Learning Algorithms for Autonomous Adaptive Agents},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Intelligent agents are being designed to automate many tasks - for e.g., traffic signal control, vehicle driving, inventory control and are also being used in improving lives of people - like in healthcare, agriculture, wildlife protection etc. The widespread deployment of intelligent agents requires that we minimize the bottlenecks which affect their performance and utility. Motivated by this challenge, my thesis proposes new algorithms and methods which helps the agent in efficiently operating in the real-world and also during interaction with humans. My work has shown significant improvements in the performance of deployed agents, when operating in real world.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2201–2203},
numpages = {3},
keywords = {continual learning, data efficiency, deep reinforcement learning, non-stationary environments, reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3511808.3557474,
author = {Zha, Daochen and Lai, Kwei-Herng and Tan, Qiaoyu and Ding, Sirui and Zou, Na and Hu, Xia Ben},
title = {Towards Automated Imbalanced Learning with Deep Hierarchical Reinforcement Learning},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557474},
doi = {10.1145/3511808.3557474},
abstract = {Imbalanced learning is a fundamental challenge in data mining, where there is a disproportionate ratio of training samples in each class. Over-sampling is an effective technique to tackle imbalanced learning through generating synthetic samples for the minority class. While numerous over-sampling algorithms have been proposed, they heavily rely on heuristics, which could be sub-optimal since we may need different sampling strategies for different datasets and base classifiers, and they cannot directly optimize the performance metric. Motivated by this, we investigate developing a learning-based over-sampling algorithm to optimize the classification performance, which is a challenging task because of the huge and hierarchical decision space. At the high level, we need to decide how many synthetic samples to generate. At the low level, we need to determine where the synthetic samples should be located, which depends on the high-level decision since the optimal locations of the samples may differ for different numbers of samples. To address the challenges, we propose AutoSMOTE, an automated over-sampling algorithm that can jointly optimize different levels of decisions. Motivated by the success of SMOTE and its extensions, we formulate the generation process as a Markov decision process (MDP) consisting of three levels of policies to generate synthetic samples within the SMOTE search space. Then we leverage deep hierarchical reinforcement learning to optimize the performance metric on the validation data. Extensive experiments on six real-world datasets demonstrate that AutoSMOTE significantly outperforms the state-of-the-art resampling algorithms. The code is at https://github.com/daochenzha/autosmote},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {2476–2485},
numpages = {10},
keywords = {automated machine learning, classification, reinforcement learning, imbalanced learning},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/2598394.2605681,
author = {Buzdalova, Arina and Kononov, Vladislav and Buzdalov, Maxim},
title = {Selecting Evolutionary Operators Using Reinforcement Learning: Initial Explorations},
year = {2014},
isbn = {9781450328814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598394.2605681},
doi = {10.1145/2598394.2605681},
abstract = {In evolutionary optimization, it is important to use efficient evolutionary operators, such as mutation and crossover. But it is often difficult to decide, which operator should be used when solving a specific optimization problem. So an automatic approach is needed. We propose an adaptive method of selecting evolutionary operators, which takes a set of possible operators as input and learns what operators are efficient for the considered problem. One evolutionary algorithm run should be enough for both learning and obtaining suitable performance. The proposed EA+RL(O) method is based on reinforcement learning. We test it by solving H-IFF and Travelling Salesman optimization problems. The obtained results show that the proposed method significantly outperforms random selection, since it manages to select efficient evolutionary operators and ignore inefficient ones.},
booktitle = {Proceedings of the Companion Publication of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1033–1036},
numpages = {4},
keywords = {parameter control, evolutionary algorithms},
location = {Vancouver, BC, Canada},
series = {GECCO Comp '14}
}

@inproceedings{10.5555/3091125.3091376,
author = {Liebman, Elad and Zavesky, Eric and Stone, Peter},
title = {Autonomous Model Management via Reinforcement Learning: Extended Abstract},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Concept drift - a change, either sudden or gradual, in the underlying properties of data - is one of the most prevalent challenges to maintaining high-performing learned models over time in autonomous systems. In the face of concept drift, one can hope that the old model is sufficiently representative despite concept drift. Alternatively, one can discard the old data and retrain a new model with (often limited) new data, or use transfer learning to combine the old data with the new to create an updated model. Which of these three options is chosen affects not only near-term decisions, but also future modeling actions. In this abstract, we model response to concept drift as a sequential decision making problem and formally frame it as a Markov Decision Process. Our reinforcement learning approach to the problem shows promising results balancing cost with performance in maintaining model accuracy.},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {1601–1603},
numpages = {3},
keywords = {reinforcement learning, continual learning, adaptive control},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@inproceedings{10.1145/3449301.3449311,
author = {Liu, Botong},
title = {Implementing Game Strategies Based on Reinforcement Learning},
year = {2021},
isbn = {9781450388597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449301.3449311},
doi = {10.1145/3449301.3449311},
abstract = {Artificial intelligence (AI) technology such as reinforcement learning is increasingly used in playing game in recent years. A deep reinforcement learning model was used to play the game Flappy Bird. This paper aimed to let the computer play a simple game and get the corresponding data for AI learning. Game image was sequentially scaled, grayed, and adjusted for brightness. Before the current frame entered a state, the multi-dimensional image data of several frames of image superposition and combination was processed. Deep Q Network algorithm realized the best action prediction of the game execution in a specific game state, and successfully converted a game decision problem into the classification and recognition problem of instant multi-dimensional images and solved it with a convolutional neural network. After analysis, computer players controlled by deep neural networks had better results than human players. This experiment was a model combined between a deep neural network model and reinforcement learning, and could be applied in other games.},
booktitle = {Proceedings of the 6th International Conference on Robotics and Artificial Intelligence},
pages = {53–56},
numpages = {4},
keywords = {Deep Q Network algorithm, Flappy bird, Multi-dimensional image, Reinforcement learning},
location = {Singapore, Singapore},
series = {ICRAI '20}
}

@inproceedings{10.1145/3600211.3604669,
author = {Kasirzadeh, Atoosa and Evans, Charles},
title = {User Tampering in Reinforcement Learning Recommender Systems},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604669},
doi = {10.1145/3600211.3604669},
abstract = {In this paper, we introduce new formal methods and provide empirical evidence to highlight a unique safety concern prevalent in reinforcement learning (RL)-based recommendation algorithms – ’user tampering.’ User tampering is a situation where an RL-based recommender system may manipulate a media user’s opinions through its suggestions as part of a policy to maximize long-term user engagement. We use formal techniques from causal modeling to critically analyze prevailing solutions proposed in the literature for implementing scalable RL-based recommendation systems, and we observe that these methods do not adequately prevent user tampering. Moreover, we evaluate existing mitigation strategies for reward tampering issues, and show that these methods are insufficient in addressing the distinct phenomenon of user tampering within the context of recommendations. We further reinforce our findings with a simulation study of an RL-based recommendation system focused on the dissemination of political content. Our study shows that a Q-learning algorithm consistently learns to exploit its opportunities to polarize simulated users with its early recommendations in order to have more consistent success with subsequent recommendations that align with this induced polarization. Our findings emphasize the necessity for developing safer RL-based recommendation systems and suggest that achieving such safety would require a fundamental shift in the design away from the approaches we have seen in the recent literature.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {58–69},
numpages = {12},
keywords = {AI Ethics, Recommendation Systems, Reinforcement Learning, Recommender Systems, Value Alignment, AI Safety},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@article{10.1145/1778765.1778859,
author = {Lee, Seong Jae and Popovi\'{c}, Zoran},
title = {Learning Behavior Styles with Inverse Reinforcement Learning},
year = {2010},
issue_date = {July 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/1778765.1778859},
doi = {10.1145/1778765.1778859},
abstract = {We present a method for inferring the behavior styles of character controllers from a small set of examples. We show that a rich set of behavior variations can be captured by determining the appropriate reward function in the reinforcement learning framework, and show that the discovered reward function can be applied to different environments and scenarios. We also introduce a new algorithm to recover the unknown reward function that improves over the original apprenticeship learning algorithm. We show that the reward function representing a behavior style can be applied to a variety of different tasks, while still preserving the key features of the style present in the given examples. We describe an adaptive process where an author can, with just a few additional examples, refine the behavior so that it has better generalization properties.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {122},
numpages = {7},
keywords = {apprenticeship learning, data driven animation, human animation, optimal control, inverse reinforcement learning}
}

