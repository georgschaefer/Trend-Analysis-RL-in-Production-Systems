"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Graph Convolutional Reinforcement Learning for Advanced Energy-Aware Process Planning","Q. Xiao; B. Niu; B. Xue; L. Hu","College of Management, Shenzhen University, Shenzhen, China; College of Management and the Institute of Big Data Intelligent Management and Decision, Shenzhen University, Shenzhen, China; Evolutionary Computation Research Group, Victoria University of Wellington, Wellington, New Zealand; Department of Mechanical Engineering, School of Engineering, Zhejiang University City College, Hangzhou, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","14 Apr 2023","2023","53","5","2802","2814","With the growing demands on green short life-cycle products, advanced energy-aware process planning (AEPP) becomes critical. A major limitation of the existing methods is the poor resistance to the perturbations encountered in advanced machining systems. Therefore, a graph convolutional reinforcement learning (GCRL) method is proposed to overcome such limitations. In this method, a graph convolutional policy network is trained to rapidly adapt the learned commonalities to specific tasks. Unlike algorithms that fix decision variables before optimization, this method employs graph generation to represent AEPP while taking into consideration the flexibilities of operations, machines, and cutting tools. The problem is reformulated as a novel Markov decision process (MDP) to describe the dynamic generation procedure of process plans. A graph convolutional network (GCN) is concurrently used to perform graph embedding to compress the topology of input graphs. Additionally, reinforcement learning (RL) is used to achieve robust and intuitive learning for process planning. To improve the adaption performance of the proposed GCRL, a two-phase multitask training strategy is adopted. Learning efficiency is improved because agents can incorporate both intertask similarities and task-specific rules. A comprehensive case study, including energy characteristics and algorithm performance analyses, is also performed to validate the developed method.","2168-2232","","10.1109/TSMC.2022.3219407","Natural Science Foundation of China(grant numbers:71971143); Major Research Plan for National Natural Science Foundation of China(grant numbers:91846301); Natural Science Foundation of Guangdong Province(grant numbers:2020A1515110541); China Postdoctoral Science Foundation(grant numbers:2020M682890); Key Research Foundation of Higher Education of Guangdong Provincial Education Bureau(grant numbers:2019KZDXM030); Guangdong Province Innovation Team “Intelligent Management and Interdisciplinary Innovation”(grant numbers:2021WCXTD002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9953065","Advanced process planning;graph generation;reinforcement learning (RL);sustainable manufacturing","Process planning;Task analysis;Manufacturing;Reinforcement learning;Machining;Metaheuristics;Machine tools","convolutional neural nets;cutting tools;graph theory;learning (artificial intelligence);Markov processes;process planning;reinforcement learning","advanced energy-aware process planning;advanced machining systems;AEPP;graph convolutional network;graph convolutional policy network;graph convolutional reinforcement learning method;graph generation;green short life-cycle products;input graphs;intuitive learning;learned commonalities;learning efficiency;novel Markov decision process;process plans;robust learning","","3","","33","IEEE","16 Nov 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning for Robust Parameterized Locomotion Control of Bipedal Robots","Z. Li; X. Cheng; X. B. Peng; P. Abbeel; S. Levine; G. Berseth; K. Sreenath","University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","2811","2817","Developing robust walking controllers for bipedal robots is a challenging endeavor. Traditional model-based locomotion controllers require simplifying assumptions and careful modelling; any small errors can result in unstable control. To address these challenges for bipedal locomotion, we present a model-free reinforcement learning framework for training robust locomotion policies in simulation, which can then be transferred to a real bipedal Cassie robot. To facilitate sim-to-real transfer, domain randomization is used to encourage the policies to learn behaviors that are robust across variations in system dynamics. The learned policies enable Cassie to perform a set of diverse and dynamic behaviors, while also being more robust than traditional controllers and prior learning-based methods that use residual control. We demonstrate this on versatile walking behaviors such as tracking a target walking velocity, walking height, and turning yaw. (Video1)","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560769","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560769","","Legged locomotion;Training;Learning systems;Target tracking;Automation;System dynamics;Conferences","collision avoidance;humanoid robots;learning (artificial intelligence);legged locomotion;mobile robots;motion control;robot dynamics","careful modelling;unstable control;bipedal locomotion;model-free reinforcement learning framework;robust locomotion policies;bipedal Cassie robot;sim-to-real transfer;learned policies;diverse behaviors;dynamic behaviors;traditional controllers;prior learning-based methods;residual control;versatile walking;target walking velocity;robust parameterized locomotion control;bipedal robots;robust walking controllers;challenging endeavor;traditional model-based locomotion controllers","","51","","39","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Robot reinforcement learning using EEG-based reward signals","I. Iturrate; L. Montesano; J. Minguez","Instituto de Investigación en ingeniería de Aragón (I3A) and Departmento de Informática e Ingeniería de Sistemas, Universidad de Zaragoza, Spain; Instituto de Investigación en ingeniería de Aragón (I3A) and Departmento de Informática e Ingeniería de Sistemas, Universidad de Zaragoza, Spain; Instituto de Investigación en ingeniería de Aragón (I3A) and Departmento de Informática e Ingeniería de Sistemas, Universidad de Zaragoza, Spain","2010 IEEE International Conference on Robotics and Automation","15 Jul 2010","2010","","","4822","4829","Reinforcement learning algorithms have been successfully applied in robotics to learn how to solve tasks based on reward signals obtained during task execution. These reward signals are usually modeled by the programmer or provided by supervision. However, there are situations in which this reward is hard to encode, and so would require a supervised approach of reinforcement learning, where a user directly types the reward on each trial. This paper proposes to use brain activity recorded by an EEG-based BCI system as reward signals. The idea is to obtain the reward from the activity generated while observing the robot solving the task. This process does not require an explicit model of the reward signal. Moreover, it is possible to capture subjective aspects which are specific to each user. To achieve this, we designed a new protocol to use brain activity related to the correct or wrong execution of the task. We showed that it is possible to detect and classify different levels of error in single trials. We also showed that it is possible to apply reinforcement learning algorithms to learn new similar tasks using the rewards obtained from brain activity.","1050-4729","978-1-4244-5038-1","10.1109/ROBOT.2010.5509734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5509734","","Learning;Enterprise resource planning;Robotics and automation;Humans;Electroencephalography;Mobile robots;Brain modeling;Programming profession;Signal processing;Orbital robotics","electroencephalography;learning (artificial intelligence);robots","robot reinforcement learning;EEG-based reward signals;task execution;brain activity;EEG-based BCI system","","50","2","26","IEEE","15 Jul 2010","","","IEEE","IEEE Conferences"
"Vision-based reinforcement learning for purposive behavior acquisition","M. Asada; S. Noda; S. Tawaratsumida; K. Hosoda","Department of Mechanical Engineering for Computer-Controlled Machinery, Osaka University, Suita, Osaka, Japan; Department of Mechanical Engineering for Computer-Controlled Machinery, Osaka University, Suita, Osaka, Japan; Department of Mechanical Engineering for Computer-Controlled Machinery, Osaka University, Suita, Osaka, Japan; Department of Mechanical Engineering for Computer-Controlled Machinery, Osaka University, Suita, Osaka, Japan","Proceedings of 1995 IEEE International Conference on Robotics and Automation","6 Aug 2002","1995","1","","146","153 vol.1","This paper presents a method of vision-based reinforcement learning by which a robot learns to shoot a ball into a goal, and discusses several issues in applying the reinforcement learning method to a real robot with vision sensor. First, a ""state-action deviation"" problem is found as a form of perceptual aliasing in constructing the state and action spaces that reflect the outputs from physical sensors and actuators, respectively. To cope with this, an action set is constructed in such a way that one action consists of a series of the same action primitive which is successively executed until the current state changes. Next, to speed up the learning time, a mechanism of learning form easy missions (or LEM) which is a similar technique to ""shaping"" in animal learning is implemented. LEM reduces the learning time from the exponential order in the size of the state space to about the linear order in the size of the state space. The results of computer simulations and real robot experiments are given.","1050-4729","0-7803-1965-6","10.1109/ROBOT.1995.525277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=525277","","Robot sensing systems;Orbital robotics;Robotics and automation;Machine learning;State-space methods;Robot vision systems;Actuators;Robot control;Mobile robots;Computer vision","learning (artificial intelligence);robot vision;robot programming;digital simulation","vision-based reinforcement learning;purposive behavior acquisition;vision sensor;state-action deviation problem;perceptual aliasing;action set;action primitive;learning form easy missions","","38","","13","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Decentralized Structural-RNN for Robot Crowd Navigation with Deep Reinforcement Learning","S. Liu; P. Chang; W. Liang; N. Chakraborty; K. Driggs-Campbell","Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign; Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign; Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign; Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign; Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","3517","3524","Safe and efficient navigation through human crowds is an essential capability for mobile robots. Previous work on robot crowd navigation assumes that the dynamics of all agents are known and well-defined. In addition, the performance of previous methods deteriorates in partially observable environments and environments with dense crowds. To tackle these problems, we propose decentralized structural-Recurrent Neural Network (DS-RNN), a novel network that reasons about spatial and temporal relationships for robot decision making in crowd navigation. We train our network with model-free deep reinforcement learning without any expert supervision. We demonstrate that our model outperforms previous methods in challenging crowd navigation scenarios. We successfully transfer the policy learned in the simulator to a real-world TurtleBot 2i.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561595","","Automation;Navigation;Conferences;Neural networks;Decision making;Reinforcement learning;Mobile robots","decision making;learning (artificial intelligence);mobile robots;multi-robot systems;path planning;recurrent neural nets;traffic engineering computing","efficient navigation;human crowds;mobile robots;robot crowd navigation;previous methods deteriorates;partially observable environments;dense crowds;structural-Recurrent Neural Network;robot decision;model-free deep reinforcement learning;crowd navigation scenarios;decentralized structural-RNN;safe navigation","","29","","49","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Bridging Hamilton-Jacobi Safety Analysis and Reinforcement Learning","J. F. Fisac; N. F. Lugovoy; V. Rubies-Royo; S. Ghosh; C. J. Tomlin","Department of Electrical Engineering and Computer Sciences, University of California, Berkeley; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","8550","8556","Safety analysis is a necessary component in the design and deployment of autonomous robotic systems. Techniques from robust optimal control theory, such as Hamilton-Jacobi reachability analysis, allow a rigorous formalization of safety as guaranteed constraint satisfaction. Unfortunately, the computational complexity of these tools for general dynamical systems scales poorly with state dimension, making existing tools impractical beyond small problems. Modern reinforcement learning methods have shown promising ability to find approximate yet proficient solutions to optimal control problems in complex and high-dimensional systems, however their application has in practice been restricted to problems with an additive payoff over time, unsuitable for reasoning about safety. In recent work, we introduced a time-discounted modification of the problem of maximizing the minimum payoff over time, central to safety analysis, through a modified dynamic programming equation that induces a contraction mapping. Here, we show how a similar contraction mapping can render reinforcement learning techniques amenable to quantitative safety analysis as tools to approximate the safe set and optimal safety policy. This opens a new avenue of research connecting control-theoretic safety analysis and the reinforcement learning domain. We validate the correctness of our formulation by comparing safety results computed through Q-learning to analytic and numerical solutions, and demonstrate its scalability by learning safe sets and control policies for simulated systems of up to 18 state dimensions using value learning and policy gradient techniques.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794107","","Safety;Automation;Reinforcement learning;Robots;Optimal control;Jacobian matrices;Reachability analysis","approximation theory;control engineering computing;dynamic programming;gradient methods;learning (artificial intelligence);mobile robots;optimal control;partial differential equations","dynamic programming equation;contraction mapping;Hamilton-Jacobi safety analysis;control-theoretic safety analysis;optimal safety policy;quantitative safety analysis;reinforcement learning techniques;time-discounted modification;optimal control problems;robust optimal control theory;autonomous robotic systems;policy gradient techniques;value learning","","23","1","32","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"TrojDRL: Evaluation of Backdoor Attacks on Deep Reinforcement Learning","P. Kiourti; K. Wardega; S. Jha; W. Li","ECE Boston University, Boston, USA; ECE Boston University, Boston, USA; CSL SRI International, Menlo Park, USA; ECE Boston University, Boston, USA","2020 57th ACM/IEEE Design Automation Conference (DAC)","9 Oct 2020","2020","","","1","6","We present TrojDRL, a tool for exploring and evaluating backdoor attacks on deep reinforcement learning agents. TrojDRL exploits the sequential nature of deep reinforcement learning (DRL) and considers different gradations of threat models. We show that untargeted attacks on state-of-the-art actor-critic algorithms can circumvent existing defenses built on the assumption of backdoors being targeted. We evaluated TrojDRL on a broad set of DRL benchmarks and showed that the attacks require only poisoning as little as 0.025% of the training data. Compared with existing works of backdoor attacks on classification models, TrojDRL provides a first step towards understanding the vulnerability of DRL agents.","0738-100X","978-1-7281-1085-1","10.1109/DAC18072.2020.9218663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9218663","I.2.6.g Machine learning;C.1.3.i Neural nets;K.4.4.f Security;G.4.g Reliability and robustness","Design automation;Training data;Reinforcement learning;Benchmark testing;Reliability engineering;Robustness;Classification algorithms","learning (artificial intelligence);multi-agent systems;neural nets;security of data","TrojDRL;backdoor attacks;deep reinforcement learning agents;actor-critic algorithms;DRL agents","","17","","24","IEEE","9 Oct 2020","","","IEEE","IEEE Conferences"
"DWA-RL: Dynamically Feasible Deep Reinforcement Learning Policy for Robot Navigation among Mobile Obstacles","U. Patel; N. K. S. Kumar; A. J. Sathyamoorthy; D. Manocha","Dept. of Computer Science, University of Maryland, College Park, MD, USA; Dept. of Computer Science, University of Maryland, College Park, MD, USA; Dept. of Electrical and Computer Engineering, University of Maryland, College Park, MD, USA; Dept. of Computer Science, University of Maryland, College Park, MD, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","6057","6063","We present a novel Deep Reinforcement Learning (DRL) based policy to compute dynamically feasible and spatially aware velocities for a robot navigating among mobile obstacles. Our approach combines the benefits of the Dynamic Window Approach (DWA) in terms of satisfying the robot’s dynamics constraints with state-of-the-art DRL-based navigation methods that can handle moving obstacles and pedestrians well. Our formulation achieves these goals by embedding the environmental obstacles’ motions in a novel low-dimensional observation space. It also uses a novel reward function to positively reinforce velocities that move the robot away from the obstacle’s heading direction leading to significantly lower number of collisions. We evaluate our method in realistic 3-D simulated environments and on a real differential drive robot in challenging dense indoor scenarios with several walking pedestrians. We compare our method with state-of-the-art collision avoidance methods and observe significant improvements in terms of success rate (up to 33% increase), number of dynamics constraint violations (up to 61% decrease), and smoothness. We also conduct ablation studies to highlight the advantages of our observation space formulation, and reward structure.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561462","","Legged locomotion;Automation;Navigation;Conferences;Dynamics;Reinforcement learning;Collision avoidance","collision avoidance;learning (artificial intelligence);mobile robots;navigation;path planning","mobile obstacles;novel Deep Reinforcement Learning based policy;dynamically feasible velocities;spatially aware velocities;robot navigating;Dynamic Window Approach;dynamics constraints;state-of-the-art DRL-based navigation methods;pedestrians;environmental obstacles;novel low-dimensional observation space;obstacle;differential drive robot;state-of-the-art collision avoidance methods;dynamics constraint violations;observation space formulation;DWA-RL;dynamically feasible Deep Reinforcement Learning policy;robot navigation","","15","","27","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"A Safe Hierarchical Planning Framework for Complex Driving Scenarios based on Reinforcement Learning","J. Li; L. Sun; J. Chen; M. Tomizuka; W. Zhan","Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","2660","2666","Autonomous vehicles need to handle various traffic conditions and make safe and efficient decisions and maneuvers. However, on the one hand, a single optimization/sampling-based motion planner cannot efficiently generate safe trajectories in real time, particularly when there are many interactive vehicles near by. On the other hand, end-to-end learning methods cannot assure the safety of the outcomes. To address this challenge, we propose a hierarchical behavior planning framework with a set of low-level safe controllers and a high-level reinforcement learning algorithm (H-CtRL) as a coordinator for the low-level controllers. Safety is guaranteed by the low-level optimization/sampling-based controllers, while the high-level reinforcement learning algorithm makes H-CtRL an adaptive and efficient behavior planner. To train and test our proposed algorithm, we built a simulator that can reproduce traffic scenes using real-world datasets. The proposed HCtRL is proved to be effective in various realistic simulation scenarios, with satisfying performance in terms of both safety and efficiency.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561195","","Learning systems;Adaptation models;Automation;Conferences;Reinforcement learning;Real-time systems;Safety","collision avoidance;learning (artificial intelligence);mobile robots;path planning","safe hierarchical planning framework;complex driving scenarios;autonomous vehicles;traffic conditions;efficient decisions;maneuvers;motion planner;safe trajectories;interactive vehicles;end-to-end learning methods;hierarchical behavior;low-level safe controllers;high-level reinforcement learning algorithm;low-level controllers;adaptive behavior planner;efficient behavior planner","","13","","27","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning with a Supervisor for a Mobile Robot in a Real-world Environment","K. Conn; R. A. Peters","Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, USA; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, USA","2007 International Symposium on Computational Intelligence in Robotics and Automation","16 Jul 2007","2007","","","73","78","This paper describes two experiments with supervised reinforcement learning (RL) on a real, mobile robot. Two types of experiments were preformed. One tests the robot's reliability in implementing a navigation task it has been taught by a supervisor. The other, in which new obstacles are placed along the previously learned path to the goal, measures the robot's robustness to changes in environment. Supervision consisted of human-guided, remote-controlled runs through a navigation task during the initial stages of reinforcement learning. The RL algorithms deployed enabled the robot to learn a path to a goal yet retain the ability to explore different solutions when confronted with a new obstacle. Experimental analysis was based on measurements of average time to reach the goal, the number of failed states encountered during an episode, and how closely the RL learner matched the supervisor's actions.","","1-4244-0789-3","10.1109/CIRA.2007.382878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4269878","Reinforcement learning;mobile robots;Q-learning","Learning;Mobile robots;Orbital robotics;Testing;Navigation;State-space methods;Computational modeling;Computational intelligence;Robotics and automation;USA Councils","control engineering computing;learning (artificial intelligence);mobile robots;navigation","mobile robot;real-world environment;supervised reinforcement learning;robot reliability;navigation task;human-guided remote-controlled runs","","12","","16","IEEE","16 Jul 2007","","","IEEE","IEEE Conferences"
"TERP: Reliable Planning in Uneven Outdoor Environments using Deep Reinforcement Learning","K. Weerakoon; A. J. Sathyamoorthy; U. Patel; D. Manocha","Dept. of Electrical and Computer Engineering, University of Maryland, College Park, MD, USA; Dept. of Electrical and Computer Engineering, University of Maryland, College Park, MD, USA; Dept. of Computer Science, University of Maryland, College Park, MD, USA; Dept. of Computer Science, University of Maryland, College Park, MD, USA","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","9447","9453","We present a novel method for reliable robot navigation in uneven outdoor terrains. Our approach employs a fully-trained Deep Reinforcement Learning (DRL) network that uses elevation maps of the environment, robot pose, and goal as inputs to compute an attention mask of the environment. The attention mask is used to identify reduced stability regions in the elevation map and is computed using channel and spatial attention modules and a novel reward function. We continuously compute and update a navigation cost-map that encodes the elevation information or the level-of-flatness of the terrain using the attention mask. We then generate locally least-cost waypoints on the cost-map and compute the final dynamically feasible trajectory using another DRL-based method. Our approach guarantees safe, locally least-cost paths and dynamically feasible robot velocities in uneven terrains. We observe an increase of 35.18% in terms of success rate and, a decrease of 26.14% in the cumulative elevation gradient of the robot's trajectory compared to prior navigation methods in high-elevation regions. We evaluate our method on a Husky robot in real-world uneven terrains (∼ $4m$ of elevation gain) and demonstrate its benefits.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812238","ARO(grant numbers:W911NF1910069,W911NF2110026); U.S. Army(grant numbers:W911NF2120076); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812238","","Automation;Navigation;Reinforcement learning;Trajectory;Planning;Reliability;Robots","deep learning (artificial intelligence);mobile robots;navigation;reinforcement learning;stability;trajectory control","stability regions;spatial attention modules;navigation cost-map;dynamically feasible trajectory;DRL-based method;dynamically feasible robot velocities;high-elevation regions;Husky robot;outdoor environments;deep reinforcement learning;robot navigation;TERP","","10","","46","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Decentralized Circle Formation Control for Fish-like Robots in the Real-world via Reinforcement Learning","T. Zhang; Y. Li; S. Li; Q. Ye; C. Wang; G. Xie","The State Key Laboratory of Turbulence and Complex Systems, Intelligent Biomimetic Design Lab, College of Engineering, Peking University, Beijing, China; The State Key Laboratory of Turbulence and Complex Systems, Intelligent Biomimetic Design Lab, College of Engineering, Peking University, Beijing, China; The State Key Laboratory of Turbulence and Complex Systems, Intelligent Biomimetic Design Lab, College of Engineering, Peking University, Beijing, China; Microsoft Research Asia, Beijing, China; The State Key Laboratory of Turbulence and Complex Systems, Intelligent Biomimetic Design Lab, College of Engineering, Peking University, Beijing, China; The State Key Laboratory of Turbulence and Complex Systems, Intelligent Biomimetic Design Lab, College of Engineering, Peking University, Beijing, China","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","8814","8820","In this paper, the circle formation control problem is addressed for a group of cooperative underactuated fish-like robots involving unknown nonlinear dynamics and disturbances. Based on the reinforcement learning and cognitive consistency theory, we propose a decentralized controller without the knowledge of the dynamics of the fish-like robots. The proposed controller can be transferred from simulation to reality. It is only trained in our established simulation environment, and the trained controller can be deployed to real robots without any manual tuning. Simulation results confirm that the proposed model-free robust formation control method is scalable with respect to the group size of the robots and outperforms other representative RL algorithms. Several experiments in the real world verify the effectiveness of our RL-based approach for circle formation control.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9562019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562019","","Solid modeling;Automation;Simulation;Conferences;Reinforcement learning;Manuals;Aerospace electronics","adaptive control;decentralised control;distributed control;learning (artificial intelligence);mobile robots;multi-robot systems;nonlinear control systems;nonlinear dynamical systems;robust control","reinforcement learning;circle formation control problem;underactuated fish-like;unknown nonlinear dynamics;cognitive consistency theory;decentralized controller;established simulation environment;trained controller;model-free robust formation control method;decentralized circle formation control","","9","","28","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Constructing action set from basis functions for reinforcement learning of robot control","A. Yamaguchi; Jun Takamatsu; Tsukasa Ogasawara","Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Nara, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Nara, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Nara, Japan","2009 IEEE International Conference on Robotics and Automation","18 Aug 2009","2009","","","2525","2532","Continuous action sets are used in many reinforcement learning (RL) applications in robot control since the control input is continuous. However, discrete action sets also have the advantages of ease of implementation and compatibility with some sophisticated RL methods, such as the Dyna [1]. However, one of the problem is the absence of general principles on designing a discrete action set for robot control in higher dimensional input space. In this paper, we propose to construct a discrete action set given a set of basis functions (BFs). We designed the action set so that the size of the set is proportional to the number of the BFs. This method can exploit the function approximator's nature, that is, in practical RL applications, the number of BFs does not increase exponentially with the dimension of the state space (e.g. [2]). Thus, the size of the proposed action set does not increase exponentially with the dimension of the input space. We apply an RL with the proposed action set to a robot navigation task and a crawling and a jumping tasks. The simulation results demonstrate that the proposed action set has the advantages of improved learning speed, and better ability to acquire performance, compared to a conventional discrete action set.","1050-4729","978-1-4244-2788-8","10.1109/ROBOT.2009.5152840","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5152840","Reinforcement learning;discrete action set;motion learning;crawling;jumping","Learning;Robot control;Orbital robotics;State-space methods;Space technology;Humanoid robots;Robotics and automation;Information science;Navigation;Legged locomotion","","","","9","","29","IEEE","18 Aug 2009","","","IEEE","IEEE Conferences"
"Generalization in Reinforcement Learning by Soft Data Augmentation","N. Hansen; X. Wang","University of California, San Diego, CA, USA; University of California, San Diego, CA, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","13611","13617","Extensive efforts have been made to improve the generalization ability of Reinforcement Learning (RL) methods via domain randomization and data augmentation. However, as more factors of variation are introduced during training, optimization becomes increasingly challenging, and empirically may result in lower sample efficiency and unstable training. Instead of learning policies directly from augmented data, we propose SOft Data Augmentation (SODA), a method that decouples augmentation from policy learning. Specifically, SODA imposes a soft constraint on the encoder that aims to maximize the mutual information between latent representations of augmented and non-augmented data, while the RL optimization process uses strictly non-augmented data. Empirical evaluations are performed on diverse tasks from DeepMind Control suite as well as a robotic manipulation task, and we find SODA to significantly advance sample efficiency, generalization, and stability in training over state-of-the-art vision-based RL methods.1","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561103","","Training;Automation;Conferences;Reinforcement learning;Task analysis;Optimization;Robots","generalisation (artificial intelligence);manipulators;reinforcement learning","generalization ability;reinforcement learning;sample efficiency;unstable training;SOft Data Augmentation;SODA;policy learning;soft constraint;nonaugmented data;RL optimization;vision-based RL methods;DeepMind Control suite","","9","","47","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Differentiable Physics Models for Real-world Offline Model-based Reinforcement Learning","M. Lutter; J. Silberbauer; J. Watson; J. Peters","Computer Science Department, Technical University of Darmstadt; Computer Science Department, Technical University of Darmstadt; Computer Science Department, Technical University of Darmstadt; Computer Science Department, Technical University of Darmstadt","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","4163","4170","A limitation of model-based reinforcement learning (MBRL) is the exploitation of errors in the learned models. Blackbox models can fit complex dynamics with high fidelity, but their behavior is undefined outside of the data distribution. Physics-based models are better at extrapolating, due to the general validity of their informed structure, but underfit in the real world due to the presence of unmodeled phenomena. In this work, we demonstrate experimentally that for the offline model-based reinforcement learning setting, physics-based models can be beneficial compared to high-capacity function approximators if the mechanical structure is known. Physics-based models can learn to perform the ball in a cup (BiC) task on a physical manipulator using only 4 minutes of sampled data using offline MBRL. We find that black-box models consistently produce unviable policies for BiC as all predicted trajectories diverge to physically impossible state, despite having access to more data than the physics-based model. In addition, we generalize the approach of physics parameter identification from modeling holonomic multi-body systems to systems with nonholonomic dynamics using end-to-end automatic differentiation.Videos: https://sites.google.com/view/ball-in-a-cup-in-4-minutes/","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561805","","Parameter estimation;Automation;Conferences;Reinforcement learning;Predictive models;Manipulators;Data models","control engineering computing;function approximation;manipulators;parameter estimation;reinforcement learning","differentiable physics models;real-world offline model-based reinforcement learning;blackbox models;MBRL;data distribution;ball in a cup;BiC;physical manipulator","","9","","51","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Explore-Bench: Data Sets, Metrics and Evaluations for Frontier-based and Deep-reinforcement-learning-based Autonomous Exploration","Y. Xu; J. Yu; J. Tang; J. Qiu; J. Wang; Y. Shen; Y. Wang; H. Yang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","6225","6231","Autonomous exploration and mapping of unknown terrains employing single or multiple robots is an essential task in mobile robotics and has therefore been widely investigated. Nevertheless, given the lack of unified data sets, metrics, and platforms to evaluate the exploration approaches, we develop an autonomous robot exploration benchmark en-titled Explore-Bench. The benchmark involves various explo-ration scenarios and presents two types of quantitative metrics to evaluate exploration efficiency and multi-robot cooperation. Explore-Bench is extremely useful as, recently, deep rein-forcement learning (DRL) has been widely used for robot exploration tasks and achieved promising results. However, training DRL-based approaches requires large data sets, and additionally, current benchmarks rely on realistic simulators with a slow simulation speed, which is not appropriate for training exploration strategies. Hence, to support efficient DRL training and comprehensive evaluation, the suggested Explore-Bench designs a 3-level platform with a unified data flow and 12 × speed-up that includes a grid-based simulator for fast evaluation and efficient training, a realistic Gazebo simulator, and a remotely accessible robot testbed for high-accuracy tests in physical environments. The practicality of the proposed benchmark is highlighted with the application of one DRL-based and three frontier-based exploration approaches. Fur-thermore, we analyze the performance differences and provide some insights about the selection and design of exploration methods. Our benchmark is available at https://github.com/efc-robot/Explore-Bench.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812344","National Natural Science Foundation of China(grant numbers:U19B2019,M-0248); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812344","","Training;Measurement;Automation;Design methodology;Benchmark testing;Data models;Task analysis","deep learning (artificial intelligence);mobile robots;multi-robot systems;reinforcement learning","deep-reinforcement-learning-based autonomous exploration;mobile robotics;autonomous robot exploration benchmark;quantitative metrics;multirobot cooperation;robot exploration tasks;training DRL-based approaches;grid-based simulator;realistic Gazebo simulator;three frontier-based exploration approaches;Explore-Bench","","8","","31","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Offline Meta-Reinforcement Learning for Industrial Insertion","T. Z. Zhao; J. Luo; O. Sushkov; R. Pevceviciute; N. Heess; J. Scholz; S. Schaal; S. Levine","Work done as an intern at X, The Moonshot Factory, Mountain View, CA, USA; Intrinsic Innovation LLC, Mountain View, CA, USA; Deepmind, London, UK; Deepmind, London, UK; Deepmind, London, UK; Deepmind, London, UK; Work done as an intern at X, The Moonshot Factory, Mountain View, CA, USA; Google Brain, Mountain View, CA, USA","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","6386","6393","Reinforcement learning (RL) can in principle let robots automatically adapt to new tasks, but current RL methods require a large number of trials to accomplish this. In this paper, we tackle rapid adaptation to new tasks through the framework of meta-learning, which utilizes past tasks to learn to adapt with a specific focus on industrial insertion tasks. Fast adaptation is crucial because prohibitively large number of on-robot trials will potentially damage hardware pieces. Additionally, effective adaptation is also feasible in that experience among different insertion applications can be largely leveraged by each other. In this setting, we address two specific challenges when applying meta-learning. First, conventional meta-RL algorithms require lengthy online meta-training. We show that this can be replaced with appropriately chosen offline data, resulting in an offline meta- RL method that only requires demonstrations and trials from each of the prior tasks, without the need to run costly meta-RL procedures online. Second, meta-RL methods can fail to generalize to new tasks that are too different from those seen at meta-training time, which poses a particular challenge in industrial applications, where high success rates are critical. We address this by combining contextual meta-learning with direct online finetuning: if the new task is similar to those seen in the prior data, then the contextual meta-learner adapts immediately, and if it is too different, it gradually adapts through finetuning. We show that our approach is able to quickly adapt to a variety of different insertion tasks, with a success rate of 100% using only a fraction of the samples needed for learning the tasks from scratch. Experiment videos and details are available at //sites.google.com/view/offline-metarl-insertion.https:","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812312","","Automation;Service robots;Reinforcement learning;Hardware;Task analysis;Videos","industrial manipulators;reinforcement learning","offline meta-reinforcement learning;industrial insertion tasks;on-robot trials;meta-RL algorithms;offline meta- RL method;industrial applications;online meta-training;contextual meta-learner;meta-RL procedures;insertion applications","","8","","49","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Secure Planning Against Stealthy Attacks via Model-Free Reinforcement Learning","A. K. Bozkurt; Y. Wang; M. Pajic","Duke University, Durham, NC, USA; Duke University, Durham, NC, USA; Duke University, Durham, NC, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","10656","10662","We consider the problem of security-aware planning in an unknown stochastic environment, in the presence of attacks on control signals (i.e., actuators) of the robot. We model the attacker as an agent who has the full knowledge of the controller as well as the employed intrusion-detection system and who wants to prevent the controller from performing tasks while staying stealthy. We formulate the problem as a stochastic game between the attacker and the controller and present an approach to express the objective of such an agent and the controller as a combined linear temporal logic (LTL) formula. We then show that the planning problem, described formally as the problem of satisfying an LTL formula in a stochastic game, can be solved via model-free reinforcement learning when the environment is completely unknown. Finally, we illustrate and evaluate our methods on two robotic planning case studies.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560940","","Actuators;Automation;Conferences;Stochastic processes;Intrusion detection;Reinforcement learning;Games","learning (artificial intelligence);security of data;stochastic games;stochastic processes;temporal logic","planning problem;LTL formula;stochastic game;model-free reinforcement learning;robotic planning case studies;secure planning;stealthy attacks;security-aware planning;unknown stochastic environment;control signals;employed intrusion-detection system;combined linear temporal logic formula","","7","","42","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Sample-efficient Reinforcement Learning in Robotic Table Tennis","J. Tebbe; L. Krauch; Y. Gao; A. Zell","Computer Science Department, Cognitive Systems Group, University of Tuebingen, Tuebingen, Germany; Computer Science Department, Cognitive Systems Group, University of Tuebingen, Tuebingen, Germany; Computer Science Department, Cognitive Systems Group, University of Tuebingen, Tuebingen, Germany; Computer Science Department, Cognitive Systems Group, University of Tuebingen, Tuebingen, Germany","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","4171","4178","Reinforcement learning (RL) has achieved some impressive recent successes in various computer games and simulations. Most of these successes are based on having large numbers of episodes from which the agent can learn. In typical robotic applications, however, the number of feasible attempts is very limited. In this paper we present a sample-efficient RL algorithm applied to the example of a table tennis robot. In table tennis every stroke is different, with varying placement, speed and spin. An accurate return therefore has to be found depending on a high-dimensional continuous state space. To make learning in few trials possible the method is embedded into our robot system. In this way we can use a one-step environment. The state space depends on the ball at hitting time (position, velocity, spin) and the action is the racket state (orientation, velocity) at hitting. An actor-critic based deterministic policy gradient algorithm was developed for accelerated learning. Our approach performs competitively both in a simulation and on the real robot in a number of challenging scenarios. Accurate results are obtained without pre-training in under 200 episodes of training. The video presenting our experiments is available at https://youtu.be/uRAtdoL6Wpw.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560764","","Training;Automation;Conferences;Computational modeling;Reinforcement learning;Games;Robots","computer games;control engineering computing;gradient methods;mobile robots;reinforcement learning;sport;state-space methods","sample-efficient reinforcement learning;robotic table tennis;computer games;sample-efficient RL algorithm;table tennis robot;high-dimensional continuous state space;robot system;hitting time;racket state;actor-critic based deterministic policy gradient algorithm;accelerated learning","","7","","51","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Smoothed Sarsa: Reinforcement learning for robot delivery tasks","D. Ramachandran; R. Gupta","Computer Science Department, University of Illinois, Urbana-Champaign, Urbana, IL, USA; Honda Research Institute USA, Inc., Mountain View, CA, USA","2009 IEEE International Conference on Robotics and Automation","18 Aug 2009","2009","","","2125","2132","Our goal in this work is to make high level decisions for mobile robots. In particular, given a queue of prioritized object delivery tasks, we wish to find a sequence of actions in real time to accomplish these tasks efficiently. We introduce a novel reinforcement learning algorithm called Smoothed Sarsa that learns a good policy for these delivery tasks by delaying the backup reinforcement step until the uncertainty in the state estimate improves. The state space is modeled by a Dynamic Bayesian Network and updated using a Region-based Particle Filter. We take advantage of the fact that only discrete (topological) representations of entity locations are needed for decision-making, to make the tracking and decision making more efficient. Our experiments show that policy search leads to faster task completion times as well as higher total reward compared to a manually crafted policy. Smoothed Sarsa learns a policy orders of magnitude faster than previous policy search algorithms. We demonstrate our results on the Player/Stage simulator and on the Pioneer robot.","1050-4729","978-1-4244-2788-8","10.1109/ROBOT.2009.5152707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5152707","","Learning;Decision making;Delay estimation;Mobile robots;State estimation;Particle filters;Robotics and automation;Uncertainty;Navigation;Orbital robotics","","","","6","1","18","IEEE","18 Aug 2009","","","IEEE","IEEE Conferences"
"Deep reinforcement learning of event-triggered communication and control for multi-agent cooperative transport","K. Shibata; T. Jimbo; T. Matsubara","Autonomous Distributed Cooperative Control Program, Data Analytics Research-Domain, Toyota Central R&D Labs., Inc., Japan; Autonomous Distributed Cooperative Control Program, Data Analytics Research-Domain, Toyota Central R&D Labs., Inc., Japan; Graduate School of Science and Technology, Division of Information Science, Nara Institute of Science and Technology, Nara, Japan","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","8671","8677","In this paper, we explore a multi-agent reinforcement learning approach to address the design problem of communication and control strategies for multi-agent cooperative transport. Typical end-to-end deep neural network policies may be insufficient for covering communication and control; these methods cannot decide the timing of communication and can only work with fixed-rate communications. Therefore, our framework exploits event-triggered architecture, namely, a feedback controller that computes the communication input and a triggering mechanism that determines when the input has to be updated again. Such event-triggered control policies are efficiently optimized using a multi-agent deep deterministic policy gradient. We confirmed that our approach could balance the transport performance and communication savings through numerical simulations.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561274","","Deep learning;Automation;Conferences;Reinforcement learning;Computer architecture;Numerical simulation;Timing","deep learning (artificial intelligence);multi-agent systems;reinforcement learning","event-triggered architecture;feedback controller;communication input;triggering mechanism;event-triggered control policies;multiagent deep deterministic policy gradient;transport performance;communication savings;deep reinforcement learning;event-triggered communication;multiagent reinforcement;control strategies;typical end-to-end deep neural network policies;fixed-rate communications","","6","","28","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"An insect-based method for learning landmark reliability using expectation reinforcement in dynamic environments","Z. Mathews; P. F. M. J. Verschure; S. Bermúdez i Badia","SPECS Laboratory, Institut Universitari de l'Audiovisual (IUA), Technology Department, Universitat Pompeu Fabra, Spain; SPECS Laboratory, Institut Universitari de l'Audiovisual (IUA), Technology Department, Universitat Pompeu Fabra, Spain; SPECS Laboratory, Institut Universitari de l'Audiovisual (IUA), Technology Department, Universitat Pompeu Fabra, Spain","2010 IEEE International Conference on Robotics and Automation","15 Jul 2010","2010","","","3805","3812","Navigation in unknown dynamic environments still remains a major challenge in robotics. Whereas insects like the desert ant with very limited computing and memory capacities solve this task with great efficiency. Thus, the understanding of the underlying neural mechanisms of insect navigation can inform us on how to build simpler yet robust autonomous robots. Based on recent developments in insect neuroethology and cognitive psychology, we propose a method for landmark navigation in dynamic environments. Our method enables the navigator to learn the reliability of landmarks using an expectation reinforcement method. For that end, we implemented a real-time neuronal model based on the Distributed Adaptive Control framework. The results demonstrate that our model is capable of learning the stability of landmarks by reinforcing its expectations. Also, the proposed mechanism allows the navigator to optimally restore its confidence when its expectations are violated. We also perform navigational experiments with real ants to compare with the results of our model. The behavior of the proposed autonomous navigator closely resembles real ant navigational behavior. Moreover, our model explains navigation in dynamic environments as a memory consolidation process, harnessing expectations and their violations.","1050-4729","978-1-4244-5038-1","10.1109/ROBOT.2010.5509935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5509935","","Navigation;Insects;Vehicle dynamics;Biological systems;Robotics and automation;Cognitive robotics;Mobile robots;Biomimetics;Artificial intelligence;Information representation","learning (artificial intelligence);mobile robots;navigation;path planning;reliability theory","insect-based method;landmark reliability learning;expectation reinforcement;unknown dynamic environment;robotics;desert ant;neural mechanism;insect navigation;robust autonomous robot;insect neuroethology;cognitive psychology;landmark navigation;real-time neuronal model;distributed adaptive control framework;autonomous navigator;navigational behavior;memory consolidation process","","5","","36","IEEE","15 Jul 2010","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Picking Cluttered General Objects with Dense Object Descriptors","H. Cao; W. Zeng; I. Wu","Department of Computer Science, National Yang Ming Chiao Tung University, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University, Taiwan","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","6358","6364","Picking cluttered general objects is a challenging task due to the complex geometries and various stacking configurations. Many prior works utilize pose estimation for picking, but pose estimation is difficult on cluttered objects. In this paper, we propose Cluttered Objects Descriptors (CODs), a dense cluttered objects descriptor which can represent rich object structures, and use the pre-trained CODs network along with its intermediate outputs to train a picking policy. Additionally, we train the policy with reinforcement learning, which enable the policy to learn picking without supervision. We conduct experiments to demonstrate that our CODs is able to consistently represent seen and unseen cluttered objects, which allowed for the picking policy to robustly pick cluttered general objects. The resulting policy can pick 96.69% of unseen objects in our experimental environment that are twice as cluttered as the training scenarios.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811911","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811911","","Geometry;Training;Automation;Pose estimation;Stacking;Reinforcement learning;Grasping","image filtering;object detection;object recognition;pose estimation;reinforcement learning","reinforcement learning;cluttered general objects;dense object Descriptors;Objects Descriptors;dense cluttered objects;rich object structures;pre-trained CODs network;picking policy;unseen cluttered objects;unseen objects;CODs","","5","","28","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Reinforcement learning with function approximation for cooperative navigation tasks","F. S. Melo; M. I. Ribeiro","Institute for Systems and Robotics, Instituto Superior Technico, Lisboa, Portugal; Institute for Systems and Robotics, Instituto Superior Technico, Lisboa, Portugal","2008 IEEE International Conference on Robotics and Automation","13 Jun 2008","2008","","","3321","3327","In this paper, we propose a reinforcement learning approach to address multi-robot cooperative navigation tasks in infinite settings. We propose an algorithm to simultaneously address the problems of learning and coordination in multi-robot problems. The proposed algorithm extends those existing in the literature, allowing to address simultaneous learning and coordination in problems with an infinite state-space. We also present the results obtained in several test scenarios featuring multi-robot navigation situations with partial observability.","1050-4729","978-1-4244-1646-2","10.1109/ROBOT.2008.4543717","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4543717","","Learning;Function approximation;Navigation;Robot kinematics;Robotics and automation;Mobile robots;Robot sensing systems;USA Councils;Testing;Observability","control engineering computing;cooperative systems;function approximation;learning (artificial intelligence);mobile robots;multi-robot systems;observability;path planning;state-space methods","reinforcement learning;function approximation;multirobot cooperative navigation tasks;infinite state-space;multirobot navigation;partial observability;mobile robot","","5","","27","IEEE","13 Jun 2008","","","IEEE","IEEE Conferences"
"Quadruped robot obstacle negotiation via reinforcement learning","Honglak Lee; Yirong Shen; Chih-Han Yu; G. Singh; A. Y. Ng","Stanford University, Stanford, CA, US; Department of Electrical Engineering, University of Stanford, Stanford, CA, USA; NA; Scientific Computing and Computational Mathematics Program, University of Stanford, Stanford, CA, USA; Computer Science Department, University of Stanford, Stanford, CA, USA","Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.","26 Jun 2006","2006","","","3003","3010","Legged robots can, in principle, traverse a large variety of obstacles and terrains. In this paper, we describe a successful application of reinforcement learning to the problem of negotiating obstacles with a quadruped robot. Our algorithm is based on a two-level hierarchical decomposition of the task, in which the high-level controller selects the sequence of foot-placement positions, and the low-level controller generates the continuous motions to move each foot to the specified positions. The high-level controller uses an estimate of the value function to guide its search; this estimate is learned partially from supervised data. The low-level controller is obtained via policy search. We demonstrate that our robot can successfully climb over a variety of obstacles which were not seen at training time","1050-4729","0-7803-9505-0","10.1109/ROBOT.2006.1642158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1642158","","Learning;Robot kinematics;Legged locomotion;Foot;Mobile robots;Leg;Motion control;Robotics and automation;Path planning;Computer science","control engineering computing;learning (artificial intelligence);legged locomotion;position control","quadruped robot obstacle negotiation;reinforcement learning;legged robots;two-level hierarchical decomposition;low-level controller;high-level controller","","5","1","35","IEEE","26 Jun 2006","","","IEEE","IEEE Conferences"
"Multi-robot Cooperative Pursuit via Potential Field-Enhanced Reinforcement Learning","Z. Zhang; X. Wang; Q. Zhang; T. Hu","Machine Intelligence and Collective Robotics (MICRO) Lab, Sun Yat-sen University, Guangzhou, China; Machine Intelligence and Collective Robotics (MICRO) Lab, Sun Yat-sen University, Guangzhou, China; Machine Intelligence and Collective Robotics (MICRO) Lab, Sun Yat-sen University, Guangzhou, China; Machine Intelligence and Collective Robotics (MICRO) Lab, Sun Yat-sen University, Guangzhou, China","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","8808","8814","It is of great challenge, though promising, to coordinate collective robots for hunting an evader in a decentralized manner purely in light of local observations. In this paper, this challenge is addressed by a novel hybrid cooperative pursuit algorithm that combines reinforcement learning with the artificial potential field method. In the proposed algorithm, decentralized deep reinforcement learning is employed to learn cooperative pursuit policies that are adaptive to dynamic environments. The artificial potential field method is integrated into the learning process as predefined rules to improve the data efficiency and generalization ability. It is shown by numerical simulations that the proposed hybrid design outperforms the pursuit policies either learned from vanilla reinforcement learning or designed by the potential field method. Furthermore, experiments are conducted by transferring the learned pursuit policies into real-world mobile robots. Experimental results demonstrate the feasibility and potential of the proposed algorithm in learning multiple cooperative pursuit strategies.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812083","","Automation;Heuristic algorithms;Robot kinematics;Reinforcement learning;Learning (artificial intelligence);Numerical simulation;Mobile robots","control engineering computing;cooperative systems;deep learning (artificial intelligence);mobile robots;multi-robot systems;reinforcement learning","multirobot cooperative pursuit strategies;pursuit policies;dynamic environments;hybrid cooperative pursuit algorithm;data efficiency;learning process;decentralized deep reinforcement learning;artificial potential field method;local observations;collective robots;potential field-enhanced reinforcement learning;mobile robots;hybrid design;generalization ability","","5","","35","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Transfer and Online Reinforcement Learning in STT-MRAM Based Embedded Systems for Autonomous Drones","I. Yoon; A. Anwar; T. Rakshit; A. Raychowdhury","Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Samsung semiconductor, advanced logic lab, Austin, TX, USA; Georgia Institute of Technology, Atlanta, GA, USA","2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)","16 May 2019","2019","","","1489","1494","In this paper we present an algorithm-hardware co-design for camera-based autonomous flight in small drones. We show that the large write-latency and write-energy for nonvolatile memory (NVM) based embedded systems makes them unsuitable for real-time reinforcement learning (RL). We address this by performing transfer learning (TL) on meta-environments and RL on the last few layers of a deep convolutional network. While the NVM stores the meta-model from TL, an on-die SRAM stores the weights of the last few layers. Thus all the real-time updates via RL are carried out on the SRAM arrays. This provides us with a practical platform with comparable performance as end-to-end RL and 83.4% lower energy per image frame.","1558-1101","978-3-9819263-2-3","10.23919/DATE.2019.8715066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715066","","Drones;Real-time systems;Automation;Europe;System-on-chip;Reinforcement learning;Cameras","autonomous aerial vehicles;control engineering computing;convolutional neural nets;embedded systems;learning (artificial intelligence);learning systems;MRAM devices;random-access storage;remotely operated vehicles;SRAM chips","write-energy;nonvolatile memory;real-time reinforcement learning;transfer learning;meta-environments;deep convolutional network;NVM stores;meta-model;SRAM stores;SRAM arrays;end-to-end RL;online reinforcement learning;autonomous drones;algorithm-hardware co-design;camera-based autonomous flight;write-latency;STT-MRAM based embedded systems;image frame","","5","","16","","16 May 2019","","","IEEE","IEEE Conferences"
"Model-Free Reinforcement Learning for Stochastic Games with Linear Temporal Logic Objectives","A. K. Bozkurt; Y. Wang; M. M. Zavlanos; M. Pajic","Duke University, Durham, NC, USA; Duke University, Durham, NC, USA; Duke University, Durham, NC, USA; Duke University, Durham, NC, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","10649","10655","We study synthesis of control strategies from linear temporal logic (LTL) objectives in unknown environments. We model this problem as a turn-based zero-sum stochastic game between the controller and the environment, where the transition probabilities and the model topology are fully unknown. The winning condition for the controller in this game is the satisfaction of the given LTL specification, which can be captured by the acceptance condition of a deterministic Rabin automaton (DRA) directly derived from the LTL specification. We introduce a model-free reinforcement learning (RL) methodology to find a strategy that maximizes the probability of satisfying a given LTL specification when the Rabin condition of the derived DRA has a single accepting pair. We then generalize this approach to any LTL formulas, for which the Rabin accepting condition may have more than one pairs, providing a lower bound on the satisfaction probability. Finally, we show applicability of our RL method on two planning case studies.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561989","","Automation;Learning automata;Conferences;Stochastic processes;Games;Reinforcement learning;Transforms","automata theory;game theory;learning (artificial intelligence);probability;stochastic games;stochastic processes;temporal logic","stochastic game;linear temporal logic objectives;control strategies;turn-based zero-sum;transition probabilities;model topology;winning condition;given LTL specification;acceptance condition;deterministic Rabin automaton;model-free reinforcement learning methodology;Rabin condition;derived DRA;single accepting pair;LTL formulas;Rabin accepting condition;satisfaction probability","","5","","38","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Graph-based Cluttered Scene Generation and Interactive Exploration using Deep Reinforcement Learning","K. N. Kumar; I. Essa; S. Ha",Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology,"2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","7521","7527","We introduce a novel method to teach a robotic agent to interactively explore cluttered yet structured scenes, such as kitchen pantries and grocery shelves, by leveraging the physical plausibility of the scene. We propose a novel learning framework to train an effective scene exploration policy to discover hidden objects with minimal interactions. First, we define a novel scene grammar to represent structured clutter. Then we train a Graph Neural Network (GNN) based Scene Generation agent using deep reinforcement learning (deep RL), to manipulate this Scene Grammar to create a diverse set of stable scenes, each containing multiple hidden objects. Given such cluttered scenes, we then train a Scene Exploration agent, using deep RL, to uncover hidden objects by interactively rearranging the scene. We show that our learned agents hide and discover significantly more objects than the baselines. We present quantitative results that prove the generalization capabilities of our agents. We also demonstrate sim-to-real transfer by successfully deploying the learned policy on a real UR10 robot to explore real-world cluttered scenes. The supplemental video can be found at: https://www.youtube.com/watch?v=T2Jo7wwaXss.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811874","","Training;Automation;Scalability;Reinforcement learning;Graph neural networks;Grammar;Clutter","control engineering computing;deep learning (artificial intelligence);graph theory;mobile robots;reinforcement learning;robot vision","multiple hidden objects;deep RL;learned agents hide;learned policy;real-world cluttered scenes;interactive exploration;deep reinforcement learning;robotic agent;structured scenes;kitchen pantries;grocery shelves;physical plausibility;learning framework;effective scene exploration policy;minimal interactions;scene grammar;structured clutter;graph neural network;stable scenes;scene generation agent;graph-based cluttered scene generation;scene exploration agent","","5","","42","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Circuit Routing Using Monte Carlo Tree Search and Deep Reinforcement Learning","Y. He; H. Li; T. Jin; F. S. Bao","Dept. of Computer Science, Iowa State University, Ames, Iowa; Dept. of Computer Science, Iowa State University, Ames, Iowa; Dept. of Computer Science, Iowa State University, Ames, Iowa; Dept. of Computer Science, Iowa State University, Ames, Iowa","2022 International Symposium on VLSI Design, Automation and Test (VLSI-DAT)","9 May 2022","2022","","","1","5","We propose a new approach to circuit routing by modeling it as a sequential decision problem and solving it in MCTS with DRL-guided rollout. Compared with conventional routing algorithms that are either manually designed with domain knowledge or tailored to specific design rules, our approach can be reconfigured for nearly any routing constraints and goals without changing the algorithm itself because the AI agent explores solutions in a general search strategy. Experimental results on both randomly generated circuits and popular open-source hardware projects show that our method achieves 33.3% higher success rate than traditional A *-based approach.","2472-9124","978-1-6654-0921-6","10.1109/VLSI-DAT54769.2022.9768074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9768074","Circuit routing;Deep reinforcement learning;Monte Carlo tree search","Backtracking;Monte Carlo methods;Design automation;Reinforcement learning;Very large scale integration;Routing;Search problems","circuit analysis computing;deep learning (artificial intelligence);Monte Carlo methods;reinforcement learning;telecommunication network routing;tree searching","circuit routing;sequential decision problem;DRL-guided rollout;domain knowledge;routing constraints;general search strategy;randomly generated circuits;deep reinforcement learning;Monte Carlo tree search;open-source hardware projects","","4","","14","IEEE","9 May 2022","","","IEEE","IEEE Conferences"
"Transfer of knowledge for a climbing Virtual Human: A reinforcement learning approach","B. Libeau; A. Micaelli; O. Sigaud","Laboratoire d''Intégration des Systèmes et des Technologies, Fontenay-aux-roses, France; Laboratoire d''Intégration des Systèmes et des Technologies, Fontenay-aux-roses, France; Systèmes Intelligents et de Robotique, CNRS UMR 7222, Université Pierre et Marie Curie, Paris, France","2009 IEEE International Conference on Robotics and Automation","6 Jul 2009","2009","","","2119","2124","In the reinforcement learning literature, transfer is the capability to reuse on a new problem what has been learnt from previous experiences on similar problems. Adapting transfer properties for robotics is a useful challenge because it can reduce the time spent in the first exploration phase on a new problem. In this paper we present a transfer framework adapted to the case of a climbing virtual human (VH). We show that our VH learns faster to climb a wall after having learnt on a different previous wall.","1050-4729","978-1-4244-2788-8","10.1109/ROBOT.2009.5152553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5152553","","Humans;Humanoid robots;Control systems;Intelligent robots;Robotics and automation;Mechanical systems;Supervised learning;Robot control;Context modeling;Centralized control","computer animation;control engineering computing;learning (artificial intelligence);robots;virtual reality","knowledge transfer;climbing virtual human;reinforcement learning;robotics","","4","","20","IEEE","6 Jul 2009","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Assisted Cache Cleaning to Mitigate Long-Tail Latency in DM-SMR","Y. Pan; Z. Jia; Z. Shen; B. Li; W. Chang; Z. Shao","School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK, United States; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, NT, Hong Kong","2021 58th ACM/IEEE Design Automation Conference (DAC)","8 Nov 2021","2021","","","103","108","DM-SMR adopts Persistent Cache (PC) to accommodate non-sequential write operations. However, the PC cleaning process induces severe long-tail latency. In this paper, we propose to mitigate the tail latency of PC cleaning by using Reinforcement Learning (RL). Specifically, a real-time lightweight Q-learning model is built to analyze the idle window of I/O workloads, based on which PC cleaning is judiciously scheduled, thereby maximally utilizing the I/O idle window and effectively hiding the tail latency from regular requests. We implement our technique inside a Linux device driver with an emulated SMR drive. Experimental results show that our technique can reduce the tail latency by 57.65% at 99.9th percentile and the average response time by 46.11% compared to a typical SMR design.","0738-100X","978-1-6654-3274-0","10.1109/DAC18074.2021.9586084","National Science Foundation; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586084","Shingled Magnetic Recording;Cleaning Process;Tail-Latency;Idle Time Window;Reinforcement Learning","Design automation;Linux;Reinforcement learning;Cleaning;Real-time systems;Timing;Time factors","cache storage;device drivers;learning (artificial intelligence);Linux;scheduling","real-time lightweight Q-learning model;SMR drive;long-tail latency;DM-SMR;persistent cache;PC cleaning process;reinforcement learning-assisted cache cleaning;nonsequential write operations;Linux device driver;SMR design","","4","","17","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Interference aware self-organization for wireless sensor networks: A reinforcement learning approach","L. Stabellini; J. Zander","Royal institute of Technology, Kista, Sweden; Royal institute of Technology, Kista, Sweden","2008 IEEE International Conference on Automation Science and Engineering","19 Sep 2008","2008","","","560","565","Reliability is a key issue in wireless sensor networks. Depending on the targeted application, reliability is achieved by establishing and maintaining a certain number of network functionalities: the greatest among those is certainly the capability of nodes to communicate. Sensors communications are sensible to interference that might corrupt packets transmission and even preclude the process of network formation. In this paper we propose a new scheme that allows to establish and maintain a connected topology while dealing with this problem. The idea of channel surfing is exploited to avoid interference; in the resulting multi-channel environment nodes discover their neighbors in a distributed fashion using a reinforcement learning (RL) algorithm. Our scheme allows the process of network formation even in presence of interference, overcoming thus the limit of algorithms currently implemented in state of the art standards for wireless sensor networks. By means of reinforcement learning the process of neighbor discovery is carried out in a fast and energy efficient way.","2161-8089","978-1-4244-2022-3","10.1109/COASE.2008.4626424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4626424","","Bridges;USA Councils;Automation;Conferences","distributed algorithms;learning (artificial intelligence);telecommunication network reliability;telecommunication network topology;wireless sensor networks","interference aware self-organization;wireless sensor networks;reinforcement learning algorithm;network formation;connected topology;channel surfing;multichannel environment;distributed fashion;neighbor discovery process;RL","","4","","14","IEEE","19 Sep 2008","","","IEEE","IEEE Conferences"
"Safe RAN control: A Symbolic Reinforcement Learning Approach","A. Nikou; A. Mujumdar; V. Sundararajan; M. Orlic; A. V. Feljan","Research Area Artificial Intelligence (AI), Ericsson Research; Research Area Artificial Intelligence (AI), Ericsson Research; University of California; Research Area Artificial Intelligence (AI), Ericsson Research; Research Area Artificial Intelligence (AI), Ericsson Research","2022 IEEE 17th International Conference on Control & Automation (ICCA)","25 Jul 2022","2022","","","332","337","In this paper, we present a Symbolic Reinforcement Learning (SRL) based architecture for safety control of Radio Access Network (RAN) applications. In particular, we provide a purely automated procedure in which a user can specify high-level logical safety specifications for a given cellular network topology in order for the latter to execute optimal safe performance which is measured through certain Key Performance Indicators (KPIs). The network consists of a set of fixed Base Stations (BS) which are equipped with antennas, which one can control by adjusting their vertical tilt angles. The aforementioned process is called Remote Electrical Tilt (RET) optimization. Recent research has focused on performing this RET optimization by employing Reinforcement Learning (RL) strategies due to the fact that they have self-learning capabilities to adapt in uncertain environments. The term safety refers to particular constraints bounds of the network KPIs in order to guarantee that when the algorithms are deployed in a live network, the performance is maintained. In our proposed architecture the safety is ensured through model-checking techniques over combined discrete system models (automata) that are abstracted through the learning process. We introduce a user interface (UI) developed to help a user set intent specifications to the system, and inspect the difference in agent proposed actions, and those that are allowed and blocked according to the safety specification.","1948-3457","978-1-6654-9572-1","10.1109/ICCA54724.2022.9831850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9831850","Reinforcement Learning (SRL);Formal methods;Remote Electrical Tilt (RET);RAN control","Training;Adaptation models;Automation;Learning automata;Reinforcement learning;User interfaces;Safety","cellular radio;optimisation;radio access networks;reinforcement learning;telecommunication computing;telecommunication network performance;telecommunication network topology","safe RAN control;high-level logical safety specifications;cellular network topology;key performance indicators;fixed base stations;vertical tilt angles;remote electrical tilt optimization;RET optimization;network KPI;combined discrete system models;radio access network applications;symbolic reinforcement learning based architecture;symbolic reinforcement learning approach;model-checking techniques","","4","","27","IEEE","25 Jul 2022","","","IEEE","IEEE Conferences"
"Stable and Efficient Shapley Value-Based Reward Reallocation for Multi-Agent Reinforcement Learning of Autonomous Vehicles","S. Han; H. Wang; S. Su; Y. Shi; F. Miao","Department of Computer Science and Engineering, University of Connecticut, Storrs Mansfield, CT, USA; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Department of Computer Science and Engineering, University of Connecticut, Storrs Mansfield, CT, USA; Electrical and Computer Engineering Department, University of California, San Diego, La Jolla, CA, USA; Department of Computer Science and Engineering, University of Connecticut, Storrs Mansfield, CT, USA","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","8765","8771","With the development of sensing and communication technologies in networked cyber-physical systems (CPSs), multi-agent reinforcement learning (MARL)-based methodologies are integrated into the control process of physical systems and demonstrate prominent performance in a wide array of CPS domains, such as connected autonomous vehicles (CAVs). However, it remains challenging to mathematically characterize the improvement of the performance of CAVs with communication and cooperation capability. When each individual autonomous vehicle is originally self-interest, we can not assume that all agents would cooperate naturally during the training process. In this work, we propose to reallocate the system's total reward efficiently to motivate stable cooperation among autonomous vehicles. We formally define and quantify how to reallocate the system's total reward to each agent under the proposed transferable utility game, such that communication-based cooperation among multi-agents increases the system's total reward. We prove that Shapley value-based reward reallocation of MARL locates in the core if the transferable utility game is a convex game. Hence, the cooperation is stable and efficient and the agents should stay in the coalition or the cooperating group. We then propose a cooperative policy learning algorithm with Shapley value reward reallocation. In experiments, compared with several literature algorithms, we show the improvement of the mean episode system reward of CAV systems using our proposed algorithm.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811626","NSF(grant numbers:1849246,1952096,2047354); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811626","","Training;Automation;Process control;Games;Reinforcement learning;Cyber-physical systems;Communications technology","control engineering computing;game theory;mobile robots;multi-agent systems;multi-robot systems;reinforcement learning;road vehicles;robot programming;stability","sensing technologies;CAV systems;mean episode system reward;policy learning algorithm;cooperating group;communication-based cooperation;transferable utility game;stable cooperation;individual autonomous vehicle;connected autonomous vehicles;networked cyber-physical systems;MARL;communication technologies;multiagent reinforcement learning;efficient Shapley value-based reward reallocation;stable Shapley value-based reward reallocation","","4","","47","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Nearest-Neighbor-based Collision Avoidance for Quadrotors via Reinforcement Learning","R. Ourari; K. Cui; A. Elshamanhory; H. Koeppl","Department of Electrical Engineering, Technische Universität Darmstadt, Darmstadt, Germany; Department of Electrical Engineering, Technische Universität Darmstadt, Darmstadt, Germany; Department of Electrical Engineering, Technische Universität Darmstadt, Darmstadt, Germany; Department of Electrical Engineering, Technische Universität Darmstadt, Darmstadt, Germany","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","293","300","Collision avoidance algorithms are of central interest to many drone applications. In particular, decentralized approaches may be the key to enabling robust drone swarm solutions in cases where centralized communication becomes computationally prohibitive. In this work, we draw biological inspiration from flocks of starlings (Sturnus vulgaris) and apply the insight to end-to-end learned decentralized collision avoidance. More specifically, we propose a new, scalable observation model following a biomimetic nearest-neighbor information constraint that leads to fast learning and good collision avoidance behavior. By proposing a general reinforcement learning approach, we obtain an end-to-end learning-based approach to integrating collision avoidance with arbitrary tasks such as package collection and formation change. To validate the generality of this approach, we successfully apply our methodology through motion models of medium complexity, modeling momentum and nonetheless allowing direct application to real world quadrotors in conjunction with a standard PID controller. In contrast to prior works, we find that in our sufficiently rich motion model, nearest-neighbor information is indeed enough to learn effective collision avoidance behavior. Our learned policies are tested in simulation and subsequently transferred to real-world drones to validate their real-world applicability.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812221","","Automation;Computational modeling;Biological system modeling;Reinforcement learning;Behavioral sciences;Complexity theory;Collision avoidance","biomimetics;collision avoidance;learning (artificial intelligence);mobile robots;multi-robot systems;three-term control","general reinforcement learning approach;end-to-end learning-based approach;integrating collision avoidance;motion models;world quadrotors;sufficiently rich motion model;effective collision avoidance behavior;learned policies;real-world drones;real-world applicability;nearest-neighbor-based collision avoidance;central interest;drone applications;particular approaches;decentralized approaches;robust drone swarm solutions;centralized communication;biological inspiration;Sturnus vulgaris;decentralized collision avoidance;scalable observation model;nearest-neighbor information constraint;fast learning;good collision avoidance behavior","","4","","44","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"DisCo RL: Distribution-Conditioned Reinforcement Learning for General-Purpose Policies","S. Nasiriany; V. H. Pong; A. Nair; A. Khazatsky; G. Berseth; S. Levine","University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","6635","6641","Can we use reinforcement learning to learn general-purpose policies that can perform a wide range of different tasks, resulting in flexible and reusable skills? Contextual policies provide this capability in principle, but the representation of the context determines the degree of generalization and expressivity. Categorical contexts preclude generalization to entirely new tasks. Goal-conditioned policies may enable some generalization, but cannot capture all tasks that might be desired. In this paper, we propose goal distributions as a general and broadly applicable task representation suitable for contextual policies. Goal distributions are general in the sense that they can represent any state-based reward function when equipped with an appropriate distribution class, while the particular choice of distribution class allows us to trade off expressivity and learnability. We develop an off-policy algorithm called distribution-conditioned reinforcement learning (DisCo RL) to efficiently learn these policies. We evaluate DisCo RL on a variety of robot manipulation tasks and find that it significantly outperforms prior methods on tasks that require generalization to new goal distributions.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561402","Office of Naval Research; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561402","","Automation;Conferences;Reinforcement learning;Task analysis;Robots","image representation;manipulators;reinforcement learning;robot vision","goal distributions;general task representation;off-policy algorithm;distribution-conditioned reinforcement learning;DisCo RL;robot manipulation tasks;general-purpose policies;goal-conditioned policies;categorical contexts;state-based reward function","","4","","45","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Enhancing Deep Reinforcement Learning Approaches for Multi-Robot Navigation via Single-Robot Evolutionary Policy Search","E. Marchesini; A. Farinelli","Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","5525","5531","Recent Multi-Agent Deep Reinforcement Learning approaches factorize a global action-value to address non-stationarity and favor cooperation. These methods, however, hinder exploration by introducing constraints (e.g., additive value-decomposition) to guarantee the factorization. Our goal is to enhance exploration and improve sample efficiency of multi-robot mapless navigation by incorporating a periodical Evolutionary Policy Search (EPS). In detail, the multi-agent training “specializes” the robots' policies to learn the collision avoidance skills that are mandatory for the task. Concurrently, in this work we propose the use of Evolutionary Algorithms to explore different regions of the policy space in an environment with only a single robot. The idea is that core navigation skills, originated by the multi-robot policies using mutation operators, improve faster in the single-robot EPS. Hence, policy parameters can be injected into the multi-robot setting using crossovers, leading to improved performance and sample efficiency. Experiments in tasks with up to 12 robots confirm the beneficial transfer of navigation skills from the EPS to the multi-robot setting, improving the performance of prior methods.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812341","","Training;Automation;Additives;Navigation;Reinforcement learning;Evolutionary computation;Collision avoidance","collision avoidance;evolutionary computation;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;navigation;path planning;robot vision","robots;collision avoidance skills;Evolutionary Algorithms;policy space;core navigation skills;multirobot policies;single-robot EPS;policy parameters;multirobot setting;sample efficiency;Deep Reinforcement Learning approaches;Multirobot navigation;single-robot Evolutionary Policy Search;Recent MultiAgent Deep Reinforcement;global action-value;nonstationarity;additive value-decomposition;multirobot mapless navigation;periodical Evolutionary Policy Search;multiagent training","","4","","34","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"A3C-S: Automated Agent Accelerator Co-Search towards Efficient Deep Reinforcement Learning","Y. Fu; Y. Zhang; C. Li; Z. Yu; Y. Lin",Rice University; Rice University; Rice University; Rice University; Rice University,"2021 58th ACM/IEEE Design Automation Conference (DAC)","8 Nov 2021","2021","","","13","18","Driven by the explosive interest in applying deep reinforcement learning (DRL) agents to numerous real-time control and decision-making applications, there has been a growing demand to deploy DRL agents to empower daily-life intelligent devices, while the prohibitive complexity of DRL stands at odds with limited on-device resources. In this work, we propose an Automated Agent Accelerator Co-Search (A3C-S) framework, which to our best knowledge is the first to automatically co-search the optimally matched DRL agents and accelerators that maximize both test scores and hardware efficiency. Extensive experiments consistently validate the superiority of our A3C-S over state-of-the-art techniques.","0738-100X","978-1-6654-3274-0","10.1109/DAC18074.2021.9586305","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586305","Network Accelerator Co-design;Deep Reinforcement Learning;AutoML","Training;Knowledge engineering;Design automation;Accelerated aging;DNA;Life estimation;Reinforcement learning","deep learning (artificial intelligence);reinforcement learning;search problems;software agents","automated agent accelerator co-search;deep reinforcement learning agents;daily-life intelligent devices;decision making;on-device resources;optimally matched DRL agents;real time control;A3C-S framework","","3","","28","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"FedLight: Federated Reinforcement Learning for Autonomous Multi-Intersection Traffic Signal Control","Y. Ye; W. Zhao; T. Wei; S. Hu; M. Chen","MoE Research Center of HW/SW Co-design, East China Normal University, Shanghai, China; MoE Research Center of HW/SW Co-design, East China Normal University, Shanghai, China; MoE Research Center of HW/SW Co-design, East China Normal University, Shanghai, China; School of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom; MoE Research Center of HW/SW Co-design, East China Normal University, Shanghai, China","2021 58th ACM/IEEE Design Automation Conference (DAC)","8 Nov 2021","2021","","","847","852","Although Reinforcement Learning (RL) has been successfully applied in traffic control, it suffers from the problems of high average vehicle travel time and slow convergence to optimized solutions. This is because, due to the scalability restriction, most existing RL-based methods focus on the optimization of individual intersections while the impact of their cooperation is neglected. Without taking all the correlated intersections as a whole into account, it is difficult to achieve global optimization goals for complex traffic scenarios. To address this issue, this paper proposes a novel federated reinforcement learning approach named FedLight to enable optimal signal control policy generation for multi-intersection traffic scenarios. Inspired by federated learning, our approach supports knowledge sharing among RL agents, whose models are trained using decentralized traffic data at intersections. Based on such model-level collaborations, both the overall convergence rate and control quality can be significantly improved. Comprehensive experimental results demonstrate that compared with the state-of-the-art techniques, our approach can not only achieve better average vehicle travel time for various multi-intersection configurations, but also converge to optimal solutions much faster.","0738-100X","978-1-6654-3274-0","10.1109/DAC18074.2021.9586175","National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586175","Federated Reinforcement Learning;Neural Network;Advantage Actor-Critic;Traffic Signal Control","Design automation;Scalability;Process control;Collaboration;Reinforcement learning;Traffic control;Collaborative work","control engineering computing;optimisation;reinforcement learning;road traffic control;traffic engineering computing;vehicles","correlated intersections;federated reinforcement learning;FedLight;optimal signal control policy generation;RL agents;decentralized traffic data;optimal solutions;autonomous multiintersection traffic signal control;optimization","","3","","16","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Prioritized Reinforcement Learning for Analog Circuit Optimization With Design Knowledge","N. S. Karthik Somayaji; H. Hu; P. Li","Electrical and Computer Engineering, University of California, Santa Barbara, Santa Barbara, USA; Electrical and Computer Engineering, University of California, Santa Barbara, Santa Barbara, USA; Electrical and Computer Engineering, University of California, Santa Barbara, Santa Barbara, USA","2021 58th ACM/IEEE Design Automation Conference (DAC)","8 Nov 2021","2021","","","1231","1236","Analog circuit design and optimization manifests as a critical phase in IC design, which still heavily relies on extensive and time-consuming manual designing by experienced experts. In recent years, the development of reinforcement learning (RL) algorithms draws attention with related techniques being introduced into the analog design field for circuit optimization. However, for robust and efficient analog circuit design, a smart and rapid search for high-quality design points is more desired than finding a globally optimal agent as in traditional RL applications, which was a point not fully considered in some previous works. In this work, we propose three techniques within the RL framework aiming at fast high-quality design point search in a data efficient manner. In particular, we (i) incorporate design knowledge from experienced designers into the critic network design to achieve a better reward evaluation with less data; (ii) guide the RL training with non-uniform sampling techniques prioritizing exploitation over high quality designs and exploration for poorly-trained space; (iii) leverage the trained critic network and limited additional circuit simulation for smart and efficient sampling to get high-quality design points. The experimental results demonstrate the effectiveness and efficiency of our proposed techniques.","0738-100X","978-1-6654-3274-0","10.1109/DAC18074.2021.9586189","Semiconductor Research Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586189","Reinforcement learning;analog circuit design;design knowledge;smart sampling","Training;Knowledge engineering;Integrated circuits;Circuit optimization;Design automation;Reinforcement learning;Manuals","analogue integrated circuits;circuit optimisation;circuit simulation;integrated circuit design;reinforcement learning;sampling methods;search problems","analog circuit optimization;IC design;reinforcement learning;analog circuit design;smart search;rapid search;high-quality design point search;design knowledge;critic network design;RL training;nonuniform sampling;trained critic network;circuit simulation;reward evaluation","","3","","11","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Fault-Aware Robust Control via Adversarial Reinforcement Learning","F. Yang; C. Yang; D. Guo; H. Liu; F. Sun","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","10 Nov 2021","2021","","","109","115","Robots have limited adaptation ability compared to humans and animals in the case of damage. However, robot damages are prevalent in realworld applications, especially for robots deployed in extreme environments. The fragility of robots greatly limits their widespread application. We propose an adversarial reinforcement learning framework, which significantly increases robot robustness over joint damage cases in both manipulation tasks and locomotion tasks. The agent is trained iteratively under the joint damage cases where it has poor performance. We validate our algorithm on a three-fingered robot hand and a quadruped robot. Our algorithm can be trained only in simulation and directly deployed on a real robot without any fine-tuning. It also demonstrates exceeding success rates over arbitrary joint damage cases.","2642-6633","978-1-6654-2527-8","10.1109/CYBER53097.2021.9588329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9588329","","Robust control;Resistance;Automation;Conferences;Reinforcement learning;Control systems;Robustness","dexterous manipulators;learning (artificial intelligence);legged locomotion;mobile robots;motion control;robots;robust control","fault-aware robust control;adaptation ability;robot damages;realworld applications;extreme environments;widespread application;adversarial reinforcement learning framework;robot robustness;manipulation tasks;locomotion tasks;robot hand;quadruped robot;arbitrary joint damage cases","","2","","29","IEEE","10 Nov 2021","","","IEEE","IEEE Conferences"
"Time-delayed Data Transmission in Heterogeneous Multi-agent Deep Reinforcement Learning System","E. Fard; R. R. Selmic","Department of Electrical and Computer Engineering, Concordia University, Montreal, Canada; Department of Electrical and Computer Engineering, Concordia University, Montreal, Canada","2022 30th Mediterranean Conference on Control and Automation (MED)","1 Aug 2022","2022","","","636","642","This paper studies the data transmission between agents of a multi-agent, deep reinforcement learning (MADRL) system (leaderless and leader-follower) using the deep Q-network (DQN) algorithm. The structure of the MADRL system consists of various clusters of agents. The agents in a cluster have the same architectures. The DQN architecture is used to present the first cluster’s agents structure. The other clusters, including various architectures, are considered as the environment of the first cluster’s deep reinforcement learning (DRL) agent. The goal of each static agent is to transfer data with the maximum average reward. We consider two novel observations in data transmission termed on-time and time-delay. The two proposed observations are considered when the data transmission channel is idle and the data is transmitted on-time or time-delayed. Moreover, by considering the distance between the neighboring agents, we present a novel immediate reward function by appending a distance-based reward to the previously utilized reward. We have rigorously shown which system (on-time or time-delayed) has a superior performance based on the DQN loss and team reward for the entire team of agents. The claims have been proven theoretically, and the simulation confirms theoretical findings.","2473-3504","978-1-6654-0673-4","10.1109/MED54222.2022.9837194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837194","","Automation;Clustering algorithms;Reinforcement learning;Data communication","data communication;deep learning (artificial intelligence);delays;multi-agent systems;multi-robot systems;reinforcement learning","deep Q-network algorithm;MADRL system;DQN architecture;static agent;maximum average reward;time-delay;data transmission channel;neighboring agents;distance-based reward;time-delayed data transmission;heterogeneous multiagent deep reinforcement learning system;first-cluster deep reinforcement learning agent;DRL agent","","2","","24","IEEE","1 Aug 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Collaborative Quadrupedal Manipulation of a Payload over Challenging Terrain","Y. Ji; B. Zhang; K. Sreenath","College of Artificial Intelligence, Nankai University, Tianjin, CHN; Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA","2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)","5 Oct 2021","2021","","","899","904","Motivated towards performing missions in unstructured environments using a group of robots, this paper presents a reinforcement learning-based strategy for multiple quadrupedal robots executing collaborative manipulation tasks. By taking target position, velocity tracking, and height adjustment into account, we demonstrate that the proposed strategy enables four quadrupedal robots manipulating a payload to walk at desired linear and angular velocities, as well as over challenging terrain. The learned policy is robust to variations of payload mass and can be parameterized by different commanded velocities. (Video11https://youtu.be/i8kZSYdi9Nk)","2161-8089","978-1-6654-1873-7","10.1109/CASE49439.2021.9551481","National Science Foundation(grant numbers:CMMI-1944722); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551481","","Target tracking;Computer aided software engineering;Automation;Conferences;Collaboration;Reinforcement learning;Angular velocity","human-robot interaction;learning (artificial intelligence);legged locomotion;manipulators;mobile robots;motion control;path planning;position control","collaborative manipulation tasks;target position;velocity tracking;height adjustment;desired linear;angular velocities;challenging terrain;learned policy;payload mass;different commanded velocities;collaborative quadrupedal manipulation;unstructured environments;reinforcement learning-based strategy;multiple quadrupedal robots","","2","","28","IEEE","5 Oct 2021","","","IEEE","IEEE Conferences"
"Abstract Demonstrations and Adaptive Exploration for Efficient and Stable Multi-step Sparse Reward Reinforcement Learning","X. Yang; Z. Ji; J. Wu; Y. -K. Lai","School of Engineering, Cardiff University, Cardiff, UK; School of Engineering, Cardiff University, Cardiff, UK; School of Computer Science and Informatics, Cardiff University, Cardiff, UK; School of Computer Science and Informatics, Cardiff University, Cardiff, UK","2022 27th International Conference on Automation and Computing (ICAC)","10 Oct 2022","2022","","","1","6","Although Deep Reinforcement Learning (DRL) has been popular in many disciplines including robotics, state-of-the-art DRL algorithms still struggle to learn long-horizon, multistep and sparse reward tasks, such as stacking several blocks given only a task-completion reward signal. To improve learning efficiency for such tasks, this paper proposes a DRL exploration technique, termed $\mathbf{A^{2}}$, which integrates two components inspired by human experiences: Abstract demonstrations and Adaptive exploration. $\mathbf{A^{2}}$ starts by decomposing a complex task into subtasks, and then provides the correct orders of subtasks to learn. During training, the agent explores the environment adaptively, acting more deterministically for well-mastered subtasks and more stochastically for ill-learnt subtasks. Ablation and comparative experiments are conducted on several grid-world tasks and three robotic manipulation tasks. We demonstrate that $\mathbf{A^{2}}$ can aid popular DRL algorithms (DQN, DDPG, and SAC) to learn more efficiently and stably in these environments.","","978-1-6654-9807-4","10.1109/ICAC55051.2022.9911100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9911100","","Training;Automation;Stacking;Reinforcement learning;Task analysis;Robots","manipulators;reinforcement learning","abstract demonstrations;adaptive exploration;sparse reward tasks;task-completion reward signal;learning efficiency;DRL exploration technique;well-mastered subtasks;ill-learnt subtasks;grid-world tasks;robotic manipulation tasks;DRL algorithms;multistep sparse reward reinforcement learning;deep reinforcement learning","","2","","25","IEEE","10 Oct 2022","","","IEEE","IEEE Conferences"
"Discovering Synergies for Robot Manipulation with Multi-Task Reinforcement Learning","Z. He; M. Ciocarlie","Department of Computer Science, Columbia University, New York, USA; Department of Mechanical Engineering, Columbia University, New York, USA","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","2714","2721","Controlling robotic manipulators with high-dimensional action spaces for dexterous tasks is a challenging problem. Inspired by human manipulation, researchers have studied generating and using postural synergies for robot hands to accomplish manipulation tasks, leveraging the lower dimensional nature of synergistic action spaces. However, many of these works require pre-collected data from an existing controller in order to derive such a subspace by means of dimensionality reduction. In this paper, we present a framework that simultaneously discovers both a synergy space and a multi-task policy that operates on this low-dimensional action space to accomplish diverse manipulation tasks. We demonstrate that our end-to-end method is able to perform multiple tasks using few synergies, and outperforms sequential methods that apply dimensionality reduction to independently collected data. We also show that deriving synergies using multiple tasks can lead to a subspace that enables robots to efficiently learn new manipulation tasks and interactions with new objects.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812170","NSF(grant numbers:IIS-1551631); ONR(grant numbers:N00014-21-1-4010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812170","","Dimensionality reduction;Automation;Pipelines;Reinforcement learning;Aerospace electronics;Multitasking;Manipulators","dexterous manipulators;intelligent robots;reinforcement learning","dimensionality reduction;synergy space;multitask policy;low-dimensional action space;end-to-end method;multitask reinforcement learning;robotic manipulators;high-dimensional action;dexterous tasks;human manipulation;postural synergies;robot hands;lower dimensional nature;synergistic action","","2","","25","USGov","12 Jul 2022","","","IEEE","IEEE Conferences"
"OPIRL: Sample Efficient Off-Policy Inverse Reinforcement Learning via Distribution Matching","H. Hoshino; K. Ota; A. Kanezaki; R. Yokota","Department of Computer Science, School of Computing, Tokyo Institute of Technology, Japan; Department of Computer Science, School of Computing, Tokyo Institute of Technology, Japan; Department of Computer Science, School of Computing, Tokyo Institute of Technology, Japan; Global Scientific Information and Computing Center, Tokyo Institute of Technology, Japan","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","448","454","Inverse Reinforcement Learning (IRL) is attractive in scenarios where reward engineering can be tedious. However, prior IRL algorithms use on-policy transitions, which require intensive sampling from the current policy for stable and optimal performance. This limits IRL applications in the real world, where environment interactions can become highly expensive. To tackle this problem, we present Off-Policy Inverse Reinforcement Learning (OPIRL), which (1) adopts off-policy data distribution instead of on-policy and enables significant reduction of the number of interactions with the environment, (2) learns a reward function that is transferable with high generalization capabilities on changing dynamics, and (3) leverages mode-covering behavior for faster convergence. We demonstrate that our method is considerably more sample efficient and generalizes to novel environments through the experiments. Our method achieves better or comparable results on policy performance baselines with significantly fewer interactions. Furthermore, we empirically show that the recovered reward function generalizes to different tasks where prior arts are prone to fail.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811660","Imitation Learning;Transfer Learning;Learning from Demonstration;Inverse Reinforcement Learning","Training;Automation;Reinforcement learning;Control systems;Behavioral sciences;Usability;Task analysis","optimisation;reinforcement learning","OPIRL;reward engineering;IRL algorithms;on-policy transitions;intensive sampling;stable performance;optimal performance;IRL applications;environment interactions;off-policy data distribution;policy performance baselines;reward function;sample efficient off-policy inverse reinforcement learning;convergence;distribution matching","","2","","38","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Sim-and-Real Reinforcement Learning for Manipulation: A Consensus-based Approach","W. Liu; H. Niu; W. Pan; G. Herrmann; J. Carrasco","Remote Applications in Challenging Environments (RACE), United Kingdom Atomic Energy Authority, Culham, UK; Remote Applications in Challenging Environments (RACE), United Kingdom Atomic Energy Authority, Culham, UK; the Department of Computer Science, The University of Manchester, Manchester, UK; the Department of Electrical & Electronic Engineering, The University of Manchester, Manchester, UK; the Department of Electrical & Electronic Engineering, The University of Manchester, Manchester, UK","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","3911","3917","Sim-and-real training is a promising alternative to sim-to-real training for robot manipulations. However, the current sim-and-real training is neither efficient, i.e., slow con-vergence to the optimal policy, nor effective, i.e., sizeable real-world robot data. Given limited time and hardware budgets, the performance of sim-and-real training is not satisfactory. In this paper, we propose a Consensus-based Sim-And-Real deep reinforcement learning algorithm (CSAR) for manipulator pick-and-place tasks, which shows comparable performance in both sim-and- real worlds. In this algorithm, we train the agents in simulators and the real world to get the optimal policies for both sim-and-real worlds. We found two interesting phenomenons: (1) Best policy in simulation is not the best for sim-and-real training. (2) The more simulation agents, the better sim-and-real training. The experimental video is available at: https://youtu.be/mcHJtNIsTEQ.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161062","","Training;Deep learning;Costs;Automation;Reinforcement learning;Manipulators;Hardware","deep learning (artificial intelligence);learning (artificial intelligence);manipulators;multi-agent systems;reinforcement learning","-real training;sim- training;sim-to-real training","","1","","36","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Seeking Visual Discomfort: Curiosity-driven Representations for Reinforcement Learning","E. Aljalbout; M. Ulmer; R. Triebel","Technical University of Munich (TUM), Munich, Germany; Technical University of Munich (TUM), Munich, Germany; Technical University of Munich (TUM), Munich, Germany","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","3591","3597","Vision-based reinforcement learning (RL) is a promising approach to solve control tasks involving images as the main observation. State-of-the-art RL algorithms still struggle in terms of sample efficiency, especially when using image observations. This has led to increased attention on integrating state representation learning (SRL) techniques into the RL pipeline. Work in this field demonstrates a substantial improvement in sample efficiency among other benefits. However, to take full advantage of this paradigm, the quality of samples used for training plays a crucial role. More importantly, the diversity of these samples could affect the sample efficiency of vision-based RL, but also its generalization capability. In this work, we present an approach to improve sample diversity for state representation learning. Our method enhances the exploration capability of RL algorithms, by taking advantage of the SRL setup. Our experiments show that our proposed approach boosts the visitation of problematic states, improves the learned state representation, and outperforms the baselines for all tested environments. These results are most apparent for environments where the baseline methods struggle. In simple environments, our method contributes to stabilizing the training, reducing the reward variance, and improving sample efficiency.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811663","","Representation learning;Training;Visualization;Automation;Transfer learning;Pipelines;Reinforcement learning","computer vision;generalisation (artificial intelligence);reinforcement learning","curiosity-driven representations;vision-based reinforcement learning;sample efficiency;image observations;state representation learning;RL pipeline;vision-based RL;sample diversity;problematic states;learned state representation;visual discomfort;generalization capability","","1","","41","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Coding for Distributed Multi-Agent Reinforcement Learning","B. Wang; J. Xie; N. Atanasov","Department of Electrical and Computer Engineering, University of California, San Diego; Department of Electrical and Computer Engineering, San Diego State University, San Diego, CA; Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, CA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","10625","10631","This paper aims to mitigate straggler effects in synchronous distributed learning for multi-agent reinforcement learning (MARL) problems. Stragglers arise frequently in a distributed learning system, due to the existence of various system disturbances such as slow-downs or failures of compute nodes and communication bottlenecks. To resolve this issue, we propose a coded distributed learning framework, which speeds up the training of MARL algorithms in the presence of stragglers, while maintaining the same accuracy as the centralized approach. As an illustration, a coded distributed version of the multi-agent deep deterministic policy gradient (MADDPG) algorithm is developed and evaluated. Different coding schemes, including maximum distance separable (MDS) code, random sparse code, replication-based code, and regular low density parity check (LDPC) code are also investigated. Simulations in several multi-robot problems demonstrate the promising performance of the proposed framework.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561645","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561645","","Training;Computer aided instruction;Automation;Distance learning;Simulation;Conferences;Reinforcement learning","gradient methods;multi-agent systems;multi-robot systems;parity check codes;random codes;reinforcement learning","distributed multiagent reinforcement learning;straggler effects;multiagent reinforcement learning problems;distributed learning system;system disturbances;MARL algorithms;multiagent deep deterministic policy gradient algorithm;different coding schemes;random sparse code;replication-based code;regular low density parity check code;multirobot problems","","1","","45","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"REST: Constructing Rectilinear Steiner Minimum Tree via Reinforcement Learning","J. Liu; G. Chen; E. F. Y. Young","CSE Department, CUHK; CSE Department, CUHK; CSE Department, CUHK","2021 58th ACM/IEEE Design Automation Conference (DAC)","8 Nov 2021","2021","","","1135","1140","Rectilinear Steiner Minimum Tree (RSMT) is the shortest way to interconnect a net’s n pins using rectilinear edges only. Constructing the optimal RSMT is NP-complete and nontrivial. In this work, we design a reinforcement learning based algorithm called REST for RSMT construction. After training, REST constructs RSMT of $\leq 0.36\%$ length error on average for nets with $\leq 50$ pins. The average time needed for one net is fewer than 1.9 ms, and is much faster than traditional heuristics of similar quality. This is also the first successful attempt to solve this problem using a machine learning approach.","0738-100X","978-1-6654-3274-0","10.1109/DAC18074.2021.9586209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586209","","Training;Machine learning algorithms;Design automation;Reinforcement learning;Pins","computational complexity;reinforcement learning;trees (mathematics)","reinforcement learning;RSMT;REST;nets;machine learning;pins;rectilinear edges;NP-complete;rectilinear steiner minimum tree","","1","","17","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Multiple Omnidirectional Mobile Robots Control","H. Tian; Y. Lu; O. Zhang; G. Sun; C. Wu; W. Yao","School of Astronautics, Harbin Institute of Technology, Harbin, China; School of Astronautics, Harbin Institute of Technology, Harbin, China; School of Astronautics, Harbin Institute of Technology, Harbin, China; School of Astronautics, Harbin Institute of Technology, Harbin, China; School of Astronautics, Harbin Institute of Technology, Harbin, China; School of Astronautics, Harbin Institute of Technology, Harbin, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","7226","7231","Three-wheeled omnidirectional mobile robot (TWODMR) is a new type of holonomic mobile robot, which performs better moving flexibility than nonholonomic mobile robot. In this paper, we focus on the multi-robot formation control problem of three-wheeled omnidirectional mobile robots. The deep deterministic policy gradient (DDPG) algorithm is applied to carry out the formation motion planning and controlling task and the artificial potential field (APF) algorithm is used as a comparison. According to the experiment results, the DDPG algorithm solves the multi-robot formation control problem. The DDPG algorithm optimizes its control strategy and the trained DDPG controller achieves more accuracy and adaptability than the APF controller.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728181","National Natural Science Foundation of China; Natural Science Foundation of Heilongjiang Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728181","Formation control;omnidirectional mobile robot;artificial potential field;deep deterministic policy gradient","Automation;Reinforcement learning;Mean square error methods;Planning;Mobile robots;Task analysis","control engineering computing;deep learning (artificial intelligence);mobile robots;motion control;multi-robot systems;path planning;reinforcement learning","deep reinforcement learning;three-wheeled omnidirectional mobile robot;holonomic mobile robot;nonholonomic mobile robot;multirobot formation control problem;deep deterministic policy gradient algorithm;motion planning;artificial potential field algorithm;DDPG algorithm;APF controller;TWODMR","","1","","9","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"From Scratch to Sketch: Deep Decoupled Hierarchical Reinforcement Learning for Robotic Sketching Agent","G. Lee; M. Kim; M. Lee; B. -T. Zhang","Interdisciplinary Program in Cognitive Science, Seoul National University, Seoul, Korea; Interdisciplinary Program in Neuroscience, Seoul National University; AIIS, Seoul National University; Dept. of Computer Science and Engineering, Seoul National University","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","5553","5559","We present an automated learning framework for a robotic sketching agent that is capable of learning stroke-based rendering and motor control simultaneously. We formulate the robotic sketching problem as a deep decoupled hierarchical reinforcement learning; two policies for stroke-based rendering and motor control are learned independently to achieve sub-tasks for drawing, and form a hierarchy when cooperating for real-world drawing. Without hand-crafted features, drawing sequences or trajectories, and inverse kinematics, the proposed method trains the robotic sketching agent from scratch. We performed experiments with a 6-DoF robot arm with 2F gripper to sketch doodles. Our experimental results show that the two policies successfully learned the sub-tasks and collaborated to sketch the target images. Also, the robustness and flexibility were examined by varying drawing tools and surfaces.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811858","IITP(grant numbers:2018-0-00622-RMI/20%,2019-0-01371-BabyMind/20%,2015-0-00310-SW.StarLab/10%,2021-0-02068-AIHub/10%); KIAT(grant numbers:P0006720-ILIAS/10%); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811858","","Motor drives;Automation;Reinforcement learning;Kinematics;Rendering (computer graphics);Manipulators;Robustness","control engineering computing;feature extraction;grippers;reinforcement learning;rendering (computer graphics)","deep decoupled hierarchical reinforcement learning;robotic sketching agent;automated learning framework;stroke-based rendering;motor control;robotic sketching problem;robot arm;drawing tools;gripper;6-DoF robot arm","","1","","32","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Distributed Circle Formation Control for Quadrotors Based on Multi-agent Deep Reinforcement Learning","B. Li; S. Li; C. Wang; R. Fan; J. Shao; G. Xie","College of Engineering, Peking University, Beijing, China; College of Engineering, Peking University, Beijing, China; National Engineering Research Center of Software Engineering, Peking University, Beijing, China; College of Engineering, Peking University, Beijing, China; College of Engineering, Peking University, Beijing, China; College of Engineering, Peking University, Beijing, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","4750","4755","Multi-rotor unmanned aerial vehicles (UAVs) have been increasingly used in both civil and military applications in recent years, and it is difficult for a single multi-rotor UAV to accomplish a specific task due to the load and range limitations. In this paper, we take the most commonly used quadrotor among multi-rotor UAVs as the research object, and study the formation control problems such as formation generation, formation transformation and collision avoidance. We design a distributed circle formation algorithm based on deep reinforcement learning, which is combined with an artificial potential based law to ensure collision avoidance. In order to simulate the real environments, a multi-quadrotor simulator is built based on ROS and gazebo. The algorithm is trained and tested in the simulator, whose results show the effectiveness of the proposed algorithm.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727879","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727879","quadrotor;formation;deep reinforcement learning;distributed","Automation;Reinforcement learning;Autonomous aerial vehicles;Collision avoidance;Task analysis","autonomous aerial vehicles;collision avoidance;helicopters;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;remotely operated vehicles","formation transformation;collision avoidance;distributed circle formation algorithm;artificial potential based law;multiquadrotor simulator;distributed circle formation control;quadrotors;multiagent deep reinforcement learning;multirotor unmanned aerial vehicles;UAVs;civil applications;military applications;multirotor UAV;range limitations;commonly used quadrotor;formation control problems;formation generation","","1","","25","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Real-Robot Deep Reinforcement Learning: Improving Trajectory Tracking of Flexible-Joint Manipulator with Reference Correction","D. Pavlichenko; S. Behnke","Autonomous Intelligent Systems (AIS) Group, Computer Science Institute VI, University of Bonn, Germany; Autonomous Intelligent Systems (AIS) Group, Computer Science Institute VI, University of Bonn, Germany","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","2671","2677","Flexible-joint manipulators are governed by complex nonlinear dynamics, defining a challenging control problem. In this work, we propose an approach to learn an outer-loop joint trajectory tracking controller with deep reinforcement learning. The controller represented by a stochastic policy is learned in under two hours directly on the real robot. This is achieved through bounded reference correction actions and use of a model-free off-policy learning method. In addition, an informed policy initialization is proposed, where the agent is pre-trained in a learned simulation. We test our approach on the 7 DOF manipulator of a Baxter robot. We demonstrate that the proposed method is capable of consistent learning across multiple runs when applied directly on the real robot. Our method yields a policy which significantly improves the trajectory tracking accuracy in comparison to the vendor-provided controller, generalizing to an unseen payload.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812023","German Research Foundation(grant numbers:BE 2556/16-2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812023","","Learning systems;Automation;Trajectory tracking;Stochastic processes;Reinforcement learning;Nonlinear dynamical systems;Behavioral sciences","adaptive control;control system synthesis;flexible manipulators;learning (artificial intelligence);manipulator dynamics;manipulators;nonlinear control systems;position control;tracking;trajectory control","robot deep reinforcement learning;improving trajectory tracking;flexible-joint manipulator;complex nonlinear dynamics;challenging control problem;outer-loop joint trajectory;stochastic policy;bounded reference correction actions;model-free off-policy learning method;informed policy initialization;learned simulation;7 DOF manipulator;Baxter robot;consistent learning;trajectory tracking accuracy;vendor-provided controller","","1","","33","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Exploiting Abstract Symmetries in Reinforcement Learning for Complex Environments","K. Gupta; H. Najjaran","School of Engineering, The University of British Columbia, Kelowna, BC, Canada; School of Engineering, The University of British Columbia, Kelowna, BC, Canada","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","3631","3637","Reinforcement Learning is rapidly establishing itself as the foremost choice for optimization of sequential autonomous decision-making problems. Encumbered by its sample inefficiency, the extension of the field to large state space and dynamic environments remains an open problem. We present a novel concept that exploits abstract spatial symmetry in complex environments for extending the skills of naïvely trained agents in local abstractions of the environment. The concept of EASE (Exploitation of Abstract Symmetry of Environments), when incorporated, improves the sample efficiency of traditional reinforcement learning algorithms. The presented work exemplifies the concept of EASE by presenting three distinct settings; EASE with heuristics-based planning, EASE with learning from demonstrations and EASE with state-space abstraction and proposes a novel algorithm for each setting.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811652","Natural Sciences and Engineering Research Council (NSERC) Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811652","","Training;Technological innovation;Automation;Navigation;Heuristic algorithms;Decision making;Reinforcement learning","complex networks;data structures;decision making;large-scale systems;optimisation;reinforcement learning;search problems","abstract symmetries;complex environments;sequential autonomous decision-making problems;dynamic environments;naïvely trained agents;traditional reinforcement learning algorithms;state-space abstraction;abstract spatial symmetry;EASE;heuristics-based planning","","1","","31","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Deep reinforcement learning in POMDPs for 3-D palletization problem","A. Bo; J. Lu; C. Zhao","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","577","582","Online 3D palletization problem is a generic variant in the family of bin packing problem (BPP). However, conventional deep reinforcement learning (DRL) methods merely have an excellent performance on combinatorial optimization problem modeled as Markov decision process (MDP). Since online BPP only provides information on fragments in successive items sequence, it is hard to describe online 3D palletization problem as MDP. Thereby, we formulated online 3D palletization problem as partially observable Markov decision processes (POMDPs) and proposed a novel DRL method to estimate state with observations trajectories. We also devised a DRL framework and train agents on environments with different boxes types. The result shows that our method is effective in a range of experimental settings and achieves higher space utilization than conventional heuristic algorithms.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10054950","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054950","bin packing problem (BPP);deep reinforcement learning (DRL);partially observable Markov decision process (POMDP)","Deep learning;Three-dimensional displays;Automation;Heuristic algorithms;Reinforcement learning;Markov processes;Trajectory","bin packing;combinatorial mathematics;control engineering computing;deep learning (artificial intelligence);Markov processes;optimisation;palletising;reinforcement learning","bin packing problem;combinatorial optimization problem;deep reinforcement learning methods;DRL method;heuristic algorithms;MDP;online 3D palletization problem;online BPP;partially observable Markov decision processes;POMDP","","1","","34","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Group and Socially Aware Multi-Agent Reinforcement Learning","M. Vallecha; R. Kala","Centre of Intelligent Robotics, Indian Institute of Information Technology, Allahabad; Centre of Intelligent Robotics, Indian Institute of Information Technology, Allahabad","2022 30th Mediterranean Conference on Control and Automation (MED)","1 Aug 2022","2022","","","73","78","Many researches in the field of robot navigation show the effectiveness of Deep Reinforcement Learning and Reward Function Modeling for Crowd Navigation and Multi-Agent Reinforcement Learning. The notion of groups has not yet been studied in the context of Reinforcement Learning. A robot using the current approaches is likely to walk in-between a group of people, while a robot moving alongside with a group of people is unlikely to make an extra effort to avoid group splitting when avoiding other people. We learn the behavior of multiple-robots to be group-aware to avoid breaking of the groups, while also being-socially aware to leave comforting personal space from the other people. The work uses Imitation Learning on a dataset produced by using the Social Potential Field algorithm to kick start the learning of the Reinforcement Learning policy. The learning is facilitated by the reward function that is specifically modelled to learn the desired behaviours. The proposed work is compared against the Artificial Potential Field Algorithm, Social Potential Field Algorithm, Optimal Reciprocal Collision Avoidance and Reinforcement Learning baselines and found to be the best among all these approaches.","2473-3504","978-1-6654-0673-4","10.1109/MED54222.2022.9837206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837206","","Legged locomotion;Automation;Navigation;Data visualization;Reinforcement learning;Behavioral sciences;Collision avoidance","collision avoidance;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems","robot navigation;Deep Reinforcement Learning;Reward Function Modeling;Crowd Navigation;MultiAgent Reinforcement Learning;group splitting;multiple-robots;group-aware;Imitation Learning;Social Potential Field algorithm;Reinforcement Learning policy;Artificial Potential Field Algorithm;Social Potential Field Algorithm;Reinforcement Learning baselines","","1","","16","IEEE","1 Aug 2022","","","IEEE","IEEE Conferences"
"Deep Drifting: Autonomous Drifting of Arbitrary Trajectories using Deep Reinforcement Learning","F. Domberg; C. C. Wembers; H. Patel; G. Schildbach","Institute for Electrical Engineering in Medicine, University of Lübeck, Lübeck, Germany; Institute for Electrical Engineering in Medicine, University of Lübeck, Lübeck, Germany; Institute for Electrical Engineering in Medicine, University of Lübeck, Lübeck, Germany; Institute for Electrical Engineering in Medicine, University of Lübeck, Lübeck, Germany","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","7753","7759","In this paper, a Deep Neural Network is trained using Reinforcement Learning in order to drift on arbitrary trajectories which are defined by a sequence of waypoints. In a first step, a highly accurate vehicle simulation is used for the training process. Then, the obtained policy is refined and validated on a self-built model car. The chosen reward function is inspired by the scoring process of real life drifting competitions. It is kept simple and thus applicable to very general scenarios. The experimental results demonstrate that a relatively small network, given only a few measurements and control inputs, already achieves an outstanding performance. In simulation, the learned controller is able to reliably hold a steady state drift. Moreover, it is capable of generalizing to arbitrary, previously unknown trajectories and different driving conditions. After transferring the learned controller to the model car, it also performs surprisingly well given the physical constraints.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812249","","Training;Deep learning;Automation;Neural networks;Reinforcement learning;Trajectory;Steady-state","deep learning (artificial intelligence);driver information systems;neural nets;reinforcement learning","autonomous drifting;arbitrary trajectories;deep neural network;training process;scoring process;life drifting competitions;control inputs;learned controller;steady state drift;deep drifting;deep reinforcement learning;reward function","","","","35","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"An intelligent control approach for heavy haul trains using deep reinforcement learning","W. Liu; S. Su; T. Tang","State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China","2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)","5 Oct 2021","2021","","","2262","2267","One of the main challenges for the control of the heavy haul train of China is the cyclic air braking strategy on the long steep downward slopes. To address this problem, this paper proposes an intelligent control approach using a deep reinforcement learning algorithm to achieve safe operation, low maintenance costs and high running efficiency. The train control problem is firstly described considering the characteristics of the heavy haul railways of China. Then the cyclic air braking strategy is defined as a Markov decision process (MDP) and the key elements in the reinforcement learning framework are designed. To reduce the overestimation of action values in the Deep-Q-Network (DQN) based method, the Double DQN (DDQN) algorithm is used to solve the train control problem in the paper. The simulation experiments are conducted based on the real-word data of Shuozhou-Huanghua Line and the effectiveness of the DDQN-based approach is illustrated by comparing the performances of the proposed approach with those of the DQN-based method.","2161-8089","978-1-6654-1873-7","10.1109/CASE49439.2021.9551392","National Natural Science Foundation of China(grant numbers:61803021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551392","Heavu haul train;Cyclic air braking strategy;Double DQN","Costs;Computer aided software engineering;Automation;Conferences;Reinforcement learning;Markov processes;Maintenance engineering","braking;freight containers;intelligent control;learning (artificial intelligence);Markov processes;railways;vehicle dynamics","Shuozhou-Huanghua Line;MDP;Markov decision process;China;DDQN-based approach;Deep-Q-Network based method;train control problem;high running efficiency;low maintenance costs;deep reinforcement learning algorithm;long steep downward slopes;cyclic air braking strategy;heavy haul trains;intelligent control approach","","","","21","IEEE","5 Oct 2021","","","IEEE","IEEE Conferences"
"Option-Aware Adversarial Inverse Reinforcement Learning for Robotic Control","J. Chen; T. Lan; V. Aggarwal","School of Industrial Engineering, Purdue University, West Lafayette, IN, USA; Department of Electrical and Computer Engineering, George Washington University, Washington D.C., USA; CS Department, KAUST, Thuwal, Saudi Arabia","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5902","5908","Hierarchical Imitation Learning (HIL) has been proposed to recover highly-complex behaviors in long-horizon tasks from expert demonstrations by modeling the task hierarchy with the option framework. Existing methods either overlook the causal relationship between the subtask and its corresponding policy or cannot learn the policy in an end-to-end fashion, which leads to suboptimality. In this work, we develop a novel HIL algorithm based on Adversarial Inverse Reinforcement Learning and adapt it with the Expectation-Maximization algorithm in order to directly recover a hierarchical policy from the unannotated demonstrations. Further, we introduce a directed information term to the objective function to enhance the causality and propose a Variational Autoencoder framework for learning with our objectives in an end-to-end fashion. Theoretical justifications and evaluations on challenging robotic control tasks are provided to show the superiority of our algorithm. The codes are available at https://github.com/LucasCJYSDL/HierAIRL.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160374","","Codes;Automation;Reinforcement learning;Solids;Multitasking;Linear programming;Behavioral sciences","control engineering computing;expectation-maximisation algorithm;reinforcement learning;robots","expectation-maximization algorithm;expert demonstrations;hierarchical imitation learning;hierarchical policy;HIL algorithm;long-horizon tasks;option-aware adversarial inverse reinforcement learning;robotic control tasks;unannotated demonstrations;variational autoencoder framework","","","","43","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Dynamics-Aware Context Representation for Domain Adaptation in Reinforcement Learning","K. Deng; B. Chen; L. Li","School of vehicle and mobility, Tsinghua University, Beijing, China; School of vehicle and mobility, Tsinghua University, Beijing, China; School of vehicle and mobility, Tsinghua University, Beijing, China","2022 7th International Conference on Robotics and Automation Engineering (ICRAE)","8 Mar 2023","2022","","","406","413","A context can be used as an embedding extracted from historical trajectories of dynamic systems to provide meaningful information for reinforcement learning (RL) agents, thus improving the domain adaptability and robustness of RL method. However, the process of context extraction involves two key issues: How to efficiently train an encoder to extract context information from historical trajectories? And how to ensure that context information can distinguish different dynamics clearly? To tackle the problems above, a dynamics-aware context representation reinforcement learning (DacRL) is proposed in this study. We leverage the Cycle-Consistent VAE method to extract a meaningful context from historical trajectories and then divide it into domain-specific and domain-general embedding. Furthermore, we consider the contrastive nature between different tasks and use it to improve the quality of domain-specific information, so that it can represent dynamics more clearly. Finally, the current state combined with the domain-specific information is delivered into the RL agent, so as to improve the generalization of the RL agent. The simulation results illustrate that the proposed DacRL is superior to other baselines.","","978-1-6654-8918-8","10.1109/ICRAE56463.2022.10056207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10056207","domain adaptation;reinforcement learning;representation learning","Training;Automation;Simulation;Reinforcement learning;Robustness;Trajectory;Data mining","deep learning (artificial intelligence);learning (artificial intelligence);reinforcement learning;telecommunication computing","context extraction;context information;Cycle-Consistent VAE method;domain adaptability;domain adaptation;domain-general embedding;domain-specific information;dynamic systems;dynamics-aware context representation reinforcement learning;historical trajectories;meaningful context;reinforcement learning agents;RL agent;RL method;robustness","","","","30","IEEE","8 Mar 2023","","","IEEE","IEEE Conferences"
"Explainable Action Advising for Multi-Agent Reinforcement Learning","Y. Guo; J. Campbell; S. Stepputtis; R. Li; D. Hughes; F. Fang; K. Sycara","Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5515","5521","Action advising is a knowledge transfer technique for reinforcement learning based on the teacher-student paradigm. An expert teacher provides advice to a student during training in order to improve the student's sample efficiency and policy performance. Such advice is commonly given in the form of state-action pairs. However, it makes it difficult for the student to reason with and apply to novel states. We introduce Explainable Action Advising, in which the teacher provides action advice as well as associated explanations indicating why the action was chosen. This allows the student to self-reflect on what it has learned, enabling advice generalization and leading to improved sample efficiency and learning performance - even in environments where the teacher is sub-optimal. We empirically show that our framework is effective in both single-agent and multi-agent scenarios, yielding improved policy returns and convergence rates when compared to state-of-the-art methods.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160557","DARPA(grant numbers:HR001120C0036); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160557","","Training;Automation;Reinforcement learning;Benchmark testing;Reflection;Knowledge transfer;Convergence","computer aided instruction;multi-agent systems;reinforcement learning","action advice;advice generalization;convergence rates;expert teacher;explainable action advising;improved policy returns;improved sample efficiency;knowledge transfer technique;learning performance;multiagent reinforcement learning;multiagent scenarios;state-action pairs;teacher-student paradigm","","","","35","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Promoting Quality and Diversity in Population-based Reinforcement Learning via Hierarchical Trajectory Space Exploration","J. Miao; T. Zhou; K. Shao; M. Zhou; W. Zhang; J. Hao; Y. Yu; J. Wang","Shanghai Jiao Tong University; Beijing Institute of Technology; Noah's Ark Lab, Huawei Technologies; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Noah's Ark Lab, Huawei Technologies; Shanghai Jiao Tong University; University College London","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","7544","7550","Quality Diversity (QD) algorithms in population-based reinforcement learning aim to optimize agents' returns and diversity among the population simultaneously. It is conducive to solving exploration problems in reinforcement learning and potentially getting multiple good and diverse strategies. However, previous methods typically define behavioral embedding in action space or outcome space, which neglect trajectory characteristics during the execution process. In this paper, we introduce a trajectory embedding model trained by Variational Autoencoder with similarity constraint to characterize trajectory features. Based on that, we propose a hierarchical trajectory-space exploration (HTSE) framework using Determinantal Point Processes (DPP) to generate high-quality and diverse solutions in the selection and mutation process. The experimental results show that our HTSE method effectively completes several simulated tasks, outperforming other Quality-Diversity Reinforcement Learning algorithms.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811888","","Automation;Sociology;Reinforcement learning;Trajectory;Space exploration;Behavioral sciences;Task analysis","neural nets;reinforcement learning","population-based reinforcement;hierarchical trajectory space exploration;agents;exploration problems;diverse strategies;behavioral embedding;action space;neglect trajectory characteristics;trajectory features;hierarchical trajectory-space exploration framework;quality-diversity reinforcement learning algorithms","","","","32","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Hierarchical Reinforcement Learning for In-hand Robotic Manipulation Using Davenport Chained Rotations","F. R. Sanchez; Q. Wang; D. C. Bulens; K. McGuinness; S. J. Redmond; N. E. O'Connor","Insight Centre for Data Analytics, Dublin City University, Dublin, Ireland; University College Dublin, Dublin, Ireland; University College Dublin, Dublin, Ireland; Insight Centre for Data Analytics, Dublin City University, Dublin, Ireland; Insight Centre for Data Analytics, University College Dublin, Dublin, Ireland; Insight Centre for Data Analytics, Dublin City University, Dublin, Ireland","2023 9th International Conference on Automation, Robotics and Applications (ICARA)","23 May 2023","2023","","","160","164","End-to-end reinforcement learning techniques are among the most successful methods for robotic manipulation tasks. However, the training time required to find a good policy capable of solving complex tasks is prohibitively large. Therefore, depending on the computing resources available, it might not be feasible to use such techniques. The use of domain knowledge to decompose manipulation tasks into primitive skills, to be performed in sequence, could reduce the overall complexity of the learning problem, and hence reduce the amount of training required to achieve dexterity. In this paper, we propose the use of Davenport chained rotations to decompose complex 3D rotation goals into a concatenation of a smaller set of more simple rotation skills. State-of-the-art reinforcement-learning-based methods can then be trained using less overall simulated experience. We compare this learning approach with the popular Hindsight Experience Replay method, trained in an end-to-end fashion using the same amount of experience in a simulated robotic hand environment. Despite a general decrease in performance of the primitive skills when being sequentially executed, we find that decomposing arbitrary 3D rotations into elementary rotations is beneficial when computing resources are limited, obtaining increases of success rates of approximately 10% on the most complex 3D rotations with respect to the success rates obtained by a HER-based approach trained in an end-to-end fashion, and increases of success rates between 20% and 40% on the most simple rotations.","2767-7745","978-1-6654-8921-8","10.1109/ICARA56516.2023.10125281","Science Foundation Ireland (SFI)(grant numbers:SFI/12/RC/2289_P2); European Regional Development Fund; China Scholarship Council (CSC)(grant numbers:202006540003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10125281","Robotic manipulation;deep reinforcement learning;hierarchical reinforcement learning","Training;Three-dimensional displays;Automation;Reinforcement learning;Companies;Complexity theory;Task analysis","dexterous manipulators;reinforcement learning","complex 3D rotation goals;computing resources;Davenport chained rotations;decomposing arbitrary 3D;elementary rotations;end-to-end reinforcement;hierarchical reinforcement;in-hand robotic manipulation;learning approach;learning problem;popular hindsight experience replay method;primitive skills;reinforcement-learning-based methods;robotic manipulation tasks;simple rotation skills;simple rotations;simulated experience;simulated robotic hand environment","","","","22","IEEE","23 May 2023","","","IEEE","IEEE Conferences"
"Exploration via Distributional Reinforcement Learning with Epistemic and Aleatoric Uncertainty Estimation","Q. Liu; Y. Li; Y. Liu; M. Chen; S. Lv; Y. Xu","Department of Control Science and Engineering, Harbin Institute of Technology, Shenzhen, China; Department of Control Science and Engineering, Harbin Institute of Technology, Shenzhen, China; Department of Control Science and Engineering, Harbin Institute of Technology, Shenzhen, China; Department of Control Science and Engineering, Harbin Institute of Technology, Shenzhen, China; Department of Control Science and Engineering, Harbin Institute of Technology, Shenzhen, China; Department of Control Science and Engineering, Harbin Institute of Technology, Shenzhen, China","2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)","5 Oct 2021","2021","","","2256","2261","The problem of exploration remains one of the major challenges in deep reinforcement learning (RL). This paper proposes an approach to improve the exploration efficiency for distributional RL. First, this paper proposes a novel method to estimate the epistemic and aleatoric uncertainty for distributional RL using deep ensembles, which is inspired by Bayesian Deep Learning. Second, This paper presents a method to improve the exploration efficiency for deep distributional RL by using estimated epistemic uncertainty. Experimental results show that the proposed approach outperforms the baseline in Atari games.","2161-8089","978-1-6654-1873-7","10.1109/CASE49439.2021.9551544","National Natural Science Foundation(grant numbers:U1813206,61977019); Shenzhen basic research program(grant numbers:JCYJ20180507183837726); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551544","","Deep learning;Uncertainty;Computer aided software engineering;Automation;Conferences;Estimation;Reinforcement learning","hydrological techniques;learning (artificial intelligence)","deep distributional RL;Bayesian Deep Learning;deep ensembles;aleatoric uncertainty;epistemic uncertainty;exploration efficiency;deep reinforcement learning;distributional reinforcement Learning","","","","25","IEEE","5 Oct 2021","","","IEEE","IEEE Conferences"
"Active Predictive Coding: Brain-Inspired Reinforcement Learning for Sparse Reward Robotic Control Problems","A. Ororbia; A. Mali","Department of Computer Science, Rochester Institute of Technology, Rochester, NY, USA; Department of Computer Science and Engineering, University of South Florida, Tampa, FL, USA","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","3015","3021","In this article, we propose a backpropagation-free approach to robotic control through the neuro-cognitive computational framework of neural generative coding (NGC), designing an agent completely built from predictive processing circuits that facilitate dynamic, online learning from sparse rewards, embodying the principles of planning-as-inference. Concretely, we craft an adaptive agent system, which we call active predictive coding (ActPC), that balances an internally-generated epistemic signal (meant to encourage intelligent exploration) with an internally-generated instrumental signal (meant to encourage goal-seeking behavior) to learn how to control various simulated robotic systems as well as a complex robotic arm using a realistic simulator, i.e., the Surreal Robotics Suite, for the block lifting task and the can pick-and-place problem. Notably, our results demonstrate that the proposed ActPC agent performs well in the face of sparse (extrinsic) reward signals and is competitive with or outperforms several powerful backpropagation-based reinforcement learning approaches.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160530","","Automation;Instruments;Process control;Reinforcement learning;Predictive coding;Manipulators;Encoding","backpropagation;learning (artificial intelligence);mobile robots;reinforcement learning","-place problem;active predictive coding;ActPC agent;adaptive agent system;backpropagation-free approach;brain-inspired reinforcement learning;complex robotic arm;dynamic learning;epistemic signal;instrumental signal;neural generative coding;neuro-cognitive computational framework;online learning;pick- place problem;planning-as-inference;powerful backpropagation-based reinforcement learning approaches;predictive processing circuits;simulated robotic systems;sparse reward robotic control problems;sparse reward signals;sparse rewards;Surreal Robotics Suite","","","","34","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Multi-Target Pursuit by a Decentralized Heterogeneous UAV Swarm using Deep Multi-Agent Reinforcement Learning","M. Kouzeghar; Y. Song; M. Meghjani; R. Bouffanais","Singapore University of Technology and Design, Singapore; Singapore University of Technology and Design, Singapore; Singapore University of Technology and Design, Singapore; University of Ottawa, Canada","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","3289","3295","Multi-agent pursuit-evasion tasks involving intelligent targets are notoriously challenging coordination problems. In this paper, we investigate new ways to learn such coordinated behaviors of unmanned aerial vehicles (UAVs) aimed at keeping track of multiple evasive targets. Within a Multi-Agent Reinforcement Learning (MARL) framework, we specifically propose a variant of the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) method. Our approach addresses multi-target pursuit-evasion scenarios within non-stationary and unknown environments with random obstacles. In addition, given the critical role played by collective exploration in terms of detecting possible targets, we implement heterogeneous roles for the pursuers for enhanced exploratory actions balanced by exploitation (i.e. tracking) of previously identified targets. Our proposed role-based MADDPG algorithm is not only able to track multiple targets, but also is able to explore for possible targets by means of the proposed Voronoi-based rewarding policy. We implemented, tested and validated our approach in a simulation environment prior to deploying a real-world multi-robot system comprising of Crazyflie drones. Our results demonstrate that a multi-agent pursuit team has the ability to learn highly efficient coordinated control policies in terms of target tracking and exploration even when confronted with multiple fast evasive targets in complex environments.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160919","National Research Foundation Singapore(grant numbers:AISG2-RP-2020-016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160919","","Target tracking;Automation;Reinforcement learning;Autonomous aerial vehicles;Behavioral sciences;Multi-robot systems;Task analysis","autonomous aerial vehicles;control engineering computing;deep learning (artificial intelligence);gradient methods;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;reinforcement learning;remotely operated vehicles;target tracking;telecommunication computing;trajectory control","coordinated behaviors;coordination problems;decentralized heterogeneous UAV swarm;Deep MultiAgent Reinforcement Learning;heterogeneous roles;highly efficient coordinated control policies;identified targets;intelligent targets;MultiAgent Deep Deterministic Policy Gradient method;multiagent pursuit team;Multiagent pursuit-evasion tasks;MultiAgent Reinforcement Learning framework;multiple evasive targets;multiple fast evasive targets;multitarget pursuit-evasion scenarios;possible targets;real-world multirobot system comprising;role-based MADDPG algorithm;target tracking;track multiple targets;UAVs;unmanned aerial vehicles","","","","36","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"A Continuous Off-Policy Reinforcement Learning Scheme for Optimal Motion Planning in Simply-Connected Workspaces","P. Rousseas; C. P. Bechlioulis; K. J. Kyriakopoulos","School of Mechanical Engineering, Control Systems Laboratory, National Technical University of Athens, Greece; Department of Electrical and Computer Engineering, University of Patras; Center of AI & Robotics (CAIR), New York University, Abu Dhabi","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","10247","10253","In this work, an Integral Reinforcement Learning (RL) framework is employed to provide provably safe, convergent and almost globally optimal policies in a novel Off-Policy Iterative method for simply-connected workspaces. This restriction stems from the impossibility of strictly global navigation in multiply connected manifolds, and is necessary for formulating continuous solutions. The current method generalizes and improves upon previous results, where parametrized controllers hindered the method in scope and results. Through enhancing the traditional reactive paradigm with RL, the proposed scheme is demonstrated to outperform both previous reactive methods as well as an RRT* method in path length, cost function values and execution times, indicating almost global optimality.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161189","","Manifolds;Automation;Navigation;Reinforcement learning;Cost function;Planning;Iterative methods","iterative methods;learning (artificial intelligence);mobile robots;optimisation;path planning;reinforcement learning","current method generalizes;formulating continuous solutions;global optimality;Integral Reinforcement Learning framework;multiply connected manifolds;Off-Policy Iterative method;Off-Policy Reinforcement Learning scheme;optimal motion planning;optimal policies;previous reactive methods;RL;RRT* method;simply-connected workspaces;strictly global navigation","","","","34","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Application of Soft Actor-Critic Reinforcement Learning to a Search and Rescue Task for Humanoid Robots","H. Ji; C. Yin","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","3954","3960","This paper proposes a novel maximum entropy based reinforcement learning for dealing with a robotic search and rescue task in a complex enclosed environment. The search and rescue task is described as a Markov Decision Process, under which an auxiliary reward function at multiple stages is designed for the robot and its interaction with the specified environment. A variant of the state-of-art reinforcement learning algorithm, goal-based Soft Actor-Critic (SAC), is developed to train a humanoid robot. Simulation results verify the effectiveness of the proposed goal-based SAC algorithm and its advantages comparing with the prototype of SAC algorithm for the same task.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10056003","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10056003","reinforcement learning;soft actor-critic;search and rescue;humanoid robot","Training;Automation;Simulation;Humanoid robots;Prototypes;Reinforcement learning;Markov processes","entropy;humanoid robots;learning (artificial intelligence);Markov processes;reinforcement learning","auxiliary reward function;complex enclosed environment;goal-based Soft Actor-Critic;humanoid robot;Markov Decision Process;novel maximum entropy based reinforcement;rescue task;robotic search;Soft Actor-Critic reinforcement;specified environment;state-of-art reinforcement","","","","16","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Minimizing Human Assistance: Augmenting a Single Demonstration for Deep Reinforcement Learning","A. George; A. Bartsch; A. B. Farimani","Department of Mechanical Engineering, Carnegie Mellon University; Department of Mechanical Engineering, Carnegie Mellon University; Department of Mechanical Engineering, Carnegie Mellon University","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5027","5033","The use of human demonstrations in reinforcement learning has proven to significantly improve agent performance. However, any requirement for a human to manually ‘teach’ the model is somewhat antithetical to the goals of reinforcement learning. This paper attempts to minimize human involvement in the learning process while retaining the performance advantages by using a single human example collected through a simple-to-use virtual reality simulation to assist with RL training. Our method augments a single demonstration to generate numerous human-like demonstrations that, when combined with Deep Deterministic Policy Gradients and Hindsight Experience Replay (DDPG + HER) significantly improve training time on simple tasks and allows the agent to solve a complex task (block stacking) that DDPG + HER alone cannot solve. The model achieves this significant training advantage using a single human example, requiring less than a minute of human input. Moreover, despite learning from a human example, the agent is not constrained to human-level performance, often learning a policy that is significantly different from the human demonstration.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161119","","Training;Deep learning;Solid modeling;Automation;Stacking;Reinforcement learning;Virtual reality","control engineering computing;deep learning (artificial intelligence);gradient methods;learning (artificial intelligence);reinforcement learning;virtual reality","agent performance;Deep Deterministic Policy Gradients;Deep reinforcement learning;human demonstration;human input;human involvement;human-level performance;learning process;minimizing human assistance;RL training;significant training advantage;simple-to-use virtual reality simulation;single demonstration;single human example","","","","35","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Assisted Management for Convertible SSDs","Q. Wei; Y. Li; Z. Jia; M. Zhao; Z. Shen; B. Li","School of Computer Science and Technology, Shandong University, Qingdao, China; School of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK, USA; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK, USA","2023 60th ACM/IEEE Design Automation Conference (DAC)","15 Sep 2023","2023","","","1","6","Convertible SSDs, which allow flash cells to convert between different types of flash cells (e.g., SLC/MLC/TLC/QLC), are designed for achieving both high performance and high density. However, previous designs with two types of flash cells encounter a performance cliff degradation once the flash cells of single bit mode are consumed. In this work, we propose a novel level-based convertible SSD (e.g., including SLC-MLC-QLC), named RL-cSSD, that adopts an intermediate layer (e.g., MLC) as a performance cushion. A reinforcement learning-assisted device management scheme is designed to coordinate the data allocation, garbage collection and flash conversion processes considering both the SSD internal status and workload patterns. We evaluated RL-cSSD with various real-world workloads based on simulation. The experimental results show that the proposed RL-cSSD provides 72.98% higher performance on average compared with state-of-the-art schemes.","","979-8-3503-2348-1","10.1109/DAC56929.2023.10247929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247929","Flash memory;Convertible SSD;Reinforcement learning","Performance evaluation;Degradation;Q-learning;Design automation;Throughput;Real-time systems;Resource management","flash memories;reinforcement learning;solid state drives;storage management","data allocation;flash conversion processes;garbage collection;novel level-based convertible SSD;performance cliff degradation;reinforcement learning-assisted device management scheme;RL-cSSD;single bit mode flash cells;SLC-MLC-QLC;SSD internal status;workload patterns","","","","17","IEEE","15 Sep 2023","","","IEEE","IEEE Conferences"
"SoLo T-DIRL: Socially-Aware Dynamic Local Planner based on Trajectory-Ranked Deep Inverse Reinforcement Learning","Y. Xu; T. Chakhachiro; T. Kathuria; M. Ghaffari","University of Michigan, Ann Arbor, MI, USA; University of Michigan, Ann Arbor, MI, USA; University of Michigan, Ann Arbor, MI, USA; University of Michigan, Ann Arbor, MI, USA","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","12045","12051","This work proposes a novel framework for socially-aware robot navigation in dynamic, crowded environments using a Deep Inverse Reinforcement Learning. To address the social navigation problem, our multi-modal learning based planner explicitly considers social interaction factors, as well as social-awareness factors, into the DIRL pipeline to learn a reward function from human demonstrations. Moreover, we propose a novel trajectory ranking score using the sudden velocity change of pedestrians around the robot to address the sub-optimality in human demonstrations. Our evaluation shows that this method can successfully make a robot navigate in a crowded social environment and outperforms the state-of-art social navigation methods in terms of the success rate, navigation time, and invasion rate.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160536","NSF(grant numbers:2118818); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160536","","Visualization;Pedestrians;Automation;Navigation;Pipelines;Human-robot interaction;Reinforcement learning","deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;navigation;path planning;pedestrians;reinforcement learning;traffic engineering computing","crowded environments;crowded social environment;DIRL pipeline;dynamic environments;human demonstrations;multimodal learning based planner;navigation time;robot navigate;social interaction factors;social navigation problem;social-awareness factors;socially-aware dynamic local planner;socially-aware robot navigation;SoLo t-DIRL;state-of-art social navigation methods;trajectory-ranked Deep Inverse Reinforcement Learning","","","","44","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Demonstration-Bootstrapped Autonomous Practicing via Multi-Task Reinforcement Learning","A. Gupta; C. Lynch; B. Kinman; G. Peake; S. Levine; K. Hausman",UC Berkeley; Robotics at Google; Robotics at Google; Robotics at Google; UC Berkeley; Robotics at Google,"2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5020","5026","Reinforcement learning systems have the potential to enable continuous improvement in unstructured environments, leveraging data collected autonomously. However, in practice these systems require significant amounts of instrumentation or human intervention to learn in the real world. In this work, we propose a system for reinforcement learning that leverages multi-task reinforcement learning bootstrapped with prior data to enable continuous autonomous practicing, minimizing the number of resets needed while being able to learn temporally extended behaviors. We show how appropriately provided prior data can help bootstrap both low-level multi-task policies and strategies for sequencing these tasks one after another to enable learning with minimal resets. This mechanism enables our robotic system to practice with minimal human intervention at training time, while being able to solve long horizon tasks at test time. We show the efficacy of the proposed system on a challenging kitchen manipulation task both in simulation and the real world, demonstrating the ability to practice autonomously in order to solve temporally extended problems.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161447","","Training;Sequential analysis;Automation;Instruments;Reinforcement learning;Multitasking;Behavioral sciences","learning (artificial intelligence);learning systems;mobile robots;reinforcement learning","appropriately provided prior data;challenging kitchen manipulation task;continuous autonomous practicing;continuous improvement;demonstration-bootstrapped autonomous practicing;long horizon tasks;low-level multitask policies;minimal human intervention;minimal resets;multitask reinforcement;practice these systems;reinforcement learning systems;robotic system;unstructured environments","","","","40","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Pedestrian Avoidance with and Without Incoming Traffic by Using Deep Reinforcement Learning","D. Guan; S. Xu; Q. Liu; J. Ma","Advanced Algorithms Research Center, China Nanhu Academy of Electronics and Information Technology, Jiaxing, China; Advanced Algorithms Research Center, China Nanhu Academy of Electronics and Information Technology, Jiaxing, China; Advanced Algorithms Research Center, China Nanhu Academy of Electronics and Information Technology, Jiaxing, China; Advanced Algorithms Research Center, China Nanhu Academy of Electronics and Information Technology, Jiaxing, China","2021 6th International Conference on Robotics and Automation Engineering (ICRAE)","4 Jan 2022","2021","","","244","249","Pedestrian avoidance is one of the most challenging autonomous driving operations in the field of intelligent vehicles. In an emergency, the optimal maneuver is to steer to avoid pedestrians and other vehicles. In this paper, a deep reinforcement learning based method has been proposed, in which the agent is trained to maneuver the ego vehicle steering away from the pedestrian with a safety clearance to the adjacent lane. A scenario both with and without other traffic have been investigated. By using TensorFlow as the learning framework and Unity3D to model the environment and different scenarios, the agent has been trained to obtain the maximum reward and take optimal policy in the process of continuously interacting with the environment. The capsule tool in Unity can ensure that the agent would keep the ego vehicle a safe distance from pedestrians during the training. The success rate of the trained agent in different conditions have proven the effectiveness of the proposed approach.","","978-1-6654-0697-0","10.1109/ICRAE53653.2021.9657771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9657771","deep reinforcement learning;TensorFlow;Unity3D;pedestrian avoidance;steering","Training;Solid modeling;Three-dimensional displays;Monte Carlo methods;Automation;Reinforcement learning;Trajectory","control engineering computing;deep learning (artificial intelligence);mobile robots;pedestrians;reinforcement learning;road safety;road traffic control;road vehicles;solid modelling;traffic engineering computing","pedestrian avoidance;intelligent vehicles;optimal maneuver;ego vehicle;optimal policy;deep reinforcement learning;autonomous driving operations;safety clearance;adjacent lane;TensorFlow;Unity3D","","","","20","IEEE","4 Jan 2022","","","IEEE","IEEE Conferences"
"Handling Sparse Rewards in Reinforcement Learning Using Model Predictive Control","M. Dawood; N. Dengler; J. de Heuvel; M. Bennewitz","Humanoid Robots Lab., University of Bonn, Germany; Humanoid Robots Lab., University of Bonn, Germany; Humanoid Robots Lab., University of Bonn, Germany; Humanoid Robots Lab., University of Bonn, Germany","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","879","885","Reinforcement learning (RL) has recently proven great success in various domains. Yet, the design of the reward function requires detailed domain expertise and tedious fine-tuning to ensure that agents are able to learn the desired behaviour. Using a sparse reward conveniently mitigates these challenges. However, the sparse reward represents a challenge on its own, often resulting in unsuccessful training of the agent. In this paper, we therefore address the sparse reward problem in RL. Our goal is to find an effective alternative to reward shaping, without using costly human demonstrations, that would also be applicable to a wide range of domains. Hence, we propose to use model predictive control (MPC) as an experience source for training RL agents in sparse reward environments. Without the need for reward shaping, we successfully apply our approach in the field of mobile robot navigation both in simulation and real-world experiments with a Kuboki Turtlebot 2. We furthermore demonstrate great improvement over pure RL algorithms in terms of success rate as well as number of collisions and timeouts. Our experiments show that MPC as an experience source improves the agent's learning process for a given task in the case of sparse rewards.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161492","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161492","","Training;Automation;Navigation;Reinforcement learning;Prediction algorithms;Mobile robots;Collision avoidance","control engineering computing;learning (artificial intelligence);mobile robots;path planning;predictive control;reinforcement learning","detailed domain expertise;model predictive control;reinforcement learning;reward function;reward shaping;sparse reward environments;sparse reward problem;training RL agents","","","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Guiding Reinforcement Learning with Shared Control Templates","A. Padalkar; G. Quere; F. Steinmetz; A. Raffin; M. Nieuwenhuisen; J. Silvério; F. Stulp","German Aerospace Center (DLR), Robotics and Mechatronics Center (RMC), Weßling, Germany; German Aerospace Center (DLR), Robotics and Mechatronics Center (RMC), Weßling, Germany; German Aerospace Center (DLR), Robotics and Mechatronics Center (RMC), Weßling, Germany; German Aerospace Center (DLR), Robotics and Mechatronics Center (RMC), Weßling, Germany; Fraunhofer Institute for Communication, Information Processing and Ergonomics FKIE, Wachtberg; German Aerospace Center (DLR), Robotics and Mechatronics Center (RMC), Weßling, Germany; German Aerospace Center (DLR), Robotics and Mechatronics Center (RMC), Weßling, Germany","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","11531","11537","Purposeful interaction with objects usually requires certain constraints to be respected, e.g. keeping a bottle upright to avoid spilling. In reinforcement learning, such constraints are typically encoded in the reward function. As a consequence, constraints can only be learned by violating them. This often precludes learning on the physical robot, as it may take many trials to learn the constraints, and the necessity to violate them during the trial-and-error learning may be unsafe. We have serendipitously discovered that constraint representations for shared control - in particular Shared Control Templates (SCTs) - are ideally suited for safely guiding RL. Representing constraints explicitly, rather than implicitly in the reward function, also simplifies the design of the reward function. The main advantage of the approach is safer, faster learning without constraint violations (even with sparse reward functions). We demonstrate this in a pouring task in simulation and on a real robot, where learning the task requires only 65 episodes in 16 minutes.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161058","","Automation;Dynamics;Reinforcement learning;Safety;Task analysis;Robots","control engineering computing;mobile robots;reinforcement learning;robot programming","constraint representations;constraint violations;physical robot;reinforcement learning;RL;SCT;shared control templates;sparse reward functions;trial-and-error learning","","","","26","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning with Probabilistically Safe Control Barrier Functions for Ramp Merging","S. Udatha; Y. Lyu; J. Dolan","Carnegie Mellon University, Pittsburgh, United States; Carnegie Mellon University, Pittsburgh, United States; Carnegie Mellon University, Pittsburgh, United States","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5625","5630","Prior work has looked at applying reinforcement learning (RL) approaches to autonomous driving scenarios, but the safety of the algorithm is often compromised due to instability or the presence of ill-defined reward functions. With the use of control barrier functions embedded into the RL policy, we arrive at safe policies to optimize the performance of the autonomous driving vehicle through the advantage of a safety layer over the RL methods to ease the design of reward functions. However, control barrier functions need a good approximation of the model of the system. We use probabilistic control barrier functions [4] to account for model uncertainty. Our Safety-Assured Policy Optimization - Ramp Merging (SAPO-RM) algorithm is implemented online in the CARLA [1] Simulator and offline on the US I-80 dataset extracted from the NGSIM Database provided by NHTSA [2]. We further test the algorithm and perform ablation studies of it on the US-101 and exi-D datasets to compare the approaches. The proposed algorithm can also be applied to other driving scenarios by changing the reward and safety constraints.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161418","","Uncertainty;Automation;Databases;Merging;Reinforcement learning;Probabilistic logic;Approximation algorithms","learning (artificial intelligence);reinforcement learning;road safety;road traffic;road vehicles;traffic engineering computing","autonomous driving scenarios;autonomous driving vehicle;ill-defined reward functions;probabilistic control barrier functions;probabilistically safe control barrier functions;reinforcement learning;RL methods;RL policy;safe policies;safety layer;Safety-Assured Policy Optimization - Ramp Merging","","","","35","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"A Pick-and-Throw Method for Enhancing Robotic Sorting Ability via Deep Reinforcement Learning","Z. Fang; Y. Hou; J. Li","Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, China; Shenzhen Research Institute, Southsast University, Shenzhen, China","2021 36th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","26 Jul 2021","2021","","","479","484","To promote the work capacity and efficiency in a weakly structured logistics sorting scene, a pick-and-throw method based on reinforcement learning is proposed. First, a D N-based learning algorithm is used to obtain a picking feasibility distribution map for guiding an optimal picking action. Second, Deep Deterministic Policy Gradient (DDPG) algorithm is utilized to train a throwing policy. The throwing policy outputs a throwing velocity of the end-effector of a robot to throw the picked object to a target area. In addition, a memory pool optimization algorithm is also proposed to enhance the convergence of the throwing policy. Experiments are conducted in both simulation and real scenarios and results demonstrate that the proposed method can improve the sorting efficiency and expand the sorting space significantly.","","978-1-6654-3712-7","10.1109/YAC53711.2021.9486466","National Natural Science Foundation of China; Shenzhen Fundamental Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9486466","reinforcement learning;robot;picking;throwing;self-supervised","Automation;Reinforcement learning;End effectors;Robots;Optimization;Sorting;Logistics","end effectors;learning (artificial intelligence);logistics;optimisation;sorting","weakly structured logistics;picking feasibility distribution map;optimal picking action;end-effector;D N-based learning algorithm;deep reinforcement learning;pick-and-throw method;robotic sorting ability;memory pool optimization algorithm;deep deterministic policy gradient algorithm","","","","17","IEEE","26 Jul 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Optimal Multiple Waypoint Navigation","C. Vlachos; P. Rousseas; C. P. Bechlioulis; K. J. Kyriakopoulos","Department of Electrical and Computer Engineering, University of Patras, Greece; School of Mechanical Engineering, Control Systems Laboratory, National Technical University of Athens, Greece; Department of Electrical and Computer Engineering, University of Patras, Greece; Center of AI & Robotics (CAIR), New York University, Abu Dhabi","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","1537","1543","In this paper, a novel method based on Artificial Potential Field (APF) theory is presented, for optimal motion planning in fully-known, static workspaces, for multiple final goal configurations. Optimization is achieved through a Reinforcement Learning (RL) framework. More specifically, the parameters of the underlying potential field are adjusted through a policy gradient algorithm in order to minimize a cost function. The main novelty of the proposed scheme lies in the method that provides optimal policies for multiple final positions, in contrast to most existing methodologies that consider a single final configuration. An assessment of the optimality of our results is conducted by comparing our novel motion planning scheme against a RRT* method.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160725","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160725","","Automation;Navigation;Reinforcement learning;Cost function;Planning","gradient methods;mobile robots;optimisation;path planning;reinforcement learning","Artificial Potential Field theory;motion planning scheme;multiple final goal configurations;multiple final positions;optimal motion;optimal policies;policy gradient algorithm;Reinforcement Learning-based optimal multiple waypoint navigation;RRT* method;single final configuration;static workspaces;underlying potential field","","","","24","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"GAN-Based Interactive Reinforcement Learning from Demonstration and Human Evaluative Feedback","J. Huang; J. Hao; R. Juan; R. Gomez; K. Nakamura; G. Li","College of Information Science and Engineering, Ocean University of China; College of Information Science and Engineering, Ocean University of China; College of Information Science and Engineering, Ocean University of China; Honda Research Institute Japan Co., Ltd, Wako, Japan; Honda Research Institute Japan Co., Ltd, Wako, Japan; College of Information Science and Engineering, Ocean University of China","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","4991","4998","Generative adversarial imitation learning (GAIL) — a general model-free imitation learning method, allows robots to directly learn policies from expert trajectories in large environments. However, GAIL shares the limitation of other imitation learning methods that they can seldom surpass the performance of demonstrations. In this paper, to address the limit of GAIL, we propose GAN-based interactive reinforcement learning (GAIRL) from demonstrations and human evaluative feedback, by combining the advantages of GAIL and interactive reinforcement learning. We test GAIRL in six physics-based control tasks, ranging from simple low-dimensional control tasks — Cart Pole, Mountain Car and Lunar Lander, to difficult high-dimensional tasks — Inverted Double Pendulum, Hopper and HalfCheetah. Our results suggest that, the GAIRL agent can generally surpass the performance of demonstrations in both low-dimensional and high-dimensional tasks and get an optimal or close to optimal policy.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160939","Natural Science Foundation of China(grant numbers:51809246); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160939","","Learning systems;Space vehicles;Automation;Moon;Reinforcement learning;Distance measurement;Trajectory","learning (artificial intelligence);pendulums;reinforcement learning","GAIL shares;GAN-based interactive reinforcement learning;general model-free imitation learning method;generative adversarial imitation learning;high-dimensional tasks;human evaluative feedback;low-dimensional control tasks;physics-based control tasks","","","","50","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Zero-Shot Policy Transfer with Disentangled Task Representation of Meta-Reinforcement Learning","Z. Wu; Y. Xie; W. Lian; C. Wang; Y. Guo; J. Chen; S. Schaal; M. Tomizuka","University of California, Berkeley, Berkeley, CA, USA; University of California, Berkeley, Berkeley, CA, USA; Intrinsic Innovation LLC, Mountain View, CA, USA; University of California, Berkeley, Berkeley, CA, USA; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Intrinsic Innovation LLC, Mountain View, CA, USA; University of California, Berkeley, Berkeley, CA, USA","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","7169","7175","Humans are capable of abstracting various tasks as different combinations of multiple attributes. This perspective of compositionality is vital for human rapid learning and adaption since previous experiences from related tasks can be combined to generalize across novel compositional settings. In this work, we aim to achieve zero-shot policy generalization of Reinforcement Learning (RL) agents by leveraging the task compositionality. Our proposed method is a meta-RL algorithm with disentangled task representation, explicitly encoding different aspects of the tasks. Policy generalization is then performed by inferring unseen compositional task representations via the obtained disentanglement without extra exploration. The evaluation is conducted on three simulated tasks and a challenging real-world robotic insertion task. Experimental results demonstrate that our proposed method achieves policy generalization to unseen compositional tasks in a zero-shot manner.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160764","","Automation;Reinforcement learning;Encoding;Task analysis;Robots","control engineering computing;inference mechanisms;reinforcement learning;robots","compositional settings;disentangled task representation;human rapid learning;meta-reinforcement learning;meta-RL algorithm;multiple attributes;real-world robotic insertion task;reinforcement learning agents;simulated tasks;task compositionality;unseen compositional task representations;unseen compositional tasks;zero-shot manner;zero-shot policy generalization;zero-shot policy transfer","","","","33","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Extendable Navigation Network based Reinforcement Learning for Indoor Robot Exploration","W. -C. Lee; M. C. Lim; H. -L. Choi","Department of Aerospace Engineering, KAIST, Daejeon, Korea; Department of Aerospace Engineering, KAIST, Daejeon, Korea; Department of Aerospace Engineering, KAIST, Daejeon, Korea","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","11508","11514","This paper presents a navigation network based deep reinforcement learning framework for autonomous indoor robot exploration. The presented method features a pattern cognitive non-myopic exploration strategy that can better reflect universal preferences for structure. We propose the Extendable Navigation Network (ENN) to encode the partially observed high-dimensional indoor Euclidean space to a sparse graph representation. The robot’s motion is generated by a learned Q-network whose input is the ENN. The proposed framework is applied to a robot equipped with a 2D LIDAR sensor in the GAZEBO simulation where floor plans of real buildings are implemented. The experiments demonstrate the efficiency of the framework in terms of exploration time.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561040","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561040","","Laser radar;Automation;Navigation;Conferences;Buildings;Reinforcement learning;Robot sensing systems","control engineering computing;graph theory;learning (artificial intelligence);mobile robots;optical radar;path planning","deep reinforcement learning framework;autonomous indoor robot exploration;presented method features;pattern cognitive nonmyopic exploration strategy;Extendable Navigation Network;ENN;high-dimensional indoor Euclidean space;learned Q-network;exploration time","","","","28","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning in Smart Grid: Progress and Prospects","A. Akagic; I. Džafić","Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina","2022 XXVIII International Conference on Information, Communication and Automation Technologies (ICAT)","4 Jul 2022","2022","","","1","6","The combination of reinforcement learning and deep learning has shown some remarkable results in many scientific fields. Deep reinforcement learning algorithms are particularly good at understanding and modeling adaptive decision-making in dynamic environments. In recent years, this concept has been successfully applied to smart grids. In this paper, we provide a brief introduction to the concepts of reinforcement and deep reinforcement learning to the power system engineers and present research progress and prospects in the field. Additionally, we identify smart grid engineering domains that need extensive pattern-based modeling as being particularly suitable for deep reinforcement learning.","2643-1858","978-1-6654-6692-9","10.1109/ICAT54566.2022.9811131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811131","Smart Grid;Machine Learning;Reinforcement Learning;Deep Reinforcement Learning","Deep learning;Adaptation models;Automation;Heuristic algorithms;Taxonomy;Power system dynamics;Decision making","decision making;learning (artificial intelligence);smart power grids","deep learning;deep reinforcement learning algorithms;smart grid engineering","","","","61","IEEE","4 Jul 2022","","","IEEE","IEEE Conferences"
"Decentralized Ride-sharing of Shared Autonomous Vehicles Using Graph Neural Network-Based Reinforcement Learning","B. Li; N. Ammar; P. Tiwari; H. Peng","Mechanical Engineering Department, University of Michigan, Ann Arbor, MI, USA; Toyota Motor North America; Toyota Motor North America; Mechanical Engineering Department, University of Michigan, Ann Arbor, MI, USA","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","912","918","Ride-sharing has important implications for improving the efficiency of mobility-on-demand systems. However, it remains a challenge due to the complex dynamics between vehicles and requests. This paper presents a decentralized ride-sharing algorithm suitable for shared autonomous vehicles (SAVs) deployment. The ride-sharing problem is formulated as a multi-agent reinforcement learning problem. We explore state representation with the request-vehicle graph to encode shareability and potential coordination information. We use a graph attention network to build a hierarchical structure that unifies ride-sharing assignments with rebalancing and handles real-world scenarios where hundreds of user requests can be associated with vehicles. We show results in both generic grid-world and SUMO simulation with real-world data from the Manhattan area. We empirically demonstrate that our proposed approach can achieve similar performance compared with a state-of-the-art centralized optimization method and higher computation efficiency.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811596","Toyota Motor North America R&D, InfoTech Labs; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811596","","Automation;Heuristic algorithms;Computational modeling;Optimization methods;Reinforcement learning;Data models;Computational efficiency","graph theory;multi-agent systems;neural nets;reinforcement learning;traffic engineering computing","decentralized ride-sharing algorithm;shared autonomous vehicles;multiagent reinforcement learning problem;state representation;request-vehicle graph;coordination information;graph attention network;user requests;graph neural network-based reinforcement learning;mobility-on-demand systems;SUMO simulation;generic grid-world simulation","","","","23","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"ExploreFault: Identifying Exploitable Fault Models in Block Ciphers with Reinforcement Learning","H. Guo; S. Saha; V. Gohil; S. Patnaik; D. Mukhopadhyay; J. J. Rajendran","Electrical & Computer Engineering, Texas A&M University, College Station, Texas, USA; UCL Crypto Group, ICTEAM/ELEN, Université Catholique de Louvain, Belgium; Electrical & Computer Engineering, Texas A&M University, College Station, Texas, USA; Electrical & Computer Engineering, Texas A&M University, College Station, Texas, USA; Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Electrical & Computer Engineering, Texas A&M University, College Station, Texas, USA","2023 60th ACM/IEEE Design Automation Conference (DAC)","15 Sep 2023","2023","","","1","6","Exploitable fault models for block ciphers are typically cipher-specific, and their identification is essential for evaluating and certifying fault attack-protected implementations. However, identifying exploitable fault models has been a complex manual process. In this work, we utilize reinforcement learning (RL) to identify exploitable fault models generically and automatically. In contrast to the several weeks/months of tedious analyses required from experts, our RL-based approach identifies exploitable fault models for protected/unprotected AES and GIFT ciphers within 12 hours. Notably, in addition to all existing fault models, we identify/discover a novel fault model for GIFT, illustrating the power and promise of our approach in exploring new attack avenues.","","979-8-3503-2348-1","10.1109/DAC56929.2023.10247953","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247953","Reinforcement Learning;Fault Attack","Fault diagnosis;Ciphers;Analytical models;Design automation;Reinforcement learning;Manuals;Space exploration","computer network security;cryptography;reinforcement learning","block ciphers;cipher-specific;fault attack-protected implementations;fault model;identifying exploitable fault models;reinforcement learning;time 12.0 hour","","","","26","IEEE","15 Sep 2023","","","IEEE","IEEE Conferences"
"Managing Shaping Complexity in Reinforcement Learning with State Machines - Using Robotic Tasks with Unspecified Repetition as an Example","Y. -W. Mo; C. Ho; C. -T. King","Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan","2022 IEEE International Conference on Mechatronics and Automation (ICMA)","22 Aug 2022","2022","","","544","550","Reinforcement learning (RL) has been widely used in the field of robotics in recent years. It is an optimization process that finds a policy to maximize the expected reward. Thus, designing a suitable reward function is critical. Sparse reward is easier to design, but for complex tasks, such as robot operations with unspecified repetition, the agent can rarely receive useful reward signals. Dense reward with shaping may be used, but current practices are mostly ad hoc. In this paper, we demonstrate that reward shaping may be done in a more systematic way by using a set of design principles together with the reward machine. We applied the method to train neural networks with RL that can perform block stacking and block lining up tasks with unspecified repetition. The experimental results show that the resultant neural networks can achieve success rates up to 66% and 81% for block stacking and block lining up tasks respectively, which are far better than those using ad hoc sparse rewards.","2152-744X","978-1-6654-0853-0","10.1109/ICMA54519.2022.9856243","Ministry of Science and Technology; Industrial Technology Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9856243","Machine learning;Reinforcement learning;Reward design;Reward shaping;Robotic control","Systematics;Mechatronics;Automation;Stacking;Neural networks;Reinforcement learning;Complexity theory","neural nets;reinforcement learning;robots","shaping complexity;reinforcement learning;state machines;robotic tasks;unspecified repetition;RL;optimization process;expected reward;suitable reward function;sparse reward;robot operations;reward signals;dense reward;reward shaping;design principles;reward machine;resultant neural networks;block stacking;ad hoc sparse rewards","","","","13","IEEE","22 Aug 2022","","","IEEE","IEEE Conferences"
"Show me What you want: Inverse Reinforcement Learning to Automatically Design Robot Swarms by Demonstration","I. Gharbi; J. Kuckling; D. G. Ramos; M. Birattari","IRIDIA, Université libre de Bruxelles, Brussels, Belgium; IRIDIA, Université libre de Bruxelles, Brussels, Belgium; IRIDIA, Université libre de Bruxelles, Brussels, Belgium; IRIDIA, Université libre de Bruxelles, Brussels, Belgium","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5063","5070","Automatic design is a promising approach to generating control software for robot swarms. So far, automatic design has relied on mission-specific objective functions to specify the desired collective behavior. In this paper, we explore the possibility to specify the desired collective behavior via demonstrations. We develop Demo-Cho, an automatic design method that combines inverse reinforcement learning with automatic modular design of control software for robot swarms. We show that, only on the basis of demonstrations and without the need to be provided with an explicit objective function, Demo-Cho successfully generated control software to perform four missions. We present results obtained in simulation and with physical robots.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160947","European Research Council(grant numbers:681872); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160947","","Measurement;Learning systems;Protocols;Automation;Design methodology;Reinforcement learning;Linear programming","control engineering computing;mobile robots;multi-robot systems;reinforcement learning","automatic design method;control software;Demo-Cho;design robot swarms;explicit objective function;inverse reinforcement learning;mission-specific objective functions;physical robots","","","","55","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"RLAlloc: A Deep Reinforcement Learning-Assisted Resource Allocation Framework for Enhanced Both I/O Throughput and QoS Performance of Multi-Streamed SSDs","M. Li; C. Wu; C. Gao; C. Ji; K. Li","School of Computer Science and Electronic Engineering, Hunan University, CN; Department of Electrical and Computer Engineering, Northeastern University, USA; School of Information, Xiameng University, CN; School of Computer Science and Engineering, Nanjing University of Technology, CN; School of Computer Science and Electronic Engineering, Hunan University, CN","2023 60th ACM/IEEE Design Automation Conference (DAC)","15 Sep 2023","2023","","","1","6","Multi-streamed Solid-State Disks (SSDs) have attracted increasing adoption in modern flash storage devices. Despite their excellent promise, effective flash resource allocation is still limiting both their achievable I/O performance and practical implementation. To this end, we develop the first-of-its-kind framework dubbed RLAlloc, which for the first time demonstrates deep Reinforcement Learning-assisted resource Allocation for boosting both I/O throughput and QoS performance of multi-streamed SSDs. Extensive experiments consistently validate the effectiveness of RLAlloc, improving up to 39.9% on I/O throughput and 44.0% on QoS performance over the state-of-the-art competitors.","","979-8-3503-2348-1","10.1109/DAC56929.2023.10247988","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247988","","Performance evaluation;Deep learning;Limiting;Design automation;Quality of service;Reinforcement learning;Throughput","deep learning (artificial intelligence);flash memories;quality of service;reinforcement learning;resource allocation;solid state drives;storage management","deep Reinforcement Learning-assisted resource Allocation framework;effective flash resource allocation;first-of-its-kind framework dubbed RLAlloc;modern flash storage devices;Multistreamed Solid-State Disks;multistreamed SSDs;QoS performance","","","","23","IEEE","15 Sep 2023","","","IEEE","IEEE Conferences"
"MAQD: Cooperative Multi-Agent Reinforcement Learning Q-value Decomposition in Actor-Critic Framework","Q. Zou; Y. Jiang; W. Li; B. Gao; D. Li; M. He","Information Engineering Faculty, Dalian University, Dalian, China; Information Engineering Faculty, Dalian University, Dalian, China; Information Engineering Faculty, Dalian University, Dalian, China; Information Engineering Faculty, Dalian University, Dalian, China; Information Engineering Faculty, Dalian University, Dalian, China; Information Engineering Faculty, Dalian University, Dalian, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","8194","8199","Multi-agent reinforcement learning (MARL) methods witness vigorous progress recently. Decentralized agents coordinate their behaviors while acting in many non-communication and partially observable real-world, and centralized training is a way to obtain global state information. However, centralized training cannot maximize individual and collective rational actions and coordinate the contradiction between them. For this problem, we investigate the causes that hinder the performance of multi-agent Actor-Critic algorithms and make basic assumptions. Meanwhile, this paper presents a cooperative MARL 4-value decomposition algorithm (MA4D). This method introduces a 4-value decomposition network to convert the joint 4-value output by the centralized critic network into a local 4-value for a single agent through accumulation and approximation, and then guide the gradient update of the actor network respectively. The MA4D ensures the consistency of the global optimal action and the local optimal action of MARL, individual and collective rationality is fully utilized to explore joint policy under Nash equilibrium. In addition, we design cooperative agents experimental scenarios to verify this algorithm on the multi-agent particle environment, where agents take actions in continuous spaces. Empirical evaluations demonstrate that our method significantly outperforms state-of-the-art multi-agent Actor-Critic algorithms in terms of performance and stability.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728098","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728098","multi-agent deep reinforcement learning;Nash equilibrium;Actor-Critic;value decomposition","Training;Automation;Scalability;Reinforcement learning;Nash equilibrium;Approximation algorithms;Stability analysis","game theory;multi-agent systems;reinforcement learning;singular value decomposition","actor-critic framework;collective rational actions;cooperative multiagent reinforcement learning Q-value decomposition;global state information;MA4D;MARL 4-value decomposition algorithm;multiagent actor-critic algorithms;multiagent particle environment;multiagent reinforcement learning methods witness vigorous progress;Nash equilibrium","","","","21","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Area-Driven FPGA Logic Synthesis Using Reinforcement Learning","G. Zhou; J. H. Anderson","Dept. of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Dept. of Electrical and Computer Engineering, University of Toronto, Toronto, Canada","2023 28th Asia and South Pacific Design Automation Conference (ASP-DAC)","23 Feb 2023","2023","","","159","165","Logic synthesis involves a rich set of optimization algorithms ap-plied in a specific sequence to a circuit netlist prior to technology mapping. A conventional approach is to apply a fixed “recipe” of such algorithms deemed to work well for a wide range of differ-ent circuits. We apply reinforcement learning (RL) to determine a unique recipe of algorithms for each circuit. Feature-importance analysis is conducted using a random-forest classifier to prune the set of features visible to the RL agent. We demonstrate conclusive learning by the RL agent and show significant FPGA area reductions vs. the conventional approach (resyn2). In addition to circuit-by-circuit training and inference, we also train an RL agent on multiple circuits, and then apply the agent to optimize: 1) the same set of circuits on which it was trained, and 2) an alternative set of “unseen” circuits. In both scenarios, we observe that the RL agent produces higher-quality implementations than the conventional approach. This shows that the RL agent is able to generalize, and perform beneficial logic synthesis optimizations across a variety of circuits.","2153-697X","978-1-4503-9783-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10044776","","Training;Design automation;Asia;Reinforcement learning;Benchmark testing;Inference algorithms;Classification algorithms","","","","","","16","","23 Feb 2023","","","IEEE","IEEE Conferences"
"Robot in a China Shop: Using Reinforcement Learning for Location-Specific Navigation Behaviour","B. Xihan; O. Mendez; S. Hadfield",NA; NA; NA,"2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","5959","5965","Robots need to be able to work in multiple different environments. Even when performing similar tasks, different behaviour should be deployed to best fit the current environment. In this paper, We propose a new approach to navigation, where it is treated as a multi-task learning problem. This enables the robot to learn to behave differently in visual navigation tasks for different environments while also learning shared expertise across environments. We evaluated our approach in both simulated environments as well as real-world data. Our method allows our system to converge with a 26% reduction in training time, while also increasing accuracy.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561545","Engineering and Physical Sciences Research Council; Innovate UK; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561545","","Training;Visualization;Automation;Navigation;Conferences;Reinforcement learning;Resource management","learning (artificial intelligence);mobile robots;path planning;robot vision","China shop;reinforcement learning;multitask learning problem;visual navigation tasks;location-specific navigation behaviour","","","","22","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models","Y. Wu; M. Mozifian; F. Shkurti","Division of Engineering Science, University of Toronto Robotics Institute; Montreal Institute of Learning Algorithms (MILA); Division of Engineering Science, University of Toronto Robotics Institute","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","6628","6634","The potential benefits of model-free reinforcement learning to real robotics systems are limited by its uninformed exploration that leads to slow convergence, lack of data-efficiency, and unnecessary interactions with the environment. To address these drawbacks we propose a method that combines reinforcement and imitation learning by shaping the reward function with a state-and-action-dependent potential that is trained from demonstration data, using a generative model. We show that this accelerates policy learning by specifying high-value areas of the state and action space that are worth exploring first. Unlike the majority of existing methods that assume optimal demonstrations and incorporate the demonstration data as hard constraints on policy optimization, we instead incorporate demonstration data as advice in the form of a reward shaping potential trained as a generative model of states and actions. In particular, we examine both normalizing flows and Generative Adversarial Networks to represent these potentials. We show that, unlike many existing approaches that incorporate demonstrations as hard constraints, our approach is unbiased even in the case of suboptimal and noisy demonstrations. We present an extensive range of simulations, as well as experiments on the Franka Emika 7DOF arm, to demonstrate the practicality of our method.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561333","","Automation;Conferences;Cloning;Reinforcement learning;Generative adversarial networks;Manipulators;Data models","control engineering computing;manipulators;optimisation;reinforcement learning;state-space methods;suboptimal control","robotics systems;uninformed exploration;data-efficiency;imitation learning;policy learning;high-value areas;action space;hard constraints;policy optimization;generative adversarial networks;suboptimal demonstrations;noisy demonstrations;imperfect demonstrations;model-free reinforcement learning;reward function shaping;state-and-action-dependent potential;state space;Franka Emika 7DOF arm","","","","43","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"RL-CCD: Concurrent Clock and Data Optimization using Attention-Based Self-Supervised Reinforcement Learning","Y. -C. Lu; W. -T. Chan; D. Guo; S. Kundu; V. Khandelwal; S. K. Lim","School of ECE, Georgia Institute of Technology, Atlanta, GA; Synopsys Inc., Hillsboro, OR; Synopsys Inc., Mountain View, CA; Synopsys Inc., Mountain View, CA; Synopsys Inc., Hillsboro, OR; School of ECE, Georgia Institute of Technology, Atlanta, GA","2023 60th ACM/IEEE Design Automation Conference (DAC)","15 Sep 2023","2023","","","1","6","Concurrent Clock and Data (CCD) optimization is a well-adopted approach in modern commercial tools that resolves timing violations using a mixture of clock skewing and delay fixing strategies. However, existing CCD algorithms are flawed. Particularly, they fail to prioritize violating endpoints for different optimization strategies correctly, leading to flow-wise globally sub-optimal results. In this paper, we overcome this issue by presenting RL-CCD, a Reinforcement Learning (RL) agent that selects endpoints for useful skew prioritization using the proposed EP-GNN, an endpoint-oriented Graph Neural Network (GNN) model, and a Transformer-based self-supervised attention mechanism. Experimental results on 19 industrial designs in 5 − 12nm technologies demonstrate that RL-CCD achieves up to 64% Total Negative Slack (TNS) reduction and 66.5% number of violating endpoints (NVE) improvement over the native implementation of a commercial tool.","","979-8-3503-2348-1","10.1109/DAC56929.2023.10248008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10248008","","Charge coupled devices;Design automation;Reinforcement learning;Transformers;Graph neural networks;Delays;Optimization","clocks;deep learning (artificial intelligence);graph neural networks;optimisation;reinforcement learning;supervised learning","attention-based self-supervised reinforcement learning;CCD algorithms;clock skewing;commercial tool;concurrent clock;data optimization;delay fixing strategies;different optimization strategies;endpoint-oriented Graph Neural Network model;endpoints improvement;modern commercial tools;RL-CCD;selects endpoints;size 5.0 nm to 12.0 nm;sub-optimal results;Transformer-based self-supervised attention mechanism;useful skew prioritization;violating endpoints","","","","12","IEEE","15 Sep 2023","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach to Generating Test Cases for Web Applications","X. Chang; Z. Liang; Y. Zhang; L. Cui; Z. Long; G. Wu; Y. Gao; W. Chen; J. Wei; T. Huang","State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, Beijing, China; Joint Laboratory on Cyberspace Security, China Southern Power Grid, Guangzhou, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, Beijing, China; Joint Laboratory on Cyberspace Security, China Southern Power Grid, Guangzhou, China; Joint Laboratory on Cyberspace Security, China Southern Power Grid, Guangzhou, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, Beijing, China","2023 IEEE/ACM International Conference on Automation of Software Test (AST)","12 Jul 2023","2023","","","13","23","Web applications play an important role in modern society. Quality assurance of web applications requires lots of manual efforts. In this paper, we propose WebQT, an automatic test case generator for web applications based on reinforcement learning. Specifically, to increase testing efficiency, we design a new reward model, which encourages the agent to mimic human testers to interact with the web applications. To alleviate the problem of state redundancy, we further propose a novel state abstraction technique, which can identify different web pages with the same functionality as the same state, and yields a simplified state space. We evaluate WebQT on seven open-source web applications. The experimental results show that WebQT achieves 45.4% more code coverage along with higher efficiency than the state-of-the-art technique. In addition, WebQT also reveals 69 exceptions in 11 real-world web applications.","2833-9061","979-8-3503-2402-0","10.1109/AST58925.2023.00006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10173983","State exploration;Reinforcement learning;Software testing","Software testing;Quality assurance;Codes;Automation;Redundancy;Web pages;Reinforcement learning","Internet;program testing;reinforcement learning","automatic test case generator;open-source Web applications;real-world Web applications;reinforcement learning approach;Web pages;WebQT","","","","40","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Robust Reinforcement Learning via Genetic Curriculum","Y. Song; J. Schneider","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","5560","5566","Achieving robust performance is crucial when applying deep reinforcement learning (RL) in safety critical systems. Some of the state of the art approaches try to address the problem with adversarial agents, but these agents often require expert supervision to fine tune and prevent the adversary from becoming too challenging to the trainee agent. While other approaches involve automatically adjusting environment setups during training, they have been limited to simple environments where low-dimensional encodings can be used. Inspired by these approaches, we propose genetic curriculum, an algorithm that automatically identifies scenarios in which the agent currently fails and generates an associated curriculum to help the agent learn to solve the scenarios and acquire more robust behaviors. As a non-parametric optimizer, our approach uses a raw, non-fixed encoding of scenarios, reducing the need for expert supervision and allowing our algorithm to adapt to the changing performance of the agent. Our empirical studies show improvement in robustness over the existing state of the art algorithms, providing training curricula that result in agents being 2 - 8x times less likely to fail without sacrificing cumulative reward. We include an ablation study and share insights on why our algorithm outperforms prior approaches.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812420","Boeing Company; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812420","","Training;Automation;Reinforcement learning;Genetics;Encoding;Robustness;Safety","deep learning (artificial intelligence);genetic algorithms;multi-agent systems;reinforcement learning","adversarial agents;expert supervision;trainee agent;environment setups;low-dimensional encodings;genetic curriculum;robust behaviors;nonparametric optimizer;nonfixed encoding;training curricula;robust reinforcement learning;RL;safety critical systems;deep reinforcement learning","","","","40","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning of Dexterous Pre-Grasp Manipulation for Human-Like Functional Categorical Grasping","D. Pavlichenko; S. Behnke","Autonomous Intelligent Systems (AIS) Group, Computer Science Institute VI, University of Bonn, Germany; Autonomous Intelligent Systems (AIS) Group, Computer Science Institute VI, University of Bonn, Germany","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","8","Many objects such as tools and household items can be used only if grasped in a very specific way-grasped functionally. Often, a direct functional grasp is not possible, though. We propose a method for learning a dexterous pre-grasp manipulation policy to achieve human-like functional grasps using deep reinforcement learning. We introduce a dense multi-component reward function that enables learning a single policy, capable of dexterous pre-grasp manipulation of novel instances of several known object categories with an anthropomorphic hand. The policy is learned purely by means of reinforcement learning from scratch, without any expert demonstrations, and implicitly learns to reposition and reorient objects of complex shapes to achieve given functional grasps. Learning is done on a single GPU in less than three hours.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260385","German Ministry of Education and Research (BMBF)(grant numbers:01IS21080); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260385","","Deep learning;Computer aided software engineering;Automation;Shape;Graphics processing units;Reinforcement learning;Grasping","deep learning (artificial intelligence);dexterous manipulators;graphics processing units;reinforcement learning","anthropomorphic hand;deep reinforcement learning;dexterous pre-grasp manipulation policy;GPU;human-like functional categorical grasping;multicomponent reward function","","","","34","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Improving Cooperative Multi-Target Tracking Control for UAV Swarm Using Multi-Agent Reinforcement Learning","L. Yue; M. Lv; M. Yan; X. Zhao; A. Wu; L. Li; J. Zuo","Air Traffic Control and Navigation College, Air Force Engineering University, Xi'an, China; Air Traffic Control and Navigation College, Air Force Engineering University, Xi'an, China; Air Traffic Control and Navigation College, Air Force Engineering University, Xi'an, China; Air Traffic Control and Navigation College, Air Force Engineering University, Xi'an, China; Air Traffic Control and Navigation College, Air Force Engineering University, Xi'an, China; Air Traffic Control and Navigation College, Air Force Engineering University, Xi'an, China; Air Traffic Control and Navigation College, Air Force Engineering University, Xi'an, China","2023 9th International Conference on Control, Automation and Robotics (ICCAR)","21 Jun 2023","2023","","","179","186","Past few years have witnessed vigorous progress in multi-target tracking (MTT) control of unmanned aerial vehicle (UAV) swarm. Most existing target tracking approaches rely on ideally assuming a preset target trajectory. However, in practice, the trajectory of moving target cannot be known by UAV in advance, which brings a great challenge to realize real-time tracking. Meanwhile, state-of-the-art multi-agent value-based methods has achieved significant progress in cooperative tasks. In contrast, multi-agent actor-critic (MAAC) methods do not provide satisfying performance. To address aforementioned issues, this paper proposes factored multi-agent soft actor-critic (FMASAC) scheme, where UAV swarm is enabled to learn cooperative MTT in unknown environment. This method introduces the idea of value decomposition into the MAAC setting to reduce the variance in policy updates and learn efficient credit assignment. Experiments demonstrate that FMASAC significantly improves cooperative MTT performance of UAV swarm, and outperforms the MAAC baselines in terms of the mean return and tracking success rate.","2251-2454","979-8-3503-2251-4","10.1109/ICCAR57134.2023.10151768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10151768","UAV swarm;multi-target tracking;multi-agent reinforcement learning;soft actor-critic;value decomposition","Target tracking;Automation;Simulation;Reinforcement learning;Autonomous aerial vehicles;Real-time systems;Trajectory","autonomous aerial vehicles;learning (artificial intelligence);multi-agent systems;reinforcement learning;target tracking","existing target tracking approaches;mean return;moving target;MTT performance;multiagent actor-critic methods;multiagent reinforcement learning;multiagent soft actor-critic scheme;multitarget tracking control;preset target trajectory;real-time tracking;state-of-the-art multiagent value-based methods;tracking success rate;UAV swarm;unmanned aerial vehicle swarm;vigorous progress","","","","39","IEEE","21 Jun 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning Control for a 2-DOF Helicopter With State Constraints: Theory and Experiments","Z. Zhao; W. He; C. Mu; T. Zou; K. -S. Hong; H. -X. Li","School of Mechanical and Electrical Engineering, Guangzhou University, Guangzhou, China; School of Mechanical and Electrical Engineering, Guangzhou University, Guangzhou, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Mechanical and Electrical Engineering, Guangzhou University, Guangzhou, China; School of Mechanical Engineering, Pusan National University, Busan, South Korea; Department of Advanced Design and Systems Engineering, City University of Hong Kong, Hong Kong, Hong Kong","IEEE Transactions on Automation Science and Engineering","","2022","PP","99","1","11","This study focuses on the novel reinforcement learning control strategy of a nonlinear two-degrees-of-freedom (2-DOF) helicopter system for tracking the desired trajectory while minimizing the tracking error. First, gradient descent algorithm is incorporated in the context of the reinforcement learning control scheme to obtain the adaptive laws. Subsequently, considering the uncertainties in the nonlinear system, radial basis function (RBF) neural networks (NNs) are exploited to approximate the unknown internal dynamics. In contrast to the previous studies, aiming at accelerating the convergence in reinforcement learning control, a barrier Lyapunov function is constructed to constrain the states to ensure that the tracking error rapidly converges to a neighborhood of zero. Under the proposed control strategy, the states of the closed-loop system are proven to be semi-globally uniformly ultimately bounded through rigorous Lyapunov analyses, and the state constraints are satisfied. Furthermore, the simulations and experiments conducted on a Quanser laboratory platform reveal that the proposed control functions are suitable and effective. Note to Practitioners—This paper is motivated by designing a reinforcement learning control strategy to enhance online learning capability and control performance of the controller for a nonlinear 2-DOF helicopter system. The control framework is divided into the design of the critic and actor NNs, responsible primarily for evaluating the control performance and approximating uncertainties in the system separately. Unlike the adaptive NN control, the actor NN weights are updated by combining information of states and inputs from the critic NN. In addition, aiming at accelerating the convergence, a barrier Lyapunov function is constructed to constrain the states to ensure that the tracking error rapidly converges to a neighborhood of zero. Finally, the proposed control strategy is validated in simulation and experiment on the Quanser laboratory platform.","1558-3783","","10.1109/TASE.2022.3215738","National Natural Science Foundation of China(grant numbers:62273112,52171331,62022061); Scientific Research Projects of Guangzhou Education Bureau(grant numbers:202032793); Science and Technology Planning Project of Guangzhou City(grant numbers:202102010398,202102010411,202201010758); Guangzhou University-Hong Kong University of Science and Technology Joint Research Collaboration Fund(grant numbers:YH202205); Open Research Fund from the Guangdong Laboratory of Artificial Intelligence and Digital Economy [Shenzhen (SZ)](grant numbers:GML-KF-22-27); Tianjin Natural Science Foundation(grant numbers:20JCYBJC00880); Korea Institute of Energy Technology Evaluation and Planning through the Auspices of the Ministry of Trade, Industry and Energy, Republic of Korea(grant numbers:20213030020160); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931924","Reinforcement learning;2-DOF helicopter;gradient descent;RBF neural networks;barrier Lyapunov function;Quanser laboratory platform","Helicopters;Artificial neural networks;Uncertainty;Convergence;Propellers;Lyapunov methods;DC motors","","","","5","","","IEEE","28 Oct 2022","","","IEEE","IEEE Early Access Articles"
"Reinforcement Learning Approach to Autonomous PID Tuning","O. Dogru; K. Velswamy; F. Ibrahim; Y. Wu; A. S. Sundaramoorthy; B. Huang; S. Xu; M. Nixon; N. Bell","Department of Chemical and Materials Engineering, University of Alberta, Edmonton, AB, Canada; Department of Chemical and Materials Engineering, University of Alberta, Edmonton, AB, Canada; Department of Chemical and Materials Engineering, University of Alberta, Edmonton, AB, Canada; Department of Electrical and Electronics Engineering, University of Alberta, Edmonton, AB, Canada; Department of Chemical and Materials Engineering, University of Alberta, Edmonton, AB, Canada; Department of Chemical and Materials Engineering, University of Alberta, Edmonton, AB, Canada; Emerson Electric Co., Austin, TX, Canada; Emerson Electric Co., Austin, TX, Canada; Emerson Electric Co., Austin, TX, Canada","2022 American Control Conference (ACC)","5 Sep 2022","2022","","","2691","2696","Many industrial processes utilize proportional-integral-derivative (PID) controllers due to their practicality and often satisfactory performance. The proper controller parameters depend highly on the operational conditions and process uncertainties. This dependence brings the necessity of frequent tuning for real-time control problems due to process drifts and operational condition changes. This study combines the recent developments in computer sciences and control theory to address the tuning problem. It formulates the PID tuning problem as a reinforcement learning task with constraints. The proposed scheme identifies an initial approximate step-response model and lets the agent learn dynamics off-line from the model with minimal effort. After achieving a satisfactory training performance on the model, the agent is fine-tuned on-line on the actual process to adapt to the real dynamics, thereby minimizing the training time on the real process and avoiding unnecessary wear, which can be beneficial for industrial applications. This sample efficient method is applied to a pilot-scale multi-modal tank system. The performance of the method is demonstrated by setpoint tracking and disturbance regulatory experiments.","2378-5861","978-1-6654-5196-3","10.23919/ACC53348.2022.9867687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9867687","","Training;Industries;Adaptation models;Uncertainty;Computational modeling;Process control;Reinforcement learning","control engineering computing;control system synthesis;industrial robots;mobile robots;multi-agent systems;process control;production engineering computing;reinforcement learning;step response;three-term control","industrial processes;proportional-integral-derivative controllers;computer sciences;reinforcement learning;step-response model;autonomous PID tuning;PID controllers;process uncertainties;multimodal tank system","","","","17","","5 Sep 2022","","","IEEE","IEEE Conferences"
