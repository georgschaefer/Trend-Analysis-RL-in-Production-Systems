@article{HAM2022100485,
title = {Actor-Critic reinforcement learning for optimal design of piping support constraint combinations},
journal = {International Journal of Naval Architecture and Ocean Engineering},
volume = {14},
pages = {100485},
year = {2022},
issn = {2092-6782},
doi = {https://doi.org/10.1016/j.ijnaoe.2022.100485},
url = {https://www.sciencedirect.com/science/article/pii/S2092678222000516},
author = {Jong-Ho Ham and Jung-Eun An and Hee-Sung Lee and Gun-il Park and Dong-Yeon Lee},
keywords = {Actor-Critic (AC), Finite Element Method (FEM), Markov Decision Process (MDP), Markov Process (MP), Monte-Carlo (MC), Temporal-Difference (TD)},
abstract = {Piping systems operating under various conditions may cause extremely unstable structural stress and deformation, leading to permanent deformation and damage to the piping structure, bringing about serious safety problems. So, piping stress analysis shall be performed along with proper design. Meanwhile, structural instability due to local stress concentration, which is one of the main interests of stress analysis, can be resolved through the appropriate selection of constraint conditions of piping supports at a relatively low cost. However, selecting an appropriate combination of constraints is time-consuming because analysis results are derived through many iterations. In addition, there is a big difference in the quality of the final design according to the experiences and capabilities of engineers. As a solution to this situation, this paper introduces a reinforcement learning strategy for deriving the optimal combination of constraints of piping supports. Primarily, we suggest applying the Actor-Critic(AC) learning algorithm for this problem, which is a type of reinforcement learning that has recently been in the spotlight for this problem because it can evaluate an action policy directly by the on-policy method inside the AC algorithm. During learning, it traces the correspondence between State and Action and suggests appropriate labeling criteria. We present the suitability and usefulness of the algorithm applied to the pipe stress analysis procedure through comparative verification between the results derived from the well-trained AC model and the results accomplished by engineers.}
}
@incollection{MAHADEVAN1991328,
title = {Scaling Reinforcement Learning to Robotics by Exploiting the Subsumption Architecture},
editor = {Lawrence A. Birnbaum and Gregg C. Collins},
booktitle = {Machine Learning Proceedings 1991},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {328-332},
year = {1991},
isbn = {978-1-55860-200-7},
doi = {https://doi.org/10.1016/B978-1-55860-200-7.50068-4},
url = {https://www.sciencedirect.com/science/article/pii/B9781558602007500684},
author = {Sridhar Mahadevan and Jonathan Connell},
abstract = {Making robots learn complex tasks from reinforcement seems attractive, but a number of problems are encountered in practice. The learning converges slowly because rewards are infrequent, and it is difficult to find effective ways of encoding state history. This paper shows how these problems are overcome by using a subsumption architecture: each module can be given its own simple reward function, and state history information can be easily encoded in a module's applicability predicate. A real robot called OBELIX (see Figure 1) is described that learns several component behaviors in an example task involving pushing boxes. An experimental study demonstrates the feasibility of the subsumption-based approach, and its superiority to a monolithic architecture.}
}
@article{DIXIT2022105106,
title = {Stochastic optimal well control in subsurface reservoirs using reinforcement learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {114},
pages = {105106},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105106},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622002469},
author = {Atish Dixit and Ahmed H. ElSheikh},
keywords = {Reinforcement learning, Stochastic optimal control, Subsurface flow control, Artificial intelligence in reservoir management, Optimal control for partially observable system},
abstract = {We present a case study of model-free reinforcement learning (RL) framework to solve stochastic optimal control for a predefined parameter uncertainty distribution and partially observable system. We focus on robust optimal well control problem which is a subject of intensive research activities in the field of subsurface reservoir management. For this problem, the system is partially observed since the data is only available at well locations. Furthermore, the model parameters are highly uncertain due to sparsity of available field data. In principle, RL algorithms are capable of learning optimal action policies – a map from states to actions – to maximize a numerical reward signal. In deep RL, this mapping from state to action is parameterized using a deep neural network. In the RL formulation of the robust optimal well control problem, the states are represented by saturation and pressure values at well locations while the actions represent the valve openings controlling the flow through wells. The numerical reward refers to the total sweep efficiency and the uncertain model parameter is the subsurface permeability field. The model parameter uncertainties are handled by introducing a domain randomization scheme that exploits cluster analysis on its uncertainty distribution. We present numerical results using two state-of-the-art RL algorithms, proximal policy optimization (PPO) and advantage actor–critic (A2C), on two subsurface flow test cases representing two distinct uncertainty distributions of permeability field. The results were benchmarked against optimization results obtained using differential evolution algorithm. Furthermore, we demonstrate the robustness of the proposed use of RL by evaluating the learned control policy on unseen samples drawn from the parameter uncertainty distribution that were not used during the training process.}
}
@article{FARIAS202017393,
title = {Position control of a mobile robot using reinforcement learning},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {17393-17398},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2093},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320327440},
author = {G. Farias and G. Garcia and G. Montenegro and E. Fabregas and S. Dormido-Canto and S. Dormido},
keywords = {Control Education, Mobile Robot, Position Control, Reinforcement Learning},
abstract = {Robotics has been introduced in education at all levels during the last years. In particular, the application of mobile robots for teaching automatic control is becoming more popular in engineering because of the attractive experiments that can be performed. This paper presents the design, development, and implementation of an algorithm to control the position of a wheeled mobile robot using Reinforcement Learning in an advanced 3D simulation environment. In this approach, the learning process occurs when the agent makes some actions in the environment to get some rewards. Trying to make a balance between the new information of the environment and the current knowledge about it. In this way, the algorithm is divided into two phases: 1) the learning stage, and 2) the operational stage. In the first stage, the robot learns how to reach a known destination point from its current position. To do it, it uses the information of the environment and the rewards, to build a learning matrix that is used later during the operational stage. The main advantage of this algorithm concerning traditional control algorithms is that the learning process is carried out automatically with a recursive procedure and the result is a controller that can make the specific task, without the need for a dynamic model. Its main drawback is that the learning stage can take a long time to finish and it depends on the hardware resources of the computer used during the learning process.}
}
@article{MITJANA2022104308,
title = {Managing chance-constrained hydropower with reinforcement learning and backoffs},
journal = {Advances in Water Resources},
volume = {169},
pages = {104308},
year = {2022},
issn = {0309-1708},
doi = {https://doi.org/10.1016/j.advwatres.2022.104308},
url = {https://www.sciencedirect.com/science/article/pii/S0309170822001713},
author = {Florian Mitjana and Michel Denault and Kenjy Demeester},
keywords = {Reservoir management, Hydropower, Reinforcement learning, Policy gradient, Chance constraint, Backoffs},
abstract = {The control of multi-reservoir hydropower systems is a crucial cogwheel in the current transition of power systems towards renewable energy. In particular in northern regions where inflows vary enormously through the year, the satisfaction of storage constraints cannot be ensured at all times. We thus add chance constraints to the underlying Markov Decision Process problem, and solve it with a Reinforcement Learning, policy gradient approach. “Backoffs”, common in other areas of optimal control, are introduced to better manage the numerous chance constraints as a single joint constraint. Stochastic Dynamic Programming (SDP) is used as a benchmark approach. We show numerically that our approach can deliver high quality policies, and just as importantly, that a three-reservoir system is solved in proportionally little more time than a one-reservoir system.}
}
@article{ZHOU2022104379,
title = {Reinforcement learning-based scheduling strategy for energy storage in microgrid},
journal = {Journal of Energy Storage},
volume = {51},
pages = {104379},
year = {2022},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2022.104379},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X22004030},
author = {Kunshu Zhou and Kaile Zhou and Shanlin Yang},
keywords = {Microgrid, Energy storage scheduling, Deep learning, Reinforcement learning},
abstract = {Integrated energy microgrids (IEMs) have developed rapidly in the past years with the advancement of renewable energy and energy storage technologies. As a result, dealing with uncertainty on the source and load sides and optimizing energy storage scheduling in IEMs have become critical research issues. However, some existing methods have limitations in terms of solution accuracy and efficiency. In order to accurately grasp the uncertainty at both source and load sides while effectively reducing the cost and improving the computational efficiency, this study proposes an adaptive and lightweight algorithm to obtain the optimal scheduling strategy for energy storage. First, a modified deep learning method is proposed to predict the PV power and load demand. Then, based on the prediction results, a reinforcement learning algorithm is used to solve the energy storage scheduling model and obtain the optimal scheduling strategy. In addition, to further investigate the effects of greedy and non-greedy actions on the agent's training, this study compares the results under different action exploration policies and different time scales. The outcomes of this study were compared with the ones obtained by mixed-integer linear programming. The results show that the reinforcement learning algorithm reduces 61.17% of the solution time though loses 3.13% of the solution accuracy. The increase in computational efficiency is essential for the real-time energy storage applications.}
}
@article{LUO2020109,
title = {PMA-DRL: A parallel model-augmented framework for deep reinforcement learning algorithms},
journal = {Neurocomputing},
volume = {403},
pages = {109-120},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.04.091},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220306767},
author = {Xufang Luo and Yunhong Wang},
keywords = {Reinforcement learning, Environment dynamics, Parallel framework},
abstract = {In recent years, aided by deep neural networks (DNNs), reinforcement learning (RL) algorithms have been achieving great success in more and more tasks. In general, model-free RL algorithms are widely applicable, but sometimes suffer from low sample efficiency. Although the environment dynamics can be incorporated into model-free RL algorithms to enhance sample efficiency, which can be transformed into the joint algorithm of model-free and model-based RL algorithms, the model bias of dynamics model may still hurt the performance. Another attractive study direction for RL algorithms, is using parallel strategy. Meanwhile, parallel RL algorithms can achieve outstanding results under less time cost, but are still with low sample efficiency, because most of such algorithms are vanilla model-free ones. Therefore, aiming to enhance the model performance for RL algorithms with DNNs, in this paper, we propose a novel parallel model-augmented framework, called PMA-DRL, to combine the dynamics model and parallel RL algorithms together. First, we introduce a local dynamics model (LDM) for each local agent model (LAM) in parallel RL algorithms. Next, we propose to build a better LDM by using an ensemble of LDMs, and such ensemble can help to reduce model bias, because the experienced observations under different LAMs are always diverse. Then, the observation diversity can be further increased by employing LDMs to explore. Finally, we implement experiments on classic control tasks (CCTs) and Atari games, which demonstrates the efficiency of our proposed framework.}
}
@article{MU20196946,
title = {Q-learning solution for optimal consensus control of discrete-time multiagent systems using reinforcement learning},
journal = {Journal of the Franklin Institute},
volume = {356},
number = {13},
pages = {6946-6967},
year = {2019},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2019.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0016003219304375},
author = {Chaoxu Mu and Qian Zhao and Zhongke Gao and Changyin Sun},
abstract = {This paper investigates a Q-learning scheme for the optimal consensus control of discrete-time multiagent systems. The Q-learning algorithm is conducted by reinforcement learning (RL) using system data instead of system dynamics information. In the multiagent systems, the agents are interacted with each other and at least one agent can communicate with the leader directly, which is described by an algebraic graph structure. The objective is to make all the agents achieve synchronization with leader and make the performance indices reach Nash equilibrium. On one hand, the solutions of the optimal consensus control for multiagent systems are acquired by solving the coupled Hamilton–Jacobi–Bellman (HJB) equation. However, it is difficult to get analytical solutions directly of the discrete-time HJB equation. On the other hand, accurate mathematical models of most systems in real world are hard to be obtained. To overcome these difficulties, Q-learning algorithm is developed using system data rather than the accurate system model. We formulate performance index and corresponding Bellman equation of each agent i. Then, the Q-function Bellman equation is acquired on the basis of Q-function. Policy iteration is adopted to calculate the optimal control iteratively, and least square (LS) method is employed to motivate the implementation process. Stability analysis of proposed Q-learning algorithm for multiagent systems by policy iteration is given. Two simulation examples are experimented to verify the effectiveness of the proposed scheme.}
}
@article{BELBEKKOUCHE20092091,
title = {Novel reinforcement learning-based approaches to reduce loss probability in buffer-less OBS networks},
journal = {Computer Networks},
volume = {53},
number = {12},
pages = {2091-2105},
year = {2009},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2009.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S1389128609001212},
author = {Abdeltouab Belbekkouche and Abdelhakim Hafid and Michel Gendreau},
keywords = {Multi-path routing, Deflection routing, Optical Burst Switching, Unsupervised learning, Wavelength Division Multiplexing},
abstract = {Optical Burst Switching (OBS) is a promising switching paradigm for the next generation Internet. A buffer-less OBS network can be implemented simply and cost-effectively without the need for either wavelength converters or optical buffers which are, currently, neither cost-effective nor technologically mature. However, this type of OBS networks suffers from relatively high loss probability caused by wavelength contentions at core nodes. This could prevent or, at least, delay the adoption of OBS networks as a solution for the next generation optical Internet. To enhance the performance of buffer-less OBS networks, we propose three approaches: (a) a reactive approach, called Reinforcement Learning-Based Deflection Routing Scheme (RLDRS) that aims to resolve wavelength contentions, after they occur, using deflection routing; (b) a proactive multi-path approach, called Reinforcement Learning-Based Alternative Routing (RLAR), that aims to reduce wavelength contentions; and (c) an approach, called Integrated Reinforcement Learning-based Routing and Contention Resolution (IRLRCR), that combines RLAR and RLDRS to conjointly deal with wavelength contentions proactively and reactively. Simulation results show that both RLAR and RLDRS reduce, effectively, loss probability in buffer-less OBS networks and outperform the existing multi-path and deflection routing approaches, respectively. Moreover, simulation results show that a substantial performance improvement, in terms of loss probability, is obtained using IRLRCR.}
}
@article{PENG2023108934,
title = {Data-driven optimal control of wind turbines using reinforcement learning with function approximation},
journal = {Computers & Industrial Engineering},
volume = {176},
pages = {108934},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108934},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222009226},
author = {Shenglin Peng and Qianmei Feng},
keywords = {Markov decision process, Reinforcement learning, Function approximation, Optimal control, Wind turbines},
abstract = {We propose a reinforcement learning approach with function approximation for maximizing the power output of wind turbines (WTs). The optimal control of wind turbines majorly uses the maximum power point tracking (MPPT) strategy for sequential decision-making that can be modeled as a Markov decision process (MDP). In the literature, the continuous control variables are typically discretized to cope with the curse of dimensionality in traditional dynamic programming methods. To provide a more accurate prediction, we formulate the problem into an MDP with continuous state and action spaces by utilizing the function approximation in reinforcement learning. The commonly used pitch angle is selected as a control variable we are concerned with, which is regarded as the system state along with some other controllable and uncontrollable variables proven to affect the power output. Computational studies of real data are conducted to demonstrate that the proposed method outperforms the existing methods in the literature in obtaining the optimal power output.}
}
@article{ALTHOEFER200151,
title = {Reinforcement learning in a rule-based navigator for robotic manipulators},
journal = {Neurocomputing},
volume = {37},
number = {1},
pages = {51-70},
year = {2001},
issn = {0925-2312},
doi = {https://doi.org/10.1016/S0925-2312(00)00307-6},
url = {https://www.sciencedirect.com/science/article/pii/S0925231200003076},
author = {Kaspar Althoefer and Bart Krekelberg and Dirk Husmeier and Lakmal Seneviratne},
keywords = {Reinforcement learning, Behavior, Fuzzy logic, Navigation, Robotic manipulator},
abstract = {This paper reports on a navigation system for robotic manipulators. The control system combines a repelling influence related to the distance between manipulator and nearby obstacles with the attracting influence produced by the angular difference between actual and final manipulator configuration to generate actuating motor commands. The use of fuzzy logic for the implementation of these behaviors leads to a transparent system that can be tuned by hand or by a learning algorithm. The proposed learning algorithm, based on reinforcement-learning neural network techniques, can adapt the navigator to the idiosyncratic requirements of particular manipulators, as well as the environments they operate in. The navigation method, combining the transparency of fuzzy logic with the adaptability of neural networks, has successfully been applied to robot arms in different environments.}
}
@article{LIU2020105537,
title = {Barrier Lyapunov function based reinforcement learning control for air-breathing hypersonic vehicle with variable geometry inlet},
journal = {Aerospace Science and Technology},
volume = {96},
pages = {105537},
year = {2020},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2019.105537},
url = {https://www.sciencedirect.com/science/article/pii/S1270963819309113},
author = {Chen Liu and Chaoyang Dong and Zhijie Zhou and Zhaolei Wang},
keywords = {Hypersonic vehicle, Variable geometry inlet, Reinforcement learning, Barrier Lyapunov function, Performance-guaranteed tracking},
abstract = {Based on barrier Lyapunov functions, a reinforcement learning control method is proposed for air-breathing hypersonic vehicles with variable geometry inlet (AHV-VGI) subject to external disturbances and diversified uncertainties. The longitudinal dynamic for the AHV-VGI is transformed into strict feedback form. Controllers for velocity and altitude subsystems are designed, respectively. Taking advantage of the reinforcement learning strategy, two radial basis function (RBF) neural networks are applied to estimate the “total disturbances” in the flight control system. Actor network is used for generating the estimate of the disturbance. Critic network is used for evaluating the estimation accuracy. Prescribed tracking performances and state constraints can be guaranteed by introducing barrier Lyapunov functions (BLFs). Tracking differentiators are used to generate the derivatives of virtual controllers in the backstepping design process. Simulation results illustrate the effectiveness and advantages of the proposed control strategy.}
}
@article{ZHANG2020115401,
title = {Reinforcement learning-based intelligent energy management architecture for hybrid construction machinery},
journal = {Applied Energy},
volume = {275},
pages = {115401},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.115401},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920309132},
author = {Wei Zhang and Jixin Wang and Yong Liu and Guangzong Gao and Siwen Liang and Hongfeng Ma},
keywords = {Hybrid construction machinery, Energy management, Reinforcement learning, Dyna-Q learning, Virtual world model},
abstract = {Power allocation is of crucial significance to energy management system in the hybrid construction machinery (HCM). Most of the existing HCM energy management strategies are only formulated based on the predefined rules, which causes the system unable to adapt to the changeable and complicated working conditions, thus seriously limiting the energy saving potential of hybrid technology. In this paper, we build a reinforcement learning-based intelligent energy management architecture for HCM. Given the working conditions and operating characteristics of HCM, a Q-function updating method combining direct learning and indirect learning is proposed to enhance the performance and practicability of reinforcement learning. A virtual world model (VWM) is introduced to approximate the real-world environment and facilitate the identification of data-driven environment, so as to enhance the real-time performance and adaptability of the architecture. Based on the characteristics of HCM working conditions, the load cycle is subdivided, and the stationary Markov chain is employed to yield real-time transfer probability matrices of required power to accelerate the updating of the environment model. An HCM experiment platform is built, in which the typical signal of working condition is sampled for simulation. The results indicate that DYNA-Q based architecture outperforms Q-learning and rule-based strategy (RBS) in terms of adaptivity, real-time performance and optimality. The results also demonstrate that with the proposed architecture, the working condition of internal combustion engine (ICE) and the charge-discharge of ultracapacitor are more rational and efficient.}
}
@article{OZKAN2021882,
title = {Inverse Reinforcement Learning Based Stochastic Driver Behavior Learning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {20},
pages = {882-888},
year = {2021},
note = {Modeling, Estimation and Control Conference MECC 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.11.283},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321023272},
author = {Mehmet F. Ozkan and Abishek J. Rocque and Yao Ma},
keywords = {Driver behavior modeling, Inverse reinforcement learning},
abstract = {Drivers have unique and rich driving behaviors when operating vehicles in traffic. This paper presents a novel driver behavior learning approach that captures the uniqueness and richness of human driver behavior in realistic driving scenarios. A stochastic inverse reinforcement learning (SIRL) approach is proposed to learn a distribution of cost function, which represents the richness of the human driver behavior with a given set of driver-specific demonstrations. Evaluations are conducted on the realistic driving data collected from the 3D driver-in-the-loop driving simulation. The results show that the learned stochastic driver model is capable of expressing the richness of the human driving strategies under different realistic driving scenarios. Compared to the deterministic baseline driver behavior model, the results reveal that the proposed stochastic driver behavior model can better replicate the driver’s unique and rich driving strategies in a variety of traffic conditions.}
}
@article{QIN202299,
title = {Deep reinforcement learning based active disturbance rejection control for ship course control},
journal = {Neurocomputing},
volume = {484},
pages = {99-108},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.06.096},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221015812},
author = {Huayang Qin and Panlong Tan and Zengqiang Chen and Mingwei Sun and Qinglin Sun},
keywords = {Deep deterministic policy gradient (DDPG), Reinforcement learning (RL), Linear active disturbance rejection control (LADRC), Ship course control, Parameter optimization},
abstract = {The linear active disturbance rejection control (LADRC) has been applied in many control practices and achieved satisfactory results. However, the controller with fixed parameters cannot achieve the optimal control performance for the controlled plant. In order to optimize the control effort of LADRC with online parameter adjustment, a deep reinforcement learning algorithm, the deep deterministic policy gradient (DDPG), is applied to enhance the LADRC in this paper for the ship course control and to obtain the optimized parameters of LADRC in different conditions. Specifically, the proposed strategy adopts the deep neural network to adjust the control parameters according to the measured states. This makes the proposed method applicable to the nonlinear control systems with high dimensional continuous states and actions. In addition, the stability of the closed-loop system is analyzed based on the Lyapunov method. In simulations, comparisons with Q-learning based LADRC and conventional LADRC controllers are also presented. Simulation results are provided to demonstrate the effectiveness of the proposed method.}
}
@article{LI2023128139,
title = {Deep reinforcement learning-based eco-driving control for connected electric vehicles at signalized intersections considering traffic uncertainties},
journal = {Energy},
volume = {279},
pages = {128139},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.128139},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223015335},
author = {Jie Li and Abbas Fotouhi and Wenjun Pan and Yonggang Liu and Yuanjian Zhang and Zheng Chen},
keywords = {Eco-driving, Deep reinforcement learning, Velocity optimization, Signalized intersection, Connected electric vehicle},
abstract = {Eco-driving control poses great energy-saving potential at multiple signalized intersection scenarios. However, traffic uncertainties can often lead to errors in ecological velocity planning and result in increased energy consumption. This study proposes an eco-driving approach with a hierarchical framework to be leveraged at signalized intersections that considers the impact of traffic uncertainty. The proposed approach leverages a queue-based traffic model in the upper level to estimate the impact of traffic uncertainty and generate dynamic modified traffic light information. In the lower level, a deep reinforcement learning-based controller is constructed to optimize velocity subject to the constraints from the traffic lights and traffic uncertainty, thereby reducing energy consumption while ensuring driving safety. The effectiveness of the proposed control strategy is demonstrated through numerous simulation case studies. The simulation results show that the proposed method significantly improves energy economy and prevents unnecessary idling in uncertain traffic scenarios, as compared to other approaches that ignore traffic uncertainty. Furthermore, the proposed method is adaptable to different traffic scenarios and showcases energy efficiency.}
}
@article{SKORDILIS2020106600,
title = {A deep reinforcement learning approach for real-time sensor-driven decision making and predictive analytics},
journal = {Computers & Industrial Engineering},
volume = {147},
pages = {106600},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106600},
url = {https://www.sciencedirect.com/science/article/pii/S036083522030334X},
author = {Erotokritos Skordilis and Ramin Moghaddass},
keywords = {Particle filters, Deep reinforcement learning, Real-time control, Decision-making, Remaining useful life estimation},
abstract = {The increased complexity of sensor-intensive systems with expensive subsystems and costly repairs and failures calls for efficient real-time control and decision making policies. Deep reinforcement learning has demonstrated great potential in addressing highly complex and challenging control and decision making problems. Despite its potential to derive real-time policies using real-time data for dynamic systems, it has been rarely used for sensor-driven maintenance related problems. In this paper, we propose two novel decision making methods in which reinforcement learning and particle filtering are utilized for (i) deriving real-time maintenance policies and (ii) estimating remaining useful life for sensor-monitored degrading systems. The proposed framework introduces a new direction with many potential opportunities for system monitoring. To demonstrate the effectiveness of the proposed methods, numerical experiments are provided from a set of simulated data and a turbofan engine dataset provided by NASA.}
}
@article{NASON200551,
title = {Soar-RL: integrating reinforcement learning with Soar},
journal = {Cognitive Systems Research},
volume = {6},
number = {1},
pages = {51-59},
year = {2005},
note = {Special Issue of Cognitive Systems Research - The Best Papers from ICCM2004},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2004.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S1389041704000646},
author = {Shelley Nason and John E. Laird},
keywords = {Soar, Reinforcement learning, Cognitive architectures},
abstract = {In this paper, we describe an architectural modification to Soar that gives a Soar agent the opportunity to learn statistical information about the past success of its actions and utilize this information when selecting an operator. This mechanism serves the same purpose as production utilities in ACT-R, but the implementation is more directly tied to the standard definition of the reinforcement learning (RL) problem. The paper explains our implementation, gives a rationale for adding an RL capability to Soar, and shows results for Soar-RL agents’ performance on two tasks.}
}
@article{DING2023111823,
title = {Integration test order generation based on reinforcement learning considering class importance},
journal = {Journal of Systems and Software},
volume = {205},
pages = {111823},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111823},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223002182},
author = {Yanru Ding and Yanmei Zhang and Guan Yuan and Shujuan Jiang and Wei Dai and Yinghui Zhang},
keywords = {Integration testing, Class integration test order, Class importance, Reinforcement learning, Stubbing cost},
abstract = {The task of ordering classes reasonably in the context of integration testing has been discussed by many researchers. Existing methods regard the class integration test order with the minimum stubbing cost as the optimal result. However, they ignore that class importance can also affect the class integration test order. This paper presents a design of a new algorithm, which considered the class importance, and its evaluation using computational experiments. Specifically, two novel reinforcement learning-based methods to generate class integration test orders are proposed, which aim to consider the class importance and minimize the stubbing cost. First, we advance the concept of class importance and optimize its measurement method. Then, we refine the calculation of stubbing complexity, which is the evaluation indicator of stubbing cost. After that, we combine both class importance and the stubbing complexity into the reinforcement learning algorithm to guide the agent to explore. Finally, we evaluate the proposed methods using computational experiments on five benchmark programs and four open-source programs. The experimental results show that our proposed methods can significantly reduce the stubbing cost while prioritizing the classes of high importance.}
}
@article{LIKMETA2020103568,
title = {Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103568},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103568},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304085},
author = {Amarildo Likmeta and Alberto Maria Metelli and Andrea Tirinzoni and Riccardo Giol and Marcello Restelli and Danilo Romano},
keywords = {Autonomous driving, Decision making, Interpretability, Reinforcement learning, Parameter-based exploration},
abstract = {The design of high-level decision-making systems is a topical problem in the field of autonomous driving. In this paper, we combine traditional rule-based strategies and reinforcement learning (RL) with the goal of achieving transparency and robustness. On the one hand, the use of handcrafted rule-based controllers allows for transparency, i.e., it is always possible to determine why a given decision was made, but they struggle to scale to complex driving scenarios, in which several objectives need to be considered. On the other hand, black-box RL approaches enable us to deal with more complex scenarios, but they are usually hardly interpretable. In this paper, we combine the best properties of these two worlds by designing parametric rule-based controllers, in which interpretable rules can be provided by domain experts and their parameters are learned via RL. After illustrating how to apply parameter-based RL methods (PGPE) to this setting, we present extensive numerical simulations in the highway and in two urban scenarios: intersection and roundabout. For each scenario, we show the formalization as an RL problem and we discuss the results of our approach in comparison with handcrafted rule-based controllers and black-box RL techniques.}
}
@article{BALTES2023106941,
title = {A deep reinforcement learning algorithm to control a two-wheeled scooter with a humanoid robot},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106941},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106941},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623011259},
author = {Jacky Baltes and Guilherme Christmann and Saeed Saeedvand},
keywords = {Deep reinforcement learning, Proximal policy optimization (PPO), Two-wheeled vehicles, PID control, Humanoid robotics},
abstract = {Balancing a two-wheeled scooter is considered a challenging task for robots, as it is a non-linear control problem in a highly dynamic environment. The rapid pace of development of deep reinforcement learning has enabled robots to perform complex control tasks. In this paper, a deep reinforcement learning algorithm is proposed to learn the steering control of the scooter for balancing and patch tracking using an unmodified humanoid robot. Two control strategies are developed, analyzed, and compared: a classical Proportional–Integral–Derivative (PID) controller and a Deep Reinforcement Learning (DRL) controller based on Proximal Policy Optimization (PPO) algorithm. The ability of the robot to balance the scooter using both approaches is extensively evaluated. Challenging control scenarios are tested at low scooter speeds, including 2.5, 5, and 10 km/h. Steering velocities are also varied, including 10, 20, and 40 rad/s. The evaluations include upright balance without disturbances, upright balance under disturbances, tracking sinusoidal path, and path tracking. A 3D model of the humanoid robot and scooter system is developed, which is simulated in a state-of-the-art GPU-based simulation environment as a training and test bed (NVidia’s Isaac Gym). Despite the fact that the PID controller successfully balances the robot, better final results are achieved with the proposed DRL. The results indicate a 52% improvement on average in different speeds with better performance in path tracking control. Controller command evaluation on the real robot and scooter indicates the robot’s complete capability to realize steering control velocities.}
}
@article{ZHU2023,
title = {Mastering air combat game with deep reinforcement learning},
journal = {Defence Technology},
year = {2023},
issn = {2214-9147},
doi = {https://doi.org/10.1016/j.dt.2023.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S2214914723002349},
author = {Jingyu Zhu and Minchi Kuang and Wenqing Zhou and Heng Shi and Jihong Zhu and Xu Han},
keywords = {Air combat, MCLDPPO, Interruption mechanism, Digital twin, Distributed system},
abstract = {Reinforcement learning has been applied to air combat problems in recent years, and the idea of curriculum learning is often used for reinforcement learning, but traditional curriculum learning suffers from the problem of plasticity loss in neural networks. Plasticity loss is the difficulty of learning new knowledge after the network has converged. To this end, we propose a motivational curriculum learning distributed proximal policy optimization (MCLDPPO) algorithm, through which trained agents can significantly outperform the predictive game tree and mainstream reinforcement learning methods. The motivational curriculum learning is designed to help the agent gradually improve its combat ability by observing the agent's unsatisfactory performance and providing appropriate rewards as a guide. Furthermore, a complete tactical maneuver is encapsulated based on the existing air combat knowledge, and through the flexible use of these maneuvers, some tactics beyond human knowledge can be realized. In addition, we designed an interruption mechanism for the agent to increase the frequency of decision-making when the agent faces an emergency. When the number of threats received by the agent changes, the current action is interrupted in order to reacquire observations and make decisions again. Using the interruption mechanism can significantly improve the performance of the agent. To simulate actual air combat better, we use digital twin technology to simulate real air battles and propose a parallel battlefield mechanism that can run multiple simulation environments simultaneously, effectively improving data throughput. The experimental results demonstrate that the agent can fully utilize the situational information to make reasonable decisions and provide tactical adaptation in the air combat, verifying the effectiveness of the algorithmic framework proposed in this paper.}
}
@article{WU202294,
title = {Sub-AVG: Overestimation reduction for cooperative multi-agent reinforcement learning},
journal = {Neurocomputing},
volume = {474},
pages = {94-106},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.12.039},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221018774},
author = {Haolin Wu and Jianwei Zhang and Zhuang Wang and Yi Lin and Hui Li},
keywords = {Cooperative multi-agent reinforcement learning, Joint action value decomposition, Overestimation error, Lower update target},
abstract = {Decomposing the centralized joint action value(JAV) into per-agent individual action value(IAV) is attractive in cooperative multi-agent reinforcement learning(MARL). In such tasks, IAVs based on local observation can perform decentralized policies, and the JAV is used for end-to-end training through traditional reinforcement learning methods, especially through the Q-learning algorithm. However, the Q-learning-based method suffers from overestimation, in which the overestimated action values may result in a suboptimal policy. In this paper, we show that such overestimation can occur in the above Q-learning-based decomposition method. Our solution is Sub-AVG, which utilizes a lower update target by discarding the larger of previously learned IAVs and averaging the retained ones, thus eliminating the excessive overestimation errors. Experiments in the StarCraft Multi-Agent Challenge(SMAC) environment show that Sub-AVG can lead to lower JAV estimations and better-performing policies.}
}
@article{ALJOHANI2021106962,
title = {Real-Time metadata-driven routing optimization for electric vehicle energy consumption minimization using deep reinforcement learning and Markov chain model},
journal = {Electric Power Systems Research},
volume = {192},
pages = {106962},
year = {2021},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2020.106962},
url = {https://www.sciencedirect.com/science/article/pii/S0378779620307604},
author = {Tawfiq M. Aljohani and Ahmed Ebrahim and Osama Mohammed},
keywords = {Energy minimization framework, Artificial intelligence (AI), Electric vehicles (EVs), Routing optimization, Markov Chain Model (MCM), Reinforcement Learning (RL), Google's API platform, Double Deep Q-Network (DDQN)},
abstract = {A real-time, data-driven electric vehicle (EVs) routing optimization to achieve energy consumption minimization is proposed in this work. The proposed framework utilizes the concept of Double Deep Q-learning Network (DDQN) in learning the maximum travel policy of the EV as an agent. The policy model is trained to estimate the agent's optimal action per the obtained reward signals and Q-values, representing the feasible routing options. The agent's energy requirement on the road is assessed following Markov Chain Model (MCM), with Markov's unit step represented as the average energy consumption that takes into consideration the different driving patterns, agent's surrounding environment, road conditions, and applicable restrictions. The framework offers a better exploration strategy, continuous learning ability, and the adoption of individual routing preferences. A real-time simulation in the python environment that considered real-life driving data from Google's API platform is performed. Results obtained for two geographically different drives show that the proposed energy consumption minimization framework reduced the energy utilization of the EVs to reach its intended destination by 5.89% and 11.82%, compared with Google's proposed routes originally. Both drives started at 4.30 PM on April 25th, 2019, in Los Angeles, California, and Miami, Florida, to reach EV's charging stations that are located six miles away from both of the starting locations.}
}
@article{MING2023281,
title = {Cooperative modular reinforcement learning for large discrete action space problem},
journal = {Neural Networks},
volume = {161},
pages = {281-296},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.01.046},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000588},
author = {Fangzhu Ming and Feng Gao and Kun Liu and Chengmei Zhao},
keywords = {Deep reinforcement learning, Modular reinforcement learning, Large discrete action space, Parallel training},
abstract = {Deep reinforcement learning (DRL) has achieved remarkable results on high-dimension state tasks. However, it suffers in hard convergence and low sample efficiency when solving large discrete action space problems. To meet these challenges, we develop a cooperative modular reinforcement learning (CMRL) method to distributedly solve the problems with a large discrete action space. A general yet effective task decomposition method is proposed to decompose the complex decision task in a large action space into multiple decision sub-tasks in small action subsets, using a rule-based action division method. The CMRL method consisting of multiple Critic networks is proposed to settle the multiple sub-tasks, where each Critic network learns a decomposed value function to obtain the local optimal action in a sub-task. The global optimal action is cooperatively chosen by all local optimal actions. Moreover, we propose a new parallel training mechanism, which trains multiple Critic networks with different models and multi-data in parallel. Mathematical properties are proposed to analyze the rationality and superiority of CMRL. Four different simulation experiments are conducted to verify the generality and effectiveness of CMRL for large action space problems. The results show that CMRL has superior performance on training efficiency compared with classical and latest DRL methods while maintaining the accuracy of the solution.}
}
@article{OMALLEY2023121659,
title = {Reinforcement learning and mixed-integer programming for power plant scheduling in low carbon systems: Comparison and hybridisation},
journal = {Applied Energy},
volume = {349},
pages = {121659},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121659},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923010231},
author = {Cormac O’Malley and Patrick {de Mars} and Luis Badesa and Goran Strbac},
keywords = {Unit commitment, Reinforcement learning, Mixed-integer programming, Renewable power uncertainty},
abstract = {Decarbonisation is driving dramatic growth in renewable power generation. This increases uncertainty in the load to be served by power plants and makes their efficient scheduling, known as the unit commitment (UC) problem, more difficult. UC is solved in practice by mixed-integer programming (MIP) methods; however, there is growing interest in emerging data-driven methods including reinforcement learning (RL). In this paper, we extensively test two MIP (deterministic and stochastic) and two RL (model-free and with lookahead) scheduling methods over a large set of test days and problem sizes, for the first time comparing the state-of-the-art of these two approaches on a level playing field. We find that deterministic and stochastic MIP consistently produce lower-cost UC schedules than RL, exhibiting better reliability and scalability with problem size. Average operating costs of RL are more than 2 times larger than stochastic MIP for a 50-generator test case, while the cost is 13 times larger in the worst instance. However, the key strength of RL is the ability to produce solutions practically instantly, irrespective of problem size. We leverage this advantage to produce various initial solutions for warm starting concurrent stochastic MIP solves. By producing several near-optimal solutions simultaneously and then evaluating them using Monte Carlo methods, the differences between the true cost function and the discrete approximation required to formulate the MIP are exploited. The resulting hybrid technique outperforms both the RL and MIP methods individually, reducing total operating costs by 0.3% on average.}
}
@article{LI2024121458,
title = {Nash double Q-based multi-agent deep reinforcement learning for interactive merging strategy in mixed traffic},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121458},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121458},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423019607},
author = {Lin Li and Wanzhong Zhao and Chunyan Wang and Abbas Fotouhi and Xuze Liu},
keywords = {Reinforcement learning, Trajectory planning, Deep reinforcement learning, Autonomous driving},
abstract = {The interaction between ramp and mainline vehicles plays a crucial role in merging areas, especially in the mixed-traffic environment. The driving behaviours of human drivers are uncertain and diverse, and the uncertainty makes it more complex for connected automated vehicles (CAV) to plan trajectories and merge into the mainline. To overcome this problem, a interactive merging strategy based on multi-agent deep reinforcement learning (MADRL) is designed, enabling the ramp vehicle (CAV) to consider the dynamic reaction of mainline vehicles. There are two agents in our interactive strategy, one of which is to predict and analyse the behaviour of mainline vehicles (human-driven vehicles, HDV, or non-connected vehicles). The other is created for exploring optimal merging actions of ramp vehicles. Firstly, game theory is used to model the competitive behaviours between ramp and mainline vehicles, and the Nash equilibrium of joint actions guides the ramp vehicle to learn best response to the mainline vehicle. Secondly, the Nash double Q algorithm is developed to ensure the outputs of Q networks are trained to efficiently converge to the Nash equilibrium point. The trained Q networks are then used for online control. Finally, our strategy is compared with single RL and existing MADRL algorithms in real on-ramp scenarios. Simulations show our strategy to be successful in coordinating both vehicles via analysis of human drivers, resulting in improved driving performance in terms of global safety, efficiency, and comfort.}
}
@article{YANG201430,
title = {Discrete-time online learning control for a class of unknown nonaffine nonlinear systems using reinforcement learning},
journal = {Neural Networks},
volume = {55},
pages = {30-41},
year = {2014},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608014000719},
author = {Xiong Yang and Derong Liu and Ding Wang and Qinglai Wei},
keywords = {Adaptive critic design, Neural network, Nonaffine nonlinear system, Online learning, Reinforcement learning},
abstract = {In this paper, a reinforcement-learning-based direct adaptive control is developed to deliver a desired tracking performance for a class of discrete-time (DT) nonlinear systems with unknown bounded disturbances. We investigate multi-input–multi-output unknown nonaffine nonlinear DT systems and employ two neural networks (NNs). By using Implicit Function Theorem, an action NN is used to generate the control signal and it is also designed to cancel the nonlinearity of unknown DT systems, for purpose of utilizing feedback linearization methods. On the other hand, a critic NN is applied to estimate the cost function, which satisfies the recursive equations derived from heuristic dynamic programming. The weights of both the action NN and the critic NN are directly updated online instead of offline training. By utilizing Lyapunov’s direct method, the closed-loop tracking errors and the NN estimated weights are demonstrated to be uniformly ultimately bounded. Two numerical examples are provided to show the effectiveness of the present approach.}
}
@article{SANCHEZ201582,
title = {A priori-knowledge/actor-critic reinforcement learning architecture for computing the mean–variance customer portfolio: The case of bank marketing campaigns},
journal = {Engineering Applications of Artificial Intelligence},
volume = {46},
pages = {82-92},
year = {2015},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2015.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0952197615001980},
author = {Emma M. Sánchez and Julio B. Clempner and Alexander S. Poznyak},
keywords = {Reinforcement learning, Preprocessing, Actor-critic, Mean–variance customer portfolio, Markov chains},
abstract = {In this paper we propose a novel recurrent reinforcement learning approach for controllable Markov chains that adjusts its policies according to a preprocessing and an actor-critic architecture. The preprocessing is proposed when learning a new task is needed from reinforcement based on a priori knowledge, in order to decrease computation time and not explore and not learn everything from scratch. The actor-critic architecture is based on an iterated quadratic/Lagrange programming maximization algorithm for computing the optimal strategies of the mean–variance customer portfolio. This process can be viewed as a specific form of asynchronous value iteration with optimized computational properties. The use of only the value-maximizing action at each state is unlikely in practice. Then, a specific selection of policies is used to ensure convergence. The reinforcement model proposed predicts a learning process that takes the risk of the customer portfolio into account. The resulting policies dynamically optimize the customer portfolio. We propose to apply three different learning rules, based on the transition matrices, the utilities and the costs, to estimate the objective function for the current policies. In particular, the learning rule related to estimate the real costs imposes restrictions over the formulation of the portfolio: costs cannot be underestimated or overestimated. The learning rules allow the process to make use of past experiences and decide on future actions to take in or around a given state of the Markov chain. We provide implementation details of the learning process and the complete algorithm. In addition, we illustrate our approach with a bank marketing application example for showing the viability of the model for solving realistic problems.}
}
@article{PETSAGKOURAKIS2020106649,
title = {Reinforcement learning for batch bioprocess optimization},
journal = {Computers & Chemical Engineering},
volume = {133},
pages = {106649},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2019.106649},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419304168},
author = {P. Petsagkourakis and I.O. Sandoval and E. Bradford and D. Zhang and E.A. {del Rio-Chanona}},
keywords = {Machine learning, Batch optimization, Recurrent neural networks, Bioprocesses, Policy gradient, Uncertain dynamic systems, Nonsmooth},
abstract = {Bioprocesses have received a lot of attention to produce clean and sustainable alternatives to fossil-based materials. However, they are generally difficult to optimize due to their unsteady-state operation modes and stochastic behaviours. Furthermore, biological systems are highly complex, therefore plant-model mismatch is often present. To address the aforementioned challenges we propose a Reinforcement learning based optimization strategy for batch processes. In this work we applied the Policy Gradient method from batch-to-batch to update a control policy parametrized by a recurrent neural network. We assume that a preliminary process model is available, which is exploited to obtain a preliminary optimal control policy. Subsequently, this policy is updated based on measurements from the true plant. The capabilities of our proposed approach were tested on three case studies (one of which is nonsmooth) using a more complex process model for the true system embedded with adequate process disturbance. Lastly, we discussed advantages and disadvantages of this strategy compared against current existing approaches such as nonlinear model predictive control.}
}
@article{SONG2023109902,
title = {Search and tracking strategy of autonomous surface underwater vehicle in oceanic eddies based on deep reinforcement learning},
journal = {Applied Soft Computing},
volume = {132},
pages = {109902},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109902},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622009516},
author = {Dalei Song and Wenhao Gan and Peng Yao},
keywords = {Autonomous surface underwater vehicle, Oceanic eddy, Search and tracking strategy, Deep reinforcement learning},
abstract = {Due to dynamic changes and instability of oceanic eddies, continuous tracking and sampling using mobile platforms is a challenging field. Aiming at the requirements of accurate observation of mesoscale eddies, this paper studies the problem of searching and tracking the eddy center in a mapless environment with an underactuated autonomous surface underwater vehicle (ASUV) and proposes a path planning method based on the deep reinforcement learning (DRL). Firstly, the existing observation methods are summarized, and the dynamic tracking framework of mesoscale eddies is established. Then, the DRL and long short-term memory (LSTM) are combined to train end-to-end and real-time planning strategies in an eddy environment. Finally, a high-fidelity simulation platform in the eddy environment is built, and actual data from the Kuroshio Extension region is used to verify the effectiveness and feasibility of the strategies. The experimental results show that the ASUV with the proposed strategies can realize the autonomous search and tracking of the eddy center, have good stability and real-time performance, and the tracking error is always within an acceptable range.}
}
@article{HUANG2023284,
title = {Reinforcement learning based dynamic distributed routing scheme for mega LEO satellite networks},
journal = {Chinese Journal of Aeronautics},
volume = {36},
number = {2},
pages = {284-291},
year = {2023},
issn = {1000-9361},
doi = {https://doi.org/10.1016/j.cja.2022.06.021},
url = {https://www.sciencedirect.com/science/article/pii/S1000936122001297},
author = {Yixin HUANG and Shufan WU and Zeyu KANG and Zhongcheng MU and Hai HUANG and Xiaofeng WU and Andrew Jack TANG and Xuebin CHENG},
keywords = {LEO satellite networks, Mega constellation, Multi-objective optimization, Routing algorithm, Reinforcement learning},
abstract = {Recently, mega Low Earth Orbit (LEO) Satellite Network (LSN) systems have gained more and more attention due to low latency, broadband communications and global coverage for ground users. One of the primary challenges for LSN systems with inter-satellite links is the routing strategy calculation and maintenance, due to LSN constellation scale and dynamic network topology feature. In order to seek an efficient routing strategy, a Q-learning-based dynamic distributed Routing scheme for LSNs (QRLSN) is proposed in this paper. To achieve low end-to-end delay and low network traffic overhead load in LSNs, QRLSN adopts a multi-objective optimization method to find the optimal next hop for forwarding data packets. Experimental results demonstrate that the proposed scheme can effectively discover the initial routing strategy and provide long-term Quality of Service (QoS) optimization during the routing maintenance process. In addition, comparison results demonstrate that QRLSN is superior to the virtual-topology-based shortest path routing algorithm.}
}
@article{MOWBRAY2022107630,
title = {Safe chance constrained reinforcement learning for batch process control},
journal = {Computers & Chemical Engineering},
volume = {157},
pages = {107630},
year = {2022},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107630},
url = {https://www.sciencedirect.com/science/article/pii/S0098135421004087},
author = {M. Mowbray and P. Petsagkourakis and E.A. {del Rio-Chanona} and D. Zhang},
keywords = {Safe reinforcement learning, Optimal control, Dynamic optimization, Bioprocess operation, Machine learning},
abstract = {Reinforcement Learning (RL) controllers have generated excitement within the control community. The primary advantage of RL controllers relative to existing methods is their ability to optimize uncertain systems independently of explicit assumption of process uncertainty. Recent focus on engineering applications has been directed towards the development of safe RL controllers. Previous works have proposed approaches to account for constraint satisfaction through constraint tightening from the domain of stochastic model predictive control. Here, we extend these approaches to account for plant-model mismatch. Specifically, we propose a data-driven approach that utilizes Gaussian processes for the offline simulation model and use the associated posterior uncertainty prediction to account for joint chance constraints and plant-model mismatch. The method is benchmarked against nonlinear model predictive control via case studies. The results demonstrate the ability of the methodology to account for process uncertainty, enabling satisfaction of joint chance constraints even in the presence of plant-model mismatch.}
}
@article{ZHAO2020328,
title = {Deep reinforcement learning based lane detection and localization},
journal = {Neurocomputing},
volume = {413},
pages = {328-338},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.06.094},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220310833},
author = {Zhiyuan Zhao and Qi Wang and Xuelong Li},
keywords = {Lane detection, Deep reinforcement learning, Lane localization, Q-Learning},
abstract = {Recently, deep-learning based lane detection methods effectively boost the development of Advanced Driver Assistance Systems (ADAS) and Self-Driving Systems. However, these methods only detect lane lines with sketchy bounding boxes while ignore the shape of specific curved lanes. To address the above problems, this paper introduces deep reinforcement learning into cursory lane detection models for accurate lane detection and localization. This model consists of two stages, namely the bounding box detector and landmark point localizer. To be specific, a bounding box level convolution neural network lane detector outputs the preliminary location of lanes in the form of bounding boxes. Then, a reinforcement based Deep Q-Learning Localizer (DQLL) accurately localizes the lanes as a group of landmarks to achieve better representation of curved lanes. Moreover, a pixel-level lane detection dataset named NWPU Lanes Dataset is constructed and released. It contains a variety of real traffic scenes and accurate masks of the lane lines. This approach achieves competitive performance in the released dataset and TuSimple Lane dataset. Furthermore, the codes and dataset will be released on https://github.com/tuzixini/DQLL.}
}
@article{ZHANG2022418,
title = {A deep reinforcement learning based hyper-heuristic for combinatorial optimisation with uncertainties},
journal = {European Journal of Operational Research},
volume = {300},
number = {2},
pages = {418-427},
year = {2022},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2021.10.032},
url = {https://www.sciencedirect.com/science/article/pii/S0377221721008821},
author = {Yuchang Zhang and Ruibin Bai and Rong Qu and Chaofan Tu and Jiahuan Jin},
keywords = {Transportation, 2D packing, Hyper-heuristics, Deep reinforcement learning, Container truck routing},
abstract = {In the past decade, considerable advances have been made in the field of computational intelligence and operations research. However, the majority of these optimisation approaches have been developed for deterministically formulated problems, the parameters of which are often assumed perfectly predictable prior to problem-solving. In practice, this strong assumption unfortunately contradicts the reality of many real-world problems which are subject to different levels of uncertainties. The solutions derived from these deterministic approaches can rapidly deteriorate during execution due to the over-optimisation without explicit consideration of the uncertainties. To address this research gap, a deep reinforcement learning based hyper-heuristic framework is proposed in this paper. The proposed approach enhances the existing hyper-heuristics with a powerful data-driven heuristic selection module in the form of deep reinforcement learning on parameter-controlled low-level heuristics, to substantially improve their handling of uncertainties while optimising across various problems. The performance and practicality of the proposed hyper-heuristic approach have been assessed on two combinatorial optimisation problems: a real-world container terminal truck routing problem with uncertain service times and the well-known online 2D strip packing problem. The experimental results demonstrate its superior performance compared to existing solution methods for these problems. Finally, the increased interpretability of the proposed deep reinforcement learning hyper-heuristic has been exhibited in comparison with the conventional deep reinforcement learning methods.}
}
@article{ANDERSEN2020467,
title = {Towards safe reinforcement-learning in industrial grid-warehousing},
journal = {Information Sciences},
volume = {537},
pages = {467-484},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520305740},
author = {Per-Arne Andersen and Morten Goodwin and Ole-Christoffer Granmo},
keywords = {Model-based reinforcement learning, Neural networks, Variational autoencoder, Markov decision processes, Exploration, Safe reinforcement learning},
abstract = {Reinforcement learning has shown to be profoundly successful at learning optimal policies for simulated environments using distributed training with extensive compute capacity. Model-free reinforcement learning uses the notion of trial and error, where the error is a vital part of learning the agent to behave optimally. In mission-critical, real-world environments, there is little tolerance for failure and can cause damaging effects on humans and equipment. In these environments, current state-of-the-art reinforcement learning approaches are not sufficient to learn optimal control policies safely. On the other hand, model-based reinforcement learning tries to encode environment transition dynamics into a predictive model. The transition dynamics describes the mapping from one state to another, conditioned on an action. If this model is accurate enough, the predictive model is sufficient to train agents for optimal behavior in real environments. This paper presents the Dreaming Variational Autoencoder (DVAE) for safely learning good policies with a significantly lower risk of catastrophes occurring during training. The algorithm combines variational autoencoders, risk-directed exploration, and curiosity to train deep-q networks inside ”dream” states. We introduce a novel environment, ASRS-Lab, for research in the safe learning of autonomous vehicles in grid-based warehousing. The work shows that the proposed algorithm has better sample efficiency with similar performance to novel model-free deep reinforcement learning algorithms while maintaining safety during training.}
}
@article{WANG2022109998,
title = {Uncertainty quantification for operators in online reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {258},
pages = {109998},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109998},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122010917},
author = {Bi Wang and Jianqing Wu and Xuelian Li and Jun Shen and Yangjun Zhong},
keywords = {Q-learning, Reinforcement learning, Uncertainty},
abstract = {In online reinforcement learning, operators predict the return by weighting the successors’ estimated value. However, due to the lack of uncertainty quantification, weights assigned by operators are affected by the potentially biased estimations. As a result, the partial order of estimated values is ineffective. To increase the probability of outputting an optimal partial order, this paper introduces the hedonistic expected value (HEV), an upper bound of the return’s expectation to quantify the uncertainty. Notably, for compatibility reasons, some complex operators are rewritten as the weighted-sum forms. Based on the weighted-sum form of the operator, the variant Q-learning, namely uncertainty quantification based Q-learning is proposed in this paper. In the proposed algorithm, the weights assigned by HEV of the successors are compatible with the existing operators. The prediction of the return is not only the sum over the weights succeeding the operator but also over the weights following HEV through re-weighting. The greediness of the re-weighted operator is unchanged, and the contraction mapping indicates the convergence can be maintained. We demonstrate that the proposed algorithm with HEV performs favorably in practice.}
}
@article{WANG201831,
title = {A Novel Approach to Feedback Control with Deep Reinforcement Learning⁎⁎This work is supported by in part by MITACS, Alberta Innovates and Natural Sciences Engineering Research Council of Canada.},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {18},
pages = {31-36},
year = {2018},
note = {10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.241},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318319177},
author = {Yuan Wang and Kirubakaran Velswamy and Biao Huang},
keywords = {Reinforcement Learning, Feedback Control, Deep Learning, Artificial Intelligence, Neural Networks},
abstract = {A novel deep reinforcement learning (RL) algorithm is applied for feedback control application. We propose Proximal Actor-Critic, a model-free reinforcement learning algorithm that can learn robust feedback control laws from direct interaction data from the plant. We show efficacy of the algorithm on a benchmark problem in Heating Ventilation and Air Conditioning (HVAC) heating system, with the RL controller achieving lower Integral Absolute Error (IAE) and Integral Square Error(ISE) as compared to baseline Proportional-Integral (PI) and Linear Quadratic Regulator (LQR) controllers. We also provide details on establishing feedback control problems within the deep reinforcement learning framework, including policy parameterization, neural network architecture and training procedures.}
}
@article{YOO2021107133,
title = {Reinforcement learning based optimal control of batch processes using Monte-Carlo deep deterministic policy gradient with phase segmentation},
journal = {Computers & Chemical Engineering},
volume = {144},
pages = {107133},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.107133},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420307912},
author = {Haeun Yoo and Boeun Kim and Jong Woo Kim and Jay H. Lee},
keywords = {Batch process, Reinforcement learning, Optimal control, Actor-Critic},
abstract = {Batch process control represents a challenge given its dynamic operation over a large operating envelope. Nonlinear model predictive control (NMPC) is the current standard for optimal control of batch processes. The performance of conventional NMPC can be unsatisfactory in the presence of uncertainties. Reinforcement learning (RL) which can utilize simulation or real operation data is a viable alternative for such problems. To apply RL to batch process control effectively, however, choices such as the reward function design and value update method must be made carefully. This study proposes a phase segmentation approach for the reward function design and value/policy function representation. In addition, the deep deterministic policy gradient algorithm (DDPG) is modified with Monte-Carlo learning to ensure more stable and efficient learning behavior. A case study of a batch polymerization process producing polyols is used to demonstrate the improvement brought by the proposed approach and to highlight further issues.}
}
@article{JIANG2023,
title = {Optimal scheduling of distributed hydrogen refueling stations for fuel supply and reserve demand service with evolutionary transfer multi-agent reinforcement learning},
journal = {International Journal of Hydrogen Energy},
year = {2023},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2023.04.128},
url = {https://www.sciencedirect.com/science/article/pii/S0360319923018633},
author = {Yuewen Jiang and Jianshu Liu and Hongqi Zheng},
keywords = {Distributed hydrogen refueling stations, Collaborative scheduling, Transfer multi-agent reinforcement learning, Reserve ancillary service, Fuel supply},
abstract = {With the extensive utilization of fossil fuels, environmental concerns have been growing. Hydrogen-powered vehicles (HPVs) as well as hydrogen refueling stations (HRSs) are expected to proliferate to alleviate the pollution. In order to promote the economic valibity of HRSs, this paper proposes an optimal scheduling model for distributed HRSs to maximize HRSs’ revenue during a day. HRSs serves both the mobility sector with fuel individually and the power system with reserve demand response collaboratively. The elaborated operating of electrolysers and relaxed control of hydrogen storage are involved to enhance practicability and flexibility of the scheduling. Considering that this model has multiple variables, intractable high-dimension nonlinear constraints, a transfer multi-agent reinforcement learning algorithm is developed. The algorithm consists of two parts: the one is the multi-agent optimization method by transforming multi-decision optimization into a multi-agent optimization; the other is to transfer the empirical knowledge of the source task to reinforcement learning according to the similarity between the source task and the target task, making it cut down the blindness of exploration and accelerate convergence. Numerical studies reveal that participating in the reserve services market collaboratively increases overall revenues of HRSs by up to 32.90%, and the computation time is sharply shortened by approximately 34 times compared to the basic reinforcement learning.}
}
@article{ALIBABAEI2022107480,
title = {Irrigation optimization with a deep reinforcement learning model: Case study on a site in Portugal},
journal = {Agricultural Water Management},
volume = {263},
pages = {107480},
year = {2022},
issn = {0378-3774},
doi = {https://doi.org/10.1016/j.agwat.2022.107480},
url = {https://www.sciencedirect.com/science/article/pii/S0378377422000270},
author = {Khadijeh Alibabaei and Pedro D. Gaspar and Eduardo Assunção and Saeid Alirezazadeh and Tânia M. Lima},
keywords = {Agriculture, LSTM, Deep reinforcement learning, Irrigation scheduling},
abstract = {In the field of agriculture, the water used for irrigation should be given special treatment, as it is responsible for a large proportion of total water consumption. Irrigation scheduling is critical to food production because it guarantees producers a consistent harvest and minimizes the risk of losses due to water shortages. Therefore, the creation of an automatic irrigation method using new technologies is essential. New methods such as deep learning algorithms have attracted a lot of attention in agriculture and are already being used successfully. In this work, a Deep Q-Network was trained for irrigation scheduling. The agent was trained to schedule irrigation for a tomato field in Portugal. Two Long Short Term Memory models were used as the agent environment. One predicts the total water in the soil profile on the next day. The other one was employed to estimate the yield based on the environmental condition during a season and then measure the net return. The agent uses this information to decide the following irrigation amount. An Artificial Neural Network, a Long Short Term Memory, and a Convolutional Neural Network were used to estimating the Q-table during training. Unlike the Long-Short Terms Memory model, the Artificial Neural Network and the Convolutional Neural Network could not estimate the Q-table, and the agent’s reward decreased during training. The comparison of the performance of the model was done with fixed base irrigation and threshold based irrigation. The trained model increased productivity by 11% and decreased water consumption by 20–30% compared to the fixed method.}
}
@article{QIN2022103852,
title = {Reinforcement learning for ridesharing: An extended survey},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {144},
pages = {103852},
year = {2022},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.103852},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X22002716},
author = {Zhiwei (Tony) Qin and Hongtu Zhu and Jieping Ye},
keywords = {Ridesharing, Ride-pooling, Reinforcement learning, Multi-agent systems, Decision intelligence},
abstract = {In this paper, we present a comprehensive, in-depth survey of the literature on reinforcement learning approaches to decision optimization problems in a typical ridesharing system. Papers on the topics of rideshare matching, vehicle repositioning, ride-pooling, routing, and dynamic pricing are covered. Most of the literature has appeared in the last few years, and several core challenges are to continue to be tackled: model complexity, agent coordination, and joint optimization of multiple levers. Hence, we also introduce popular data sets and open simulation environments to facilitate further research and development. Subsequently, we discuss a number of challenges and opportunities for reinforcement learning research on this important domain.}
}
@article{LI2021116977,
title = {Cloud-based health-conscious energy management of hybrid battery systems in electric vehicles with deep reinforcement learning},
journal = {Applied Energy},
volume = {293},
pages = {116977},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116977},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921004499},
author = {Weihan Li and Han Cui and Thomas Nemeth and Jonathan Jansen and Cem Ünlübayir and Zhongbao Wei and Xuning Feng and Xuebing Han and Minggao Ouyang and Haifeng Dai and Xuezhe Wei and Dirk Uwe Sauer},
keywords = {Energy management, Vehicle-to-cloud, Reinforcement learning, Battery aging, Lithium-ion, Battery safety},
abstract = {In order to fulfill the energy and power demand of battery electric vehicles, a hybrid battery system with a high-energy and a high-power battery pack can be implemented as the energy source. This paper explores a cloud-based multi-objective energy management strategy for the hybrid architecture with a deep deterministic policy gradient, which increases the electrical and thermal safety, and meanwhile minimizes the system’s energy loss and aging cost. In order to simulate the electro-thermal dynamics and aging behaviors of the batteries, models are built for both high-energy and high-power cells based on the characterization and aging tests. A cloud-based training approach is proposed for energy management with real-world vehicle data collected from various road conditions. Results show the improvement of electrical and thermal safety, as well as the reduction of energy loss and aging cost of the whole system with the proposed strategy based on the collected real-world driving data. Furthermore, processor-in-the-loop tests verify that the proposed strategy can achieve a much higher convergence rate and a better performance in terms of the minimization of both energy loss and aging cost compared with state-of-the-art learning-based strategies.}
}
@article{ROCCHETTA2019291,
title = {A reinforcement learning framework for optimal operation and maintenance of power grids},
journal = {Applied Energy},
volume = {241},
pages = {291-301},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.03.027},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919304222},
author = {R. Rocchetta and L. Bellani and M. Compare and E. Zio and E. Patelli},
keywords = {Reinforcement learning, Artificial neural networks, Prognostic and health management, Operation and maintenance, Power grid, Uncertainty},
abstract = {We develop a Reinforcement Learning framework for the optimal management of the operation and maintenance of power grids equipped with prognostics and health management capabilities. Reinforcement learning exploits the information about the health state of the grid components. Optimal actions are identified maximizing the expected profit, considering the aleatory uncertainties in the environment. To extend the applicability of the proposed approach to realistic problems with large and continuous state spaces, we use Artificial Neural Networks (ANN) tools to replace the tabular representation of the state-action value function. The non-tabular Reinforcement Learning algorithm adopting an ANN ensemble is designed and tested on the scaled-down power grid case study, which includes renewable energy sources, controllable generators, maintenance delays and prognostics and health management devices. The method strengths and weaknesses are identified by comparison to the reference Bellman’s optimally. Results show good approximation capability of Q-learning with ANN, and that the proposed framework outperforms expert-based solutions to grid operation and maintenance management.}
}
@article{SUN2019286,
title = {An integrated critic-actor neural network for reinforcement learning with application of DERs control in grid frequency regulation},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {111},
pages = {286-299},
year = {2019},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2019.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0142061518336767},
author = {Jian Sun and Zhiqin Zhu and Huaqing Li and Yi Chai and Guanqiu Qi and Huiwei Wang and Yu Hen Hu},
keywords = {Power grid, Control theory, Reinforcement learning, Neural networks},
abstract = {As the electronically-interfaced distributed energy resources (DERs) grow rapidly in power grid, power demand satisfaction and frequency regulation are two main challenges in control area. However, it is difficult to model the analysis of a large-scale grid as well as design a stable and optimal control scheme. With the support of DERs, this paper proposes an actor-critic neural network that integrates a distributed reinforcement learning control scheme to compensate frequency regulation of power grid. The short-term performance and stability is improved by a deterministic learning algorithm that is used to obtain the approximation of desired control output. Meanwhile, a long-term strategic utility function is estimated by the integrated actor-critic neural network. The mapping from system state and control output to the strategic utility function value is identified by neural network, as well as utilized in sub-optimal control learning for further improvement of long-term system performance. Theoretical analysis guarantees the stability. Frequency deviation, tie-line power flow, and long-term cost are coincident with uniform ultimate boundness (UUB). In addition, the upper bound of long-term system cost is also reckoned. The effectiveness and advantages of proposed scheme are illustrated in two case studies. The simulation results indicate that the proposed scheme has better performance under certain condition, compared with some actor-critic network control schemes in frequency regulation of power grid.}
}
@article{WANG20073764,
title = {A fuzzy Actor–Critic reinforcement learning network},
journal = {Information Sciences},
volume = {177},
number = {18},
pages = {3764-3781},
year = {2007},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2007.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0020025507001569},
author = {Xue-Song Wang and Yu-Hu Cheng and Jian-Qiang Yi},
keywords = {Reinforcement learning, Actor–Critic learning, Fuzzy inference system, Radial basis function neural network},
abstract = {One of the difficulties encountered in the application of reinforcement learning methods to real-world problems is their limited ability to cope with large-scale or continuous spaces. In order to solve the curse of the dimensionality problem, resulting from making continuous state or action spaces discrete, a new fuzzy Actor–Critic reinforcement learning network (FACRLN) based on a fuzzy radial basis function (FRBF) neural network is proposed. The architecture of FACRLN is realized by a four-layer FRBF neural network that is used to approximate both the action value function of the Actor and the state value function of the Critic simultaneously. The Actor and the Critic networks share the input, rule and normalized layers of the FRBF network, which can reduce the demands for storage space from the learning system and avoid repeated computations for the outputs of the rule units. Moreover, the FRBF network is able to adjust its structure and parameters in an adaptive way with a novel self-organizing approach according to the complexity of the task and the progress in learning, which ensures an economic size of the network. Experimental studies concerning a cart–pole balancing control illustrate the performance and applicability of the proposed FACRLN.}
}
@article{CORREAJULLIAN2020114943,
title = {Operation scheduling in a solar thermal system: A reinforcement learning-based framework},
journal = {Applied Energy},
volume = {268},
pages = {114943},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.114943},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920304554},
author = {Camila Correa-Jullian and Enrique {López Droguett} and José Miguel Cardemil},
keywords = {Solar hot water systems, Reinforcement learning, Intelligent control systems, Condition-based decision making, Q-learning, Machine learning},
abstract = {Reinforcement learning (RL) provides an alternative method for designing condition-based decision making in engineering systems. In this study, a simple and flexible RL tabular Q-learning framework is employed to identify the optimal operation schedules for a solar hot water system according to action–reward feedback. The system is simulated in TRNSYS software. Three energy sources must supply a building’s hot-water demand: low-cost heat from solar thermal collectors and a heat-recovery chiller, coupled to a conventional heat pump. Key performance indicators are used as rewards for balancing the system’s performance with regard to energy efficiency, heat-load delivery, and operational costs. A sensitivity analysis is performed for different reward functions and meteorological conditions. Optimal schedules are obtained for selected scenarios in January, April, July, and October, according to the dynamic conditions of the system. The results indicate that when solar radiation is widely available (October through April), the nominal operation schedule frequently yields the highest performance. However, the obtained schedule differs when the solar radiation is reduced, for instance, in July. On average, with prioritization of the efficient use of both low-cost energy sources, the performance in July can be on average 21% higher than under nominal schedule-based operation.}
}
@article{HAN2023126789,
title = {Symmetric actor–critic deep reinforcement learning for cascade quadrotor flight control},
journal = {Neurocomputing},
volume = {559},
pages = {126789},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126789},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223009128},
author = {Haoran Han and Jian Cheng and Zhilong Xi and Maolong Lv},
keywords = {Quadrotor, Flight control, Deep reinforcement learning, Symmetric actor and critic},
abstract = {Even though deep reinforcement learning (DRL) has been extensively applied to quadrotor flight control to simplify parameter adjustment, it has some drawbacks in terms of control performance, such as instability and asymmetry. To address these problems, we propose an odd symmetric actor to achieve stable and symmetric control performance, and an even critic to stabilize the training process. Concretely, the bias of neural networks is eliminated, and the absolute value operation is adopted to construct the activation function. Furthermore, we devise a cascade architecture, where each module trained with DRL controls a symmetric subsystem of the quadrotor. Comparative simulations have verified the effectiveness of the proposed control scheme, which shows superiority in dealing with high-dimensional, nonlinear subsystems and disadvantage in dealing with low-dimensional, linear subsystems.}
}
@article{MUGNINI2022100264,
title = {Advanced control techniques for CHP-DH systems: A critical comparison of Model Predictive Control and Reinforcement Learning},
journal = {Energy Conversion and Management: X},
volume = {15},
pages = {100264},
year = {2022},
issn = {2590-1745},
doi = {https://doi.org/10.1016/j.ecmx.2022.100264},
url = {https://www.sciencedirect.com/science/article/pii/S2590174522000873},
author = {A. Mugnini and F. Ferracuti and M. Lorenzetti and G. Comodi and A. Arteconi},
keywords = {Model Predictive Control, Reinforcement Learning, Combined Heat and Power, District Heating, Operational Data},
abstract = {District heating (DH) network is a key infrastructure to decarbonize the heating sector through the centralized production of heat distributed to final users. The implementation of advanced control techniques is increasingly common in the field of energy optimization since they can provide a more efficient way of minimizing energy demand by appropriate scheduling of the control variables. The aim of this work is to present the application of two control strategies, i.e., Model Predictive Control (MPC) and Reinforcement Learning (RL), to a system based on a DH network supplied by a Combined Heat and Power plant (CHP-DH plant). The analyzed case study is a real CHP-DH plant operating in the small Italian town of Osimo (central Italy). The DH network currently connects more than 1200 users, generating peak heat demand of about 9.7 MWth. The heat generator is composed of a natural gas fueled internal combustion engine coupled with natural gas boilers. The work provides a comparison between the current control strategy (deduced from measured data) and the performance of the CHP-DH plant controlled with an MPC and an RL control. The results showed the effectiveness of the two controls in satisfying the thermal demand of the users, while minimizing the thermal losses towards the ground. Both MPC and RL allow to implement control strategies different from the current control in terms of supply temperature and flow rate circulating in the network. Referring to the winter months, in which the current operation of the system tends to prefer high supply temperatures, the advanced controls made it possible to reduce the thermal heat supply by reducing the thermal losses of about 3.9 % with the MPC and 6.54 % with the RL, corresponding to emission avoidances up to 23.3 tCO2 and 12.6 tCO2, respectively. The paper, as well as showing the application of the controls, contains a critical discussion of all the positive aspects and weaknesses found in the application of the MPC and the RL control to the case study.}
}
@article{XIONG2023216,
title = {Coordinated energy management strategy for multi-energy hub with thermo-electrochemical effect based power-to-ammonia: A multi-agent deep reinforcement learning enabled approach},
journal = {Renewable Energy},
volume = {214},
pages = {216-232},
year = {2023},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2023.05.067},
url = {https://www.sciencedirect.com/science/article/pii/S0960148123006900},
author = {Kang Xiong and Weihao Hu and Di Cao and Sichen Li and Guozhou Zhang and Wen Liu and Qi Huang and Zhe Chen},
keywords = {Power-to-ammonia, Renewable energy, Multi-energy hub, Multi-agent deep reinforcement learning},
abstract = {Power-to-ammonia (P2A) technology has attracted more and more attention since ammonia is recognized as a natural zero-carbon fuel. In this context, this paper constructs a renewable energy powered multi-energy hub (MEH) system which integrates with a thermo-electrochemical effect based P2A facility. Subsequently, the energy management of proposed MEH system is casted to a multi-agent coordinated optimization problem, which aims to minimize operating cost and carbon dioxide emissions while satisfying constraints. Then, a novel multi-agent deep reinforcement learning method called CommNet is applied to solve this problem to obtain the optimal coordinated energy management strategy of each energy hub by achieving the distributed computation of global information. Finally, the simulation results show that the proposed method can achieve better performance on reducing operating cost and carbon emissions than other benchmark methods.}
}
@article{KHALID2023104311,
title = {Deep reinforcement learning-based long-range autonomous valet parking for smart cities},
journal = {Sustainable Cities and Society},
volume = {89},
pages = {104311},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.104311},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722006151},
author = {Muhammad Khalid and Liang Wang and Kezhi Wang and Nauman Aslam and Cunhua Pan and Yue Cao},
keywords = {Long-range autonomous valet parking (LAVP), Autonomous vehicle, Deep reinforcement learning, Ant colony optimization (ACO), Sustainable cities and communities},
abstract = {In this paper, to reduce the congestion rate at the city center and increase the traveling quality of experience (QoE) of each user, the framework of long-range autonomous valet parking is presented. Here, an Autonomous Vehicle (AV) is deployed to pick up, and drop off users at their required spots, and then drive to the car park around well-organized places of city autonomously. In this framework, we aim to minimize the overall distance of AV, while guarantee all users are served with great QoE, i.e., picking up, and dropping off users at their required spots through optimizing the path planning of the AV and number of serving time slots. To this end, we first present a learning-based algorithm, which is named as Double-Layer Ant Colony Optimization (DLACO) algorithm to solve the above problem in an iterative way. Then, to make the fast decision, while considers the dynamic environment (i.e., the AV may pick up and drop off users from different locations), we further present a deep reinforcement learning-based algorithm, i.e., Deep Q-learning Network (DQN) to solve this problem. Experimental results show that the DL-ACO and DQN-based algorithms both achieve the considerable performance.}
}
@article{QIU2021116940,
title = {Scalable coordinated management of peer-to-peer energy trading: A multi-cluster deep reinforcement learning approach},
journal = {Applied Energy},
volume = {292},
pages = {116940},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116940},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921004189},
author = {Dawei Qiu and Yujian Ye and Dimitrios Papadaskalopoulos and Goran Strbac},
keywords = {Energy management, Distributed energy resources, Multi-agent deep reinforcement learning, Peer-to-peer energy trading, Smart grid},
abstract = {The increasing penetration of small-scale distributed energy resources (DER) has the potential to support cost-efficient energy balancing in emerging electricity systems, but is also fundamentally affecting the conventional operation paradigm of the latter. In this context, innovative market mechanisms need to be devised to better coordinate and provide incentives for DER to utilize their flexibility. Peer-to-Peer (P2P) energy trading has emerged as an alternative approach to facilitate direct trading between consumers and prosumers interacting in an energy collective and fosters more efficient local demand–supply balancing. While previous research has primarily focused on the technical and economic benefits of P2P trading, little effort has been made towards the incorporation of prosumers’ heterogeneous characteristics in the P2P trading problem. Here, we address this research gap by classifying the participating prosumers into multiple clusters with regard to their portfolio of DER, and analyzing their trading decisions in a simulated P2P trading platform. The latter employs the mid-market rate (MMR) local pricing mechanism to enable energy trading among prosumers and penalizes the contribution to the system demand peak of each prosumer. We formulate the P2P trading problem as a multi-agent coordination problem and propose a novel multi-agent deep reinforcement learning (MADRL) method to address it. The proposed method is founded on the combination of the multi-agent deep deterministic policy gradient (MADDPG) algorithm and the technique of parameter sharing (PS), which not only enables accelerating the training speed by sharing experiences and learned policies between all agents in each cluster, but also sustains the policies’ diversity between multiple clusters. To address the non-stationarity and computational complexity of MADRL as well as persevering the privacy of prosumers, the P2P trading platform acts as a trusted third party which augments the market collective trading information to help training of prosumer agents. Experiments with a large-scale real-world data-set involving 300 residential households demonstrate that the proposed MADRL method exhibits a strong generalization capability in the test data-set and outperforms the state-of-the-art MADRL methods with regard to the system operation cost, demand peak as well as computational time.}
}
@article{TUFENKCI2023119192,
title = {A theoretical demonstration for reinforcement learning of PI control dynamics for optimal speed control of DC motors by using Twin Delay Deep Deterministic Policy Gradient Algorithm},
journal = {Expert Systems with Applications},
volume = {213},
pages = {119192},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.119192},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422022102},
author = {Sevilay Tufenkci and Baris {Baykant Alagoz} and Gurkan Kavuran and Celaleddin Yeroglu and Norbert Herencsar and Shibendu Mahata},
keywords = {Deep reinforcement learning, DC motor, PI controller, Twin-delayed deep deterministic policy gradient, Metaheuristic optimization},
abstract = {To benefit from the advantages of Reinforcement Learning (RL) in industrial control applications, RL methods can be used for optimal tuning of the classical controllers based on the simulation scenarios of operating conditions. In this study, the Twin Delay Deep Deterministic (TD3) policy gradient method, which is an effective actor-critic RL strategy, is implemented to learn optimal Proportional Integral (PI) controller dynamics from a Direct Current (DC) motor speed control simulation environment. For this purpose, the PI controller dynamics are introduced to the actor-network by using the PI-based observer states from the control simulation environment. A suitable Simulink simulation environment is adapted to perform the training process of the TD3 algorithm. The actor-network learns the optimal PI controller dynamics by using the reward mechanism that implements the minimization of the optimal control objective function. A setpoint filter is used to describe the desired setpoint response, and step disturbance signals with random amplitude are incorporated in the simulation environment to improve disturbance rejection control skills with the help of experience based learning in the designed control simulation environment. When the training task is completed, the optimal PI controller coefficients are obtained from the weight coefficients of the actor-network. The performance of the optimal PI dynamics, which were learned by using the TD3 algorithm and Deep Deterministic Policy Gradient algorithm, are compared. Moreover, control performance improvement of this RL based PI controller tuning method (RL-PI) is demonstrated relative to performances of both integer and fractional order PI controllers that were tuned by using several popular metaheuristic optimization algorithms such as Genetic Algorithm, Particle Swarm Optimization, Grey Wolf Optimization and Differential Evolution.}
}
@article{XU2019232,
title = {Morphing control of a new bionic morphing UAV with deep reinforcement learning},
journal = {Aerospace Science and Technology},
volume = {92},
pages = {232-243},
year = {2019},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2019.05.058},
url = {https://www.sciencedirect.com/science/article/pii/S127096381930344X},
author = {Dan Xu and Zhe Hui and Yongqi Liu and Gang Chen},
keywords = {Morphing aircraft, Deep neural networks, Reinforcement learning, Actor-critic, Model-free, Deep deterministic policy gradient},
abstract = {With rapid development of aviation technology, materials science and artificial intelligence, aircraft design is pursuing higher requirements both in civil and military fields. The new generation of aircraft should have the autonomous capable of performing a variety of tasks (such as take-off and landing, cruising, maneuvering, hover, attack, etc.) under a highly variable flight environment (height, Mach number, etc.) and meanwhile maintaining good performance. Morphing aircraft can use smart materials and actuators to autonomously deform the shape according to the changes in flight environment and mission, and always maintain an optimal aerodynamic shape, therefore get flourished developments. Based on the ability of birds to stretch wings when flying at low speed and to constrict wings at high speed, a new bionic morphing UAV has been designed and developed as the study model by our team. In order to make this new aircraft be able to complete rapid autonomous morphing and aerodynamic performance optimization under different missions and flight conditions, we developed deep neural networks and reinforcement learning techniques as a control strategy. Considering the continuity of the state and action spaces for model, the Deep Deterministic Policy Gradient (DDPG) algorithm based on the actor-critic, model-free algorithm was adopted and verified on the classic nonlinear Pendulum model and Cart Pole game. After the feasibility was verified, morphing aircraft model was controlled to complete prescribed deformation using DDPG algorithm. Furthermore, on the condition that the DDPG algorithm can control morphing well, through training and testing on model using simulation data from wind tunnel tests and actual flight, the autonomous morphing control for the shape optimization of the bionic morphing UAV model could be realized.}
}
@incollection{ZOU2023267,
title = {Chapter 7 - Meta-reinforcement learning},
editor = {Lan Zou},
booktitle = {Meta-Learning},
publisher = {Academic Press},
pages = {267-297},
year = {2023},
isbn = {978-0-323-89931-4},
doi = {https://doi.org/10.1016/B978-0-323-89931-4.00011-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323899314000110},
author = {Lan Zou},
keywords = {Artificial intelligence, Machine learning, Meta-reinforcement learning, Reinforcement learning, Visual navigation, Deep reinforcement learning, Robotics, Computer vision, Applied linguistics},
abstract = {Meta-reinforcement learning (Meta-RL) is an ambitious technology step toward general AI learning as automatically and efficiently as, or even exceeding, human beings. In this chapter, initially, the basics of deep reinforcement learning and Meta-RL are introduced: basic components, simulated environments, limitations, early development, and formalism. Three fundamental components (i.e., a memory, a Meta-RL paradigm, and a Markov decision processes distribution) is explored in the following sections: Section 7.3 discusses different approaches regarding memory. Even as Meta-RL is just emerging, the passion from the researcher is dramatically increasing while a variety of Meta-RL methods has been proposed for multiple scenarios. These are investigated in Section 7.4. Section 7.5 examines the reward function and environments for the Meta-RL setup. Besides the above essential components during Meta-RL training periods, a standardized benchmark for broad Meta-RL assessments and evolution is reviewed. Visual navigation is investigated with couple meta-learning strategies. The chapter ends up with an evaluation of pros and cons of Meta-RL.}
}
@article{ZHANG2023108965,
title = {Performance analysis of deep reinforcement learning-based intelligent cooperative jamming method confronting multi-functional networked radar},
journal = {Signal Processing},
volume = {207},
pages = {108965},
year = {2023},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2023.108965},
url = {https://www.sciencedirect.com/science/article/pii/S0165168423000397},
author = {Wenxu Zhang and Tong Zhao and Zhongkai Zhao and Dan Ma and Feiran Liu},
keywords = {Double deep q network, Cognitive jamming decision-making, Multi-functional networked radar, Prioritized experience replay},
abstract = {With the development of artificial intelligence technology, more and more intelligent countermeasure methods are applied in military confrontation fields to improve the intelligent level of weapons. Traditional radar jammers generate different jamming types by template matching, game theory or reasoning, which lack intelligent and adaptive jamming strategies in the battlefield environment with intelligent confrontation. To solve the intelligent decision-making problem of jammers in radar countermeasure, a cooperative jamming decision-making (CJDM) method based on reinforcement learning (RL) is proposed in this paper. The double deep Q network based on priority experience replay (PER-DDQN) is brought into the cooperative jamming strategy, and the CJDM model based on PER-DDQN is established in this paper. The scene of multiple jammers against multi-functional networked radar was built to simulate and analyze the performance of the proposed CJDM model based on PER-DDQN. The simulation results show that the proposed PER-DDQN can overcome the problem of data correlation and avoid unnecessary iteration, which is more suitable for sparse reward environment compared with deep Q network (DQN). Meanwhile, the proposed CJDM method based on PER-DDQN can effectively and intelligently realize optimal jamming decision-making.}
}
@article{BABALOLA2017118,
title = {Adaptive Immune System reinforcement Learning-Based algorithm for real-time Cascading Failures prevention},
journal = {Engineering Applications of Artificial Intelligence},
volume = {57},
pages = {118-133},
year = {2017},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2016.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S095219761630166X},
author = {Adeniyi Abdulrasheed Babalola and Rabie Belkacemi and Sina Zarrabian and Robert Craven},
keywords = {Immune System, Cascading Failures, N-2 Contingency, N-1-1 Contingency, Reinforcement Learning, Experimental Validation},
abstract = {Artificial intelligent algorithms have found a wide-range of applications in power systems, especially in solving long-existing problems immune to non-intelligent algorithms. Cascading Failures (CF), one of such problems, require load shedding as a current industrial solution. Load shedding results in losses to all power system stakeholders. This work proposes the use of an Artificial Immune System (AIS) algorithm to intelligently adjust the power output of the generators in the power system relative to one another in real time to prevent CF. AIS gives the artificial intelligent algorithm reinforcement learning capability by enabling it to pick the appropriate combination(s) for a particular system state; hence, the algorithm is called Immune System Reinforcement Learning-Based (ISRL-Based) algorithm. The algorithm was trained offline using both static and dynamic power equations and the effectiveness of both approaches was evaluated through statistical deviation. Analyses showed that using dynamic equations resulted in a more accurate solution than the static equations. CF was dynamically simulated on the IEEE 118-Bus system after an N-2 contingency, the results obtained agrees with the results from the analysis of the 2003 Northeast USA CF event. The effectiveness of the algorithm and online training were also experimentally validated after an N-1-1 contingency in a nine-bus system.}
}
@article{HUANG2023232717,
title = {Longevity-aware energy management for fuel cell hybrid electric bus based on a novel proximal policy optimization deep reinforcement learning framework},
journal = {Journal of Power Sources},
volume = {561},
pages = {232717},
year = {2023},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2023.232717},
url = {https://www.sciencedirect.com/science/article/pii/S0378775323000927},
author = {Ruchen Huang and Hongwen He and Xuyang Zhao and Miaojue Gao},
keywords = {Fuel cell hybrid electric bus, Energy management strategy, Deep reinforcement learning, Proximal policy optimization (PPO), Multi-thread distributed computation},
abstract = {With the prosperity of artificial intelligence and new energy vehicles, energy-saving technologies for zero-emission fuel cell hybrid electric vehicles through high-efficient deep reinforcement learning algorithms have become a research focus. This article proposes an energy management strategy based on a novel deep reinforcement learning framework to reduce the hydrogen consumption of a fuel cell hybrid electric bus while suppressing the degradation of the fuel cell. To begin, a novel proximal policy optimization framework is designed by taking advantage of multi-thread distributed computation, and then a promising energy management strategy based on this novel framework is proposed. Furthermore, the fuel cell degradation model is established and fuel cell longevity is incorporated into the optimization objective. Finally, the adaptability and computational efficiency of the proposed strategy are verified under the test cycle. Simulation results indicate that the proposed strategy improves the training efficiency effectively, and achieves efficient optimization of hydrogen conservation and fuel cell degradation suppression compared with the strategy based on the proximal policy optimization algorithm. This article contributes to energy conservation and lifespan extension for fuel cell vehicles through deep reinforcement learning methods.}
}
@article{DING2023133767,
title = {Control of chaos with time-delayed feedback based on deep reinforcement learning},
journal = {Physica D: Nonlinear Phenomena},
volume = {451},
pages = {133767},
year = {2023},
issn = {0167-2789},
doi = {https://doi.org/10.1016/j.physd.2023.133767},
url = {https://www.sciencedirect.com/science/article/pii/S0167278923001215},
author = {Jianpeng Ding and Youming Lei},
keywords = {Chaos control, Time-delayed feedback, Deep reinforcement learning, Non-invasive control},
abstract = {The time-delayed feedback control method, as one of popular methods for chaos control, is noninvasive and flexible for various dynamical systems from different fields of science and technology. In the method, however, an appropriate choice of the feedback gain is challenging, which requires the explicit mathematical model of the controlled system for stability analysis. Additionally, another limitation of the method is the so-called odd number limitation. The two problems restrict its application to practical situations. Fortunately, a technique called deep reinforcement learning is capable of learning the controlled environment (the dynamical system), by continually interacting with the controlled system, which makes it possible to obtain the right feedback gain without the requirement of the accurate mathematical model of the system. Hence, in this work, a time-delayed feedback control method based on deep reinforcement learning is put forward to solve the two problems. Compared with the traditional time-delayed feedback control, the proposed method, as a data-driven method due to the combination with deep reinforcement learning, offers a time-varying feedback gain according to the well-trained policy learned by the deep reinforcement learning algorithm. It maintains the non-invasive property of the time-delayed feedback control, but expands the operating range due to the time-varying feedback gain overcoming the odd number limitation. With numerical simulations, the proposed method is successfully applied to three different kinds of systems, the discrete logistic map, the non-autonomous Duffing oscillator and the autonomous Lorenz system.}
}
@article{MILJKOVIC20131721,
title = {Neural network Reinforcement Learning for visual control of robot manipulators},
journal = {Expert Systems with Applications},
volume = {40},
number = {5},
pages = {1721-1736},
year = {2013},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0957417412010640},
author = {Zoran Miljković and Marko Mitić and Mihailo Lazarević and Bojan Babić},
keywords = {Reinforcement Learning, Neural network, Robot manipulator, Image Based Visual Servo control, Intelligent hybrid control},
abstract = {It is known that most of the key problems in visual servo control of robots are related to the performance analysis of the system considering measurement and modeling errors. In this paper, the development and performance evaluation of a novel intelligent visual servo controller for a robot manipulator using neural network Reinforcement Learning is presented. By implementing machine learning techniques into the vision based control scheme, the robot is enabled to improve its performance online and to adapt to the changing conditions in the environment. Two different temporal difference algorithms (Q-learning and SARSA) coupled with neural networks are developed and tested through different visual control scenarios. A database of representative learning samples is employed so as to speed up the convergence of the neural network and real-time learning of robot behavior. Moreover, the visual servoing task is divided into two steps in order to ensure the visibility of the features: in the first step centering behavior of the robot is conducted using neural network Reinforcement Learning controller, while the second step involves switching control between the traditional Image Based Visual Servoing and the neural network Reinforcement Learning for enabling approaching behavior of the manipulator. The correction in robot motion is achieved with the definition of the areas of interest for the image features independently in both control steps. Various simulations are developed in order to present the robustness of the developed system regarding calibration error, modeling error, and image noise. In addition, a comparison with the traditional Image Based Visual Servoing is presented. Real world experiments on a robot manipulator with the low cost vision system demonstrate the effectiveness of the proposed approach.}
}
@article{CRESPI2023110758,
title = {A population-based approach for multi-agent interpretable reinforcement learning},
journal = {Applied Soft Computing},
volume = {147},
pages = {110758},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110758},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623007767},
author = {Marco Crespi and Andrea Ferigo and Leonardo Lucio Custode and Giovanni Iacca},
keywords = {Multi-Agent Reinforcement Learning, Evolutionary algorithm, Explainable artificial intelligence},
abstract = {Multi-Agent Reinforcement Learning (MARL) made significant progress in the last decade, mainly thanks to the major developments in the field of Deep Neural Networks (DNNs). However, DNNs suffer from a fundamental issue: their lack of interpretability. While this is true for most applications of DNNs, this is exacerbated in their applications in MARL. In fact, the mutual interactions between agents and environment, as well as across agents, make it particularly difficult to understand learned strategies in these settings. One possible way to achieve explainability in MARL is through the use of interpretable models, such as decision trees, that allow for a direct inspection and understanding of their inner workings. In this work, we make a step forward in this direction, proposing a population-based algorithm that combines evolutionary principles with RL for training interpretable models in multi-agent systems. We evaluate the proposed approach in a highly dynamic task where two teams of agents compete with each other. We test different variants of the proposed method in different settings, namely with/without coevolution and with/without initialization from a handcrafted policy. We find that, in most settings, our method is able to find fairly effective policies. Moreover, we show that the learned policies are easy to inspect and, possibly, interpreted based on domain knowledge.}
}
@article{WAQAS2023121004,
title = {A novel duplex deep reinforcement learning based RRM framework for next-generation V2X communication networks},
journal = {Expert Systems with Applications},
volume = {233},
pages = {121004},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121004},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423015063},
author = {Syed Muhammad Waqas and Yazhe Tang and Fakhar Abbas and Hongyang Chen and Mehboob Hussain},
keywords = {Deep reinforcement learning, Radio resource management (RRM), Q-learning, Duplex deep Q-learning, Next-generation vehicle-to-everything communication},
abstract = {Resource management in the next-generation vehicle-to-everything (V2X) communication networks is a demanding research problem. It is difficult to achieve the best results if the resources are not managed efficiently. To address this challenge and facilitate the dynamic movement of network demands, efficient management of resources is crucial. In this context, we propose a duplex deep reinforcement learning (DDRL)-based resource management framework for next-generation V2X communication networks. Our framework considers multiple network state parameters and utilizes these resources during the learning phase to address resource management problems effectively. The key objective was to select the forthcoming resource control state through the duplex deep reinforcement learning phase, thereby optimizing resource utilization efficiency. To achieve this, we jointly implement time division duplexing (TDD) and frequency division duplexing (FDD), which allow for adaptive management of TDD and FDD to meet high mobility requirements and efficiently utilize limited radio resources. This study introduces a dynamic duplex deep Q-learning-assisted sequential decision-making algorithm to manage band resources for the next-generation V2X network. By employing this algorithm, we enable the efficient allocation of resources, considering factors such as latency, system throughput, network utilization, busy channel rate (BCR), and packet received rate. To validate the performance of our proposed framework, we conducted simulation experiments using benchmarking techniques. The results demonstrate that our framework outperforms benchmark approaches across various performance parameters. It exhibits superior efficiency in terms of latency, system throughput, network utilization, BCR, and packet received rate.}
}
@article{LUO2023107002,
title = {Solving combined economic and emission dispatch problems using reinforcement learning-based adaptive differential evolution algorithm},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {107002},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107002},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623011867},
author = {Wenguan Luo and Xiaobing Yu and Yifan Wei},
keywords = {Differential evolution, Reinforcement learning, CEED, Price penalty factors},
abstract = {Nowadays, economic and environmental concerns in production have become increasingly significant. To address these issues, the Combined Economic and Emission Dispatch (CEED) problem has been introduced to optimize the power generation process by considering fuel cost and emitted substances. However, due to the nonlinearity and nonconvexity of the objective function, the optimization of CEED remains a challenge. In this paper, we develop a Reinforcement Learning-based Adaptive Differential Evolution (RLADE) algorithm to enhance the optimization performance. The mutation strategy and crossover probability of RLADE are optimized using Reinforcement Learning (RL) to respectively ensure better convergence speed and searchability. Additionally, two modifications of RL, namely the adaptive population size-based state division and fitness-ranking-based reward mechanism, are proposed to improve the accuracy of state division and reward calculation in RL. The experiments conducted in this paper consider two objective formulation methods of CEED problems, namely the quadratic and cubic criterion functions. The mean values and standard deviations of the obtained solutions were utilized to assess the performance of RLADE, as well as other comparative algorithms, namely DE algorithm and two RL-based DE variants. The results clearly demonstrate that RLADE surpasses its counterparts with proportion of 100%, 85.7%, and 100% for the 6-unit and 11-unit quadratic CEED problems, as well as cubic criterion functions, in terms of both search accuracy and convergence ability. Furthermore, the significance of RLADE's superiority is confirmed through the Wilcoxon's signed rank test.}
}
@article{XUAN2023110189,
title = {Multi-agent deep reinforcement learning algorithm with self-adaption division strategy for VNF-SC deployment in SDN/NFV-Enabled Networks},
journal = {Applied Soft Computing},
volume = {138},
pages = {110189},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110189},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623002077},
author = {Hejun Xuan and Yi Zhou and Xuelin Zhao and Zhenghui Liu},
keywords = {Deep reinforcement learning, SDN/NFV-Enabled Networks, Network virtualization, VNF-SC, Self-adaption division strategy},
abstract = {Network function virtualization can decouple the traditional network function from the dedicated hardware, abstracts the software-based virtual network function from the specialized network equipment, and promotes the fundamental transformation of network service deployment mode. However, the deployment of virtual network function (VNF) service chain is an important and crucial problem and key technology faced and must be rescued. In this paper, the problem of VNF service chain deployment in SDN/NFV-Enabled Networks is investigated. The existing solution strategies based on optimization methods (dynamic programming, linear programming, etc.) and heuristic methods (genetic algorithm, particle swarm optimization, etc.) are only suitable for operation deployment in the case of predictable operations, and it is difficult to meet the real-time support operation scheduling requirements in high dynamic combat scenarios. A new real-time algorithm for VNF service chain deployment based on multi-agent deep reinforcement learning with self-adaption division strategy (MDRL-SaDS) to minimize energy consumption in a period of time is proposed. In proposed algorithm, an oriented self-adaptive strategy to determination the number of agents and the optimal division method of VNF service chain for the markov process modeling is designed. Constructing a new neural network model and design a training strategy of joint supervised and unsupervised learning. The global and long-term benefits are used to optimize the scheduling process, and the decision-making framework of offline learning and online deployment is used to solve the VNF service chain deployment problem. Finally, experimental results indicate that the MDRL-SaDS has more advantages and has higher convergence speed, average reward value and stability than compared algorithms, while decreasing the energy consumption in a period of time.}
}
@article{SUN2020417,
title = {A method for determining parameter weight early warning model based on reinforcement learning},
journal = {Computer Communications},
volume = {157},
pages = {417-422},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.04.044},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420302681},
author = {Meiyu Sun},
keywords = {Reinforcement learning, Early warning model, Public events},
abstract = {To solve the problem of low efficiency of early warning in public security emergencies, this paper proposes a method for determining parameter weight in public events early warning model which was based on reinforcement learning. Firstly, using the calibrated conflict early warning label, using reinforcement learning algorithm to build the public early warning event model; secondly, through iterative training to obtain the arrival path of the agent to the abnormal sequence, that is, the public early warning event; finally, by analyzing the weight parameters in the neural network, to determine the early warning event. Simulation showed that under this algorithm, convergence happened when the number of steps was in the range from 500 to 800, 37.5% smaller than that when using the original data. This result of the experiment demonstrated that this method greatly improved the efficiency of early warning for public incidents.}
}
@article{SHAO2020107000,
title = {Predictive scheduling of wet flue gas desulfurization system based on reinforcement learning},
journal = {Computers & Chemical Engineering},
volume = {141},
pages = {107000},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.107000},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419312827},
author = {Zhuang Shao and Fengqi Si and Daniel Kudenko and Peng Wang and Xiaozhong Tong},
keywords = {WFGD system, Adaptive predictive scheduling, Reinforcement learning, Power plant, Operation optimization},
abstract = {With the development of renewable energy, loads of thermal power units fluctuate, resulting in the trade-off between the frequent switching of auxiliary equipment and the economic-emission benefits in the wet flue gas desulfurization (WFGD) system. In this paper, the predictive scheduling problem is formalized, considering the power consumption, emission punishment and the switching frequency of slurry circulation pumps with finite prediction sequence of load and sulfur in coal. Model-free off-policy reinforcement learning (RL) is applied to solve the unclear and drifting system dynamic. Considering a real system, the framework setting and an emulator is introduced. Compared with traditional scheduling policies and the case without prediction, the proposed framework shows obvious advantages in terms of comprehensive performance and approximates the theoretical optimal solution at the steady-state. Moreover, the policy keeps the performance by adapting to the drifting without manual intervention, which demonstrates a broad application prospect in similar scenarios.}
}
@article{PARK2023109029,
title = {Adaptive inventory replenishment using structured reinforcement learning by exploiting a policy structure},
journal = {International Journal of Production Economics},
volume = {266},
pages = {109029},
year = {2023},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2023.109029},
url = {https://www.sciencedirect.com/science/article/pii/S092552732300261X},
author = {Hyungjun Park and Dong Gu Choi and Daiki Min},
keywords = {Inventory replenishment policy, Reinforcement learning, Structural properties, Stochastic approximation},
abstract = {We consider an inventory replenishment problem with unknown and non-stationary demand. We design a structured reinforcement learning algorithm that efficiently adapts the replenishment policy to changing demand without any prior knowledge. Our proposed method integrates the known structural properties of a well-performing inventory replenishment policy with reinforcement learning. By exploiting the policy structure, we tune reinforcement learning to characterize the inventory replenishment policy and approximate the value function. In particular, we propose two methods for stochastic approximation on the gradient of the objective function. These novel reinforcement learning algorithms ensure an efficient convergence rate and lower algorithmic complexity for solving practical problems. The numerical results demonstrate that the proposed algorithms adaptively update the policy to changing demand and lower inventory costs compared to various benchmarks. We also conduct a numerical validation for a South Korean retail shop to validate the practical feasibility of the proposed method. Understanding the policy structure is beneficial for designing reinforcement learning algorithms that can address the inventory replenishment problem. These well-designed reinforcement learning algorithms are particularly promising when we require policy updates based on observations without precise knowledge of non-stationary demand. These research findings could be extended to address the various inventory decisions in which policy structures are available.}
}
@article{YANG2018307,
title = {Reinforcement learning for robust adaptive control of partially unknown nonlinear systems subject to unmatched uncertainties},
journal = {Information Sciences},
volume = {463-464},
pages = {307-322},
year = {2018},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.06.022},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518304626},
author = {Xiong Yang and Haibo He and Qinglai Wei and Biao Luo},
keywords = {Adaptive dynamic programming, Neural networks, Optimal control, Reinforcement learning, Robust control, Unmatched uncertainty},
abstract = {This paper proposes a novel robust adaptive control strategy for partially unknown continuous-time nonlinear systems subject to unmatched uncertainties. Initially, the robust nonlinear control problem is converted into a nonlinear optimal control problem by constructing an appropriate value function for the auxiliary system. After that, within the framework of reinforcement learning, an identifier-critic architecture is developed. The presented architecture uses two neural networks: the identifier neural network (INN) which aims at estimating the unknown internal dynamics and the critic neural network (CNN) which tends to derive the approximate solution of the Hamilton-Jacobi-Bellman equation arising in the obtained optimal control problem. The INN is updated by using both the back-propagation algorithm and the e-modification technique. Meanwhile, the CNN is updated via the modified gradient descent method, which uses historical and current state data simultaneously. Based on the classic Lyapunov technique, all the signals in the closed-loop auxiliary system are proved to be uniformly ultimately bounded. Moreover, the original system is kept asymptotically stable under the obtained approximate optimal control. Finally, two illustrative examples, including the F-16 aircraft plant, are provided to demonstrate the effectiveness of the developed method.}
}
@article{PREIL2022108578,
title = {Bandit-based inventory optimisation: Reinforcement learning in multi-echelon supply chains},
journal = {International Journal of Production Economics},
volume = {252},
pages = {108578},
year = {2022},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2022.108578},
url = {https://www.sciencedirect.com/science/article/pii/S0925527322001670},
author = {Deniz Preil and Michael Krapp},
keywords = {Multi-armed bandits, Inventory optimisation, Reinforcement learning, Supply chain management, Simulation optimisation},
abstract = {Even though base-stock policies are per se straightforward, determining them in complex, stochastic multi-echelon supply chains is often cumbersome or even analytically impossible. Therefore, a wide range of heuristics has been proposed for this purpose. This is the first study considering the problem as a multi-armed bandit problem. In this context, we investigate two algorithms: first, we propose an approach that is based on upper confidence bounds and priority queues. This so-called PQ-UCB algorithm allows us to drastically reduce the runtime of upper confidence bound allocation strategies in problems with large action spaces. Subsequently, we apply the parameter-free sequential halving (SH) algorithm. We investigate various scenarios to compare the performance of both algorithms with the performance of a genetic algorithm and a simulated annealing algorithm taken from the literature. PQ-UCB as well as SH outperform both benchmark metaheuristics and require substantially less effort related to parameter tuning (or even no effort in the case of SH). As multi-armed bandits are not common in inventory optimisation so far, we aim to emphasise their strengths and hope to promote their dissemination also in other domains of supply chain management.}
}
@article{XIANG2022100038,
title = {Deep Reinforcement Learning- based load balancing strategy for multiple controllers in SDN},
journal = {e-Prime - Advances in Electrical Engineering, Electronics and Energy},
volume = {2},
pages = {100038},
year = {2022},
issn = {2772-6711},
doi = {https://doi.org/10.1016/j.prime.2022.100038},
url = {https://www.sciencedirect.com/science/article/pii/S2772671122000110},
author = {Min Xiang and Mengxin Chen and Duanqiong Wang and Zhang Luo},
keywords = {Software-Defined Network (SDN), Controller load balancing, Deep Reinforcement Learning (DRL), Switch immigration},
abstract = {In Software-Defined Network (SDN) with multiple controllers, static mapping relationship between switches and controllers may cause some controllers to be overloaded, while some controller resources are underutilized. A Deep Reinforcement Learning-based switch migration strategy (DRL-SMS) is proposed to solve the load imbalance problem in the multi-controller control plane. Based on Markov Decision Process (MDP), modeling analysis is performed for SDN to obtain system state, migration action set, and system reward. Q-values of switch migration actions are obtained by fitting approximate function using Double Deep Q-Network (DDQN), and then the DDQN is trained by using the experience replay mechanism to optimize Q-Network parameters. After training, the DRL-based strategy calculates the Q-value in the current system state and selects the migration action corresponding to the maximum Q-value to perform switch migration. Simulation experiments show that DRL-SMS can effectively balance the controller load and significantly reduce the balance time.}
}
@article{XU20141,
title = {Reinforcement learning algorithms with function approximation: Recent advances and applications},
journal = {Information Sciences},
volume = {261},
pages = {1-31},
year = {2014},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2013.08.037},
url = {https://www.sciencedirect.com/science/article/pii/S0020025513005975},
author = {Xin Xu and Lei Zuo and Zhenhua Huang},
keywords = {Reinforcement learning, Function approximation, Approximate dynamic programming, Learning control, Generalization},
abstract = {In recent years, the research on reinforcement learning (RL) has focused on function approximation in learning prediction and control of Markov decision processes (MDPs). The usage of function approximation techniques in RL will be essential to deal with MDPs with large or continuous state and action spaces. In this paper, a comprehensive survey is given on recent developments in RL algorithms with function approximation. From a theoretical point of view, the convergence and feature representation of RL algorithms are analyzed. From an empirical aspect, the performance of different RL algorithms was evaluated and compared in several benchmark learning prediction and learning control tasks. The applications of RL with function approximation are also discussed. At last, future works on RL with function approximation are suggested.}
}
@article{AHMED202013733,
title = {Fault-Tolerant Control of Degrading Systems with On-Policy Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {13733-13738},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.878},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320312167},
author = {Ibrahim Ahmed and Marcos Quiñones-Grueiro and Gautam Biswas},
keywords = {Fault tolerance, Reinforcement learning, Neural networks, Control system design, Machine learning},
abstract = {We propose a novel adaptive reinforcement learning control approach for fault tolerant control of degrading systems that is not preceded by a fault detection and diagnosis step. Therefore, a priori knowledge of faults that may occur in the system is not required. The adaptive scheme combines online and offline learning of the on-policy control method to improve exploration and sample efficiency, while guaranteeing stable learning. The offline learning phase is performed using a data-driven model of the system, which is frequently updated to track the system’s operating conditions. We conduct experiments on an aircraft fuel transfer system to demonstrate the effectiveness of our approach.}
}
@article{TIAN2022108529,
title = {A prescriptive Dirichlet power allocation policy with deep reinforcement learning},
journal = {Reliability Engineering & System Safety},
volume = {224},
pages = {108529},
year = {2022},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2022.108529},
url = {https://www.sciencedirect.com/science/article/pii/S0951832022001831},
author = {Yuan Tian and Minghao Han and Chetan Kulkarni and Olga Fink},
keywords = {Reinforcement learning, Deep learning, Prescriptive operation, Multi-power source systems},
abstract = {Prescribing optimal operation based on the condition of the system, and thereby potentially prolonging its remaining useful lifetime, has tremendous potential in terms of actively managing the availability, maintenance, and costs of complex systems. Reinforcement learning (RL) algorithms are particularly suitable for this type of problem given their learning capabilities. A special case of a prescriptive operation is the power allocation task, which can be considered as a sequential allocation problem whereby the action space is bounded by a simplex constraint. A general continuous action-space solution of such sequential allocation problems has still remained an open research question for RL algorithms. In continuous action space, the standard Gaussian policy applied in reinforcement learning does not support simplex constraints, while the Gaussian-softmax policy introduces a bias during training. In this work, we propose the Dirichlet policy for continuous allocation tasks and analyze the bias and variance of its policy gradients. We demonstrate that the Dirichlet policy is bias-free and provides significantly faster convergence, better performance, and better robustness to hyperparameter changes as compared to the Gaussian-softmax policy. Moreover, we demonstrate the applicability of the proposed algorithm on a prescriptive operation case in which we propose the Dirichlet power allocation policy and evaluate its performance on a case study of a set of multiple lithium-ion (Li-I) battery systems. The experimental results demonstrate the potential to prescribe optimal operation, improving the efficiency and sustainability of multi-power source systems.}
}
@article{NAKAGAWA202341,
title = {Imitation of piping warm-up operation and estimation of operational intention by inverse reinforcement learning},
journal = {Journal of Process Control},
volume = {122},
pages = {41-48},
year = {2023},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2022.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0959152422002335},
author = {Yosuke Nakagawa and Hitoi Ono and Yusuke Hazui and Sachiyo Arai},
keywords = {Inverse reinforcement learning, Plant operation, Estimation of operational intention},
abstract = {In this study, AIRL, which is a method of adversarial inverse reinforcement learning, was applied to the warm-up operation of steam pipes. We tried to imitate the operation of the expert and guessed the operation intention. As a result, we were able to learn the stepwise expert operation of heating → pressurizing → reheating performed by trials equivalent to 2800 times in the plant operation. In addition, by simultaneous learning that alternately gives two expert data of summer and winter, we got a robust policy function applicable to operating conditions of different seasons. Furthermore, by visualizing the state value function and reward function of the discriminator, it was found that the policy learned under a single condition in summer cannot be applied in winter because it has not been able to learn the operation to maintain the degree of superheat of steam immediately after the start of heating. In the field of process control, it was confirmed that AIRL, which divides the value function and the reward function and can transfer and visualize the reward function, is effective for imitating expert operations and guessing their intentions.}
}
@article{XU2021107022,
title = {Deep reinforcement learning assisted edge-terminal collaborative offloading algorithm of blockchain computing tasks for energy Internet},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {131},
pages = {107022},
year = {2021},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107022},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521002623},
author = {Siya Xu and Boxian Liao and Chao Yang and Shaoyong Guo and Bo Hu and Jinghong Zhao and Lei Jin},
keywords = {Blockchain, Edge-terminal collaboration, Task offloading, Deep reinforcement learning},
abstract = {In the regional distribution network, microgrid is often used to build local energy system to realize regional autonomy in the process of power generation, transmission, and consumption. Applying blockchain technology in microgrid can meet the needs of security and privacy in energy transactions, and can conduct secure point-to-point transactions between anonymous entities. However, blockchain nodes will generate numerous computing-intensive tasks in the process of mining, and cause high delay in energy transaction. Therefore, we take advantage of mobile edge computing (MEC) technology and propose an edge-terminal collaborative mining task processing framework to increase the computing ability of the blockchain system. This framework includes three working modes: local computing, user collaboration and edge node collaboration. Particularly, the trust value of collaborative user nodes is considered to avoid security threats caused by malicious nodes. Furthermore, we establish a delay-and-throughput-based blockchain computing task offloading model, and use asynchronous advantage actor-critic (A3C) algorithm to jointly optimize offloading decision, transmission power allocation, block interval and size configuration. Simulation results show that, compared with Only-MEC and Fixed-BlockSize algorithms, the proposed algorithm can reduce the average delay by 1.7% and 2.5%, and improve the average transaction throughput by 12.1% and 28.5% respectively.}
}
@article{HUANG2014209,
title = {Reinforcement learning with automatic basis construction based on isometric feature mapping},
journal = {Information Sciences},
volume = {286},
pages = {209-227},
year = {2014},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2014.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0020025514006987},
author = {Zhenhua Huang and Xin Xu and Lei Zuo},
keywords = {Reinforcement learning, Isometric feature mapping, Value function approximation, Approximate policy iteration, Learning control},
abstract = {Value function approximation (VFA) has been a major research topic in reinforcement learning. Although various reinforcement learning algorithms with VFA have been proposed, the performance of most previous algorithms depends on the predefined structure of the basis functions. To address this problem, this paper presents a novel basis learning method for VFA based on isometric feature mapping (IFM). In the proposed method, basis functions for VFA are automatically generated by constructing the optimal embedding basis of the data in a d-dimensional Euclidean space, which best preserves the estimated intrinsic geometry of the manifold. Furthermore, the IFM-based basis learning method is integrated with approximation policy iteration (API) for learning control in Markov decision problems with large state spaces. A new manifold reinforcement learning framework termed IFM-based API (IFM-API) is presented. Three learning control problems, including a real control system of the Googol single inverted pendulum, were studied to evaluate the performance of the proposed IFM-API algorithm. The simulation and experimental results show that, compared with other basis selection or learning methods, the IFM-based basis learning method can automatically compute an efficient set of basis functions with much fewer predefined parameters and less computational costs. Besides, it is illustrated that the proposed IFM-API algorithm can obtain better learning control policies than other API methods.}
}
@article{TIGHTIZ2023471,
title = {A novel deep reinforcement learning based business model arrangement for Korean net-zero residential micro-grid considering whole stakeholders’ interests},
journal = {ISA Transactions},
volume = {137},
pages = {471-491},
year = {2023},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2022.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0019057822006395},
author = {Lilia Tightiz and Joon Yoo},
keywords = {Business model, Deep reinforcement learning, Demand response, Energy management system, Mixed integer nonlinear programming, Net-zero building},
abstract = {In this paper, we put forward a deep reinforcement learning (DRL) based energy management system (EMS) solution for a typical Korean net-zero residential micro-grid (NZR-MG). We model NZR-MG EMS to extract a profitable business model that respects whole stakeholders’ interests and meets Korean power system regulations and specifications. We deployed the value-based DRL technique, dual deep Q-learning (DDQN), as a solution for our EMS problem since of its simplicity, stability in the learning process, and non-dependency on hyper-parameter selection compared to actor–critic methods. Due to the implementation of mixed-integer nonlinear programming (MINLP) to solve the reward function in this paper, DDQN, despite other DRL methods, provides precise, explicit, and meaningful rewards. In addition to encouraging the agent to choose profitable actions, this approach releases the proposed DRL-based method from the hindrance of redesigning the reward function experimentally in any future extension of the environment elements. Moreover, attaching transfer learning (TL) to the process of training DDQN agent defeat the MINLP imposed latency in training convergence. An extensive benchmark is proposed to test the superiority of the proposed method versus other DRL algorithms.}
}
@article{SYAFIIE2007767,
title = {Model-free learning control of neutralization processes using reinforcement learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {20},
number = {6},
pages = {767-782},
year = {2007},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2006.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0952197606001916},
author = {S. Syafiie and F. Tadeo and E. Martinez},
keywords = {Learning control, Goal-seeking control, Process control, Intelligent control, Online learning, Neutralization process, pH control},
abstract = {The pH process dynamic often exhibits severe nonlinear and time-varying behavior and therefore cannot be adequately controlled with a conventional PI control. This article discusses an alternative approach to pH process control using model-free learning control (MFLC), which is based on reinforcement learning algorithms. The MFLC control technique is proposed because this algorithm gives a general solution for acid–base systems, yet is simple enough to be implemented in existing control hardware without a model. Reinforcement learning is selected because it is a learning technique based on interaction with a dynamic system or process for which a goal-seeking control task must be performed. This “on-the-fly” learning is suitable for time varying or nonlinear processes for which the development of a model is too costly, time consuming or even not feasible. Results obtained in a laboratory plant show that MFLC gives good performance for pH process control. Also, control actions generated by MFLC are much smoother than conventional PID controller.}
}
@article{ZHAO2022107645,
title = {A reinforcement learning brain storm optimization algorithm (BSO) with learning mechanism},
journal = {Knowledge-Based Systems},
volume = {235},
pages = {107645},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107645},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121009072},
author = {Fuqing Zhao and Xiaotong Hu and Ling Wang and Jinlong Zhao and Jianxin Tang and  Jonrinaldi},
keywords = {Brain storm optimization, Q-learning, Mutation strategies, Self-learning mechanism},
abstract = {Brain storm optimization algorithm (BSO), which is inspired by brain storm process of human, has been adopted as an efficient optimizer for various complex problems. A reinforcement learning brain storm optimization algorithm (RLBSO) to improve the performance of BSO is proposed in this paper. Four mutation strategies are designed to enhance the search capability of the algorithm in different stages. Elites are adopted as the guidance to ensure the quality of the population. The global best is utilized to guide the search direction of individuals. The cluster centers are employed to improve the exploitation ability of the algorithm. The historical individuals are utilized to the increase the diversity of the population. The Q-learning mechanism is introduced to guide the selection of strategies according to the historical information fed back by the corresponding strategies. A self-learning mechanism, which is based on the evolutionary state of the population and the experience of previous successful individuals, is utilized to determine the update method of individual. The RLBSO algorithm is tested on the CEC 2017 benchmark test suite and a practical engineering problem. The results show that RLBSO has better performance than the other state of the arts algorithms.}
}
@article{ABOUHEAF20143038,
title = {Multi-agent discrete-time graphical games and reinforcement learning solutions},
journal = {Automatica},
volume = {50},
number = {12},
pages = {3038-3053},
year = {2014},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2014.10.047},
url = {https://www.sciencedirect.com/science/article/pii/S0005109814004282},
author = {Mohammed I. Abouheaf and Frank L. Lewis and Kyriakos G. Vamvoudakis and Sofie Haesaert and Robert Babuska},
keywords = {Dynamic graphical games, Optimal control, Nash equilibrium, Best response, Reinforcement learning},
abstract = {This paper introduces a new class of multi-agent discrete-time dynamic games, known in the literature as dynamic graphical games. For that reason a local performance index is defined for each agent that depends only on the local information available to each agent. Nash equilibrium policies and best-response policies are given in terms of the solutions to the discrete-time coupled Hamilton–Jacobi equations. Since in these games the interactions between the agents are prescribed by a communication graph structure we have to introduce a new notion of Nash equilibrium. It is proved that this notion holds if all agents are in Nash equilibrium and the graph is strongly connected. A novel reinforcement learning value iteration algorithm is given to solve the dynamic graphical games in an online manner along with its proof of convergence. The policies of the agents form a Nash equilibrium when all the agents in the neighborhood update their policies, and a best response outcome when the agents in the neighborhood are kept constant. The paper brings together discrete Hamiltonian mechanics, distributed multi-agent control, optimal control theory, and game theory to formulate and solve these multi-agent dynamic graphical games. A simulation example shows the effectiveness of the proposed approach in a leader-synchronization case along with optimality guarantees.}
}
@article{DU2023129394,
title = {A novel crude oil futures trading strategy based on volume-price time-frequency decomposition with ensemble deep reinforcement learning},
journal = {Energy},
pages = {129394},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.129394},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223027883},
author = {Xiao-xu Du and Zhen-peng Tang and Kai-jie Chen},
keywords = {Crude oil trading strategy, Volume-price, Time frequency decomposition, Deep reinforcement learning ensemble},
abstract = {With the deepening financialization of crude oil, an increasing number of investors are involved in trading crude oil futures. Still, because various factors influence crude oil futures price fluctuations, they present complex characteristics. For this reason, a trading strategy plays a crucial role in investment. In this paper, we construct an ensemble trading strategy combining time-frequency feature extraction of crude oil volume-price series with deep reinforcement learning. We decompose the original oil volume-price series by using the optimized variational mode decomposition (OVMD). Additionally, taking into account the long memory characteristics of the crude oil time series, the Sharpe ratio is used to filter the appropriate agents corresponding to the market volatility. Moreover, the best features of the three agents are effectively combined and used in the next phase of trading, thus robustly adapting to different market volatility situations. Finally, we tested our strategy using Brent crude oil futures. Compared with other benchmark models, the proposed OVMD(V–P)-DRL-Ensemble strategy is suitable for the highly volatile crude oil market and performs well in terms of the corresponding performance evaluation metrics, showing excellent return performance and stability.}
}
@article{S2023100298,
title = {An intelligent data routing strategy based on deep reinforcement learning for IoT enabled WSNs},
journal = {e-Prime - Advances in Electrical Engineering, Electronics and Energy},
volume = {6},
pages = {100298},
year = {2023},
issn = {2772-6711},
doi = {https://doi.org/10.1016/j.prime.2023.100298},
url = {https://www.sciencedirect.com/science/article/pii/S2772671123001936},
author = {Sebastin Suresh S and Prabhu V and Parthasarathy V},
keywords = {Double cluster pairing method, Intelligent data routing, Multi-criterion correlation agent model, Time response delay, Maximum data rate, Energy efficiency},
abstract = {Routing data across numerous access points becomes complicated due to the frequent relocation of sensor nodes. This can cause packet loss, node selection errors, an inability to extend the lifespan of individual nodes, latency in response times, and an increase in computing complexity. An intelligent data routing strategy based on deep reinforcement learning (DRL) is proposed in this study. It considers factors like message overhead, time complexity, maximum data sum rate, and other factors like reduced communication delay and improved energy efficiency to find an optimistic path for better performance of IoT-enabled WSNs. The fundamental instant data load is divided into different cluster pairs, each containing one strong and one weak sensor node, using the double cluster pairing approach. It can be implemented on any network platform, including mobile and non-mobile nodes, by reducing regulated message overhead and increasing data throughput. The technique proposed here is similar to other existing routing protocols in that it helps nodes last longer while using less power.}
}
@article{KANG2023128623,
title = {Optimal planning of hybrid energy storage systems using curtailed renewable energy through deep reinforcement learning},
journal = {Energy},
volume = {284},
pages = {128623},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.128623},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223020170},
author = {Dongju Kang and Doeun Kang and Sumin Hwangbo and Haider Niaz and Won Bo Lee and J. Jay Liu and Jonggeol Na},
keywords = {Process planning, Reinforcement learning, Curtailed renewable energy, Machine learning, Energy management system, Mathematical programming},
abstract = {Energy management systems are becoming increasingly important to utilize the continuously growing curtailed renewable energy. Promising energy storage systems, such as batteries and green hydrogen, should be employed to maximize the efficiency of energy stakeholders. However, optimal decision-making, i.e., planning the leveraging between different strategies, is confronted with the complexity and uncertainties of large-scale problems. A sophisticated deep reinforcement learning methodology with a policy-based algorithm is proposed to achieve real-time optimal energy storage systems planning under the curtailed renewable energy uncertainty. A quantitative performance comparison proved that the deep reinforcement learning agent outperforms the scenario-based stochastic optimization algorithm, even with a wide action and observation space. A robust performance, with maximizing net profit and a stable system, confirmed the uncertainty rejection capability of the deep reinforcement learning under a large uncertainty of the curtailed renewable energy. Action mapping was performed to visually assess the action the deep reinforcement learning agent took according to the state. The corresponding results confirmed that the deep reinforcement learning agent learns how the deterministic solution performs and demonstrates more than 90% profit accuracy compared to the solution.}
}
@article{MOHAMMADI2023109781,
title = {RLS2: An energy efficient reinforcement learning- based sleep scheduling for energy harvesting WBANs},
journal = {Computer Networks},
volume = {229},
pages = {109781},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109781},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623002268},
author = {Razieh Mohammadi and Zahra Shirmohammadi},
keywords = {Wireless body area networks, Energy harvesting, Reinforcement learning, Sleep schedule},
abstract = {In the Energy-Harvesting Wireless Body Area Networks (EH-WBAN), one of the fundamental challenges is preserving the self-sustainability of sensors without compromising network reliability and connectivity. Determining the sleep/wake schedule of body nodes (BNs) is an efficient way to achieve self-sustainability. Sleeping nodes should be connected to at least one active node to reduce delay and keep the network connected. There are two fundamental problems with previous methods for determining BN's sleep/wake schedule: (1) BN suffers from emergency packet loss and unnecessary frequent sleeping and waking up, and (2) They do not guarantee network connectivity. Studies that have only examined connectivity in EH-WBAN also have two main issues: (1) BNs are considered homogenous in terms of energy harvesting and its consumption, (2) These methods cannot adapt to the time-varying behavior of energy-harvesting resources. This study proposes a new method for sleep/wake scheduling called Reinforcement Learning-based Sleep Scheduling (RLS2). RLS2 has the following innovative points: (1) To avoid emergency packet loss or unnecessary frequent sleeping and waking up, each BN has its own sleep/wake schedule based on its energy level and sensed data changes, (2) Lowest possible number of BNs are determined as relay nodes in each round to increase network reliability and connectivity; these BNs remain active in each round, while the others operate according to the determined schedule. In this part of the proposed method: (1) Heterogeneous BNs are considered, (2) As a first step in solving adaptability, the problem of finding the optimal active groups is formulated as a Markov decision process (MDP), followed by a Q-learning algorithm capable of learning time-varying behavior of energy harvesting resources, (3) The unavailable action space is removed to reduce the problem's complexity, (4) To achieve good Q-learning performance, a reward function based on residual energy level and neighborhood degree of BNs is defined. It can find an active group with the lowest cardinality in the current round, which is maximum in terms of the residual energy of its sensors. The performed simulations indicate the appropriate convergence of the proposed method. The results show that, on average, the proposed method improves network connectivity and energy efficiency by 50% and 31%, respectively, and reduces network delay by 27%.}
}
@article{PANDIT2018844,
title = {Online Tuning of PID controller using Black Box Multi-Objective Optimization and Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {32},
pages = {844-849},
year = {2018},
note = {17th IFAC Workshop on Control Applications of Optimization CAO 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.440},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318331380},
author = {Ashwad Pandit and Bipin Hingu},
keywords = {PID controller, multiobjective optimizations, Genetic Algorithms, Learning algorithms, Parameter optimization},
abstract = {A PID Controller is the most widely used controller due to its ease and convenience of use. Manual tuning of a PID Controller is a time-consuming task. Hence, employing intelligent algorithms is necessary. The Cummins engine controller has a complex structure. To fine-tune it, a substantial amount of time is required. To reduce this time requirement, a black box approach was selected for online tuning. This would not only reduce the required time, but also reduce the efforts. Black box optimization would mean the engineers have to spend less time trying to understand the controller structure. With this aim in mind, a PID system simulation was set up in MATLAB. A function would randomize a system, resulting in a true black box to tune. This removed any bias the authors might have. The algorithm has shown promising results, with tuned controller gains in just over 20 iterations on average. This could then be extended to not only Cummins controllers, but other industrial controllers as well.}
}
@article{CUI2023111425,
title = {Numerical simulation and optimization of Lonicerae Japonicae Flos extract spray drying process based on temperature field verification and deep reinforcement learning},
journal = {Journal of Food Engineering},
volume = {345},
pages = {111425},
year = {2023},
issn = {0260-8774},
doi = {https://doi.org/10.1016/j.jfoodeng.2023.111425},
url = {https://www.sciencedirect.com/science/article/pii/S0260877423000237},
author = {Pengdi Cui and Yang Yu and Qilong Xue and Zhouyou Wu and Kunhong Miao and Changqing Liu and Lijun Zhao and Zheng Li},
keywords = {Spray drying, Numerical simulation, Deep learning, Temperature field verification, Process optimization},
abstract = {This paper presents a computational fluid dynamics (CFD) method combined with deep reinforcement learning to simulate and optimize the spray drying process of Lonicerae Japonicae Flos (LJF) extract. The computational model firstly incorporates the drying kinetics information, which was experimentally determined by drying of individual droplets. Secondly, the difference between this study and previous work is that a distributed optical fiber temperature measurement system (DTS) was used to measure the temperature field of a pilot-scale drying tower for model verification. The mean percentage errors between the experimentally measured temperature and the simulated values at 3 heights (0.18 m, 0.48 m, and 0.78 m) were 8.8%, 7.1%, and 3.1%, respectively. The measured temperature in the drying tower is consistent with the simulation, which can well explain the change of droplets during the drying process. Based on experimental and simulation data, a powder yield prediction model was established. Deep reinforcement learning model was then applied to continuously interact and iterate with the prediction model, realizing the automatic optimization of the spray drying process. The results show that the process can be optimized to increase the powder yield by around 5%. The model can thus be used as a basis for equipment improvement and to provide optimal operating conditions for spray drying process to replace traditional empirical adjustment method.}
}
@incollection{MILLAN1995185,
title = { - Efficient reinforcement learning of navigation strategies in an autonomous robot},
editor = {Volker Graefe},
booktitle = {Intelligent Robots and Systems},
publisher = {Elsevier Science B.V.},
address = {Amsterdam},
pages = {185-199},
year = {1995},
isbn = {978-0-444-82250-5},
doi = {https://doi.org/10.1016/B978-044482250-5/50014-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444822505500141},
author = {José del R. Millán and Carme Torras},
abstract = {Publisher Summary
This chapter describes a reinforcement connectionist learning architecture that allows an autonomous mobile robot to adapt to an unknown indoor environment. As a result, the robot learns efficient reactive-navigation strategies. The robot learns on top of built-in reflexes. Also, the neural controller is a modular network that is built automatically. In this way, the robot is operational from the very start and improves its performance rapidly and incrementally as it safely explores the environment. These appealing features make this learning robot's architecture very well-suited to real-world applications. The chapter also reports experimental results obtained with a real mobile robot that demonstrate the feasibility of this approach.}
}
@article{LUO2023104545,
title = {Reinforcement learning in robotic motion planning by combined experience-based planning and self-imitation learning},
journal = {Robotics and Autonomous Systems},
volume = {170},
pages = {104545},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104545},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023001847},
author = {Sha Luo and Lambert Schomaker},
keywords = {Self-imitation learning, Reinforcement learning, Robotics, Motion planning, Obstacle avoidance},
abstract = {High-quality and representative data is essential for both Imitation Learning (IL)- and Reinforcement Learning (RL)-based motion planning tasks. For real robots, it is challenging to collect enough qualified data either as demonstrations for IL or experiences for RL due to safety consideration in environments with obstacles. We target this challenge by proposing the self-imitation learning by planning plus (SILP+) algorithm, which efficiently embeds experience-based planning into the learning architecture to mitigate the data-collection problem. The planner generates demonstrations based on successfully visited states from the current RL policy, and the policy improves by learning from these demonstrations. In this way, we relieve the demand for human expert operators to collect demonstrations required by IL and improve the RL performance as well. Various experimental results shows that SILP+ achieves better training efficiency, higher and more stable success rate in complex motion planning tasks compared to several other methods. Extensive tests on physical robots illustrate the effectiveness of SILP+ in a physical setting, retaining a success rate of 90% where the next-best contender drops from 87% to 75% in the Sim2Real transition.}
}
@article{GIANNOCCARO2002153,
title = {Inventory management in supply chains: a reinforcement learning approach},
journal = {International Journal of Production Economics},
volume = {78},
number = {2},
pages = {153-161},
year = {2002},
issn = {0925-5273},
doi = {https://doi.org/10.1016/S0925-5273(00)00156-0},
url = {https://www.sciencedirect.com/science/article/pii/S0925527300001560},
author = {Ilaria Giannoccaro and Pierpaolo Pontrandolfo},
keywords = {Supply chain, Inventory management, Markov decision processes, Reinforcement learning},
abstract = {A major issue in supply chain inventory management is the coordination of inventory policies adopted by different supply chain actors, such as suppliers, manufacturers, distributors, so as to smooth material flow and minimize costs while responsively meeting customer demand. This paper presents an approach to manage inventory decisions at all stages of the supply chain in an integrated manner. It allows an inventory order policy to be determined, which is aimed at optimizing the performance of the whole supply chain. The approach consists of three techniques: (i) Markov decision processes (MDP) and (ii) an artificial intelligent algorithm to solve MDPs, which is based on (iii) simulation modeling. In particular, the inventory problem is modeled as an MDP and a reinforcement learning (RL) algorithm is used to determine a near optimal inventory policy under an average reward criterion. RL is a simulation-based stochastic technique that proves very efficient particularly when the MDP size is large.}
}
@article{KOKSAL202147,
title = {Performance characterization of reinforcement learning-enabled evolutionary algorithms for integrated school bus routing and scheduling problem},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {2},
pages = {47-56},
year = {2021},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2021.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666307421000061},
author = {Eda Koksal and Abhishek R. Hegde and Haresh P. Pandiarajan and Bharadwaj Veeravalli},
keywords = {Reinforcement learning, Ant colony optimization, Genetic algorithm, Particle swarm optimization, School bus routing and scheduling, Combinatorial optimization},
abstract = {Bi-objective school bus scheduling optimization problem that is a subset of vehicle fleet scheduling problem is focused in this paper. In the literature, school bus routing and scheduling problem is proven to be an NP-Hard problem. The processed data supplied by our framework is utilized to search a near-optimum schedule with the aid of reinforcement learning by evolutionary algorithms. They are named as reinforcement learning-enabled genetic algorithm (RL-enabled GA), reinforcement learning-enabled particle swarm optimization algorithm (RL-enabled PSO), and reinforcement learning-enabled ant colony optimization algorithm (RL-enabled ACO). In this paper, the performance characterization of reinforcement learning-enabled evolutionary algorithms for integrated school bus routing and scheduling problem is investigated. The efficiency of the conventional algorithms is improved, and the near-optimal schedule is achieved significantly in a shorter duration with the active guidance of the reinforcement learning algorithm. We attempt to carry out extensive performance evaluation and conducted experiments on a geospatial dataset comprising road networks, trip trajectories of buses, and the address of students. The conventional and reinforcement learning integrated algorithms are improving the travel time of buses and the students. More than 50% saving by the conventional and the reinforcement learning-enabled ant colony optimization algorithm compared to the constructive heuristic algorithm is achieved from 92nd and 54th iterations, respectively. Similarly, the saving by the conventional and the reinforcement learning-enabled genetic algorithm is 41.34% at 500th iterations and more than 50% improvement from 281st iterations, respectively. Lastly, more than 10% saving by the conventional and the reinforcement learning-enabled particle swarm algorithm is achieved from 432nd and 28th iterations, respectively.}
}
@article{ZHANG2021106605,
title = {Service skill improvement for home robots: Autonomous generation of action sequence based on reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {212},
pages = {106605},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106605},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120307346},
author = {Mengyang Zhang and Guohui Tian and Ying Zhang and Peng Duan},
keywords = {Home robots, Action sequence, Reinforcement learning, Natural language, A priori knowledge},
abstract = {It still remains a challenge for robots to obtain knowledge automatically for performing home services. In the human learning process, natural languages act as an outline in guiding human beings complete tasks. From this point, a conditional generation method transforming textual manipulation instructions into action sequences is proposed, to provide home robots with knowledge automatically and improve the service skills finally. Due to the limited learning ability of the generation model on understanding complex semantic information, we present a two-phase conditional generation strategy in which the action space is reduced at the syntax level before generating action sequences semantically. For representing action sequences effectively, functional labels (FLs) are designed according to the requirements of performing home services, to identify six relationships about objects and actions. In action sequence generation, reinforcement learning is employed to guide the action sequence generation by introducing hierarchical rewards related to a priori knowledge, semantic similarity, and action logic. Based on statistic learning, a priori knowledge is constructed by modeling the relationship about object co-occurrence, action collaboration, and action–object correlation. The semantic similarity with Semantic Role Labeling enables the similarity evaluation between textual sentences (inputs) and produced sequences (outputs). And action logic, represented by the verb sequence in instructions, guides the production of action sequences logically. Experimental results demonstrate that the proposed method can produce competitive action sequences from textual instructions, and produced action sequences can be applied to robot for performing services.}
}
@article{LIU2022390,
title = {A distributed deep reinforcement learning method for traffic light control},
journal = {Neurocomputing},
volume = {490},
pages = {390-399},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.11.106},
url = {https://www.sciencedirect.com/science/article/pii/S092523122101818X},
author = {Bo Liu and Zhengtao Ding},
keywords = {Distributed learning, Consensus, Deep Q-network, Convolutional neural network, Traffic light control},
abstract = {This paper proposes a distributed deep reinforcement learning algorithm for the traffic light control problem, which consists of local learning and global consensus. Firstly, the reinforcement learning environment for the traffic light control problem is built by defining the three key elements of state, action, and reward. Then, the CNN-based deep Q-network is designed to process the quantized traffic state information to obtain the state-action values. After locally optimizing the deep Q-networks of multiple traffic light agents based on their experience samples, the consensus algorithm is subsequently applied to globally update these agents that are connected over a decentralized communication topology. In this way, the distributed learning agents learn from their neighbors’ experience to optimize the modeling process without actually sharing experience data samples. Lastly, homogeneous and heterogeneous traffic flow patterns on different intersections are simulated in SUMO to verify the superiority of the proposed distributed deep Q-networks, with the comparison to the fixed-time strategy, local learning and centralized learning algorithms. The simulation study demonstrates that the distributed learning algorithm without a central server shows comparable performance with centralized learning, which is much better than fixed-time strategy and the local learning method in both homogeneous and heterogeneous traffic scenarios.}
}
@article{ABID2024122029,
title = {A novel multi-objective optimization based multi-agent deep reinforcement learning approach for microgrid resources planning},
journal = {Applied Energy},
volume = {353},
pages = {122029},
year = {2024},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.122029},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923013934},
author = {Md. Shadman Abid and Hasan Jamil Apon and Salman Hossain and Ashik Ahmed and Razzaqul Ahshan and M.S. Hossain Lipu},
keywords = {Reinforcement learning, Microgrid, Deep learning, Optimization, Electric vehicle},
abstract = {Multi-agent deep reinforcement learning (MADRL) approaches are at the forefront of contemporary research in optimum electric vehicle (EV) charging scheduling challenges. These techniques involve multiple agents that respond to a dynamic simulation environment to strategically integrate EV charging stations (EVCSs) on microgrids by incorporating the constraints posed by stochastic trip durations. In addition, recent research works have demonstrated that planning frameworks based on multi-objective optimization (MOO) techniques are suitable for the efficient functioning of microgrids comprising renewable energy sources (RESs) and battery energy storage systems (BESSs). Even though MADRL techniques have been used to solve the optimum EV charging scheduling challenges and MOO frameworks have been developed to determine the optimal RES-BESS allocation, the potential of merging MADRL and MOO is yet to be explored. Therefore, this research provides an opportunity to determine the effectiveness of combined MOO-MADRL dynamics and their computational efficacy. In this context, this work presents a novel Multi-objective Artificial Vultures Optimization Algorithm based on Multi-agent Deep Deterministic Policy Gradient (MOAVOA-MADDPG) planning framework for allocating RESs, BESSs, and EVCSs on microgrids. The objective function is formulated to optimize the network power losses, total installation and operational costs, greenhouse gas emissions, and system voltage stability. Moreover, the proposed framework incorporates the sporadic nature of RES systems and intends to improve the state of charge (SOC) of the EVs present in the network. The presented approach is validated using practical weather data and EV commuting behavior on the modified IEEE 33 bus network, two practical distribution feeders in Bangladesh, and the Turkish 141 bus network. According to the findings, the MOAVOA-MADDPG framework effectively accommodated the financial, technical, and environmental considerations with improved average SOC of the vehicles. Furthermore, statistical analysis, spacing, convergence, and hyper-volume metrics are employed to compare the suggested MOAVOA-MADDPG framework with five contemporary techniques. The findings indicate that, in every metric considered, the MOAVOA-MADDPG Pareto fronts provide superior solutions.}
}
@article{ZHOU2023109443,
title = {Reinforcement Learning-based approach for dynamic vehicle routing problem with stochastic demand},
journal = {Computers & Industrial Engineering},
volume = {182},
pages = {109443},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109443},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223004679},
author = {Chenhao Zhou and Jingxin Ma and Louis Douge and Ek Peng Chew and Loo Hay Lee},
keywords = {Dynamic vehicle routing problem, Stochastic demand, Single courier, Supervised learning, Reinforcement learning},
abstract = {This paper studies a dynamic vehicle routing problem under stochastic demands, drawn from a real-world situation. Specifically, a single courier must accomplish two kinds of tasks: deliveries known at the beginning of the operation and pickups that appear throughout the daily operation with specific patterns. The objective is to maximise the rewards obtained from serving both types of customers during a limited period. Our contribution lies in using the neural network and historical couriers’ decisions to learn a base policy that captures human experience for better decision making. The reinforcement learning framework is then used to make the base policy explore new scenarios through simulations and further train the base policy with newly generated data. We show that our approach allows the serving of an average of 12% and 8% more customers under some conditions than the nearest-neighbour policy in high density area and low density area, respectively.}
}
@article{SHAW2022101722,
title = {Applying Reinforcement Learning towards automating energy efficient virtual machine consolidation in cloud data centers},
journal = {Information Systems},
volume = {107},
pages = {101722},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101722},
url = {https://www.sciencedirect.com/science/article/pii/S030643792100003X},
author = {Rachael Shaw and Enda Howley and Enda Barrett},
keywords = {Energy efficiency, Virtual machine consolidation, Reinforcement learning, Artificial intelligence},
abstract = {Energy awareness presents an immense challenge for cloud computing infrastructure and the development of next generation data centers. Virtual Machine (VM) consolidation is one technique that can be harnessed to reduce energy related costs and environmental sustainability issues of data centers. In recent times intelligent learning approaches have proven to be effective for managing resources in cloud data centers. In this paper we explore the application of Reinforcement Learning (RL) algorithms for the VM consolidation problem demonstrating their capacity to optimize the distribution of virtual machines across the data center for improved resource management. Determining efficient policies in dynamic environments can be a difficult task, however, the proposed RL approach learns optimal behavior in the absence of complete knowledge due to its innate ability to reason under uncertainty. Using real workload data we provide a comparative analysis of popular RL algorithms including SARSA and Q-learning. Our empirical results demonstrate how our approach improves energy efficiency by 25% while also reducing service violations by 63% over the popular Power-Aware heuristic algorithm.}
}
@article{ZHANG2023116921,
title = {Energy management strategy for fuel cell vehicles via soft actor-critic-based deep reinforcement learning considering powertrain thermal and durability characteristics},
journal = {Energy Conversion and Management},
volume = {283},
pages = {116921},
year = {2023},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2023.116921},
url = {https://www.sciencedirect.com/science/article/pii/S0196890423002674},
author = {Yuanzhi Zhang and Caizhi Zhang and Ruijia Fan and Chenghao Deng and Song Wan and Hicham Chaoui},
keywords = {Energy management strategy, Fuel cell vehicles, Deep reinforcement learning, Soft actor-critic, Lifespan durability, Thermal stability},
abstract = {Temperature can significantly affect the water equilibrium, electrochemical kinetics and mass transmission in a proton exchange membrane fuel cell (PEMFC) stack, meanwhile it also impacts the lifespan and safety of a lithium-ion battery (LIB). Yet, energy management strategy (EMS) is rarely to synchronously study the durability performances of the LIB and PEMFC stack with their thermal effects in fuel cell vehicles (FCVs) under real-world driving scenarios. Thus, this study proposes a deep reinforcement learning (DRL)-based EMS to minimize transient costs of the LIB and PEMFC stack, which include their state-of-health (SOH) descents and overtemperature penalties. Meanwhile, the transient costs are incorporated into the overall cost, which comprises the hydrogen consumption rate of the PEMFC stack, and penalty of maintaining the LIB state-of-charge (SOC). Moreover, the soft actor-critic (SAC) is applied to the DRL-based EMS due to its advantage of stability across different random environments and no meticulous hyperparameter calibration. Specifically, the proposed EMS intelligently allocates the direct current (DC) bus power of FCVs in real time to maximize a multi-objective reward in accordance with FCV states, in which the reward is the negative overall cost. Then, long-term real-world driving scenarios in Chongqing city, China, are used for off-line training and real-time control to advance the adaptability of the proposed EMS. The results show that in comparison with the deep Q-network (DQN)-based EMS considering the powertrain temperature and durability, and the SAC-based EMS neglecting the powertrain temperature and durability, the proposed strategy can actualize overall SOH increments of the powertrain up to 14.01 % and 3.45 %, respectively, and restrict the maximum temperatures of the PEMFC stack and LIB. In addition, the generalization of the proposed EMS is verified, in which the trained model of the proposed EMS is tested in other FCV and driving cycles, and it can acquire similar effectiveness. Thus, the proposed strategy can enforce the lifespan durability and thermal stability of the powertrain system.}
}
@article{CHOWDHURY201951,
title = {DA-DRLS: Drift adaptive deep reinforcement learning based scheduling for IoT resource management},
journal = {Journal of Network and Computer Applications},
volume = {138},
pages = {51-65},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S1084804519301341},
author = {Abishi Chowdhury and Shital A. Raut and Husnu S. Narman},
keywords = {IoT resources, Deep reinforcement learning, Demand drift, Energy consumption, Response time, Resource utilization, Simulation},
abstract = {In order to fulfill the tremendous resource demand by diverse IoT applications, the large-scale resource-constrained IoT ecosystem requires a robust resource management technique. An optimal resource provisioning in IoT ecosystem deals with an efficient request-resource mapping which is difficult to achieve due to the heterogeneity and dynamicity of IoT resources and IoT requests. In this paper, we investigate the scheduling and resource allocation problem for dynamic user requests with varying resource requirements. Specifically, we formulate the complete problem as an optimization problem and try to generate an optimal policy with the objectives to minimize the overall energy consumption and to achieve a long-term user satisfaction through minimum response time. We introduce the paradigm of a deep reinforcement learning (DRL) mechanism to escalate the resource management efficiency in IoT ecosystem. To maximize the numerical performance of the entire resource management activities, our method learns to select the optimal resource allocation policy among a number of possible solutions. Moreover, the proposed approach can efficiently handle a sudden hike or fall in users' demand, which we call demand drift, through adaptive learning maintaining the optimal resource utilization. Finally, our simulation analysis illustrates the effectiveness of the proposed mechanism as it achieves substantial improvements in various factors, like reducing energy consumption and response time by at least 36.7% and 59.7% respectively and increasing average resource utilization by at least 10.4%. Our approach also attains a good convergence and a trade-off between the monitoring metrics.}
}
@article{INCREMONA202189,
title = {Experimental Assessment of Deep Reinforcement Learning for Robot Obstacle Avoidance: A LPV Control Perspective},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {8},
pages = {89-94},
year = {2021},
note = {4th IFAC Workshop on Linear Parameter Varying Systems LPVS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.586},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321013628},
author = {Gian Paolo Incremona and Nikolas Sacchi and Bianca Sangiovanni and Antonella Ferrara},
keywords = {deep reinforcement learning, robot control, collision avoidance, LPV},
abstract = {This work presents the experimental assessment of a hybrid control scheme based on Deep Reinforcement Learning (DRL) for obstacle avoidance in robot manipulators. More precisely, relying on an equivalent Linear Parameter Varying (LPV) state-space representation of the system, two operative modes, one based on both joint positions and velocities, one only based on velocity inputs, are activated depending on the measurement of the distance between the robot and the obstacle. Therefore, when the obstacle is close to the robot, a switching mechanism is introduced to enable the DRL algorithm instead of the basic motion planner, thus giving rise to a self-configuring architecture to cope with objects randomly moving in the workspace. The experimental tests of the DRL based collision avoidance hybrid strategy are carried out on a physical EPSON VT6 robot manipulator with satisfactory results.}
}
@article{DELRMILLAN1995275,
title = {Reinforcement learning of goal-directed obstacle-avoiding reaction strategies in an autonomous mobile robot},
journal = {Robotics and Autonomous Systems},
volume = {15},
number = {4},
pages = {275-299},
year = {1995},
note = {Reinforcement Learning and Robotics},
issn = {0921-8890},
doi = {https://doi.org/10.1016/0921-8890(95)00021-7},
url = {https://www.sciencedirect.com/science/article/pii/0921889095000217},
author = {José {del R. Millán}},
keywords = {Reinforcement learning, Mobile robots, Neural networks, Grounded planning},
abstract = {In this paper we argue for building reactive autonomous mobile robots through reinforcement connectionist learning. Nevertheless, basic reinforcement learning is a slow process. This paper describes an architecture which deals with complex— high-dimensional and/or continuous—situation and action spaces effectively. This architecture is based on two main ideas. The first is to organize the reactive component into a set of modules in such a way that, roughly, each one of them codifies the prototypical action for a given cluster of situations. The second idea is to use a particular kind of planning for figuring out what part of the action space deserves attention for each cluster of situations. Salient features of the planning process are that it is grounded and that it is invoked only when the reactive component does not generalize correctly its previous experience to the new situation. We also report our experience in solving a basic task that most autonomous mobile robots must face, namely path finding.}
}
@article{ALLEN2018578,
title = {Reward-based Monte Carlo-Bayesian reinforcement learning for cyber preventive maintenance},
journal = {Computers & Industrial Engineering},
volume = {126},
pages = {578-594},
year = {2018},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.09.051},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218304674},
author = {Theodore T. Allen and Sayak Roychowdhury and Enhao Liu},
keywords = {Preventative maintenance, Cyber security, Markov decision processes, Parametric uncertainty},
abstract = {This article considers a preventive maintenance problem related to cyber security in universities. A Bayesian Reinforcement Learning (BRL) problem is formulated using limited data from scan results and intrusion detection system warnings. The median estimated learning time (MELT) measure is introduced to evaluate the speed at which a control system effectively eliminates parametric uncertainty and probability is concentrated on a single scenario. It is demonstrated that the Monte Carlo BRL with enhancements including Latin hypercube sampling (LHS) to generate scenarios, identical systems multi-task learning, and reward-based learning achieves shorter MELT values, i.e., “faster” learning, and improved objective values compared with alternatives in a numerical study. Rigorous results establish the optimality of the derived control strategies and the fact that optimal learning is possible under steady state assumptions. Also, the real-world case study of policies for patching Linux critical server cyber vulnerabilities generates insights including the potential to reduce expenditure per host by mandating compensating controls for critical vulnerabilities.}
}