@article{HAILEMICHAEL2022149,
title = {Safety Filtering for Reinforcement Learning-based Adaptive Cruise Control},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {24},
pages = {149-154},
year = {2022},
note = {10th IFAC Symposium on Advances in Automotive Control AAC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.276},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322023084},
author = {Habtamu Hailemichael and Beshah Ayalew and Lindsey Kerbel and Andrej Ivanco and Keith Loiselle},
keywords = {Adaptive cruise control, Safe reinforcement learning, Safety filtering, Control barrier functions},
abstract = {Reinforcement learning (RL)-based adaptive cruise control systems (ACC) that learn and adapt to road, traffic and vehicle conditions are attractive for enhancing vehicle energy effciency and traffic flow. However, the application of RL in safety critical systems such as ACC requires strong safety guarantees which are diffcult to achieve with learning agents that have a fundamental need to explore. In this paper, we derive control barrier functions as safety filters that allow an RL-based ACC controller to explore freely within a collision safe set. Specifically, we derive control barrier functions for high relative degree nonlinear systems to take into account inertia effects relevant for commercial vehicles. We also outline an algorithm for accommodating actuation saturation with these barrier functions. While any RL algorithm can be used as the performance ACC controller together with these filters, we implement the Maximum A Posteriori Policy Optimization (MPO) algorithm with a hybrid action space that learns fuel optimal gear selection and torque control policies. The safety filtering RL approach is contrasted with a reward shaping RL approach that only learns to avoid collisions after sufficient training. Evaluations on different drive cycles demonstrate significant improvements in fuel economy with the proposed approach compared to baseline ACC algorithms.}
}
@article{REN20212787,
title = {Solving flow-shop scheduling problem with a reinforcement learning algorithm that generalizes the value function with neural network},
journal = {Alexandria Engineering Journal},
volume = {60},
number = {3},
pages = {2787-2800},
year = {2021},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2021.01.030},
url = {https://www.sciencedirect.com/science/article/pii/S1110016821000338},
author = {Jianfeng Ren and Chunming Ye and Feng Yang},
keywords = {Flow-shop scheduling problem (FSP), Reinforcement learning (RL), Generalized value function, Neural network (NN)},
abstract = {This paper solves the flow-shop scheduling problem (FSP) through the reinforcement learning (RL), which approximates the value function with neural network (NN). Under the RL framework, the state, strategy, action, reward signal, and value function of FSP were described in details. Considering the intrinsic features of FSP, various information of FSP was mapped into RL states, including the maximum, minimum, and mean of makespan, the maximum, minimum, and mean of remaining operations, as well as the load of machines. Besides, the optimal scheduling rules corresponding to specific states were mapped into the actions of RL. On this basis, the NN was trained to establish the mapping between states and actions, and select the action of the highest probability under a specific state. In addition, a reward function was constructed based on the idle time (IT) of machines, and the value function was generalized by the NN. Finally, our algorithm was tested on 23 benchmark examples and more than 7 sets of example machines. Small relative errors were achieved on 20 of the 23 benchmark examples and satisfactory results were realized on all 7 machine sets. The results confirm the superiority and universality of our algorithm, and indicate that FSP can be solved effectively by completely mapping it into our RL framework. The research results provide a reference for solving similar problems with RL algorithm based on value function approximation.}
}
@article{MOEINIZADE2022200076,
title = {A reinforcement Learning approach to resource allocation in genomic selection},
journal = {Intelligent Systems with Applications},
volume = {14},
pages = {200076},
year = {2022},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2022.200076},
url = {https://www.sciencedirect.com/science/article/pii/S2667305322000175},
author = {Saba Moeinizade and Guiping Hu and Lizhi Wang},
keywords = {Reinforcement learning, Genomic selection, Plant breeding, Resource allocation, Markov decision process, Function approximation},
abstract = {Genomic selection (GS) is a technique that plant breeders use to select individuals to mate and produce new generations of species. Allocation of resources is a key factor in GS. At each selection cycle, breeders are facing the choice of budget allocation to make crosses and produce the next generation of breeding parents. Inspired by recent advances in reinforcement learning for AI problems, we develop a reinforcement learning-based algorithm to automatically learn to allocate limited resources across different generations of breeding. We mathematically formulate the problem in the framework of Markov Decision Process (MDP) by defining state and action spaces. To avoid the explosion of the state space, an integer linear program is proposed that quantifies the trade-off between resources and time. Finally, we propose a value function approximation method to estimate the action-value function and then develop a greedy policy improvement technique to find the optimal resources. We demonstrate the effectiveness of the proposed method in enhancing genetic gain using a case study with realistic data.}
}
@article{WANG2023110816,
title = {MF^2: Model-free reinforcement learning for modeling-free building HVAC control with data-driven environment construction in a residential building},
journal = {Building and Environment},
volume = {244},
pages = {110816},
year = {2023},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2023.110816},
url = {https://www.sciencedirect.com/science/article/pii/S0360132323008430},
author = {Man Wang and Borong Lin},
keywords = {HVAC, Energy efficiency, Reinforcement learning, Environment, DQN, DDPG},
abstract = {Reinforcement Learning (RL) has advanced energy-efficient control of building Heating, Ventilation and Air Conditioning (HVAC) systems. Constructing a suitable RL environment for buildings is a crucial challenge. Compared to widely-used simulation-based environments, data-driven approaches offer higher training efficiency but face convergence difficulties due to influential factors, limiting their current application. To explore data-driven construction of RL environments for building HVAC systems, this study proposes two strategies for controlling room temperature setpoints in a residential building. XGBoost and Long Short-Term Memory Network (LSTM) are trained for energy consumption and room temperature prediction. One strategy predicts parameters for on-off states, while the other for power-on states. The XGBoost models are integrated into an OpenAI Gym environment. The first strategy achieves 0.8634 R2 and 0.2423 Root Mean Squared Error (RMSE) for energy consumption prediction. The R2 of room air temperature models are approximately 0.99 and the RMSE are lower than 0.31. The second strategy achieves 0.9181 R2 and 0.1042 RMSE for energy consumption prediction and similar performance for room temperature prediction. Deep Q-learning (DQN) and Deep Deterministic Policy Gradient (DDPG) algorithms are separately trained using these environments. Results show that the first strategy fails to induce the correct training of RL models, while the second strategy successfully induces a useable DDPG model for controlling building HVAC systems but fails to induce a useable DQN model. We analyze the reasons behind these observations. Compared to the original room temperature setpoint method, the DDPG-based HVAC control logic achieves a 10.06% energy-saving effect while ensuring comfort.}
}
@article{SONG2022104,
title = {Reinforcement learning facilitates an optimal interaction intensity for cooperation},
journal = {Neurocomputing},
volume = {513},
pages = {104-113},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.09.109},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222012000},
author = {Zhao Song and Hao Guo and Danyang Jia and Matjaž Perc and Xuelong Li and Zhen Wang},
keywords = {Evolutionary game theory, Cooperation, Reinforcement learning theory, Interaction intensity},
abstract = {Our social interactions vary over time and they depend on various factors that determine our preferences and goals, both in personal and professional terms. Researches have shown that this plays an important role in promoting cooperation and prosocial behavior in general. Indeed, it is natural to assume that ties among cooperators would become stronger over time, while ties with defectors (non-cooperators) would eventually be severed. Here we introduce reinforcement learning as a determinant of adaptive interaction intensity in social dilemmas and study how this translates into the structure of the social network and its propensity to sustain cooperation. We merge the iterated prisoner’s dilemma game with the Bush-Mostelle reinforcement learning model and show that there exists a moderate switching dynamics of the interaction intensity that is optimal for the evolution of cooperation. Besides, the results of Monte Carlo simulations are further supported by the calculations of dynamical pair approximation. These observations show that reinforcement learning is sufficient for the emergence of optimal social interaction patterns that facilitate cooperation. This in turn supports the social capital hypothesis with a minimal set of assumptions that guide the self-organization of our social fabric.}
}
@article{SUN2024109896,
title = {Application-oriented mode decision for energy management of range-extended electric vehicle based on reinforcement learning},
journal = {Electric Power Systems Research},
volume = {226},
pages = {109896},
year = {2024},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2023.109896},
url = {https://www.sciencedirect.com/science/article/pii/S0378779623007848},
author = {Ziyi Sun and Rong Guo and Xiang Xue and Ze Hong and Maohui Luo and Pak Kin Wong and Jason J.R. Liu and Xiaozheng Wang},
keywords = {Energy management strategy, Deep Q-network, Mode decision, Application-oriented},
abstract = {Energy management strategies play an important role in range-extended electric vehicles. An application-oriented energy management strategy based on deep Q-network is proposed, which takes the deviation of the SOC from the reference curve, vehicle speed, and required power of the vehicle as the state, and takes the mode decision as the action. The designed concise Q agent can be well-trained under standard driving cycles and real-world driving cycle. The high-dimensional mappings can be extracted from trained agents and are applicable to actual vehicle controllers. Compared with the rule-based and Q learning-based EMS, the proposed strategy has more mode selection judgment conditions and continuous state space, which can better select the timing of mode switching, thus effectively improving the engine operating efficiency and optimizing fuel economy. The experimental tests are conducted to verify the practical feasibility.}
}
@article{GANESH2022111833,
title = {A review of reinforcement learning based energy management systems for electrified powertrains: Progress, challenge, and potential solution},
journal = {Renewable and Sustainable Energy Reviews},
volume = {154},
pages = {111833},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111833},
url = {https://www.sciencedirect.com/science/article/pii/S136403212101100X},
author = {Akhil Hannegudda Ganesh and Bin Xu},
keywords = {Electrified powertrain, Reinforcement learning, Energy management strategies, Deep reinforcement learning, Connected & autonomous vehicles},
abstract = {The impact of internal combustion engine-powered automobiles on climate change due to emissions and the depletion of fossil fuels has contributed to the progress of electrified powertrains. Energy management strategies (EMS) have shown huge impact on the energy efficiency of the electrified powertrains. In recent years, Reinforcement Learning (RL) based algorithms have been intensely investigated to develop EMS for electrified powertrain with specific depth in hybrid electric vehicles (HEV), battery electric vehicles (BEV) and fuel cell vehicles (FCV) and the research in this area is still acelerating. However, a comprehensive review of RL-based EMS is lacking in literature. This article reviews the recent penetration of RL based EMS like Q-learning, Deep Q Learning, deep deterministic policy gradient in the electrified powertrains domain. Extensive importance is given to the classification of the literature based on powertrain architecture, RL algorithm, and the different features and operation mechanisms of relevant algorithms are highlighted. The use of connected and autonomous vehicles and relevant communication technology to develop RL-based systems have also been discussed. More importantly, the challenges with regards to existing research in the field of RL-based EMS and the potential solutions with scope for future research are also discussed.}
}
@article{NGUYEN2022156,
title = {DeepPlace: Deep reinforcement learning for adaptive flow rule placement in Software-Defined IoT Networks},
journal = {Computer Communications},
volume = {181},
pages = {156-163},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421003789},
author = {Tri Gia Nguyen and Trung V. Phan and Dinh Thai Hoang and Hai Hoang Nguyen and Duc Tran Le},
keywords = {Flow rule placement, Quality-of-Service, Software-Defined Networking, Internet of Things, Deep reinforcement learning},
abstract = {In this paper, we propose a novel and adaptive flow rule placement system based on deep reinforcement learning, namely DeepPlace, in Software-Defined Internet of Things (SDIoT) networks. DeepPlace can provide a fine-grained traffic analysis capability while assuring QoS of traffic flows and proactively avoiding the flow-table overflow issue in the data plane. Specifically, we first investigate the traffic forwarding process in an SDIoT network, i.e., routing and flow rule placement tasks. We design a cost function for the routing to set up traffic flow paths in the data plane. Next, we propose an adaptive flow rule placement approach to maximize the number of match-fields in a flow rule at SDN switches. To deal with the dynamics of IoT traffic flows, we model the system operation by using the Markov decision process (MDP) with a continuous action space and formulate its optimization problem. Subsequently, we develop a deep deterministic policy gradient-based algorithm to help the system obtain the optimal policy. The evaluation results demonstrate that DeepPlace can efficiently maintain a significant number of match-fields in a flow rule, i.e., approximately 86% of the maximum level, while minimizing the QoS violation ratio of traffic flows, i.e., 6.7%, in a highly dynamic traffic scenario, which outperforms three other existing solutions, i.e., FlowMan, FlowStat, and DeepMatch.}
}
@article{LV2022110391,
title = {A reinforcement learning based method for protein’s differential scanning calorimetry signal separation},
journal = {Measurement},
volume = {188},
pages = {110391},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2021.110391},
url = {https://www.sciencedirect.com/science/article/pii/S0263224121012823},
author = {Xin Lv and Shuyu Wang and Yuliang Zhao and Peng Shan},
keywords = {Differential scanning calorimetry, Peak separation, Reinforcement learning, Automatic data analysis},
abstract = {Differential scanning calorimetry (DSC) is a powerful technique to study protein stability, since the DSC test data provides valuable insights to characterize protein folding thermodynamics. Researchers in the drug discovery field need to manually analyze the DSC curves in multiple steps, such as baseline subtraction, data fitting, integration, and domain deconvolution. To improve the efficiency and consistency of data processing, machine learning methods for automatic DSC peak identification and baseline estimation were seen in prior research. However, the DSC’s automatic peak separation remained unexplored, despite its significant role in explaining the multi-domain protein unfolding. In this research, we propose a method based on reinforcement learning to separate the overlapping peaks of the DSC signal. We use two types of protein data to verify the effectiveness of this method. It automatically deconvolutes the peak signals into multiple sub-peaks. Our automated analysis method could lead to improved efficiency in DSC signal analysis when high volume data is involved. The code and data for this work can be found at: https://github.com/shuyu-wang/DSC_analysis_peak_separation.}
}
@article{KHAN201242,
title = {Reinforcement learning and optimal adaptive control: An overview and implementation examples},
journal = {Annual Reviews in Control},
volume = {36},
number = {1},
pages = {42-59},
year = {2012},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2012.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1367578812000053},
author = {Said G. Khan and Guido Herrmann and Frank L. Lewis and Tony Pipe and Chris Melhuish},
keywords = {Reinforcement learning, ADP, Q-learning, Optimal adaptive control},
abstract = {This paper provides an overview of the reinforcement learning and optimal adaptive control literature and its application to robotics. Reinforcement learning is bridging the gap between traditional optimal control, adaptive control and bio-inspired learning techniques borrowed from animals. This work is highlighting some of the key techniques presented by well known researchers from the combined areas of reinforcement learning and optimal control theory. At the end, an example of an implementation of a novel model-free Q-learning based discrete optimal adaptive controller for a humanoid robot arm is presented. The controller uses a novel adaptive dynamic programming (ADP) reinforcement learning (RL) approach to develop an optimal policy on-line. The RL joint space tracking controller was implemented for two links (shoulder flexion and elbow flexion joints) of the arm of the humanoid Bristol-Elumotion-Robotic-Torso II (BERT II) torso. The constrained case (joint limits) of the RL scheme was tested for a single link (elbow flexion) of the BERT II arm by modifying the cost function to deal with the extra nonlinearity due to the joint constraints.}
}
@article{HUANG2023116678,
title = {Deep reinforcement learning based energy management strategy for range extend fuel cell hybrid electric vehicle},
journal = {Energy Conversion and Management},
volume = {277},
pages = {116678},
year = {2023},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2023.116678},
url = {https://www.sciencedirect.com/science/article/pii/S0196890423000249},
author = {Yin Huang and Haoqin Hu and Jiaqi Tan and Chenlei Lu and Dongji Xuan},
keywords = {Range extend fuel cell hybrid electric vehicle, Working pattern, Energy management strategy, Deep deterministic policy gradient, Previous action guidance mechanism},
abstract = {To meet the power and long-range driving requirements of the vehicle, this paper presents a dual mode operation scheme for a range extend fuel cell hybrid vehicle for the first time, with an in-depth study of the pure electric mode and the range extend mode. The deep deterministic policy gradient algorithm is a well-known deep reinforcement learning algorithm that can solve complex nonlinear problems. To achieve the optimal power distribution among energy sources in the two modes, a dual deep deterministic policy gradient algorithm framework is proposed for the first time in this paper. In addition, a pervious action guidance mechanism is proposed to enable networks to approximate the action value function more efficiently in training. The training results show that the adopted previous action guidance mechanism helps to improve the learning convergence and exploration ability. The validation results show that the proposed strategy improves the operating economy by about 30% compared to the rule-based strategy, reduces the average fuel cell output fluctuation to less than 100 W, and reduces the fuel cell lifetime loss greatly. It is hoped that the proposed new structure, patterns, and energy management strategy will provide more ideas for scholars in future research.}
}
@article{GARG2023109760,
title = {Deep reinforcement learning for next-generation IoT networks},
journal = {Computer Networks},
volume = {228},
pages = {109760},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109760},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623002050},
author = {Sahil Garg and Jia Hu and Giancarlo Fortino and Laurence T. Yang and Mohsen Guizani and Xianjun Deng and Danda B. Rawat}
}
@article{LI2023103063,
title = {Multi-objective reinforcement learning in process control: A goal-oriented approach with adaptive thresholds},
journal = {Journal of Process Control},
volume = {129},
pages = {103063},
year = {2023},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2023.103063},
url = {https://www.sciencedirect.com/science/article/pii/S0959152423001506},
author = {Dazi Li and Wentao Gu and Tianheng Song},
keywords = {Multi-objective reinforcement learning, Adaptive thresholded lexicographic ordering, Goal selection strategy, Fermentation process},
abstract = {In practical control problems with multiple conflicting objectives, multi-objective optimization (MOO) problems must be simultaneously addressed. To tackle these challenges, scholars have extensively studied multi-objective reinforcement learning (MORL) in recent years. However, due to the complexity of the system and the difficulty in determining preferences between objectives, complex continuous control processes involving MOO problems still require further research. In this study, an innovative goal-oriented MORL algorithm is proposed. The agent is better guided for optimization through adaptive thresholds and goal selection strategy. Additionally, the reward function is refined based on the chosen objective. To validate the approach, a comprehensive environment for the fermentation process is designed. Experimental results show that our proposed algorithm surpasses other benchmark algorithms in most performance metrics. Moreover, the Pareto solution set found by our algorithm is closer to the true Pareto frontier of fermentation problems.}
}
@article{SOLEYMANI2021115127,
title = {Deep graph convolutional reinforcement learning for financial portfolio management – DeepPocket},
journal = {Expert Systems with Applications},
volume = {182},
pages = {115127},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115127},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421005686},
author = {Farzan Soleymani and Eric Paquet},
keywords = {Portfolio management, Deep reinforcement learning, Restricted stacked autoencoder, Online leaning, Actor-critic, Graph convolutional network},
abstract = {Portfolio management aims at maximizing the return on investment while minimizing risk by continuously reallocating the assets forming the portfolio. These assets are not independent but correlated during a short time period. A graph convolutional reinforcement learning framework called DeepPocket is proposed whose objective is to exploit the time-varying interrelations between financial instruments. These interrelations are represented by a graph whose nodes correspond to the financial instruments while the edges correspond to a pair-wise correlation function in between assets. DeepPocket consists of a restricted, stacked autoencoder for feature extraction, a convolutional network to collect underlying local information shared among financial instruments and an actor-critic reinforcement learning agent. The actor-critic structure contains two convolutional networks in which the actor learns and enforces an investment policy which is, in turn, evaluated by the critic in order to determine the best course of action by constantly reallocating the various portfolio assets to optimize the expected return on investment. The agent is initially trained offline with online stochastic batching on historical data. As new data become available, it is trained online with a passive concept drift approach to handle unexpected changes in their distributions. DeepPocket is evaluated against five real-life datasets over three distinct investment periods, including during the Covid-19 crisis, and clearly outperformed market indexes.}
}
@article{PADHYE2023126314,
title = {A deep actor critic reinforcement learning framework for learning to rank},
journal = {Neurocomputing},
volume = {547},
pages = {126314},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126314},
url = {https://www.sciencedirect.com/science/article/pii/S092523122300437X},
author = {Vaibhav Padhye and Kailasam Lakshmanan},
keywords = {Reinforcement learning, Learning to Rank, Deep reinforcement learning, Policy gradient},
abstract = {In this paper, we propose a Deep Reinforcement learning based approach for Learning to rank task. Reinforcement Learning has been applied in the ranking task with good success, but the existing Policy Gradient based approaches suffer from noisy gradients and high variance, resulting in unstable learning. The natural policy gradient methods like REINFORCE perform Monte Carlo sampling, thus taking samples randomly, which leads to high variance. As the action space becomes large, i.e., with a very large number of documents, traditional RL techniques lack the complex model required in the scenario to deal with a large number of items. We propose a Deep Reinforcement learning based approach for learning to rank task in this paper to address these issues. By combining Deep learning with the Reinforcement Learning framework, our approach can learn a complex function as deep neural networks can provide significant function approximation. We used Actor-Critic framework where the critic network can reduce variance by utilizing techniques such as clipped delayed policy updates, clipped double q learning, etc. Also, due to the enormous space of the web, the most relevant results are needed to be returned for the corresponding query from within a large action space. Policy gradient algorithms have been effectively applied to problems in large action spaces(items) with deep neural networks as they don’t rely on finding value for each action(item) as in value-based methods. Further, we use an actor-network with a CNN layer in the ranking process to capture the sequential patterns among the documents. We utilize the TD3 method to train our Reinforcement Learning agent with a listwise loss function, which performs delayed policy updates resulting in value estimates with lower variance. To the best of our knowledge, this is the first Deep reinforcement learning method applied in Learning to Rank for document retrieval. We performed experiments on the various Letor datasets and showed that our method outperforms various state-of-the-art baselines.}
}
@article{JHA2023200218,
title = {An appropriate and cost-effective hospital recommender system for a patient of rural area using deep reinforcement learning},
journal = {Intelligent Systems with Applications},
volume = {18},
pages = {200218},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200218},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323000431},
author = {Rajesh K. Jha and Sujoy Bag and Debbani Koley and Giridhar Reddy Bojja and Subhas Barman},
keywords = {Hospital, Doctors, Patients, Recommender systems, Monte Carlo learning, Deep reinforcement learning},
abstract = {Insufficient doctors and nurses enable a weak healthcare system in developing and undeveloped countries. This study aims to mitigate the demand-supply gap of doctor patients of an undeveloped or developing county. We observe people in a rural area, unaware of an appropriate hospital and doctors for their disease, and randomly go to the nearest hospital to check-up their health. However, each doctor has expertise in a specific disease, and hospitals' treatment performance varies. As a result, the patient engages multiple doctors and hospitals to cure their disease. This study develops as an appropriate and cost-effective hospital recommender system for a specific disease to provide the best hospital to a patient using deep reinforcement learning. Hence, the patient's treatment time, insignificant medicine consumption, the side effect of using inappropriate medicine, and a doctor's load can be minimized using the developed hospital recommender system.}
}
@article{SEYYEDABBASI2021107044,
title = {Hybrid algorithms based on combining reinforcement learning and metaheuristic methods to solve global optimization problems},
journal = {Knowledge-Based Systems},
volume = {223},
pages = {107044},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107044},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121003075},
author = {Amir Seyyedabbasi and Royal Aliyev and Farzad Kiani and Murat Ugur Gulle and Hasan Basyildiz and Mohammed Ahmed Shah},
keywords = {Metaheuristic algorithm, Reinforcement learning algorithm, Grey wolf optimization algorithm, Whale optimization algorithm, Q-learning},
abstract = {This paper introduces three hybrid algorithms that help in solving global optimization problems using reinforcement learning along with metaheuristic methods. Using the algorithms presented, the search agents try to find a global optimum avoiding the local optima trap. Compared to the classical metaheuristic approaches, the proposed algorithms display higher success in finding new areas as well as exhibiting a more balanced performance while in the exploration and exploitation phases. The algorithms employ reinforcement agents to select an environment based on predefined actions and tasks. A reward and penalty system is used by the agents to discover the environment, done dynamically without following a predetermined model or method. The study makes use of Q-Learning method in all three metaheuristic algorithms, so-called RLI−GWO, RLEx−GWO, and RLWOA algorithms, so as to check and control exploration and exploitation with Q-Table. The Q-Table values guide the search agents of the metaheuristic algorithms to select between the exploration and exploitation phases. A control mechanism is used to get the reward and penalty values for each action. The algorithms presented in this paper are simulated over 30 benchmark functions from CEC 2014, 2015 and the results obtained are compared with well-known metaheuristic and hybrid algorithms (GWO, RLGWO, I-GWO, Ex-GWO, and WOA). The proposed methods have also been applied to the inverse kinematics of the robot arms problem. The results of the used algorithms demonstrate that RLWOA provides better solutions for relevant problems.}
}
@article{ALEXANDRUZAMFIRACHE2023120112,
title = {Neural Network-based control using Actor-Critic Reinforcement Learning and Grey Wolf Optimizer with experimental servo system validation},
journal = {Expert Systems with Applications},
volume = {225},
pages = {120112},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120112},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423006140},
author = {Iuliu {Alexandru Zamfirache} and Radu-Emil Precup and Raul-Cristian Roman and Emil M. Petriu},
keywords = {Actor-critic, Grey Wolf Optimizer, Neural network training, Optimal reference tracking control, Reinforcement learning, Servo systems},
abstract = {This paper introduces a novel reference tracking control approach implemented using a combination of the Actor-Critic Reinforcement Learning (RL) framework and the Grey Wolf Optimizer (GWO) algorithm. The classical neural network (NN)-based implementation of the Critic, optimized with the Gradient Descent (GD) algorithm, is replaced with the GWO algorithm, aiming to eliminate the main drawbacks of the GD algorithm, i.e., slow convergence and the tendency to get stuck in local optimal values. The combined effort from multiple search agents and the random values involved in the search process make the GWO algorithm very efficient in exploring the solution space and finding global optimal solutions. The main objective of the proposed approach is to build a NN-based controller capable of solving an optimal reference tracking control problem on nonlinear servo system laboratory equipment. The training data needed to build the controller is collected while the actor learns how to control the servo system, using the GWO-based critic to monitor the process and step in to correct the actor when needed. A comparison study is performed across three online RL-based control approaches, namely the novel approach using GWO to implement the Critic in the Actor-Critic RL framework, the traditional approach using NNs with GD for optimization and another approach using a metaheuristic algorithm called Particle Swarm Optimization (PSO). The experimental results illustrate the superiority of the proposed approach over the competing ones.}
}
@article{CHEN2023104367,
title = {General real-time three-dimensional multi-aircraft conflict resolution method using multi-agent reinforcement learning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {157},
pages = {104367},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2023.104367},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X23003571},
author = {Yutong Chen and Yan Xu and Lei Yang and Minghua Hu},
keywords = {Air traffic management, Three-dimensional multi-aircraft conflict resolution, Multi-agent reinforcement learning, Deep q-learning network, Generalisation, Uncertainty},
abstract = {Reinforcement learning (RL) techniques have been studied for solving the conflict resolution (CR) problem in air traffic management, leveraging their potential for computation and ability to handle uncertainty. However, challenges remain that impede the application of RL methods to CR in practice, including three-dimensional manoeuvres, generalisation, trajectory recovery, and success rate. This paper proposes a general multi-agent reinforcement learning approach for real-time three-dimensional multi-aircraft conflict resolution, in which agents share a neural network and are deployed on each aircraft to form a distributed decision-making system. To address the challenges, several technologies are introduced, including a partial observation model based on imminent threats for generalisation, a safety separation relaxation model for multiple flight levels for three-dimensional manoeuvres, an adaptive manoeuvre strategy for trajectory recovery, and a conflict buffer model for success rate. The Rainbow Deep Q-learning Network (DQN) is used to enhance the efficiency of the RL process. A simulation environment that considers flight uncertainty (resulting from mechanical and navigation errors and wind) is constructed to train and evaluate the proposed approach. The experimental results demonstrate that the proposed method can resolve conflicts in scenarios with much higher traffic density than in today’s real-world situations.}
}
@article{LORK2020115426,
title = {An uncertainty-aware deep reinforcement learning framework for residential air conditioning energy management},
journal = {Applied Energy},
volume = {276},
pages = {115426},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.115426},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920309387},
author = {Clement Lork and Wen-Tai Li and Yan Qin and Yuren Zhou and Chau Yuen and Wayes Tushar and Tapan K. Saha},
keywords = {Bayesian neural networks, Air conditioning, Energy saving},
abstract = {Most existing methods for controlling the energy consumption of air conditioning (AC), focus on either scheduling the switching (on/off) of compressors or optimizing the overall energy consumption of AC system of an entire building. Unlike commercial buildings, residential apartments typically house separate ACs in individual rooms occupied by people with different thermal comfort preferences. Fortunately, the advancement of Internet-of-Things (IoT) technology has enabled the exploitation of sensory data to intelligently control the set-point temperature of ACs in individual rooms based on environmental conditions and occupant’s preferences, improving the energy efficiency of residential buildings. Indeed, control decisions based on sensory data may suffer from uncertainties due to error in data measurement and contribute to model uncertainty. This work proposes a data-driven uncertainty-aware approach to control split-type inverter ACs of residential buildings. First, information from similar AC and residential units are aggregated to reduce data imbalances, and Bayesian-Convolutional-Neural-Networks (BCNNs) are utilized to model the performance and uncertainty of the ACs from the aggregated data. Second, a Q-learning based reinforcement learning algorithm for set-point decision making is designed for setpoint optimization with transitions sampled from the BCNN models. Third, a case study is simulated based on such a framework to show that the control actions taken by the uncertainty-aware agent perform better in terms of discomfort management and energy savings compared to the uncertainty unaware agent. Further, the agent could also be adjusted to capture the trade-off between energy savings and comfort levels for varying degrees of energy and discomfort savings.}
}
@article{DAI2022107348,
title = {Aerodynamic optimization of high-lift devices using a 2D-to-3D optimization method based on deep reinforcement learning and transfer learning},
journal = {Aerospace Science and Technology},
volume = {121},
pages = {107348},
year = {2022},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2022.107348},
url = {https://www.sciencedirect.com/science/article/pii/S1270963822000220},
author = {Jiahua Dai and Peiqing Liu and Qiulin Qu and Ling Li and Tongzhi Niu},
keywords = {High-lift device, Transfer learning, Deep reinforcement learning, Three-dimensional optimization},
abstract = {The computational fluid dynamics is the main method to evaluate the aerodynamic performance for the optimization of high-lift devices. Currently, the direct three-dimensional (3D) optimization requires significant computational resources. Additionally, the commonly used heuristic algorithms can not extract the experience of two-dimensional (2D) optimization to accelerate the 3D optimization process. In order to resolve these issues, a novel 2D-to-3D optimization method based on the coupling of Deep Reinforcement Learning (DRL) and Transfer Learning (TL) is proposed to conduct the aerodynamic optimization of the 3D high-lift devices and tested on the NASA Trap Wing model. The 2D optimization is first carried out, and then its neural networks in DRL and the optimal configuration are extracted by TL to turn into the 3D optimization. Compared with the direct 3D optimization, the proposed 2D-to-3D optimization method can result in improved aerodynamic performance for the same computational cost, or can save 51%-81% of the computational cost to obtain a similar performance.}
}
@article{TOUZANI2021117733,
title = {Controlling distributed energy resources via deep reinforcement learning for load flexibility and energy efficiency},
journal = {Applied Energy},
volume = {304},
pages = {117733},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117733},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921010801},
author = {Samir Touzani and Anand Krishnan Prakash and Zhe Wang and Shreya Agarwal and Marco Pritoni and Mariam Kiran and Richard Brown and Jessica Granderson},
keywords = {Deep reinforcement learning, Deep deterministic policy gradient algorithm, Smart buildings, Control systems, Distributed energy resources, Load flexibility, Energy efficiency},
abstract = {Behind-the-meter distributed energy resources (DERs), including building solar photovoltaic (PV) technology and electric battery storage, are increasingly being considered as solutions to support carbon reduction goals and increase grid reliability and resiliency. However, dynamic control of these resources in concert with traditional building loads, to effect efficiency and demand flexibility, is not yet commonplace in commercial control products. Traditional rule-based control algorithms do not offer integrated closed-loop control to optimize across systems, and most often, PV and battery systems are operated for energy arbitrage and demand charge management, and not for the provision of grid services. More advanced control approaches, such as MPC control have not been widely adopted in industry because they require significant expertise to develop and deploy. Recent advances in deep reinforcement learning (DRL) offer a promising option to optimize the operation of DER systems and building loads with reduced setup effort. However, there are limited studies that evaluate the efficacy of these methods to control multiple building subsystems simultaneously. Additionally, most of the research has been conducted in simulated environments as opposed to real buildings. This paper proposes a DRL approach that uses a deep deterministic policy gradient algorithm for integrated control of HVAC and electric battery storage systems in the presence of on-site PV generation. The DRL algorithm, trained on synthetic data, was deployed in a physical test building and evaluated against a baseline that uses the current best-in-class rule-based control strategies. Performance in delivering energy efficiency, load shift, and load shed was tested using price-based signals. The results showed that the DRL-based controller can produce cost savings of up to 39.6% as compared to the baseline controller, while maintaining similar thermal comfort in the building. The project team has also integrated the simulation components developed during this work as an OpenAIGym environment and made it publicly available so that prospective DRL researchers can leverage this environment to evaluate alternate DRL algorithms.}
}
@article{GUO2023250,
title = {Online adaptive optimal control algorithm based on synchronous integral reinforcement learning with explorations},
journal = {Neurocomputing},
volume = {520},
pages = {250-261},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.11.055},
url = {https://www.sciencedirect.com/science/article/pii/S092523122201431X},
author = {Lei Guo and Han Zhao},
keywords = {Reinforcement learning, Neural networks, Adaptive control, Actor-critic, Explorations},
abstract = {In this study, we present a novel algorithm, based on synchronous policy iteration, to solve the continuous-time infinite-horizon optimal control problem of input affine system dynamics. The integral reinforcement is measured as an excitation signal to estimate the solution to the Hamilton–Jacobi–Bellman equation. In addition, the proposed method is completely model-free, that is, no a priori knowledge of the system is required. Using the adaptive tuning law, the actor and critic neural networks can simultaneously approximate the optimal value function and policy. The persistence of excitation condition is required to guarantee the convergence of the two networks. Unlike in traditional policy iteration algorithms, the restriction of the initial admissible policy was eliminated using this method. The effectiveness of the proposed algorithm is verified through numerical simulations.}
}
@article{LUO2022104848,
title = {A graph convolutional encoder and multi-head attention decoder network for TSP via reinforcement learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {112},
pages = {104848},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.104848},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622001038},
author = {Jia Luo and Chaofeng Li and Qinqin Fan and Yuxin Liu},
keywords = {TSP, Graph convolutional network, Attention mechanism, Deep reinforcement learning},
abstract = {For the traveling salesman problem (TSP), it is usually hard to find a high-quality solution in polynomial time. In the last two years, graph neural networks emerge as a promising technique for TSP. However, most related learning-based methods do not make full use of the hierarchical features; thereby, resulting in relatively-low performance. Furthermore, the decoder in those methods only generates single permutation and needs additional search strategies to improve the permutation, which leads to more computing time. In this work, we propose a novel graph convolutional encoder and multi-head attention decoder network (GCE-MAD Net) to fix the two drawbacks. The graph convolutional encoder realizes to aggregate neighborhood information through updated edge features and extract hierarchical graph features from all graph convolutional layers. The multi-head attention decoder takes the first and last selected node embeddings and fused graph embeddings as input to generate probability distributions of selecting next unvisited node in order to consider global features. The GCE-MAD Net further allows to choose several nodes at each time step and generate a permutations pool after decoding to increase diversity of solution space. To assess the performance of GCE-MAD Net, we conduct experiments with randomly generated instances. The simulation results show the proposed GCE-MAD Net outperforms the traditional heuristics methods and existing learning-based algorithms on all evaluation metrics. Especially, when encountering large scale problem instances, the small scale pretrained GCE-MAD Net can get much better solutions than CPLEX solver with less time.}
}
@article{XUE2022212,
title = {Event-triggered integral reinforcement learning for nonzero-sum games with asymmetric input saturation},
journal = {Neural Networks},
volume = {152},
pages = {212-223},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022001459},
author = {Shan Xue and Biao Luo and Derong Liu and Ying Gao},
keywords = {Adaptive dynamic programming, Reinforcement learning, Event-triggered mechanism, Asymmetric input saturation, Experience replay},
abstract = {In this paper, an event-triggered integral reinforcement learning (IRL) algorithm is developed for the nonzero-sum game problem with asymmetric input saturation. First, for each player, a novel non-quadratic value function with a discount factor is designed, and the coupled Hamilton–Jacobi equation that does not require a complete knowledge of the game is derived by using the idea of IRL. Second, the execution of each player is based on the event-triggered mechanism. In the implementation, an adaptive dynamic programming based learning scheme using a single critic neural network (NN) is developed. Experience replay technique is introduced into the classical gradient descent method to tune the weights of the critic NN. The stability of the system and the elimination of Zeno behavior are proved. Finally, simulation experiments verify the effectiveness of the event-triggered IRL algorithm.}
}
@article{DANG2022122919,
title = {Towards stochastic modeling for two-phase flow interfacial area predictions: A physics-informed reinforcement learning approach},
journal = {International Journal of Heat and Mass Transfer},
volume = {192},
pages = {122919},
year = {2022},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2022.122919},
url = {https://www.sciencedirect.com/science/article/pii/S0017931022003921},
author = {Zhuoran Dang and Mamoru Ishii},
keywords = {Reinforcement learning, Markov decision process, Bubbly flow, Interfacial area transport, Interfacial area concentration},
abstract = {The stochastic nature of turbulent two-phase flow determines that the deterministic modeling approaches always have limited predicting range and accuracy, which is due to the averaging and approximation made during the model developments. On the other hand, well-developed machine learning models that are suitable for complex tasks can be used as a surrogate to improve the predictions. In this paper, a physics-informed reinforcement learning-based method for interfacial area prediction is proposed. The method aims to capture the complexity of the two-phase flow using the advantage of reinforcement learning with the aid of the Interfacial Area Transport Equation. A Markov Decision Process (MDP) that describes the bubble transformation in the fluid flow is established by assuming that the development of two-phase flow is a stochastic process with Markov property. The details of the method’s framework design are described, including the design of the MDP, environment setup, and the algorithms used to solve the RL problem. The performance of the newly proposed method is tested through experiments based on an experimental database for vertical upward bubbly air-water flows. The result shows that with a knowledge-based stochastic policy, good performance of the new method is achieved with the rRMSE of 0.1573, and it is a significant performance boost to the applied IATE model. The approaches to extending the capability of this new RL-based method are discussed, which is a reference for the further development of this approach.}
}
@article{HWANG202283,
title = {Option compatible reward inverse reinforcement learning},
journal = {Pattern Recognition Letters},
volume = {154},
pages = {83-89},
year = {2022},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2022.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167865522000241},
author = {Rakhoon Hwang and Hanjin Lee and Hyung Ju Hwang},
keywords = {Reinforcement learning, Inverse reinforcement learning, Transfer learning, Machine learning},
abstract = {Reinforcement learning in complex environments is a challenging problem. In particular, the success of reinforcement learning algorithms depends on a well-designed reward function. Inverse reinforcement learning (IRL) solves the problem of recovering reward functions from expert demonstrations. In this paper, we solve a hierarchical inverse reinforcement learning problem within the options framework, which allows us to utilize intrinsic motivation of the expert demonstrations. A gradient method for parametrized options is used to deduce a defining equation for the Q-feature space, which leads to a reward feature space. Using a second-order optimality condition for option parameters, an optimal reward function is selected. Experimental results in both discrete and continuous domains confirm that our recovered rewards provide a solution to the IRL problem using temporal abstraction, which in turn are effective in accelerating transfer learning tasks. We also show that our method is robust to noises contained in expert demonstrations.}
}
@article{DURGUT2021773,
title = {Adaptive operator selection with reinforcement learning},
journal = {Information Sciences},
volume = {581},
pages = {773-790},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521010331},
author = {Rafet Durgut and Mehmet Emin Aydin and Ibrahim Atli},
keywords = {Artificial bee colony, Adaptive operator selection, Reinforcement learning, Q-learning, Clustering-based Q-learning},
abstract = {Operator selection plays a crucial role in the efficiency of heuristic-based problem solving algorithms, especially, when a pool of operators is used to let algorithms dynamically select operators to produce new candidate solutions. A sequence of selected operators forms up throughout the search which impacts the success of the algorithms. Successive operators in a bespoke sequence can be complementary and therefore diversify the search while randomly selected operators are not expected to behave in this way. State of art adaptive selection schemes have been proposed to select the best next operator without considering the problem state in the process. In this study, a reinforcement learning algorithm is proposed to embed in a standard artificial bee colony algorithm for taking the problem state on board in operator selection process. The proposed approach implies mapping the problem states to the best fitting operators in the pool so as to achieve higher diversity and shape up an optimum operator sequence throughout the search process. The experimental study successfully demonstrates that the proposed idea works towards higher efficiency. The state of art approaches are outperformed with respect to the quality of solution in solving Set Union Knapsack problem over 30 benchmarking instances.}
}
@article{ZHANG2019472,
title = {Whole building energy model for HVAC optimal control: A practical framework based on deep reinforcement learning},
journal = {Energy and Buildings},
volume = {199},
pages = {472-490},
year = {2019},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2019.07.029},
url = {https://www.sciencedirect.com/science/article/pii/S0378778818330858},
author = {Zhiang Zhang and Adrian Chong and Yuqi Pan and Chenlu Zhang and Khee Poh Lam},
keywords = {HVAC, Energy efficiency, Whole building energy model, Optimal control, Deep reinforcement learning},
abstract = {Whole building energy model (BEM) is a physics-based modeling method for building energy simulation. It has been widely used in the building industry for code compliance, building design optimization, retrofit analysis, and other uses. Recent research also indicates its strong potential for the control of heating, ventilation and air-conditioning (HVAC) systems. However, its high-order nature and slow computational speed limit its practical application in real-time HVAC optimal control. Therefore, this study proposes a practical control framework (named BEM-DRL) that is based on deep reinforcement learning. The framework is implemented and assessed in a novel radiant heating system in an existing office building as a case study. The complete implementation process is presented in this study, including: building energy modeling for the novel heating system, multi-objective BEM calibration using the Bayesian method and the Genetic Algorithm, deep reinforcement learning training and simulation results evaluation, and control deployment. By analyzing the real-life control deployment data, it is found that BEM-DRL achieves 16.7% heating demand reduction with more than 95% probability compared to the old rule-based control. However, the framework still faces the practical challenges including building energy modeling of novel HVAC systems and multi-objective model calibration. Systematic study is also needed for the design of deep reinforcement learning training to provide a guideline for practitioners.}
}
@article{ALQAHTANI2022108180,
title = {Dynamic energy scheduling and routing of a large fleet of electric vehicles using multi-agent reinforcement learning},
journal = {Computers & Industrial Engineering},
volume = {169},
pages = {108180},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108180},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222002509},
author = {Mohammed Alqahtani and Michael J. Scott and Mengqi Hu},
keywords = {Electric vehicle, Vehicle routing, Energy scheduling, Multi-agent reinforcement learning, Deep reinforcement learning},
abstract = {As the world’s population and economy grow, demand for energy increases as well. Smart grids can be a cost-effective solution to overcome increases in energy demand and ensure power security. Current applications of smart grids involve a large numbers of agents (e.g., electric vehicles). Since each agent must interact with other agents when taking decisions (e.g., movement and scheduling), the computational complexity of smart grid systems increases exponentially with the number of agents. Computational tractability of planning is a significant barrier to implementation of large-scale smart grids of electric vehicles. Existing solution approaches such as mixed-integer programming and dynamic programming are not computationally efficient for high-dimensional problems. This paper proposes a reformulation of a Mixed-Integer Programming model into a Decentralized Markov Decision Process model and solves it using a Multi-Agent Reinforcement Learning algorithm to address the scalability issues of large-scale smart grid systems. The Decentralized Markov Decision Process model uses centralized training and distributed execution: agents are trained using a unique actor network for each agent and a shared critic network, and then agent execute actions independently from other agents to reduce computation time. The performance of the Multi-Agent Reinforcement Learning model is assessed under different configurations of customers and electric vehicles, and compared to the results from deep reinforcement learning and three heuristic algorithms. The simulation results demonstrate that the Multi-Agent Reinforcement Learning algorithm can reduce simulation time significantly compared to deep reinforcement learning, genetic algorithm, particle swarm optimization, and the artificial fish swarm algorithm. The superior performance of the proposed method indicates that it may be a realistic solution for large-scale implementation.}
}
@article{PERRUSQUIA2022364,
title = {Solution of the linear quadratic regulator problem of black box linear systems using reinforcement learning},
journal = {Information Sciences},
volume = {595},
pages = {364-377},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522002031},
author = {Adolfo Perrusquía},
keywords = {Linear quadratic regulator, State observer parametrization, Q-learning, Gradient descent, Output feedback, Persistency of excitation},
abstract = {In this paper, a Q-learning algorithm is proposed to solve the linear quadratic regulator problem of black box linear systems. The algorithm only has access to input and output measurements. A Luenberger observer parametrization is constructed using the control input and a new output obtained from a factorization of the utility function. An integral reinforcement learning approach is used to develop the Q-learning approximator structure. A gradient descent update rule is used to estimate on-line the parameters of the Q-function. Stability and convergence of the Q-learning algorithm under the Luenberger observer parametrization is assessed using Lyapunov stability theory. Simulation studies are carried out to verify the proposed approach.}
}
@article{FRIKHA202198,
title = {Reinforcement and deep reinforcement learning for wireless Internet of Things: A survey},
journal = {Computer Communications},
volume = {178},
pages = {98-113},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002681},
author = {Mohamed Said Frikha and Sonia Mettali Gammar and Abdelkader Lahmadi and Laurent Andrey},
keywords = {Internet of Things, Reinforcement learning, Deep reinforcement learning, Wireless Networks},
abstract = {Nowadays, many research studies and industrial investigations have allowed the integration of the Internet of Things (IoT) in current and future networking applications by deploying a diversity of wireless-enabled devices ranging from smartphones, wearables, to sensors, drones, and connected vehicles. The growing number of IoT devices, the increasing complexity of IoT systems, and the large volume of generated data have made the monitoring and management of these networks extremely difficult. Numerous research papers have applied Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) techniques to overcome these difficulties by building IoT systems with effective and dynamic decision-making mechanisms, dealing with incomplete information related to their environments. The paper first reviews pre-existing surveys covering the application of RL and DRL techniques in IoT communication technologies and networking. The paper then analyzes the research papers that apply these techniques in wireless IoT to resolve issues related to routing, scheduling, resource allocation, dynamic spectrum access, energy, mobility, and caching. Finally, a discussion of the proposed approaches and their limits is followed by the identification of open issues to establish grounds for future research directions proposal.}
}
@article{LIU20071331,
title = {RLDDE: A novel reinforcement learning-based dimension and delay estimator for neural networks in time series prediction},
journal = {Neurocomputing},
volume = {70},
number = {7},
pages = {1331-1341},
year = {2007},
note = {Advances in Computational Intelligence and Learning},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2006.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0925231206002591},
author = {F. Liu and G.S. Ng and C. Quek},
keywords = {Reinforcement learning-based dimension and delay estimator (RLDDE), Reinforcement learning, Neural networks, Input dimension, Time delay, Time series prediction},
abstract = {Time series prediction is traditionally handled by linear models such as autoregressive and moving-average. However they are unable to adequately deal with the non-linearity in the data. Neural networks are non-linear models that are suitable to handle the non-linearity in time series. When designing a neural network for prediction, two critical factors that affect the performance of the neural network predictor should be considered; they are namely: (1) the input dimension, and (2) the time delay. The former is the number of delayed values for prediction, while the latter is the time interval between two data. Prediction accuracy can be improved using suitable input dimension and time delay. A novel method, called reinforcement learning-based dimension and delay estimator (RLDDE), is proposed in this paper to simultaneously determine the input dimension and time delay. RLDDE is a meta-learner that tries to learn the selection policy of the dimension and delay under different distribution of the data. Two benchmarked datasets with different noise levels and one stock price are used to show the effectiveness of the proposed RLDDE together with the benchmarking against other methods.}
}
@article{HART2023114679,
title = {Vessel-following model for inland waterways based on deep reinforcement learning},
journal = {Ocean Engineering},
volume = {281},
pages = {114679},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.114679},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823010636},
author = {Fabian Hart and Ostap Okhrin and Martin Treiber},
keywords = {Reinforcement learning, Autonomous vessels, Vessel-following model, Vessel traffic flow, Inland waterway, Reducing waterway congestion},
abstract = {With the growth of traffic on inland waterways, autonomous driving technologies for vessels will gain increasing significance to ensure traffic flow and safety. Inspired by car-following models for road traffic, which demonstrated their strength to reduce stop-and-go waves and increase efficiency and safety, we propose a vessel-following model for inland waterways based on deep reinforcement learning (RL). Our model is trained under consideration of realistic vessel dynamics and environmental influences, such as varying stream velocity and river profile, and with a reward function favoring observed following behavior and comfort. Aiming at high generalization capabilities, we propose a training environment that uses stochastic processes to model leading the trajectory and river dynamics. Our model demonstrated safe and comfortable driving in different unseen scenarios, including realistic vessel-following on the Middle Rhine. In comparison with an existing model, our model was able to early anticipate safety–critical situations, resulting in higher safety while maintaining comparable efficiency and comfort. In further experiments, the proposed approach demonstrated its potential to dampen traffic oscillations and reduce congestion by using a sequence of followers.}
}
@article{FANG2021107539,
title = {Smart collaborative optimizations strategy for mobile edge computing based on deep reinforcement learning},
journal = {Computers & Electrical Engineering},
volume = {96},
pages = {107539},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107539},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621004845},
author = {Juan Fang and Mengyuan Zhang and Zhiyuan Ye and Jiamei Shi and Jianhua Wei},
keywords = {Optimization strategy, Computation offloading, Reinforcement learning, Mobile edge computing, Smart collaborative},
abstract = {With the arrival of the 5th generation mobile networks (5 G) era, the data needed by mobile devices (MDs) is explosively growing. High-consumption, low-latency applications are huge challenges for resource-constrained Internet of things (IoT) devices. Mobile edge computing overcomes the limitations of computing resources on MDs by offloading tasks generated by MDs and assigning them to nearby MEC servers. Therefore, mobile edge computing (MEC) becomes important. This paper presents a task offloading strategy for the multi-device multi-server system. To meet the task requirements of different MDs, we formulate an overhead minimization problem to optimize the delay and energy consumption of the system. We propose the Double Deep Q Network (Double-DQN) algorithm to perform location selection strategies for tasks generated on the mobile devices and allocate respective computing resources. Simulation results show that the algorithm can allocate resources reasonably and reduce the overhead of the entire system.}
}
@article{CUI2023115467,
title = {Flexible unmanned surface vehicles control using probabilistic model-based reinforcement learning with hierarchical Gaussian distribution},
journal = {Ocean Engineering},
volume = {285},
pages = {115467},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.115467},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823018516},
author = {Yunduan Cui and Kun Xu and Chunhua Zheng and Jia Liu and Lei Peng and Huiyun Li},
keywords = {Reinforcement learning, Gaussian process, Unmanned surface vehicles},
abstract = {This paper focuses on improving the flexibility of probabilistic model-based reinforcement learning (MBRL) in unmanned surface vehicles (USV) against complicated ocean disturbances. Filtered probabilistic model predictive control using hierarchical Gaussian distribution (FPMPC-HG) is proposed to provide a more flexible representation of environmental uncertainties and better control capability against real-time disturbances by integrating hierarchical Gaussian distribution into existing approaches specific to USVs. The proposed approach was evaluated through position-keeping and targets-tracking tasks in a real boat data-driven simulation. The experimental results demonstrated significant improvement in control performance, generalization capability, and task completion compared to the baseline approaches without employing hierarchical Gaussian distribution. The improved expressivity and flexibility of the probabilistic model and the corresponding policy of USVs against unknown ocean disturbances indicate the potential of hierarchical Gaussian distribution in probabilistic MBRL as a growing trend in the USV domain.}
}
@article{SOMESULA2022108876,
title = {Cooperative cache update using multi-agent recurrent deep reinforcement learning for mobile edge networks},
journal = {Computer Networks},
volume = {209},
pages = {108876},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.108876},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622000809},
author = {Manoj Kumar Somesula and Rashmi Ranjan Rout and D.V.L.N. Somayajulu},
keywords = {Mobile edge networks, Cooperative caching, Multi-agent deep reinforcement learning, Partially observable Markov decision process, Multi-agent deep deterministic policy gradient},
abstract = {Caching the most likely to be requested content at the base stations in a cooperative manner can facilitate direct content delivery without fetching content from the remote content server and thus alleviate the user-perceived latency, reduce the burden on backhaul and minimize the duplicated content transmissions. Content popularity plays a vital role, and it drives caching on edge. In the literature, earlier works considered the content popularity either known earlier or obtained on prediction. However, the content popularity is time-varying and unknown in reality, so the above assumption makes it less practical. Therefore, this paper considers the cooperative cache replacement problem in a realistic scenario where the edge nodes are unaware of the content popularity in mobile edge networks. To address this problem, the main contribution of this paper is to design an intelligent content update mechanism using multi-agent deep reinforcement learning in dynamic environments. With the goal of maximizing the saved delay with deadline and capacity constraints, we formulate the cache replacement problem as Integer linear programming problem. Considering the dynamic nature of the content popularity, high dimensional parameters, and for an intelligent caching decision, we model the problem as a partially observable Markov decision process and present an efficient deep reinforcement learning algorithm by embedding the long short-term memory network (LSTM) into a multi-agent deep deterministic policy gradient formalism. The LSTM inclusion reduces the instability produced by partial observability of the environment. Extensive simulation results demonstrate that the proposed cooperative caching mechanism significantly improves the performance in terms of reward, acceleration ratio and hit ratio compared with existing mechanisms.}
}
@article{BOHN20208090,
title = {Accelerating Reinforcement Learning with Suboptimal Guidance⁎⁎The first author is financed by "PhD Scholarships at SINTEF" from the Research Council of Norway (grant no. 272402). The third author was supported by the Research Council of Norway (grant no. 223254 NTNU AMOS).},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {8090-8096},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2278},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320329384},
author = {Eivind Bøhn and Signe Moe and Tor Arne Johansen},
keywords = {Deep Reinforcement Learning, Non-Linear Control Systems, Robotics},
abstract = {Reinforcement learning in domains with sparse rewards is a difficult problem, and a large part of the training process is often spent searching the state space in a more or less random fashion for learning signals. For control problems, we often have some controller readily available which might be suboptimal but nevertheless solves the problem to some degree. This controller can be used to guide the initial exploration phase of the learning controller towards reward yielding states, reducing the time before refinement of a viable policy can be initiated. To achieve such an exploration guidance while also allowing the learning controller to outperform the demonstrations provided to it, Nair et al. (2017) proposes to use a "Q-filter" to select states where the agent should clone the behaviour of the demonstrations. The Q-filter selects states where the critic deems the demonstrations to be superior to the agent, providing a natural way to adjust the guidance in a manner that is adaptive to the proficiency of the demonstrator. The contribution of this paper lies in adapting the Q-filter concept from pre-recorded demonstrations to an online guiding controller, and further in identifying shortcomings in the formulation of the Q-filter and suggesting some ways these issues can be mitigated — notably by replacing the value comparison baseline with the guiding controller’s own value function — reducing the effects of stochasticity in the neural network value estimator. These modifications are tested on the OpenAI Gym Fetch environments, showing clear improvements in adaptivity and yielding increased performance in all robotics environments tested.}
}
@article{KATHIRGAMANATHAN2021100101,
title = {Development of a Soft Actor Critic deep reinforcement learning approach for harnessing energy flexibility in a Large Office building},
journal = {Energy and AI},
volume = {5},
pages = {100101},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100101},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000537},
author = {Anjukan Kathirgamanathan and Eleni Mangina and Donal P. Finn},
keywords = {Deep Reinforcement Learning (DRL), Building energy flexibility, Soft Actor Critic (SAC), Machine learning, Smart grid},
abstract = {This research is concerned with the novel application and investigation of ‘Soft Actor Critic’ based deep reinforcement learning to control the cooling setpoint (and hence cooling loads) of a large commercial building to harness energy flexibility. The research is motivated by the challenge associated with the development and application of conventional model-based control approaches at scale to the wider building stock. Soft Actor Critic is a model-free deep reinforcement learning technique that is able to handle continuous action spaces and which has seen limited application to real-life or high-fidelity simulation implementations in the context of automated and intelligent control of building energy systems. Such control techniques are seen as one possible solution to supporting the operation of a smart, sustainable and future electrical grid. This research tests the suitability of the technique through training and deployment of the agent on an EnergyPlus based environment of the office building. The agent was found to learn an optimal control policy that was able to minimise energy costs by 9.7% compared to the default rule-based control scheme and was able to improve or maintain thermal comfort limits over a test period of one week. The algorithm was shown to be robust to the different hyperparameters and this optimal control policy was learnt through the use of a minimal state space consisting of readily available variables. The robustness of the algorithm was tested through investigation of the speed of learning and ability to deploy to different seasons and climates. It was found that the agent requires minimal training sample points and outperforms the baseline after three months of operation and also without disruption to thermal comfort during this period. The agent is transferable to other climates and seasons although further retraining or hyperparameter tuning is recommended.}
}
@article{ZHANG2022107885,
title = {Reinforcement learning for active distribution network planning based on Monte Carlo tree search},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {138},
pages = {107885},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107885},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521010991},
author = {Xi Zhang and Weiqi Hua and Youbo Liu and Jiajun Duan and Zhiyuan Tang and Junyong Liu},
keywords = {Active Distribution Network Planning, Convex Optimization, Monte Carlo tree search, Reinforcement Learning, Renewable Energy Source},
abstract = {Active distribution network planning is of importance for utility companies in terms of distributed generation investment, reliability assessment, optimal reactive power planning, substation evaluation, and feeder reconfiguration. However, it is challenging for current model-based optimization problems to guarantee the performances of active distribution network planning, due to an empirically pre-defined solution space. To overcome this issue, this paper proposes a performance-oriented method for the active distribution network planning. The solution space of the planning model is dynamically updated through using deep neural networks which are trained by the Monte Carlo tree search-based reinforcement learning until the desired performances are satisfied. Simulation results based on the standard IEEE 33-bus test system demonstrate that the proposed method can successfully improve the performances of the active distribution network planning to a desired level at a lower investment cost compared to other cases.}
}
@article{CHOI2020355,
title = {Bayesian networks + reinforcement learning: Controlling group emotion from sensory stimuli},
journal = {Neurocomputing},
volume = {391},
pages = {355-364},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.09.109},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219316194},
author = {Seul-Gi Choi and Sung-Bae Cho},
keywords = {Adjusting emotion, Group emotion, Bayesian networks, Reinforcement Learning, IoT},
abstract = {As communication technology develops, various sensory stimuli can be collected in service spaces. To enhance the service effectiveness, it is important to determine the optimal stimuli to induce group emotion in the service space to the target emotion. In this paper, we propose a stimuli control system to adjust the group emotion. It is a stand-alone system that can determine optimal stimuli by utility table and modular tree-structured Bayesian networks designed for emotion prediction model proposed in the previous study. To verify the proposed system, we collected data using several scenarios at a kindergarten and a senior welfare center. Each space is equipped with sensors for collection and equipment for controlling stimuli. As a result, the system shows a performance of 78% in the kindergarten and 80% in the senior welfare center. The proposed method shows much better performance than other classification methods with lower complexity. Also, reinforcement learning is applied to improving the accuracy of stimuli decision for a positive effect on system performance.}
}
@article{YAO2022564,
title = {Toward reliable designs of data-driven reinforcement learning tracking control for Euler–Lagrange systems},
journal = {Neural Networks},
volume = {153},
pages = {564-575},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022001915},
author = {Zhikai Yao and Jianyong Yao},
keywords = {Reinforcement learning, Tracking control, Direct heuristic dynamic programming (dHDP), Backstepping},
abstract = {This paper addresses reinforcement learning based, direct signal tracking control with an objective of developing mathematically suitable and practically useful design approaches. Specifically, we aim to provide reliable and easy to implement designs in order to reach reproducible neural network-based solutions. Our proposed new design takes advantage of two control design frameworks: a reinforcement learning based, data-driven approach to provide the needed adaptation and (sub)optimality, and a backstepping based approach to provide closed-loop system stability framework. We develop this work based on an established direct heuristic dynamic programming (dHDP) learning paradigm to perform online learning and adaptation and a backstepping design for a class of important nonlinear dynamics described as Euler–Lagrange systems. We provide a theoretical guarantee for the stability of the overall dynamic system, weight convergence of the approximating nonlinear neural networks, and the Bellman (sub)optimality of the resulted control policy. We use simulations to demonstrate significantly improved design performance of the proposed approach over the original dHDP.}
}
@article{XU2023106281,
title = {Reinforcement learning compensated coordination control of multiple mobile manipulators for tight cooperation},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106281},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106281},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623004657},
author = {Pengjie Xu and Yuanzhe Cui and Yichao Shen and Wei Zhu and Yiheng Zhang and Bingzheng Wang and Qirong Tang},
keywords = {Multiple mobile manipulators, Reinforcement learning, Coordinated controller, Tight cooperation},
abstract = {This study presents a coordinated control method based on reinforcement learning for multiple mobile manipulators when strong constraints and close coupling are involved in the tightly cooperative tasks. The reinforcement learning strategy is specifically designed to deal with the unknown vibrations between the mobile manipulators and the common object. Firstly, the problem is converted into a Markov decision process. Next, the grasping forces of the end-effectors are regarded as the parameters to be optimized, and the system states and learning framework are described based on advantage actor–critic algorithm. Thirdly, an agent is trained through interacting with the environment based on a proposed reward policy. To eliminate joint dynamic errors caused by trajectories tracking, an adaptive controller is designed for each mobile manipulator. For the simulations and experiments, two mobile manipulators are employed for transporting a common plate under various conditions. The results demonstrate that the proposed method has better control effects than well-known controllers. This study combines the advantages of both reinforcement learning and model-based method via a coordinated controller designed with the characteristics of tight cooperation.}
}
@article{ZHANG2024121578,
title = {An efficient reinforcement learning approach for goal-based wealth management},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121578},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121578},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423020808},
author = {Jinshan Zhang and Chengquan Wan and Ming Chen and Hengjiang Liu},
keywords = {Goal-based wealth management, Hybrid reinforcement learning, Hybrid policy, PPO},
abstract = {Goals-based wealth management (GBWM), an investment philosophy, is aiming to attain the desired goal or goals specified by an investor, in a long-term investment. A good algorithm for GBWM is able to provide decision-making and enhance the confidence of consumers and investors, under a complex economic situation. However, the existing methods suffer poor adaptability, oversimplified settings, and limited solution space. In this paper, we extend existing models to more realistic scenarios and propose a new hybrid RL-based algorithm, MHPPO to optimize complex discrete and continuous decisions simultaneously. Our algorithm outperforms existing methods in both simple and complicated settings. The prototype of the algorithm will be implemented on the platform of RoyalFlush company.}
}
@article{LI201881,
title = {Deep reinforcement learning: Algorithm, applications, and ultra-low-power implementation},
journal = {Nano Communication Networks},
volume = {16},
pages = {81-90},
year = {2018},
issn = {1878-7789},
doi = {https://doi.org/10.1016/j.nancom.2018.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S1878778917300856},
author = {Hongjia Li and Ruizhe Cai and Ning Liu and Xue Lin and Yanzhi Wang},
keywords = {Deep reinforcement learning, Algorithm, Stochastic computing, Ultra-low-power},
abstract = {In order to overcome the limitation of traditional reinforcement learning techniques on the restricted dimensionality of state and action spaces, the recent breakthroughs of deep reinforcement learning (DRL) in Alpha Go and playing Atari set a good example in handling large state and action spaces of complicated control problems. The DRL technique is comprised of an offline deep neural network (DNN) construction phase and an online deep Q-learning phase. In the offline phase, DNNs are utilized to derive the correlation between each state–action pair of the system and its value function. In the online phase, a deep Q-learning technique is adopted based on the offline-trained DNN to derive the optimal action and meanwhile update the value estimates and the DNN. This paper is the first to provide a comprehensive study of applications of the DRL framework on cloud computing and residential smart grid systems along with efficient hardware implementations. Based on the introduction of the general DRL framework, we develop two applications, one for the cloud computing resource allocation problem and one for the residential smart grid user-end task scheduling problem. The former could achieve up to 54.1% energy saving compared with baselines through automatically and dynamically distributing resources to servers. The latter achieves up to 22.77% total energy cost reduction compared with the baseline algorithm. The DRL framework is mainly utilized for the complicated control problems and requires light-weight and low-power implementations in edge and portable systems. In order to achieve this goal, we develop the ultra-low-power implementation of the DRL framework using the stochastic computing technique, which has the potential of significantly enhancing the computation speed and reducing hardware footprint and therefore the power/energy consumption. The overall implementation is based on the effective stochastic computing-based implementations of approximate parallel counter-based inner product blocks and tanh activation functions. The stochastic computing-based implementation achieves only 57941.61 μm2 area and 6.30 mW power with 412.47 ns delay.}
}
@article{GULLAPALLI1995271,
title = {Direct associative reinforcement learning methods for dynamic systems control},
journal = {Neurocomputing},
volume = {9},
number = {3},
pages = {271-292},
year = {1995},
note = {Control and Robotics, Part III},
issn = {0925-2312},
doi = {https://doi.org/10.1016/0925-2312(95)00035-X},
url = {https://www.sciencedirect.com/science/article/pii/092523129500035X},
author = {Vijaykumar Gullapalli},
keywords = {Reinforcement learning, Learning control, Direct methods, Peg-in-hole insertion, Inverse kinematics, Pole balancing},
abstract = {Most problems in learning to control dynamic systems involve learning under uncertainty, noise, and the lack of explicit instructional information about how to perform a task. Under these circumstances, techniques developed by artificial intelligence researchers for ‘learning from examples’, including the ‘supervised learning’ techniques studied by neural network researchers, are impractical because of the difficulty of obtaining training information (the ‘examples’) in the form of situation-action training pairs. A useful alternative in such situations is a learning technique that can discover appropriate actions in various situations through a search process that is guided by evaluative performance feedback. Reinforcement learning methods developed by neural network researchers are examples of such techniques. This paper focuses on direct reinforcement learning techniques and discusses their role in learning control by relating them to similar adaptive control methods. Several examples are also presented to illustrate the power and utility of direct reinforcement learning techniques for learning control.}
}
@article{ELBAZ2023105104,
title = {Deep reinforcement learning approach to optimize the driving performance of shield tunnelling machines},
journal = {Tunnelling and Underground Space Technology},
volume = {136},
pages = {105104},
year = {2023},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2023.105104},
url = {https://www.sciencedirect.com/science/article/pii/S0886779823001244},
author = {Khalid Elbaz and Annan Zhou and Shui-Long Shen},
keywords = {Deep reinforcement learning, Thrust force, Cutterhead torque, Tunnel case study, Deep Q-learning network, Extreme learning machine},
abstract = {This paper proposes a deep reinforcement learning (DRL)-based model as a valuable tool to improve the performance of the driving system (i.e. thrust force and cutterhead torque) of a shield tunnelling machine. The proposed model integrates deep-Q learning algorithm (DQL) and particle swarm optimization (PSO) based on an extreme learning machine (ELM). Specifically, the DQL–PSO model initialized the biases and weights in the ELM to achieve the optimal convergence rate and avoid instability. The DQL–PSO model evaluates the reward of action at each step and thus guides the particles to perform the appropriate action in real time. The DRL process data included shield operational parameters, geometry, and geological conditions. Field data collected from the Shenzhen railway tunnelling case study were used to validate the superiority and effectiveness of the presented DQL–PSO model. The algorithm was also evaluated using four numerical benchmark problems and compared with a theoretical model. The results revealed that the promising potential of DRL as a decision tool efficiently supports the formulation of target strategy and demonstrated its potential for engineering applications.}
}
@article{LIM2022133605,
title = {Reinforcement learning-based optimal operation of ash deposit removal system to improve recycling efficiency of biomass for CO2 reduction},
journal = {Journal of Cleaner Production},
volume = {370},
pages = {133605},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.133605},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622031845},
author = {Jonghun Lim and Hyungtae Cho and Hyukwon Kwon and Hyundo Park and Junghwan Kim},
keywords = {Recovery boiler, Ash deposits, Biomass, CO neutral, Reinforcement learning},
abstract = {Black liquor from pulp mills is valuable biomass that can be recycled as a CO2-neutral, renewable fuel. However, biomass combustion produces significant ash deposits reducing the overall process efficiency. A recovery boiler generally uses an ash deposit removal system (ADRS), but ADRS operation is inefficient, and the recycling efficiency of the biomass is decreased, leading to an increase in CO2 emission. This work proposed an optimal operation of ADRS to improve the recycling efficiency of biomass for CO2 emission reduction based on reinforcement learning. The optimal operation of the ADRS was derived by the following steps. 1) Real-time process operating data (i.e., temperatures of the flue gas, water, and steam) were gathered and a computational fluid dynamics model was developed to predict the flue gas temperature in the superheater section. 2) The decrease in the heat transfer rate was calculated using the gathered data to define a reward update matrix. 3) A modified Q-learning algorithm was developed based on the defined reward update matrix, and the algorithm was used to derive the Q-matrix, a function that predicted the expected dynamic reward (i.e., priority for ash deposit removal) of performing a given action (i.e., sootblowing) at a given state (i.e., each sootblowing location). 4) Using the obtained Q-matrix, the optimal operating sequence was derived. As a result, 22.58 ton/d of black liquor was saved and the CO2 emission decreased by 755–1390 ton/y with an increase in the net profit by $1,010,000.}
}
@article{ZHANG2023109670,
title = {Distributed synchronization based on model-free reinforcement learning in wireless ad hoc networks},
journal = {Computer Networks},
volume = {227},
pages = {109670},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109670},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623001159},
author = {Hang Zhang and Dongqi Yan and Yanxi Zhang and Jiamu Liu and Mingwu Yao},
keywords = {Distributed time synchronization, Dynamic ad hoc network, Reinforcement learning},
abstract = {Time synchronization is a key issue in wireless ad hoc networks. Due to the dynamic characteristics of such networks, distributed synchronization (DS) is preferred for its reliability and validity. However, one major drawback of this synchronization mechanism is that nodes exchange time synchronization messages with their neighbors, which can be very time-consuming. In order to reduce network synchronization overhead while maintaining synchronization quality, this paper presents a model-free reinforcement learning distributed synchronization (RLDS) by evaluating the current network state and node synchronization level, adaptively deciding that the current node interacts with a certain portion of its neighbors instead of all of them for synchronization information. The simulation results indicated that during the initial network synchronization, RLDS achieves the same synchronization accuracy as the traditional DS, while reducing the total communication overhead by 15%. The superiority of RLDS is more evident in the long-term maintenance of network synchronization, reducing the communication overhead by 48% during 500 rounds of synchronization. This is because the number of node neighbors in communication can be appropriately reduced, thus achieving an adaptive trade-off between ensuring time synchronization and saving communication overhead. This study shows the latent capacity of reinforcement learning in improving the performance of traditional ad hoc networking technologies.}
}
@article{LI2023,
title = {Energy sources durability energy management for fuel cell hybrid electric bus based on deep reinforcement learning considering future terrain information},
journal = {International Journal of Hydrogen Energy},
year = {2023},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2023.05.311},
url = {https://www.sciencedirect.com/science/article/pii/S036031992302726X},
author = {Kunang Li and Jiaming Zhou and Chunchun Jia and Fengyan Yi and Caizhi Zhang},
keywords = {Energy management strategy, Future terrain information, Deep reinforcement learning, Cyber-physical system, Fuel cell hybrid electric bus},
abstract = {Environmental working conditions have a great impact on the power demand and fuel consumption of vehicles, and the rational use of road terrain information plays an important role in the improvement of vehicle economy and energy sources durability. In this paper, a fuel cell hybrid electric bus (FCHEB) energy management strategy (EMS) based on deep deterministic policy gradient (DDPG) algorithm taking into account future terrain information is proposed based on cyber-physical system (CPS) to reduce the economic cost of FCHEB. First, this paper implements the information exchange between the vehicle system and the network layer through CPS to obtain the environmental working conditions of the vehicle operation. Second, future terrain information is introduced into the framework for the first time with hydrogen consumption, battery degradation and fuel cell durability constraints to rationally allocate the power of the FCHEB. The results show that the proposed strategy improves the power battery durability by 7.39% and reduces the total operating cost by 5.76% compared to the EMS that ignores the future terrain information.}
}
@article{BRAGA200621,
title = {Influence zones: A strategy to enhance reinforcement learning},
journal = {Neurocomputing},
volume = {70},
number = {1},
pages = {21-34},
year = {2006},
note = {Neural Networks},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2006.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0925231206001998},
author = {Arthur Plínio de S. Braga and Aluízio F.R. Araújo},
keywords = {Reinforcement learning, Self-organizing map, Instantaneous topological map, Learning acceleration},
abstract = {Reinforcement Learning (RL) aims to learn through direct experimentation how to solve decision-making problems. RL algorithms often have their practical applications restricted to small or medium size problems—mainly because of their strategies for value function estimation demanding very high number of interactions. To overcome this difficulty, we propose to enhance RL performance by updating several state (or state–action) values at each interaction. Therefore, the influence zone algorithm, an improvement over the topological RL agent (TRLA) strategy, allows to reduce the number of requested interactions. Such a reduction is based on the topological-preserving characteristic of the mapping between states (or state–action pairs) and value estimates. The comparison of the influence zone approach with seven other RL algorithms suggests that the proposed algorithm is among the fastest to estimate the value function and that it takes less value function updatings to perform such an estimation. The influence zone algorithm also presents a remarkable flexibility in adapting its policy to changes of the input space topology.}
}
@article{YANG2022109342,
title = {Dynamic power allocation in cellular network based on multi-agent double deep reinforcement learning},
journal = {Computer Networks},
volume = {217},
pages = {109342},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109342},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622003760},
author = {Yi Yang and Fenglei Li and Xinzhe Zhang and Zhixin Liu and Kit Yan Chan},
keywords = {Cellular network, Power allocation, Deep reinforcement learning, Multi-agent},
abstract = {With the massively growing wireless data traffic, the dense cellular network has become a significant mode for the fifth generation (5G) network. To fully utilize the benefit of the cellular network, it is primary to design optimal allocation strategy with limited network resource. In this paper, we investigate the dynamic power allocation problem in downlink cellular network based on multi-agent reinforcement learning (RL), where each base station (BS)-user (UE) is modeled as a RL agent to learn optimal power allocation policy in order to maximize the total system capacity. Due to the non-convex and large-scale characteristic of the optimization problem, the computational complexity of centralized traditional methods is unacceptable in practice. Therefore, the power allocation problem is transformed into a multi-agent RL (MARL) issue which can be solved by Deep Reinforcement Learning (DRL) method in a distributed way. We address the expandability of reward function and state space, in order to adapt the variation of network size, such as the number of BSs or UEs and the coverage area of cells. Moreover, the impacts of learning hyperparameters are evaluated for the algorithmic performance. Finally, the effectiveness and superiority of the proposed method are validated by numerical results in different scenarios.}
}
@article{JIAO2021103289,
title = {Real-world ride-hailing vehicle repositioning using deep reinforcement learning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {130},
pages = {103289},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103289},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21003004},
author = {Yan Jiao and Xiaocheng Tang and Zhiwei (Tony) Qin and Shuaiji Li and Fan Zhang and Hongtu Zhu and Jieping Ye},
keywords = {Ridesharing, Vehicle repositioning, Deep reinforcement learning},
abstract = {We present a new practical framework based on deep reinforcement learning and decision-time planning for real-world vehicle repositioning on ride-hailing (a type of mobility-on-demand, MoD) platforms. Our approach learns the spatiotemporal state-value function using a batch training algorithm with deep value networks. The optimal repositioning action is generated on-demand through value-based policy search, which combines planning and bootstrapping with the value networks. For the large-fleet problems, we develop several algorithmic features that we incorporate into our framework and that we demonstrate to induce coordination among the algorithmically-guided vehicles. We benchmark our algorithm with baselines in a ride-hailing simulation environment to demonstrate its superiority in improving income efficiency measured by income-per-hour. We have also designed and run a real-world experiment program with regular drivers on a major ride-hailing platform. We have observed significantly positive results on key metrics comparing our method with experienced drivers who performed idle-time repositioning based on their own expertise.}
}
@article{LI2022106375,
title = {An adaptive heuristic algorithm based on reinforcement learning for ship scheduling optimization problem},
journal = {Ocean & Coastal Management},
volume = {230},
pages = {106375},
year = {2022},
issn = {0964-5691},
doi = {https://doi.org/10.1016/j.ocecoaman.2022.106375},
url = {https://www.sciencedirect.com/science/article/pii/S0964569122003519},
author = {Runfo Li and Xinyu Zhang and Lingling Jiang and Zaili Yang and Wenqiang Guo},
keywords = {Q-learning, Adaptive genetic simulated annealing algorithm, Ship traffic scheduling, Maritime transportation},
abstract = {Due to the development of ship sizes and the traffic increase in port, ships having long turnaround time in port often result in port congestion, which seriously affects the efficiency of the ship navigation and environmental sustainability of port, it has been evident that effective ship scheduling presents a solution of the fundamental and strategic importance to port congestion. In this paper, a mixed-integer linear programming mathematical model is proposed to realize the optimization of the ship scheduling in port to minimize the total time spent by ships in port. Its methodological novelty is gained by an innovative adaptive genetic simulated annealing algorithm based on a reinforcement learning algorithm (GSAA-RL) to support the developed mathematical model, in which the genetic algorithm is considered as the basic optimization algorithm, and Q-learning with a unique property of selecting suitable parameters dynamically is developed to adjust the parameters of crossover and mutation to improve the search ability of the algorithm. Meanwhile, the dynamic parameter turning process is formulated into a Markov decision process (MDP) model with well defining the state, action, and reward function in GSAA-RL. Specifically, the state sets are proposed by analyzing the key factors affecting the scheduling efficiency and a new reward mechanism that can reduce the objective value significantly based on the quality of selected parameters is designed. The annealing operation is performed on some excellent individuals to further expand the search scope. Simulation experiments demonstrate that the proposed GSAA-RL algorithm can significantly shorten the total time spent by ships in port compared to existing approaches. This study hence helps port operators/planners to improve operational efficiency and reduce port congestion, reduce ship fuel consumption, and deliver goods to cargo owners in a timely manner, which has important practical significance for achieving the “dual carbon” goal.}
}
@article{AN2021107978,
title = {A reinforcement learning approach for control of window behavior to reduce indoor PM2.5 concentrations in naturally ventilated buildings},
journal = {Building and Environment},
volume = {200},
pages = {107978},
year = {2021},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.107978},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321003826},
author = {Yuting An and Tongling Xia and Ruoyu You and Dayi Lai and Junjie Liu and Chun Chen},
keywords = {Reinforcement learning, Smart control, PM, Natural ventilation, Artificial intelligence and internet of things (AIoT)},
abstract = {Smart control of window behavior is a means of effectively reducing concentrations of indoor PM2.5 (particulate matter with aerodynamic diameter less than 2.5 μm) in naturally ventilated residential buildings without indoor air cleaning devices. This study aimed to develop a reinforcement learning approach to automatically control window behavior in real time for mitigation of indoor PM2.5 pollution. The proposed method trains the window controller with the use of a deep Q-network (DQN) in a specific naturally ventilated apartment in the course of a month. The trained controller can then be employed to control window behavior in order to reduce the indoor PM2.5 concentrations in that apartment. The required input data for the controller are the real-time indoor and outdoor PM2.5 concentrations with a 1-min resolution, which can easily be obtained with low-cost sensors available on the market. A series of simulations were conducted in a virtual typical apartment in Beijing and a real apartment in Tianjin. The results show that, compared with the baseline I/O ratio algorithm, the proposed reinforcement learning window-control algorithm reduced the average indoor PM2.5 concentration by 12.80% in a one-year period. Furthermore, the proposed algorithm reduced the indoor PM2.5 concentrations in the real apartment by 9.11% when compared with the I/O ratio algorithm and by 7.40% when compared with real window behavior.}
}
@article{XU2022110749,
title = {COLREGs-abiding hybrid collision avoidance algorithm based on deep reinforcement learning for USVs},
journal = {Ocean Engineering},
volume = {247},
pages = {110749},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.110749},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822001998},
author = {Xinli Xu and Yu Lu and Gang Liu and Peng Cai and Weidong Zhang},
keywords = {Unmanned surface vehicles, Hybrid collision avoidance, Deep reinforcement learning, COLREGs, Priority sampling mechanism with cumulative pruning},
abstract = {In order to realize the autonomous collision avoidance of unmanned surface vehicles (USVs), an intelligent hybrid collision avoidance algorithm based on deep reinforcement learning is proposed. First, the navigation situation model of the USV is designed, the geometric model of the encounter between two ships is established based on navigation practice. According to static and dynamic obstacles, hybrid risk assessment and collision avoidance model are proposed, the risk factor is calculated. Then, for static obstacles, the collision cone is introduced, for dynamic ships, the COLREGS is observed, the encounter situation is quantified into five types. Collision avoidance strategy is formulated. Finally, the state, action, reward function and network structure are designed. Aiming at the problem of low utilization for samples in random sampling, this paper improves the original sampling mechanism of DDPG, and the priority sampling mechanism with cumulative pruning is proposed in this paper. Simulation experiments are carried out in several typical encounter scenarios. The results show that this algorithm can accurately judge the encounter situation, give reasonable collision avoidance actions, and realize effective collision avoidance in a complex environment with dynamic and static obstacles. The research can provide theoretical basis and method reference for autonomous navigation of USVs.}
}
@article{ZHANG2023110083,
title = {DeepMAG: Deep reinforcement learning with multi-agent graphs for flexible job shop scheduling},
journal = {Knowledge-Based Systems},
volume = {259},
pages = {110083},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110083},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122011790},
author = {Jia-Dong Zhang and Zhixiang He and Wing-Ho Chan and Chi-Yin Chow},
keywords = {Deep learning, Reinforcement learning, Multi-agent graphs, Deep Q networks, Flexible job shop scheduling},
abstract = {The flexible job shop scheduling (FJSS) is important in real-world factories due to the wide applicability. FJSS schedules the operations of jobs to be executed by specific machines at the appropriate time slots based on two decision steps, namely, the job sequencing (i.e., the sequence of jobs executed on a machine) and the job routing (i.e., the route of a job to a machine). Most current studies utilize either deep reinforcement learning (DRL) or multi-agent reinforcement learning (MARL) for FJSS with a large search space. However, these studies suffer from two major limitations: no integration between DRL and MARL, and independent agents without cooperation. To this end, we propose a new model for FJSS, called DeepMAG based on Deep reinforcement learning with Multi-Agent Graphs. DeepMAG has two key contributions. (1) Integration between DRL and MARL. DeepMAG integrates DRL with MARL by associating a different agent to each machine and job. Each agent exploits DRL to find the best action on the job sequencing and routing. After a job-associated agent chooses the best machine, the job becomes a job candidate for the machine to proceed to its next operation, while a machine-associated agent selects the next job from its job candidate set to be processed. (2) Cooperative agents. A multi-agent graph is built based on the operation relationships among machines and jobs. An agent cooperates with its neighboring agents to take one cooperative action. Finally, we conduct experiments to evaluate the performance of DeepMAG and experimental results show that it outperforms the state-of-the-art techniques.}
}
@article{YANG2020145,
title = {Reinforcement learning in sustainable energy and electric systems: a survey},
journal = {Annual Reviews in Control},
volume = {49},
pages = {145-163},
year = {2020},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2020.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1367578820300079},
author = {Ting Yang and Liyuan Zhao and Wei Li and Albert Y. Zomaya},
keywords = {Reinforcement learning, Sustainable energy and electric systems, Deep reinforcement learning, Power system, Integrated energy system},
abstract = {The dynamic nature of sustainable energy and electric systems can vary significantly along with the environment and load change, and they represent the features of multivariate, high complexity and uncertainty of the nonlinear system. Moreover, the integration of intermittent renewable energy sources and energy consumption behaviours of households introduce more uncertainty into sustainable energy and electric systems. The operation, control and decision-making in such an environment definitely require increasing intelligence and flexibility in the control and optimization to ensure the quality of service of sustainable energy and electric systems. Reinforcement learning is a wide class of optimal control strategies that uses estimating value functions from experience, simulation, or search to learn in highly dynamic, stochastic environment. The interactive context enables reinforcement learning to develop strong learning ability and high adaptability. Reinforcement learning does not require the use of the model of system dynamics, which makes it suitable for sustainable energy and electric systems with complex nonlinearity and uncertainty. The use of reinforcement learning in sustainable energy and electric systems will certainly change the traditional energy utilization mode and bring more intelligence into the system. In this survey, an overview of reinforcement learning, the demand for reinforcement learning in sustainable energy and electric systems, reinforcement learning applications in sustainable energy and electric systems, and future challenges and opportunities will be explicitly addressed.}
}
@article{SEO2021623,
title = {A reinforcement learning approach to distribution-free capacity allocation for sea cargo revenue management},
journal = {Information Sciences},
volume = {571},
pages = {623-648},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.04.092},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521004333},
author = {Dong-Wook Seo and Kyuchang Chang and Taesu Cheong and Jun-Geol Baek},
keywords = {Revenue management, Stochastic dynamic programming, Reinforcement learning, Liner shipping},
abstract = {In this paper, we propose learning-based adaptive control based on reinforcement learning for the booking policy in sea cargo revenue management. The problem setting is that the demand distribution is unknown while the historical data is available, and the problem is formulated as a stochastic dynamic programming model. We demonstrate the existence of an optimal control limit policy and investigate the important properties and optimal policy structures of the model. We then propose a reinforcement learning approach for the data-driven approximation of the optimal booking policy to maximize shipping line revenue. The performance of the proposed approach is very close to that of the optimal policy and superior to that of the EMSR-b algorithm.}
}
@article{BIEMANN2021117164,
title = {Experimental evaluation of model-free reinforcement learning algorithms for continuous HVAC control},
journal = {Applied Energy},
volume = {298},
pages = {117164},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117164},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921005961},
author = {Marco Biemann and Fabian Scheller and Xiufeng Liu and Lizhen Huang},
keywords = {Reinforcement learning, Continuous HVAC control, Actor-critic algorithms, Robustness, Energy efficiency, Soft Actor Critic},
abstract = {Controlling heating, ventilation and air-conditioning (HVAC) systems is crucial to improving demand-side energy efficiency. At the same time, the thermodynamics of buildings and uncertainties regarding human activities make effective management challenging. While the concept of model-free reinforcement learning demonstrates various advantages over existing strategies, the literature relies heavily on value-based methods that can hardly handle complex HVAC systems. This paper conducts experiments to evaluate four actor-critic algorithms in a simulated data centre. The performance evaluation is based on their ability to maintain thermal stability while increasing energy efficiency and on their adaptability to weather dynamics. Because of the enormous significance of practical use, special attention is paid to data efficiency. Compared to the model-based controller implemented into EnergyPlus, all applied algorithms can reduce energy consumption by at least 10% by simultaneously keeping the hourly average temperature in the desired range. Robustness tests in terms of different reward functions and weather conditions verify these results. With increasing training, we also see a smaller trade-off between thermal stability and energy reduction. Thus, the Soft Actor Critic algorithm achieves a stable performance with ten times less data than on-policy methods. In this regard, we recommend using this algorithm in future experiments, due to both its interesting theoretical properties and its practical results.}
}
@article{LI2019234,
title = {Differential evolution based on reinforcement learning with fitness ranking for solving multimodal multiobjective problems},
journal = {Swarm and Evolutionary Computation},
volume = {49},
pages = {234-244},
year = {2019},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2019.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S2210650218310575},
author = {Zhihui Li and Li Shi and Caitong Yue and Zhigang Shang and Boyang Qu},
keywords = {Multimodal multiobjective optimization problem, Differential evolution, Reinforcement learning, Fitness ranking, Q-learning},
abstract = {In multimodal multiobjective optimization problems (MMOOPs), there is more than one Pareto-optimal Set (PS) in the decision space corresponding to the same Pareto Front(PF). How to dynamically adjust the evolution direction of the population adaptively is a key problem, to ensure approaching the PF in the global sense with good convergence while finding out more PSs. In this paper, a novel Differential Evolution algorithm based on Reinforcement Learning with Fitness Ranking (DE-RLFR) is proposed. The DE-RLFR is based on the Q-learning framework, and each individual in the population is considered an agent. The fitness ranking values of each agent are used to encode hierarchical state variables. Three typical DE mutation operations are employed as optional actions for the agent. Based on the analysis of the distribution characteristics of the population in objective space, decision space and fitness-ranking space, we design a reward function of the 〉state, action〈 pairs to guide the population to move to the PF asymptotically. According to its reinforcement learning experience represented by the corresponding Q table value, each agent could adaptively select a mutation strategy to generate offspring individuals. The evaluation results on eleven MMOOP test functions show that DE-RLFR could quickly and effectively find multiple PSs in the decision space, and approach PF in the global sense.}
}
@article{KHODAMIPOUR202363,
title = {Adaptive formation control of leader–follower mobile robots using reinforcement learning and the Fourier series expansion},
journal = {ISA Transactions},
volume = {138},
pages = {63-73},
year = {2023},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2023.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0019057823001179},
author = {Gholamreza Khodamipour and Saeed Khorashadizadeh and Mohsen Farshad},
keywords = {Formation control, Reinforcement learning, Actor–critic strategy, The Fourier series expansion, Leader–follower mobile robots},
abstract = {In this paper, a formation controller for leader–follower mobile robots is presented based on reinforcement learning and the Fourier series expansion. The controller is designed based on the dynamical model in which permanent magnet direct-current (DC) motors are included as actuator. Thus, motor voltages are the control signals and are designed based on the actor–critic strategy which is a well-known approach in the field of reinforcement learning. Stability analysis of formation control of leader–follower mobile robots using the proposed controller verifies that the closed-loop system is globally asymptotically stable. Due to the existence of sinusoidal terms in the model of mobile robots, the Fourier series expansion has been selected to construct the actor and critic, while previous related works utilized neural networks in actor and critic. In comparison with neural networks, the Fourier series expansion is simpler and involves the designer in fewer tuning parameters. In simulation studies, it has been assumed that some follower robots can play the role of leader for the other follower robots behind it. Simulation results show that there is no need to use large number of the sinusoidal terms in the Fourier series expansion and just the first three terms can overcome uncertainties. In addition, the proposed controller reduced the performance index of tracking errors considerably in comparison with radial basis function neural networks (RBFNN).}
}
@article{GAI201812,
title = {Optimal resource allocation using reinforcement learning for IoT content-centric services},
journal = {Applied Soft Computing},
volume = {70},
pages = {12-21},
year = {2018},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.03.056},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618302540},
author = {Keke Gai and Meikang Qiu},
keywords = {Reinforcement learning, Resource allocation, Content-centric, Internet-of-Things, Smart computing},
abstract = {The exponential growing rate of the networking technologies has led to a dramatical large scope of the connected computing environment. Internet-of-Things (IoT) is considered an alternative for obtaining high performance by the enhanced capabilities in system controls, resource allocations, data exchanges, and flexible adoptions. However, current IoT is encountering the bottleneck of the resource allocation due to the mismatching networking service quality and complicated service offering environments. This paper concentrates on the issue of resource allocations in IoT and utilizes the satisfactory level of Quality of Experience (QoE) to achieve intelligent content-centric services. A novel approach is proposed by this work, which utilizes the mechanism of Reinforcement Learning (RL) to obtain high accurate QoE in resource allocations. Two RL-based algorithms have been proposed for cost mapping tables creations and optimal resource allocations. Our experiment evaluations have assessed the efficiency of implementing the proposed approach.}
}
@incollection{BADGWELL201871,
title = {Reinforcement Learning – Overview of Recent Progress and Implications for Process Control},
editor = {Mario R. Eden and Marianthi G. Ierapetritou and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {44},
pages = {71-85},
year = {2018},
booktitle = {13th International Symposium on Process Systems Engineering (PSE 2018)},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64241-7.50008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780444642417500082},
author = {Thomas A. Badgwell and Jay H. Lee and Kuang-Hung Liu},
keywords = {Reinforcement Learning, Model Predictive Control, Process Control},
abstract = {This paper provides a brief introduction to Reinforcement Learning (RL) technology, summarizes recent developments in this area, and discusses their potential implications for the field of process control. The paper begins with a brief introduction to RL, a machine learning technology that allows an agent to learn, through trial and error, the best way to accomplish a task. We then highlight two new developments in RL that have led to the recent wave of applications and media interest. A comparison of the key features of RL and Model Predictive Control (MPC) is then presented in order to clarify their similarities and differences. This is followed by an assessment of five ways that RL technology can potentially be used in process control applications. A final section summarizes our conclusions and lists directions for future RL research that may improve its relevance for process control applications.}
}
@article{SHAKYA2023120495,
title = {Reinforcement learning algorithms: A brief survey},
journal = {Expert Systems with Applications},
volume = {231},
pages = {120495},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120495},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423009971},
author = {Ashish Kumar Shakya and Gopinatha Pillai and Sohom Chakrabarty},
keywords = {Reinforcement learning, Stochastic optimal control, Function approximation, Deep Reinforcement Learning (DRL)},
abstract = {Reinforcement Learning (RL) is a machine learning (ML) technique to learn sequential decision-making in complex problems. RL is inspired by trial-and-error based human/animal learning. It can learn an optimal policy autonomously with knowledge obtained by continuous interaction with a stochastic dynamical environment. Problems considered virtually impossible to solve, such as learning to play video games just from pixel information, are now successfully solved using deep reinforcement learning. Without human intervention, RL agents can surpass human performance in challenging tasks. This review gives a broad overview of RL, covering its fundamental principles, essential methods, and illustrative applications. The authors aim to develop an initial reference point for researchers commencing their research work in RL. In this review, the authors cover some fundamental model-free RL algorithms and pathbreaking function approximation-based deep RL (DRL) algorithms for complex uncertain tasks with continuous action and state spaces, making RL useful in various interdisciplinary fields. This article also provides a brief review of model-based and multi-agent RL approaches. Finally, some promising research directions for RL are briefly presented.}
}
@article{NWEYE2022100202,
title = {Real-world challenges for multi-agent reinforcement learning in grid-interactive buildings},
journal = {Energy and AI},
volume = {10},
pages = {100202},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2022.100202},
url = {https://www.sciencedirect.com/science/article/pii/S2666546822000489},
author = {Kingsley Nweye and Bo Liu and Peter Stone and Zoltan Nagy},
keywords = {Grid-interactive buildings, Benchmarking, Reinforcement learning},
abstract = {Building upon prior research that highlighted the need for standardizing environments for building control research, and inspired by recently introduced challenges for real life reinforcement learning (RL) control, here we propose a non-exhaustive set of nine real world challenges for RL control in grid-interactive buildings (GIBs). We argue that research in this area should be expressed in this framework in addition to providing a standardized environment for repeatability. Advanced controllers such as model predictive control (MPC) and RL control have both advantages and disadvantages that prevent them from being implemented in real world problems. Comparisons between the two are rare, and often biased. By focusing on the challenges, we can investigate the performance of the controllers under a variety of situations and generate a fair comparison. As a demonstration, we implement the offline learning challenge in CityLearn, an OpenAI Gym environment for the easy implementation of RL agents in a demand response setting to reshape the aggregated curve of electricity demand by controlling the energy storage of a diverse set of buildings in a district. We use CityLearn to study the impact of different levels of domain knowledge and complexity of RL algorithms and show that the sequence of operations (SOOs) utilized in a rule based controller (RBC) that provides fixed logs to RL agents during offline training affect the performance of the agents when evaluated on a set of four energy flexibility metrics. Longer offline training from an optimized RBC leads to improved performance in the long run. RL agents that train on the logs from a simplified RBC risk poorer performance as the offline training period increases. We also observe no impact on performance from information sharing amongst agents. We call for a more interdisciplinary effort of the research community to address the real world challenges, and unlock the potential of GIB controllers.}
}
@article{QIU2022108362,
title = {A deep reinforcement learning-based approach for the home delivery and installation routing problem},
journal = {International Journal of Production Economics},
volume = {244},
pages = {108362},
year = {2022},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2021.108362},
url = {https://www.sciencedirect.com/science/article/pii/S0925527321003388},
author = {Huaxin Qiu and Sutong Wang and Yunqiang Yin and Dujuan Wang and Yanzhang Wang},
keywords = {Delivery and installation routing problem, Deep reinforcement learning, Encoder-decoder model, Attention mechanism},
abstract = {This paper investigates a home delivery and installation routing problem with synchronization constraints stemming from a home industry company in China who provides the last-mile delivery of home decoration and furniture. The company first arranges for products to be delivered from door to door, and later the technicians come to perform the installation service for the customers. The products for each customer must be firstly delivered to the customer by a vehicle and then installed by technicians. The objective is to identify the optimal delivery routes of the vehicles and optimal service routes of the technicians so as to minimize the total travel distance of the delivery and service routes. A deep reinforcement learning method in an Encoder-Decoder fashion with multi-head attention mechanism and beam search strategy is developed to solve the problem. To evaluate the designed method, extensive numerical experiments based on real service networks provided by the company are conducted. The results show that the proposed method can effectively solve the problem, which outperforms some classical strategies, and some meaningful management implications are provided.}
}
@article{XUE2020144,
title = {Integral reinforcement learning based event-triggered control with input saturation},
journal = {Neural Networks},
volume = {131},
pages = {144-153},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020302574},
author = {Shan Xue and Biao Luo and Derong Liu},
keywords = {Adaptive dynamic programming, Integral reinforcement learning, Neural networks, Event-triggered control, Input saturation},
abstract = {In this paper, a novel integral reinforcement learning (IRL)-based event-triggered adaptive dynamic programming scheme is developed for input-saturated continuous-time nonlinear systems. By using the IRL technique, the learning system does not require the knowledge of the drift dynamics. Then, a single critic neural network is designed to approximate the unknown value function and its learning is not subjected to the requirement of an initial admissible control. In order to reduce computational and communication costs, the event-triggered control law is designed. The triggering threshold is given to guarantee the asymptotic stability of the control system. Two examples are employed in the simulation studies, and the results verify the effectiveness of the developed IRL-based event-triggered control method.}
}
@article{PINCIROLI2023121947,
title = {Optimal operation and maintenance of energy storage systems in grid-connected microgrids by deep reinforcement learning},
journal = {Applied Energy},
volume = {352},
pages = {121947},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121947},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923013119},
author = {Luca Pinciroli and Piero Baraldi and Michele Compare and Enrico Zio},
keywords = {Microgrid, Energy storage systems, Operation and maintenance, Optimization, Deep reinforcement learning},
abstract = {The operation of microgrids, i.e., energy systems composed of distributed energy generation, local loads and energy storage capacity, is challenged by the variability of intermittent energy sources and demands, the stochastic occurrence of unexpected outages of the conventional grid and the degradation of the Energy Storage System (ESS), which is strongly influenced by its operating conditions. To effectively address these challenges, a novel method for combined operation and maintenance management of ESS has been developed. Unlike the currently available solutions, which typically address the one-day-ahead scheduling problem, the present work considers, for the first time, the realistic case of a microgrid in which the ESS degrades and unexpected outages of the conventional grid can occur along the long-time horizons of the entire microgrid lifetimes. The proposed method, which is based on deep reinforcement learning, is tested on a simulated grid-connected microgrid of a residential building equipped with photovoltaic modules and an ESS. The method outperforms other state-of-the-art approaches based on heuristics and metaheuristics by increasing the profit by 15% and reducing the average number of ESS replacements during its lifetime. Therefore, it can be concluded that the proposed DRL-based framework allows achieving prescriptive maintenance since the suggested actions are optimal from the point of view of effectively maximizing the profit and minimizing the maintenance interventions over the entire lifetime of the microgrid.}
}
@article{CHEN2022111361,
title = {Reinforcement learning-based close formation control for underactuated surface vehicle with prescribed performance and time-varying state constraints},
journal = {Ocean Engineering},
volume = {256},
pages = {111361},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.111361},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822007508},
author = {Huizi Chen and Huaicheng Yan and Yueying Wang and Shaorong Xie and Dan Zhang},
keywords = {Close formation control, Finite-time sliding mode control, Reinforcement learning, Actor-critic neural network, Prescribed formation performance, Time-varying state constraints},
abstract = {This paper studies close formation control problem with prescribed performance and time-varying state constraints for a group of 4-degrees-of-freedom (DOF) underactuated surface vehicles (USVs) subject to actuator faults, input saturation and input delay. A finite-time sliding mode control (SMC) scheme based on reinforcement learning (RL) algorithm is introduced to guarantee prescribed formation performance without violating velocity error constraints. By using actor-critic neural network (NN)-based RL algorithm, the actuator faults and system uncertainties are accurately estimated. Afterwards, an exponential decreasing boundary function is developed to suppress overshoot more reasonably, and a novel mechanism of switching gain is given to alleviate chattering inherent in SMC while the RL-based compensation term is constructed to handle the formation accuracy problem caused by the reduced switching gain. Besides, auxiliary nonlinear continuous function and Pade approximation have been successfully applied to process actuator saturation and input delay, respectively. Numerical simulations and experimental results are exhibited to verify the effectiveness and superior formation performance of the proposed control method.}
}
@article{LI2022104276,
title = {Excellent treatment activity of biscoumarins and dihydropyrans against P. aeruginosa pneumonia and reinforcement learning for designing novel inhibitors},
journal = {Arabian Journal of Chemistry},
volume = {15},
number = {11},
pages = {104276},
year = {2022},
issn = {1878-5352},
doi = {https://doi.org/10.1016/j.arabjc.2022.104276},
url = {https://www.sciencedirect.com/science/article/pii/S1878535222005925},
author = {Jing Li and Jiangtao Li and Hongjiang Ren and Yingwei Qu and Huiqing Shi and Yan Wu and Zichen Ye and Di Qu},
keywords = {Biscoumarins, Dihydropyrans, , Reinforcement learning},
abstract = {Pseudomonas aeruginosa (P. aeruginosa) is a common clinical pathogen, which can easily cause cystic fibrosis and even bacteremia. In recent years, the antibiotic resistance of P. aeruginosa has been increasing. In an attempt to develop novel antibacterial agents, a series of biscoumarins (1–5) and dihydropyrans (6–10) were successfully prepared. The molecular structures of two representative compounds, that is, 1 and 6 were confirmed by single crystal X-ray diffraction study. The anti-bacterial activity of these synthesized compounds in vitro was evaluated by measuring the MIC values, as well as the P. aeruginosa growth curves. Next, the in vivo treatment activity of these compounds against the P. aeruginosa pneumonia infection was assessed by observing the survival rate of the infected mice and counting the bacterial load with colony plate counting assay. Additionally, the ELISA detection was conducted to evaluate the inflammatory response levels by measuring the IL-1β and TNF-α content released into the plasma, nasal lavage fluid and alveolar lavage fluid. The HE staining was also carried out to detect the protective effect of the compounds on the lung tissue damage. Further, novel anti-bacterial structures that are based on biscoumarin 5 are predicted and evaluated using reinforcement learning technic, and two outstanding pharmaceutical structures with low binding energy and high SA and QED scores are analyzed in detail using molecular docking simulation.}
}
@article{MAZARE2023,
title = {Reinforcement Learning-Based Fixed-Time Resilient Control of Nonlinear Cyber Physical Systems Under False Data Injection Attacks and Mismatch Disturbances},
journal = {Journal of the Franklin Institute},
year = {2023},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2023.10.026},
url = {https://www.sciencedirect.com/science/article/pii/S0016003223006907},
author = {Mahmood Mazare},
keywords = {Nonlinear cyber physical system, Security, Mismatch disturbance observer, Fixed-time resilient control, Reinforcement learning},
abstract = {This paper presents a fixed-time adaptive resilient control framework based on reinforcement learning (RL), mismatch disturbance observer and nonsingular fast terminal sliding mode scheme for nonlinear cyber-physical systems (CPS) to enhance cyber-security under False Data Injection (FDI) attacks, as well as match and mismatch disturbances. The RL algorithm contains an actor-critic neural network (NN) in which the actor NN is employed to estimate the uncertainty while the critic NN is applied to evaluate the performance cost function. Next, to compensate the detrimental effects of attacks and mismatch disturbances, a new sliding mode-based adaptive disturbance observer is designed. To achieve high precision trajectory tracking within a fixed-time interval, a nonsingular fast terminal sliding mode scheme is designed. This scheme ensures fixed-time convergence of the tracking error and facilitates disturbance attenuation and attack mitigation which are the key features of the proposed fixed-time secure control strategy. Finally, the fixed-time stability analysis of is performed through Lyapunov theory. Results reveal the effectiveness of the proposed resilient control for wind turbine system as the nonlinear CPS.}
}
@article{DENG2022108680,
title = {Towards optimal HVAC control in non-stationary building environments combining active change detection and deep reinforcement learning},
journal = {Building and Environment},
volume = {211},
pages = {108680},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.108680},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321010702},
author = {Xiangtian Deng and Yi Zhang and Yi Zhang and He Qi},
keywords = {Heating, Ventilation and air conditioning (HVAC), Non-stationary environments, Deep reinforcement learning (DRL), Change point detection},
abstract = {Energy consumption for heating, ventilation and air conditioning (HVAC) has increased significantly and accounted for a large proportion of building energy growth. Advanced control strategies are needed to reduce energy consumption with maintaining occupant thermal comfort. While compared to other control problems, HVAC control is faced with numerous restrictions from real building environments. One key restriction is non-stationarity, i.e., the varying HVAC system dynamics. Researchers have paid efforts to solve the non-stationarity problems through different approaches, among which deep reinforcement learning gains traction for its advantages in capturing real-time information, controlling adaptively to system feedbacks, avoiding tedious modeling works and combining with deep learning techniques. However, current researches solved non-stationarity in a passive manner which hinders its potential and adds instability in real application. To fill this research gap, we propose a novel HVAC control method combining active building environment change detection and deep Q network (DQN), named non-stationary DQN. This method aims to disentangle the non-stationarity by actively identifying the change points of building environments and learning effective control strategies for corresponding building environments. The simulation results demonstrate that this developed non-stationary DQN method outperforms the state-of-art DQN method in both single-zone control and multi-zone control tasks by saving unnecessary energy use and reducing thermal violation caused by non-stationarity. The improvement can reach 13% in energy-saving and 9% in thermal comfort. Besides, according to the results, our proposed method obtains stability against disturbance and generalization to an unseen building environment, which shows its robustness and potential in real-life applications.}
}
@article{BRANDI2022104128,
title = {Comparison of online and offline deep reinforcement learning with model predictive control for thermal energy management},
journal = {Automation in Construction},
volume = {135},
pages = {104128},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104128},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522000012},
author = {Silvio Brandi and Massimo Fiorentini and Alfonso Capozzoli},
keywords = {Deep reinforcement learning, Model predictive control, HVAC control, Building energy consumption, Energy savings, Building energy management},
abstract = {This paper proposes a comparison between an online and offline Deep Reinforcement Learning (DRL) formulation with a Model Predictive Control (MPC) architecture for energy management of a cold-water buffer tank linking an office building and a chiller subject to time-varying energy prices, with the objective of minimizing operating costs. The intrinsic model-free approach of DRL is generally lost in common implementations for energy management, as they are usually pre-trained offline and require a surrogate model for this purpose. Simulation results showed that the online-trained DRL agent, while requiring an initial 4 weeks adjustment period achieving a relatively poor performance (160% higher cost), it converged to a control policy almost as effective as the model-based strategies (3.6% higher cost in the last month). This suggests that the DRL agent trained online may represent a promising solution to overcome the barrier represented by the modelling requirements of MPC and offline-trained DRL approaches.}
}
@article{JIANG2024100298,
title = {Deep-reinforcement-learning-based water diversion strategy},
journal = {Environmental Science and Ecotechnology},
volume = {17},
pages = {100298},
year = {2024},
issn = {2666-4984},
doi = {https://doi.org/10.1016/j.ese.2023.100298},
url = {https://www.sciencedirect.com/science/article/pii/S2666498423000637},
author = {Qingsong Jiang and Jincheng Li and Yanxin Sun and Jilin Huang and Rui Zou and Wenjing Ma and Huaicheng Guo and Zhiyun Wang and Yong Liu},
keywords = {Dynamic water diversion optimization, Deep reinforcement learning, Process-based model, Explainable decision-making, Parameter uncertainty},
abstract = {Water diversion is a common strategy to enhance water quality in eutrophic lakes by increasing available water resources and accelerating nutrient circulation. Its effectiveness depends on changes in the source water and lake conditions. However, the challenge of optimizing water diversion remains because it is difficult to simultaneously improve lake water quality and minimize the amount of diverted water. Here, we propose a new approach called dynamic water diversion optimization (DWDO), which combines a comprehensive water quality model with a deep reinforcement learning algorithm. We applied DWDO to a region of Lake Dianchi, the largest eutrophic freshwater lake in China and validated it. Our results demonstrate that DWDO significantly reduced total nitrogen and total phosphorus concentrations in the lake by 7% and 6%, respectively, compared to previous operations. Additionally, annual water diversion decreased by an impressive 75%. Through interpretable machine learning, we identified the impact of meteorological indicators and the water quality of both the source water and the lake on optimal water diversion. We found that a single input variable could either increase or decrease water diversion, depending on its specific value, while multiple factors collectively influenced real-time adjustment of water diversion. Moreover, using well-designed hyperparameters, DWDO proved robust under different uncertainties in model parameters. The training time of the model is theoretically shorter than traditional simulation-optimization algorithms, highlighting its potential to support more effective decision-making in water quality management.}
}
@article{GUO2022187,
title = {Effect of state transition triggered by reinforcement learning in evolutionary prisoner’s dilemma game},
journal = {Neurocomputing},
volume = {511},
pages = {187-197},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.08.023},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222009973},
author = {Hao Guo and Zhen Wang and Zhao Song and Yuan Yuan and Xinyang Deng and Xuelong Li},
keywords = {Social dilemma, -learning, State transition, Action selection, Cooperation},
abstract = {Cooperative behavior is essential for conflicts between collective and individual benefits, and evolutionary game theory provides a key framework to solve this problem. Decision-making of human or automata agent occurs not only in a static environment, but also in the dynamic interactive environment. Since the reinforcement learning algorithm is well performed at explaining the problem in regard to state, action, and environment, we propose a game model with individual state transition which is influenced by the self-regarding Q-learning algorithm. In detail, we at the first time investigate a two-state two-action game, where agents can choose either to participate in the network game (i.e., active agent) or to cut off all the links based on its Q-table (i.e., inactive agent), involving in a dynamic interactive environment. Through numerical simulations, it is shown that cooperation can reach the maximal level in the moderate value space of fixed reward obtained by inactive agents. In particular, long-term expectations and large learning rates are more productive in promoting cooperation. Furthermore, when the dynamic interactive environment reaches a stable state, the number of active neighbors of active cooperators is larger than that of active defectors, which is further larger than the number of active neighbors of inactive agents. Finally, we testify the results of theoretical analysis from the perspective of state transition.}
}
@article{YI2023126455,
title = {Learning controllable elements oriented representations for reinforcement learning},
journal = {Neurocomputing},
volume = {549},
pages = {126455},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126455},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223005787},
author = {Qi Yi and Rui Zhang and Shaohui Peng and Jiaming Guo and Xing Hu and Zidong Du and Qi Guo and Ruizhi Chen and Ling Li and Yunji Chen},
keywords = {Reinforcement learning, Representation learning},
abstract = {Deep Reinforcement Learning (deep RL) has been successfully applied to solve various decision-making problems in recent years. However, the observations in many real-world tasks are often high dimensional and include much task-irrelevant information, limiting the applications of RL algorithms. To tackle this problem, we propose LCER, a representation learning method that aims to provide RL algorithms with compact and sufficient descriptions of the original observations. Specifically, LCER trains representations to retain the controllable elements of the environment, which can reflect the action-related environment dynamics and thus are likely to be task-relevant. We demonstrate the strength of LCER on the DMControl Suite, proving that it can achieve state-of-the-art performance. LCER enables the pixel-based SAC to outperform state-based SAC on the DMControl 100 K benchmark, showing that the obtained representations can match the oracle descriptions (i.e. the physical states) of the environment. We also carry out experiments to show that LCER can efficiently filter out various distractions, especially when those distractions are not controllable.}
}
@article{DAMOTTASALLESBARRETO2008454,
title = {Restricted gradient-descent algorithm for value-function approximation in reinforcement learning},
journal = {Artificial Intelligence},
volume = {172},
number = {4},
pages = {454-482},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001257},
author = {André {da Motta Salles Barreto} and Charles W. Anderson},
keywords = {Reinforcement learning, Neuro-dynamic programming, Value-function approximation, Radial-basis-function networks},
abstract = {This work presents the restricted gradient-descent (RGD) algorithm, a training method for local radial-basis function networks specifically developed to be used in the context of reinforcement learning. The RGD algorithm can be seen as a way to extract relevant features from the state space to feed a linear model computing an approximation of the value function. Its basic idea is to restrict the way the standard gradient-descent algorithm changes the hidden units of the approximator, which results in conservative modifications that make the learning process less prone to divergence. The algorithm is also able to configure the topology of the network, an important characteristic in the context of reinforcement learning, where the changing policy may result in different requirements on the approximator structure. Computational experiments are presented showing that the RGD algorithm consistently generates better value-function approximations than the standard gradient-descent method, and that the latter is more susceptible to divergence. In the pole-balancing and Acrobot tasks, RGD combined with SARSA presents competitive results with other methods found in the literature, including evolutionary and recent reinforcement-learning algorithms.}
}
@article{BALAKRISHNA2010950,
title = {Accuracy of reinforcement learning algorithms for predicting aircraft taxi-out times: A case-study of Tampa Bay departures},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {18},
number = {6},
pages = {950-962},
year = {2010},
note = {Special issue on Transportation Simulation Advances in Air Transportation Research},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2010.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X1000029X},
author = {Poornima Balakrishna and Rajesh Ganesan and Lance Sherry},
keywords = {Taxi-out prediction, Reinforcement learning, Flight delay},
abstract = {Taxi-out delay is a significant portion of the block time of a flight. Uncertainty in taxi-out times reduces predictability of arrival times at the destination. This in turn results in inefficient use of airline resources such as aircraft, crew, and ground personnel. Taxi-out time prediction is also a first step in enabling schedule modifications that would help mitigate congestion and reduce emissions. The dynamically changing operation at the airport makes it difficult to accurately predict taxi-out time. In this paper we investigate the accuracy of taxi out time prediction using a nonparametric reinforcement learning (RL) based method, set in the probabilistic framework of stochastic dynamic programming. A case-study of Tampa International Airport (TPA) shows that on an average, with 93.7% probability, on any given day, our predicted mean taxi-out time for any given quarter, matches the actual mean taxi-out time for the same quarter with a standard error of 1.5min. Also, for individual flights, the taxi-out time of 81% of them were predicted accurately within a standard error of 2min. The predictions were done 15min before gate departure. Gate OUT, wheels OFF, wheels ON, and gate IN (OOOI) data available in the Aviation System Performance Metric (ASPM) database maintained by the Federal Aviation Administration (FAA) was used to model and analyze the problem. The prediction accuracy is high even without the use of detailed track data.}
}
@article{KUMAR2022104962,
title = {Updating geostatistically simulated models of mineral deposits in real-time with incoming new information using actor-critic reinforcement learning},
journal = {Computers & Geosciences},
volume = {158},
pages = {104962},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.104962},
url = {https://www.sciencedirect.com/science/article/pii/S009830042100248X},
author = {Ashish Kumar and Roussos Dimitrakopoulos},
keywords = {Data assimilation, High-order spatial statistics, Geostatistics, Reinforcement learning, Real-time, Production sensor data, Convolutional neural network},
abstract = {The existing technologies that update geostatistically simulated models of mineral deposits cannot self-learn from incoming new information generated in operating mines and do not account for high-order spatial statistics. This work proposes a novel self-learning artificial intelligence algorithm that learns from incoming new information and accounts for high-order spatial statistics, in order to update the geostatistically simulated models of mineral deposits in real-time. The proposed algorithm uses deep policy gradient reinforcement learning with an actor and a critic agent. The grid nodes of the geostatistically simulated model are visited sequentially in a random path, the environment generates the states for each grid node, and feeds the state to the actor and critic agents that respectively predict and evaluate the updated property of the grid node The data is stored in a replay memory, which is sampled at regular intervals to train the agents. The trained agents are then used for further rounds of self-learning. An application of the proposed algorithm at a copper mining operation with incoming drilling machine sensor data (collected spatially), and processing mill sensor data (collected over time), demonstrates its applied aspects in updating the geostatistically simulated models of copper grades of the mineral deposit in real-time, while also reproducing spatial patterns and high-order spatial statistics.}
}
@article{RAJU2015202,
title = {Reinforcement Learning in Adaptive Control of Power System Generation},
journal = {Procedia Computer Science},
volume = {46},
pages = {202-209},
year = {2015},
note = {Proceedings of the International Conference on Information and Communication Technologies, ICICT 2014, 3-5 December 2014 at Bolgatty Palace & Island Resort, Kochi, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915000769},
author = {Leo Raju and R.S. Milton and Swetha Suresh and Sibi Sankar},
keywords = {Unit commitment, Economic dispatch, Reinforcement Learning, Q Learning, Optimization},
abstract = {Considering our depleting resources, efficient energy production and transmission is the need of the hour. This paper focuses on the concept of using Reinforcement Learning (RL) to control the power systems unit commitment and economic dispatch problem. The idea of reinforcement learning strives to present an ever optimal system even when there are load fluctuations. This is done by training the agent (system), thereby enriching its knowledge base which ensures that even without manual intervention all the available resources are used judiciously. Also the agent learns to reach long term objective of minimizing cost by autonomous optimization. A model free reinforcement learning method called, Q learning is used to find the cost at various loadings and is compared with the conventional priority list method and the performance improvement due to Q learning is proved.}
}
@article{CHEN20232680,
title = {Data-driven transferred energy management strategy for hybrid electric vehicles via deep reinforcement learning},
journal = {Energy Reports},
volume = {10},
pages = {2680-2692},
year = {2023},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2023.09.087},
url = {https://www.sciencedirect.com/science/article/pii/S235248472301332X},
author = {Hao Chen and Gang Guo and Bangbei Tang and Guo Hu and Xiaolin Tang and Teng Liu},
keywords = {Transfer learning, Deep reinforcement learning, Proximal policy optimization, Energy management, Hybrid electric vehicle, Real-world driving cycles},
abstract = {Real-time applications of energy management strategies (EMSs) in hybrid electric vehicles (HEVs) are the harshest requirements for researchers and engineers. Inspired by the excellent problem-solving capabilities of deep reinforcement learning (DRL), this paper proposes a real-time EMS via incorporating the DRL method and transfer learning (TL). The related EMSs are derived from and evaluated on the real-world collected driving cycle dataset from Transportation Secure Data Center (TSDC). The concrete DRL algorithm is proximal policy optimization (PPO) belonging to the policy gradient (PG) techniques. For specification, many source driving cycles are utilized for training the parameters of deep networks based on PPO. The learned parameters are transformed into the target driving cycles under the TL framework. The EMSs related to the target driving cycles are estimated and compared in different training conditions. Simulation results indicate that the presented transfer DRL-based EMS could effectively reduce the time consumption and guarantee the control performance.}
}
@article{CARDOZO2023107308,
title = {Auto-COP: Adaptation generation in Context-oriented Programming using Reinforcement Learning options},
journal = {Information and Software Technology},
volume = {164},
pages = {107308},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107308},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923001635},
author = {Nicolás Cardozo and Ivana Dusparic},
keywords = {Context-oriented programming, Reinforcement learning, Macro actions, Option learning, Self-adaptive systems},
abstract = {Context:
Self-adaptive software systems continuously adapt in response to internal and external changes in their execution environment, captured as contexts. The Context-oriented Programming (COP) paradigm posits a technique for the development of self-adaptive systems, capturing their main characteristics with specialized programming language constructs. In COP, adaptations are specified as independent modules that are composed in and out of the base system as contexts are activated and deactivated in response to sensed circumstances from the surrounding environment. However, the definition of adaptations, their contexts and associated specialized behavior, need to be specified at design time. In complex cyber–physical systems this is intractable, if not impossible, due to new unpredicted operating conditions arising.
Objective:
In this paper, we propose Auto-COP, a new technique to enable generation of adaptations at run time. Auto-COP uses Reinforcement Learning (RL) options to build action sequences, based on the previous instances of the system execution (for example, atomic system actions enacted by human operators). Options are further explored in interaction with the environment, and the most suitable options for each context are used to generate the adaptations, exploiting COP abstractions.
Method:
To validate Auto-COP, we present two case studies exhibiting different system characteristics and application domains: a driving assistant and a robot delivery system. We present examples of Auto-COP to illustrate the types of circumstances (contexts) requiring adaptation at run time, and the corresponding generated adaptations for each context.
Results:
We confirm that the generated adaptations exhibit correct system behavior measured by domain-specific performance metrics (e.g., conformance to specified speed limit), while reducing the number of required execution/actuation steps by a factor of two showing that the adaptations are regularly selected by the running system as adaptive behavior is more appropriate than the execution of atomic actions.
Conclusion:
Therefore, we demonstrate that Auto-COP is able to increase system adaptivity by enabling run-time generation of new adaptations for conditions detected at run time, while retaining the modularity offered by COP languages, and reducing the upfront specification required by system developers.}
}
@article{DING2019243,
title = {An improved reinforcement learning algorithm based on knowledge transfer and applications in autonomous vehicles},
journal = {Neurocomputing},
volume = {361},
pages = {243-255},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.06.067},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219309683},
author = {Derui Ding and Zifan Ding and Guoliang Wei and Fei Han},
keywords = {Reinforcement learning, Knowledge transfer, Case base, Case base expansion, Progressive forgetting criteria, Neural networks},
abstract = {Autonomous learning is a crucially important capability of intelligent robots. As one of the most fashionable machine learning techniques, the reinforcement learning (RL) enables an agent taking an optimized action by interacting with the environment so as to maximize some notion of cumulative reward. In this paper, an improved RL algorithm, named as the KT-HA-Q(λ) algorithm, is proposed by resorting to the knowledge transfer of source domain. First, a BP neural network and a liner sensor network are skillfully constructed to perform the knowledge transfer of source task for weight initialization in target task, and the knowledge transfer on actions of case base obtained by source domain, respectively. Then, the novel case base expansion and progressive forgetting criterion, which realize the balance between new experience via online learning and historical experience in the case base, are developed to enhance the learning efficiency and the learning rate. Furthermore, an improved heuristic function is proposed by replacing the action traditionally obtained via a selection strategy by the experience action. This function acts as a crucial role for both the best action selection and its Q value calculation. Finally, the proposed algorithm is utilized in the hill-climbing experiment of unmanned vehicles under a complex 3D scene by transferring the knowledge obtained in a 2D scene. The results of contrast experiments verified the advantages and effectiveness of the proposed algorithm.}
}
@article{CHAVAN2023110151,
title = {Reinforcement Learning approach of switching bi-stable oscillators to adapt bandgaps of 1D-meta-structures},
journal = {Mechanical Systems and Signal Processing},
volume = {191},
pages = {110151},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2023.110151},
url = {https://www.sciencedirect.com/science/article/pii/S0888327023000584},
author = {Shantanu H. Chavan and Satya Sarvani Malladi and Vijaya V.N. Sriram Malladi},
keywords = {Meta-structure, Dynamic vibrational resonators, Reinforcement learning},
abstract = {Meta-structures with dynamic vibrational resonators (DVRs) are programmed to control the propagation of waves and attenuate vibrations over a broadband frequency spectrum. Attributes of DVRs, such as their resonant frequency and mass, determine the location and width of the bandgap, respectively. As a result, to adaptively program bandgaps, one has to modify or tune the eigenvalues of individual DVRs, and a popular approach is to vary the stiffness of each resonator. However, the tunable range of bandgaps is often restricted to maximum change in DVRs’ stiffness. This work presents a novel approach to adaptively program bandgaps of a 1D flexural meta-structure over a broad frequency bandwidth. DVRs with two stable configurations are attached to a beam in developing the meta-structure. A numerical model is developed to illustrate the scope of the novel approach. An experimental investigation then validates the simulated results and shows the extent of the vibration absorption capabilities of the meta-structure. A reinforced learning approach is used to adaptively tune the bandgap over 220 Hz to 840 Hz.}
}
@article{SHI2021103421,
title = {Connected automated vehicle cooperative control with a deep reinforcement learning approach in a mixed traffic environment},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {133},
pages = {103421},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103421},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21004150},
author = {Haotian Shi and Yang Zhou and Keshu Wu and Xin Wang and Yangxin Lin and Bin Ran},
keywords = {Mixed connected automated traffic environment, Cooperative control, Deep reinforcement learning, Traffic oscillation dampening, Energy efficiency},
abstract = {This paper proposes a cooperative strategy of connected and automated vehicles (CAVs) longitudinal control for a mixed connected and automated traffic environment based on deep reinforcement learning (DRL) algorithm, which enhances the string stability of mixed traffic, car following efficiency, and energy efficiency. Since the sequences of mixed traffic are combinatory, to reduce the training dimension and alleviate communication burdens, we decomposed mixed traffic into multiple subsystems where each subsystem is comprised of human-driven vehicles (HDV) followed by cooperative CAVs. Based on that, a cooperative CAV control strategy is developed based on a deep reinforcement learning algorithm, enabling CAVs to learn the leading HDV’s characteristics and make longitudinal control decisions cooperatively to improve the performance of each subsystem locally and consequently enhance performance for the whole mixed traffic flow. For training, a distributed proximal policy optimization is applied to ensure the training convergence of the proposed DRL. To verify the effectiveness of the proposed method, simulated experiments are conducted, which shows the performance of our proposed model has a great generalization capability of dampening oscillations, fulfilling the car following and energy-saving tasks efficiently under different penetration rates and various leading HDVs behaviors.}
}
@article{LU20141112,
title = {Indirect Reinforcement Learning for Incident-responsive Ramp Control},
journal = {Procedia - Social and Behavioral Sciences},
volume = {111},
pages = {1112-1122},
year = {2014},
note = {Transportation: Can we do more with less resources? – 16th Meeting of the Euro Working Group on Transportation – Porto 2013},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.01.146},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814001475},
author = {Chao Lu and Haibo Chen and Susan Grant-Muller},
keywords = {reinforcement learning, ramp metering, incident, cell transmission model},
abstract = {A centralised strategy named indirect reinforcement learning ramp controller (IRLRC) has been developed in this paper to deal with ramp control problems for the congested traffic caused by incidents. IRLRC is developed on the basis of Dyna-Q architecture, under which a modified asymmetric cell transmission model (ACTM) and the standard Q-learning algorithm are combined together. The simulation-based test shows that compared with the no controlled situation, IRLRC can reduce the total travel time up to 24%, which outperforms the direct reinforcement learning (DRL) method with a reduction of 18% after the same number of iterations.}
}
@article{XIAO2022109279,
title = {A sub-action aided deep reinforcement learning framework for latency-sensitive network slicing},
journal = {Computer Networks},
volume = {217},
pages = {109279},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109279},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622003383},
author = {Da Xiao and Shuo Chen and Wei Ni and Jie Zhang and Andrew Zhang and Renping Liu},
keywords = {Network slicing, QoS over-provisioning, VNF-FG placement},
abstract = {Network slicing is a core technique of fifth-generation (5G) systems and beyond. To maximize the number of accepted network slices with limited hardware resources, service providers must avoid over-provisioning of quality-of-service (QoS), which could prevent them from lowering capital expenditures (CAPEX)/operating expenses (OPEX) for 5G infrastructure. In this paper, we propose a sub-action aided double deep Q-network (SADDQN)-based network slicing algorithm for latency-aware services. Specifically, we model network slicing as a Markov decision process (MDP), where we consider virtual network function (VNF) placements to be the actions of the MDP, and define a reward function based on cost and service priority. Furthermore, we adopt the Dijkstra algorithm to determine the forwarding graph (FG) embedding for a given VNF placement and design a resource allocation algorithm – binary search assisted gradient descent (BSAGD) – to allocate resources to VNFs given the VNF-FG placement. For every service request, we first use the DDQN to choose an MDP action to determine the VNF placement (main action). Next, we employ the Dijkstra algorithm (first-phase sub-action) to find the shortest path for each pair of adjacent VNFs in the given VNF chain. Finally, we implement the BSAGD (second-phase sub-action) to realize this service with the minimum cost. The joint action results in an MDP reward that can be utilized to train the DDQN. Numerical evaluations show that, compared to state-of-the-art algorithms, the proposed algorithm can improve the cost-efficiency while giving priority to higher-priority services and maximizing the acceptance ratio.}
}
@article{LAWRENCE2020236,
title = {Optimal PID and Antiwindup Control Design as a Reinforcement Learning Problem},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {236-241},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.129},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320303852},
author = {Nathan P. Lawrence and Gregory E. Stewart and Philip D. Loewen and Michael G. Forbes and Johan U. Backstrom and R. Bhushan Gopaluni},
keywords = {neural networks, reinforcement learning, actor-critic networks, process control, PID control, anti-windup compensation},
abstract = {Deep reinforcement learning (DRL) has seen several successful applications to process control. Common methods rely on a deep neural network structure to model the controller or process. With increasingly complicated control structures, the closed-loop stability of such methods becomes less clear. In this work, we focus on the interpretability of DRL control methods. In particular, we view linear fixed-structure controllers as shallow neural networks embedded in the actor-critic framework. PID controllers guide our development due to their simplicity and acceptance in industrial practice. We then consider input saturation, leading to a simple nonlinear control structure. In order to effectively operate within the actuator limits we then incorporate a tuning parameter for anti-windup compensation. Finally, the simplicity of the controller allows for straightforward initialization. This makes our method inherently stabilizing, both during and after training, and amenable to known operational PID gains.}
}
@article{SHAFI2020106939,
title = {A hierarchical constrained reinforcement learning for optimization of bitumen recovery rate in a primary separation vessel},
journal = {Computers & Chemical Engineering},
volume = {140},
pages = {106939},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106939},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420301861},
author = {Hareem Shafi and Kirubakaran Velswamy and Fadi Ibrahim and Biao Huang},
keywords = {Primary separation vessel, Oil sands, Machine learning, Reinforcement learning, Process control},
abstract = {This work proposes a two-level hierarchical constrained control structure for reinforcement learning (RL) with application in a Primary Separation Vessel (PSV). The lower level is concerned with servo tracking and regulation of the interface level against variances in ore quality by manipulating middlings flow rate. At the higher level, with the objective to optimize bitumen recovery rate, a supervisory interface level setpoint control is implemented. To prevent sanding, tailings density regulation using tailings withdrawal flow rate is proposed. For each case, an asynchronous advantage actor-critic (A3C) based agent is chosen to interact with a high-fidelity PSV model to learn the near optimal control strategy through episodic interactions. Each of the three control loops is sequentially learnt. In the interface level control loop, a behavioral cloning based two-phase learning scheme to promote stable state space exploration is proposed. The proposed hierarchical structure successfully demonstrates improved bitumen recovery rate by manipulating the interface level while preventing sanding.}
}
@article{AJAGEKAR2022406,
title = {Deep Reinforcement Learning Based Automatic Control in Semi-Closed Greenhouse Systems},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {7},
pages = {406-411},
year = {2022},
note = {13th IFAC Symposium on Dynamics and Control of Process Systems, including Biosystems DYCOPS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.07.477},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322008837},
author = {Akshay Ajagekar and Fengqi You},
keywords = {Deep reinforcement learning, Greenhouse, Climate control},
abstract = {This work proposes a novel deep reinforcement learning (DRL) based control framework for greenhouse climate control. This framework utilizes a neural network to approximate state-action value estimation. The neural network is trained by adopting a Q-learning based approach for experience collection and parameter updates. Continuous action spaces are effectively handled by the proposed approach by extracting optimal actions for a given greenhouse state from the neural network approximator through stochastic gradient ascent. Analytical gradients of the state-action value estimate are not required but can be computed effectively through backpropagation. We evaluate the performance of our DRL algorithm on a semi-closed greenhouse simulation located in New York City. The obtained computational results indicate that the proposed Q-learning based DRL framework yields higher cumulative returns. They also demonstrate that the proposed control technique consumes 61% lesser energy than deep deterministic policy gradient (DDPG) method.}
}
@article{KHANI2023102188,
title = {An Enhanced Deep Reinforcement Learning-based Slice Acceptance Control System (EDRL-SACS) for Cloud–Radio Access Network},
journal = {Physical Communication},
volume = {61},
pages = {102188},
year = {2023},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2023.102188},
url = {https://www.sciencedirect.com/science/article/pii/S187449072300191X},
author = {Mohsen Khani and Shahram Jamali and Mohammad Karim Sohrabi},
keywords = {Slice Acceptance Control, 5G, Deep reinforcement learning, Network slicing},
abstract = {In 5G networks, Network Slicing (NS) allows service providers (SPs) to create customized standalone networks on shared platforms provided by infrastructure providers (InPs). However, the challenge arises when prioritizing slice requests sent by SPs to InPs due to limited and shared resources. The Cloud Radio Access Network (C-RAN) architecture is a novel approach that supports various services in 5G networks, but efficient resource utilization and increased revenue across the three layers of C-RAN are necessary. To address this issue, we propose an Enhanced Deep Reinforcement Learning-based Slice Acceptance Control System (EDRL-SACS) technique to maximize network utility by accepting suitable requests. EDRL-SACS considers time-varying resource situations, various service requirements, and the frequency of requests from SPs to endorse model-free solutions for Slice Acceptance Control (SAC). By adding effective parameters to the deep reinforcement learning algorithm, the proposed methodology improves its efficiency and performance resulting in better resource utilization, coverage, revenue, and rejection of users’ request terms. The system’s evaluation shows that EDRL-SACS performs well and effectively maximizes the utility of the network by providing slices to SPs in response to their slice requests.}
}
@article{EPPERLEIN2022110483,
title = {Reinforcement learning with algorithms from probabilistic structure estimation},
journal = {Automatica},
volume = {144},
pages = {110483},
year = {2022},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2022.110483},
url = {https://www.sciencedirect.com/science/article/pii/S0005109822003429},
author = {Jonathan P. Epperlein and Roman Overko and Sergiy Zhuk and Christopher King and Djallel Bouneffouf and Andrew Cullen and Robert Shorten},
keywords = {Reinforcement learning, Statistical testing, Markov decision process, Machine learning, Decision support system},
abstract = {Reinforcement learning (RL) algorithms aim to learn optimal decisions in unknown environments through the experience of taking actions and observing the rewards gained. In some cases, the environment is not influenced by the actions of the RL agent, in which case the problem can be modeled as a contextual multi-armed bandit, and lightweight myopic algorithms can be employed. On the other hand, when the RL agent’s actions affect the environment, the problem must be modeled as a Markov decision process, and more complex RL algorithms are required, which take the future effects of actions into account. Moreover, in practice, it is often unknown from the outset whether or not the agent’s actions will impact the environment, and it is therefore not possible to determine which RL algorithm is most fitting. In this work, we propose to avoid this difficult decision entirely and incorporate a choice mechanism into our RL framework. Rather than assuming a specific problem structure, we use a probabilistic structure estimation procedure based on a likelihood-ratio (LR) test to make a more informed selection of the learning algorithm. We derive a sufficient condition under which myopic policies are optimal, present an LR test for this condition, and derive a bound on the regret of our framework. We provide examples of real-world scenarios where our framework is needed and provide extensive simulations to validate our approach.}
}
@article{LIU2023104779,
title = {Deep reinforcement learning based controller with dynamic feature extraction for an industrial claus process},
journal = {Journal of the Taiwan Institute of Chemical Engineers},
volume = {146},
pages = {104779},
year = {2023},
issn = {1876-1070},
doi = {https://doi.org/10.1016/j.jtice.2023.104779},
url = {https://www.sciencedirect.com/science/article/pii/S1876107023001086},
author = {Jialin Liu and Bing-Yen Tsai and Ding-Sou Chen},
keywords = {Deep reinforcement learning, industrial Claus process, Sequence-to-sequence network, Actor-critic networks, Twin delayed deep deterministic policy gradient algorithm},
abstract = {Background
The significant time delay between the manipulated and controlled variables introduces challenges in the task of system identification when implementing model predictive control (MPC) for an industrial process. Recently, deep reinforcement learning (DRL) with model-free characteristics has attracted considerable attention from the process control community. However, the model-free assumption in DRL is based on the property of the Markov decision process (MDP), in which all state variables must be observed. This assumption is not true for an industrial process.
Methods
In this study, the sequence-to-sequence (Seq2seq) network was employed to build a surrogate model based on the industrial Claus process data. Meanwhile, the hidden state output from the encoder of the Seq2seq network, which represents the dynamic feature of the process, connects to the DRL-based controller to compensate for the partial observabilities of a real process.
Significant findings
The results show that the standard deviation of the control variable, which refers to the H2S to SO2 concentration ratio in the tail gas, can be reduced by up to 55% using the proposed DRL-based controller comparing with the current control strategy.}
}
@article{CONRADIE20101627,
title = {Neurocontrol of a multi-effect batch distillation pilot plant based on evolutionary reinforcement learning},
journal = {Chemical Engineering Science},
volume = {65},
number = {5},
pages = {1627-1643},
year = {2010},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2009.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S000925090900788X},
author = {Alex v.E. Conradie and Chris Aldrich},
keywords = {Distillation, Process control, Nonlinear dynamics, Mathematical modelling, Neural networks, Evolutionary programming},
abstract = {The time cost of first-principles dynamic modelling and the complexity of nonlinear control strategies may limit successful implementation of advanced process control. The maximum return on fixed capital within the processing industries is thus compromised. This study introduces a neurocontrol methodology that uses partial system identification and symbiotic memetic neuro-evolution (SMNE) for the development of neurocontrollers. Partial system identification is achieved using singular spectrum analysis (SSA) to extract state variables from time series data. The SMNE algorithm uses a symbiotic evolutionary algorithm and particle swarm optimisation to learn optimal neurocontroller weights from the partially identified system within a reinforcement learning framework. A multi-effect batch distillation (MEBAD) pilot plant was constructed to demonstrate the real world application of the neurocontrol methodology, motivated by the nonsteady state operation and nonlinear process interaction between multiple distillation columns. Multi-loop proportional integral (PI) control was implemented as a reduced model, reflecting an approach involving no modelling or significant controller tuning. Rapid multiple input multiple out nonlinear controller development was achieved using SSA and the SMNE algorithm, demonstrating comparable time and cost to implementation in relation to the reduced model. The optimal neurocontroller reduced the batch time and therefore the energy consumption by 45% compared to conventional multi-loop SISO PI control.}
}
@article{ZHANG20121315,
title = {Minimizing mean weighted tardiness in unrelated parallel machine scheduling with reinforcement learning},
journal = {Computers & Operations Research},
volume = {39},
number = {7},
pages = {1315-1324},
year = {2012},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2011.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0305054811002127},
author = {Zhicong Zhang and Li Zheng and Na Li and Weiping Wang and Shouyan Zhong and Kaishun Hu},
keywords = {Scheduling, Unrelated parallel machines, Reinforcement learning, Tardiness},
abstract = {We address an unrelated parallel machine scheduling problem with R-learning, an average-reward reinforcement learning (RL) method. Different types of jobs dynamically arrive in independent Poisson processes. Thus the arrival time and the due date of each job are stochastic. We convert the scheduling problems into RL problems by constructing elaborate state features, actions, and the reward function. The state features and actions are defined fully utilizing prior domain knowledge. Minimizing the reward per decision time step is equivalent to minimizing the schedule objective, i.e. mean weighted tardiness. We apply an on-line R-learning algorithm with function approximation to solve the RL problems. Computational experiments demonstrate that R-learning learns an optimal or near-optimal policy in a dynamic environment from experience and outperforms four effective heuristic priority rules (i.e. WSPT, WMDD, ATC and WCOVERT) in all test problems.}
}
@article{GUPTA2021101739,
title = {Energy-efficient heating control for smart buildings with deep reinforcement learning},
journal = {Journal of Building Engineering},
volume = {34},
pages = {101739},
year = {2021},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2020.101739},
url = {https://www.sciencedirect.com/science/article/pii/S2352710220333726},
author = {Anchal Gupta and Youakim Badr and Ashkan Negahban and Robin G. Qiu},
keywords = {Deep reinforcement learning, Simulation, Occupant thermal comfort, Heating controller, HVAC},
abstract = {Buildings account for roughly 40% of the total energy consumption in the world, out of which heating, ventilation, and air conditioning are the major contributors. Traditional heating controllers are inefficient due to lack of adaptability to dynamic conditions such as changing user preferences and outside temperature patterns. Therefore, it is necessary to design energy-efficient controllers that can improvise occupant thermal comfort (deviation from setpoint temperature) while reducing energy consumption. This research presents a Deep Reinforcement Learning (DRL)-based heating controller to improve thermal comfort and minimize energy costs in smart buildings. We perform extensive simulation experiments using real-world outside temperature data. The results show that the DRL-based smart controller outperforms a traditional thermostat controller by improving thermal comfort between 15% and 30% and reducing energy costs between 5% and 12% in the simulated environment. A second set of experiments is then performed for the case of multiple buildings, each having its own heating equipment. The performance is compared when the buildings are controlled centrally (using a single DRL-based controller) versus decentralized control, where each heater is controlled independently and has its own DRL-based controller. We observe that as the number of buildings and differences in their setpoint temperatures increase, decentralized control performs better than a centralized controller. The results have practical implications for heating control, especially in areas with multiple buildings such as residential complexes with multiple houses.}
}
@article{LI2022145,
title = {Off-policy reinforcement learning-based novel model-free minmax fault-tolerant tracking control for industrial processes},
journal = {Journal of Process Control},
volume = {115},
pages = {145-156},
year = {2022},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S095915242200083X},
author = {Xueyu Li and Qiuwen Luo and Limin Wang and Ridong Zhang and Furong Gao},
keywords = {Industrial process, Off-policy reinforcement learning, Novel model-free minmax fault-tolerant tracking control, Actuator failure},
abstract = {For industrial processes with external disturbance and actuator failure, off-policy reinforcement learning-based novel model-free minmax fault-tolerant control is proposed in this paper to solve H∞ fault-tolerant tracking control problem. An augmented model equivalent to the original system is constructed, and the state of the new augmented model is composed of state increment and tracking error of the original system. The original H∞ fault-tolerant tracking problem was transformed into the linear quadratic zero-sum game problem by establishing performance index function, and the Game Algebraic Riccati Equation (GARE) was established. Then Q function was introduced and the Off-policy reinforcement learning algorithm was designed. Different from the traditional model-based fault-tolerant control method, the proposed algorithm does not need the knowledge of system dynamics, and it can learn from the measured data of the system trajectory to solve the GARE. In addition, it is proved that the probing noise added to satisfy the persistent excitation condition does not cause bias. A simulation example of injection molding process is used to verify the effectiveness of the proposed algorithm.}
}
@article{HOU2023104451,
title = {Learning 6-DoF grasping with dual-agent deep reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {166},
pages = {104451},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104451},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000908},
author = {Yanxu Hou and Jun Li},
keywords = {Robotic grasp, Cluttered objects, Multi-agent reinforcement learning, Grasp representation, Grasp quality},
abstract = {Self-supervised grasp learning (SGL) is one of the most promising approaches to challenging robotic grasping. However, most existing SGL-based methods are restricted to in 4-DoF planar grasps due to limited grasp representations and inadequate learning rewards. This paper proposes 6-DoF grasp learning (6DGL). It represents a 6-DoF grasp by exploiting both planar and spherical grasp affordance in the image space. Its underlying network is called Q Mixing Network with Planar and Spherical Affordances (QMIX-PSA). In QMIX-PSA, two agent networks, i.e., a planar affordance network (PA-Net) and a spherical affordance network (SA-Net), are used to predict grasp position and orientation. Then, the two networks’ joint action value is estimated by a QMIX to reconstruct their connection. Again, an augmented reward is presented to evaluate the quality of a 6-DoF grasp by measuring the incurred disturbance to the surroundings with scene images. Finally, extensive experiments are conducted on grasping metal workpieces and daily items. The results show that 6DGL outperforms its peers regarding grasp success rate and quality, especially in clutter where existing SGL methods are incompetent.}
}
@article{CHEN2023108060,
title = {Reinforcement-learning-based fixed-time attitude consensus control for multiple spacecraft systems with model uncertainties},
journal = {Aerospace Science and Technology},
volume = {132},
pages = {108060},
year = {2023},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2022.108060},
url = {https://www.sciencedirect.com/science/article/pii/S1270963822007349},
author = {Run-Ze Chen and Yuan-Xin Li and Choon Ki Ahn},
keywords = {Distributed control, Attitude consensus, Fixed-time observer, Reinforcement learning (RL), Sliding mode control (SMC)},
abstract = {In this study, the problem of fixed-time attitude consensus control was investigated for multiple spacecraft systems with model uncertainties. First, a distributed fixed-time adaptive observer is proposed for estimating the states of the leader. Subsequently, on the basis of the observation errors, transformed error dynamics are described and they are used to combine the unknown nonlinear terms of a spacecraft system. By using the non-singular fast terminal sliding mode technique and a reinforcement learning optimization algorithm, we implemented a neural-network-based fixed-time control strategy to achieve optimal attitude consensus control. The stability of the system and the fixed-time convergence of the tracking error are demonstrated by using the Lyapunov theory. Furthermore, the effectiveness and superiority of the control strategy are shown through numerical simulations.}
}