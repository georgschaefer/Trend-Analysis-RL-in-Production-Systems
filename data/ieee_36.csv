"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Deep Reinforcement Learning for Electric Transmission Voltage Control","B. L. Thayer; T. J. Overbye","Pacific Northwest National Laboratory located in Richland, WA, USA; Texas A&M University located in College Station, TX, USA","2020 IEEE Electric Power and Energy Conference (EPEC)","18 Jan 2021","2020","","","1","8","Today, human operators primarily perform voltage control of the electric transmission system. As the complexity of the grid increases, so does its operation, suggesting additional automation could be beneficial. A subset of machine learning known as deep reinforcement learning (DRL) has recently shown promise in performing tasks typically performed by humans. This paper applies DRL to the transmission voltage control problem, presents open-source DRL environments for voltage control, proposes a novel modification to the “deep Q network” (DQN) algorithm, and performs experiments at scale with systems up to 500 buses. The promise of applying DRL to voltage control is demonstrated, though more research is needed to enable DRL-based techniques to consistently outperform conventional methods.","2381-2842","978-1-7281-6489-2","10.1109/EPEC48502.2020.9320077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9320077","Machine learning;artificial intelligence;automatic voltage control;power system simulation","Voltage control;Generators;Power systems;Reinforcement learning;Testing;Training;Open source software","control engineering computing;deep learning (artificial intelligence);power engineering computing;power system control;power transmission;public domain software;voltage control","deep reinforcement learning;electric transmission voltage control;human operators;electric transmission system;machine learning;deep Q network algorithm;open-source DRL environments","","6","","35","IEEE","18 Jan 2021","","","IEEE","IEEE Conferences"
"Cooperative Assistance in Robotic Surgery through Multi-Agent Reinforcement Learning","P. M. Scheikl; B. Gyenes; T. Davitashvili; R. Younis; A. Schulze; B. P. Müller-Stich; G. Neumann; M. Wagner; F. Mathis-Ullrich","Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Department for General, Visceral and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Department for General, Visceral and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Department for General, Visceral and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Department for General, Visceral and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Department for General, Visceral and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany","2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","16 Dec 2021","2021","","","1859","1864","Cognitive cooperative assistance in robot-assisted surgery holds the potential to increase quality of care in minimally invasive interventions. Automation of surgical tasks promises to reduce the mental exertion and fatigue of surgeons. In this work, multi-agent reinforcement learning is demonstrated to be robust to the distribution shift introduced by pairing a learned policy with a human team member. Multi-agent policies are trained directly from images in simulation to control multiple instruments in a sub task of the minimally invasive removal of the gallbladder. These agents are evaluated individually and in cooperation with humans to demonstrate their suitability as autonomous assistants. Compared to human teams, the hybrid teams with artificial agents perform better considering completion time (44.4% to 71.2% shorter) as well as number of collisions (44.7% to 98.0% fewer). Path lengths, however, increase under control of an artificial agent (11.4% to 33.5% longer). A multi-agent formulation of the learning problem was favored over a single-agent formulation on this surgical sub task, due to the sequential learning of the two instruments. This approach may be extended to other tasks that are difficult to formulate within the standard reinforcement learning framework. Multi-agent reinforcement learning may shift the paradigm of cognitive robotic surgery towards seamless cooperation between surgeons and assistive technologies.","2153-0866","978-1-6654-1714-3","10.1109/IROS51168.2021.9636193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636193","","Laparoscopes;Minimally invasive surgery;Medical robotics;Instruments;Reinforcement learning;Gallbladder;Fatigue","assistive robots;health care;medical computing;medical image processing;multi-agent systems;patient diagnosis;reinforcement learning;surgical robots","reinforcement learning;minimally invasive interventions;human team member;assistive technologies;artificial agent;multiagent formulation;single-agent formulation;robotic surgery;surgeons;gallbladder","","5","","22","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Methodology for Interpretable Reinforcement Learning Model for HVAC Energy Control","O. Kotevska; J. Munk; K. Kurte; Y. Du; K. Amasyali; R. W. Smith; H. Zandi","Computer Science and Mathematics, Oak Ridge National Laboratory, Oak Ridge, TN, U.S.A; Energy and Transportation Science, Oak Ridge National Laboratory, Oak Ridge, TN, U.S.A; Computational Sciences and Engineering, Oak Ridge National Laboratory, Oak Ridge, TN, U.S.A; Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN, U.S.A; Computational Sciences and Engineering, Oak Ridge National Laboratory, Oak Ridge, TN, U.S.A; Computer Science and Mathematics, Oak Ridge National Laboratory, Oak Ridge, TN, U.S.A; Computational Sciences and Engineering, Oak Ridge National Laboratory, Oak Ridge, TN, U.S.A","2020 IEEE International Conference on Big Data (Big Data)","19 Mar 2021","2020","","","1555","1564","Deep reinforcement learning (DRL) approaches have been used in various application areas to improve efficiency, optimization, or automation. However, very little is known about how the DRL algorithms make decisions and what features affect their performance. Using a case study of a DRL based Heating, Ventilation and Air Conditioning (HVAC) optimization methodology, we demonstrate how we can address these challenges by applying interpretability tools and systematically exploring the model inputs for better understanding the DRL behaviour and decision making process. We developed a methodology for interpretable reinforcement learning and evaluated our approach in real-world house located in Knoxville, TN. Our findings explain the reasoning behind DRL-based optimization decisions under different circumstances which has been discussed and confirmed by the experts in the field.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9377735","Battelle; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9377735","reinforcement learning;decision making;interpretability;optimization;deep learning;machine learning;demand response","HVAC;Water heating;Reinforcement learning;Big Data;Tools;Ventilation;Optimization","building management systems;control engineering computing;decision making;deep learning (artificial intelligence);HVAC;optimisation;power control","interpretable reinforcement learning model;HVAC energy control;deep reinforcement learning approaches;DRL algorithms;interpretability tools;model inputs;decision making process;DRL-based optimization decisions;heating, ventilation and air conditioning optimization methodology;HVAC optimization methodology","","4","","30","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"An Automated Deep Reinforcement Learning Pipeline for Dynamic Pricing","R. R. Afshar; J. Rhuggenaath; Y. Zhang; U. Kaymak","Eindhoven University of Technology, Eindhoven, Netherlands; Eindhoven University of Technology, Eindhoven, Netherlands; Eindhoven University of Technology, Eindhoven, Netherlands; Jheronimus Academy of Data Science, ‘s-Hertogenbosch, Netherlands","IEEE Transactions on Artificial Intelligence","24 May 2023","2023","4","3","428","437","A dynamic pricing problem is difficult due to the highly dynamic environment and unknown demand distributions. In this article, we propose a deep reinforcement learning (DRL) framework, which is a pipeline that automatically defines the DRL components for solving a dynamic pricing problem. The automated DRL pipeline is necessary because the DRL framework can be designed in numerous ways, and manually finding optimal configurations is tedious. The levels of automation make nonexperts capable of using DRL for dynamic pricing. Our DRL pipeline contains three steps of DRL design, including Markov decision process modeling, algorithm selection, and hyperparameter optimization. It starts with transforming available information to state representation and defining reward function using a reward shaping approach. Then, the hyperparameters are tuned using a novel hyperparameter optimization method that integrates Bayesian optimization and the selection operator of the genetic algorithm. We employ our DRL pipeline on reserve price optimization problems in online advertising as a case study. We show that using the DRL configuration obtained by our DRL pipeline, a pricing policy is obtained whose revenue is significantly higher than the benchmark methods. The evaluation is performed by developing a simulation for the real-time bidding environment that makes exploration possible for the reinforcement learning agent.","2691-4581","","10.1109/TAI.2022.3186292","European Commission; E! 11582; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9807363","Automated reinforcement learning (AutoRL) pipeline;Bayesian optimization (BO);dynamic pricing (DP)","Pricing;Pipelines;Optimization;Heuristic algorithms;Mathematical models;Reinforcement learning;Machine learning algorithms","Bayes methods;deep learning (artificial intelligence);genetic algorithms;learning (artificial intelligence);Markov processes;optimisation;pricing;reinforcement learning;telecommunication computing","automated deep reinforcement learning pipeline;automated DRL pipeline;deep reinforcement learning framework;DRL components;DRL configuration;DRL design;DRL framework;dynamic pricing problem;highly dynamic environment;novel hyperparameter optimization method;optimal configurations;pricing policy;reinforcement learning agent;reserve price optimization problems","","2","","32","IEEE","27 Jun 2022","","","IEEE","IEEE Journals"
"Terrain Adaption Controller for a Walking Excavator Robot using Deep Reinforcement Learning","A. Babu; F. Kirchner","DFKI GmbH (German Research Center for Artificial Intelligence), Robotics Innovation Center, Bremen, Germany; DFKI GmbH (German Research Center for Artificial Intelligence), Robotics Innovation Center, Bremen, Germany","2021 20th International Conference on Advanced Robotics (ICAR)","5 Jan 2022","2021","","","64","70","Automation of heavy-duty vehicles using technologies developed in the robotics domain is gaining popularity. One such vehicle is the walking excavator with active suspension chassis for adapting to uneven terrain. The terrain adaption controller automates the suspension control by considering the factors stability, underlying terrain structure, wheel-ground distance, chassis-ground distance, etc. This work builds the controller, which actuates the joints that control the height of the wheels. Deep reinforcement learning is used, considering the complexity of the problem and transferability to other robots. The controller is learned and evaluated in simulation, where continuous terrain with varying slopes is automatically generated. Autoencoders compress the height-map and convert it into latent space of different sizes. Three groups of controllers are then designed based on observations given to the controller. Evaluation of controllers shows that the controllers with ground distances as observation perform better. If the ground distances are part of the observation, there is no significant difference in performance between controllers with different latent space sizes. For controllers with terrain information and no ground distances, the evaluation results match the terrain reconstruction accuracy of the corresponding autoencoder.","","978-1-6654-3684-7","10.1109/ICAR53236.2021.9659399","Federal Ministry of Education and Research(grant numbers:13N14675); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9659399","","Legged locomotion;Suspensions (mechanical systems);Wheels;Reinforcement learning;Aerospace electronics;Manipulators;Stability analysis","adaptive control;excavators;learning (artificial intelligence);legged locomotion;mobile robots;road vehicles;robot dynamics;stability;suspensions (mechanical components);terrain mapping;wheels","terrain adaption controller;walking excavator robot;deep reinforcement learning;heavy-duty vehicles;robotics domain;active suspension chassis;uneven terrain;suspension control;terrain structure;wheel-ground distance;chassis-ground distance;continuous terrain;ground distances;terrain information;terrain reconstruction accuracy","","2","","29","IEEE","5 Jan 2022","","","IEEE","IEEE Conferences"
"Neuroevolution-based Inverse Reinforcement Learning","K. K. Budhraja; T. Oates","Computer Science and Electrical Engineering, University of Maryland, Baltimore, Baltimore County, USA; Computer Science and Electrical Engineering, University of Maryland, Baltimore, Baltimore County, USA","2017 IEEE Congress on Evolutionary Computation (CEC)","7 Jul 2017","2017","","","67","76","The problem of Learning from Demonstration is targeted at learning to perform tasks based on observed examples. One approach to Learning from Demonstration is Inverse Reinforcement Learning, in which actions are observed to infer rewards. This work combines a feature-based state evaluation approach to Inverse Reinforcement Learning with neuroevolution, a paradigm for modifying neural networks based on their performance on a given task. Neural networks are used to learn from a demonstrated expert policy and are evolved to generate a policy similar to the demonstration. The algorithm is discussed and evaluated against competitive feature-based Inverse Reinforcement Learning approaches. At the cost of execution time, neural networks allow for non-linear combinations of features in state evaluations. This results in better correspondence to observed examples as opposed to using linear combinations. This work also extends existing work on Bayesian Non-Parametric Feature construction for Inverse Reinforcement Learning by using non-linear combinations of intermediate data to improve performance. The algorithm is observed to be specifically suitable for a linearly solvable non-deterministic Markov Decision Processes in which multiple rewards are sparsely scattered in state space. This translates to real-world control problems such as those in robotics and automation (e.g. the robust output tracking problem or controlling an n-joint arm), where the underlying equations can be made linear. A conclusive performance hierarchy between evaluated algorithms is presented.","","978-1-5090-4601-0","10.1109/CEC.2017.7969297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7969297","","Neural networks;Learning (artificial intelligence);Genetic algorithms;Markov processes;Robustness;Kernel;Bayes methods","Bayes methods;decision theory;learning (artificial intelligence);Markov processes;neural nets;nonparametric statistics","neuroevolution-based inverse reinforcement learning;learning from demonstration;feature-based state evaluation;neural networks;Bayesian nonparametric feature construction;nonlinear combinations;nondeterministic Markov decision processes;state space","","1","","42","IEEE","7 Jul 2017","","","IEEE","IEEE Conferences"
"A Modern Perspective on Safe Automated Driving for Different Traffic Dynamics Using Constrained Reinforcement Learning","D. Kamran; T. D. Simão; Q. Yang; C. T. Ponnambalam; J. Fischer; M. T. J. Spaan; M. Lauer","Karlsruhe Institute of Technology, Germany; Radboud University, Nijmegen, The Netherlands; Delft University of Technology, The Netherlands; Delft University of Technology, The Netherlands; Karlsruhe Institute of Technology, Germany; Delft University of Technology, The Netherlands; Karlsruhe Institute of Technology, Germany","2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)","1 Nov 2022","2022","","","4017","4023","The use of reinforcement learning (RL) in real-world domains often requires extensive effort to ensure safe behavior. While this compromises the autonomy of the system, it might still be too risky to allow a learning agent to freely explore its environment. These strict impositions come at the cost of flexibility and applying them often relies on complex parameters and hard-coded knowledge modelled by the reward function. Autonomous driving is one such domain that could greatly benefit from more efficient and verifiable methods for safe automation. We propose to approach the automated driving problem using constrained RL, a method that automates the trade off between risk and utility, thereby significantly reducing the burden on the designer. We first show that an engineered reward function for ensuring safety and utility in one specific environment might not result in the optimal behavior when traffic dynamics changes in the exact environment. Next we show how algorithms based on constrained RL which are more robust to the environmental disturbances can address this challenge. These algorithms use a simple and easy to interpret reward and cost function, and are able to maintain both, efficiency and safety without requiring reward parameter tuning. We demonstrate our approach in the automated merging scenario with different traffic configurations such as low or high chance of cooperative drivers and different cooperative driving strategies.","","978-1-6654-6880-0","10.1109/ITSC55140.2022.9921907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9921907","","Costs;Heuristic algorithms;Merging;Reinforcement learning;Cost function;Safety;Behavioral sciences","advanced driver assistance systems;multi-agent systems;reinforcement learning","safe automated driving;traffic dynamics;constrained reinforcement learning;learning agent;autonomous driving;constrained RL;traffic configurations;cooperative driving strategies;reward function","","1","","34","IEEE","1 Nov 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Air Traffic Conflict Resolution Under Traffic Uncertainties","A. Mukherjee; Y. Guleria; S. Alam","School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore","2022 IEEE/AIAA 41st Digital Avionics Systems Conference (DASC)","31 Oct 2022","2022","","","1","8","With a significant increase in air traffic over the years, the frequency of potential conflicts between aircraft has increased, which in turn has proportionally increased the workload of Air Traffic Control Operators (ATCOs). This has increased the importance of the role of automation tools in assisting the ATCOs. Deep Reinforcement Learning (RL) is a very promising way to address the problem of resolving conflicts in air traffic and find optimum alternate flight paths. Existing research on using RL in air traffic conflict resolution is skewed towards aircraft speed control which is easier to obtain optimum solutions from compared to control of other aircraft parameters. However depending upon airspace sector size, this may not always be feasible. To address this research gap, the current research proposes a deep reinforcement learning architecture for air traffic conflict resolution in a structured airspace, using heading vector control. Results from the architecture shows that the ownship aircraft achieves safe separation of 5 nautical miles for 100% of the conflicts in the designed scenarios. Further analysis reveals that the RL agent achieves promising results in the presence of surrounding aircraft as well as in the event of a potential secondary conflict. The RL agent is capable of guiding the ownship back to its original path once conflicts are resolved. The results also demonstrate that our model is able to provide safe and efficient resolution to the generated air traffic conflicts under uncertainties associated with the speed and cross-track deviation of the intruder aircraft.","2155-7209","978-1-6654-8607-1","10.1109/DASC55683.2022.9925772","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9925772","Artificial intelligence;Deep Reinforcement learning;conflict resolution;air traffic control;Deep Q-learning","Uncertainty;Q-learning;Atmospheric modeling;Velocity control;Reinforcement learning;Aerospace electronics;Trajectory","air traffic;air traffic control;aircraft;aircraft control;learning (artificial intelligence)","potential conflicts;Air Traffic Control Operators;air traffic conflict resolution;aircraft speed control;deep reinforcement learning architecture;potential secondary conflict;generated air traffic conflicts;Traffic uncertainties","","","","32","IEEE","31 Oct 2022","","","IEEE","IEEE Conferences"
"Safe Deep Reinforcement Learning for Power System Operation under Scheduled Unavailability","X. Weiss; S. Mohammadi; P. Khanna; M. R. Hesamzadeh; L. Nordström","School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden","2023 IEEE Power & Energy Society General Meeting (PESGM)","25 Sep 2023","2023","","","1","5","The electrical grid is a safety-critical system, since incorrect actions taken by a power system operator can result in grid failure and cause harm. For this reason, it is desirable to have an automated power system operator that can reliably take actions that avoid grid failure while fulfilling some objective. Given the existing and growing complexity of power system operation, the choice has often fallen on deep reinforcement learning (DRL) agents for automation, but these are neither explainable nor provably safe. Therefore in this work, the effect of shielding on DRL agent survivability, validation computational time, and convergence are explored. To do this, shielded and unshielded DRL agents are evaluated on a standard IEEE 14-bus network. Agents are tasked with balancing generation and demand through redispatch and topology changing actions at a human timescale of 5 minutes. To test survivability under controlled conditions, varying degrees of scheduled unavailability events are introduced which could cause grid failure if unaddressed. Results show improved convergence and generally greater survivability of shielded agents compared with unshielded agents. However, the safety assurances provided by the shield increase computational time. This will require trade-offs or optimizations to make real-time deployment more feasible.","1944-9933","978-1-6654-6441-3","10.1109/PESGM52003.2023.10252619","Energimyndigheten; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10252619","Deep reinforcement learning;power system operation;deep learning;safe deep reinforcement learning","Deep learning;Reinforcement learning;Prediction algorithms;Safety;Power system reliability;Topology;Reliability","deep learning (artificial intelligence);multi-agent systems;optimisation;power engineering computing;power grids;power system security;reinforcement learning","automated power system operator;deep reinforcement learning agents;DRL agent survivability;electrical grid;grid failure;power system operation;safe deep reinforcement learning;safety-critical system;scheduled unavailability;shielded agents;time 5.0 min;topology changing actions","","","","15","IEEE","25 Sep 2023","","","IEEE","IEEE Conferences"
"Payload Transporting with Two Quadrotors by Centralized Reinforcement Learning Method","D. Lin; J. Han; K. Li; J. Zhang; C. Zhang","College of Artificial Intelligence, Nankai University, China; College of Artificial Intelligence, Nankai University, China; College of Artificial Intelligence, Nankai University, China; College of Artificial Intelligence, Nankai University, China; College of Artificial Intelligence, Nankai University, China","IEEE Transactions on Aerospace and Electronic Systems","","2023","PP","99","1","14","Nowadays, quadrotors find applications in automation and artificial intelligence. Among diverse quadrotor studies, payload transport stands out, posing implementation challenges. Using multiple quadrotors reduces per-quadrotor load while increasing system complexity. Inspired by model-free reinforcement learning, we apply it to position control in a nonlinear two-quadrotor payload system. Our approach employs a reinforcement learning agent guided by the Twin Delay Deep Deterministic Policy Gradient (TD3) algorithm. Its goal is accurate cable-suspended payload delivery and system stabilization. We test the method's robustness by adding noise. Simulation results show TD3 excels in ideal conditions and handles noise during training and testing, highlighting its effectiveness. This study's scope can be expanded to encompass scenarios involving three or more quadrotors, providing valuable insights for future endeavors.","1557-9603","","10.1109/TAES.2023.3321260","National Natural Science Foundation of China(grant numbers:62073174,62073175); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10268605","Two-quadrotor transporting payload system;reinforcement learning;Twin Delayed Deep Deterministic Policy Gradient;continuous action space;continuous state space;","Quadrotors;Payloads;Reinforcement learning;Mathematical models;Angular velocity;Torque;Symbols","","","","","","","IEEE","2 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Facilitating Sim-to-Real by Intrinsic Stochasticity of Real-Time Simulation in Reinforcement Learning for Robot Manipulation","A. M. S. Enayati; R. Dershan; Z. Zhang; D. Richert; H. Najjaran","Faculty of Engineering and Computer Science, University of Victoria, Victoria, BC, Canada; School of Engineering, University of British Columbia, Kelowna, BC, Canada; School of Engineering, University of British Columbia, Kelowna, BC, Canada; School of Engineering, University of British Columbia, Kelowna, BC, Canada; Faculty of Engineering and Computer Science, University of Victoria, Victoria, BC, Canada","IEEE Transactions on Artificial Intelligence","","2023","PP","99","1","15","Simulation is essential to reinforcement learning (RL) before implementation in the real world, especially for safety-critical applications like robot manipulation. Conventionally, RL agents are sensitive to the discrepancies between the simulation and the real world, known as the sim-to-real gap. The application of domain randomization, a technique used to fill this gap, is limited to the imposition of heuristic-randomized models. We investigate the properties of intrinsic stochasticity of real-time simulation (RT-IS) of off-the-shelf simulation software and its potential to improve RL performance. This improvement includes a higher tolerance to noise and model imprecision and superiority to conventional domain randomization in terms of ease of use and automation. Firstly, we conduct analytical studies to measure the correlation of RT-IS with the utilization of computer hardware and validate its comparability with the natural stochasticity of a physical robot. Then, we exploit the RT-IS feature in the training of an RL agent. The simulation and physical experiment results verify the feasibility and applicability of RT-IS to robust agent training for robot manipulation tasks. The RT-IS-powered RL agent outperforms conventional agents on robots with modeling uncertainties. RT-IS requires less heuristic randomization, is not task-dependent, and achieves better generalizability than the conventional domain-randomization-powered agents. Our findings provide a new perspective on the sim-to-real problem in practical applications like robot manipulation tasks.","2691-4581","","10.1109/TAI.2023.3299252","Natural Sciences and Engineering Research Council of Canada(grant numbers:CRDPJ 543881-19); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10196019","Domain randomization;reinforcement learning;real-time simulation;sim-to-real","Robots;Training;Artificial intelligence;Uncertainty;Task analysis;Adaptation models;Computational modeling","","","","","","","IEEE","27 Jul 2023","","","IEEE","IEEE Early Access Articles"
"Learning-Based Navigation and Collision Avoidance Through Reinforcement for UAVs","R. Azzam; M. Chehadeh; O. A. Hay; M. A. Humais; I. Boiko; Y. Zweiri","Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University of Science and Technology, Abu Dhabi, UAE; Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University of Science and Technology, Abu Dhabi, UAE; Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University of Science and Technology, Abu Dhabi, UAE; Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University of Science and Technology, Abu Dhabi, UAE; Department of Electrical Engineering and Computer Science, Khalifa University of Science and Technology, Abu Dhabi, UAE; Department of Aerospace Engineering, Khalifa University of Science and Technology, Abu Dhabi, UAE","IEEE Transactions on Aerospace and Electronic Systems","","2023","PP","99","1","15","Reinforcement learning (RL) has been proven to enable the automation of tasks involving complex sequential decision-making. The simulation to reality (sim2real) gap, however, poses a major challenge in most engineering applications. In this work, we propose a learning approach combining RL based navigation and collision avoidance scheme with low-level advanced control to bridge the sim2real gap for unmanned aerial vehicle (UAV) applications. The proposed approach puts the RL agent at the top of the control hierarchy to focus on behavioral intelligence. We demonstrate the transferability of the RL policy trained in simulation to a real UAV without randomization of the system's dynamic parameters. The direct transfer is enabled by (1) the use of deep neural networks with the modified relay feedback test (DNN-MRFT) to identify the parameters of the UAV, and (2) formulating a reward function to penalize excessive actor actions. Particularly, the RL agent generates high-level velocity actions to achieve the sought task, while the low-level controller minimizes any unwanted disturbances and model discrepancies. The proposed approach has been tested and validated using computer simulations and real-world experiments. The real-world experimental results demonstrated the agent's capability to achieve the navigation task with a $90 \%$ success rate. The experimental results can be found in this video: https://youtu.be/I1BF4mhJLLs.","1557-9603","","10.1109/TAES.2023.3294889","Khalifa University(grant numbers:CIRA-2020-082); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10182280","Obstacle avoidance;reinforcement learning;UAV navigation;unmanned aerial vehicle","Task analysis;Navigation;Training;Collision avoidance;Aerodynamics;Autonomous aerial vehicles;Robots","","","","","","","CCBY","13 Jul 2023","","","IEEE","IEEE Early Access Articles"
"Distributed Multi-Agent Deep Reinforcement Learning based Navigation and Control of UAV Swarm for Wildfire Monitoring","A. Ali; R. Ali; M. F. Baig","Department of Computer Engineering, Aligarh Muslim University, Aligarh, India; Department of Computer Engineering, Aligarh Muslim University, Aligarh, India; Department of Mechanical Engineering, Aligarh Muslim University, Aligarh, India","2023 IEEE 4th Annual Flagship India Council International Subsections Conference (INDISCON)","10 Oct 2023","2023","","","1","8","Decentralized deep reinforcement learning is an emerging and most effective approach to solve the problem of resource allocation and coordination among the swarm of UAVs. Nowadays, the use of autonomous aerial vehicles in wildfire monitoring is increasing and considered as the reasonably feasible option as surveillance in calamity-hit areas can benefit from this kind of automation. The flocks of UAVs can generate maps of affected areas which could improve the process of relief planning so that necessary aid can be reached the burnt areas quickly. This paper presents the Multi-agent Deep Q network-based technique for planning optimized trajectories for the UAV swarm which can sense the wildfire in the forests and nearby regions. In this work, the UAV agents are trained over simulated wildfires in virtually generated forests with two reward schemes. The simulation results verify the effectiveness of the proposed strategy for leveraging it in real-world scenarios.","","979-8-3503-3355-8","10.1109/INDISCON58499.2023.10270198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10270198","swarms;reinforcement learning;navigation;wildfire;decentralized control;UAV","Training;Deep learning;Surveillance;Simulation;Forestry;Reinforcement learning;Autonomous aerial vehicles","","","","","","32","IEEE","10 Oct 2023","","","IEEE","IEEE Conferences"
"Automated Penetration Testing with Fine-Grained Control through Deep Reinforcement Learning","X. Guo; J. Ren; J. Zheng; J. Liao; C. Sun; H. Zhu; T. Song; S. Wang; W. Wang","University of Electronic Science and Technology of China, Chengdu 611731, China; Peng Cheng Laboratory, Shenzhen 518000, China; University of Electronic Science and Technology of China, Chengdu 611731, China; University of Electronic Science and Technology of China, Chengdu 611731, China; University of Electronic Science and Technology of China, Chengdu 611731, China; University of Electronic Science and Technology of China, Chengdu 611731, China; University of Electronic Science and Technology of China, Chengdu 611731, China; University of Electronic Science and Technology of China, Chengdu 611731, China; Peng Cheng Laboratory, Shenzhen 518000, China","Journal of Communications and Information Networks","4 Oct 2023","2023","8","3","212","220","Penetration testing (PT) is an active method of evaluating the security of a network by simulating various types of cyber attacks in order to identify and exploit vulnerabilities. Traditional PT involves a time-consuming and labor-intensive process that is prone to errors and cannot be easily formulated. Researchers have been investigating the potential of deep reinforcement learning (DRL) to develop automated PT (APT) tools. However, using DRL in APT is challenged by partial observability of the environment and the intractability problem of the huge action space. This paper introduces RLAPT, a novel DRL approach that directly overcomes these challenges and enables intelligent automation of the PT process with precise control. The proposed method exhibits superior efficiency, stability, and scalability in finding the optimal attacking policy on the simulated experiment scenario.","2509-3312","","10.23919/JCIN.2023.10272349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10272349","APT;cyber attack;DRL","Training;Testing;Reinforcement learning;Libraries;Knowledge engineering;Servers;Security","","","","","","","","4 Oct 2023","","","PTP","PTP Journals"
"Reinforcement Learning based Droplet Routing Algorithm for Digital Microfluidic Biochips","K. Rajesh; A. Tirkey; A. Sarkar; S. Pyne","Department of Computer Science and Engineering, National Institute of Technology, Rourkela, Odisha, India; Department of Computer Science and Engineering, National Institute of Technology, Rourkela, Odisha, India; Department of Computer Science and Engineering, National Institute of Technology, Rourkela, Odisha, India; Department of Computer Science and Engineering, National Institute of Technology, Rourkela, Odisha, India","2020 24th International Symposium on VLSI Design and Test (VDAT)","10 Sep 2020","2020","","","1","6","Digital Microfluidic Biochips (DMFBs) are part of lab-on-a-chip (LOC) devices and comes under the category of micro-electro-mechanical systems (MEMS). DMFBs are designed to be an alternative for traditional biochemical laboratories. DMFBs achieve miniaturization, automation, and programmability. DMFBs use electro-wetting-on-dielectric (EWOD) property to manipulate droplets on-chip discretely. Several computer-aided design (CAD) techniques have been designed for synthesizing DMFBs to reduce design complexity. Finding the concurrent routes between all source-target pairs of a bioassay is a challenging problem and NP-Complete. We proposed a reinforcement learning based droplet routing algorithm for DMFBs. Q-learning technique is used to determine a certain predefined number of optimal paths between a source-target pair. Q-learning is an off-policy reinforcement learning algorithm. After the paths for all the source-target pairs are determined, routes will be checked for constraint violations and collisions. If any collisions or violations are found, route compaction is done using stalling and detouring. Experimental results show that our proposed droplet routing algorithm outperformed compared algorithms.","","978-1-7281-9369-4","10.1109/VDAT50263.2020.9190306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9190306","DMFB;Lab-on-a-chip;Droplet Routing;Machine Learning;Q-learning","Routing;System-on-chip;Glass;Learning (artificial intelligence);Electrodes;Heuristic algorithms","biological techniques;bioMEMS;drops;lab-on-a-chip;learning (artificial intelligence);microfluidics;network routing;optimisation;wetting","source-target pair;reinforcement learning;droplet routing algorithm;DMFB;Q-learning technique;Digital Microfluidic Biochips;lab-on-a-chip devices;microelectro-mechanical systems;biochemical laboratories;computer-aided design techniques;droplets on-chip;electro-wetting-on-dielectric property;EWOD property;NP-complete problem","","","","24","IEEE","10 Sep 2020","","","IEEE","IEEE Conferences"
"Towards Data Driven Traffic Modelling: Safe Driving Based on Reinforcement Learning","V. Kyriazopoulos; F. Orfanou; E. I. Vlahogianni; G. Yannis","School of Civil Engineering, National Technical University of Athens, Athens, GR; School of Civil Engineering, National Technical University of Athens, Athens, GR; School of Civil Engineering, National Technical University of Athens, Athens, GR; School of Civil Engineering, National Technical University of Athens, Athens, GR","2021 IEEE International Intelligent Transportation Systems Conference (ITSC)","25 Oct 2021","2021","","","3156","3161","In recent years, the evolution of technology has allowed the introduction of automation in vehicles, that improve road safety by reducing the contribution of the human factor to the driving process. The objective of this research is to propose a reinforcement learning algorithm for controlling driving behaviour with the aim to improve safety and comfort. The learning is based on detailed trajectory data from a highly visited signalized arterial in the Athens downtown area. The safe and comfortable driving profiles are identified from the trajectory data. Next, a simple Q-learning algorithm is developed and various combinations of the exploration rate, the discount factor y and the learning rate were tested for the optimal parameterization. The final Q-Table can be used inside vehicles for collision avoidance in order to improve road safety. Results indicate that the algorithm converges fast and is trained efficiently to response to unseen conditions. Further training in extreme events or adverse weather conditions will increase the generalisability of the proposed safe driving assistance framework.","","978-1-7281-9142-3","10.1109/ITSC48978.2021.9564829","European Commission(grant numbers:MG-3.3.2018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564829","","Training;Reinforcement learning;Road safety;Data models;Trajectory;Safety;Intelligent transportation systems","collision avoidance;driver information systems;human factors;learning (artificial intelligence);road safety;road traffic;traffic engineering computing","towards data driven traffic modelling;road safety;human factor;driving process;reinforcement learning algorithm;driving behaviour;detailed trajectory data;highly visited signalized arterial;Athens downtown area;safe driving profiles;comfortable driving profiles;simple Q-learning algorithm;exploration rate;discount factor y;learning rate;optimal parameterization;safe driving assistance framework","","","","36","IEEE","25 Oct 2021","","","IEEE","IEEE Conferences"
"Self-Driving Car Simulation Using Reinforcement Learning and Xception Model Tuning","A. Balasubramaniam; R. K. Singh; A. K. Tyagi","School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, Tamilnadu, India; School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, Tamilnadu, India; Department of Fashion Technology, National Institute of Fashion Technology, New Delhi, Delhi, India","2023 4th International Conference on Smart Electronics and Communication (ICOSEC)","16 Oct 2023","2023","","","1293","1302","The main idea is to have the environment (server) and then agents (clients). The server/client architecture means that the user can of course run both the server and client locally on the same machines, but it could also run the environment (server) on one machine and multiple clients on multiple other machines. For training and predicting at the same time, reinforcement learning is used. To obtain a convincing result in the training process would mean promising improvement in the field of automation and automotives. This simulation will be very helpful in all manners when developed with more real-life objects and circumstances that get into the simulation asset. The improvement in self-driving cars comes along with the advancement in the field of deep learning and data handling. Due to the sheer amount of multidimensional data that a self-driving car could create, a more efficient data handling methods are required to compute better results. There is always a chance to add layers to this particular deep learning framework and make it more efficient as the data captured evolves. The data may evolve in the fields of addition in new sensors, new obstacle identification, addition of more real time features. Progressing further into this study and implementation of a deep learning framework for self-driving cars, it is observed that the sensor data provides sufficient parameters for the estimation of new class variables. The model accuracy and results are compared in a way that is projected towards higher volume of data along with high computational power. This study recommends using cloud service to accommodate the graphical and CPU requirements as the computation for this particular multidimensional data could be very demanding. The understanding of how autonomous vehicles and how well it would be accepted is discussed below, along with the complications it would bring. It's a social change which many will oppose in the beginning but will slowly adopt as science improves and moves forward.","","979-8-3503-0088-8","10.1109/ICOSEC58147.2023.10276222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10276222","Autonomous Vehicle;Deep learning;Sensors;Scalability;Self Driving Car","Training;Deep learning;Data handling;Reinforcement learning;Computer architecture;Sensor phenomena and characterization;Real-time systems","","","","","","40","IEEE","16 Oct 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Control Design for a Floating Piston Pneumatic Gearbox Actuator","T. Bécsi; Á. Szabó; B. Kővári; S. Aradi; P. Gáspár","Department of Control for Transportation and Vehicle Systems, Budapest University of Technology and Economics, Budapest, Hungary; Department of Control for Transportation and Vehicle Systems, Budapest University of Technology and Economics, Budapest, Hungary; Department of Control for Transportation and Vehicle Systems, Budapest University of Technology and Economics, Budapest, Hungary; Department of Control for Transportation and Vehicle Systems, Budapest University of Technology and Economics, Budapest, Hungary; Department of Control for Transportation and Vehicle Systems, Budapest University of Technology and Economics, Budapest, Hungary","IEEE Access","18 Aug 2020","2020","8","","147295","147312","Electro-pneumatic actuators play an essential role in various areas of the industry, including heavy-duty vehicles. This article deals with the control problem of an Automatic Manual Transmission, where the actuator of the system is a double-acting floating-piston cylinder, with dedicated inner-position. During the control design of electro-pneumatic cylinders, one must implement a set-valued control on a nonlinear system, when, as in the present case, non-proportional valves provide the airflow. As both the system model itself and the qualitative control goals can be formulated as a Partially Observable Markov Decision Process, Machine learning frameworks are a conspicuous choice for handling such control problems. To this end, six different solutions are compared in the article, of which a new agent named PG-MCTS, using a modified version of the “Upper Confidence bound for Trees” algorithm, is also presented. The performance and strategic choice comparison of the six methods are carried out in a simulation environment. Validation tests performed on an actual transmission system and implemented on an automotive control unit to prove the applicability of the concept. In this case, a Policy Gradient agent, selected by implementation and computation capacity restrictions. The results show that the presented methods are suitable for the control of floating-piston cylinders and can be extended to other fluid mechanical actuators, or even different set-valued nonlinear control problems.","2169-3536","","10.1109/ACCESS.2020.3015576","Higher Education Excellence Program in the frame of Artificial Intelligence research area of Budapest University of Technology and Economics (BME FIKP-MI/FM); Hungarian Government and co-financed by the European Social Fund (Talent management in autonomous vehicle control technologies)(grant numbers:EFOP-3.6.3-VEKOP-16-2017-00001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9163347","Intelligent agents;machine learning;pneumatic actuators;reinforcement learning;supervised learning;system testing","Valves;Pistons;Learning (artificial intelligence);Machine learning;Pneumatic actuators;Solenoids","closed loop systems;control engineering computing;control system synthesis;electropneumatic control equipment;gears;intelligent control;learning (artificial intelligence);linear quadratic control;Markov processes;mechanical engineering computing;nonlinear control systems;pistons;pneumatic actuators;position control;power transmission (mechanical);servomechanisms;valves","control design;floating piston pneumatic gearbox actuator;electro-pneumatic actuators;heavy-duty vehicles;control problem;Automatic Manual Transmission;floating-piston cylinder;dedicated inner-position;electro-pneumatic cylinders;nonlinear system;nonproportional valves;system model;qualitative control goals;Partially Observable Markov Decision Process;Machine learning frameworks;strategic choice comparison;actual transmission system;automotive control unit;computation capacity restrictions;fluid mechanical actuators;set-valued nonlinear control problems;reinforcement learning","","9","","53","CCBY","10 Aug 2020","","","IEEE","IEEE Journals"
"Deep-Reinforcement-Learning-Based Energy-Efficient Resource Management for Social and Cognitive Internet of Things","H. Yang; W. -D. Zhong; C. Chen; A. Alphones; X. Xie","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Chongqing Key Laboratory of Computer Network and Communication Technology, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Internet of Things Journal","12 Jun 2020","2020","7","6","5677","5689","Internet of Things (IoT) has attracted much interest due to its wide applications, such as smart city, manufacturing, transportation, and healthcare. Social and cognitive IoT is capable of exploiting social networking characteristics to optimize network performance. Considering the fact that the IoT devices have different Quality-of-Service (QoS) requirements [ranging from ultrareliable and low-latency communications (URLLCs) to minimum data rate], this article presents a QoS-driven social-aware-enhanced device-to-device (D2D) communication network model for social and cognitive IoT by utilizing social orientation information. We model the optimization problem as a multiagent reinforcement learning formulation, and a novel coordinated multiagent deep-reinforcement-learning-based resource management approach is proposed to optimize the joint radio block assignment and the transmission power control strategy. Meanwhile, the prioritized experience replay (PER) and the coordinated learning mechanisms are employed to enable communication links to work cooperatively in a distributed manner, which enhances the network performance and access success probability. The simulation results corroborate the superiority in the performance of the presented resource management approach, and it outperforms other existing approaches in terms of meeting the energy efficiency and the QoS requirements.","2327-4662","","10.1109/JIOT.2020.2980586","Delta-NTU Corporate Laboratory for Cyber-Physical Systems; Delta Electronics Inc.; National Research Foundation Singapore under the Corp Lab@University Scheme; National Natural Science Foundation of China(grant numbers:61901065,61502067); Key Research Project of Chongqing Education Commission(grant numbers:KJZD-K201800603); Key Project of Science and Technology Research of Chongqing Education Commission(grant numbers:KJZD-M201900602); Chongqing Nature Science Foundation(grant numbers:CSTC2018jcyjAX0432,CSTC2016jcyjA0455); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9035447","Device-to-device (D2D) communication;deep reinforcement learning (DRL);energy efficiency (EE);Internet of Things (IoT);Quality of Service (QoS);resource management;social awareness","Device-to-device communication;Quality of service;Resource management;Social networking (online);Internet of Things;Optimization;Reliability","computer network management;Internet of Things;learning (artificial intelligence);mobile radio;multi-agent systems;optimisation;power control;quality of service;resource allocation;telecommunication computing;telecommunication control","QoS requirements;energy efficiency;resource management approach;communication links;prioritized experience replay;transmission power control;radio block assignment;optimization problem;social IoT;smart city;multiagent deep-reinforcement-learning;social orientation information;device-to-device communication;low-latency communications;quality-of-service requirements;IoT devices;social networking characteristics;cognitive IoT;cognitive Internet of Things;deep-reinforcement-learning;resource management;coordinated learning mechanisms","","37","","41","IEEE","13 Mar 2020","","","IEEE","IEEE Journals"
"UAV Aided Search and Rescue Operation Using Reinforcement Learning","S. Kulkarni; V. Chaphekar; M. M. Uddin Chowdhury; F. Erden; I. Guvenc","Dept. Electrical & Computer Eng., NC State University, Raleigh, NC; Dept. Electrical & Computer Eng., NC State University, Raleigh, NC; Dept. Electrical & Computer Eng., NC State University, Raleigh, NC; Dept. Electrical & Computer Eng., NC State University, Raleigh, NC; Dept. Electrical & Computer Eng., NC State University, Raleigh, NC","2020 SoutheastCon","25 Mar 2021","2020","2","","1","8","Owing to the enhanced flexibility in deployment and decreasing costs of manufacturing, the demand for unmanned aerial vehicles (UAVs) is expected to soar in the upcoming years. In this paper, we explore a UAV aided search and rescue (SAR) operation in indoor environments, where the GPS signals might not be reliable. We consider a SAR scenario where the UAV tries to locate a victim trapped in an indoor environment by sensing the RF signals emitted from a smart device owned by the victim. To locate the victim as fast as possible, we leverage tools from reinforcement learning (RL). Received signal strength (RSS) at the UAV depends on the distance from the source, indoor shadowing and fading parameters, and antenna radiation pattern of the receiver mounted on the UAV. To make our analysis more realistic, we model two indoor scenarios with different dimensions using a commercial ray tracing software. Then, the corresponding RSS values at each possible discrete UAV location are extracted and used in a Q-learning framework. Unlike the traditional location-based navigation approach that exploits GPS coordinates, our method uses the RSS to define the states and rewards of the RL algorithm. We compare the performance of the proposed method where directional and omnidirectional antennas are used. The results reveal that the use of directional antennas provides faster convergence rates than the omnidirectional antennas.","1558-058X","978-1-7281-6861-6","10.1109/SoutheastCon44009.2020.9368285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9368285","Directional antenna;drone;Q-learning;navigation;ray tracing;RSS;unmanned aerial vehicle (UAV)","RF signals;Directional antennas;Omnidirectional antennas;Unmanned aerial vehicles;Indoor environment;Floors;Convergence","antenna radiation patterns;autonomous aerial vehicles;directive antennas;Global Positioning System;indoor radio;learning (artificial intelligence);mobile computing;mobile robots;multi-robot systems;ray tracing;remotely operated vehicles;synthetic aperture radar","reinforcement learning;unmanned aerial vehicles;indoor environment;GPS signals;SAR scenario;RF signals;received signal strength;indoor shadowing;fading parameters;indoor scenarios;possible discrete UAV;traditional location-based navigation approach","","13","","21","IEEE","25 Mar 2021","","","IEEE","IEEE Conferences"
"REPlanner: Efficient UAV Trajectory-Planning using Economic Reinforcement Learning","A. A. Khalil; A. J. Byrne; M. A. Rahman; M. H. Manshaei","Analytics for Cyber Defense (ACyD) Lab, Florida International University, Miami, USA; Analytics for Cyber Defense (ACyD) Lab, Florida International University, Miami, USA; Analytics for Cyber Defense (ACyD) Lab, Florida International University, Miami, USA; Analytics for Cyber Defense (ACyD) Lab, Florida International University, Miami, USA","2021 IEEE International Conference on Smart Computing (SMARTCOMP)","8 Oct 2021","2021","","","153","160","Advances in the unmanned aerial vehicle (UAV) design and capability, as well as decreases in the manufacturing cost, have opened up applications of UAVs in various fields, including surveillance, firefighting, cellular networks, and delivery purposes. The uniqueness of UAVs in systems creates a novel set of trajectory or path planning and coordination problems. Environments include many more points of interest (POIs) than UAVs, with obstacles and no-fly zones. We introduce REPlanner, a novel multi-agent reinforcement learning algorithm inspired by economic transactions to distribute tasks among UAVs. This system revolves around an economic theory, in particular an auction mechanism where UAVs trade assigned POIs. We formulate the path planning problem as a multi-agent economic game, where agents can cooperate and compete for resources. We then translate the problem into a partially observable Markov decision process (POMDP), which is solved using a reinforcement learning (RL) model deployed on each agent. As the system computes task distributions via UAV cooperation, it is highly resilient to any change in the swarm size. Our proposed network and economic game architecture can effectively coordinate the swarm as an emergent phenomenon while maintaining the swarm’s operation. Evaluation results prove that REPlanner efficiently outperforms conventional RL-based trajectory search.","2693-8340","978-1-6654-1252-0","10.1109/SMARTCOMP52413.2021.00041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9556271","Unmanned aerial vehicles;reinforcement learning;path planning;trajectory optimization;swarm robotics","Economics;Training;Surveillance;Reinforcement learning;Games;Markov processes;Unmanned aerial vehicles","autonomous aerial vehicles;learning (artificial intelligence);Markov processes;multi-agent systems;multi-robot systems;path planning;remotely operated vehicles;search problems","multiagent economic game;reinforcement learning model;UAV cooperation;economic game architecture;REPlanner;efficient UAV trajectory-planning using economic reinforcement learning;coordination problems;novel multiagent reinforcement learning algorithm;economic transactions;economic theory;UAVs trade;path planning problem","","7","","27","IEEE","8 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow Based QoE","W. Quan; Y. Pan; B. Xiang; L. Zhang","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China","2020 IEEE 18th International Conference on Industrial Informatics (INDIN)","7 Jun 2021","2020","1","","231","236","With the merit of containing full panoramic content in one camera, Virtual Reality (VR) and 360° videos have arisen in the field of industrial cloud manufacturing and training. Industrial Internet of Things (IoT), where many VR terminals needed to be online at the same time, can hardly guarantee VR's bandwidth requirement. However, by making use of users' quality of experience (QoE) awareness factors, including the relative moving speed and depth difference between the viewpoint and other content, bandwidth consumption can be reduced. In this paper, we propose Optical Flow Based VR(OFB-VR), an interactive method of VR streaming that can make use of VR users' QoE awareness to ease the bandwidth pressure. The Just-Noticeable Difference through Optical Flow Estimation (JND-OFE) is explored to quantify users' awareness of quality distortion in 360° videos. Accordingly, a novel 360° videos QoE metric based on Peak Signal-to-Noise Ratio and JND-OFE (PSNR-OF) is proposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling scheme to lessen the tiling overhead. A Reinforcement Learning (RL) method is implemented to make use of historical data to perform Adaptive BitRate (ABR). For evaluation, we take two prior VR streaming schemes, Pano and Plato, as baselines. Vast evaluations show that our system can increase the mean PSNR-OF score by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano and Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that OFB-VR is a promising prototype for actual interactive industrial VR. A prototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.","2378-363X","978-1-7281-4964-6","10.1109/INDIN45582.2020.9442114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442114","VR;interactive 360° video;adaptive;optical flow","Measurement;Prototypes;Bandwidth;Reinforcement learning;Optical distortion;Virtual reality;Quality of experience","cloud computing;image sequences;Internet;Internet of Things;learning (artificial intelligence);Long Term Evolution;motion estimation;quality of experience;video coding;video streaming;virtual reality","Reinforcement Learning driven Adaptive VR streaming;industrial cloud;VR terminals;VR's bandwidth requirement;experience awareness factors;relative moving speed;depth difference;VR users;Optical Flow Estimation;JND-OFE;prior VR streaming schemes;actual interactive industrial VR","","5","","13","IEEE","7 Jun 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Securing Software-Defined Industrial Networks With Distributed Control Plane","J. Wang; J. Liu; H. Guo; B. Mao","National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Cybersecurity, Northwestern Polytechnical University, Xi’an, China; National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Cybersecurity, Northwestern Polytechnical University, Xi’an, China; National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Cybersecurity, Northwestern Polytechnical University, Xi’an, China; National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Cybersecurity, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Industrial Informatics","21 Feb 2022","2022","18","6","4275","4285","The development of software-defined industrial networks (SDIN) promotes the programmability and customizability of the industrial networks and is suitable to cope with the challenges brought by new manufacturing modes. For building more scalable and reliable SDIN, a distributed control plane with multicontroller collaboration becomes a promising option. However, as the brain of SDIN, the security of the distributed control plane is rarely considered. In addition to suffering direct attacks, each controller is also subjected to attacks propagated by other controllers because of information sharing or management domain takeover, resulting in the spread of attacks in a wider range than a single controller. Therefore, in this article, we study attacks against SDIN with distributed control plane, demonstrate their propagation across multiple controllers, and analyze their impacts. To the best of our knowledge, we are the first to study the security of SDIN with distributed control plane. In addition, since the existing defense mechanisms are not specifically designed for distributed SDIN and cannot defend it perfectly, we propose an attack mitigation scheme based on deep reinforcement learning to adaptively prevent the spread of attacks. Specifically, the novelty of our scheme lies in its ability of learning from the environment and flexibly adjusting the switch takeover decisions to isolate the attack source, so as to tolerate attacks and enhance the resilience of SDIN.","1941-0050","","10.1109/TII.2021.3128581","National Natural Science Foundation of China(grant numbers:62001393); Natural Science Basic Research Program of Shaanxi(grant numbers:2020JC-15); Fundamental Research Funds for the Central Universities(grant numbers:D5000210817); Xi'an Unmanned System Security and Intelligent Communications ISTC Center; Special Funds for Central Universities Construction of World-Class Universities (Disciplines) and Special Development Guidance(grant numbers:0639021GH0201024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9618870","Deep reinforcement learning (DRL);industrial networks;network security;software-defined networking (SDN)","Control systems;Security;Decentralized control;Topology;Network topology;Reliability;Informatics","computer network security;deep learning (artificial intelligence);reinforcement learning;software defined networking","deep reinforcement learning;distributed control plane;distributed SDIN;secured software-defined industrial networks;programmability;customizability;attack mitigation scheme;switch takeover decisions","","3","","31","IEEE","17 Nov 2021","","","IEEE","IEEE Journals"
"A Deep Reinforcement Learning-Based Offloading Scheme for Multi-Access Edge Computing-Supported eXtended Reality Systems","B. Trinh; G. -M. Muntean","Insight SFI Research Centre for Data Analytics and Performance Engineering Laboratory, School of Electronic Engineering, Dublin City University, 9 Dublin, Ireland; Insight SFI Research Centre for Data Analytics and Performance Engineering Laboratory, School of Electronic Engineering, Dublin City University, 9 Dublin, Ireland","IEEE Transactions on Vehicular Technology","13 Jan 2023","2023","72","1","1254","1264","In recent years, eXtended Reality (XR) applications have been widely employed in various scenarios, e.g., health care, education, manufacturing, etc. Such application are now easily accessible via mobile phones, tablets, or wearable devices. However, such devices normally suffer from constraints in terms of battery capacity and processing power, limiting the range of applications supported or lowering Quality of Experience. One effective way to address these issues is to offload the computation tasks to the edge servers that are deployed at the network edges, e.g., base stations or WiFi access point, etc. This communication fashion, also named as Multi-access Edge Computing (MEC), is proposed to overcome the limitations in terms of long latency due to long propagation distance of traditional cloud computing approach. XR devices, that are limited in computation resources and energy, can then benefit from offloading the computation intensive tasks to MEC servers. However, as XR applications are comprised of multiple tasks with variety of requirements in terms of latency and energy consumption, it is important to make decision whether one task should be offloaded to MEC server or not. This paper proposes a Deep Reinforcement Learning-based offloading scheme for XR devices (DRLXR). The proposed scheme is used to train and derive the close-to optimal offloading decision whereas optimizing a utility function optimization equation that considers both energy consumption and execution delay at XR devices. The simulation results show how our proposed scheme outperforms the other counterparts in terms of total execution latency and energy consumption.","1939-9359","","10.1109/TVT.2022.3207692","Science Foundation Ireland(grant numbers:12/RC/2289_P2); Insight SFI Research Centre for Data Analytics; European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9894514","Deep reinforcement learning;energy efficiency;eXtended reality;multi-access edge computing;offloading;quality of service","X reality;Cloud computing;Servers;Task analysis;Mobile handsets;Energy consumption;Optimization","cloud computing;health care;learning (artificial intelligence);mobile computing;mobile handsets;optimisation;wireless LAN","access point;base stations;battery capacity;computation intensive tasks;computation resources;computation tasks;Deep Reinforcement Learning-based offloading scheme;edge servers;energy consumption;eXtended Reality applications;health care;long propagation distance;MEC server;mobile phones;Multiaccess Edge Computing-supported;optimal offloading decision;processing power;Quality of Experience;Reality systems;traditional cloud computing approach;wearable devices;XR applications;XR devices","","2","","48","CCBY","19 Sep 2022","","","IEEE","IEEE Journals"
"Adaptive Event-based Reinforcement Learning Control","F. Meng; A. An; E. Li; S. Yang","College of Electrical and Information Engineering, Lanzhou University of Technology, Lanzhou, China; College of Electrical and Information Engineering, Lanzhou University of Technology, Lanzhou, China; National Experimental Teaching Center of Electrical and Control Engineering, Lanzhou, China; College of Electrical and Information Engineering, Lanzhou University of Technology, Lanzhou, China","2019 Chinese Control And Decision Conference (CCDC)","12 Sep 2019","2019","","","3471","3476","Reinforcement learning (RL) methods have been successfully used to deal with the control and/or decision making problem of many engineering domains, such as industrial manufacturing, power management, industrial robot and rehabilitation robotic system, etc. However, the state-based method comes into difficulty when solving the control and/or decision making of high dimensional systems due to the computation loads and storage requirements. In addition, to acquire better control results, the state based RL method often requests as many states as possible to be exploited and explored, thus this method, in effect, does not suit to solve the control problems of those unknown and/or partial known systems. To solve those problems, a new adaptive event-based reinforcement learning algorithm (ETRL) is proposed in the paper. In the proposed ETRL approach, an event generator is employed firstly to sample a set of states (i.e., event state, shortened to ES in the paper) by effective event sampling strategies from the unknown system state space. Then Q learning controller uses the ES and ES-based reinforcement signal (i.e., reward feedback) to guide and adjust the control law. Moreover, an adaptive weighted nearest neighbor and sample reuse method (WNNSR) to sample the most sensitive actions is proposed to guarantee both control performance and stability of the proposed ETRL during the process of learning. Finally, convergence analysis verifies the proposed ETRL approach.","1948-9447","978-1-7281-0106-4","10.1109/CCDC.2019.8832922","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8832922","Reinforcement Learning;Event State;ETRL;WNNSR;Sample Reuse","Aerospace electronics;Reinforcement learning;Decision making;Generators;Process control;Convergence;Observers","adaptive control;control engineering computing;decision making;learning (artificial intelligence)","adaptive event-based reinforcement learning control;decision making problem;engineering domains;state-based method;high dimensional systems;ETRL approach;event generator;unknown system state space;Q learning controller;ES-based reinforcement signal;control law;adaptive weighted nearest neighbor","","1","","20","IEEE","12 Sep 2019","","","IEEE","IEEE Conferences"
"Reinforcement Learning with Discrete Event Simulation: The Premise, Reality, and Promise","S. Belsare; E. D. Badilla; M. Dehghanimohammadabadi","Mechanical and Industrial Engineering Department, Northeastern University, Boston, MA, USA; Mechanical and Industrial Engineering Department, Northeastern University, Boston, MA, USA; Mechanical and Industrial Engineering Department, Northeastern University, Boston, MA, USA","2022 Winter Simulation Conference (WSC)","23 Jan 2023","2022","","","2724","2735","Several studies have shown the success of Reinforcement Learning (RL) for solving sequential decision-making problems in domains like robotics, autonomous vehicles, manufacturing, supply chain, and health care. For such applications, uncertainty in real-life environments presents a significant challenge in training an RL agent. RL requires a large number of trials (training examples) to learn a good policy. One of the approaches to tackle these obstacles is augmenting RL with a Discrete Event Simulation (DES) model. Learning from a simulated environment, makes the training process of the RL agent more efficient, faster, and even safer by alleviating the need for expensive real-world trials. Therefore, integrating RL algorithms with simulation environments has inspired many researchers in recent years. In this paper, we analyze the existing literature on RL models using DES to put forward the benefits, application areas, challenges, and scope for future work in developing such models for industrial use cases.","1558-4305","978-1-6654-7661-4","10.1109/WSC57314.2022.10015503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015503","","Training;Analytical models;Uncertainty;Supply chains;Decision making;Reinforcement learning;Medical services","decision making;discrete event simulation;reinforcement learning","autonomous vehicles;DES;Discrete Event Simulation model;health care;real-life environments;Reinforcement Learning;RL agent;RL algorithms;RL models;sequential decision-making problems;simulated environment;supply chain;training process","","1","","27","IEEE","23 Jan 2023","","","IEEE","IEEE Conferences"
"Balancing Awareness Fast Charging Control for Lithium-Ion Battery Pack Using Deep Reinforcement Learning","Y. Yang; J. He; C. Chen; J. Wei","School of Management and Engineering, Nanjing University, Nanjing, China; School of Management and Engineering, Nanjing University, Nanjing, China; School of Management and Engineering, Nanjing University, Nanjing, China; School of Management and Engineering, Nanjing University, Nanjing, China","IEEE Transactions on Industrial Electronics","","2023","PP","99","1","10","Minimizing charging time without damaging the batteries is significantly crucial for the large-scale penetration of electric vehicles. However, charging inconsistency caused by inevitable manufacture and usage inconsistencies can lead to lower efficiency, capacity, and shorter durability due to the “cask effect”. This goal can be achieved by solving a series of constrained optimization problems with the model-based framework. Nevertheless, the high computational complexity, identifiability, and observability issues still limit their fidelity and robustness. To overcome these limitations and provide end-to-end learning strategies, this paper proposes a balancing-aware fast-charging control framework based on deep reinforcement learning. In particular, a cell-to-pack equalization topology is first introduced to dispatch energy among in-pack cells. Then, the balancing awareness fast charging problem is formulated as a multiobjective optimization problem by considering charging time, consistency, and over-voltage safety constraints. Thirdly, a deep reinforcement learning framework using a deep Q-network is established to find the optimal policy. By using the generalization of a neural network, the learned policies can be transferred to real-time charging and balancing control, thus improving the applicability of the proposed strategy. Finally, numerous comparative simulations and experimental results illustrate its effectiveness and superiority in terms of charging rapidity and balancing.","1557-9948","","10.1109/TIE.2023.3274853","National Natural Science Foundation of China(grant numbers:62203209); Fundamental Research Funds for the Central Universities(grant numbers:0221-14380010); Natural Science Foundation of Jiangsu(grant numbers:BK20200333); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10124815","Cell balancing;fast charging;lithium-ion battery pack;reinforcement learning","Batteries;Computational modeling;Optimization;Integrated circuit modeling;Safety;Electric vehicles;Behavioral sciences","","","","","","","IEEE","15 May 2023","","","IEEE","IEEE Early Access Articles"
"Multi-Agent Reinforcement Learning Control of a Hydrostatic Wind Turbine-Based Farm","Y. Huang; S. Lin; X. Zhao","Intelligent Control & Smart Energy (ICSE) Research Group, School of Engineering, University of Warwick, Coventry, U.K.; Department of Engineering, University of Hull, Hull, U.K.; Intelligent Control & Smart Energy (ICSE) Research Group, School of Engineering, University of Warwick, Coventry, U.K.","IEEE Transactions on Sustainable Energy","20 Sep 2023","2023","14","4","2406","2416","This paper leverages multi-agent reinforcement learning (MARL) to develop an efficient control system for a wind farm comprising a new type of wind turbines with hydrostatic transmission. The primary motivation for hydrostatic wind turbines (HWT) is increased reliability, and reduced manufacturing, operating, and maintaining costs by removing troublesome components and reducing nacelle weight. Nevertheless, the high system complexity of HWT and the wake effect pose significant challenges for the control of HWT-based wind farms. We therefore propose a MARL algorithm named multi-agent policy optimization (MAPO), which allows agents (turbines) to gradually improve their control policies by repeatedly interacting with the environment to learn an optimal operation curve for wind farms. Simulation results based on a wind farm simulator, FAST.Farm, show that MAPO outperforms the greedy policy and a popular learning-based method, multi-agent deep deterministic policy gradient (MADDPG), in terms of power generation.","1949-3037","","10.1109/TSTE.2023.3270761","European Union's Horizon 2020 Research and Innovation Program; Marie Sklodowska-Curie(grant numbers:861398); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10109125","Wind farm control;hydrostatic wind turbines;multi-agent reinforcement learning;power generation","Wind farms;Wind turbines;Reinforcement learning;Power generation","deep learning (artificial intelligence);gradient methods;hydrostatics;learning (artificial intelligence);multi-agent systems;reinforcement learning;telecommunication computing;wind power plants;wind turbines","control policies;efficient control system;FAST.Farm;high system complexity;HWT-based wind farms;hydrostatic transmission;hydrostatic wind turbine-based Farm;hydrostatic wind turbines;multiagent deep deterministic policy gradient;multiagent policy optimization;multiagent reinforcement learning control;paper leverages multiagent reinforcement learning;popular learning-based method;reducing nacelle weight;troublesome components;wind farm","","","","22","IEEE","26 Apr 2023","","","IEEE","IEEE Journals"
"A Novel FPGA-Based Circuit Simulator for Accelerating Reinforcement Learning-Based Design of Power Converters","Z. Xu; M. Yu; J. Cai; Q. Yang; Y. Jeong; T. Wei","Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, US; Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, US; Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, US; Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, US; Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, US; Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, US","2023 IEEE 34th International Conference on Application-specific Systems, Architectures and Processors (ASAP)","2 Oct 2023","2023","","","1","9","High-efficiency energy conversion systems have become increasingly important due to their wide use in all electronic systems such as data centers, smart mobile devices, E-vehicles, medical instruments, and so forth. Complex and interdependent parameters make optimal designs of power converters challenging to get. Recent research has shown that machine learning (ML) algorithms, such as reinforcement learning (RL), show great promise in design of such converter circuits. A trained RL agent can search for optimal design parameters for power conversion circuit topologies under targeted application requirements. Training an RL agent requires numerous circuit simulations. It requires significantly more training iterations when the tolerance of circuit components due to manufacturing inconsistency, aging, and temperature variation is considered. As a result, they may take days to complete, primarily because of the slow time-domain circuit simulation. This paper proposes a new FPGA architecture that accelerates the circuit simulation and hence substantially speeds up the RL-based design method for power converters. Our new architecture supports all power electronic circuit converters and their variations. It substantially improves the training speed of RL-based design methods. High-level synthesis (HLS) was used to build the accelerator on Amazon Web Service (AWS) F1 instance. An AWS virtual PC hosts the training algorithm. The host interacts with the FPGA accelerator by updating the circuit parameters, initiating simulation, and collecting the simulation results during training iterations. A script was created on the host side to facilitate this design method to convert a netlist containing circuit topology and parameters into core matrices in the FPGA accelerator. Experimental results showed $\mathbf{60}\times$ overall speedup of our RL-based design method in comparison with using a popular commercial simulator, PowerSim.","2160-052X","979-8-3503-4685-5","10.1109/ASAP57973.2023.00013","Office of Naval Research (ONR)(grant numbers:N00014-17-1-2632); National Science Foundation (NSF)(grant numbers:2027069,2106750); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10265715","Power electronics;Simulation;Reinforcement learning;FPGA accelerator","Training;Circuit topology;Technological innovation;Web services;Design methodology;Circuit simulation;Computer architecture","","","","","","24","IEEE","2 Oct 2023","","","IEEE","IEEE Conferences"
"APT: Adaptive Perceptual quality based camera Tuning using reinforcement learning","S. Paul; K. Rao; G. Coviello; M. Sankaradas; O. Po; Y. C. Hu; S. Chakradhar","Purdue University, West Lafayette, PA, USA; NEC Laboratories America,Inc., Princeton, NJ, USA; NEC Laboratories America,Inc., Princeton, NJ, USA; NEC Laboratories America,Inc., Princeton, NJ, USA; NEC Laboratories America,Inc., Princeton, NJ, USA; Purdue University, West Lafayette, PA, USA; NEC Laboratories America,Inc., Princeton, NJ, USA","2022 9th International Conference on Internet of Things: Systems, Management and Security (IOTSMS)","13 Mar 2023","2022","","","1","9","Cameras are increasingly being deployed in cities, enterprises and roads world-wide to enable many applications in public safety, intelligent transportation, retail, healthcare and manufacturing. Often, after initial deployment of the cameras, the environmental conditions and the scenes around these cameras change, and our experiments show that these changes can adversely impact the accuracy of insights from video analytics. This is because the camera parameter settings, though optimal at deployment time, are not the best settings for good-quality video capture as the environmental conditions and scenes around a camera change during operation. Capturing poor-quality video adversely affects the accuracy of analytics. To mitigate the loss in accuracy of insights, we propose a novel, reinforcement-learning based system APT that dynamically, and remotely (over 5G networks), tunes the camera parameters, to ensure a high-quality video capture, which mitigates any loss in accuracy of video analytics. As a result, such tuning restores the accuracy of insights when environmental conditions or scene content change. APT uses reinforcement learning, with no-reference perceptual quality estimation as the reward function. We conducted extensive real-world experiments, where we simultaneously deployed two cameras side-by-side overlooking an enterprise parking lot (one camera only has manufacturer-suggested default setting, while the other camera is dynamically tuned by APT during operation). Our experiments demonstrated that due to dynamic tuning by APT, the analytics insights are consistently better at all times of the day: the accuracy of object detection video analytics application was improved on average by ∼ 42%. Since our reward function is independent of any analytics task, APT can be readily used for different video analytics tasks.","2832-3033","979-8-3503-2045-9","10.1109/IOTSMS58070.2022.10062226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10062226","","Degradation;Visual analytics;Urban areas;Reinforcement learning;Object detection;Cameras;Quality assessment","5G mobile communication;cameras;learning (artificial intelligence);object detection;reinforcement learning;video signal processing","adaptive perceptual quality;analytics insights;analytics task;APT;camera change;camera parameter settings;camera parameters;camera tuning;cameras change;capturing poor-quality video adversely;deployment time;different video analytics tasks;dynamic tuning;environmental conditions;good-quality video capture;healthcare;high-quality video capture;initial deployment;intelligent transportation;manufacturer-suggested default setting;no-reference perceptual quality estimation;object detection video analytics application;real-world experiments;reinforcement learning;roads world-wide;scene content change","","","","37","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Residential Load Scheduling With Renewable Generation in the Smart Grid: A Reinforcement Learning Approach","T. Remani; E. A. Jasmin; T. P. I. Ahamed","Department of Electrical and Electronics Engineering, Government Engineering College, Thrissur, India; Department of Electrical and Electronics Engineering, Government Engineering College, Thrissur, India; Thangal Kunju Musaliar College of Engineering, Kollam, India","IEEE Systems Journal","23 Aug 2019","2019","13","3","3283","3294","The significance and need of demand response (DR) programs is realized by the utility as a means to reduce the additional production cost imposed by the accelerating energy demand. With the development in smart information and communication systems, the price-based DR programs can be effectively utilized for controlling the loads of smart residential buildings. Nowadays, the use of stochastic renewable energy sources like photovoltaic (PV) by a small domestic consumer is increasing. In this paper, a generalized model for the residential load scheduling or load commitment problem (LCP) in the presence of renewable sources for any type of tariff is presented. Reinforcement learning (RL) is an efficient tool that has been used to solve the decision making problem under uncertainty. An RL-based approach to solve the LCP is also proposed. The novelty of this paper lies in the introduction of a comprehensive model with implementable solution considering consumer comfort, stochastic renewable power, and tariff. Simulation experiments are conducted to test the efficacy and scalability of the proposed algorithm. The performance of the algorithm is investigated by considering a domestic consumer with schedulable and nonschedulable appliances along with a PV source. Guidelines are given for choosing the parameters of the load.","1937-9234","","10.1109/JSYST.2018.2855689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8426048","Demand response (DR);distributed generation (DG);load scheduling;photovoltaic (PV) source;reinforcement learning (RL);smart grid","Load modeling;Optimal scheduling;Mathematical model;Pricing;Scheduling;Temperature;Smart grids","decision making;demand side management;learning (artificial intelligence);optimisation;power generation economics;power markets;pricing;renewable energy sources;smart power grids","price-based DR programs;smart residential buildings;stochastic renewable energy sources;residential load scheduling;load commitment problem;decision making problem;renewable generation;smart grid;reinforcement learning;demand response programs;consumer comfort","","58","","37","IEEE","6 Aug 2018","","","IEEE","IEEE Journals"
"Network Abnormal Traffic Detection Model Based on Semi-Supervised Deep Reinforcement Learning","S. Dong; Y. Xia; T. Peng","School of Computer Science and Technology, Zhoukou Normal University, Zhoukou, China; School of Computer Science and Technology, Zhoukou Normal University, Zhoukou, China; School of Computer and Artificial Intelligence, Wuhan Textile University, Wuhan, China","IEEE Transactions on Network and Service Management","9 Dec 2021","2021","18","4","4197","4212","The rapid development of Internet technology has brought great convenience to our production life, and the ensuing security problems have become increasingly prominent. These problems threaten users’ privacy and pose significant security risks to the normal conduct of many aspects of society, such as politics, economy, culture, and people’s livelihood. The growth of the information transmission rate expands the scope of attacks and provides a more attack environment for intruders. Abnormal detection is an effective security protection technology that can monitor network transmission in real-time, effectively sense external attacks, and provide response decisions for relevant managers. The development of machine learning has also led to the development of abnormal traffic detection technology. The goal has been to use powerful and fast learning algorithms to deal with changing threats and respond in real-time. Most of the current abnormal detection research is based on simulation, using public and well-known datasets. On the one hand, the dataset contains high-dimensional massive data, which traditional machine learning methods cannot be processed. On the other hand, the labeled data scale is far behind the application requirements, and the dataset’s labels are all manually labeled, so the labeling cost is exceptionally high. This paper proposes a semi-supervised Double Deep Q-Network (SSDDQN)-based optimization method for network abnormal traffic detection, mainly based on Double Deep Q-Network (DDQN), a representative of Deep Reinforcement Learning algorithm. In SSDDQN, the current network first adopts the autoencoder to reconstruct the traffic features and then uses a deep neural network as a classifier. The target network first uses the unsupervised learning algorithm K-Means clustering and then uses deep neural network prediction. The experiment uses NSL-KDD and AWID datasets for training and testing and performs a comprehensive comparison with existing machine learning models. The experimental results show that SSDDQN has certain advantages in time complexity and achieved good results in various evaluation metrics.","1932-4537","","10.1109/TNSM.2021.3120804","Key Scientific and Technological Research Projects in Henan Province(grant numbers:192102210125); Open Foundation of State Key Laboratory of Networking and Switching Technology (Beijing University of Posts and Telecommunications)(grant numbers:KLNST-2020-2-01); Hubei Provincial Department of Education Youth Project(grant numbers:Q201316); Hubei Provincial Department of Education Research Program Key Project(grant numbers:D20191708); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577211","Abnormal traffic detection;semi-supervised learning;machine learning;deep reinforcement learning","Feature extraction;Reinforcement learning;Payloads;Training;Machine learning;Internet;Deep learning","communication complexity;computer crime;data protection;deep learning (artificial intelligence);Internet;pattern classification;pattern clustering;reinforcement learning;semi-supervised learning (artificial intelligence);telecommunication security;telecommunication traffic","network abnormal traffic detection;semisupervised deep reinforcement learning;Internet;security risks;information transmission rate;security protection;network transmission;high-dimensional massive data;machine learning;unsupervised learning;deep neural network prediction;AWID datasets;time complexity;semisupervised double deep Q-network-based optimization;SSDDQN-based optimization;k-means clustering","","49","","59","IEEE","18 Oct 2021","","","IEEE","IEEE Journals"
"Spear: Optimized Dependency-Aware Task Scheduling with Deep Reinforcement Learning","Z. Hu; J. Tu; B. Li","Department of Electrical and Computer Engineering, University of Toronto; Department of Electrical and Computer Engineering, University of Toronto; Department of Electrical and Computer Engineering, University of Toronto","2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)","31 Oct 2019","2019","","","2037","2046","Modern data parallel frameworks, such as Apache Spark, are designed to execute complex data processing jobs that contain a large number of tasks, with dependencies between these tasks represented by a directed acyclic graph (DAG). When scheduling these tasks, the ultimate objective is to minimize the makespan of the schedule, which is equivalent to minimizing the job completion time. With task dependencies, however, minimizing the makespan of the schedule is non-trivial, especially when tasks in the DAG have different resource demands with respect to multiple resource types. In this paper, we present Spear, a new scheduling framework designed to minimize the makespan of complex jobs, while considering both task dependencies and heterogeneous resource demands at the same time. Inspired by recent advances in artificial intelligence, Spear applies Monte Carlo Tree Search (MCTS) in the specific context of task scheduling, and trains a deep reinforcement learning model to guide the expansion and rollout steps in MCTS. With deep reinforcement learning, search efficiency can be significantly improved by focusing on more promising branches. With both simulations and experiments using traces from production workloads, we compare the scheduling performance of Spear with state-of-the-art job schedulers in the literature, and Spear can outperform those approaches by up to 20%. Our results have validated our claims that MCTS and deep reinforcement learning can readily be applied to optimize the scheduling of complex jobs with task dependencies.","2575-8411","978-1-7281-2519-0","10.1109/ICDCS.2019.00201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8885307","task scheduling, big data processing, reinforcement learning","Task analysis;Reinforcement learning;Schedules;Monte Carlo methods;Mathematical model;Scheduling algorithms;Graphene","directed graphs;learning (artificial intelligence);Monte Carlo methods;neural nets;parallel processing;scheduling;tree searching","Spear;job schedulers;task dependencies;optimized dependency-aware task scheduling;complex data processing jobs;DAG;scheduling framework;heterogeneous resource demands;deep reinforcement learning model;scheduling performance;MCTS;directed acyclic graph;Monte Carlo Tree Search;data parallel frameworks","","30","","28","IEEE","31 Oct 2019","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach for Flexible Job Shop Scheduling Problem With Crane Transportation and Setup Times","Y. Du; J. Li; C. Li; P. Duan","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information and Electrical Engineering, Shandong Jianzhu University, Jinan, China; School of Mathematics and Information Sciences, Yantai University, Yantai, China","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","15","Flexible job shop scheduling problem (FJSP) has attracted research interests as it can significantly improve the energy, cost, and time efficiency of production. As one type of reinforcement learning, deep Q-network (DQN) has been applied to solve numerous realistic optimization problems. In this study, a DQN model is proposed to solve a multiobjective FJSP with crane transportation and setup times (FJSP-CS). Two objectives, i.e., makespan and total energy consumption, are optimized simultaneously based on weighting approach. To better reflect the problem realities, eight different crane transportation stages and three typical machine states including processing, setup, and standby are investigated. Considering the complexity of FJSP-CS, an identification rule is designed to organize the crane transportation in solution decoding. As for the DQN model, 12 state features and seven actions are designed to describe the features in the scheduling process. A novel structure is applied in the DQN topology, saving the calculation resources and improving the performance. In DQN training, double deep Q-network technique and soft target weight update strategy are used. In addition, three reported improvement strategies are adopted to enhance the solution qualities by adjusting scheduling assignments. Extensive computational tests and comparisons demonstrate the effectiveness and advantages of the proposed method in solving FJSP-CS, where the DQN can choose appropriate dispatching rules at various scheduling situations.","2162-2388","","10.1109/TNNLS.2022.3208942","National Natural Science Foundation of China(grant numbers:62173216); Key Technology Research and Development Program of Shandong(grant numbers:2021CXGC011205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9913668","Deep Q-network (DQN);flexible job shop scheduling;multiobjective optimization;reinforcement learning (RL)","Cranes;Job shop scheduling;Transportation;Scheduling;Optimization;Heuristic algorithms;Reinforcement learning","","","","14","","","IEEE","10 Oct 2022","","","IEEE","IEEE Early Access Articles"
"Hierarchical reinforcement learning and central pattern generators for modeling the development of rhythmic manipulation skills","A. L. Ciancio; L. Zollo; E. Guglielmelli; D. Caligiore; G. Baldassarre","Biomedical Robotics and Biomicrosystem Laboratory, Università Campus Bio-Medico di Roma, Rome, Italy; Biomedical Robotics and Biomicrosystem Laboratory, Università Campus Bio-Medico di Roma, Rome, Italy; Biomedical Robotics and Biomicrosystem Laboratory, Università Campus Bio-Medico di Roma, Rome, Italy; Laboratory of Computational Embodied Neuroscience, Istituto di Scienze e Tecnologie della Cognizione (LOCEN-ISTC), Consiglio Nazionale delle Ricerche, Rome, Italy; Laboratory of Computational Embodied Neuroscience, Istituto di Scienze e Tecnologie della Cognizione (LOCEN-ISTC), Consiglio Nazionale delle Ricerche, Rome, Italy","2011 IEEE International Conference on Development and Learning (ICDL)","10 Oct 2011","2011","2","","1","8","The development of manipulation skills is a fundamental process for young primates as it leads them to acquire the capacity to modify the world to their advantage. As other motor skills, manipulation develops on the basis of motor babbling processes which are initially heavily based on the production of rhythmic movements. We propose a computational bio-inspired model to investigate the development of functional rhythmic hand skills from initially unstructured movements. The model is based on a hierarchical reinforcement-learning actor-critic model that searches the parameters of a set of central pattern generators (CPGs) having different degrees of sophistication. The model is tested with a simulated robotic hand engaged in rotating bottle cap-like objects having different shape and size. The results show that the model is capable of developing skills based on different combinations of CPGs so as to suitably manipulate the different objects. Overall, the model shows to be a valuable tool for the study of the development of rhythmic manipulation skills in primates.","2161-9476","978-1-61284-990-4","10.1109/DEVLRN.2011.6037370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037370","Actor critic;humanoid simulated robotic hand;motor babbling;neural networks","Iron;Shape;Thumb;Oscillators;Fires;Indexes","learning (artificial intelligence);manipulators;neural nets","central pattern generators;rhythmic manipulation skill development;motor babbling process;computational bio-inspired model;functional rhythmic hand skill development;hierarchical reinforcement-learning actor-critic model;robotic hand;bottle cap-like objects;motor skills","","10","","36","IEEE","10 Oct 2011","","","IEEE","IEEE Conferences"
"A reinforcement learning model of reaching integrating kinematic and dynamic control in a simulated arm robot","D. Caligiore; E. Guglielmelli; A. M. Borghi; D. Parisi; G. Baldassarre","Laboratory of Computational Embodied Neuroscience, Istituto di Scienze e Tecnologie della Cognizionee, Nazionale delle Ricerche, (LOCEN-ISTC-CNR), Roma, Italy; Biomedical Robotics and Biomicrosystem Laboratory, Universitá Campus Bio-Medico di Roma, Roma, Italy; Laboratory of Computational Embodied Neuroscience, Istituto di Scienze e Tecnologie della Cognizionee, Nazionale delle Ricerche, (LOCEN-ISTC-CNR), Roma, Italy; Laboratory of Computational Embodied Neuroscience, Istituto di Scienze e Tecnologie della Cognizionee, Nazionale delle Ricerche, (LOCEN-ISTC-CNR), Roma, Italy; Laboratory of Computational Embodied Neuroscience, Istituto di Scienze e Tecnologie della Cognizionee, Nazionale delle Ricerche, (LOCEN-ISTC-CNR), Roma, Italy","2010 IEEE 9th International Conference on Development and Learning","20 Sep 2010","2010","","","211","218","Models proposed within the literature of motor control have polarised around two classes of controllers which differ in terms of controlled variables: the Force-Control Models (FCMs), based on dynamic control, and the Equilibrium-Point Models (EPMs), based on kinematic control. This paper proposes a bioinspired model which aims to exploit the strengths of the two classes of models. The model is tested with a 3D physical simulator of a 2DOF-controlled arm robot engaged in a reaching task which requires the production of curved trajectories to be solved. The model is based on an actor-critic reinforcement-learning algorithm which uses neural maps to represent both percepts and actions encoded as joint-angle desired equilibrium points (EPs), and a noise generator suitable for fine tuning the exploration/exploitation ratio. The tests of the model show how it is capable of exploiting the simplicity and speed of learning of EPMs as well as the flexibility of FCMs in generating curved trajectories. Overall, the model represents a first step towards the generation of models which exploit the strengths of both EPMs and FCMs and has the potential of being used as a new tool for investigating phenomena related to the organisation and learning of motor behaviour in organisms.","2161-9476","978-1-4244-6902-4","10.1109/DEVLRN.2010.5578840","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578840","","Trajectory;Biological system modeling;Robots;Joints;Muscles;Noise;Neurons","force control;learning (artificial intelligence);position control;robot dynamics;robot kinematics","reinforcement learning model;dynamic control;kinematic control;arm robot simulation;motor control;force control models;FCM;equilibrium point models;EPM","","7","","34","IEEE","20 Sep 2010","","","IEEE","IEEE Conferences"
"Wireless Network Abnormal Traffic Detection Method Based on Deep Transfer Reinforcement Learning","Y. Xia; S. Dong; T. Peng; T. Wang","School of Computer and Artificial Intelligence, Wuhan Textile University, Wuhan, Hubei, China; School of Computer Science and Technology, Zhoukou Normal University, Zhoukou, Henan, China; School of Computer and Artificial Intelligence, Wuhan Textile University, Wuhan, Hubei, China; School of Computer and Artificial Intelligence, Wuhan Textile University, Wuhan, Hubei, China","2021 17th International Conference on Mobility, Sensing and Networking (MSN)","13 Apr 2022","2021","","","528","535","With the continuous development of information technology, the network as the infrastructure of the information age has become an indispensable and vital aspect of our daily lives. With the popularization of 5G technology, the number of handheld devices has increased significantly. Although it has brought great convenience to our production and life, it has also introduced new security risks, making the network more likely to be infiltrated and attacked. Currently, abnormal network traffic detection technology has become a vital part of network security, effectively protecting the network and computer systems from intrusion and maintaining normal operation. In the network abnormal traffic detection experiment based on simulation, most researchers use public and well-known datasets, and different datasets contain different attack samples. When testing on different datasets, the model needs to be retrained, significantly increasing the consumption of computer resources. The paper proposes a wireless network abnormal traffic detection method based on the deep transfer adversarial environment dueling double deep Q-Network (DTAE-Dueling DDQN). First, use the old NSL-KDD dataset to train AE-Dueling DDQN and save the training model weights. Then, use the idea of fine-tuning, transfer the weight of the AE-Dueling DDQN training is completed to the target model, and fine-tune the target model using the newer AWID dataset in the WiFi environment. The experiment compares the current representative deep learning (DL) and deep reinforcement learning (DRL) methods. Experimental results show that our proposed method saves computer resources significantly and achieves good results in all evaluation indicators.","","978-1-6654-0668-0","10.1109/MSN53354.2021.00083","State Key Laboratory of Networking and Switching Technology; Hubei Provincial Department of Education; Hubei Provincial Department of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751576","Abnormal traffic detection;NSL-KDD dataset;AWID dataset;fine-tuning;deep reinforcement learning","Training;Computer hacking;Wireless networks;Computational modeling;Transfer learning;Reinforcement learning;Telecommunication traffic","5G mobile communication;deep learning (artificial intelligence);radio networks;reinforcement learning;security of data;telecommunication computing;telecommunication security;telecommunication traffic","computer resources;NSL-KDD dataset;nerwork intrusion;network protection;security risks;5G technology;DTAE-Dueling DDQN;deep transfer adversarial environment dueling double deep Q-Network;network security;abnormal network traffic detection technology;information technology;deep transfer reinforcement learning;wireless network abnormal traffic detection method","","6","","48","IEEE","13 Apr 2022","","","IEEE","IEEE Conferences"
"Adaptive Static Equivalences for Active Distribution Networks With Massive Renewable Energy Integration: A Distributed Deep Reinforcement Learning Approach","B. Huang; J. Wang","Electrical and Computer Engineering Department, Southern Methodist University, USA; Electrical and Computer Engineering Department, Southern Methodist University, USA","IEEE Transactions on Network Science and Engineering","","2023","PP","99","1","13","Active distribution networks (ADNs) with 100% renewable energy sources (RESs) penetration are essential in the net-zeros emission power sector. As an efficient analysis tool, the equivalent model for ADNs (EMADNs) can be leveraged to expedite the lengthy analysis and preserve data privacy. Nevertheless, volatile RESs, along with their active management techniques, e.g., voltage regulation schemes (VRSs), deteriorate the fidelity of the developed EMADNs derived from historical data. This paper proposes an adaptive EMADN with tunable network scales and adaptive parameters to address the above challenges. A leaves-trimming network topological reduction algorithm is utilized to preserve the radial topology, the nodes of interest, and VRSs. Upon the derived network topology and components, a distributed Proximal Policy Optimization (DPPO)-based deep reinforcement learning agent is employed to adjust the parameters periodically to maintain the fidelity of EMADNs over time. The distributed training scheme extends the ordinary PPO to boost exploration and training efficiency by exploiting the multi-processors in a synchronous way. Case studies on the IEEE-33 bus and IEEE 123-bus ADNs with massive photovoltaic and wind generation demonstrate the superior accuracy and efficiency of the proposed agent-based EMADNs in various scenarios, especially when the production of RESs exceeds the load amount.","2327-4697","","10.1109/TNSE.2023.3272794","SMU Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10114954","active distribution networks;deep reinforcement learning;distributed learning;network reduction;voltage regulation schemes","Load modeling;Distribution networks;Training;Topology;Optimization;Network topology;Voltage control","","","","1","","","IEEE","3 May 2023","","","IEEE","IEEE Early Access Articles"
"Haxss: Hierarchical Reinforcement Learning for XSS Payload Generation","M. Foley; S. Maffeis","Department of Computing, Imperial College, London; Department of Computing, Imperial College, London","2022 IEEE International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)","20 Mar 2023","2022","","","147","158","Web application vulnerabilities are an ongoing problem that current black-box techniques and scanners do not entirely solve, suffering in particular from a lack of payload diversity that prevents them from capturing the long tail of vulnerabilities caused by uncommon sanitisation mistakes.In order to increase the diversity of payloads that can be automatically generated in a black-box fashion, we develop a hierarchical reinforcement learning approach where agents focus separately on the tasks of escaping the current context, and evading sanitisation. We implement this in an end-to-end prototype we call HAXSS.We compare our approach against a number of state-of-the-art black-box scanners on a new micro-benchmark for XSS payload generation, and on a macro-benchmark of established vulnerable web applications. HAXSS outperforms the other scanners on both benchmarks, identifying 131 vulnerabilities (a 20% improvement over the closest scanner), reporting 0 false positives. Finally, we demonstrate that our approach is practically useful, as HAXSS re-discovers 4 existing CVEs and discovers 5 new CVEs in 3 production-grade web applications.","2324-9013","978-1-6654-9425-0","10.1109/TrustCom56396.2022.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10063589","Reinforcement Learning;Fuzzing;Web Application Security;XSS","Training;Privacy;Closed box;Prototypes;Reinforcement learning;Tail;Benchmark testing","Internet;learning (artificial intelligence);reinforcement learning;security of data","black-box scanners;black-box techniques;HAXSS;hierarchical reinforcement learning approach;macro-benchmark;microbenchmark;payload diversity;vulnerable web applications;web application vulnerabilities;XSS payload generation","","","","34","IEEE","20 Mar 2023","","","IEEE","IEEE Conferences"
"Correlated Information Scheduling in Industrial Internet of Things Based on Multi-Heterogeneous-Agent-Reinforcement-Learning","Q. Zhang; Y. Wang","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Network Science and Engineering","","2023","PP","99","1","12","The industrial Internet of Things (IIoT) has led to the emergence of various information-based industrial applications. Due to the uninterrupted and complex nature of production processes in industrial systems, these applications often run continuously and rely on information from multiple sensors. As a result, a single sensor can support multiple applications simultaneously, leading to complex correlations in the system. To address this challenge, we introduce the concept of the age of correlated information (AoCI) and formulate the scheduling problem as a Markov game problem to optimize the information freshness of industrial applications. To solve the problem, we propose a multi-heterogeneous-agent-reinforcement-learning (MHARL) scheme, which uses neural networks with different structures to represent agents participating in the game. Our numerical results demonstrate that the proposed MHARL scheme outperforms typical baselines, such as Qmix and VDN, in terms of AoCI and energy efficiency.","2327-4697","","10.1109/TNSE.2023.3321048","National Natural Science Foundation of China(grant numbers:62171051); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10268628","Industrial Internet of Things(IIoT);Age of Correlated Information(AoCI);Multi-Agent Reinforcement Learning","Sensors;Job shop scheduling;Industrial Internet of Things;Quality of service;Correlation;Sensor systems and applications;Sensor systems","","","","","","","IEEE","2 Oct 2023","","","IEEE","IEEE Early Access Articles"
"K-nearest Multi-agent Deep Reinforcement Learning for Collaborative Tasks with a Variable Number of Agents","H. Khorasgani; H. Wang; H. -K. Tang; C. Gupta","Hitachi Industrial AI Lab, Santa Clara, CA, USA; Hitachi Industrial AI Lab, Santa Clara, CA, USA; Hitachi Industrial AI Lab, Santa Clara, CA, USA; Hitachi Industrial AI Lab, Santa Clara, CA, USA","2021 IEEE International Conference on Big Data (Big Data)","13 Jan 2022","2021","","","3883","3889","Traditionally, the performance of multi-agent deep reinforcement learning algorithms are demonstrated and validated in gaming environments where we often have a fixed number of agents. In many industrial applications, the number of available agents can change at any given day and even when the number of agents is known ahead of time, it is common for an agent to break during the operation and become unavailable for a period of time. In this paper, we propose a new deep reinforcement learning algorithm for multi-agent collaborative tasks with a variable number of agents. We demonstrate the application of our algorithm using a fleet management simulator developed by Hitachi to generate realistic scenarios in a production site.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671691","Multi-agent deep reinforcement;fleet traffic control;variable number of agents","Training;Conferences;Collaboration;Process control;Reinforcement learning;Big Data;Maintenance engineering","deep learning (artificial intelligence);groupware;multi-agent systems;nearest neighbour methods;reinforcement learning","multiagent collaborative tasks;K-nearest multiagent deep reinforcement learning;gaming environments;fleet management simulator","","","","14","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Hierarchical Reinforcement Learning, Sequential Behavior, and the Dorsal Frontostriatal System","M. Janssen; C. LeWarne; D. Burk; B. B. Averbeck","National Institute of Mental Health, Bethesda, MD; National Institute of Mental Health, Bethesda, MD; National Institute of Mental Health, Bethesda, MD; National Institute of Mental Health, Bethesda, MD","Journal of Cognitive Neuroscience","25 Oct 2022","2022","34","8","1307","1325","To effectively behave within ever-changing environments, biological agents must learn and act at varying hierarchical levels such that a complex task may be broken down into more tractable subtasks. Hierarchical reinforcement learning (HRL) is a computational framework that provides an understanding of this process by combining sequential actions into one temporally extended unit called an option. However, there are still open questions within the HRL framework, including how options are formed and how HRL mechanisms might be realized within the brain. In this review, we propose that the existing human motor sequence literature can aid in understanding both of these questions. We give specific emphasis to visuomotor sequence learning tasks such as the discrete sequence production task and the M × N (M steps × N sets) task to understand how hierarchical learning and behavior manifest across sequential action tasks as well as how the dorsal cortical–subcortical circuitry could support this kind of behavior. This review highlights how motor chunks within a motor sequence can function as HRL options. Furthermore, we aim to merge findings from motor sequence literature with reinforcement learning perspectives to inform experimental design in each respective subfield.","0898-929X","","10.1162/jocn_a_01869","National Institute of Mental Health(grant numbers:ZIA MH002928); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9928153","","","","","","","","","","25 Oct 2022","","","MIT Press","MIT Press Journals"
"Traffic and Computation Co-Offloading With Reinforcement Learning in Fog Computing for Industrial Applications","Y. Wang; K. Wang; H. Huang; T. Miyazaki; S. Guo","National Engineering Research Center of Communications and Networking, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Academic Center for Computing and Media Studies, Kyoto University, Kyoto, Japan; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan; Department of Computing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Industrial Informatics","4 Feb 2019","2019","15","2","976","986","In the past decade, network data communication has experienced a rapid growth, which has led to explosive congestion in heterogeneous networks. Moreover, the emerging industrial applications, such as automatic driving put forward higher requirements on both networks and devices. On the contrary, running computation-intensive industrial applications locally are constrained by the limited resources of devices. Correspondingly, fog computing has recently emerged to reduce the congestion of content-centric networks. It has proven to be a good way in industry and traffic for reducing network delay and processing time. In addition, device-to-device offloading is viewed as a promising paradigm to transmit network data in mobile environment, especially for autodriving vehicles. In this paper, jointly taking both the network traffic and computation workload of industrial traffic into consideration, we explore a fundamental tradeoff between energy consumption and service delay when provisioning mobile services in vehicular networks. In particular, when the available resource in mobile vehicles becomes a bottleneck, we propose a novel model to depict the users' willingness of contributing their resources to the public. We then formulate a cost minimization problem by exploiting the framework of Markov decision progress (MDP) and propose the dynamic reinforcement learning scheduling algorithm and the deep dynamic scheduling algorithm to solve the offloading decision problem. By adopting different mobile trajectory traces, we conduct extensive simulations to evaluate the performance of the proposed algorithms. The results show that our proposed algorithms outperform other benchmark schemes in the mobile edge networks.","1941-0050","","10.1109/TII.2018.2883991","National Natural Science Foundation of China(grant numbers:61872195,61572262,61872310); China Postdoctoral Science Foundation(grant numbers:2017M610252); China Postdoctoral Science Special Foundation(grant numbers:2017T100297); Shenzhen Basic Research Funding Scheme(grant numbers:JCYJ20170818103849343); Strategic Information and Communications R&D Promotion Programme(grant numbers:162302008); Open Research Fund of the Jiangsu Engineering Research Center of Communication and Network Technology; NJUPT; National Engineering Research Center of Communications and Networking (Nanjing University of Posts and Telecommunications)(grant numbers:TXKY17014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8552454","Computation offloading;fog computing;industrial application;reinforcement learning (RL);traffic offloading","Mobile handsets;Task analysis;Delays;Device-to-device communication;Servers;Energy consumption;Edge computing","cloud computing;data communication;learning (artificial intelligence);Markov processes;minimisation;mobile radio;telecommunication computing;telecommunication scheduling;telecommunication traffic","network delay reduction;mobile trajectory;cost minimization problem;Markov decision progress framework;mobile trajectory traces;mobile edge networks;offloading decision problem;deep dynamic scheduling algorithm;dynamic reinforcement learning scheduling algorithm;mobile vehicles;vehicular networks;mobile services;service delay;energy consumption;industrial traffic;computation workload;network traffic;autodriving vehicles;mobile environment;device-to-device offloading;content-centric networks;computation-intensive industrial applications;automatic driving;heterogeneous networks;explosive congestion;network data communication;fog computing","","153","","29","IEEE","29 Nov 2018","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for 5G Networks: Joint Beamforming, Power Control, and Interference Coordination","F. B. Mismar; B. L. Evans; A. Alkhateeb","Department of Electrical and Computer Engineering, Wireless Networking and Communications Group, The University of Texas at Austin, Austin, USA; Department of Electrical and Computer Engineering, Wireless Networking and Communications Group, The University of Texas at Austin, Austin, USA; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, USA","IEEE Transactions on Communications","17 Mar 2020","2020","68","3","1581","1592","The fifth generation of wireless communications (5G) promises massive increases in traffic volume and data rates, as well as improved reliability in voice calls. Jointly optimizing beamforming, power control, and interference coordination in a 5G wireless network to enhance the communication performance to end users poses a significant challenge. In this paper, we formulate the joint design of beamforming, power control, and interference coordination as a non-convex optimization problem to maximize the signal to interference plus noise ratio (SINR) and solve this problem using deep reinforcement learning. By using the greedy nature of deep Q-learning to estimate future rewards of actions and using the reported coordinates of the users served by the network, we propose an algorithm for voice bearers and data bearers in sub-6 GHz and millimeter wave (mmWave) frequency bands, respectively. The algorithm improves the performance measured by SINR and sum-rate capacity. In realistic cellular environments, the simulation results show that our algorithm outperforms the link adaptation industry standards for sub-6 GHz voice bearers. For data bearers in the mmWave frequency band, our algorithm approaches the maximum sum rate capacity, but with less than 4% of the required run time.","1558-0857","","10.1109/TCOMM.2019.2961332","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938771","Reinforcement learning (RL);deep learning;beamforming;power control;millimeter wave (mmWave)","Interference;Array signal processing;Power control;Signal to noise ratio;Reinforcement learning;Downlink;Standards","array signal processing;cellular radio;concave programming;learning (artificial intelligence);power control;radiofrequency interference;telecommunication computing;telecommunication control","signal to interference plus noise ratio;SINR;sum-rate capacity;deep Q-learning;nonconvex optimization problem;interference coordination;power control;joint beamforming;5G networks;deep reinforcement learning;frequency 6.0 GHz","","114","1","39","IEEE","23 Dec 2019","","","IEEE","IEEE Journals"
"ReLeS: A Neural Adaptive Multipath Scheduler based on Deep Reinforcement Learning","H. Zhang; W. Li; S. Gao; X. Wang; B. Ye","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE INFOCOM 2019 - IEEE Conference on Computer Communications","17 Jun 2019","2019","","","1648","1656","The Multipath TCP (MPTCP) protocol, featured by its ability of capacity aggregation across multiple links and connectivity maintenance against single-path failure, has been attracting increasing attention from the industry and academy. Multipath packet scheduling is a unique and fundamental mechanism for the design and implementation of MPTCP, which is responsible for distributing the traffic over multiple subflows. The existing multipath schedulers are facing the challenges of network heterogeneities, comprehensive QoS goals, and dynamic environments, etc. To address these challenges, we propose ReLeS, a Reinforcement Learning based Scheduler for MPTCP. ReLeS uses modern deep reinforcement learning (DRL) techniques to learn a neural network to generate the control policy for packet scheduling. It adopts a comprehensive reward function that takes diverse QoS characteristics into consideration to optimize packet scheduling. To support real-time scheduling, we propose an asynchronous training algorithm that enables parallel execution of packet scheduling, data collecting, and neural network training. We implement ReLeS in the Linux kernel and evaluate it over both emulated and real network conditions. Extensive experiments show that ReLeS significantly outperforms the state-of-the-art schedulers.","2641-9874","978-1-7281-0515-4","10.1109/INFOCOM.2019.8737649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8737649","","Scheduling algorithms;Training;Delays;Reinforcement learning;Neural networks;Scheduling;Quality of service","learning (artificial intelligence);neural nets;quality of service;telecommunication scheduling;transport protocols","single-path failure;Multipath packet scheduling;multiple subflows;network heterogeneities;ReLeS;modern deep reinforcement learning techniques;real-time scheduling;neural network training;Multipath TCP protocol;multiple links;connectivity maintenance;Linux kernel;data collection;asynchronous training algorithm;QoS characteristics;neural network;control policy;DRL techniques;deep reinforcement learning techniques;reinforcement learning based scheduler;multipath packet scheduling;capacity aggregation;MPTCP protocols;neural adaptive multipath scheduler","","51","","29","IEEE","17 Jun 2019","","","IEEE","IEEE Conferences"
"Task Offloading for Wireless VR-Enabled Medical Treatment With Blockchain Security Using Collective Reinforcement Learning","P. Lin; Q. Song; F. R. Yu; D. Wang; L. Guo","School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Internet of Things Journal","25 Oct 2021","2021","8","21","15749","15761","Wireless virtual reality (VR)-enabled medical treatment (WVMT) system, integrating the VR technology and the platform of the Internet of Medical Things (IoMT), is a promising application in future medical industries. Multiaccess edge computing (MEC) is an effective approach to support the ubiquitous applications of WVMT systems. Due to the high requirements of medical services, the computation efficiency and security are two issues in WVMT systems. In this article, we propose a blockchain-enabled task offloading scheme, where the viewport rendering tasks of VR devices (VDs) can be offloaded to edge access points (EAPs). The blockchain is integrated into the system to reach the consensus of the global information of task offloading and data processing to resist malicious attacks. To reduce VDs’ computation load under the promise of high VR QoE, we formulate the computation offloading and resource allocation to be a Markov decision problem, considering block consensus, content correlation, and fluctuating channel conditions. Then, a novel collective reinforcement learning (CRL) algorithm is proposed to adaptively allocate resources based on the requirements of viewport rendering, block consensus, and content transmission. In the simulations, the convergence rate and the performance in terms of energy consumption and stalling rate are evaluated. simulation results demonstrate the effectiveness of the proposed scheme.","2327-4662","","10.1109/JIOT.2021.3051419","National Nature Science Foundation of China(grant numbers:61771120,62025105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321476","Collective reinforcement learning (CRL);edge computing;Internet of Medical Things (IoMT);wireless virtual reality (VR)","Task analysis;Rendering (computer graphics);Wireless communication;Communication system security;Security;Blockchain;Resource management","blockchains;cloud computing;health care;Internet of Things;Markov processes;medical computing;mobile computing;reinforcement learning;rendering (computer graphics);resource allocation;virtual reality","medical treatment system;Internet of Medical Things;ubiquitous applications;WVMT systems;task offloading scheme;viewport rendering tasks;VR devices;resource allocation;block consensus;wireless virtual reality;blockchain security;VR QoE;IoMT;multiaccess edge computing;MEC;edge access points;EAP;malicious attacks;Markov decision problem;collective reinforcement learning algorithm;CRL;content transmission;content correlation","","43","","36","IEEE","13 Jan 2021","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Social-Aware Edge Computing and Caching in Urban Informatics","K. Zhang; J. Cao; H. Liu; S. Maharjan; Y. Zhang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Tongji University, Shanghai, China; Simula Metropolitan Center for Digital Engineering,, Oslo, Norway; Department of Informatics, University of Oslo, Norway","IEEE Transactions on Industrial Informatics","29 Apr 2020","2020","16","8","5467","5477","Empowered with urban informatics, transportation industry has witnessed a paradigm shift. These developments lead to the need of content processing and sharing between vehicles under strict delay constraints. Mobile edge services can help meet these demands through computation offloading and edge caching empowered transmission, while cache-enabled smart vehicles may also work as carriers for content dispatch. However, diverse capacities of edge servers and smart vehicles, as well as unpredictable vehicle routes, make efficient content distribution a challenge. To cope with this challenge, in this article we develop a social-aware nobile edge computing and caching mechanism by exploiting the relation between vehicles and roadside units. By leveraging a deep reinforcement learning approach, we propose optimal content processing and caching schemes that maximize the dispatch utility in an urban environment with diverse vehicular social characteristics. Numerical results based on real urban traffic datasets demonstrate the efficiency of our proposed schemes.","1941-0050","","10.1109/TII.2019.2953189","Opening Project of Shanghai Trusted Industrial Control Platform(grant numbers:2019YY201); Fundamental Research Funds for the Central Universities, China(grant numbers:2672018ZYGX2018J001); Xi’an Key Laboratory of Mobile Edge Computing and Security(grant numbers:201805052-ZD3CG36); Shanghai Science and Technology Commission Program(grant numbers:18511105700); European Union’s Horizon 2020 Research and Innovation Program(grant numbers:824019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8896914","Deep reinforcement learning;social aware;vehicular edge computing","Edge computing;Servers;Informatics;Social networking (online);Reinforcement learning;Delays;Vehicular ad hoc networks","cache storage;learning (artificial intelligence);mobile computing;neural nets;social networking (online)","mobile edge services;edge caching empowered transmission;cache-enabled smart vehicles;edge servers;vehicle routes;deep reinforcement learning approach;caching schemes;urban environment;diverse vehicular social characteristics;urban traffic datasets;social-aware mobile edge computing","","36","","28","IEEE","12 Nov 2019","","","IEEE","IEEE Journals"
"Performance and Cost-Efficient Spark Job Scheduling Based on Deep Reinforcement Learning in Cloud Computing Environments","M. T. Islam; S. Karunasekera; R. Buyya","Cloud Computing and Distributed Systems (CLOUDS) Lab, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia; Cloud Computing and Distributed Systems (CLOUDS) Lab, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia; Cloud Computing and Distributed Systems (CLOUDS) Lab, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia","IEEE Transactions on Parallel and Distributed Systems","23 Nov 2021","2022","33","7","1695","1710","Big data frameworks such as Spark and Hadoop are widely adopted to run analytics jobs in both research and industry. Cloud offers affordable compute resources which are easier to manage. Hence, many organizations are shifting towards a cloud deployment of their big data computing clusters. However, job scheduling is a complex problem in the presence of various Service Level Agreement (SLA) objectives such as monetary cost reduction, and job performance improvement. Most of the existing research does not address multiple objectives together and fail to capture the inherent cluster and workload characteristics. In this article, we formulate the job scheduling problem of a cloud-deployed Spark cluster and propose a novel Reinforcement Learning (RL) model to accommodate the SLA objectives. We develop the RL cluster environment and implement two Deep Reinforce Learning (DRL) based schedulers in TF-Agents framework. The proposed DRL-based scheduling agents work at a fine-grained level to place the executors of jobs while leveraging the pricing model of cloud VM instances. In addition, the DRL-based agents can also learn the inherent characteristics of different types of jobs to find a proper placement to reduce both the total cluster VM usage cost and the average job duration. The results show that the proposed DRL-based algorithms can reduce the VM usage cost up to 30%.","1558-2183","","10.1109/TPDS.2021.3124670","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9599497","Cloud computing;cost-efficiency;performance improvement;deep reinforcement learning","Sparks;Cloud computing;Costs;Task analysis;Service level agreements;Big Data;Reinforcement learning","Big Data;cloud computing;cost reduction;data handling;deep learning (artificial intelligence);parallel processing;reinforcement learning;scheduling;virtual machines","cost-efficient Spark job scheduling;cloud computing environments;big data frameworks;analytics jobs;affordable compute resources;cloud deployment;big data computing clusters;complex problem;monetary cost reduction;job performance improvement;inherent cluster;workload characteristics;job scheduling problem;cloud-deployed Spark cluster;reinforcement learning model;SLA objectives;RL cluster environment;TF-Agents framework;DRL-based scheduling agents work;fine-grained level;DRL-based agents;total cluster VM usage;average job duration;DRL-based algorithms;deep reinforce learning based schedulers;service level agreement objectives;deep reinforcement learning","","32","","40","IEEE","2 Nov 2021","","","IEEE","IEEE Journals"
"Reinforcement Learning for Service Function Chain Reconfiguration in NFV-SDN Metro-Core Optical Networks","S. Troia; R. Alvizu; G. Maier","Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Politecnico di Milano, Milan, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Politecnico di Milano, Milan, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Politecnico di Milano, Milan, Italy","IEEE Access","26 Nov 2019","2019","7","","167944","167957","With the advent of 5G technology, we are witnessing the development of increasingly bandwidth-hungry network applications, such as enhanced mobile broadband, massive machine-type communications and ultra-reliable low-latency communications. Software Defined Networking (SDN), Network Function Virtualization (NFV) and Network Slicing (NS) are gaining momentum not only in research but also in IT industry representing the drivers of 5G. NS is an approach to network operations allowing the partition of a physical topology into multiple independent virtual networks, called network slices (or slices). Within a single slice, a set of Service Function Chains (SFCs) is defined and the network resources, e.g. bandwidth, can be provisioned dynamically on demand according to specific Quality of Service (QoS) and Service Level Agreement (SLA) requirements. Traditional schemes for network resources provisioning based on static policies may lead to poor resource utilization and suffer from scalability issues. In this article, we investigate the application of Reinforcement Learning (RL) for performing dynamic SFC resources allocation in NFV-SDN enabled metro-core optical networks. RL allows to build a self-learning system able to solve highly complex problems by employing RL agents to learn policies from an evolving network environment. In particular, we build an RL system able to optimize the resources allocation of SFCs in a multi-layer network (packet over flexi-grid optical layer). The RL agent decides if and when to reconfigure the SFCs, given state of the network and historical traffic traces. Numerical simulations show significant advantages of our RL-based optimization over rule-based optimization design.","2169-3536","","10.1109/ACCESS.2019.2953498","European Community Metro-Haul Project(grant numbers:761727); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8901169","Software-defined networking;network function virtualization;network slicing;virtual service chain;reinforcement learning;deep learning;mixed integer linear programming","Quality of service;Optical fiber networks;Resource management;Reinforcement learning;Quality of experience;Optimization;Heuristic algorithms","contracts;learning (artificial intelligence);optical fibre networks;optimisation;quality of service;resource allocation;software defined networking;virtualisation","network slicing;reinforcement learning;RL-based optimization;RL agent;flexi-grid optical layer;multilayer network;RL system;self-learning system;dynamic SFC resources allocation;network resources;service function chains;network slices;multiple independent virtual networks;network operations;network function virtualization;software defined networking;massive machine-type communications;enhanced mobile broadband;bandwidth-hungry network applications;NFV-SDN metro-core optical networks;service function chain reconfiguration","","32","","42","CCBY","14 Nov 2019","","","IEEE","IEEE Journals"
"Fault Diagnosis of Rotating Machinery Based on Deep Reinforcement Learning and Reciprocal of Smoothness Index","W. Dai; Z. Mo; C. Luo; J. Jiang; H. Zhang; Q. Miao","Center of Aerospace Information Processing and Application, School of Aeronautics and Astronautics, Sichuan University, Chengdu, China; Center of Aerospace Information Processing and Application, School of Aeronautics and Astronautics, Sichuan University, Chengdu, China; Center of Aerospace Information Processing and Application, School of Aeronautics and Astronautics, Sichuan University, Chengdu, China; Center of Aerospace Information Processing and Application, School of Aeronautics and Astronautics, Sichuan University, Chengdu, China; Center of Aerospace Information Processing and Application, School of Aeronautics and Astronautics, Sichuan University, Chengdu, China; Center of Aerospace Information Processing and Application, School of Aeronautics and Astronautics, Sichuan University, Chengdu, China","IEEE Sensors Journal","2 Jul 2020","2020","20","15","8307","8315","Rotating machinery are widely used in industry, and vibration analysis is one of the most common methods to monitor health condition of rotating machinery. However, due to the presence of outliers and interference, vibration signal becomes very complicated in reality, and it is important to reduce the influence of outliers and interference. Since a bandpass filter can eliminate a lot of above influence, it is usually selected to process vibration signal in classic fault diagnosis. The selection of the lower and upper cutoff frequencies of the bandpass filter is very critical. In order to extract fault characteristics from vibration signal, this paper proposes a new method which uses deep reinforcement learning algorithm and the reciprocal of smoothness index to control the bandpass filter to select a frequency band with the highest signal-to-noise ratio. Then, envelope demodulation is performed on the filtered signal so as to diagnose the faults of rotating machinery. Two sets of data collected from the test rig are used to validate the effectiveness of the proposed method. The comparisons with fast kurtogram and GiniIndexgram show the superiority of the proposed method. It also suggests that reinforcement learning has a great potential in the field of mechanical fault diagnosis.","1558-1748","","10.1109/JSEN.2020.2970747","National Natural Science Foundation of China(grant numbers:51675355); National Key Laboratory of Science and Technology on Reliability and Environmental Engineering(grant numbers:6142004005-1); Aeronautical Science Foundation of China(grant numbers:20173333001); Open Research Fund of Key Laboratory of Space Utilization, Chinese Academy of Sciences(grant numbers:LSU-KFJJ-2018-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8977499","Fault diagnosis;deep reinforcement learning;smoothness index;envelope analysis;optimal frequency band","Indexes;Reinforcement learning;Fault diagnosis;Machinery;Resonant frequency;Vibrations;Band-pass filters","band-pass filters;condition monitoring;demodulation;electric machines;fault diagnosis;learning (artificial intelligence);neural nets;vibrational signal processing","envelope demodulation;frequency band;health condition monitoring;reciprocal of smoothness index;deep reinforcement learning;fault characteristics;cutoff frequencies;vibration signal;vibration analysis;mechanical fault diagnosis;rotating machinery;filtered signal;signal-to-noise ratio;bandpass filter","","29","","40","IEEE","31 Jan 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Scheduling in Cellular Networks","J. Wang; C. Xu; Y. Huangfu; R. Li; Y. Ge; J. Wang","Hangzhou Research Center, Huawei Technologies, Hangzhou, China; Hangzhou Research Center, Huawei Technologies, Hangzhou, China; Hangzhou Research Center, Huawei Technologies, Hangzhou, China; Hangzhou Research Center, Huawei Technologies, Hangzhou, China; Ottawa Research Center, Huawei Technologies, Ottawa, Canada; Hangzhou Research Center, Huawei Technologies, Hangzhou, China","2019 11th International Conference on Wireless Communications and Signal Processing (WCSP)","8 Dec 2019","2019","","","1","6","Integrating artificial intelligence (AI) into wireless networks has drawn significant interest in both industry and academia. A common solution is to replace partial or even all modules in the conventional systems, which is often lack of efficiency and robustness due to their ignoring of expert knowledge. In this paper, we take deep reinforcement learning (DRL) based scheduling as an example to investigate how expert knowledge can help with AI module in cellular networks. A simulation platform, which has considered link adaption, feedback and other practical mechanisms, is developed to facilitate the investigation. Besides the traditional way, which is learning directly from the environment, for training DRL agent, we propose two novel methods, i.e., learning from a dual AI module and learning from the expert solution. The results show that, for the considering scheduling problem, DRL training procedure can be improved on both performance and convergence speed by involving the expert knowledge. Hence, instead of replacing conventional scheduling module in the system, adding a newly introduced AI module, which is capable to interact with the conventional module and provide more flexibility, is a more feasible solution.","2472-7628","978-1-7281-3555-7","10.1109/WCSP.2019.8927868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8927868","artificial intelligence;cellular networks;deep reinforcement learning;scheduling;proportional fair","Training;Throughput;Cellular networks;Machine learning;Job shop scheduling;Scheduling algorithms","cellular radio;learning (artificial intelligence);telecommunication computing;telecommunication scheduling","artificial intelligence;wireless networks;deep reinforcement learning based scheduling;cellular networks;DRL agent;dual AI module;DRL training procedure","","22","","14","IEEE","8 Dec 2019","","","IEEE","IEEE Conferences"
"LEARNET: Reinforcement Learning Based Flow Scheduling for Asynchronous Deterministic Networks","J. Prados-Garzon; T. Taleb; M. Bagaa","Aalto University, Espoo, Finland; Aalto University, Espoo, Finland; Aalto University, Espoo, Finland","ICC 2020 - 2020 IEEE International Conference on Communications (ICC)","27 Jul 2020","2020","","","1","6","Time-Sensitive Networking (TSN) and Deterministic Networking (DetNet) standards come to satisfy the needs of many industries for deterministic network services. That is the ability to establish a multi-hop path over an IP network for a given flow with deterministic Quality of Service (QoS) guarantees in terms of latency, jitter, packet loss, and reliability. In this work, we propose a reinforcement learning-based solution, which is dubbed LEARNET, for the flow scheduling in deterministic asynchronous networks. The solution leverages predictive data analytics and reinforcement learning to maximize the network operator's revenue. We evaluate the performance of LEARNET through simulation in a fifth-generation (5G) asynchronous deterministic backhaul network where incoming flows have characteristics similar to the four critical 5GQoS Identifiers (5QIs) defined in Third Generation Partnership Project (3GPP) TS 23.501 V16.1.0. Also, we compared the performance of LEARNET with a baseline solution that respects the 5QIs priorities for allocating the incoming flows. The obtained results show that, for the scenario considered, LEARNET achieves a gain in the revenue of up to 45% compared to the baseline solution.","1938-1883","978-1-7281-5089-5","10.1109/ICC40277.2020.9149092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9149092","","Resource management;Delays;Quality of service;Data analysis;5G mobile communication;Analytical models;Radio frequency","5G mobile communication;IP networks;learning (artificial intelligence);quality of service;telecommunication computing;telecommunication network routing;telecommunication scheduling;telecommunication traffic","LEARNET;flow scheduling;time-sensitive networking;deterministic network services;multihop path;IP network;reinforcement learning;deterministic asynchronous networks;network operator;fifth-generation asynchronous deterministic backhaul network;baseline solution;deterministic networking standards;asynchronous deterministic networks;5QI priorities;deterministic quality of service;Third Generation Partnership Project;3GPP TS 23.501 V16.1.0;critical 5GQoS identifiers","","19","","15","IEEE","27 Jul 2020","","","IEEE","IEEE Conferences"
"A Framework for Automated Cellular Network Tuning With Reinforcement Learning","F. B. Mismar; J. Choi; B. L. Evans","Wireless Networking and Communications Group, The University of Texas at Austin, Austin, TX, USA; Wireless Networking and Communications Group, The University of Texas at Austin, Austin, TX, USA; Wireless Networking and Communications Group, The University of Texas at Austin, Austin, TX, USA","IEEE Transactions on Communications","16 Oct 2019","2019","67","10","7152","7167","Tuning cellular network performance against always occurring wireless impairments can dramatically improve reliability to end users. In this paper, we formulate cellular network performance tuning as a reinforcement learning (RL) problem and provide a solution to improve the performance for indoor and outdoor environments. By leveraging the ability of Q-learning to estimate future performance improvement rewards, we propose two algorithms: 1) closed loop power control (PC) for downlink voice over LTE (VoLTE) and 2) self-organizing network (SON) fault management. The VoLTE PC algorithm uses RL to adjust the indoor base station transmit power so that the signal-to-interference plus noise ratio (SINR) of a user equipment (UE) meets the target SINR. It does so without the UE having to send power control requests. The SON fault management algorithm uses RL to improve the performance of an outdoor base station cluster by resolving faults in the network through configuration management. Both algorithms exploit measurements from the connected users, wireless impairments, and relevant configuration parameters to solve a non-convex performance optimization problem using RL. Simulation results show that our proposed RL-based algorithms outperform the industry standards today in realistic cellular communication environments.","1558-0857","","10.1109/TCOMM.2019.2926715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758212","Framework;reinforcement learning;artificial intelligence;VoLTE;MOS;QoE;wireless;tuning;optimization;SON","Cellular networks;Interference;Long Term Evolution;Tuning;Wireless communication;Base stations;Quality of experience","cellular radio;control engineering computing;fault diagnosis;learning (artificial intelligence);Long Term Evolution;optimisation;power control;self-organising feature maps;telecommunication computing;telecommunication control;telecommunication network management","Q-learning;closed loop power control;VoLTE PC algorithm;indoor base station transmit power;user equipment;UE;SINR;configuration management;nonconvex performance optimization problem;RL-based algorithms;realistic cellular communication environments;automated cellular network tuning;cellular network performance tuning;reinforcement learning problem;downlink voice over LTE;VoLTE;SON fault management;self-organizing network fault management;signal-to-interference plus noise ratio","","18","","45","IEEE","9 Jul 2019","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for the management of Software-Defined Networks in Smart Farming","R. S. Alonso; I. Sittón-Candanedo; R. Casado-Vara; J. Prieto; J. M. Corchado","BISITE Research Group, University of Salamanca, Salamanca, Spain; BISITE Research Group, University of Salamanca, Salamanca, Spain; BISITE Research Group, University of Salamanca, Salamanca, Spain; BISITE Research Group, University of Salamanca, Salamanca, Spain; IoT Digital Innovation Hub, AIR Institute, Salamanca, Spain","2020 International Conference on Omni-layer Intelligent Systems (COINS)","10 Sep 2020","2020","","","1","6","The Internet of Things and the millions of devices that generate and collect data through sensors to send it to the Cloud are part of the life of users in many contexts, including smart farming and precision agriculture scenarios. This volume of data is stored and processed in the Cloud, with the purpose of obtaining knowledge and valuable information for organizations. Edge Computing has emerged to reduce the costs associated with transferring, processing and storing data from IoT environments in the Cloud. This paradigm allows data to be pre-processed at the edge of the network before they are sent to the Cloud, obtaining shorter response times and maintaining service even during communication breakdowns between the IoT and Cloud layers. Furthermore, there is a increasing trend to shared physical network resources among diverse user entities through Software-Defined Networks and Network Function Virtualization with the aim to reduce costs. In this sense, smart mechanisms are required to optimize virtual dataflows in the networks, as Deep Reinforcement Learning techniques. This paper proposes a Double Deep-Q Learning approach to manage virtual dataflows in SDN/NFV using an Edge-IoT architecture, formerly applied in smart farming and Industry 4.0 scenarios.","","978-1-7281-6371-0","10.1109/COINS49042.2020.9191634","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9191634","Internet of Things;Software Defined Networks;Reinforcement Learning;Network Function Virtualization;Smart Farming","Computer architecture;Cloud computing;Edge computing;Machine learning;Internet of Things;Agriculture;Network function virtualization","agriculture;cloud computing;Internet of Things;learning (artificial intelligence);neural nets;software architecture;software defined networking;virtualisation","IoT environments;software-defined networks;network function virtualization;smart mechanisms;Edge-IoT architecture;smart farming;Internet of Things;precision agriculture;virtual dataflows optimization;double deep-Q learning approach;deep reinforcement learning;edge computing","","15","","44","IEEE","10 Sep 2020","","","IEEE","IEEE Conferences"
"Delay-aware Cellular Traffic Scheduling with Deep Reinforcement Learning","T. Zhang; S. Shen; S. Mao; G. -K. Chang","Department of Electrical and Computer Engineering, Auburn University, Auburn, AL; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA","GLOBECOM 2020 - 2020 IEEE Global Communications Conference","25 Jan 2021","2020","","","1","6","Radio access network (RAN) in 5G is expected to satisfy the stringent delay requirements of a variety of applications. The packet scheduler plays an important role by allocating spectrum resources to user equipments (UEs) at each transmit time interval (TTI). In this paper, we show that optimal scheduling is a challenging combinatorial optimization problem, which is hard to solve within the channel coherence time with conventional optimization methods. Rule-based scheduling methods, on the other hand, are hard to adapt to the time-varying wireless channel conditions and various data request patterns of UEs. Recently, integrating artificial intelligence (AI) into wireless networks has drawn great interest from both academia and industry. In this paper, we incorporate deep reinforcement learning (DRL) into the design of cellular packet scheduling. A delay-aware cell traffic scheduling algorithm is developed to map the observed system state to scheduling decision. Due to the huge state space, a recurrent neural network (RNN) is utilized to approximate the optimal action-policy function. Different from conventional rule-based scheduling methods, the proposed scheme can learn from the interactions with the environment and adaptively choosing the best scheduling decision at each TTI. Simulation results show that the DRL-based packet scheduling can achieve the lowest average delay compared with several conventional approaches. Meanwhile, the UEs' average queue lengths can also be significantly reduced. The developed method also exhibits great potential in real-time scheduling in delay-sensitive scenarios.","2576-6813","978-1-7281-8298-8","10.1109/GLOBECOM42002.2020.9322560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9322560","Delay;Deep reinforcement learning (DRL);Packet scheduling;Recurrent neural network (RNN)","Delays;Scheduling algorithms;Job shop scheduling;Optimal scheduling;Wireless communication;Resource management;Recurrent neural networks","cellular radio;deep learning (artificial intelligence);optimisation;packet radio networks;queueing theory;radio access networks;recurrent neural nets;telecommunication computing;telecommunication traffic;wireless channels","combinatorial optimization problem;channel coherence time;time-varying wireless channel conditions;wireless networks;deep reinforcement learning;cellular packet scheduling;delay-aware cell traffic scheduling algorithm;observed system state;recurrent neural network;conventional rule-based scheduling methods;TTI;DRL-based packet scheduling;real-time scheduling;delay-sensitive scenarios;delay-aware cellular traffic scheduling;5G radio access network;stringent delay requirements;packet scheduler;spectrum resource allocation;user equipments;transmit time interval;optimal scheduling;UE average queue lengths;optimal action-policy function;rule-based scheduling methods;artificial intelligence","","15","","16","IEEE","25 Jan 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-Based Offloading Decision Optimization in Mobile Edge Computing","H. Zhang; W. Wu; C. Wang; M. Li; R. Yang","Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China","2019 IEEE Wireless Communications and Networking Conference (WCNC)","31 Oct 2019","2019","","","1","7","As a promising technique, mobile edge computing (MEC) has attracted significant attention from both academia and industry. However, the offloading decision for computing tasks in MEC is usually complicated and intractable. In this paper, we propose a novel framework for offloading decision in MEC based on Deep Reinforcement Learning (DRL). We consider a typical network architecture with one MEC server and one mobile user, in which the tasks of the device arrive as a flow in time. We model the offloading decision process of the task flow as a Markov Decision Process (MDP). The optimization object is minimizing the weighted sum of offloading latency and power consumption, which is decomposed into the reward of each time slot. The elements of DRL such as policy, reward and value are defined according to the proposed optimization problem. Simulation results reveal that the proposed method could significantly reduce the energy consumption and latency compared to the existing schemes.","1558-2612","978-1-5386-7646-2","10.1109/WCNC.2019.8886332","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8886332","Mobile edge computing;deep reinforcement learning;Markov decision process;wireless networks","Task analysis;Mobile handsets;Energy consumption;Servers;Optimization;Computational modeling;Edge computing","learning (artificial intelligence);Markov processes;mobile computing;optimisation","mobile edge computing;DRL;typical network architecture;MEC server;mobile user;offloading decision process;task flow;Markov decision process;optimization object;optimization problem;deep reinforcement learning-based offloading decision","","15","","17","IEEE","31 Oct 2019","","","IEEE","IEEE Conferences"
"Exploring Fault Parameter Space Using Reinforcement Learning-based Fault Injection","M. Moradi; B. J. Oakes; M. Saraoglu; A. Morozov; K. Janschek; J. Denil","University of Antwerp and Flanders Make vzw, Belgium; University of Antwerp and Flanders Make vzw, Belgium; Technische Universität Dresden; Technische Universität Dresden; Technische Universität Dresden; University of Antwerp and Flanders Make vzw, Belgium","2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)","29 Jul 2020","2020","","","102","109","Assessing the safety of complex Cyber-Physical Systems (CPS) is a challenge in any industry. Fault Injection (FI) is a proven technique for safety analysis and is recommended by the automotive safety standard ISO 26262. Traditional FI methods require a considerable amount of effort and cost as FI is applied late in the development cycle and is driven by manual effort or random algorithms. In this paper, we propose a Reinforcement Learning (RL) approach to explore the fault space and find critical faults. During the learning process, the RL agent injects and parameterizes faults in the system to cause catastrophic behavior. The fault space is explored based on a reward function that evaluates previous simulation results such that the RL technique tries to predict improved fault timing and values. In this paper, we apply our technique on an Adaptive Cruise Controller with sensor fusion and compare the proposed method with Monte Carlo-based fault injection. The proposed technique is more efficient in terms of fault coverage and time to find the first critical fault.","2325-6664","978-1-7281-7263-7","10.1109/DSN-W50199.2020.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9151674","Fault injection;reinforcement learning;safety assessment;cyber-physical systems;machine learning","Space exploration;Circuit faults;Safety;Complexity theory;Integrated circuit modeling;Learning (artificial intelligence);Acceleration","adaptive control;automotive engineering;control engineering computing;cyber-physical systems;fault diagnosis;ISO standards;learning (artificial intelligence);Monte Carlo methods;road safety;sensor fusion;velocity control","CPS;sensor fusion;catastrophic behavior;adaptive cruise controller;reinforcement learning-based fault injection;FI methods;fault coverage;Monte Carlo-based fault injection;RL technique;critical fault;automotive safety standard ISO 26262;complex cyber-physical systems;fault parameter space","","15","","22","IEEE","29 Jul 2020","","","IEEE","IEEE Conferences"
"Task-Driven Semantic Coding via Reinforcement Learning","X. Li; J. Shi; Z. Chen","Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China; Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China; Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China","IEEE Transactions on Image Processing","13 Jul 2021","2021","30","","6307","6320","Task-driven semantic video/image coding has drawn considerable attention with the development of intelligent media applications, such as license plate detection, face detection, and medical diagnosis, which focuses on maintaining the semantic information of videos/images. Deep neural network (DNN)-based codecs have been studied for this purpose due to their inherent end-to-end optimization mechanism. However, the traditional hybrid coding framework cannot be optimized in an end-to-end manner, which makes task-driven semantic fidelity metric unable to be automatically integrated into the rate-distortion optimization process. Therefore, it is still attractive and challenging to implement task-driven semantic coding with the traditional hybrid coding framework, which should still be widely used in practical industry for a long time. To solve this challenge, we design semantic maps for different tasks to extract the pixelwise semantic fidelity for videos/images. Instead of directly integrating the semantic fidelity metric into traditional hybrid coding framework, we implement task-driven semantic coding by implementing semantic bit allocation based on reinforcement learning (RL). We formulate the semantic bit allocation problem as a Markov decision process (MDP) and utilize one RL agent to automatically determine the quantization parameters (QPs) for different coding units (CUs) according to the task-driven semantic fidelity metric. Extensive experiments on different tasks, such as classification, detection and segmentation, have demonstrated the superior performance of our approach by achieving an average bitrate saving of 34.39% to 52.62% over the High Efficiency Video Coding (H.265/HEVC) anchor under equivalent task-related semantic fidelity.","1941-0042","","10.1109/TIP.2021.3091909","NSFC(grant numbers:U1908209,61632001); National Key Research and Development Program of China(grant numbers:2018AAA0101400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9472999","HEVC intra coding;task-driven semantic coding;bit allocation;reinforcement learning","Semantics;Image coding;Task analysis;Bit rate;Measurement;Optimization;Encoding","deep learning (artificial intelligence);Markov processes;optimisation;rate distortion theory;video coding","pixelwise semantic fidelity;hybrid coding;task-driven semantic coding;semantic bit allocation problem;high efficiency video coding;semantic map design;semantic image information;semantic video information;end-to-end optimization mechanism;task-driven semantic fidelity metric;quantization parameters;QP;coding units;Markov decision process;MDP;reinforcement learning agent;RL agent;DNN-based codecs;deep neural network-based codecs","","14","","73","IEEE","2 Jul 2021","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Empowered Adaptivity for Future Blockchain Networks","C. Qiu; X. Ren; Y. Cao; T. Mai","College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Open Journal of the Computer Society","3 Mar 2021","2021","2","","99","105","Recently, blockchain has elicited escalating attention from academia to industry. However, blockchain is still in its initial stage, and remains a great number of non-trivial problems to be delved before being used as a generic platform. The most intractable one is the scalability problem. The deep reinforcement learning empowered adaptivity can help the blockchain network break through the bottleneck. In this paper, we study a deep reinforcement learning empowered adaptivity approach for future blockchain networks, so as to improve the scalability and meet the requirements of different users. Specifically, rather than using one consensus protocol as the best fit one, the blockchain networks launch different consensus protocols, based on users’ quality of service (QoS) requirements. To this end, we quantify four consensus protocols. Additionally, the blockchain networks are heavily hampered by the limited computation and bandwidth resources. We also dynamically allocate computation and bandwidth resources to the blockchain networks. Then we formulate these thress items, i.e., the selection of consensus protocols, computation resource, and network bandwidth resource, as a joint optimization problem. A deep reinforcement learning approach is used to solve this problem. Simulation results are presented to show the effectiveness of our proposed scheme.","2644-1268","","10.1109/OJCS.2020.3010987","National Natural Science Foundation of China(grant numbers:62072332); China NSFC (Youth)(grant numbers:62002260); China Postdoctoral Science Foundation(grant numbers:2020M670654); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9145696","Blockchain networks;adaptivity;state machine replication protocol;deep reinforcement learning","Reinforcement learning;Bandwidth;Scalability;Quality of service;Machine learning;Throughput;Blockchains;Deep learning","blockchains;deep learning (artificial intelligence);protocols;quality of service","future blockchain networks;scalability problem;consensus protocol;network bandwidth resource;deep reinforcement learning approach;quality of service;computation resource","","12","","22","CCBY","21 Jul 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning based Task Scheduling in Mobile Blockchain for IoT Applications","Y. Gao; W. Wu; H. Nan; Y. Sun; P. Si","Faculty of Information Technology, Beijing University of Technology, Beijing, P.R. China; Faculty of Information Technology, Beijing University of Technology, Beijing, P.R. China; Faculty of Information Technology, Beijing University of Technology, Beijing, P.R. China; Faculty of Information Technology, Beijing University of Technology, Beijing, P.R. China; Faculty of Information Technology, Beijing University of Technology, Beijing, P.R. China","ICC 2020 - 2020 IEEE International Conference on Communications (ICC)","27 Jul 2020","2020","","","1","7","Nowadays, the Internet of Things (IoT) has developed rapidly. To deal with the security problems in some of the IoT applications, blockchain has aroused lots of attention in both academia and industry. In this paper, we consider the mobile blockchain supporting IoT applications, and the mobile edge computing (MEC) is deployed at the Small-cell Base Station (SBS) as a supplement to enhance the computation ability of IoT devices. To encourage the participation of the SBS in the mobile blockchain networks, the long-term revenue of the SBS is considered. The task scheduling problem maximizing the long-term mining reward and minimizing the resource cost of the SBS is formulated as a Markov Decision Process (MDP). To achieve an efficient intelligent strategy, the deep reinforcement learning (DRL) based solution named policy gradient based computing tasks scheduling (PG-CTS) algorithm is proposed. The policy mapping from the system state to the task scheduling decision is represented by a deep neural network. The episodic simulations are built and the REINFORCE algorithm with baseline is used to train the policy network. According to the training results, the PG-CTS method is about 10% better than the second-best method greedy. The generalization ability of PG-CTS is proved theoretically, and the testing results also show that the PG-CTS method has better performance over the other three strategies, greedy, first-in-first-out (FIFO) and random in different environments.","1938-1883","978-1-7281-5089-5","10.1109/ICC40277.2020.9148888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9148888","","Task analysis;Servers;Processor scheduling;Computational modeling;Scheduling;Optimization","cryptography;gradient methods;Internet of Things;learning (artificial intelligence);Markov processes;neural nets;scheduling","PG-CTS method;IoT applications;security problems;mobile edge computing;small-cell base station;IoT devices;SBS;mobile blockchain networks;long-term revenue;task scheduling problem;long-term mining reward;task scheduling decision;deep neural network;REINFORCE algorithm;policy network;deep reinforcement learning;policy gradient;Markov decision process;MDP;FIFO;first-in first-out","","12","","29","IEEE","27 Jul 2020","","","IEEE","IEEE Conferences"
"A Reinforcement Learning-Based Framework for the Exploitation of Multiple RATs in the IoT","R. M. Sandoval; S. Canovas-Carrasco; A. -J. Garcia-Sanchez; J. Garcia-Haro","Department of Information and Communication Technologies, Universidad Politécnica de Cartagena, Cartagena, Spain; Department of Information and Communication Technologies, Universidad Politécnica de Cartagena, Cartagena, Spain; Department of Information and Communication Technologies, Universidad Politécnica de Cartagena, Cartagena, Spain; Department of Information and Communication Technologies, Universidad Politécnica de Cartagena, Cartagena, Spain","IEEE Access","10 Sep 2019","2019","7","","123341","123354","The IoT is the cornerstone of many innovating processes such as those behind Smart Cities and Smart Industries. As more and more wireless IoT devices are deployed, a newer, more congestion-resilient communication infrastructure is required to absorb the traffic from the 50 billion IoT nodes expected by the year 2020. Although 5G is said to be a key technology for the future IoT, it is not a silver bullet. Therefore, providing nodes with different Radio Access Technologies (RAT) would allow them to reap the various benefits offered by each RAT. However, the process of determining which technology should be used at any given time should not be based on uninformed intuition, but on mathematically educated choices. By making use of the mathematical framework of Reinforcement Learning, we have allowed IoT nodes to learn from previous real world data in order to derive optimal RAT-selection policies. These policies, which are implemented as Artificial Neural Networks (ANN), maximize a predefined reward closely related to classic throughput, while maintaining power consumption and operational costs below a certain limit. To allow hardware-constrained IoT nodes to use these ANNs, we have proposed the application of a quantization technique that reduces computation and memory requirements and have validated it by its implementation in a real IoT device. Finally, to evaluate the proposal, we have simulated a network of 1000 devices deployed in the city of Chicago. The obtained results are compared to those achieved with other intuitive policies to further highlight the benefits of our proposal.","2169-3536","","10.1109/ACCESS.2019.2938084","Projects AIM, AEI/FEDER, UE, e-DIVITA(grant numbers:TEC2016-76465-C2-1-R); Proof of Concept, 2018(grant numbers:20509/PDC/18); ATENTO, Fundación Seneca, Región de Murcia(grant numbers:20889/PI/18); Spanish MECD for an FPU Pre-Doctoral Fellowship(grant numbers:FPU14/03424,FPU16/03530); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8817919","Multi-RAT;IoT;reinforcement learning;5G;LPWAN","Radio access technologies;Proposals;Cellular networks;5G mobile communication;Throughput;Radio access networks;Heuristic algorithms","5G mobile communication;Internet of Things;learning (artificial intelligence);neural nets;quantisation (signal);radio access networks;smart cities;telecommunication computing;telecommunication congestion control;telecommunication traffic","Reinforcement Learning-based framework;Smart Cities;wireless IoT devices;congestion-resilient communication infrastructure;optimal RAT-selection policies;Artificial Neural Networks;ANN;hardware-constrained IoT nodes;IoT device;Radio Access Technologies;5G technology","","11","","56","CCBY","28 Aug 2019","","","IEEE","IEEE Journals"
"A Method for Deploying Distributed Denial of Service Attack Defense Strategies on Edge Servers Using Reinforcement Learning","H. Zhang; J. Hao; X. Li","Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China","IEEE Access","13 May 2020","2020","8","","78482","78491","Cloud-based filtering, as the most commonly used distributed denial of service attack mitigation method in the industry, has flaws that can cause privacy leaks and delays like other cloud applications. A new DDoS mitigation method which moving cloud filtering services to edge servers is proposed in this paper. In this method, the edge servers are deployed at various router locations and run classifiers to filter the traffic passing through. For cutting attack traffic, reserving user traffic and reducing inspection delays, a novel deep reinforcement learning framework is developed to balance the deployment of computing resource and tasks allocation, in which graph neural network used to coding the network structure information transformation as vector, and the traffic information to input into Q-Network to obtain the best allocation results. The simulation experiments show that our method has advantages in optimizing effects and computing time compared with other deployment methods.","2169-3536","","10.1109/ACCESS.2020.2989353","National Science Foundation of China(grant numbers:61872262,61572349); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9075183","Edge computing;distributed denial of service attack;reinforcement learning;graph convolutional network","Computer crime;Servers;Cloud computing;Delays;Edge computing;Resource management","cloud computing;computer network security;data privacy;graph theory;learning (artificial intelligence);neural nets;telecommunication traffic","graph neural network;Q-network;inspection delay reduction;distributed denial of service attack defense strategies;attack traffic;DDoS mitigation method;cloud applications;privacy leaks;service attack mitigation method;edge servers;deployment methods;traffic information;network structure information transformation;deep reinforcement learning framework;user traffic","","11","","24","CCBY","21 Apr 2020","","","IEEE","IEEE Journals"
"Dynamic Resource Aware VNF Placement with Deep Reinforcement Learning for 5G Networks","A. Dalgkitsis; P. -V. Mekikis; A. Antonopoulos; G. Kormentzas; C. Verikoukis","Iquadrat Informatica S.L., Barcelona, Spain; Iquadrat Informatica S.L., Barcelona, Spain; Telecommunications Technological Centre of Catalonia (CTTC/CERCA), Castelldefels, Barcelona, Spain; Information and Communication Systems Engineering Dept., University of the Aegean, Samos, Greece; Telecommunications Technological Centre of Catalonia (CTTC/CERCA), Castelldefels, Barcelona, Spain","GLOBECOM 2020 - 2020 IEEE Global Communications Conference","25 Jan 2021","2020","","","1","6","The increasing demand for fast, reliable, and robust network services has driven the telecommunications industry to design novel network architectures that employ Network Functions Virtualization and Software Defined Networking. Despite the advancements in cellular networks, there is a need for an automatic, self-adapting orchestrating mechanism that can manage the placement of resources. Deep Reinforcement Learning can perform such tasks dynamically, without any prior knowledge. In this work, we leverage a Deep Deterministic Policy Gradient Reinforcement Learning algorithm, to fully automate the Virtual Network Functions deployment process between edge and cloud network nodes. We evaluate the performance of our implementation and compare it with alternative solutions to prove its superiority while demonstrating results that pave the way for Experiential Network Intelligence and fully automated, Zero touch network Service Management.","2576-6813","978-1-7281-8298-8","10.1109/GLOBECOM42002.2020.9322512","EMI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9322512","Deep Reinforcement Learning;Software Defined Networking;Virtual Network Functions;Live Migration","Artificial neural networks;5G mobile communication;Ultra reliable low latency communication;Topology;Reinforcement learning;Optimization;Hardware","5G mobile communication;cloud computing;computer network management;computer network performance evaluation;computer network reliability;deep learning (artificial intelligence);resource allocation;software defined networking;telecommunication computing;telecommunication network planning;virtualisation","cloud network nodes;dynamic resource aware VNF placement;fast network services;robust network services;cellular networks;deep deterministic policy gradient reinforcement learning algorithm;virtual network functions deployment;experiential network intelligence;deep reinforcement learning;5G networks;reliable network services;network architectures;network functions virtualization;software defined networking;self-adapting orchestrating mechanism;resources placement;edge network node;zero touch network service management","","11","","23","IEEE","25 Jan 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning based Handoff Algorithm in End-to-End Network Slicing Enabling HetNets","F. Yang; W. Wu; X. Wang; Y. Zhang; P. Si","Faculty of Information Technology, Beijing University of Technology, Beijing, P.R. China; Faculty of Information Technology, Beijing University of Technology, Beijing, P.R. China; Faculty of Information Technology, Beijing University of Technology, Beijing, P.R. China; Faculty of Information Technology, Beijing University of Technology, Beijing, P.R. China; Faculty of Information Technology, Beijing University of Technology, Beijing, P.R. China","2021 IEEE Wireless Communications and Networking Conference (WCNC)","5 May 2021","2021","","","1","7","End-to-end network slicing, as a key technology in 5G and B5G mobile communication systems, is to enable traditional wireless networks to support different services in vertical industries. In a heterogeneous cellular network (HetNets) with network slicing functions, due to the dense deployment of base stations (BSs) and the mobility of user equipments (UEs), dynamically switching network slices (NSs) is necessary for better system performance. This paper models the handoff problem of end-to-end NS as a Markov decision process (MDP) maximizing the utility related to the UE’s profit of being served, the handoff cost and the outage penalty. Both the states of the radio access network resources and the core network resources of each end-to-end NS are considered. The deep reinforcement learning (DRL) is adopted as the solution, and a double deep Q network (DQN) based NS handoff algorithm is designed. Numerical results confirm the convergence of the DQN used to make handoff decisions and show that compared with typical handoff algorithms, the algorithm we proposed performs the best from the aspect of the cumulative reward designed in this paper.","1558-2612","978-1-7281-9505-6","10.1109/WCNC49053.2021.9417502","Natural Science Foundation of Beijing Municipality; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9417502","","Training;Network slicing;Wireless networks;System performance;Simulation;Reinforcement learning;Switches","cellular radio;learning (artificial intelligence);Markov processes;mobile communication;mobility management (mobile radio);radio access networks;telecommunication network reliability","network slicing functions;end-to-end NS;radio access network resources;core network resources;deep reinforcement learning;double deep Q network based NS;end-to-end network slicing;traditional wireless networks;heterogeneous cellular network","","9","","17","IEEE","5 May 2021","","","IEEE","IEEE Conferences"
"Towards autonomous reinforcement learning: Automatic setting of hyper-parameters using Bayesian optimization","J. C. Barsce; J. A. Palombarini; E. C. Martínez","Depto. de Ing. en Sistemas de Información, Universidad Tecnológica Nacional; CIT Villa María - CONICET-UNVM GISIQ, Universidad Tecnológica Nacional; Facultad Regional Santa Fe, Universidad Tecnológica Nacional","2017 XLIII Latin American Computer Conference (CLEI)","21 Dec 2017","2017","","","1","9","With the increase of machine learning usage by industries and scientific communities in a variety of tasks such as text mining, image recognition and self-driving cars, automatic setting of hyper-parameter in learning algorithms is a key factor for obtaining good performances regardless of user expertise in the inner workings of the techniques and methodologies. In particular, for a reinforcement learning task, the efficiency of an agent learning a policy in an uncertain environment has a strong dependency on how hyper-parameters in the algorithm are set. In this work, an autonomous framework that employs Bayesian optimization and Gaussian process regression to optimize the hyper-parameters of a reinforcement learning algorithm is proposed. A gridworld example is discussed in order to show how hyper-parameter configurations of a learning algorithm (SARSA) are iteratively improved based on two performance functions.","","978-1-5386-3057-0","10.1109/CLEI.2017.8226439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8226439","autonomous reinforcement learning;hyperparameter optimization;Bayesian optimization;Gaussian process regression","Bayes methods;Optimization;Learning (artificial intelligence);Gaussian processes;Machine learning algorithms;Electronic mail","Bayes methods;Gaussian processes;learning (artificial intelligence);regression analysis","reinforcement learning algorithm;hyper-parameter configurations;Bayesian optimization;machine learning usage;reinforcement learning task;autonomous reinforcement learning;Gaussian process regression;SARSA","","8","","22","IEEE","21 Dec 2017","","","IEEE","IEEE Conferences"
"Slice Reconfiguration based on Demand Prediction with Dueling Deep Reinforcement Learning","W. Guan; H. Zhang; V. C. M. Leung","School of Computer and Communication Engineering, University of Science and Technology Beijing; School of Computer and Communication Engineering, University of Science and Technology Beijing; Department of Electrical and Computer Engineering, The University of British Columbia","GLOBECOM 2020 - 2020 IEEE Global Communications Conference","25 Jan 2021","2020","","","1","6","Network slicing is capable of satisfying differentiated service demands of vertical industries by tailoring a common infrastructure to multiple logical networks which are isolated. Considering that the dynamic of service demands makes it difficult to maintain high quality of user experience and high revenue of tenants, slice reconfiguration is necessary to avoid performance degradation. Hence, this paper proposes an optimal and fast slice reconfiguration (OFSR) solution by leveraging advanced deep reinforcement Learning. To deal with the uncertain changes in resources requirement, a demand prediction model based on Markov renewal process is introduced in decision-making. Taking into account the operation costs of reconfiguring diversified slices and the constraints of available resources, the proposed OFSR scheme aims at obtaining high long-term revenue with low operation cost. Given that the convergence of the conventional reinforcement learning approach is slow to learn the optimal reconfiguration policy for different classes of slices, deep dueling neural network combined with Q-learning is applied to improve the speed of convergence. Simulation results validate that the proposed framework is effective in achieving long-term revenue for tenants and the dueling deep Q-learning approach performs better than other current approaches.","2576-6813","978-1-7281-8298-8","10.1109/GLOBECOM42002.2020.9322180","National Natural Science Foundation of China(grant numbers:61822104,61771044); National Natural Science Foundation of China(grant numbers:FRF-TP-19-051A1,FRF-BD-20-11A,FRF-TP-19-002C1,RC1631); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9322180","Network Slicing;Slice Reconfiguration;Deep Reinforcement Learning;Resource Allocation;Dueling Deep Q-Learning","Resource management;Reinforcement learning;Dynamic scheduling;Bandwidth;Servers;Real-time systems;Indium phosphide","deep learning (artificial intelligence);Markov processes","slice reconfiguration;performance degradation;demand prediction model;Markov renewal process;decision-making;diversified slices;long-term revenue;reinforcement learning;optimal reconfiguration policy;deep dueling neural network;dueling deep Q-learning;dueling deep reinforcement Learning;network slicing;differentiated service demands;multiple logical networks;user experience","","7","","18","IEEE","25 Jan 2021","","","IEEE","IEEE Conferences"
"Simulating Human Behavior in Fighting Games Using Reinforcement Learning and Artificial Neural Networks","M. R. F. Mendonça; H. S. Bernardino; R. F. Neto","Universidade Federal de Juiz de Fora; Department of Computer Science, Universidade Federal de Juiz de Fora; Department of Computer Science, Universidade Federal de Juiz de Fora","2015 14th Brazilian Symposium on Computer Games and Digital Entertainment (SBGames)","19 Dec 2016","2015","","","152","159","The study of intelligent agent training is of great interest to the gaming industry due to its wide application in various game genres and its capabilities of simulating a human-like behavior. In this work two machine learning techniques, namely, a reinforcement learning approach and an Artificial Neural Network (ANN), are used in a fighting game in order to allow the agent/fighter to emulate a human player. We propose a special reward function for the reinforcement learning approach that is capable of integrating specific human-like behaviors to the agent. The ANN is trained with several recorded battles of a human player. The proposed methods are compared to other two reinforcement learning methods presented in the literature. Furthermore, we present a detailed discussion of the empirical evaluations performed, regarding the training process and the main characteristics of each method used. The results obtained in the experiments indicated that the proposed methods have a good performance against human players and are also more enjoyable to play against when compared to the other existing methods.","2159-6662","978-1-4673-8843-6","10.1109/SBGames.2015.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785852","Reinforcement Learning;Intelligent Agents;Game Applications;Fighting Games","Games;Learning (artificial intelligence);Training;Process control;Neural networks;Electronic mail;Intelligent agents","computer games;learning (artificial intelligence);multi-agent systems;neural nets","intelligent agent training;machine learning;game fighting;artificial neural networks;reinforcement learning;human behavior","","7","","18","IEEE","19 Dec 2016","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Edge Computing Resource Allocation in Blockchain Network Slicing Broker Framework","Y. Gong; S. Sun; Y. Wei; M. Song","School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, P.R. China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, P.R. China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, P.R. China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, P.R. China","2021 IEEE 93rd Vehicular Technology Conference (VTC2021-Spring)","15 Jun 2021","2021","","","1","6","With the constant development of 5G technology, such as softwareization and virtualization, the novel concept of network slicing has been appeared. Blockchain is a decentralized technology for managing transactions and data which can ensure the security of transaction. Recently, the classical mobile network introduces a new role such as network slice agent to provide slices for one or across multiple operators, providing services for users or vertical industries over a larger time and space range. In this paper, we propose the blockchain network slicing broker (BNSB), an intermediate broker, which receive resources request and response then allocation resources between Network Slice Tenants (NST) and then schedule physical resources from Infrastructure Provider (InP) through smart contracts. The topology information is obtained according to the Complex Network theory and the value of nodes is defined according to their importance. In addition, Deep Reinforcement Learning algorithms is used to explore the optimal policy under the condition of meeting the service Level Agreement (SLA).","2577-2465","978-1-7281-8964-2","10.1109/VTC2021-Spring51267.2021.9449081","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9449081","network slicing;blockchain;complex network theory;resource allocation;deep reinforcement learning","Network slicing;Smart contracts;Blockchain;Reinforcement learning;Complex networks;Resource management;Security","5G mobile communication;blockchains;complex networks;computer network management;contracts;deep learning (artificial intelligence);mobile computing;network theory (graphs);resource allocation;telecommunication network topology;telecommunication scheduling;transaction processing","edge computing resource allocation;blockchain Network slicing broker framework;decentralized technology;mobile network;network slice agent;intermediate broker;allocation resources;Network Slice Tenants;physical resources;Infrastructure Provider;Complex Network theory;Deep Reinforcement Learning algorithms;NST;InP;smart contracts;topology information;service level agreement;SLA;BNSB;5G technology","","6","","16","IEEE","15 Jun 2021","","","IEEE","IEEE Conferences"
"Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain","J. Hao; T. Yang; H. Tang; C. Bai; J. Liu; Z. Meng; P. Liu; Z. Wang","College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; OPtics and Electronics (iOPEN) and the School of Cyberspace, School of Artificial Intelligence, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","21","Deep reinforcement learning (DRL) and deep multiagent reinforcement learning (MARL) have achieved significant success across a wide range of domains, including game artificial intelligence (AI), autonomous vehicles, and robotics. However, DRL and deep MARL agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning toward the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and nonstationary co-learners. In this article, we conduct a comprehensive survey on existing exploration methods for both single-agent RL and multiagent RL. We start the survey by identifying several key challenges to efficient exploration. Then, we provide a systematic survey of existing approaches by classifying them into two major categories: uncertainty-oriented exploration and intrinsic motivation-oriented exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for DRL on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in DRL and deep MARL and point out a few future directions.","2162-2388","","10.1109/TNNLS.2023.3236361","National Science Fund for Distinguished Young Scholars(grant numbers:62025602); National Natural Science Foundation of China(grant numbers:U22B2036,11931015,U1836214); New Generation of Artificial Intelligence Science and Technology Major Project of Tianjin(grant numbers:19ZXZNGX00010); Tencent Foundation and XPLORER PRIZE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021988","Deep reinforcement learning (DRL);exploration;intrinsic motivation;multiagent systems;uncertainty","Reinforcement learning;Markov processes;Games;Deep learning;Uncertainty;Taxonomy;Benchmark testing","","","","5","","","IEEE","19 Jan 2023","","","IEEE","IEEE Early Access Articles"
"RIATA: A Reinforcement Learning-Based Intelligent Routing Update Scheme for Future Generation IoT Networks","Z. Nain; A. Musaddiq; Y. A. Qadri; A. Nauman; M. K. Afzal; S. W. Kim","Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, South Korea; ICT Convergence Research Center, Kumoh National Institute of Technology, Gumi, South Korea; Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, South Korea; Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, South Korea; Department of Computer Science, Comsats University Islamabad, Wah campus Rawalpindi, Pakistan; Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, South Korea","IEEE Access","8 Jun 2021","2021","9","","81161","81172","Future generation Internet of Things (IoT) communication infrastructure is expected to pave the path for innovative applications like smart cities, smart grids, smart industries, and smart healthcare. To support these diverse applications, the communication protocols are required to be adaptive and intelligent. At the network layer, an efficient and lightweight algorithm known as trickle-timer is designed to perform the route updates and it utilizes control messages to share the updated route information between IoT nodes. Trickle-timer tends to generate higher control overhead ratio and achieves lower reliability. Therefore, this article aims to propose an RL-based Intelligent Adaptive Trickle-Timer Algorithm (RIATA). The proposed algorithm performs three-fold optimization of the trickle-timer algorithm. Firstly, the RIATA assigns higher probability to control message transmission to nodes that have received an inconsistent control message in the past intervals. Secondly, the RIATA utilizes RL to learn the optimal policy to transmit or suppress a control message in the current network environment. Lastly, the RIATA selects an adaptive redundancy constant value to avoid unnecessary transmissions of control messages. Simulation results show that RIATA outperforms the other state-of-the-art mechanisms in terms of reducing control overhead ratio by an average of 21%, decreasing the average total power consumption by 10%, and increasing the packet delivery ratio by 4% on an average.","2169-3536","","10.1109/ACCESS.2021.3084217","Ministry of Science and Information and Communication Technology (ICT) (MSIT), South Korea, under the Information Technology Research Center (ITRC) support program supervised by the Institute for Information and Communications Technology Planning and Evaluation (IITP)(grant numbers:IITP-2021-2016-0-00313); Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education(grant numbers:2018R1D1A1A09082266); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442721","Internet of Things (IoT);trickle-timer;reinforcement learning;RPL","Redundancy;Convergence;Energy consumption;Protocols;Internet of Things;Routing protocols;Linear programming","electronic messaging;Internet of Things;learning (artificial intelligence);next generation networks;optimisation;power consumption;probability;routing protocols;telecommunication computing;telecommunication network reliability;telecommunication power management","current network environment;adaptive redundancy constant value;future generation IoT networks;innovative applications;communication protocols;network layer;route updates;IoT nodes;higher control overhead ratio;lower reliability;higher probability;message transmission;inconsistent control message;Internet of Things communication infrastructure;future generation Internet of Things;reinforcement learning-based intelligent routing update scheme;updated route information sharing;RL-based intelligent adaptive trickle-timer algorithm;three-fold optimization;average total power consumption reduction;packet delivery ratio;RIATA algorithm","","5","","31","CCBY","27 May 2021","","","IEEE","IEEE Journals"
"RAN Slice Strategy Based on Deep Reinforcement Learning for Smart Grid","S. Meng; Z. Wang; H. Ding; S. Wu; X. Li; P. Zhao; C. Zhu; X. Wang","China Electric Power Research Institute Co., Ltd, Beijing, China; China Electric Power Research Institute Co., Ltd, Beijing, China; China Electric Power Research Institute Co., Ltd, Beijing, China; China Electric Power Research Institute Co., Ltd, Beijing, China; China Electric Power Research Institute Co., Ltd, Beijing, China; Information & Telecommunications Company State Grid Shandong Electric Power Company, Jinan, Shandong, China; Information & Telecommunications Company State Grid Shandong Electric Power Company, Jinan, Shandong, China; Beijing University of Posts and Telecommunications, Beijing, China","2019 Computing, Communications and IoT Applications (ComComAp)","2 Mar 2020","2019","","","6","11","As one of the important application scenarios of the Green Internet of Things (IoT), the development of smart grid is an important means to promote the energy system revolution. Future power grid will rely heavily on smart devices based on the IoT concept. As the 5G standard matures and is put into commercial use, the green interconnected 5G mobile network has shown great potential. With 5G network slicing technology, a wireless private network can be virtualized for each industry to better meet the requirements of smart grid security, reliability and flexibility. Considering the differentiated service characteristics of the smart grid and the challenges of flexibility and adaptability of the communication platform, this paper aims to solve the resource allocation problem of the radio access network (RAN) slice of smart grid. Since the type of service arrival of smart grid is unknown and lacks prior knowledge, deep reinforcement learning (DRL) is used to conduct this research. The mapping from the RAN slice resource management to DRL of the smart grid is analyzed at first, and then discussion from the elastic application and the real-time application is presented. Furthermore, a smart grid RAN slice allocation strategy based on DRL is proposed. Finally, simulation results prove that the proposed algorithm can satisfy the maximization of the resource allocation demand of the smart grid on the RAN side while reducing the cost.","","978-1-7281-1973-1","10.1109/ComComAp46287.2019.9018826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9018826","Green IoT;smart grid;5G;network slicing;deep reinforcement learning","Smart grids;Resource management;Bandwidth;Network slicing;5G mobile communication;Real-time systems;Machine learning","Internet of Things;learning (artificial intelligence);power engineering computing;radio access networks;resource allocation;smart power grids;telecommunication computing","5G network slicing technology;smart grid security;radio access network;resource allocation problem;communication platform;wireless private network;5G standard;green Internet of Things;RAN slice allocation;green interconnected 5G mobile network;deep reinforcement learning","","5","","16","IEEE","2 Mar 2020","","","IEEE","IEEE Conferences"
"Energy-Aware Deep Reinforcement Learning Scheduling for Sensors Correlated in Time and Space","J. Hribar; A. Marinescu; A. Chiumento; L. A. Dasilva","CONNECT, Trinity College Dublin, Dublin 2, Ireland; Center for Intelligent Power, Eaton, Dublin 4, Ireland; Pervasive Systems–Electrical Engineering, Mathematics and Computer Science Faculty, University of Twente, Enschede, NB, The Netherlands; Commonwealth Cyber Initiative, Virginia Tech, Blacksburg, VA, USA","IEEE Internet of Things Journal","25 Apr 2022","2022","9","9","6732","6744","Millions of battery-powered sensors deployed for monitoring purposes in a multitude of scenarios, e.g., agriculture, smart cities, industry, etc., require energy-efficient solutions to prolong their lifetime. When these sensors observe a phenomenon distributed in space and evolving in time, it is expected that collected observations will be correlated in time and space. This article proposes a deep reinforcement learning (DRL)-based scheduling mechanism capable of taking advantage of correlated information. The designed solution employs deep deterministic policy gradient (DDPG) algorithm. The proposed mechanism can determine the frequency with which sensors should transmit their updates, to ensure accurate collection of observations, while simultaneously considering the energy available. The solution is evaluated with multiple data sets containing environmental observations obtained in multiple real deployments. The real observations are leveraged to model the environment with which the mechanism interacts as realistically as possible. The proposed solution can significantly extend the sensors’ lifetime and is compared to an idealized, all-knowing scheduler to demonstrate that its performance is near optimal. Additionally, the results highlight the unique feature of the proposed design, energy-awareness, by displaying the impact of sensors’ energy levels on the frequency of updates.","2327-4662","","10.1109/JIOT.2021.3114102","European Regional Development Fund through the SFI Research Centres Programme(grant numbers:13/RC/2077_2 SFI CONNECT); SFI-NSFC Partnership Programme(grant numbers:17/NSFC/5224); Commonwealth Cyber Initiative, in Virginia, USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9542952","Deep reinforcement learning (DRL);Internet of Things (IoT);low-power sensors;reinforcement learning","Sensor systems;Intelligent sensors;Sensor phenomena and characterization;Internet of Things;Job shop scheduling;Logic gates;Reinforcement learning","deep learning (artificial intelligence);energy conservation;gradient methods;reinforcement learning;sensor placement;telecommunication computing;telecommunication power management;telecommunication scheduling;wireless sensor networks","deep deterministic policy gradient algorithm;multiple data sets;multiple real deployments;battery-powered sensors;smart cities;energy-efficient solutions;deep reinforcement learning-based scheduling mechanism;correlated information;energy-aware deep reinforcement learning scheduling;DDPG algorithm;DRL-based scheduling mechanism","","4","","44","IEEE","21 Sep 2021","","","IEEE","IEEE Journals"
"A Deep Reinforcement Learning-based Resource Management Scheme for SDN-MEC-supported XR Applications","B. Trinh; G. -M. Muntean","Insight SFI Research Centre for Data Analytics, Dublin City University, Dublin, Republic of Ireland; School of Electronic Engineering, Dublin City University, Dublin, Republic of Ireland","2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC)","10 Feb 2022","2022","","","790","795","The Multi-Access Edge Computing (MEC) paradigm provides a promising solution for efficient computing services at edge nodes, such as base stations (BS), access points (AP), etc. By offloading highly intensive computational tasks to MEC servers, critical benefits in terms of reducing energy consumption at mobile devices and lowering processing latency can be achieved to support high Quality of Service (QoS) to many applications. Among the services which would benefit from MEC deployments are eXtended Reality (XR) applications which are receiving increasing attention from both academia and industry. XR applications have high resource requirements, mostly in terms of network bandwidth, computation and storage. Often these resources are not available in classic network architectures and especially not when XR applications are run by mobile devices. This paper leverages the concepts of Software Defined Networking (SDN) and Network Function Virtualization (NFV) to propose an innovative resource management scheme considering heterogeneous QoS requirements at the MEC server level. The resource assignment is formulated by employing a Deep Reinforcement Learning (DRL) technique to support high quality of XR services. The simulation results show how our proposed solution outperforms other state-of-the-art resource management-based schemes.","2331-9860","978-1-6654-3161-3","10.1109/CCNC49033.2022.9700522","Science Foundation Ireland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9700522","SDN;NFV;Edge computing;QoS;extended Reality","Simulation;Quality of service;Reinforcement learning;Mobile handsets;Network function virtualization;Servers;Resource management","computer network management;deep learning (artificial intelligence);distributed processing;mobile computing;mobile radio;multi-access systems;quality of service;reinforcement learning;resource allocation;software defined networking;telecommunication power management;virtualisation","mobile devices;innovative resource management scheme;heterogeneous QoS requirements;MEC server level;state-of-the-art resource management-based schemes;SDN-MEC-supported XR applications;edge nodes;base stations;highly intensive computational tasks;MEC servers;energy consumption;software defined networking;network function virtualization;deep reinforcement learning-based resource management scheme;multiaccess edge computing paradigm;DRL technique;quality of service;extended reality applications","","4","","24","IEEE","10 Feb 2022","","","IEEE","IEEE Conferences"
"A Hybrid Reinforcement Learning-Based Model for the Vehicle Routing Problem in Transportation Logistics","T. Phiboonbanakit; T. Horanont; V. -N. Huynh; T. Supnithi","School of Knowledge Science, Japan Advanced Institute of Science and Technology, Nomi, Ishikawa, Japan; School of Information, Computer, and Communication Technology, Sirindhorn International Institute of Technology, Thammasat University, Pathum Thani, Thailand; School of Knowledge Science, Japan Advanced Institute of Science and Technology, Nomi, Ishikawa, Japan; National Electronics and Computer Technology Center (NECTEC), National Science and Technology Development Agency, Pathum Thani, Thailand","IEEE Access","17 Dec 2021","2021","9","","163325","163347","Currently, the number of deliveries handled by transportation logistics is rapidly increasing because of the significant growth of the e-commerce industry, resulting in the need for improved functional vehicle routing measures for logistic companies. The effective management of vehicle routing helps companies reduce operational costs and increases its competitiveness. The vehicle routing problem (VRP) seeks to identify optimal routes for a fleet of vehicles to deliver goods to customers while simultaneously considering changing requirements and uncertainties in the transportation environment. Due to its combinatorial nature and complexity, conventional optimization approaches may not be practical to solve VRP. In this paper, a new optimization model based on reinforcement learning (RL) and a complementary tree-based regression method is proposed. In our proposed model, when the RL agent performs vehicle routing optimization, its state and action are fed into the tree-based regression model to assess whether the current route is feasible according to the given environment, and the response received is used by the RL agent to adjust actions for optimizing the vehicle routing task. The procedure repeats iteratively until the maximum iteration is reached, then the optimal vehicle route is returned and can be utilized to assist in decision making. Multiple logistics agency case studies are conducted to demonstrate the application and practicality of the proposed model. The experimental results indicate that the proposed technique significantly improves profit gains up to 37.63% for logistics agencies compared with the conventional approaches.","2169-3536","","10.1109/ACCESS.2021.3131799","Excellent Thai Student Scholarship from the Sirindhorn International Institute of Technology, Thammasat University(grant numbers:ETS-G-S1Y17/061); SIIT-JAIST Dual-Degree Scholarship Program; U.S. Office of Naval Research Global(grant numbers:N62909-19-1-2031); Center of Excellence in Urban Mobility Research and Innovation, Thammasat University, Pathum Thani, Thailand; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9631282","Freight;intelligent transportation;logistics;policy;reinforcement learning;vehicle routing problem","Adaptation models;Vehicle routing;Optimization;Logistics;Costs;Uncertainty;Transportation","decision making;electronic commerce;learning (artificial intelligence);logistics;optimisation;profitability;regression analysis;transportation;trees (mathematics);vehicle routing","complementary tree-based regression method;RL agent performs vehicle routing optimization;tree-based regression model;current route;vehicle routing task;optimal vehicle route;multiple logistics agency case studies;logistics agencies;hybrid reinforcement learning-based model;vehicle routing problem;transportation logistics;improved functional vehicle routing measures;logistic companies;VRP;optimal routes;transportation environment;conventional optimization approaches;optimization model","","4","","44","CCBY","30 Nov 2021","","","IEEE","IEEE Journals"
"SMART USAGE OF MULTIPLE RAT IN IOT-ORIENTED 5G NETWORKS: A REINFORCEMENT LEARNING APPROACH","R. M. Sandoval; S. Canovas-Carrasco; A. -J. Garcia-Sanchez; J. Garcia-Haro","Department of Information and Communication Technologies, Technical University of Cartagena, Cartagena, Spain; Department of Information and Communication Technologies, Technical University of Cartagena, Cartagena, Spain; Department of Information and Communication Technologies, Technical University of Cartagena, Cartagena, Spain; Department of Information and Communication Technologies, Technical University of Cartagena, Cartagena, Spain","2018 ITU Kaleidoscope: Machine Learning for a 5G Future (ITU K)","3 Jan 2019","2018","","","1","8","Smart Cities and Smart Industries are the flagships of the future IoT due to their potential to revolutionize the way in which people live and produce in advanced societies. In these two scenarios, a robust and ubiquitous communication infrastructure is needed to accommodate the traffic generated by the 10 billion devices that are expected by the year 2020. Due to its future world-wide presence, 5G is called to be this enabling technology. However, 5G is not a perfect solution, thus providing IoT nodes with different Radio Access Technologies (RATs) would allow them to exploit the various benefits offered by each RAT (such as lower power consumption or reduced operational costs). By making use of the mathematical framework of Reinforcement Learning, we have formulated the problem of deciding which RAT should an IoT node employ when reporting events. These so-called transmission policies maximize a predefined reward closely related to classical throughput while keeping power consumption and operational costs below a certain limit. A set of simulations are performed for IoT nodes provided with two RATs: LoRa and 5G. The results obtained are compared to those achieved under other intuitive policies to further highlight the benefits of our proposal.","","978-92-61-26921-0","10.23919/ITU-WT.2018.8597940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8597940","5G;IoT;Reinforcement Learning;Multi-RAT;LPWAN;Machine Learning","Radio access technologies;5G mobile communication;Cellular networks;Performance evaluation;Power demand;Mathematical model;Reinforcement learning","Internet of Things;learning (artificial intelligence);radio access networks;telecommunication traffic","Smart usage;multiple rat;IoT-oriented 5G networks;reinforcement learning approach;advanced societies;robust communication infrastructure;ubiquitous communication infrastructure;IoT nodes;operational costs;Radio Access Technologies;power consumption;transmission policies","","3","","31","","3 Jan 2019","","","IEEE","IEEE Conferences"
"Dynamic Spectrum Access for Internet-of-Things Based on Federated Deep Reinforcement Learning","F. Li; B. Shen; J. Guo; K. -Y. Lam; G. Wei; L. Wang","School of Information and Electronic Engineering, Zhejiang Gongshang University, Hangzhou, China; School of Information and Electronic Engineering, Zhejiang Gongshang University, Hangzhou, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Information and Electronic Engineering, Zhejiang Gongshang University, Hangzhou, China; College of Marine Electrical Engineering, Dalian Maritime University, Dalian, China","IEEE Transactions on Vehicular Technology","15 Jul 2022","2022","71","7","7952","7956","The explosive growth of Internet-of-Things (IoT) applications such as smart cities and Industry 4.0 have led to drastic increase in demand for wireless bandwidth, hence motivating the rapid development of new techniques for enhancing spectrum utilization needed by new generation wireless communication technologies. Among others, dynamic spectrum access (DSA) is one of the most widely accepted approaches. In this paper, as an enhancement of existing works, we take into consideration of inter-node collaborations in a dynamic spectrum environment. Typically, in such distributed circumstances, intelligent dynamic spectrum access almost invariably relies on self-learning to achieve dynamic spectrum access improvement. Whereas, this paper proposes a DSA scheme based on deep reinforcement learning to enhance spectrum and access efficiency. Unlike traditional Q-learning-based DSA, we introduce the following to enhance the spectrum efficiency in dynamic IoT spectrum environments. First, deep double Q-learning is adopted to perform local self-spectrum-learning for IoT terminals in order to achieve better dynamic access accuracy. Second, to accelerate learning convergence, federated learning (FL) in edge nodes is used to improve the self-learning. Third, multiple secondary users, who do not interfere with each other and have similar operation condition, are clustered for federated learning to enhance the efficiency of deep reinforcement learning. Comparing with the traditional distributed DSA with deep learning, the proposed scheme has faster access convergence speed due to the characteristic of global optimization for federated learning. Based on this, a framework of federated deep reinforcement learning (FDRL) for DSA is proposed. Furthermore, this scheme preserves privacy of IoT users in that FDRL only requires model parameters to be uploaded to edge servers. Simulations are performed to show the effectiveness of theproposed FDRL-based DSA framework.","1939-9359","","10.1109/TVT.2022.3166535","National Research Foundation Singapore; Strategic Capability Research Centres Funding Initiative any opinions; National Research Foundation Singapore; Fundamental Research Funds for the Central Universities(grant numbers:3132021335); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9760286","Deep reinforcement learning;dynamic spectrum access;federated learning;Internet of Things (IoT)","Internet of Things;Collaborative work;Wireless communication;Training;Interference;Dynamic spectrum access;Servers","cognitive radio;deep learning (artificial intelligence);Internet of Things;radio spectrum management;telecommunication computing","access efficiency;spectrum efficiency;dynamic IoT spectrum environments;deep double Q-learning;local self-spectrum-learning;federated deep reinforcement learning;Internet-of-Things applications;spectrum utilization;intelligent dynamic spectrum access;Q-learning-based DSA;new generation wireless communication technologies;FDRL-based DSA framework;distributed DSA;global optimization","","3","","14","IEEE","19 Apr 2022","","","IEEE","IEEE Journals"
"Provision of a Recommender Model for Blockchain-Based IoT with Deep Reinforcement Learning","E. Rabieinejad; S. Mohammadi; M. Yadegari","K.N. Toosi University of Technology, Tehran, Iran; K.N. Toosi University of Technology, Tehran, Iran; K.N. Toosi University of Technology, Tehran, Iran","2021 5th International Conference on Internet of Things and Applications (IoT)","7 Jul 2021","2021","","","1","8","With developments in human societies and the information and communication technology, the Internet of Things (IoT) has penetrated various aspects of daily life and different industries. The newly emerging blockchain technology has become a viable solution to the IoT security due to its inherent characteristics such as distribution, security, immutability, and traceability. However, integrating the IoT with the blockchain technology faces certain challenges such as latency, throughput, device power limitation, and scalability. Recent studies have focused on the role of artificial intelligence methods in improving the IoT performance in a blockchain. According to their results, there are only a few effects on the improvement of IoT-based performance with limited power. This study proposes a conceptual model to improve the blockchain throughput in IoT-based devices with limited power through deep reinforcement learning. This model benefits from a recommender agent based on deep reinforcement learning in the mobile edge computing layer to improve the throughput and select the right mining method.","","978-1-6654-4448-4","10.1109/IoT52625.2021.9469708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9469708","IoT;blockchain;deep reinforcement learning","Performance evaluation;Analytical models;Computational modeling;Scalability;Reinforcement learning;Blockchain;Throughput","blockchains;data mining;Internet of Things;learning (artificial intelligence);recommender systems","recommender model;blockchain-based IoT;deep reinforcement learning;human societies;communication technology;newly emerging blockchain technology;IoT security;immutability;device power limitation;artificial intelligence methods;IoT performance;IoT-based performance;conceptual model;blockchain throughput;IoT-based devices;model benefits;recommender agent","","3","","19","IEEE","7 Jul 2021","","","IEEE","IEEE Conferences"
"Policy Controlled Multi-domain cloud-network Slice Orchestration Strategy based on Reinforcement Learning","A. I. Swapna; R. V. Rosa; C. E. Rothenberg; R. Pasquini; J. Baliosian","University of Campinas (UNICAMP), Sao Paulo, Brazil; University of Campinas (UNICAMP), Sao Paulo, Brazil; University of Campinas (UNICAMP), Sao Paulo, Brazil; Federal University of Uberlândia (UFU), Brazil; University of the Republic, Uruguay","2020 IEEE Conference on Network Function Virtualization and Software Defined Networks (NFV-SDN)","24 Dec 2020","2020","","","167","173","The concept of network slicing plays a thriving role as 5G rolls out business models vouched by different stakeholders. The dynamic and variable characterization of end-to-end cloud-network slices encompasses the composition of different slice parts laying at different administrative domains. Following a profit-maximizing Slice-as-a-Service (SaaS) model, such a multi-domain facet offers promising business opportunities in support of diverse vertical industries, rendering to network slicing marketplace members the roles of Infrastructure Provider, Slice Provider, and Tenants. The effective realization of SaaS approaches introduces a dynamic resource allocation problem, manifested as challenging run-time decisions upon on-demand slice part requests. The Orchestrator is hence responsible to perform an optimized decision on-the-fly on which elasticity requests to address based on an orchestration policy defined within the context of Network Slice architecture for the followed revenue model. This paper presents a slice management strategy for such an orchestrator can follow, based on reinforcement learning, able to efficiently orchestrate slice elasticity requests to comprehend the maximum revenue for the stakeholders of end-to-end network slice lifecycle. The proposed strategy orients a Slice Orchestrator to learn which slice requests to address as per availability of the required resources at the different participating Infrastructure Providers. The experimental results show the Reinforcement Learning based Orchestrator outperforms several benchmark heuristics focused on revenue maximization.","","978-1-7281-8159-2","10.1109/NFV-SDN50289.2020.9289852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9289852","","Cloud computing;Network slicing;Software as a service;Reinforcement learning;Elasticity;Stakeholders;Business","cloud computing;learning (artificial intelligence);resource allocation","end-to-end network slice lifecycle;Slice Orchestrator;slice requests;different participating Infrastructure Providers;reinforcement learning;policy controlled multidomain cloud-network Slice orchestration strategy;thriving role;business models;dynamic characterization;variable characterization;end-to-end cloud-network slices;different slice parts;different administrative domains;profit-maximizing Slice-as-a-Service model;multidomain facet;promising business opportunities;network slicing marketplace members;Slice Provider;dynamic resource allocation problem;part requests;orchestration policy;Network Slice architecture;followed revenue model;slice management strategy;efficiently orchestrate slice elasticity requests","","2","","24","IEEE","24 Dec 2020","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Approach for Point Cloud Video Transmissions","H. Lin; B. Zhang; Y. Cao; Z. Liu; X. Chen","Zhengzhou University, China; Zhengzhou University, China; Zhengzhou University, China; University of Electro-Communications, Japan; VTT Technical Research Centre of Finland, Finland","2021 IEEE 94th Vehicular Technology Conference (VTC2021-Fall)","10 Dec 2021","2021","","","1","5","The point cloud videos, thanks to the multi-view and immersive experiences, have recently attracted notable attentions from both academia and industry. Due to the high data volume, a point cloud video also raises the challenge of quality-of-experience (QoE), which is in terms of the balance between playback quality and buffering delay during the transmission under time-varying system conditions. In this paper, we propose a deep reinforcement learning (DRL) approach to optimize the expected long-term QoE for the client. Over the time horizon, the proposed approach learns to select the tiles of the corresponding video for transmissions in an iterative way. Under various settings, numerical experiments based on real throughput data traces are conducted to evaluate the proposed approach. Compared to the baselines, our approach not only enhances the video quality but also reduces the re-buffering time, obtaining an improvement of average QoE for the client by 9%–14%.","2577-2465","978-1-6654-1368-8","10.1109/VTC2021-Fall52928.2021.9625496","National Natural Science Foundation of China(grant numbers:61972092); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9625496","Point cloud video;quality of experience;Markov decision process;deep reinforcement learning","Point cloud compression;Measurement;Vehicular and wireless technologies;System dynamics;Reinforcement learning;Throughput;Quality assessment","deep learning (artificial intelligence);quality of experience;reinforcement learning;video signal processing","point cloud video transmissions;immersive experiences;data volume;quality-of-experience;playback quality;buffering delay;time-varying system conditions;deep reinforcement learning approach;long-term QoE;video quality","","2","","16","IEEE","10 Dec 2021","","","IEEE","IEEE Conferences"
"Cycle-level vs. Second-by-Second Adaptive Traffic Signal Control using Deep Reinforcement Learning","S. M. A. Shabestary; B. Abdulhai; H. Ma; Y. Huo","Civil & Mineral Engineering Department, University of Toronto, Toronto, ON, Canada; Civil & Mineral Engineering Department, University of Toronto, Toronto, ON, Canada; Huawei Technologies Canada, Markham, ON, Canada; Huawei Technologies Co.Ltd., Hangzhou, China","2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)","24 Dec 2020","2020","","","1","8","With unrelenting growth in population and urbanization, cities face escalating challenges in providing fast and reliable mobility. A significant source of delay for urban commuters is related to signalized intersections. optimizing traffic signals to maximize the capacity of intersections is of critical importance for cities. Although this topic has been around for decades, recently, Deep Reinforcement Learning (DRL) approaches have begun to be used for intelligent traffic signal control. The result is a promising new generation of traffic signal controllers (TSCs). They are model-free, adaptive, and capable of exploiting newer pervasive sensory technologies (e.g., connected vehicles). While most of the RL-based TSCs focus on second-by-second level decision making, the industry favors controllers that manipulate signals less frequently. This changes the RL control problem from being in a discrete to a continuous action space. With the lack of RL-based TSCs that can handle a continuous action space, we propose a novel RL-based cycle-level TSC that determines the phase timings once every cycle. Our controller uses Proximal Policy optimization (PPO) as one of the most promising continuous action RL algorithms to produce signal timings in a cycle. We test our proposed controller against one RL-based second-level controller as well as an optimized fixed-time traffic signal controller and compare the results.","","978-1-7281-4149-7","10.1109/ITSC45102.2020.9294171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9294171","","Aerospace electronics;Reinforcement learning;Green products;Switches;Optimization;Optimal control;Delays","decision making;learning (artificial intelligence);road traffic control;traffic engineering computing","level decision making;continuous action space;proximal policy optimization;signal timings;RL-based second-level controller;fixed-time traffic signal controller;cycle-level;adaptive traffic signal control;urbanization;reliable mobility;urban commuters;signalized intersections;traffic signals;deep reinforcement learning approaches;intelligent traffic signal control;RL-based cycle-level TSC","","2","","28","IEEE","24 Dec 2020","","","IEEE","IEEE Conferences"
"Reinforcement learning for condition-based control of gas turbine engines","I. Sanusi; A. Mills; P. Trodden; V. Kadirkamanathan; T. Dodd","Department of Automatic Control and Systems Engineering, University of Sheffield, Sheffield, United Kingdom; Department of Automatic Control and Systems Engineering, University of Sheffield, Sheffield, United Kingdom; Department of Automatic Control and Systems Engineering, University of Sheffield, Sheffield, United Kingdom; Department of Automatic Control and Systems Engineering, University of Sheffield, Sheffield, United Kingdom; Department of Automatic Control and Systems Engineering, University of Sheffield, Sheffield, United Kingdom","2019 18th European Control Conference (ECC)","15 Aug 2019","2019","","","3928","3933","A condition-based control framework is proposed for gas turbine engines using reinforcement learning and adaptive dynamic programming (RL-ADP). The system behaviour, specifically the fuel efficiency function and constraints, exhibit unknown degradation patterns which vary from engine to engine. Due to these variations, accurate system models to describe the true system states over the life of the engines are difficult to obtain. Consequently, model-based control techniques are unable to fully compensate for the effects of the variations on the system performance. The proposed RL-ADP control framework is based on Q-learning and uses measurements of desired performance quantities as reward signals to learn and adapt the system efficiency maps. This is achieved without knowledge of the system variation or degradation dynamics, thus providing a through life adaptation strategy that delivers improved system performance. In order to overcome the long standing difficulties associated with the application of adaptive techniques in a safety critical setting, a dual-control loop structure is proposed in the implementation of the RL-ADP scheme. The overall control framework maintains guarantees on the main thrust control loop whilst extracting improved performance as the engine degrades by tuning sets of variable geometry components in the RL-ADP control loop. Simulation results on representative engine data sets demonstrate the effectiveness of this approach as compared to an industry standard gain scheduling.","","978-3-907144-00-8","10.23919/ECC.2019.8795878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8795878","","","adaptive control;control system synthesis;dynamic programming;engines;gas turbines;learning (artificial intelligence)","performance quantities;representative engine data sets;RL-ADP control loop;engine degrades;main thrust control loop;RL-ADP scheme;dual-control loop structure;adaptive techniques;life adaptation strategy;system efficiency maps;Q-learning;RL-ADP control framework;system performance;model-based control techniques;accurate system models;unknown degradation patterns;fuel efficiency function;system behaviour;adaptive dynamic programming;condition-based control framework;gas turbine engines;reinforcement learning","","2","","23","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Reinforcement Learning With Constrained Uncertain Reward Function Through Particle Filtering","O. Dogru; R. Chiplunkar; B. Huang","Department of Chemical and Materials Engineering, University of Alberta, Edmonton, AB, Canada; Department of Chemical and Materials Engineering, University of Alberta, Edmonton, AB, Canada; Department of Chemical and Materials Engineering, University of Alberta, Edmonton, AB, Canada","IEEE Transactions on Industrial Electronics","10 Feb 2022","2022","69","7","7491","7499","Advancements in computational sciences have stimulated the use of an abundant amount of data in control and monitoring. Recent studies have reemphasized that the performance of the data-driven control significantly depends on the data quality. This quality is affected by uncertainties such as process and measurement noises. This study addresses a type of noise commonly seen in industry and shows how it degrades the performance of a deep reinforcement learning (RL) agent. Then, a novel filter is proposed to reduce the effect of this noise when it causes skewed probabilistic distributions in the reward functions. We demonstrate that the RL policy can be improved by using a constrained filter with a combination of the optimal filtering and RL concepts. The proposed algorithm is applied to a pilot-scale separation process that resembles an industrial separation vessel. The experimental results demonstrate that the proposed algorithm can improve the process operation efficiency.","1557-9948","","10.1109/TIE.2021.3099234","Natural Sciences and Engineering Research Council of Canada; Industrial Research Chair Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499958","Actor–critic;computer vision;particle filtering;process control;reinforcement learning (RL);state estimation","Uncertainty;Mathematical model;Reinforcement learning;Robustness;State estimation;Noise measurement;Sensors","chemical engineering computing;deep learning (artificial intelligence);process control;reinforcement learning;separation;statistical distributions","data-driven control;data quality;measurement noises;deep reinforcement learning agent;skewed probabilistic distributions;reward functions;RL policy;constrained filter;optimal filtering;RL concepts;pilot-scale separation process;industrial separation vessel;process operation efficiency;constrained uncertain reward function;computational sciences","","2","","52","IEEE","28 Jul 2021","","","IEEE","IEEE Journals"
"A Contextual Reinforcement Learning Approach for Electricity Consumption Forecasting in Buildings","D. Ramos; P. Faria; L. Gomes; Z. Vale","Intelligent Systems Associate Laboratory (LASI), Research Group on Intelligent Engineering and Computing for Advanced Innovation and Development (GECAD), Polytechnic Institute of Porto (ISEP/IPP), Porto, Portugal; Intelligent Systems Associate Laboratory (LASI), Research Group on Intelligent Engineering and Computing for Advanced Innovation and Development (GECAD), Polytechnic Institute of Porto (ISEP/IPP), Porto, Portugal; Intelligent Systems Associate Laboratory (LASI), Research Group on Intelligent Engineering and Computing for Advanced Innovation and Development (GECAD), Polytechnic Institute of Porto (ISEP/IPP), Porto, Portugal; Intelligent Systems Associate Laboratory (LASI), Research Group on Intelligent Engineering and Computing for Advanced Innovation and Development (GECAD), Polytechnic Institute of Porto (ISEP/IPP), Porto, Portugal","IEEE Access","14 Jun 2022","2022","10","","61366","61374","The energy management of buildings plays a vital role in the energy sector. With that in mind, and targeting an accurate forecast of electricity consumption, in the present paper is aimed to provide decision on the best prediction algorithm for each context. It may also increase energy usage related with renewables. In this way, the identification of different contexts is an advantage that may improve prediction accuracy. This paper proposes an innovative approach where a decision tree is used to identify different contexts in energy patterns. One week of five-minutes data sampling is used to test the proposed methodology. Each context is evaluated with a decision criterion based on reinforcement learning to find the best suitable forecasting algorithm. Two forecasting models are approached in this paper, based on K-Nearest Neighbor and Artificial Neural Networks, to illustrate the application of the proposed methodology. The reinforcement learning criterion consists of using the Multiarmed Bandit algorithm. The obtained results validate the adequacy of the proposed methodology in two case-studies: building; and industry.","2169-3536","","10.1109/ACCESS.2022.3180754","Infrastructure and Energy management for Intelligent carbon-Neutral smArt cities (RETINA)(grant numbers:NORTE-01-0145-FEDER-000062); Norte Portugal Regional Operational Programme(grant numbers:NORTE 2020); PORTUGAL 2020 Partnership Agreement, through the European Regional Development Fund (ERDF)(grant numbers:CEECIND/02887/2017); Research Group on Intelligent Engineering and Computing for Advanced Innovation and Development (GECAD)(grant numbers:UIDB/00760/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9791389","Consumption forecast;contextual operation;decision tree;reinforcement learning","Forecasting;Decision trees;Reinforcement learning;Buildings;Artificial neural networks;Predictive models;Prediction algorithms","building management systems;decision trees;load forecasting;nearest neighbour methods;neural nets;power consumption;power engineering computing;reinforcement learning","contextual reinforcement learning approach;electricity consumption forecasting;energy sector;energy usage;decision tree;energy patterns;data sampling;artificial neural networks;multiarmed bandit algorithm;building energy management;renewable energy;k-nearest neighbor model","","2","","18","CCBY","8 Jun 2022","","","IEEE","IEEE Journals"
"Stiffness Control for a Soft Robotic Finger based on Reinforcement Learning for Robust Grasping","J. Dai; M. Zhu; Y. Feng","College of Information Engineering, Zhejiang University of Technology, Hangzhou, China; College of Information Engineering, Zhejiang University of Technology, Hangzhou, China; College of Information Engineering, Zhejiang University of Technology, Hangzhou, China","2021 27th International Conference on Mechatronics and Machine Vision in Practice (M2VIP)","7 Jan 2022","2021","","","540","545","Soft robotic grippers have attracted a great attention for industry application. However, due to the characteristics of non-linearity, it is challenging to control the stiffness of soft robotic grippers. As some of the controlling methods of the soft gripper’s stiffness are developed by physical models which are based on force analysis with strong assumptions, it can be difficult to apply to real-life scenarios. Recent efforts to resolve the problem of soft grippers’ stiffness control based on reinforcement learning might become a potential resolution to this challenge. In this paper, a model-free reinforcement learning based on Deep Deterministic Policy Gradient (DDPG) is trained for controlling the stiffness of the soft gripper with rotation jamming layers. The control strategy is validated in the simulation, the best results of Errori (i=1, 2, 3, 4) reported in this paper are 11.69%,0.57%,0.18%,0.30%. Compared with the classic PID, DDPG has smaller errors and higher stability, which improves the grasping robustness. The results show that the proposed controlling method is effective and superior, which can ensure grasping robustness during high-speed pick-and-place manipulation.","","978-1-6654-3153-8","10.1109/M2VIP49856.2021.9665056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665056","","Three-dimensional displays;Force;Fingers;Reinforcement learning;Grasping;Object segmentation;Soft robotics","control engineering computing;deep learning (artificial intelligence);dexterous manipulators;force control;grippers;reinforcement learning;rigidity;robust control","stiffness control;soft robotic finger;soft robotic grippers;model-free reinforcement learning;grasping robustness;deep deterministic policy gradient;DDPG;force analysis","","2","","21","IEEE","7 Jan 2022","","","IEEE","IEEE Conferences"
"Safety and Comfort in Autonomous Braking System with Deep Reinforcement Learning","M. P. Fanti; A. M. Mangini; D. Martino; I. Olivieri; F. Parisi; F. Popolizio","Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Electrical and Information Engineering Politecnico di Bari, Bari, Italy; Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy","2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","18 Nov 2022","2022","","","1786","1791","Safety issues related to autonomous vehicles are of great concern both in the academy and industry, identifying the braking system performance as a crucial research field. In this work, an autonomous braking system based on deep reinforcement learning is proposed, employing an intelligent agent trained in city scenarios to manage both pedestrians’ safety and passengers’ comfort. The agent is modelled via the deep deterministic policy gradient algorithm in a software environment and its performance is tested showing good results in maximizing both pedestrians’ safety and passengers’ comfort.","2577-1655","978-1-6654-5258-8","10.1109/SMC53654.2022.9945383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9945383","Autonomous vehicles;autonomous braking system;Deep Reinforcement Learning;Safety;Comfort","Training;Deep learning;System performance;Software algorithms;Urban areas;Reinforcement learning;Software","deep learning (artificial intelligence);gradient methods;mobile robots;reinforcement learning;road safety;road vehicles","academy;autonomous braking system;autonomous vehicles;braking system performance;city scenarios;crucial research field;deep deterministic policy gradient algorithm;deep reinforcement learning;intelligent agent;passengers comfort;pedestrians;software environment","","1","","17","IEEE","18 Nov 2022","","","IEEE","IEEE Conferences"
"Joint Latency and Energy Consumption Optimization with Deep Reinforcement Learning for Proximity Detection in Road Networks","T. Zhao; Y. Liu; G. Shou; Y. Hu","Beijing Key Laboratory of Network System Architecture and Convergence School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Network System Architecture and Convergence School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Network System Architecture and Convergence School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Network System Architecture and Convergence School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China","2021 7th International Conference on Computer and Communications (ICCC)","17 Jan 2022","2021","","","1272","1277","Artificial intelligence and the automobile business have grown significantly in recent years, and autonomous driving has progressively been the industry's focus. The problem of proximity detection in road networks refers to determining if two moving objects are close to one other in real time. However, in the real world, mobile devices' battery life and computing capabilities are restricted, resulting in excessive latency and energy usage. As a result, determining the proximity relationship between mobile users with low latency and energy usage is a difficult task. We formalize the joint latency and energy consumption optimization problem for proximity detection in road networks into a constrained optimization problem (COP) and solve it using a deep Q network (DQN). The simulation results show that DQN can ensure the desired sum cost under a variety of important factors, effectively reducing latency and energy usage.","","978-1-6654-0950-6","10.1109/ICCC54389.2021.9674311","National Natural Science Foundation of China; Beijing Key Laboratory of Network System Architecture and Convergence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9674311","proximity detection;mobile edge computing;road networks;deep reinforcement learning;DQN","Energy consumption;Roads;Computational modeling;Simulation;Reinforcement learning;Real-time systems;Mobile handsets","computer centres;cost reduction;deep learning (artificial intelligence);energy consumption;mobile computing;optimisation;reinforcement learning","excessive latency;energy usage;mobile users;joint latency;energy consumption optimization problem;proximity detection;road networks;constrained optimization problem;deep Q network;deep reinforcement learning;artificial intelligence;automobile business;autonomous driving;mobile devices","","1","","8","IEEE","17 Jan 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-Based Ground-Via Placement Optimization for EMI Mitigation","Z. Gu; L. Zhang; H. Jin; T. Tao; D. Li; E. -P. Li","Key Laboratory of Advanced Micro/Nano Electronic Devices and Smart Systems and Applications, College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; Key Laboratory of Advanced Micro/Nano Electronic Devices and Smart Systems and Applications, College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; Key Laboratory of Advanced Micro/Nano Electronic Devices and Smart Systems and Applications, College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; Key Laboratory of Advanced Micro/Nano Electronic Devices and Smart Systems and Applications, College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; Key Laboratory of Advanced Micro/Nano Electronic Devices and Smart Systems and Applications, College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; Key Laboratory of Advanced Micro/Nano Electronic Devices and Smart Systems and Applications, College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Electromagnetic Compatibility","19 Apr 2023","2023","65","2","564","573","Placing ground vias plays a crucial role in mitigating electromagnetic radiation from printed circuit board (PCB) edges. Stitching dense ground vias along PCB edges is not always feasible due to limited layout areas. Finding a ground-via placement strategy to achieve the best electromagnetic interference (EMI) mitigation using a specified number of ground vias is desired in the industry. However, this process is usually tedious and labor-intensive because of the enormous search space. This article proposes an optimization algorithm based on deep reinforcement learning to adaptively seek the optimal ground-via placement strategy in complex packages. First, an evaluation module based on convolutional neural networks (CNNs) is trained to predict the EMI mitigation for any possible ground-via placement. Then, relying on the well-trained CNN, an optimization module based on dueling double deep Q network is developed to find the best ground-via placement strategy through exploration and training without prior electromagnetic knowledge. The final optimization strategy obtained by our proposed algorithm has more effective EMI mitigation than the commonly used “edge-wrapping” solution in industrial products. Our proposed algorithm also provides guidelines and insights about the design rule and the behind physics.","1558-187X","","10.1109/TEMC.2022.3222034","Zhejiang Provincial Natural Science Foundation of China(grant numbers:LD21F010002); National Natural Science Foundation of China(grant numbers:62071424); National Natural Science Foundation of China(grant numbers:62027805); Zhejiang Lab Project Fund(grant numbers:2020KCDAB01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9959881","Deep reinforcement learning (DRL);dueling double deep Q network (D3QN);electromagnetic radiation suppression;shield structure design","Optimization;Predictive models;Training;Electromagnetic interference;Markov processes;Transmission line matrix methods;Prediction algorithms","convolutional neural nets;deep learning (artificial intelligence);electromagnetic interference;learning (artificial intelligence);optimisation;printed circuits;reinforcement learning","deep reinforcement learning;dense ground;dueling double deep Q network;effective EMI mitigation;electromagnetic interference mitigation;electromagnetic radiation;final optimization strategy;ground-via placement optimization;optimal ground-via placement strategy;optimization algorithm;optimization module;PCB edges;possible ground-via placement;printed circuit board edges","","1","","33","IEEE","23 Nov 2022","","","IEEE","IEEE Journals"
"Local Control is All You Need: Decentralizing and Coordinating Reinforcement Learning for Large-Scale Process Control","N. Bougie; T. Onishi; Y. Tsuruoka","NEC-AIST AI Cooperative Research Laboratory, National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; NEC-AIST AI Cooperative Research Laboratory, National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; NEC-AIST AI Cooperative Research Laboratory, National Institute of Advanced Industrial Science and Technology, Tokyo, Japan","2022 61st Annual Conference of the Society of Instrument and Control Engineers (SICE)","6 Oct 2022","2022","","","468","474","Deep reinforcement learning (RL) approaches are an appealing alternative to conventional controllers in process industries as such methods are inherently flexible and have generalization abilities to unseen situations. Namely, they alleviate the need for constant parameter tuning, tedious design of control laws, and re-identification procedures in the event of performance degradation. However, it remains challenging to apply RL to real-world process tasks, which commonly feature large state-action spaces and complex dynamics. Such tasks may be difficult to solve due to computational complexity and sample insufficiency. To tackle these limitations, we present a sample-efficient RL approach for large-scale control that expresses the global policy as a collection of local policies. Every local policy receives local observations and is responsible for controlling a different region of the environment. In order to enable coordination among local policies, we present a mechanism based on action sharing and message passing. The model is evaluated on a set of robotic tasks and a large-scale vinyl acetate monomer (VAM) plant. The experiments demonstrate that the proposed model exhibits significant improvements over baselines in terms of mean scores and sample efficiency.","","978-4-9077-6478-4","10.23919/SICE56594.2022.9905798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9905798","deep reinforcement learning;large-scale reinforcement learning;chemical process control","Training;Message passing;Robot kinematics;Process control;Reinforcement learning;Communication channels;Aerospace electronics","computational complexity;control engineering computing;deep learning (artificial intelligence);message passing;process control;reinforcement learning","appealing alternative;complex dynamics;computational complexity;constant parameter tuning;control laws;coordinating reinforcement learning;deep reinforcement learning approaches;generalization abilities;global policy;large-scale control;large-scale process control;large-scale vinyl acetate monomer plant;local control;local observations;local policy;message passing;performance degradation;re-identification procedures;real-world process tasks;robotic tasks;sample insufficiency;sample-efficient RL approach;state-action spaces;tedious design;unseen situations;VAM plant","","1","","18","","6 Oct 2022","","","IEEE","IEEE Conferences"
"Applied Reinforcement Learning for Decision Making in Industrial Simulation Environments","A. Devanga; E. D. Badilla; M. Dehghanimohammadabadi","Department of Mechanical and Industrial Engineering, Northeastern University, Boston, MA, USA; Department of Mechanical and Industrial Engineering, Northeastern University, Boston, MA, USA; Department of Mechanical and Industrial Engineering, Northeastern University, Boston, MA, USA","2022 Winter Simulation Conference (WSC)","23 Jan 2023","2022","","","2819","2829","The industrial sector is going through a digital transformation with a high emphasis on Artificial Intelligence-driven operations. In this transformation, the combination between simulation and machine learning has been a key enhancer to provide state-of-the-art solutions to complex systems by training algorithms over virtual representations of industry. In recent years, Reinforcement Learning (RL) has gained traction in this field and shown success in solving sequential decision-making problems. The purpose of this paper is to show how to implement simulation-based Reinforcement Learning. The proposed framework integrates Simio, as a discrete-event simulation environment, and Python, to include the RL algorithm. To demonstrate the applicability of this framework, a job-shop scheduling problem under different scenarios is tested and its results are compared with benchmark heuristic dispatching rules.","1558-4305","978-1-6654-7661-4","10.1109/WSC57314.2022.10015282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015282","","Machine learning algorithms;Computational modeling;Decision making;Reinforcement learning;Tutorials;Data science;Data models","decision making;discrete event simulation;job shop scheduling;reinforcement learning","artificial intelligence-driven operations;complex systems;decision making;digital transformation;discrete-event simulation environment;industrial sector;industrial simulation environments;job-shop scheduling problem;machine learning;Python;RL algorithm;sequential decision-making problems;simulation-based reinforcement learning;training algorithms","","1","","21","IEEE","23 Jan 2023","","","IEEE","IEEE Conferences"
"Al-based Peak Load Reduction Approach for Residential Buildings using Reinforcement Learning","A. Kumari; S. Tanwar","Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, Gujarat, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, Gujarat, India","2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)","12 Apr 2021","2021","","","972","977","In recent years, energy management has gained significant interest from industries and the research community. The goal of this paper is to explore the Artificial Intelligence (AI) for a residential building to reduce peak load using Reinforcement Learning (RL). Denning an Al-based system improves the likelihood to save energy and reduce burden on Smart Grid (SG) during peak hour along with computational aspects of RL. Here, we proposed a Q-learning-based Peak Load Reduction (QL-PLR) approach to provide an optimum solution for Residential Energy Management (REM). Further, to handle uncertainties, a scenario-based mechanism is used with the real data along with probability density functions. Deterministic numerical calculations are done to justify the efficacy of the proposed QL-PLR approach. The experimental results indicate that the application of the proposed QL-PLR approach leads to lower energy consumption associated with consumer dissatisfaction compare to existing approaches with respect to different evaluation metrics.","","978-1-7281-8529-3","10.1109/ICCCIS51004.2021.9397241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9397241","Artificial Intelligence;Demand Response Management;Reinforcement Learning;Residential Energy Management;Q-Learning","Uncertainty;Buildings;Reinforcement learning;Pricing;Smart grids;Reliability;Energy management","building management systems;demand side management;energy conservation;energy consumption;energy management systems;learning (artificial intelligence);power consumption;power engineering computing;probability;smart power grids","residential energy management;scenario-based mechanism;probability density functions;deterministic numerical calculations;QL-PLR approach;energy consumption;residential buildings;reinforcement learning;research community;artificial intelligence;residential building;RL;smart grid;peak hour;computational aspects;peak load reduction approach","","1","","21","IEEE","12 Apr 2021","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Based Approach for Controlling Autonomous Vehicles in Complex Scenarios","B. B. Elallid; M. Bagaa; N. Benamar; N. Mrani","School of Technology, Moulay Ismail University of Meknes, Meknes, Morocco; Department of Electrical and Computer Engineering, Université du Québec à Trois-Rivières, Trois-Rivières, QC, Canada; Moulay Ismail University of Meknes School of Science and Engineering Al Akhawayn University in Ifrane, Ifrane, Morocco; School of Technology, Moulay Ismail University of Meknes, Meknes, Morocco","2023 International Wireless Communications and Mobile Computing (IWCMC)","21 Jul 2023","2023","","","1358","1364","Autonomous driving has gained an increased interest in both academia and industry, as autonomous vehicles (AVs) significantly improve road safety by reducing traffic accidents and human injuries. Motion control remains one of the main functions of Autonomous Vehicles, which generates the steering angle and velocity of the vehicle. While traditional Machine Learning techniques have been extensively used in the past to improve motion control in AVs, the attention has been recently drawn to the use of Deep Learning (DL) and Deep Reinforcement Learning (DRL) techniques. These techniques have been applied to improve motion control of AVs and to help them learn from their environment. However, existing works are limited to dealing with simple scenarios without taking into consideration other road participants (e.g., other vehicles, pedestrians, cyclists, and motorcycles). In this paper, we propose a DRL-based model using Deep-Q Networks to control the AV in a complex scenario with dense traffic involving road participants. The AV learns the policy of different actions to reach its destination in an intersection without accidents. We tested and validated our proposed approach using the CARLA simulator. The obtained results demonstrated the efficiency of our solution by achieving better learning in terms of travel delay and avoiding collisions.","2376-6506","979-8-3503-3339-8","10.1109/IWCMC58020.2023.10182377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10182377","Autonomous Vehicles;Reinforcement Learning;Vehicle Control","Deep learning;Wireless communication;Pedestrians;Motorcycles;Reinforcement learning;Stability analysis;Delays","deep learning (artificial intelligence);injuries;intelligent transportation systems;learning (artificial intelligence);mobile robots;motion control;motorcycles;pedestrians;reinforcement learning;road accidents;road safety;road traffic;road vehicles;telecommunication computing;traffic engineering computing","Autonomous driving;autonomous vehicles;AV;complex scenario;Deep Learning;DRL-based model;motion control;Reinforcement Learning;road participants;road safety;steering angle;traditional Machine Learning techniques","","","","21","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Mobile Energy Transmitter Scheduling in Energy Harvesting IoT Networks using Deep Reinforcement Learning","A. Singh; R. Rustagi; S. Redhu; R. M. Hegde","Department of Electrical Engineering, Indian Institute of Technology, Kanpur, India; Department of Electrical Engineering, Indian Institute of Technology, Kanpur, India; Department of ICT, University of Agder, Norway; Department of Electrical Engineering, Indian Institute of Technology, Kanpur, India","2022 IEEE 8th World Forum on Internet of Things (WF-IoT)","22 Jun 2023","2022","","","1","6","Maintaining adequate energy in low-powered Internet of Things (IoT) nodes is crucial for the development of several applications like smart homes, autonomous industries, etc. These IoT nodes exploit adaptive duty cycling techniques for the efficient utilization of energy resources. However, such adaptive duty cycling of IoT nodes results in their asynchronous operations thereby inducing energy holes in the network. These energy holes lead to information loss and poor quality of services of IoT networks. In this regard, energy harvesting using Mobile Energy Transmitters (MET) can improve the lifetime of an IoT network. In this work, we are introducing a metric named Age of Charging (AoC) metric to quantify the repetitive charging of power deficit IoT nodes. Energy-efficient scheduling of MET is proposed to minimize the expected average AoC such that the energy harvested by IoT nodes is maximized. In this regard, the optimization problem is first remodeled into a Markov decision process. Subsequently, a deep reinforcement learning algorithm is developed based upon the twin delayed deep deterministic policy gradient scheme for energy-efficient scheduling of MET in asynchronous IoT networks. The simulation results indicate that the proposed algorithm outperforms the conventional Deep Q-networks and soft-actor-critic algorithms. These results motivate the usage of MET-aided energy harvesting in self-sustaining IoT networks.","","978-1-6654-9153-2","10.1109/WF-IoT54382.2022.10152078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10152078","Age of Charging (AoC);Deep Deterministic Policy Gradient;Energy Harvesting;IoT Network;Mobile Energy Transmitter;Wireless Power Transfer","Measurement;Deep learning;Job shop scheduling;Transmitters;Reinforcement learning;Smart homes;Energy efficiency","computer network security;deep learning (artificial intelligence);energy harvesting;gradient methods;Internet of Things;Markov processes;optimisation;reinforcement learning;telecommunication computing;telecommunication power management","adaptive duty cycling techniques;adequate energy;asynchronous IoT networks;conventional Deep Q-networks;deep reinforcement learning algorithm;Energy harvesting IoT networks;energy holes lead;energy resources;energy-efficient scheduling;IoT network;low-powered Internet;MET-aided energy harvesting;Mobile Energy transmitter scheduling;Mobile Energy Transmitters;power deficit IoT nodes","","","","17","IEEE","22 Jun 2023","","","IEEE","IEEE Conferences"
"Fruit Picking Robot Arm Training Solution Based on Reinforcement Learning in Digital Twin","X. Tian; B. Pan; L. Bai; G. Wang; D. Mo","Macau Institute of Systems Engineering, Macau University of Science and Technology, Taipa, Macau, China; Sichuan Digital Transportation Tech Co. Ltd., Chengdu, Sichuan, China; Macau Institute of Systems Engineering, Macau University of Science and Technology, Taipa, Macau, China; School of Mechanical and Electrical Engineering, Lingnan Normal University, Zhanjiang, Guangdong, China; Macau Institute of Systems Engineering, Macau University of Science and Technology, Taipa, Macau, China","Journal of ICT Standardization","22 Sep 2023","2023","11","3","261","282","In the era of Industry 4.0, digital agriculture is developing very rapidly and has achieved considerable results. Nowadays, digital agriculture-based research is more focused on the use of robotic fruit picking technology, and the main research direction of such topics is algorithms for computer vision. However, when computer vision algorithms successfully locate the target object, it is still necessary to use robotic arm movement to reach the object at the physical level, but such path planning has received minimal attention. Based on this research deficiency, we propose to use Unity software as a digital twin platform to plan the robotic arm path and use ML-Agent plug-in as a reinforcement learning means to train the robotic arm path, to improve the accuracy of the robotic arm to reach the fruit, and happily the effect of this method is much improved than the traditional method.","2246-0853","","10.13052/jicts2245-800X.1133","FDCT(grant numbers:0003/2021/ITP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255409","Robot arm;digital twin;reinforcement learning;unity;ML-agent","Training;Smart agriculture;Computer vision;Three-dimensional displays;Service robots;Software algorithms;Reinforcement learning","","","","","","26","","22 Sep 2023","","","River Publishers","River Publishers Journals"
"Cooperative Variable Speed Limit Control using Multi-agent Reinforcement Learning and Evolution Strategy for Improved Throughput in Mixed Traffic","K. Lin; Z. Jia; P. Li; T. Shi; A. Khamis","Electrical and Computer Engineering, University of Toronto, Canada; Electrical and Computer Engineering, University of Toronto, Canada; Electrical and Computer Engineering, University of Toronto, Canada; Toronto Intelligent Transportation Systems Centre, University of Toronto, Canada; Canadian Technical Centre, General Motors Canada","2023 IEEE International Conference on Smart Mobility (SM)","3 May 2023","2023","","","27","32","Improving the traffic throughput in mixed traffic scenarios including both human-driving vehicles and Connected and Automated Vehicles (CAVs) has long been a hot spot in automated driving. In recent years, variable speed limit (VSL) has been a promising solution and attracts considerable attention from both industry and academy. In this paper, a multi-agent reinforcement learning model and evolution strategy-based approach is proposed to provide both macroscopic and microscopic control in mixed traffic scenarios. In this approach, Graph Attention Networks (GATs) are introduced into Deep Q-Networks for vehicles' decision making. The architecture of the VSL network is designed using an evolution strategy to provide real-time speed limit. A dedicated reward function has been implemented to consider both the actions and speed limit. Extensive experiments are conducted focusing on Bottleneck networks. The experimental results show that the proposed approach has demonstrated superior performance compared with other baselines in terms of several metrics such as throughput, average speed, and safety.","","979-8-3503-1275-1","10.1109/SM57895.2023.10112494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10112494","Variable Speed Limit;Connected and Automated Vehicles;Multi-agent Reinforcement Learning;Evolution Strategy;Graph Attention Networks","Microscopy;Scalability;Roads;Reinforcement learning;Throughput;Real-time systems;Safety","decision making;graph theory;intelligent transportation systems;learning (artificial intelligence);multi-agent systems;reinforcement learning;road traffic;road traffic control;road vehicles;telecommunication traffic;traffic engineering computing","automated driving;Bottleneck networks;Connected Automated Vehicles;Deep Q-Networks;evolution strategy;Graph Attention Networks;human-driving vehicles;improved throughput;macroscopic control;microscopic control;mixed traffic scenarios;multiagent reinforcement learning model;real-time speed limit;traffic throughput;variable speed limit control;VSL network","","","","16","IEEE","3 May 2023","","","IEEE","IEEE Conferences"
"Reinforcement learning for antennas’ electric tilts optimization in self organizing networks","A. Massaro; D. Wellington; A. Aghasaryan; R. Seidl","Nokia Bell Labs, Paris, France; Nokia, USA; Nokia Bell Labs, Paris, France; Nokia Bell Labs, Munich, Germany","2021 IEEE 32nd Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)","21 Oct 2021","2021","","","1122","1127","Wireless networks are becoming more and more complex. New radio access technologies and cell densification have provided increased efficiency and met the needs of an increasing traffic demand. On the other hand, such increment in complexity poses new challenges to legacy network management and optimization solutions. Traditionally, network optimization has been performed by applying static, rule-based schemes. Both in academia and industry, there is a consensus about the need to transit from static optimization approaches to more automated, dynamic, cognitive techniques. In this paper we embrace this perspective and we propose a set of cognitive algorithms for network performance optimization. We leverage the electric antenna tilt to achieve improved network performance. The proposed algorithms rely on reinforcement learning principles and are validated in a simulated environment.","2166-9589","978-1-7281-7586-7","10.1109/PIMRC50174.2021.9569403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9569403","reinforcement learning;self organizing networks","Heuristic algorithms;Software algorithms;Reinforcement learning;Self-organizing networks;Performance gain;Software;Task analysis","antennas;optimisation;radio access networks;reinforcement learning;telecommunication computing;telecommunication network management;telecommunication network performance;telecommunication traffic","improved network performance;reinforcement learning principles;self organizing networks;wireless networks;new radio access technologies;cell densification;traffic demand;legacy network management;optimization solutions;network optimization;rule-based schemes;static optimization;automated techniques;dynamic techniques;cognitive techniques;cognitive algorithms;network performance optimization;electric antenna tilt;antennas electric tilts optimization","","","","11","IEEE","21 Oct 2021","","","IEEE","IEEE Conferences"
"Video Game Recommender System Using Deep Reinforcement Learning","M. A. F. Ali; Z. K. A. Baizal","School of Computing, Telkom University, Bandung, Indonesia; School of Computing, Telkom University, Bandung, Indonesia","2023 International Conference on Advancement in Data Science, E-learning and Information System (ICADEIS)","5 Oct 2023","2023","","","1","6","The gaming industry has seen ramatic shifts in recent decades. The variety of games available and the number of people who play them have exploded due to the growth of online communities dedicated to this kind of entertainment. The difficulty of keeping up with the always-changing preferences of players and the constant stream of new video games is a challenging problem. Although several video game recommender systems have been presented, the algorithm used has difficulties recording changes in user's preferences, leading to a repetition of recommended games. Therefore, a recommender system that actively learns from user behavior and personalizes its recommendations accordingly. We propose developing video game recommendations based on Deep Reinforcement Learning (DRL). The deep reinforcement learning algorithm uses data from Steam platform and user past interactions, such as playtime forever and playtime in the past 2 weeks, to provide future rewards for video game recommendations. We compared the experiment on DRL variance with Singular Value Decomposition (SVD) and Logistic Regression (LR). The results indicate that the proposed method consistently assesses the item’s importance and relative ranks based on user’s past interactions.","","979-8-3503-0341-4","10.1109/ICADEIS58666.2023.10270905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10270905","Recommender System;Deep Reinforcement Learning;Double Q Learning;Video Game;Steam Platform","Deep learning;Measurement;Video games;Logistic regression;Reinforcement learning;Games;Recording","","","","","","20","IEEE","5 Oct 2023","","","IEEE","IEEE Conferences"
"Plume: Lightweight and generalized congestion control with deep reinforcement learning","D. Wei; J. Zhang; X. Zhang; C. Huang","State Key Laboratory of Networking and Switching Technology, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing, China","China Communications","12 Dec 2022","2022","19","12","101","117","Congestion control (CC) is always an important issue in the field of networking, and the enthusiasm for its research has never diminished in both academia and industry. In current years, due to the rapid development of machine learning (ML), the combination of reinforcement learning (RL) and CC has a striking effect. However, These complicated schemes lack generalization and are too heavyweight in storage and computing to be directly implemented in mobile devices. In order to address these problems, we propose Plume, a high-performance, lightweight and generalized RL-CC scheme. Plume proposes a lightweight framework to reduce the overheads while preserving the original performance. Besides, Plume innovatively modifies the framework parameters of the reward function during the retraining process, so that the algorithm can be applied to a variety of scenarios. Evaluation results show that Plume can retain almost all the performance of the original model but the size and decision latency can be reduced by more than 50% and 20%, respectively. Moreover, Plume has better performances in some special scenes.","1673-5447","","10.23919/JCC.2022.00.019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9772474","congestion control;deep reinforcement learning;lightweight;generalization","Packet loss;Training;Convergence;Bandwidth;Throughput;Low latency communication;Noise measurement","multimedia communication;reinforcement learning;telecommunication computing;telecommunication congestion control","complicated schemes;current years;deep reinforcement learning;generalized congestion control;generalized RL-CC scheme;lightweight framework;machine learning;original performance","","","","","","10 May 2022","","","IEEE","IEEE Magazines"
"Reinforcement-Learning-based Mixed-Signal IC Placement for Fogging Effect Control","M. Hajijafari; M. Ahmadi; Z. Zhao; L. Zhang","Department of Electrical and Computer Engineering, Memorial University of Newfoundland, St. John’s, Canada; Department of Electrical and Computer Engineering, Memorial University of Newfoundland, St. John’s, Canada; Department of Electrical and Computer Engineering, Memorial University of Newfoundland, St. John’s, Canada; Department of Electrical and Computer Engineering, Memorial University of Newfoundland, St. John’s, Canada","2022 23rd International Symposium on Quality Electronic Design (ISQED)","29 Jun 2022","2022","","","127","132","As the technology node shrinks to the nanometer regime, the demand for new lithography methods with high resolution and low cost is increasing. Electron beam lithography (EBL) is one of the promising next-generation lithography (NGL) technologies that can tackle both challenges compared to the traditional lithography methods. Fogging effect, a culprit of pattern distortion in layout, is one of the major challenges that prevent industry from adopting EBL in technologies below 22nm. This paper proposes a reinforcement-learning (RL) placement method that trains a neural network as an agent to effectively control fogging effect. To speed up our method, we benefit from the following innovations: using topological floorplan representation for the layouts during placement, and deploying a RL trained agent that can intelligently take actions. The experimental results show that our proposed placer is able to more effectively and efficiently reduce the fogging effect variation in the analog circuits in comparison with the conventional simulated-annealing-based placement method.","1948-3295","978-1-6654-9466-3","10.1109/ISQED54688.2022.9806259","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9806259","Electron Beam Lithography;Fogging Effect;Placement;Reinforcement Learning","Integrated circuits;Technological innovation;Runtime;Q-learning;Electron beams;Lithography;Layout","circuit layout;electron beam lithography;electronic engineering computing;integrated circuit layout;mixed analogue-digital integrated circuits;reinforcement learning;simulated annealing","reinforcement-learning-based mixed-signal IC placement;fogging effect control;technology node;nanometer regime;electron beam lithography;EBL;next-generation lithography technologies;traditional lithography methods;pattern distortion;reinforcement-learning placement method;RL trained agent;fogging effect variation;size 22.0 nm","","","","25","IEEE","29 Jun 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning","A. Santra; S. Hazra; L. Servadei; T. Stadelmayer; M. Stephan; A. Dubey",NA; NA; NA; NA; NA; NA,"Methods and Techniques in Deep Learning: Advancements in mmWave Radar Solutions","","2023","","","115","150","Deep reinforcement learning (DRL) utilizes deep learning techniques to model reinforcement learning (RL) tasks. Recently, DRL has gained significant attention due to its remarkable success. Thanks to brilliant solutions in gaming, supply chain, flight schedule, and many other areas, DRL made a clear movement from academia to industry. In order to show how DRL can support real‐life problems, in this chapter, we review the entire RL framework. Here, we present the conditions and settings for applying RL to different tasks. Afterward, we describe different RL algorithms and their use in practice. By doing that, we analyze in detail the most popular algorithms and explain their characteristics and functioning. At the end, a continuous RL‐based optimization problem is presented for the optimization of tracking parameters in radars. Here, it is shown how RL approaches can be used in the complex problem of choosing tracking‐parameters for radar in indoor scenarios. We show that the proposed approach overcomes baseline methods which generally rely on manually selected parameters from the engineers. This points out how automated methods of parameter selection reach outstanding performance and help in real‐life tasks, thus, making a step toward industrial cases. The big gap in terms of tracking performance shows the clear advantage of DRL toward state‐of‐the‐art methods.","","9781119910664","10.1002/9781119910695.ch4","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9963487.pdf&bkn=9962819&pdfType=chapter","","Deep learning;Q-learning;Task analysis;Robots;Oscillators;Object detection;Neural networks","","","","","","","","24 Nov 2022","","","IEEE","Wiley-IEEE Press eBook Chapters"
"Reinforcement Learning-Based Genetic Algorithm for Aging State Analysis of Insulating Paper at Transformer Hotspot","Z. Jiang; J. Liu; X. Fan; Q. Wang; Y. Zhang; T. Wu","School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China","IEEE Transactions on Instrumentation and Measurement","29 Sep 2023","2023","72","","1","10","The aging state evaluation of insulating paper at the transformer hotspot is a pain point in the industry. To address this issue, the modified dielectric response (MDR) model and the reinforcement learning-based genetic algorithm (RLGA) are proposed to analyze the aging state of the insulating paper at the hotspot. First, the aging state-related polarization and depolarization currents (PDC) of transformer insulation are collected. Then, the MDR model is reported to describe the PDC property of the insulating paper. The RLGA is later proposed to search the optimal model parameters defined in MDR to characterize the aging state of the insulating paper at the hotspot. Verification results present the feasibility and validity of the extracted optimal model parameters for aging analysis. Furthermore, the quantitative correlation between these parameters and the aging state of the hotspot is analyzed. Regarding this, the present work provides a potential method for obtaining aging information on the insulating paper at the hotspot.","1557-9662","","10.1109/TIM.2023.3272043","Guangxi Natural Science Foundation Project(grant numbers:2023GXNSFFA026012); Local Funds for Science and Technology Development Guided Central Government(grant numbers:2023ZYZX1251); Innovation Project of Guangxi Graduate Education(grant numbers:YCSW2023087); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10113815","Aging state;dielectric response technique;hotspot;reinforcement learning (RL);transformer insulation","Aging;Power transformer insulation;Dielectrics;Temperature measurement;Oil insulation;Oils;Feature extraction","ageing;genetic algorithms;power transformer insulation;reinforcement learning","aging analysis;aging state evaluation;aging state-related polarization;depolarization currents;insulating paper;modified dielectric response model;reinforcement learning-based genetic algorithm;state analysis;transformer hotspot;transformer insulation","","","","31","IEEE","1 May 2023","","","IEEE","IEEE Journals"
