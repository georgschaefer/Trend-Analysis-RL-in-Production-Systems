"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Uncertainty-Aware Model-Based Reinforcement Learning: Methodology and Application in Autonomous Driving","J. Wu; Z. Huang; C. Lv","School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Intelligent Vehicles","23 Jan 2023","2023","8","1","194","203","To further improve learning efficiency and performance of reinforcement learning (RL), a novel uncertainty-aware model-based RL method is proposed and validated in autonomous driving scenarios in this paper. First, an action-conditioned ensemble model with the capability of uncertainty assessment is established as the environment model. Then, a novel uncertainty-aware model-based RL method is developed based on the adaptive truncation approach, providing virtual interactions between the agent and environment model, and improving RL’s learning efficiency and performance. The proposed method is then implemented in end-to-end autonomous vehicle control tasks, validated and compared with state-of-the-art methods under various driving scenarios. Validation results suggest that the proposed method outperforms the model-free RL approach with respect to learning efficiency, and model-based approach with respect to both efficiency and performance, demonstrating its feasibility and effectiveness.","2379-8904","","10.1109/TIV.2022.3185159","Agency for Science, Technology and Research (A*STAR); Advanced Manufacturing and Engineering; Young Individual Research(grant numbers:A2084c0156); Start-Up Grant; Nanyang Technological University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9802913","Model-based reinforcement learning;uncertainty awareness;adaptive truncation;autonomous driving","Autonomous vehicles;Adaptation models;Predictive models;Uncertainty;Virtual environments;Computational modeling;Task analysis","control engineering computing;learning (artificial intelligence);mobile robots;reinforcement learning","action-conditioned ensemble model;autonomous driving scenarios;end-to-end autonomous vehicle control tasks;environment model;learning efficiency;model-based approach;model-free RL approach;novel uncertainty-aware model-based RL method;uncertainty assessment;uncertainty-aware model-based reinforcement learning","","15","","40","IEEE","21 Jun 2022","","","IEEE","IEEE Journals"
"Fuzzy H∞ Control of Discrete-Time Nonlinear Markov Jump Systems via a Novel Hybrid Reinforcement Q-Learning Method","J. Wang; J. Wu; H. Shen; J. Cao; L. Rutkowski","Anhui Province Key Laboratory of Special Heavy Load Robot and the School of Electrical and Information Engineering, Anhui University of Technology, Ma’anshan, China; Anhui Province Key Laboratory of Special Heavy Load Robot and the School of Electrical and Information Engineering, Anhui University of Technology, Ma’anshan, China; School of Electrical and Information Engineering, Anhui University of Technology, Ma’anshan, China; School of Mathematics, Southeast University, Nanjing, China; AGH University of Science and Technology, Krakòw, Poland","IEEE Transactions on Cybernetics","17 Oct 2023","2023","53","11","7380","7391","In this article, a novel hybrid reinforcement  $Q$ -learning control method is proposed to solve the adaptive fuzzy  $H_{\infty }$  control problem of discrete-time nonlinear Markov jump systems based on the Takagi–Sugeno fuzzy model. First, the core problem of adaptive fuzzy  $H_{\infty }$  control is converted to solving fuzzy game coupled algebraic Riccati equation, which can hardly be solved by mathematical methods directly. To solve this problem, an offline parallel hybrid learning algorithm is first designed, where system dynamics should be known as a prior. Furthermore, an online parallel  $Q$ -learning hybrid learning algorithm is developed. The main characteristics of the proposed online hybrid learning algorithms are threefold: 1) system dynamics are avoided during the learning process; 2) compared with the policy iteration method, the restriction of the initial stable control policy is removed; and 3) compared with the value iteration method, a faster convergence rate can be obtained. Finally, we provide a tunnel diode circuit system model to validate the effectiveness of the present learning algorithm.","2168-2275","","10.1109/TCYB.2022.3220537","National Natural Science Foundation of China(grant numbers:62273006,62173001,61873002); Major Natural Science Foundation of Higher Education Institutions of Anhui Province(grant numbers:KJ2020ZD28); Natural Science Foundation for Excellent Young Scholars of Anhui Province(grant numbers:2108085Y21); Major Technologies Research and Development Special Program of Anhui Province(grant numbers:202003a05020001); Key Research and Development Projects of Anhui Province(grant numbers:202104a05020015); Open Project of China International Science and Technology Cooperation Base on Intelligent Equipment Manufacturing in Special Service Environment(grant numbers:ISTC2021KF04); Fundamental Research Funds for the Central Universities(grant numbers:2021ACOCP05); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9961931","Coupled algebraic Riccati equation (CARE);fuzzy H∞ control;hybrid reinforcement Q-learning;Markov jump nonlinear systems","Optimal control;Q-learning;System dynamics;Markov processes;Heuristic algorithms;Games;Convergence","","","","5","","46","IEEE","23 Nov 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Acceleration for Real-Time Edge Computing Mixed Integer Programming Problems","G. Gerogiannis; M. Birbas; A. Leftheriotis; E. Mylonas; N. Tzanis; A. Birbas","Department of Electrical and Computer Engineering, University of Patras, Patras, Greece; Department of Electrical and Computer Engineering, University of Patras, Patras, Greece; Department of Electrical and Computer Engineering, University of Patras, Patras, Greece; Department of Electrical and Computer Engineering, University of Patras, Patras, Greece; Department of Electrical and Computer Engineering, University of Patras, Patras, Greece; Department of Electrical and Computer Engineering, University of Patras, Patras, Greece","IEEE Access","21 Feb 2022","2022","10","","18526","18543","In this work, we present the design and implementation of an ultra-low latency Deep Reinforcement Learning (DRL) FPGA based accelerator for addressing hard real-time Mixed Integer Programming problems. The accelerator exhibits ultra-low latency performance for both training and inference operations, enabled by training-inference parallelism, pipelined training, on-chip weights and replay memory, multi-level replication-based parallelism and DRL algorithmic modifications such as distribution of training over time. The design principles can be extended to support hardware acceleration for other relevant DRL algorithms (embedding the experience replay technique) with hard real time constraints. We evaluate the accuracy of the accelerator in a task offloading and resource allocation problem stemming from a Mobile Edge Computing (MEC/5G) scenario. The design has been implemented on a Xilinx Zynq Ultrascale+ MPSoC ZCU104 evaluation kit using High Level Synthesis. The accelerator achieves near optimal performance and exhibits a 10-fold decrease in training-inference execution latency when compared to a high-end CPU-based implementation.","2169-3536","","10.1109/ACCESS.2022.3147674","H2020 Project “ENERgy-efficient manufacturing system MANagement (ENERMAN)”(grant numbers:958478); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9696340","Accelerator;deep reinforcement learning;edge computing;FPGA;high level synthesis;mixed integer programming;5G","Training;Inference algorithms;Real-time systems;Artificial neural networks;Task analysis;Resource management;Field programmable gate arrays","distributed processing;field programmable gate arrays;high level synthesis;integer programming;learning (artificial intelligence);multiprocessing systems;resource allocation;system-on-chip","relevant DRL algorithms;experience replay technique;task offloading;resource allocation problem;mobile edge computing scenario;Xilinx Zynq Ultrascale+ MPSoC ZCU104 evaluation kit;optimal performance;training-inference execution;high-end CPU-based implementation;ultra-low latency performance;inference operations;training-inference parallelism;pipelined training;multilevel replication-based parallelism;DRL algorithmic modifications;design principles;hardware acceleration;real-time edge computing mixed integer programming problems;ultra-low latency deep reinforcement learning;real-time mixed integer programming problems","","2","","25","CCBY","28 Jan 2022","","","IEEE","IEEE Journals"
"A Fuzzy-Model-Based Approach to Optimal Control for Nonlinear Markov Jump Singularly Perturbed Systems: A Novel Integral Reinforcement Learning Scheme","H. Shen; Y. Wang; J. Wang; J. H. Park","AnHui Province Key Laboratory of Special Heavy Load Robot and the School of Electrical and Information Engineering, Anhui University of Technology, Ma'anshan, China; AnHui Province Key Laboratory of Special Heavy Load Robot and the School of Electrical and Information Engineering, Anhui University of Technology, Ma'anshan, China; AnHui Province Key Laboratory of Special Heavy Load Robot and the School of Electrical and Information Engineering, Anhui University of Technology, Ma'anshan, China; Department of Electrical Engineering, Yeungnam University, Kyongsan, South Korea","IEEE Transactions on Fuzzy Systems","5 Oct 2023","2023","31","10","3734","3740","A fuzzy-model-based approach is developed to investigate the reinforcement learning-based optimization for nonlinear Markov jump singularly perturbed systems. As the first attempt, an offline parallel iteration learning algorithm is presented to solve the coupled algebraic Riccati equations with singular perturbation and jumping parameters. Furthermore, based on the integral reinforcement learning approach, a novel online parallel learning algorithm is proposed by employing the slow and fast sampled data simultaneously, where the impacts of stochastic jumping and ill-conditioned numerical problems are avoided. Meanwhile, the convergence of the proposed learning algorithms is proved. Finally, we present a tunnel diode circuit model to demonstrate the efficacy of the proposed methods.","1941-0034","","10.1109/TFUZZ.2023.3265666","National Research Foundation of Korea; Ministry of Science and Information and Communications Technology(grant numbers:2019R1A5A8080290); National Natural Science Foundation of China(grant numbers:62273006,62173001,61873002); Natural Science Foundation for Distinguished Young Scholars of Higher Education Institutions of Anhui Province(grant numbers:2022AH020034); Natural Science Foundation for Excellent Young Scholars of Higher Education Institutions of Anhui Province(grant numbers:2022AH030049); Major Natural Science Foundation of Higher Education Institutions of Anhui Province(grant numbers:KJ2020ZD28); Natural Science Foundation for Excellent Young Scholars of Anhui Province(grant numbers:2108085Y21); Major Technologies Research and Development Special Program of Anhui Province(grant numbers:202003a05020001); Key research and development projects of Anhui Province(grant numbers:202104a05020015); Open Project of China International Science and Technology Cooperation Base on Intelligent Equipment Manufacturing in Special Service Environment(grant numbers:ISTC2021KF04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10098903","Fuzzy-model-based approach;Markov jump singularly perturbed systems (MJSPSs);parallel algorithm;reinforcement learning (RL)","Markov processes;Optimal control;Parallel algorithms;Heuristic algorithms;Reinforcement learning;System dynamics;Fuzzy systems","","","","","","37","CCBYNCND","11 Apr 2023","","","IEEE","IEEE Journals"
"Toward Intelligent Connected E-Mobility: Energy-Aware Cooperative Driving With Deep Multiagent Reinforcement Learning","X. He; C. Lv","Nanyang Technological University, Singapore; Nanyang Technological University, Singapore","IEEE Vehicular Technology Magazine","18 Sep 2023","2023","18","3","101","109","In recent years, electrified mobility (e-mobility), especially connected and autonomous electric vehicles (CAEVs), has been gaining momentum along with the rapid development of emerging technologies such as artificial intelligence (AI) and Internet of Things. The social benefits of CAEVs are manifested in the form of safer transportation, lower energy consumption, and reduced congestion and emissions. Nevertheless, it is highly difficult to design driving policies that ensure road safety, travel efficiency, and energy conservation for all CAEVs in traffic flows, particularly in a mixed-autonomy scenario where both CAEVs and human-driven vehicles (HDVs) are on the road and interact with each other. Here we present a novel deep multiagent reinforcement learning (DMARL)-enabled energy-aware cooperative driving solution, facilitating CAEVs to learn vehicular platoon management policies for guaranteeing overall traffic flow performance. Specifically, with the aid of information communication technology (ICT), CAEVs can share their vehicle state and learned knowledge, such as their state of charge (SoC), speed, and driving policies. Additionally, a cooperative multiagent actor–critic (CMAAC) technique is developed to optimize vehicular platoon management policies that map perceptual information directly to the group decision-making behaviors for the CAEV platoon. The proposed approach is evaluated in highway on-ramp merging scenarios with two different mixed-autonomy traffic flows. The results demonstrate the benefits of our scheme. Finally, we discuss the challenges and potential research directions for the proposed energy-aware cooperative driving solution.","1556-6080","","10.1109/MVT.2023.3291171","Advanced Manufacturing and Engineering (AME) Young Individual Research(grant numbers:A2084c0156); ANR-NRF(grant numbers:NRF2021-NRF-ANR003 HM Science); MTC(grant numbers:M22K2c0079); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190753","","Decision making;Task analysis;Artificial intelligence;Safety;Batteries;Energy consumption;Energy conservation;Electric vehicles;Autonomous vehicles","automotive electric vehicles;battery powered vehicles;control engineering computing;decision making;deep learning (artificial intelligence);electric vehicle charging;energy conservation;energy consumption;intelligent transportation systems;Internet of Things;mobile robots;multi-agent systems;multi-robot systems;reinforcement learning;road safety;road traffic control;road vehicles;traffic engineering computing;transportation","artificial intelligence;CAEV platoon;CMAAC technique;connected and autonomous electric vehicles;cooperative multiagent actor-critic technique;deep multiagent reinforcement learning-enabled driving solution;DMARL;driving policies;electrified mobility;energy conservation;energy-aware cooperative driving;HDV;highway on-ramp merging scenarios;human-driven vehicles;ICT;information communication technology;intelligent connected e-mobility;lower energy consumption;mixed-autonomy traffic;road safety;SoC;state of charge;traffic flow performance;vehicle state;vehicular platoon management policies","","","","15","IEEE","24 Jul 2023","","","IEEE","IEEE Magazines"
"An Experience Aggregative Reinforcement Learning With Multi-Attribute Decision-Making for Obstacle Avoidance of Wheeled Mobile Robot","C. Hu; B. Ning; M. Xu; Q. Gu","School of Computer Engineering, Hubei University of Arts and Science, Xiangyang, China; School of Computer Engineering, Hubei University of Arts and Science, Xiangyang, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Engineering, Hubei University of Arts and Science, Xiangyang, China","IEEE Access","17 Jun 2020","2020","8","","108179","108190","A variety of reinforcement learning (RL) methods are developed to achieve the motion control for the robotic systems, which has been a hot issue. However, the performance of the conventional RL methods often encounters a bottleneck, because the robots have difficulty in choosing an appropriate action in the control task due to the exploration-exploitation dilemma. To address this problem and improve the learning performance, this work introduces an experience aggregative reinforcement learning method with a Multi-Attribute Decision-Making (MADM) to achieve the real-time obstacle avoidance of wheeled mobile robot (WMR). The proposed method employs an experience aggregation method to cluster experiential samples and it can achieve more effective experience storage. Moreover, to achieve the effective action selection using the prior experience, an action selection policy based on a Multi-Attribute Decision-Making is proposed. Inspired by the hierarchical decision-making, this work decomposes the original obstacle avoidance task into two sub-tasks using a divide-and-conquer approach. Each sub-task is trained individually by a double Q-learning using a simple reward function. Each sub-task learns an action policy, which enables the sub-task to selects an appropriate action to achieve a single goal. The standardized rewards of sub-tasks are calculated when fusing these sub-tasks to eliminate differences in rewards for sub-tasks. Then, the proposed method integrates the prior experience of three trained sub-tasks via an action policy based on a MADM to complete the source task. Simulation results show that the proposed method outperforms competitors.","2169-3536","","10.1109/ACCESS.2020.3001143","Ministry of Education Science and Technology Development Center, University, Industry, Education, and Research Innovation Fund-New Generation Information Technology Innovation; Natural Science Foundation of Hubei Province(grant numbers:2013CFC026); International Science and Technology Cooperation Program of Hubei Province(grant numbers:2017AHB060); Key Programs of Science and Technology Funds of the Xiangyang city(grant numbers:2020ABA00778); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9112198","Reinforcement learning;experience aggregation;multi-attribute decision-making;obstacle avoidance;wheeled mobile robot","Task analysis;Collision avoidance;Reinforcement learning;Decision making;Mobile robots;Training","collision avoidance;decision making;learning (artificial intelligence);mobile robots;motion control","motion control;exploration-exploitation dilemma;simple reward function;WMR;MADM;multiattribute decision-making;double Q-learning;obstacle avoidance;experience aggregative reinforcement learning method;RL methods;robotic systems;wheeled mobile robot","","3","","43","CCBY","9 Jun 2020","","","IEEE","IEEE Journals"
"Symbolic Regression Methods for Reinforcement Learning","J. Kubalík; E. Derner; J. Žegklitz; R. Babuška","Czech Institute of Informatics, Robotics, and Cybernetics, Czech Technical University in Prague, Prague, Czech Republic; Czech Institute of Informatics, Robotics, and Cybernetics, Czech Technical University in Prague, Prague, Czech Republic; Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic; Czech Institute of Informatics, Robotics, and Cybernetics, Czech Technical University in Prague, Prague, Czech Republic","IEEE Access","19 Oct 2021","2021","9","","139697","139711","Reinforcement learning algorithms can solve dynamic decision-making and optimal control problems. With continuous-valued state and input variables, reinforcement learning algorithms must rely on function approximators to represent the value function and policy mappings. Commonly used numerical approximators, such as neural networks or basis function expansions, have two main drawbacks: they are black-box models offering little insight into the mappings learned, and they require extensive trial and error tuning of their hyper-parameters. In this paper, we propose a new approach to constructing smooth value functions in the form of analytic expressions by using symbolic regression. We introduce three off-line methods for finding value functions based on a state-transition model: symbolic value iteration, symbolic policy iteration, and a direct solution of the Bellman equation. The methods are illustrated on four nonlinear control problems: velocity control under friction, one-link and two-link pendulum swing-up, and magnetic manipulation. The results show that the value functions yield well-performing policies and are compact, mathematically tractable, and easy to plug into other algorithms. This makes them potentially suitable for further analysis of the closed-loop system. A comparison with an alternative approach using neural networks shows that our method outperforms the neural network-based one.","2169-3536","","10.1109/ACCESS.2021.3119000","European Regional Development Fund under the project Robotics for Industry 4.0(grant numbers:CZ.02.1.01/0.0/0.0/15_003/0000470); Grant Agency of the Czech Republic (GA.R) titled “Symbolic Regression for Reinforcement Learning in Continuous Spaces”(grant numbers:15-22731S); Grant Agency of the Czech Technical University in Prague(grant numbers:SGS19/174/OHK3/3T/13); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9565900","Reinforcement learning;value iteration;policy iteration;symbolic regression;genetic programming;nonlinear optimal control","Mathematical models;Reinforcement learning;Genetic programming;Numerical models;Approximation algorithms;Tuning;Training","approximation theory;decision theory;function approximation;iterative methods;learning (artificial intelligence);Markov processes;nonlinear control systems;optimal control;pendulums;regression analysis","smooth value functions;off-line methods;state-transition model;symbolic value iteration;symbolic policy iteration;nonlinear control problems;velocity control;neural networks;neural network-based;symbolic regression methods;reinforcement learning algorithms;dynamic decision-making;optimal control problems;continuous-valued state;input variables;function approximators;value function;policy mappings;commonly used numerical approximators;basis function expansions;black-box models;extensive trial;error tuning","","1","","41","CCBY","8 Oct 2021","","","IEEE","IEEE Journals"
"Optimizing Reinforcement Learning Control Model in Furuta Pendulum and Transferring it to Real-World","M. R. Hong; S. Kang; J. Lee; S. Seo; S. Han; J. -S. Koh; D. Kang","Department of Mechanical Engineering, Multiscale Bio-Inspired Technology Laboratory, Ajou University, Suwon, Republic of Korea; Department of Mechanical Engineering, Multiscale Bio-Inspired Technology Laboratory, Ajou University, Suwon, Republic of Korea; Department of Sustainable Environment Research, Korea Institute of Machinery ad Materials, Daejeon, Republic of Korea; Department of Nano-Chemical, Biological and Environmental Engineering, Seokyeong University, Seoul, Seongbuk, Republic of Korea; Department of Mechanical Engineering, Multiscale Bio-Inspired Technology Laboratory, Ajou University, Suwon, Republic of Korea; Department of Mechanical Engineering, Multiscale Bio-Inspired Technology Laboratory, Ajou University, Suwon, Republic of Korea; Department of Mechanical Engineering, Multiscale Bio-Inspired Technology Laboratory, Ajou University, Suwon, Republic of Korea","IEEE Access","8 Sep 2023","2023","11","","95195","95200","Reinforcement learning does not require explicit robot modeling as it learns on its own based on data, but it has temporal and spatial constraints when transferred to real-world environments. In this research, we trained a balancing Furuta pendulum problem, which is difficult to model, in a virtual environment (Unity) and transferred it to the real world. The challenge of the balancing Furuta pendulum problem is to maintain the pendulum’s end effector in a vertical position. We resolved the temporal and spatial constraints by performing reinforcement learning in a virtual environment. Furthermore, we designed a novel reward function that enabled faster and more stable problem-solving compared to the two existing reward functions. We validate each reward function by applying it to the soft actor-critic (SAC) and proximal policy optimization (PPO). The experimental result shows that cosine reward function is trained faster and more stable. Finally, SAC algorithm model using a cosine reward function in the virtual environment is an optimized controller. Additionally, we evaluated the robustness of this model by transferring it to the real environment.","2169-3536","","10.1109/ACCESS.2023.3310405","Ajou University research fund; National Research Foundation of Korea (NRF); Korea Ministry of Science and ICT (MSIT)(grant numbers:2022R1A2C2093100); Korea Environment Industry & Technology Institute (KEITI) through Digital Infrastructure Building Project for Monitoring, Surveying and Evaluating the Environmental Health Program; Korea Ministry of Environment (MOE)(grant numbers:2021003330009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234431","Furuta pendulum;inverted pendulum problem;reward design;reinforcement learning;Sim2Real","Reinforcement learning;Task analysis;Virtual environments;Mathematical models;Robots;Data models;Learning systems","deep learning (artificial intelligence);end effectors;learning (artificial intelligence);optimal control;optimisation;pendulums;reinforcement learning","balancing Furuta;cosine reward function;existing reward functions;explicit robot;faster problem-solving;more stable problem-solving;optimized controller;pendulum;proximal policy optimization;real-world environments;reinforcement learning control model;SAC algorithm model;spatial constraints;temporal constraints;virtual environment","","","","18","CCBYNCND","30 Aug 2023","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Smart Joint Control Scheme for On/Off Pumping Systems in Wastewater Treatment Plants","G. Seo; S. Yoon; M. Kim; C. Mun; E. Hwang","Department of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology (GIST), Gwangju, Republic of Korea; Department of Mechanical Engineering, Gwangju Institute of Science and Technology (GIST), Gwangju, Republic of Korea; Department of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology (GIST), Gwangju, Republic of Korea; Sewage Business Department, Busan Environmental Corporation, Busan, Republic of Korea; Department of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology (GIST), Gwangju, Republic of Korea","IEEE Access","13 Jul 2021","2021","9","","95360","95371","In this paper, we propose a deep reinforcement learning (DRL) based predictive control scheme for reducing the energy consumption and energy cost of pumping systems in wastewater treatment plants (WWTP), in which the pumps are operated in a binary mode, using on/off signals. As global energy consumption increases, the efficient operation of energy-intensive facilities has also become important. A WWTP in Busan, Republic of Korea is used as the target of this study. This WWTP is a large energy-consuming facility, and the pumping station accounts for a significant portion of the energy consumption of the WWTP. The framework of the proposed scheme consists of a deep neural network (DNN) model for forecasting wastewater inflow and a DRL agent for controlling the on/off signals of the pumping system, where proximal policy optimization (PPO) and deep Q-neural network (DQN) are employed as the DRL agents. To implement smart control with DRL, a reward function is designed to consider the energy consumption amount and electricity price information. In particular, new features and penalty factors for pump switching, which are essential for preventing pump wear, are also considered. The performance of our designed DRL agents is compared with those of WWTP experts and conventional approaches such as scheduling method and model predictive control (MPC), in which integer linear programming (ILP) optimization is employed. Results show that the designed agents outperform the other approaches in terms of compliance with operating rules and reducing energy costs.","2169-3536","","10.1109/ACCESS.2021.3094466","Energy AI Convergence Research & Development Program through the National IT Industry Promotion Agency of Korea (NIPA) funded by the Ministry of Science and ICT(grant numbers:1711120811); GIST Research Institute (GRI) Grant funded by the GIST in 2021; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474446","Predictive control;deep neural network;reinforcement learning;pumping system;cost-effective energy efficiency","Job shop scheduling;Energy consumption;Wastewater;Predictive models;Tariffs;Predictive control;Forecasting","control engineering computing;cost reduction;energy consumption;integer programming;learning (artificial intelligence);linear programming;neural nets;predictive control;pricing;pumping plants;wastewater treatment;water pumps;wear","DRL agent;pump switching;WWTP experts;scheduling method;model predictive control;energy costs reduction;deep reinforcement learning;smart joint control scheme;wastewater treatment plants;predictive control scheme;global energy consumption;energy-intensive facilities;energy-consuming facility;pumping station;wastewater inflow forecasting;on-off pumping system;pump wear prevention;deep Q-neural network;Busan;Republic of Korea;proximal policy optimization;PPO;DQN;electricity price information;integer linear programming;ILP","","11","","52","CCBY","5 Jul 2021","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Tf-agent-based Object Tracking with Virtual Autonomous Drone in a Game Engine","K. Farkhodov; S. -H. Lee; I. J. Platos; K. -R. Kwon","Department of AI Convergence, Pukyong National University, Busan, South Korea; Department of Computer Engineering, Donga University, Busan, South Korea; Department of Electrical Engineering and Computer Science, VSB-Technical University of Ostrava, Czech Republic; Department of AI Convergence, Pukyong National University, Busan, South Korea","IEEE Access","","2023","PP","99","1","1","The recent development of object-tracking framework inventions has affected the performance of many manufacturing and service industries, such as product delivery, autonomous driving systems, security systems, military and transportation, retailing industries, smart cities, healthcare systems, agriculture, etc. Object tracking in physical environments and conditions is much more challenging to achieve accurate results. However, the process can be experimented using simulation techniques or platforms to evaluate and check the model’s performance under different simulation conditions and weather changes. This paper represents one of the target tracking approaches based on the reinforcement learning technique integrated with tf-agent (TensorFlow-Agent) to accomplish the tracking process in the Unreal Game Engine simulation platform, Blocks. The productivity of these platforms can be seen while experimenting in virtual-reality conditions with virtual drone agents and performing fine-tuning to achieve the best or desired performance. In this proposal, the tf-agent drone learns how to track an object integration with a deep reinforcement learning process to control the actions, states, and tracking by receiving sequential frames from a simple Blocks environment. The TF-agent is trained in a Blocks environment for adaptation to the environment and existing objects in a simulation environment for further testing and evaluation regarding the accuracy of tracking and speed. We have tested and compared two approaches to the algorithm methods based on the DQN and PPO trackers integrated with the simulation process regarding stability, rewards, and numerical performance.","2169-3536","","10.1109/ACCESS.2023.3325062","Ministry of Science and ICT, South Korea(grant numbers:IITP-2023-2016-0-00318,IITP-2023-2020-0-01797); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10286478","Object Tracking;Object Detection;Reinforcement Learning;AirSim;Virtual Environment;Virtual Simulation;tf-agent;Unreal Game Engine","Drones;Target tracking;Object tracking;Adaptation models;Deep learning;Training;Testing;Reinforcement learning;Virtual environments","","","","","","","CCBYNCND","16 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Residential Demand Response of Thermostatically Controlled Loads Using Batch Reinforcement Learning","F. Ruelens; B. J. Claessens; S. Vandael; B. De Schutter; R. Babuška; R. Belmans","Department of Electrical Engineering, Katholieke Universiteit Leuven/EnergyVille, Leuven, Belgium; Energy Department, Flemish Institute for Technological Research, Mol, Belgium; Department of Electrical Engineering, Katholieke Universiteit Leuven/EnergyVille, Leuven, Belgium; Delft Center for Systems and Control, Delft University of Technology, Delft, The Netherlands; Delft Center for Systems and Control, Delft University of Technology, Delft, The Netherlands; Department of Electrical Engineering, Katholieke Universiteit Leuven/EnergyVille, Leuven, Belgium","IEEE Transactions on Smart Grid","21 Aug 2017","2017","8","5","2149","2159","Driven by recent advances in batch Reinforcement Learning (RL), this paper contributes to the application of batch RL to demand response. In contrast to conventional model-based approaches, batch RL techniques do not require a system identification step, making them more suitable for a large-scale implementation. This paper extends fitted Q-iteration, a standard batch RL technique, to the situation when a forecast of the exogenous data is provided. In general, batch RL techniques do not rely on expert knowledge about the system dynamics or the solution. However, if some expert knowledge is provided, it can be incorporated by using the proposed policy adjustment method. Finally, we tackle the challenge of finding an open-loop schedule required to participate in the day-ahead market. We propose a model-free Monte Carlo method that uses a metric based on the state-action value function or Q-function and we illustrate this method by finding the day-ahead schedule of a heat-pump thermostat. Our experiments show that batch RL techniques provide a valuable alternative to model-based controllers and that they can be used to construct both closed-loop and open-loop policies.","1949-3061","","10.1109/TSG.2016.2517211","Flemish Institute for the Promotion of Scientific and Technological Research in Industry (IWT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401112","Batch reinforcement learning;demand response;electric water heater;fitted Q-iteration;heat pump","Load management;Water heating;Resistance heating;Atmospheric modeling;Load modeling;Feature extraction;Learning (artificial intelligence)","demand side management;heat pumps;learning (artificial intelligence);Monte Carlo methods;power engineering computing;power markets","residential demand response;thermostatically controlled loads;batch reinforcement learning;model-based approaches;batch RL techniques;system identification step;fitted Q-iteration;policy adjustment method;open-loop schedule;day-ahead market;model-free Monte Carlo method;state-action value function;Q-function;day-ahead schedule;heat-pump thermostat","","211","","45","IEEE","8 Feb 2016","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Energy Management for a Series Hybrid Electric Vehicle Enabled by History Cumulative Trip Information","Y. Li; H. He; J. Peng; H. Wang","National Engineering Laboratory for Electric Vehicles, School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; National Engineering Laboratory for Electric Vehicles, School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; National Engineering Laboratory for Electric Vehicles, School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; Department of Mechanical and Mechatronics Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Transactions on Vehicular Technology","13 Aug 2019","2019","68","8","7416","7430","It is essential to develop proper energy management strategies (EMSs) with broad adaptability for hybrid electric vehicles (HEVs). This paper utilizes deep reinforcement learning (DRL) to develop EMSs for a series HEV due to DRL's advantages of requiring no future driving information in derivation and good generalization in solving energy management problem formulated as a Markov decision process. History cumulative trip information is also integrated for effective state of charge guidance in DRL-based EMSs. The proposed method is systematically introduced from offline training to online applications; its learning ability, optimality, and generalization are validated by comparisons with fuel economy benchmark optimized by dynamic programming, and real-time EMSs based on model predictive control (MPC). Simulation results indicate that without a priori knowledge of future trip, original DRL-based EMS achieves an average 3.5% gap from benchmark, superior to MPC-based EMS with accurate prediction; after further applying output frequency adjustment, a mean gap of 8.7%, which is comparable with MPC-based EMS with mean prediction error of 1 m/s, is maintained with concurrently noteworthy improvement in reducing engine start times. Besides, its impressive computation speed of about 0.001 s per simulation step proves its practical application potential, and this method is independent of powertrain topology such that it is applicative for any type of HEVs even when future driving information is unavailable.","1939-9359","","10.1109/TVT.2019.2926472","National Basic Research Program of China (973 Program)(grant numbers:2018YFB0105900); National Natural Science Foundation of China(grant numbers:51705020,51675042); China Postdoctoral Science Foundation(grant numbers:2016M600933); Project funded by Chief Expert for Modern Industry of Dezhou(grant numbers:GQDZKT2017001); International Graduate Exchange Program of the Beijing Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754786","Deep reinforcement learning;dynamic programming;energy management;generalization;model predictive control;optimality","Energy management;Engines;Hybrid electric vehicles;History;Training;Batteries;Computational modeling","dynamic programming;energy management systems;fuel economy;hybrid electric vehicles;learning (artificial intelligence);Markov processes;power engineering computing;traffic engineering computing","deep reinforcement learning;series hybrid electric vehicle;history cumulative trip information;hybrid electric vehicles;series HEV;future driving information;energy management problem;Markov decision process;DRL-based EMSs;fuel economy benchmark;model predictive control;MPC-based EMS;mean prediction error;energy management strategies;generalization;DRL-based EMS;velocity 1.0 m/s;time 0.001 s","","112","","41","IEEE","3 Jul 2019","","","IEEE","IEEE Journals"
"Cooperative Management for PV/ESS-Enabled Electric Vehicle Charging Stations: A Multiagent Deep Reinforcement Learning Approach","M. Shin; D. -H. Choi; J. Kim","School of Computer Science and Engineering, Chung-Ang University, Seoul, South Korea; School of Electrical and Electronics Engineering, Chung-Ang University, Seoul, South Korea; School of Electrical Engineering, Korea University, Seoul, South Korea","IEEE Transactions on Industrial Informatics","17 Feb 2020","2020","16","5","3493","3503","This article proposes a novel multiagent deep reinforcement learning method for the energy management of distributed electric vehicle charging stations with a solar photovoltaic system and energy storage system. In the literature, the conventional method is to calculate the optimal electric vehicle charging schedule in a centralized manner. However, in general, the centralized approach is not realistic under certain environments where the system operators for multiple electric vehicle charging stations handle dynamically varying data, such as the status of the energy storage system and electric vehicle-related information. Therefore, this article proposes a method that can compute the scheduling solutions of multiple electric vehicle charging stations in a distributed manner while handling run-time time-varying dynamic data. As shown in the data-intensive performance evaluation, it can be observed that the proposed method achieves a desirable performance in terms of reducing the operation costs of electric vehicle charging stations.","1941-0050","","10.1109/TII.2019.2944183","National Research Foundation of Korea(grant numbers:2019R1A2C4070663); Human Resources Development; Korea Institute of Energy Technology Evaluation and Planning; Ministry of Trade, Industry and Energy(grant numbers:20184030202070); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8851270","Electric vehicles;multi-agent systems;neural networks;scheduling algorithms","Electric vehicle charging;Optimization;Reinforcement learning;Companies;Energy management;Multi-agent systems;Planning","battery powered vehicles;electric vehicles;energy management systems;energy storage;learning (artificial intelligence);multi-agent systems;power engineering computing;power grids;scheduling","energy storage system;electric vehicle-related information;multiple electric vehicle;run-time time-varying dynamic data;multiagent deep reinforcement learning approach;energy management;distributed electric vehicle;solar photovoltaic system;optimal electric vehicle;centralized manner;system operators;PV-ESS-enabled electric vehicle charging stations","","104","","25","IEEE","27 Sep 2019","","","IEEE","IEEE Journals"
"Virtual Network Function Placement Optimization With Deep Reinforcement Learning","R. Solozabal; J. Ceberio; A. Sanchoyerto; L. Zabala; B. Blanco; F. Liberal","Networking Quality and Security Department, University of the Basque Country (UPV/EHU), Bilbao, Spain; Department of Computer Science and Artificial Intelligence, University of the Basque Country, Donostia, Spain; Networking Quality and Security Department, University of the Basque Country (UPV/EHU), Bilbao, Spain; Networking Quality and Security Department, University of the Basque Country (UPV/EHU), Bilbao, Spain; Department of Computer Languages and System, University of the Basque Country (UPV/EHU), Bilbao, Spain; Networking Quality and Security Department, University of the Basque Country (UPV/EHU), Bilbao, Spain","IEEE Journal on Selected Areas in Communications","21 Feb 2020","2020","38","2","292","303","Network Function Virtualization (NFV) introduces a new network architecture framework that evolves network functions, traditionally deployed over dedicated equipment, to software implementations that run on general-purpose hardware. One of the main challenges for deploying NFV is the optimal resource placement of demanded network services in the NFV infrastructure. The virtual network function placement and network embedding can be formulated as a mathematical optimization problem concerned with a set of feasibility constraints that express the restrictions of the network infrastructure and the services contracted. This problem has been reported to be NP-hard, as a result most of the optimization work carried out in the area has focused on designing heuristic and metaheuristic algorithms. Nevertheless, in highly constrained problems, as in this case, inferring a competitive heuristic can be a daunting task that requires expertise. Consequently, an interesting solution is the use of Reinforcement Learning to model an optimization policy. The work presented here extends the Neural Combinatorial Optimization theory by considering constraints in the definition of the problem. The resulting agent is able to learn placement decisions by exploring the NFV infrastructure with the aim of minimizing the overall power consumption. The experiments conducted demonstrate that when the proposed strategy is also combined with heuristics, highly competitive results are achieved using relatively simple algorithms.","1558-0008","","10.1109/JSAC.2019.2959183","European Commission(grant numbers:761592); Spanish Ministry of Economy, Industry and Competitiveness through the projects TIN2016-78365-R and 5RANVIR(grant numbers:TEC2016-80090-C2-2-R); Basque Government IT1003-16 and ELKARTEK Programs; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945291","Constrained combinatorial optimization;Reinforcement Leaning;5G;NFV","Optimization;Heuristic algorithms;Neural networks;Reinforcement learning;Network function virtualization;Power demand;Mathematical model","combinatorial mathematics;learning (artificial intelligence);optimisation;virtualisation","NFV infrastructure;network function virtualization;network architecture framework;general-purpose hardware;optimal resource placement;mathematical optimization problem;virtual network function placement optimization;deep reinforcement learning;neural combinatorial optimization theory;NP-hard problem","","61","","28","IEEE","30 Dec 2019","","","IEEE","IEEE Journals"
"Comparative Analysis of Energy Management Strategies for HEV: Dynamic Programming and Reinforcement Learning","H. Lee; C. Song; N. Kim; S. W. Cha","Department of Mechanical and Aerospace Engineering, Seoul National University, Seoul, South Korea; Department of Mechanical and Aerospace Engineering, Seoul National University, Seoul, South Korea; Department of Mechanical Engineering, Hanyang University, Ansan, South Korea; Department of Mechanical and Aerospace Engineering, Seoul National University, Seoul, South Korea","IEEE Access","17 Apr 2020","2020","8","","67112","67123","Energy management strategy is an important factor in determining the fuel economy of hybrid electric vehicles; thus, much research on how to distribute the required power to engines and motors of hybrid vehicles is required. Recently, various studies have been conducted based on reinforcement learning to optimally control the hybrid electric vehicle. In fact, the fundamental control approach of reinforcement learning shares many control frameworks with the control approach by using deterministic dynamic programming or stochastic dynamic programming. In this study, we compare the reinforcement learning based strategy by using these dynamic programming-based control approaches. For optimal control of hybrid electric vehicle, each control method was compared in terms of fuel efficiency by performing simulation by using various driving cycles. Based on our simulations, we showed the reinforcement learning-based strategy can obtain global optimality in the optimal control problem with an infinite horizon, which can also be obtained by stochastic dynamic programming. We also showed that the reinforcement learning-based strategy can present a solution close to the optimal one using deterministic dynamic programming, while a reinforcement learning-based strategy is more appropriate for a time variant controller with boundary value constraints. In addition, we verified the convergence characteristics of the control strategy based on reinforcement learning, when transfer learning was performed through value initialization using stochastic dynamic programming.","2169-3536","","10.1109/ACCESS.2020.2986373","Ministry of Trade, Industry, and Energy (MOTIE), South Korea, through the Technology Innovation Program (Development of RDE DB and Application Source Technology for Improvement of Real Road CO2 and Particulate Matter)(grant numbers:20002762); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9058677","Dynamic programming;hybrid electric vehicle;optimal control;reinforcement learning;power management","Hybrid electric vehicles;Energy management;Optimal control;Engines;Dynamic programming;Fuel economy;Learning (artificial intelligence)","control engineering computing;dynamic programming;energy management systems;fuel economy;hybrid electric vehicles;learning (artificial intelligence);optimal control","energy management strategy;hybrid electric vehicle;fundamental control approach;deterministic dynamic programming;stochastic dynamic programming;reinforcement learning based strategy;dynamic programming-based control approaches;reinforcement learning-based strategy;optimal control problem;control strategy","","59","","37","CCBY","7 Apr 2020","","","IEEE","IEEE Journals"
"USV Formation and Path-Following Control via Deep Reinforcement Learning With Random Braking","Y. Zhao; Y. Ma; S. Hu","Department of Maritime Management, School of Navigation, Hubei Key Laboratory of Inland Shipping Technology, Wuhan University of Technology, Wuhan, China; Department of Maritime Management, School of Navigation, Hubei Key Laboratory of Inland Shipping Technology, Wuhan University of Technology, Wuhan, China; Institute of Advanced Technology, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Transactions on Neural Networks and Learning Systems","30 Nov 2021","2021","32","12","5468","5478","This article addresses the problem of path following for underactuated unmanned surface vessels (USVs) formation via a modified deep reinforcement learning with random braking (DRLRB). A formation control model based on deep reinforcement learning (DRL) is constructed to urge USVs to form a preset formation. Specifically, an efficient reward function is designed from the perspective of velocity and error distance of each USV related to the given formation, and then a novel random braking mechanism is formulated to prevent the training of the decision-making network from falling into the local optimum and failing to achieve the training objectives. Following that, a virtual leader-based path-following guidance system is developed for the USV formation problem. Wherein, with the aid of DRLRB, our proposed system can adjust formation automatically and flexibly even when some USVs deviate from the formation. Simulation verifies the effectiveness and superiority of our formation and path-following control strategy.","2162-2388","","10.1109/TNNLS.2021.3068762","National Science Foundation of China(grant numbers:52022073,62073251,51911540478); Excellent Youth Foundation of Hubei Scientific Committee(grant numbers:2020CFA055); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20201377); High Level Talents Cultivation Project in Transport Industry(grant numbers:2019-011); National Key Research and Development Plan, China(grant numbers:2018YFC1407400); Hubei Key Laboratory of Inland Shipping Technology(grant numbers:NHHY2019003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9393593","Deep reinforcement learning (DRL);random braking;unmanned surface vessel (USV) formation path following;virtual leader","Training;Reinforcement learning;Usability;Task analysis;Navigation;Machine learning algorithms;Learning systems;Deep learning;Autonomous vehicles","braking;control engineering computing;deep learning (artificial intelligence);marine control;marine vehicles;mobile robots;multi-robot systems;path planning;position control;reinforcement learning;unmanned surface vehicles","underactuated unmanned surface vessels formation;DRLRB;formation control;preset formation;error distance;virtual leader-based path-following guidance system;USV formation problem;path-following control strategy;deep reinforcement learning with random braking;reward function","","56","","31","IEEE","1 Apr 2021","","","IEEE","IEEE Journals"
"Deep Distributional Reinforcement Learning Based High-Level Driving Policy Determination","K. Min; H. Kim; K. Huh","Department of Automotive Engineering, Hanyang University, Seoul, South Korea; Department of Automotive Engineering, Hanyang University, Seoul, South Korea; Department of Automotive Engineering, Hanyang University, Seoul, South Korea","IEEE Transactions on Intelligent Vehicles","26 Aug 2019","2019","4","3","416","424","Even though some of the driver assistant systems have been commercialized to provide safety and convenience to the driver, they can be applied for autonomous driving in limited situations such as highways. In this paper, we propose a supervisor agent that can enhance the driver assistant systems by using deep distributional reinforcement learning. The supervisor agent is trained using end-to-end approach that directly maps both a camera image and LIDAR data into action plan. Because the well-trained network of deep reinforcement learning can lead to unexpected actions, collision avoidance function is added to prevent dangerous situations. In addition, the highway driving case is a stochastic environment with inherent randomness and, thus, its training is performed through the distributional reinforcement learning algorithm, which is specialized for stochastic environment. The optimal action for autonomous driving is selected through the return value distribution. Finally, the proposed algorithm is verified through a highway driving simulator, which is implemented by the Unity ML-agents.","2379-8904","","10.1109/TIV.2019.2919467","Ministry of Land Infrastructure and Transport of Korean government(grant numbers:18TLRPB101406-04); Ministry of Trade Industry and Energy MOTIE Korea(grant numbers:10079730); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8723635","Distributional reinforcement learning;policy determination;highway driving","Reinforcement learning;Autonomous vehicles;Road transportation;Prediction algorithms;Training","collision avoidance;driver information systems;learning (artificial intelligence);multi-agent systems;neural nets;optical radar;stochastic processes","camera image;deep reinforcement learning;highway driving case;stochastic environment;distributional reinforcement learning algorithm;autonomous driving;highway driving simulator;driver assistant systems;supervisor agent;driver safety;deep distributional reinforcement learning;Unity ML-agents;LIDAR data;collision avoidance","","42","","28","IEEE","28 May 2019","","","IEEE","IEEE Journals"
"A Model-Driven Deep Reinforcement Learning Heuristic Algorithm for Resource Allocation in Ultra-Dense Cellular Networks","X. Liao; J. Shi; Z. Li; L. Zhang; B. Xia","State Key Laboratory of Integrated Service Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; School of Engineering, University of Glasgow, Glasgow, U.K.; Silo.AI Oy, Helsinki, Finland","IEEE Transactions on Vehicular Technology","16 Jan 2020","2020","69","1","983","997","Resource allocation in ultra dense network (UDN) is an multi-objective optimization problem since it has to consider the tradeoff among spectrum efficiency (SE), energy efficiency (EE) and fairness. The existing methods can not effectively solve this NP-hard nonconvex problem, especially in the presence of limited channel state information (CSI). In this paper, we investigate a novel model-driven deep reinforcement learning assisted resource allocation method. We first design a novel deep neural network (DNN)-based optimization framework consisting of a series of Alternating Direction Method of Multipliers (ADMM) iterative procedures, which makes the CSI as the learned weights. Then a novel channel information absent Q-learning resource allocation (CIAQ) algorithm is proposed to train the DNN-based optimization framework without massive labeling data, where the SE, the EE, and the fairness can be jointly optimized by adjusting discount factor. Our simulation results show that, the proposed CIAQ with rapid convergence speed not only well characterizes the extent of optimization objective with partial CSI, but alsoconcave programming significantly outperforms the current random initialization method of neural network and the other existing resource allocation algorithms in term of the tradeoff among the SE, EE and fairness.","1939-9359","","10.1109/TVT.2019.2954538","National Natural Science Foundation of China(grant numbers:61631015,61901327); National Natural Science Foundation for Distinguished Young Scholar of China(grant numbers:61825104); China Postdoctoral Science Foundation(grant numbers:2018M631122); Key Laboratory Foundation of Ministry of Industry and Information Technology(grant numbers:KF20181912); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8907392","Ultra-dense cellular networks;resource allocation;deep reinforcement learning;model-driven;optimization","Resource management;Optimization;Reinforcement learning;Heuristic algorithms;Deep learning;Neural networks;Vehicle dynamics","cellular radio;concave programming;heuristic programming;iterative methods;learning (artificial intelligence);neural nets;radio spectrum management;resource allocation;telecommunication computing;telecommunication power management","random initialization method;convergence speed;discount factor adjusting;joint optimisation;labeling data independent;CIAQ algorithm;channel information absent Q-learning resource allocation algorithm;Alternating Direction Method of Multipliers series;channel state information;telecommunication fairness efficiency;energy efficiency;spectrum efficiency;ultra-dense cellular networks;resource allocation;deep reinforcement learning heuristic algorithm;NP-hard nonconvex problem;ADMM iterative procedures;DNN-based optimization framework;multiobjective optimization problem","","31","","39","IEEE","20 Nov 2019","","","IEEE","IEEE Journals"
"Optimization of Task Offloading Strategy for Mobile Edge Computing Based on Multi-Agent Deep Reinforcement Learning","H. Lu; C. Gu; F. Luo; W. Ding; S. Zheng; Y. Shen","School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China","IEEE Access","18 Nov 2020","2020","8","","202573","202584","Combined with wireless power transfer (WPT) technology, mobile edge computing can provide continuous energy supply and computing resources for mobile devices, and improve their battery life and business application scenarios. This article first designs the mobile edge computing (MEC) model of mobile devices with random mobility and hybrid access point (HAP) with data transmission and energy transmission. On this basis, the selection of target server and the amount of data offloading are taken as the learning objectives, and the task offloading strategy based on multi-agent deep reinforcement learning is constructed. Then combined with MADDPG algorithm and SAC algorithm, the problems of multi-agent environment instability and the difficulty of convergence are solved. The final experimental results show that the improved algorithm based on MADDPG and SAC has good stability and convergence. Compared with other algorithms, it has achieved good results in energy consumption, delay and task failure rate.","2169-3536","","10.1109/ACCESS.2020.3036416","National Natural Science Foundation of China(grant numbers:61472139); Shanghai Automobile Industry Science and Technology Development Foundation(grant numbers:H100-2-19160); Shanghai Sailing Program(grant numbers:20YF1410900); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9250483","Mobile edge computing;task offloading;wireless power transfer;multi-agent;deep reinforcement learning","Task analysis;Servers;Mobile handsets;Reinforcement learning;Delays;Energy consumption;Performance evaluation","inductive power transmission;learning (artificial intelligence);mobile computing;multi-agent systems;neural nets;power engineering computing;resource allocation","optimization;HAP;hybrid access point;convergence;SAC;MADDPG;multiagent environment instability;task offloading strategy;data offloading;energy transmission;data transmission;random mobility;mobile edge computing model;business application scenarios;battery life;mobile devices;computing resources;continuous energy supply;wireless power transfer technology;multiagent deep reinforcement learning","","29","","27","CCBY","6 Nov 2020","","","IEEE","IEEE Journals"
"Deterministic Promotion Reinforcement Learning Applied to Longitudinal Velocity Control for Automated Vehicles","Y. Zhang; L. Guo; B. Gao; T. Qu; H. Chen","State Key Laboratory of Automotive Simulation and Control, Jilin University, Changchun, China; Center for Cyber-Physical Systems, University of Georgia, Athens, USA; State Key Laboratory of Automotive Simulation and Control, Jilin University, Changchun, China; State Key Laboratory of Automotive Simulation and Control, Jilin University, Changchun, China; Clean Energy Automotive Engineering Center, Tongji University, Shanghai, China","IEEE Transactions on Vehicular Technology","16 Jan 2020","2020","69","1","338","348","Reinforcement learning is regarded as a potential method to be applied in automated vehicles, but the stability and efficiency of algorithms are concerns. To improve them, the deterministic promotion reinforcement learning method is put forward, which can promote the policy determinately. Correspondingly, the policy evaluation in critic and the exploration in actor are improved, which combines a normalization-based evaluation and a model-free search guide. The aim is finding the right action exploration direction by critic, then the direction is used to update and guide action exploration in actor. The modified method decreases the dependencies of exploring a good action for promotional updating and only makes deterministic promotion in policy. Consequently, the efficiency of the algorithm is improved without loss in stability. More notably, it can relieve the cold-start and circumvent the limitations in learning with constrained physical systems. To verify the proposed method, the longitudinal velocity control problem for automated vehicles is considered, which contains car-following and non-car-following conditions in a unitized form. The learning system is established in Carsim. Furthermore, some different reinforcement learning technologies are used to accelerate learning. Real-vehicle experiments for validation are also given. The results indicate that the proposed method can achieve permissible learning performance in the longitudinal velocity continuous control problem.","1939-9359","","10.1109/TVT.2019.2955959","National Basic Research Program of China (973 Program)(grant numbers:2016YFB0100904); National Natural Science Foundation of China(grant numbers:61790564); China Automobile Industry Innovation and Development Joint Fund(grant numbers:U1664257); State Key Laboratory of Comprehensive Technology on Automobile Vibration and Noise and Safety Control(grant numbers:W65-GNZX-2018-0242); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8913608","Reinforcement Learning;Cold-start;Policy Gradient Method;Continuous Control;Longitudinal Velocity Control","Gradient methods;Velocity control;Stability analysis;Reinforcement learning;Indexes;Automotive engineering;Approximation algorithms","control engineering computing;intelligent transportation systems;learning (artificial intelligence);road traffic control;traffic engineering computing;velocity control","automated vehicles;deterministic promotion reinforcement learning method;policy evaluation;normalization-based evaluation;model-free search guide;action exploration direction;learning system;real-vehicle experiments;permissible learning performance;longitudinal velocity continuous control problem;noncar-following conditions;car-following conditions;Carsim","","25","","27","IEEE","26 Nov 2019","","","IEEE","IEEE Journals"
"Model-Based Reinforcement Learning for Eco-Driving Control of Electric Vehicles","H. Lee; N. Kim; S. W. Cha","Department of Mechanical Engineering, Seoul National University, Seoul, South Korea; Department of Mechanical Engineering, Hanyang University, Ansan, South Korea; Department of Mechanical Engineering, Seoul National University, Seoul, South Korea","IEEE Access","13 Nov 2020","2020","8","","202886","202896","With the development of autonomous vehicles, research on energy-efficient eco-driving is becoming increasingly important. The optimal control problem of determining the speed profile of the vehicle for minimizing energy consumption is a challenging problem that necessitates the consideration of various aspects, such as the vehicle energy consumption, slope of the road, and driving environment, e.g., the traffic and other vehicles on the road. In this study, an approach using reinforcement learning was applied to the eco-driving problem for electric vehicles considering road slopes. A novel model-based reinforcement learning algorithm for eco-driving was developed, which separates the vehicle's energy consumption approximation model and driving environment model. Thus, the domain knowledge of vehicle dynamics and the powertrain system is utilized for the reinforcement learning process, while model-free characteristics are maintained by updating the approximation model using experience replay. The proposed algorithm was tested via a vehicle simulation and compared with a solution obtained using dynamic programming (DP), and as well as conventional cruise control driving with constant speed. The simulation results indicated that the speed profile optimized using model-based reinforcement learning had similar behavior to the global solution obtained via DP and energy saving performance compared with cruise control.","2169-3536","","10.1109/ACCESS.2020.3036719","National Research Foundation of Korea (NRF); Korea Government (MSIT)(grant numbers:NRF-2019R1A4A1025848); Technology Innovation Program (Development of Application Technologies For Heavy Duty Fuel Cell Electric Trucks Using Multi-Input Motor-Based 400kW Class Multi Speed Electrified Powertrain System) funded by the Ministry of Trade, Industry and Energy (MOTIE, South Korea)(grant numbers:20011834); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252098","Eco-driving control;electric vehicles;model-based reinforcement learning;optimal control;Q-learning;reinforcement learning","Reinforcement learning;Optimal control;Roads;Optimization;Torque;Vehicles;Vehicle dynamics","dynamic programming;energy consumption;hybrid electric vehicles;learning (artificial intelligence);optimal control;power transmission (mechanical);road vehicles;vehicle dynamics;velocity control","model-free characteristics;reinforcement learning process;vehicle dynamics;driving environment model;energy consumption approximation model;model-based reinforcement learning algorithm;road slopes;eco-driving problem;vehicle energy consumption;speed profile;optimal control problem;energy-efficient eco-driving;autonomous vehicles;electric vehicles;eco-driving control;energy saving performance;vehicle simulation","","25","","25","CCBY","9 Nov 2020","","","IEEE","IEEE Journals"
"Risk-Aware Energy Scheduling for Edge Computing With Microgrid: A Multi-Agent Deep Reinforcement Learning Approach","M. S. Munir; S. F. Abedin; N. H. Tran; Z. Han; E. -N. Huh; C. S. Hong","Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea; School of Computer Science, University of Sydney, Sydney, NSW, Australia; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea","IEEE Transactions on Network and Service Management","8 Sep 2021","2021","18","3","3476","3497","In recent years, multi-access edge computing (MEC) is a key enabler for handling the massive expansion of Internet of Things (IoT) applications and services. However, energy consumption of a MEC network depends on volatile tasks that induces risk for energy demand estimations. As an energy supplier, a microgrid can facilitate seamless energy supply. However, the risk associated with energy supply is also increased due to unpredictable energy generation from renewable and non-renewable sources. Especially, the risk of energy shortfall is involved with uncertainties in both energy consumption and generation. In this article, we study a risk-aware energy scheduling problem for a microgrid-powered MEC network. First, we formulate an optimization problem considering the conditional value-at-risk (CVaR) measurement for both energy consumption and generation, where the objective is to minimize the expected residual of scheduled energy for the MEC networks and we show this problem is an NP-hard problem. Second, we analyze our formulated problem using a multi-agent stochastic game that ensures the joint policy Nash equilibrium, and show the convergence of the proposed model. Third, we derive the solution by applying a multi-agent deep reinforcement learning (MADRL)-based asynchronous advantage actor-critic (A3C) algorithm with shared neural networks. This method mitigates the curse of dimensionality of the state space and chooses the best policy among the agents for the proposed problem. Finally, the experimental results establish a significant performance gain by considering CVaR for high accuracy energy scheduling of the proposed model than both the single and random agent models.","1932-4537","","10.1109/TNSM.2021.3049381","Korea Institute of Energy Technology Evaluation and Planning (KETEP) and the Ministry of Trade, Industry & Energy (MOTIE) of the Republic of Korea(grant numbers:20199810100050); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9313066","Multi-access edge computing (MEC);microgrid;multi-agent deep reinforcement learning;conditional value-at-risk (CVaR);stochastic game;demand-response (DR)","Microgrids;Energy consumption;Task analysis;Wireless networks;Renewable energy sources;Estimation;Uncertainty","computational complexity;deep learning (artificial intelligence);distributed power generation;distributed processing;energy consumption;Internet of Things;multi-agent systems;optimisation;power engineering computing;power generation scheduling;risk management;stochastic games","NP-hard problem;multiagent stochastic game;asynchronous advantage actor-critic algorithm;multiagent deep reinforcement learning approach;multiaccess edge computing;energy consumption;energy demand estimations;risk-aware energy scheduling problem;microgrid-powered MEC network;value-at-risk measurement;Internet of Things;energy generation;Nash equilibrium;MADRL;neural networks","","21","","89","IEEE","5 Jan 2021","","","IEEE","IEEE Journals"
"Prediction of Reward Functions for Deep Reinforcement Learning via Gaussian Process Regression","J. Lim; S. Ha; J. Choi","School of Mechanical Engineering, Yonsei University, Seoul, South Korea; School of Mechanical Engineering, Yonsei University, Seoul, South Korea; School of Mechanical Engineering, Yonsei University, Seoul, South Korea","IEEE/ASME Transactions on Mechatronics","13 Aug 2020","2020","25","4","1739","1746","Inverse reinforcement learning (IRL) is a technique for automatic reward acquisition, however, it is difficult to apply to high-dimensional problems with unknown dynamics. This article proposes an efficient way to solve the IRL problem based on the sparse Gaussian process (GP) prediction with l1-regularization only using a highly limited number of expert demonstrations. A GP model is proposed to be trained to predict a reward function using trajectory-reward pair data generated by deep reinforcement learning with different reward functions. The trained GP successfully predicts the reward functions of human experts from their collected demonstration trajectory datasets. To demonstrate our approach, the proposed approach is applied to the obstacle avoidance navigation of the mobile robot. The experimental results clearly show that the robots can clone the experts' optimality in navigation trajectories avoiding obstacles using only with a very small number of expert demonstration datasets (e.g., ≤ 6). Therefore, the proposed approach shows great potential to be applied to complex real-world applications in an expert data-efficient manner.","1941-014X","","10.1109/TMECH.2020.2993564","Korea Evaluation Institute of Industrial Technology; Ministry of Trade, Industry and Energy(grant numbers:10073129); National Research Foundation of Korea; Ministry of Science, ICT and Future Planning(grant numbers:NRF-2018R1A2B6008063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9091100","Gaussian processes;inverse reinforcement learning;mobile robots","Trajectory;Data models;Machine learning;Mobile robots;IEEE transactions;Mechatronics","collision avoidance;Gaussian processes;learning (artificial intelligence);mobile robots;regression analysis","expert demonstrations;GP model;reward function;trajectory-reward pair data;deep reinforcement learning;reward functions;trained GP;collected demonstration trajectory datasets;expert demonstration datasets;expert data-efficient manner;Gaussian process regression;inverse reinforcement learning;automatic reward acquisition;high-dimensional problems;IRL problem;sparse Gaussian process prediction","","19","","30","IEEE","11 May 2020","","","IEEE","IEEE Journals"
"A Real-Time Intelligent Energy Management Strategy for Hybrid Electric Vehicles Using Reinforcement Learning","W. Lee; H. Jeoung; D. Park; T. Kim; H. Lee; N. Kim","Department of Mechanical Engineering, Hanyang University, Ansan, Republic of Korea; Department of Mechanical Engineering, Hanyang University, Ansan, Republic of Korea; Department of Mechanical Engineering, Hanyang University, Ansan, Republic of Korea; Department of Mechanical Engineering, Hanyang University, Ansan, Republic of Korea; Department of Mechanical Engineering, Seoul National University, Seoul, Republic of Korea; Department of Mechanical Engineering, Hanyang University, Ansan, Republic of Korea","IEEE Access","20 May 2021","2021","9","","72759","72768","Equivalent Consumption Management Strategy (ECMS), a representative energy management strategy for hybrid electric vehicles (HEVs) derived from Pontryagin's minimum principle, is known to produce a near-optimal solution if the costate or equivalent factor of electric use is appropriately determined according to the driving conditions. One problem when applying the control concept to real-world scenarios is that it is difficult to precisely evaluate the performance of the control parameter before driving is complete, so the costate cannot be determined properly. To address this issue, this study proposes a practical method for estimating an appropriate costate based on Deep Q-Networks (DQNs), which is a reinforcement learning algorithm that uses a Deep Neural Network to evaluate the performances and determine the best control parameter or costate. The control concept benefits vehicle energy management by selecting the control parameter most related to stochastic conditions or future driving information based on artificial intelligence (AI), while optimal control is deterministically conducted by ECMS if the control parameter is given. Simply, only the implicit part of the optimal controller is solved via artificial intelligence. In the simulation results, not only does the proposed control concept outperform an existing ECMS that uses an adaptive technique for determining the costate, but the concept is also very feasible, in that it does not need a model for evaluating the performances.","2169-3536","","10.1109/ACCESS.2021.3079903","Technology Innovation Program (Development of application technologies for heavy duty fuel cell electric trucks using multi-input motor based 400kW class multi speed electrified powertrain system) through the Ministry of Trade, Industry and Energy (MOTIE), South Korea(grant numbers:20011834); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9430498","Energy management strategy;adaptive ECMS;machine learning;reinforcement learning;hybrid electric vehicles;deep Q-learning;optimal control","Electronic countermeasures;Energy management;Batteries;Reinforcement learning;Engines;Fuels;State of charge","energy management systems;hybrid electric vehicles;learning (artificial intelligence);neural nets;optimal control","Pontryagin's minimum principle;near-optimal solution;equivalent factor;driving conditions;control parameter;appropriate costate;reinforcement learning algorithm;Deep Neural Network;control concept benefits vehicle energy management;future driving information;artificial intelligence;optimal control;ECMS;optimal controller;real-time intelligent energy Management Strategy;hybrid electric vehicles;Equivalent Consumption Management Strategy;representative energy management strategy","","19","","37","CCBY","13 May 2021","","","IEEE","IEEE Journals"
"Optimal Power Allocation for Rate Splitting Communications With Deep Reinforcement Learning","N. Q. Hieu; D. T. Hoang; D. Niyato; D. I. Kim","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea","IEEE Wireless Communications Letters","8 Dec 2021","2021","10","12","2820","2823","This letter introduces a novel framework to optimize the power allocation for users in a Rate Splitting Multiple Access (RSMA) network. In the network, messages intended for users are split into different parts that are a single common part and respective private parts. This mechanism enables RSMA to flexibly manage interference and thus enhance energy and spectral efficiency. Although possessing outstanding advantages, optimizing power allocation in RSMA is very challenging under the uncertainty of the communication channel and the transmitter has limited knowledge of the channel information. To solve the problem, we first develop a Markov Decision Process framework to model the dynamic of the communication channel. The deep reinforcement algorithm is then proposed to find the optimal power allocation policy for the transmitter without requiring any prior information of the channel. The simulation results show that the proposed scheme can outperform baseline schemes in terms of average sum-rate under different power and QoS requirements.","2162-2345","","10.1109/LWC.2021.3118441","National Research Foundation (NRF), Singapore, funded under Energy Research Test-Bed and Industry Partnership Funding Initiative, part of the Energy Grid (EG) 2.0 Programme, Alibaba Group through Alibaba Innovative Research (AIR) Program and Alibaba–NTU Singapore Joint Research Institute (JRI); National Research Foundation, Singapore, through the AI Singapore Programme (AISG)(grant numbers:AISG2-RP-2020-019); WASP/NTU(grant numbers:M4082187 (4080)); Singapore Ministry of Education (MOE) Tier 1(grant numbers:RG16/20); National Research Foundation of Korea (NRF) Grant funded by the Korean Government (MSIT)(grant numbers:2021R1A2C2007638); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562976","Rate splitting;multiple access;deep reinforcement learning;proximal policy optimization;MDP","Resource management;Transmitters;Quality of service;Interference;Signal to noise ratio;Optimization;Dynamic scheduling","deep learning (artificial intelligence);energy conservation;Markov processes;multi-access systems;quality of service;radio transmitters;radiofrequency interference;reinforcement learning;telecommunication computing;telecommunication power management","communication channel;deep reinforcement algorithm;optimal power allocation policy;average sum-rate;Rate Splitting communications;deep reinforcement learning;Rate Splitting Multiple Access network;RSMA;single common part;spectral efficiency;channel information;Markov Decision Process framework","","18","","8","IEEE","7 Oct 2021","","","IEEE","IEEE Journals"
"Adaptive Decision-Making for Automated Vehicles Under Roundabout Scenarios Using Optimization Embedded Reinforcement Learning","Y. Zhang; B. Gao; L. Guo; H. Guo; H. Chen","State Key Laboratory of Automotive Simulation and Control, Jilin University, Changchun, China; State Key Laboratory of Automotive Simulation and Control, Jilin University, Changchun, China; Center for Cyber-Physical Systems, University of Georgia, Athens, GA, USA; Department of Control Science and Engineering, Jilin University, Changchun, China; State Key Laboratory of Automotive Simulation and Control, Jilin University, Changchun, China","IEEE Transactions on Neural Networks and Learning Systems","30 Nov 2021","2021","32","12","5526","5538","The roundabout is a typical changeable, interactive scenario in which automated vehicles should make adaptive and safe decisions. In this article, an optimization embedded reinforcement learning (OERL) is proposed to achieve adaptive decision-making under the roundabout. The promotion is the modified actor of the Actor–Critic framework, which embeds the model-based optimization method in reinforcement learning to explore continuous behaviors in action space directly. Therefore, the proposed method can determine the macroscale behavior (change lane or not) and medium-scale behaviors of desired acceleration and action time simultaneously with high sample efficiency. When scenarios change, medium-scale behaviors can be adjusted timely by the embedded direct search method, promoting the adaptability of decision-making. More notably, the modified actor matches human drivers’ behaviors, macroscale behavior captures the human mind’s jump, and medium-scale behaviors are preferentially adjusted through driving skills. To enable the agent adapts to different types of the roundabout, task representation is designed to restructure the policy network. In experiments, the algorithm efficiency and the learned driving strategy are compared with decision-making containing macroscale behavior and constant medium-scale behaviors of the desired acceleration and action time. To investigate the adaptability, the performance under an untrained type of roundabout and two more dangerous situations are simulated to verify that the proposed method changes the decisions with changeable scenarios accordingly. The results show that the proposed method has high algorithm efficiency and better system performance.","2162-2388","","10.1109/TNNLS.2020.3042981","National Nature Science Foundation of China(grant numbers:61790564,U19A2069); Research Team of Optimization and Control of Automotive Powertrain, Jilin Provincial Science and Technology Department(grant numbers:20200301011RQ); China Automobile Industry Innovation and Development Joint Fund(grant numbers:U1664257); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9311168","Decision-making;direct search;parameterization;reinforcement learning (RL)","Decision making;Adaptation models;Acceleration;Automotive engineering;Space vehicles;Reinforcement learning","decision making;driver information systems;learning (artificial intelligence);mobile robots;optimisation;road traffic","adaptive decision-making;automated vehicles;roundabout scenarios;optimization embedded reinforcement learning;interactive scenario;modified actor;actor-critic framework;model-based optimization method;continuous behaviors;action space;desired acceleration;action time;high sample efficiency;scenarios change;embedded direct search method;adaptability;human drivers;macroscale behavior captures;learned driving strategy;constant medium-scale behaviors;changeable scenarios","","17","","37","IEEE","30 Dec 2020","","","IEEE","IEEE Journals"
"DDoS Mitigation Based on Space-Time Flow Regularities in IoV: A Feature Adaption Reinforcement Learning Approach","Z. Li; Y. Kong; C. Wang; C. Jiang","College of Information Science and Technology, Donghua University, Shanghai, China; College of Information Science and Technology, Donghua University, Shanghai, China; Department of Computer Science, Key Laboratory of the Ministry of Education for Embedded System and Service Computing, Tongji University, Shanghai, China; Department of Computer Science, Key Laboratory of the Ministry of Education for Embedded System and Service Computing, Tongji University, Shanghai, China","IEEE Transactions on Intelligent Transportation Systems","10 Mar 2022","2022","23","3","2262","2278","With the development of 5G technology, mobile edge computing (MEC) is introduced into the construction of internet of vehicles (IoV). However, the distributed denial of services (DDoS) attacks become a serious problem in IoV under MEC. Although numbers of studies have been done on DDoS detection in common wired or wireless networks, they cannot satisfy the high dynamic requirement and cannot cope with the complex and diverse DDoS attacks in IoV. Fortunately, the data traffic flows in IoV exist potential and predictable space-time regularities. By employing reinforcement learning, we propose a feature adaption reinforcement learning approach based on the space-time flow regularities in IoV for DDoS mitigation, named FAST. In FAST, we elaborately design a combinational action space, and a reward function based on Kalman filter method and historical data traffic flows, which can make FAST to recognize DDoS attacks more quickly and accurately. Then through combining Q-learning and DDQN, FAST can select features and disconnect DDoS attacks adaptively according to the changes of the environment. In experiments, we evaluate the performance of FAST based on Shenzhen taxicab dataset. We simulate and inject DDoS attacks into Shenzhen taxicabs through two DDoS simulation tools named ‘ddosflowgen’ and ‘hping3’. The experimental results show that FAST has a high quality in detecting multiple types of DDoS attacks compared with other detection methods.","1558-0016","","10.1109/TITS.2021.3066404","National Natural Science Foundation of China(grant numbers:61972080); Shanghai Rising-Star Program(grant numbers:19QA1400300); National Key Research and Development Project(grant numbers:2018YFB2100801); Major Project of the Ministry of Industry and Information Technology of China(grant numbers:TC200H01J); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9408414","Internet of Vehicles;reinforcement learning;feature adaption;DDoS mitigation","Computer crime;Denial-of-service attack;Reinforcement learning;Feature extraction;Base stations;Unsupervised learning;Heuristic algorithms","5G mobile communication;Kalman filters;mobile computing;reinforcement learning;telecommunication security;telecommunication traffic;vehicular ad hoc networks","DDoS mitigation;space-time flow regularities;IoV;feature adaption reinforcement learning approach;mobile edge computing;MEC;DDoS detection;common wired networks;wireless networks;complex DDoS attacks;diverse DDoS attacks;potential space-time regularities;predictable space-time regularities;FAST;combinational action space;historical data traffic flows;disconnect DDoS attacks;DDoS simulation tools","","8","","35","IEEE","19 Apr 2021","","","IEEE","IEEE Journals"
"Optimizing Energy Efficiency for Data Center via Parameterized Deep Reinforcement Learning","Y. Ran; H. Hu; Y. Wen; X. Zhou","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Communications and Electronics, Jiangxi Science and Technology Normal University, Nanchang, Jiangxi, China","IEEE Transactions on Services Computing","7 Apr 2023","2023","16","2","1310","1323","The rapid advancements in cloud computing, Big Data and their related applications have led to a skyrocketing increase in data center energy consumption year by year. The prior approaches for improving data center energy efficiency mostly suffer from high system dynamics or the complexity of data centers. In this paper, we propose an optimization framework based on deep reinforcement learning, named DeepEE, to jointly optimize energy consumption from the perspectives of task scheduling and cooling control. In DeepEE, a PArameterized action space based Deep Q-Network (PADQN) algorithm is proposed to tackle the hybrid action space problem. Then, a dynamic time factor mechanism for adjusting cooling control interval is introduced into PADQN (PADQN-D) to achieve more accurate and efficient coordination of IT and cooling subsystems. Finally, in order to train and evaluate the proposed algorithms safely and quickly, a simulation platform is built to model the dynamics of IT and cooling subsystems. Extensive real-trace based experiments illustrate that: 1) the proposed PADQN algorithm can save up to 15% and 10% energy consumption compared with the baseline siloed and joint optimization approaches respectively; 2) the proposed PADQN-D algorithm with dynamic cooling control interval can better adapt to the change of IT workload; 3) our proposed algorithms achieve more stable performance gain in terms of power consumption by adopting the parameterized action space.","1939-1374","","10.1109/TSC.2022.3184835","National Research Foundation Singapore; Energy Research Test-Bed and Industry Partnership Funding Initiative; National Research Foundation(grant numbers:NRF2020NRF-CG001-027); NTUitive Gap; National Natural Science Foundation of China(grant numbers:62003067,61971457); National Key Research and Development Program of China(grant numbers:2021YFC3300200); Education Department Foundation of Jiangxi Province(grant numbers:GJJ211111); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9801653","Deep reinforcement learning;data center;energy efficiency;task scheduling;cooling control","Cooling;Data centers;Task analysis;Aerospace electronics;Heuristic algorithms;Optimization;Servers","Big Data;cloud computing;computer centres;cooling;deep learning (artificial intelligence);energy conservation;energy consumption;power aware computing;reinforcement learning;scheduling","Big Data;cloud computing;cooling subsystems;data center energy consumption year;data center energy efficiency;data centers;Deep Q-Network algorithm;DeepEE;dynamic cooling control interval;dynamic time factor mechanism;energy consumption;extensive real-trace based experiments;hybrid action space problem;joint optimization;PADQN algorithm;PADQN-D algorithm;parameterized action space;parameterized deep reinforcement learning;power consumption;system dynamics","","6","","40","IEEE","20 Jun 2022","","","IEEE","IEEE Journals"
"Proactive Content Caching Based on Actor–Critic Reinforcement Learning for Mobile Edge Networks","W. Jiang; D. Feng; Y. Sun; G. Feng; Z. Wang; X. -G. Xia","Shenzhen Key Laboratory of Digital Creative Technology, Guangdong Province Engineering Laboratory for Digital Creative Technology, Guangdong Key Laboratory of Intelligent Information Processing, College of Electronics and Information Engineering, Shenzhen University, Shenzhen, China; Shenzhen Key Laboratory of Digital Creative Technology, Guangdong Province Engineering Laboratory for Digital Creative Technology, Guangdong Key Laboratory of Intelligent Information Processing, College of Electronics and Information Engineering, Shenzhen University, Shenzhen, China; James Watt School of Engineering, University of Glasgow, Glasgow, U.K.; Yangtze Delta Region Institute (Huzhou), University of Electronic Science and Technology of China, Huzhou, China; Technical Management Center, China Media Group, Beijing, China; Department of Electrical and Computer Engineering, University of Delaware, Newark, DE, USA","IEEE Transactions on Cognitive Communications and Networking","8 Jun 2022","2022","8","2","1239","1252","Mobile edge caching/computing (MEC) has emerged as a promising approach for addressing the drastic increasing mobile data traffic by bringing high caching and computing capabilities to the edge of networks. Under MEC architecture, content providers (CPs) are allowed to lease some virtual machines (VMs) at MEC servers to proactively cache popular contents for improving users’ quality of experience. The scalable cache resource model rises the challenge for determining the ideal number of leased VMs for CPs to obtain the minimum expected downloading delay of users at the lowest caching cost. To address these challenges, in this paper, we propose an actor-critic (AC) reinforcement learning based proactive caching policy for mobile edge networks without the prior knowledge of users’ content demand. Specifically, we formulate the proactive caching problem under dynamical users’ content demand as a Markov decision process and propose a AC based caching algorithm to minimize the caching cost and the expected downloading delay. Particularly, to reduce the computational complexity, a branching neural network is employed to approximate the policy function in the actor part. Numerical results show that the proposed caching algorithm can significantly reduce the total cost and the average downloading delay when compared with other popular algorithms.","2332-7731","","10.1109/TCCN.2021.3130995","National Key Research and Development Program of China(grant numbers:2020YFB1807601); Innovation Project of Guangdong Educational Department(grant numbers:2019KTSCX147); Shenzhen Science and Technology Program(grant numbers:JCYJ20210324095209025); ZTE Industry-Academia-Research Cooperation Funds; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9627729","Actor-critic algorithm;branching neural network;reinforcement learning;mobile edge caching","Costs;Delays;Reinforcement learning;Quality of experience;Cloud computing;Servers;Heuristic algorithms","cache storage;Markov processes;mobile computing;mobile radio;neural nets;quality of experience;reinforcement learning;telecommunication traffic","lowest caching cost;actor-critic reinforcement;proactive caching policy;mobile edge networks;proactive caching problem;dynamical users;caching algorithm;computational complexity;branching neural network;proactive content caching;actor-critic reinforcement learning;mobile data traffic;high caching;MEC architecture;content providers;MEC servers;scalable cache resource model;leased VMs;minimum expected downloading delay","","5","","45","IEEE","26 Nov 2021","","","IEEE","IEEE Journals"
"Optimizing Data Center Energy Efficiency via Event-Driven Deep Reinforcement Learning","Y. Ran; X. Zhou; H. Hu; Y. Wen","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Communications and Electronics, Jiangxi Science and Technology Normal University, Nanchang, Jiangxi, China; School of Information and Electronics, Beijing Institute of Technolog, Beijing, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Services Computing","7 Apr 2023","2023","16","2","1296","1309","To reduce the skyrocketing energy consumption of data centers, the prevailing approaches adopt the time-driven manner to control IT and cooling subsystems. These methods suffer from highly dynamic system states, complex action spaces and the risk of instability caused by frequent and unnecessary control operations. To tackle these problems, we propose a novel event-driven control paradigm and an optimization algorithm, under the deep reinforcement learning (DRL) framework. The principle is to make decisions based on certain critical events (e.g., overheating), rather than fixed periodic control. Specifically, we design an event-driven optimization framework to trigger control operations. Then, we present several models to describe IT and cooling subsystems, and mathematically define events to capture four types of prior factors that impact system performance. Furthermore, we develop an event-driven DRL (E-DRL) optimization algorithm to dispatch jobs and regulate cooling facilities for energy efficiency. Using two different types of real workload traces, we conduct extensive experiments to demonstrate that: 1) E-DRL reduces the number of regulating decisions by 70%$\sim$∼95% while achieving a comparable or even better energy efficiency in comparison with the state-of-the-art algorithm; and 2) E-DRL can adapt the control frequency to the changing operational conditions and diverse workloads.","1939-1374","","10.1109/TSC.2022.3157145","National Research Foundation Singapore; Energy Research Test-Bed and Industry Partnership Funding Initiative; Energy Grid (EG) 2.0 Programme; National Research Foundation(grant numbers:NRF2020NRF-CG001-027); NTUitive Gap Fund; National Natural Science Foundation of China(grant numbers:62003067,61971457); National Key Research and Development Program of China(grant numbers:2021YFC3300200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9729602","Data center;deep reinforcement learning;energy efficiency;event-driven optimization","Cooling;Data centers;Optimization;Task analysis;Aerospace electronics;Computational modeling;Atmospheric modeling","computer centres;deep learning (artificial intelligence);energy conservation;energy consumption;optimisation;power aware computing;reinforcement learning","complex action;control frequency;cooling facilities;data centers;deep reinforcement learning framework;E-DRL;event-driven control paradigm;event-driven deep reinforcement;event-driven DRL optimization algorithm;event-driven optimization framework;fixed periodic control;frequent control operations;highly dynamic system states;optimizing data center energy efficiency;prevailing approaches;prior factors that impact system performance;skyrocketing energy consumption;state-of-the-art algorithm;time-driven manner;unnecessary control operations","","5","","42","IEEE","7 Mar 2022","","","IEEE","IEEE Journals"
"Joint Optimization of Trajectory and User Association via Reinforcement Learning for UAV-Aided Data Collection in Wireless Networks","G. Chen; X. B. Zhai; C. Li","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Electronics and Communication Engineering, Sun Yat-sen University, Shenzhen, China","IEEE Transactions on Wireless Communications","9 May 2023","2023","22","5","3128","3143","Unmanned Aerial Vehicles (UAVs) can be used as aerial base stations for data collection in next-generation wireless networks due to their high adaptability and maneuverability. This paper investigates the scenario where multiple UAVs cooperatively fly over heterogeneous ground users (GUs) and collect data without a central controller. With the consideration of signal-to-interference-and-noise ratio (SINR) and fairness among users, we jointly optimize the trajectories of UAVs and the GUs associations to maximize the total throughput and energy efficiency. We formulate the long-term optimization problem as a decentralized partially observed Markov decision processes (DEC-POMDP) and derive an approach combining the coalition formation game (CFG) and multi-agent deep reinforcement learning (MADRL). We first formulate the discrete association scheduling problem as a non-cooperative theoretical game and use the CFG algorithm to achieve a decentralized scheme converging to Nash equilibrium (NE). Then, a MARL-based technique is developed to optimize the trajectories and energy consumption continuously in a centralized-training but decentralized-execution manner. Simulation results demonstrate that the proposed algorithm outperforms the commonly used schemes in the literature, regarding the fair throughput and energy consumption in a distributed manner.","1558-2248","","10.1109/TWC.2022.3216049","China National Key Research and Development Program during the 14th Five-year Plan Period(grant numbers:2022YFB2901600); Fund of Prospective Layout of Scientific Research for the Nanjing University of Aeronautics and Astronautics (NUAA); National Science Foundation of China (NSFC)(grant numbers:61901534,62271514,61701231); Foundations of Key Laboratory of Safety-Critical Software, NUAA, Ministry of Industry and Information Technology(grant numbers:NJ2020022); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2019B1515120032); Science, Technology and Innovation Commission of Shenzhen Municipality(grant numbers:JCYJ20210324120002007,JCYJ20190807155617099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931560","UAV trajectory design;fair throughputs;energy-efficiency;coalition formation games;multi-agent deep reinforcement learning","Trajectory;Optimization;Games;Throughput;Wireless networks;Resource management;Interference","aerospace communication;autonomous aerial vehicles;deep learning (artificial intelligence);energy consumption;game theory;learning (artificial intelligence);Markov processes;multi-agent systems;optimisation;reinforcement learning;telecommunication computing;telecommunication control;telecommunication scheduling","aerial base stations;central controller;centralized-training;CFG algorithm;DEC-POMDP;decentralized partially observed Markov decision;decentralized scheme;decentralized-execution manner;discrete association scheduling problem;energy consumption;energy efficiency;fair throughput;GUs associations;heterogeneous ground users;high adaptability;joint optimization;long-term optimization problem;maneuverability;MARL-based technique;multiple UAVs;next-generation wireless networks;noncooperative theoretical game;reinforcement learning;signal-to-interference;UAV-aided data collection;Unmanned Aerial Vehicles","","4","","39","IEEE","27 Oct 2022","","","IEEE","IEEE Journals"
"RAN Information-Assisted TCP Congestion Control Using Deep Reinforcement Learning With Reward Redistribution","M. Chen; R. Li; J. Crowcroft; J. Wu; Z. Zhao; H. Zhang","College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; Department of Computer Science, University of Cambridge, Cambridge, U.K.; Huawei Technologies Company, Ltd., Shanghai, China; Zhejiang Lab, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Communications","14 Jan 2022","2022","70","1","215","230","In this paper, we aim to propose a novel transmission control protocol (TCP) congestion control method from a cross-layer-based perspective and present a deep reinforcement learning (DRL)-driven method called DRL-3R (DRL for congestion control with Radio access network information and Reward Redistribution) so as to learn the TCP congestion control policy in a superior manner. In particular, we incorporate the RAN information to timely grasp the dynamics of RAN, and empower DRL to learn from the delayed RAN information feedback potentially induced by several consecutive actions. Meanwhile, we relax the implicit assumption (that the feedback to one specific action returns at a round-trip-time (RTT) after the action is applied) in previous researches, by redistributing the rewards and evaluating the merits of actions more accurately. Experiment results show that besides maintaining a reasonable fairness, DRL-3R significantly outperforms classical congestion control methods (e.g., TCP Reno, Westwood, Cubic, BBR and DRL-CC) on network utility by achieving a higher throughput while reducing delay in various network environments.","1558-0857","","10.1109/TCOMM.2021.3123130","National Key Research and Development Program of China(grant numbers:2020YFB1804800); National Natural Science Foundation of China(grant numbers:61731002,62071425); Zhejiang Key Research and Development Plan(grant numbers:2019C01002,2019C03131); Huawei Cooperation Project; a Project sponsored by the Zhejiang Lab(grant numbers:2019LC0AB01); a Project sponsored by the Ministry of Industry and Information Technology(grant numbers:2019-00891-2-1); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LY20F010016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585436","Deep reinforcement learning;congestion control;radio access network;reward redistribution;delayed feedback","Reinforcement learning;Servers;Internet;Throughput;Radio access networks;Bandwidth;5G mobile communication","learning (artificial intelligence);radio access networks;telecommunication congestion control;transport protocols","RAN information-assisted TCP congestion control;Reward Redistribution;novel transmission control protocol;congestion control method;cross-layer-based perspective;deep reinforcement learning-driven method;DRL-3R;Radio access network information;TCP congestion control policy;empower DRL;delayed RAN information feedback;consecutive actions;specific action;classical congestion control methods;TCP Reno;DRL-CC","","3","","33","IEEE","26 Oct 2021","","","IEEE","IEEE Journals"
"Distributed Deep Reinforcement Learning Assisted Resource Allocation Algorithm for Space-Air-Ground Integrated Networks","P. Zhang; Y. Li; N. Kumar; N. Chen; C. -H. Hsu; A. Barnawi","College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; Department of Computer Science and Engineering, Thapar Institute of Engineering and Technology (Deemed to be University), Patiala, India; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; Department of Computer Science and Information Engineering, Asia University, Taichung, Taiwan; Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia","IEEE Transactions on Network and Service Management","9 Oct 2023","2023","20","3","3348","3358","To realize the Interconnection of Everything (IoE) in the 6G vision, the space-based, air-based, and ground-based networks have shown a trend of integration. Compared with the traditional communications system, Space-Air-Ground Integrated Networks (SAGINs) can provide a seamless global network connection, while making full use of different network characteristics for synergy and complementarity. However, the increasing global coverage of the Internet, the growing number and variety of smart terminals, and the emergence of various high-bandwidth services have led to an explosion in communication data transmission. Despite the continuous development of communication technologies such as airborne processing and forwarding and high-throughput satellites, the quality of service (QoS) and quality of experience (QoE) for different users still cannot be guaranteed due to the power limitations of satellites and the scarcity of spectrum resources. In this work, drawing on wireless edge caching, considering that the relay of SAGIN has edge caching capability, the hot task is cached in the network nodes in advance. More, this process is optimized using distributed Deep Reinforcement Learning (DRL), thereby reducing transmission delay and relieving the pressure of task offloading on space-based networks. Compared with advanced related works, the long-term node utilization, link utilization, long-term average revenue-to-cost ratio and acceptance ratio of the proposed algorithm are increased by about 4.22%, 31.36%, 11.75% and 7.14%, respectively.","1932-4537","","10.1109/TNSM.2022.3232414","Shandong Provincial Natural Science Foundation, China(grant numbers:ZR2020MF006,ZR2022LZH015); Industry-university Research Innovation Foundation of Ministry of Education of China(grant numbers:2021FNA01001); Major Scientific and Technological Projects of CNPC(grant numbers:ZD2019-183-006); Open Foundation of State Key Laboratory of Integrated Services Networks (Xidian University)(grant numbers:ISN23-09); Open Foundation of State Key Laboratory of Networking and Switching Technology (Beijing University of Posts and Telecommunications)(grant numbers:SKLNST-2021-1-17); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9999560","Deep reinforcement learning;space-air-ground integrated networks;resource allocation;quality of service","Resource management;Satellites;Reinforcement learning;Market research;Handover;Delays;Ad hoc networks","","","","3","","36","IEEE","27 Dec 2022","","","IEEE","IEEE Journals"
"Hierarchical Multi-Agent Deep Reinforcement Learning for Energy-Efficient Hybrid Computation Offloading","H. Zhou; Y. Long; S. Gong; K. Zhu; D. T. Hoang; D. Niyato","School of Intelligent Systems Engineering, Shenzhen Campus of Sun Yat-Sen University, Shenzhen, China; School of Intelligent Systems Engineering, Shenzhen Campus of Sun Yat-Sen University, Shenzhen, China; School of Intelligent Systems Engineering, Shenzhen Campus of Sun Yat-Sen University, Shenzhen, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Vehicular Technology","13 Jan 2023","2023","72","1","986","1001","Mobile edge computing (MEC) provides an economical way for the resource-constrained edge users to offload computational workload to MEC servers co-located with the access point (AP). In this article, we consider a hybrid computation offloading scheme that allows edge users to offload workloads by using active RF communications and backscatter communications. We aim to maximize the overall energy efficiency subject to the completion of all workload by jointly optimizing the AP's beamforming and the users' offloading decisions. Considering a dynamic environment, we propose a hierarchical multi-agent deep reinforcement learning (H-MADRL) framework to solve this problem. The high-level agent resides in the AP and optimizes the beamforming strategy, while the low-level user agents learn and adapt individuals' offloading strategies. To further improve the learning efficiency, we propose a novel optimization-driven learning algorithm that allows the AP to estimate the low-level users' actions by solving approximate optimization problem efficiently. Then, the action estimation can be shared with all users and drive them to update individuals' actions independently. Simulation results reveal that our algorithm can improve the system performance by 50%. The learning efficiency and reliability are also improved significantly comparing to the model-free learning methods.","1939-9359","","10.1109/TVT.2022.3202525","National Natural Science Foundation of China(grant numbers:61972434); Shenzhen Fundamental Research Program(grant numbers:JCYJ20190807154009444); Australian Research Council(grant numbers:DE210100651); National Research Foundation; Info-communications Media Development Authority; Future Communications Research and Development Programme; AI Singapore Programme(grant numbers:AISG2-RP-2020-019); Energy Research Test-Bed and Industry Partnership Funding Initiative; Energy Grid (EG) 2.0 Programme; DesCartes and the Campus for Research Excellence and Technological Enterprise; Alibaba Group; Alibaba Innovative Research; Alibaba-NTU Singapore Joint Research Institute; Singapore Ministry of Education Tier 1 (RG16/20); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869750","Backscatter communications;mobile edge computing;multi-agent deep reinforcement learning","Servers;Internet of Things;Resource management;Array signal processing;Optimization;Backscatter;Wireless communication","deep learning (artificial intelligence);edge computing;energy conservation;mobile computing;multi-agent systems;optimisation;reinforcement learning","access point;approximate optimization problem;backscatter communications;beamforming strategy;computational workload;energy-efficient hybrid computation offloading;hierarchical multiagent deep reinforcement learning;high-level agent;low-level user agents;MEC;mobile edge computing;model-free learning;resource-constrained edge users","","2","","42","IEEE","29 Aug 2022","","","IEEE","IEEE Journals"
"Adaptive Task Offloading in Coded Edge Computing: A Deep Reinforcement Learning Approach","N. Van Tam; N. Q. Hieu; N. T. Thanh Van; N. C. Luong; D. Niyato; D. I. Kim","Faculty of Computer Science, Phenikaa University, Hanoi, Vietnam; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Faculty of Electrical and Electronic Engineering, Phenikaa University, Hanoi, Vietnam; Faculty of Computer Science, Phenikaa University, Hanoi, Vietnam; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea","IEEE Communications Letters","10 Dec 2021","2021","25","12","3878","3882","In this letter, we consider a Coded Edge Computing (CEC) network in which a client encodes its computation subtasks using the Maximum Distance Separable (MDS) code before offloading them to helpers. The CEC network is heterogeneous in which the helpers are different in computing capacity, wireless communication stability, and computing price. Thus, the client needs to determine a desirable size of MDS-coded subtasks and selects proper helpers such that the computation latency is within the deadline and the incentive cost is minimal. This problem is challenging since the helpers are generally dynamic and random in the computing, communication, and computing price. We thus propose to adopt a Deep Reinforcement Learning (DRL) algorithm that allows the client to learn and find optimal decisions without any prior knowledge of network environments. The experiment results reveal that the proposed algorithm outperforms the standard Q-learning and baseline algorithms in both terms of computation latency and incentive cost.","1558-2558","","10.1109/LCOMM.2021.3116036","National Research Foundation (NRF), Singapore, funded under Energy Research Test-Bed and Industry Partnership Funding Initiative; Energy Grid (EG) 2.0 programme, Alibaba Group through Alibaba Innovative Research (AIR) Program and Alibaba-NTU Singapore Joint Research Institute (JRI); National Research Foundation, Singapore under the AI Singapore Programme (AISG) (AISG2-RP-2020-019), WASP/NTU grant M4082187 (4080) and Singapore Ministry of Education (MOE) Tier 1 (RG16/20), and the MSIT (Ministry of Science and ICT), Korea, under the ICT Creative Consilience program (IITP-2020-0-01821) supervised by the IITP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551283","Maximum distance separable code;coded edge computing;deep reinforcement learning","Task analysis;Costs;Reinforcement learning;Codes;Edge computing;Partitioning algorithms;Optimization","deep learning (artificial intelligence);distributed processing;optimisation;reinforcement learning","adaptive task offloading;coded edge computing;maximum distance separable code;MDS;CEC network;deep reinforcement learning","","2","","9","IEEE","28 Sep 2021","","","IEEE","IEEE Journals"
"Decoupled Association with Rate Splitting Multiple Access in UAV-assisted Cellular Networks Using Multi-agent Deep Reinforcement Learning","J. Ji; L. Cai; K. Zhu; D. Niyato","College of Future Science and Engineering, Soochow University, Suzhou, China; Department of Electrical and Computer Engineering, University of Victoria, BC, Canada; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Mobile Computing","","2023","PP","99","1","15","In unmanned aerial vehicles (UAVs) assisted cellular networks, user association plays an important role in interference control and spectrum efficiency. In this paper, we study the performance of uplink-downlink decoupled (UDDe) user association in a multi-UAV assisted network in which each user can associate with different UAVs or the macro base station (MBS) for uplink (UL) and downlink (DL) transmissions. Since some popular data may be requested by multiple users, grouping these users and applying multicasting can significantly improve spectral efficiency. Unlike traditional linear precoding that treats interference entirely as noise, we propose a rate-splitting multiple access (RSMA) policy that employs rate splitting at the transmitter and successive interference cancellation (SIC) at the receiver. To be specific, the transmitted signal is split into a common part and a private part, and the interference is partially decoded and partially treated as noise. In this context, we formulate a joint optimization problem of UL-DL association and beamforming for maximizing the sum-rate of users in UL and that of multicast groups in DL under the constraints of UAV backhaul capacity and power budget. Since the formulated problem is non-convex with intricate states and an individual UAV may not know the rewards of other UAVs, we convert it into a robust partially observable Markov decision process (POMDP). Then we resort to multi-agent deep reinforcement learning (MADRL) that enables each UAV to learn and optimize its policy in a distributed manner. To achieve an optimal policy, we further propose an improved clip and count-based proximal policy optimization (PPO) algorithm to train actor and critic networks. Simulation results demonstrate the superiority of the proposed decoupled association strategy with RSMA and the MADRL learning algorithm.","1558-0660","","10.1109/TMC.2023.3256404","National Natural Science Foundation of China(grant numbers:62071230); National Research Foundation (NRF), Singapore and Infocomm Media Development Authority under the Future Communications Research Development Programme (FCP); DSO National Laboratories under the AI Singapore Programme(grant numbers:AISG2-RP-2020-019); Energy Research Test-Bed and Industry Partnership Funding Initiative; Energy Grid 2.0 programme under DesCartes and the Campus for Research Excellence and Technological Enterprise (CREATE) programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071986","UAV-assisted networks;rate splitting;decoupled multiple association;multi-agent deep reinforcement learning","Array signal processing;Uplink;Cellular networks;NOMA;Interference cancellation;Backhaul networks;Reinforcement learning","","","","2","","","IEEE","15 Mar 2023","","","IEEE","IEEE Early Access Articles"
"Reinforcement Learning Assisted Bandwidth Aware Virtual Network Resource Allocation","P. Zhang; Y. Su; J. Wang; C. Jiang; C. -H. Hsu; S. Shen","College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; School of Cyber Science and Technology, Beihang University, Beijing, China; Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Information Engineering, Asia University, Taichung, Taiwan; Department of Computer Science and Engineering, Shaoxing University, Shaoxing, China","IEEE Transactions on Network and Service Management","1 Feb 2023","2022","19","4","4111","4123","Space-air-ground integration to support seamless coverage of ground, satellite, airborne, and marine communications, is likely to be a key trend in the 6G era. One of several key challenges in such space-air-ground integration networks (SAGINs) is to design efficient scheduling approaches for multi-dimension network resources. Due to the inherent heterogeneity characteristics, we demonstrate how can transform the network resource allocation problem in SAGINs into a multi-domain virtual network resource allocation problem, as well as proposing a reinforcement learning assisted bandwidth aware virtual network resource allocation algorithm (RL-BA-VNA). Specifically, RL-BA-VNA leverages reinforcement learning and uses a policy network as an agent to perform the node embedding. In order to support users’ exacting bandwidth requirements, we prefer to select virtual network requests with large bandwidth for embedding. Experiment findings show that the proposed algorithm RL-BA-VNA outperforms respectively the other three conventional virtual network resource allocation algorithms RL, DRL and BASELINE by an average of 2.06%, 4.93%, 11.07% in terms of long-term average reward, acceptance rate, and long term reward/cost.","1932-4537","","10.1109/TNSM.2022.3199471","National Key Research and Development Program of China(grant numbers:2020YFB1804800); Shandong Provincial Natural Science Foundation, China(grant numbers:ZR2020MF006); National Natural Science Foundation of China(grant numbers:61922050,62071268); Young Elite Scientist Sponsorship Program by CAST(grant numbers:2020QNRC001); Industry-university Research Innovation Foundation of Ministry of Education of China(grant numbers:2021FNA01001); Major Scientific and Technological Projects of CNPC(grant numbers:ZD2019-183-006); Open Foundation of State Key Laboratory of Integrated Services Networks (Xidian University)(grant numbers:ISN23-09); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LZ22F020002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858867","Network resource allocation;reinforcement learning;bandwidth requirement;space-air-ground integrated network","Resource management;Bandwidth;Substrates;Heuristic algorithms;6G mobile communication;Satellites;Reinforcement learning","learning (artificial intelligence);resource allocation;scheduling;virtualisation","algorithm RL-BA-VNA;bandwidth aware virtual network resource allocation algorithm;efficient scheduling approaches;inherent heterogeneity characteristics;key trend;multidimension network resources;multidomain virtual network resource allocation problem;policy network;RL-BA-VNA leverages reinforcement learning;SAGINs;space-air-ground integration networks;users;virtual network requests","","2","","48","IEEE","17 Aug 2022","","","IEEE","IEEE Journals"
"DPRL: Task Offloading Strategy Based on Differential Privacy and Reinforcement Learning in Edge Computing","P. Zhang; P. Gan; L. Chang; W. Wen; M. Selvi; G. Kibalya","College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; Research Institute of Petroleum Exploration and Development, PetroChina Tarim Oilfield Company, Korla, China; School of Computer Science and Cyber Engineering, Guangzhou University, Guangzhou, China; School of Computer Science and Engineering, Vellore Institute of Technology, Vellore, India; Department of Network Engineering, Technical University of Catalonia (UPC), Barcelona, Spain","IEEE Access","25 May 2022","2022","10","","54002","54011","Mobile edge computing has been widely used in various IoT devices due to its excellent computing power and good interaction speed. Task offloading is the core of mobile edge computing. However, most of the existing task offloading strategies only focus on improving the unilateral performance of MEC, such as security, delay, and overhead. Therefore, focus on the security, delay and overhead of MEC, we propose a task offloading strategy based on differential privacy and reinforcement learning. This strategy optimizes the overhead required for the task offloading process while protecting user privacy. Specifically, before task offloading, differential privacy is used to interfere with the user’s location information to avoid malicious edge servers from stealing user privacy. Then, on the basis of ensuring user privacy and security, combined with the resource environment of the MEC network, reinforcement learning is used to select appropriate edge servers for task offloading. Simulation results show that our scheme improves the performance of MEC in many aspects, especially in security and resource consumption. Compared with the typical privacy protection scheme, the security is improved by 7%, and the resource consumption is reduced by 9% compared with the typical task offloading strategy.","2169-3536","","10.1109/ACCESS.2022.3175194","Shandong Provincial Natural Science Foundation, China(grant numbers:ZR2020MF006); Industry-University Research Innovation Foundation of Ministry of Education of China(grant numbers:2021FNA01001,2021FNA01005); Major Scientific and Technological Projects of the China National Petroleum Corp. (CNPC)(grant numbers:ZD2019-183-006); Open Foundation of State Key Laboratory of Integrated Services Networks, Xidian University(grant numbers:ISN23-09); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9775102","Mobile edge computing;task offloading;differential privacy;reinforcement learning","Task analysis;Privacy;Servers;Optimization;Reinforcement learning;Differential privacy;Delays","cloud computing;data privacy;Internet of Things;mobile computing","typical task offloading strategy;differential privacy;reinforcement learning;mobile edge computing;excellent computing power;existing task offloading strategies;task offloading process;user privacy;malicious edge servers;appropriate edge servers;typical privacy protection scheme","","1","","34","CCBY","16 May 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Channel Estimation in RIS-Aided Wireless Networks","K. Kim; Y. K. Tun; M. S. Munir; W. Saad; C. S. Hong","Department of Computer Science and Engineering, Kyung Hee University, Yongin-si, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin-si, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin-si, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin-si, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin-si, South Korea","IEEE Communications Letters","11 Aug 2023","2023","27","8","2053","2057","Accurate channel estimation and allocation are vital in the provision of reconfigurable intelligent surfaces (RIS)-aided wireless network services to mobile users. Typically, channel estimation is carried out using a pilot signal. However, RIS elements cannot transmit or receive pilot signals because they are passive elements. Therefore, to maximize the gain of using the RIS, it is essential to accurately estimate a cascaded channel using a pilot signal between a base station (BS) and a terminal through an RIS. Moreover, although using a large number of pilot signals can guarantee accurate channel estimation performance, this can also drastically lower the wireless communication system’s efficiency. Thus, in this letter, a new paradigm for learning-based pilot allocation and channel estimation in RIS systems is proposed. A masked autoencoder (MAE) is trained to achieve high channel estimation accuracy with a limited number of pilots. Then, a deep reinforcement learning(DRL) agent learns pilot allocation policies through MAE. Simulation results show that the MAE channel estimator has almost the same channel estimation performance even though it uses up to 33% fewer pilots than the autoencoder (AE)-based channel estimator. Furthermore, the proposed DRL-based pilot optimization method achieves higher channel estimation performance with 20% fewer pilots than the general autoencoder and other learning algorithms without the proposed RL-based pilot optimization algorithm.","1558-2558","","10.1109/LCOMM.2023.3280821","National Research Foundation of Korea(NRF); Korea government(MSIT)(grant numbers:RS-2023-00207816); Institute of Information and Communications Technology Planning and Evaluation (IITP) Grant; Korea Government (MSIT) (Artificial Intelligence Innovation Hub)(grant numbers:2021-0-02068,RS-2022-00155911); Artificial Intelligence Convergence Innovation Human Resources Development (Kyung Hee University)(grant numbers:2019-0-01287); Evolvable Deep Learning Model Generation Platform for Edge Computing; Korea Institute of Energy Technology Evaluation and Planning(KETEP) and the Ministry of Trade, Industry & Energy(MOTIE) of the Republic of Korea(grant numbers:20209810400030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10137638","Reconfigurable intelligent surfaces;deep reinforcement learning;autoencoder;channel estimation","Channel estimation;Resource management;Computer science;Wireless networks;Training;Estimation;Base stations","channel estimation;deep learning (artificial intelligence);optimisation;reinforcement learning;telecommunication computing;wireless channels","autoencoder-based channel estimator;cascaded channel;channel estimation performance;deep reinforcement learning;DRL-based pilot optimization method;high channel estimation accuracy;higher channel estimation performance;learning-based pilot allocation;MAE channel estimator;masked autoencoder;mobile users;pilot allocation policies;pilot signal;reconfigurable intelligent surfaces;RIS elements;RIS systems;RIS-aided wireless networks;RL-based pilot optimization algorithm;wireless communication system;wireless network services","","1","","20","IEEE","29 May 2023","","","IEEE","IEEE Journals"
"Option-Based Multi-Agent Reinforcement Learning for Painting With Multiple Large-Sized Robots","X. Liu; G. Wang; K. Chen","State Key Laboratory of Tribology, Tsinghua University, Beijing, China; State Key Laboratory of Tribology, Tsinghua University, Beijing, China; State Key Laboratory of Tribology, Tsinghua University, Beijing, China","IEEE Transactions on Intelligent Transportation Systems","14 Sep 2022","2022","23","9","15707","15715","To ensure an efficient and conflict-free task allocation, this paper presents an option-based multi-agent reinforcement learning (OMARL) method for a cooperative multi-station multi-robot system in aircraft painting application. The problem can be described as a fully-connected graph, whose nodes and edges represent stations and trajectories between stations, respectively. The aim is to minimize the maximum execution time of conflict-free aircraft painting with multiple robots. Traditional methods cannot handle mutual collision avoidance well when robots are in large size. Therefore, this work solved the planning problem with multi-agent reinforcement learning algorithm based on option framework. The moving trajectories were pre-planned by a simplified 2D A* algorithm, which is fast and reduces rotations. The agents would finish the whole task with less time, and potential collisions were punished at every time step. Hierarchical option-based model with shared network parameters greatly reduced the complexity of training for painting planning scenarios. A fuzzy clustering method was introduced, and the clustering scores of stations would contribute to greedy agents. Comparisons were made on static and dynamic situations among several planning methods. OMARL can give optimal schedules as well as, or better than the most efficient traditional method does. Furthermore, OMARL can provide satisfactory adjustments in real time when practical process is different from plan or there is one broken robot. In contrast, traditional methods take lots of time to recalculate the plan. The results indicate that OMARL can achieve better performance in multi-station multi-robot task allocation when the collision between robots cannot be ignored.","1558-0016","","10.1109/TITS.2022.3145375","National Natural Science Foundation of China(grant numbers:51975308); Foundation of Ministry of Industry and Information Technology of China(grant numbers:MJ-2018-G-54); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9700783","Option framework;multi-agent reinforcement learning;aircraft painting;multiple mobile robots","Painting;Task analysis;Robots;Aircraft;Mobile robots;Trajectory;Job shop scheduling","collision avoidance;control engineering computing;graph theory;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;pattern clustering;scheduling","option-based multiagent reinforcement learning;multiple large-sized robots;efficient conflict-free task allocation;multistation multirobot system;aircraft painting application;conflict-free aircraft painting;multiple robots;hierarchical option-based model;painting planning scenarios;planning methods;OMARL;efficient traditional method;multistation multirobot task allocation","","1","","28","IEEE","1 Feb 2022","","","IEEE","IEEE Journals"
"Cooperative Multi-Agent Reinforcement Learning with Hierarchical Relation Graph under Partial Observability","Y. Li; X. Wang; J. Wang; W. Wang; X. Luo; S. Xie","School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer Engineering and Science, Shanghai University, Shanghai, China","2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)","24 Dec 2020","2020","","","1","8","Cooperation among agents with partial observation is an important task in multi-agent reinforcement learning (MARL), aiming to maximize a common reward. Most existing cooperative MARL approaches focus on building different model frameworks, such as centralized, decentralized, and centralized training with decentralized execution. These methods employ partial observation of agents as input directly, but rarely consider the local relationship between agents. The local relationship can help agents integrate observation information among different agents in a local range, and then adopt a more effective cooperation policy. In this paper, we propose a MARL method based on spatial relationship called hierarchical relation graph soft actor-critic (HRG-SAC). The method first uses a hierarchical relation graph generation module to represent the spatial relationship between agents in local space. Second, it integrates feature information of the relation graph through the graph convolution network (GCN). Finally, the soft actor-critic (SAC) is used to optimize agents' actions in training for compliance control. We conduct experiments on the Food Collector task and compare HRG-SAC with three baseline methods. The results demonstrate that the hierarchical relation graph can significantly improve MARL performance in the cooperative task.","2375-0197","978-1-7281-9228-4","10.1109/ICTAI50040.2020.00011","National Natural Science Foundation of China(grant numbers:61991415,91746203); Ministry of Industry and Information Technology(grant numbers:MC-201920-X01); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288352","Reinforcement Learning;Multi-Agent;Hierarchical Relation Graph","Training;Convolution;Conferences;Reinforcement learning;Tools;Task analysis;Observability","graph theory;learning (artificial intelligence);multi-agent systems;software agents","multiagent reinforcement learning;partial observability;partial observation;different model frameworks;centralized training;local relationship;observation information;local range;effective cooperation policy;MARL method;spatial relationship;hierarchical relation graph soft actor-critic;hierarchical relation graph generation module;local space;graph convolution network","","1","","33","IEEE","24 Dec 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Dynamic Dimensioning of Cloud Caches: A Restless Bandit Approach","G. Xiong; S. Wang; G. Yan; J. Li","Department of Electrical and Computer Engineering, Binghamton University, State University of New York, Binghamton, NY, USA; Department of Electrical and Computer Engineering, Binghamton University, State University of New York, Binghamton, NY, USA; Department of Electrical and Computer Engineering, Binghamton University, State University of New York, Binghamton, NY, USA; Department of Electrical and Computer Engineering, Binghamton University, State University of New York, Binghamton, NY, USA","IEEE/ACM Transactions on Networking","16 Oct 2023","2023","31","5","2147","2161","We study the dynamic cache dimensioning problem, where the objective is to decide how much storage to place in the cache to minimize the total costs with respect to the storage and content delivery latency. We formulate this problem as a Markov decision process, which turns out to be a restless multi-armed bandit problem and is provably hard to solve. For given dimensioning decisions, it is possible to develop solutions based on the celebrated Whittle index policy. However, Whittle index policy has not been studied for dynamic cache dimensioning, mainly because cache dimensioning needs to be repeatedly solved and jointly optimized with content caching. To overcome this difficulty, we propose a low-complexity fluid Whittle index policy, which jointly determines dimensioning and content caching. We show that this policy is asymptotically optimal. We further develop a lightweight reinforcement learning augmented algorithm dubbed fW-UCB when the content request and delivery rates are unavailable. fW-UCB is shown to achieve a sub-linear regret as it fully exploits the structure of the near-optimal fluid Whittle index policy and hence can be easily implemented. Extensive simulations using real traces support our theoretical results.","1558-2566","","10.1109/TNET.2023.3235480","National Science Foundation (NSF)(grant numbers:CRII-CNS-NeTS-2104880,RINGS-2148309); Office of the Under Secretary of Defense, Research and Engineering (OUSD R&E), National Institute of Standards and Technology (NIST), and industry partners as specified in the Resilient and Intelligent NextG Systems (RINGS) Program; U.S. Department of Energy(grant numbers:DE-EE0009341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021290","Cloud cache dimensioning;index policy;restless bandits;reinforcement learning","Indexes;Costs;Cloud computing;Reinforcement learning;Heuristic algorithms;Computational modeling;IEEE transactions","","","","1","","74","IEEE","18 Jan 2023","","","IEEE","IEEE Journals"
"A Seed Scheduling Method With a Reinforcement Learning for a Coverage Guided Fuzzing","G. Choi; S. Jeon; J. Cho; J. Moon","Department of Information Security, School of Cybersecurity, Korea University, Seoul, Republic of Korea; Department of Information Security, School of Cybersecurity, Korea University, Seoul, Republic of Korea; Department of Engineering, Computer Science, Lewis University, Romeoville, IL, USA; Department of Information Security, School of Cybersecurity, Korea University, Seoul, Republic of Korea","IEEE Access","10 Jan 2023","2023","11","","2048","2057","Seed scheduling, which determines which seed is input to the fuzzer first and the number of mutated test cases that are generated for the input seed, significantly influences crash detection performance in fuzz testing. Even for the same fuzzer, the performance in terms of detecting crashes that cause program failure varies considerably depending on the seed-scheduling method used. Most existing coverage-guided fuzzers use a heuristic seed-scheduling method. These heuristic methods can’t properly determine the seed with a high potential to cause the crash; thus, the fuzzer detects the crash inefficiently. Moreover, the fuzzer’s crash detection performance is affected by the characteristics of target programs. To address this problem, we propose a general-purpose reinforced seed-scheduling method that not only improves the crash detection performance of fuzz testing but also remains unaffected by the characteristics of the target program. The fuzzer with the proposed method detected the most crashes in all but one of the target programs in which crashes were detected in the experimental results conducted on various programs, and showed better crash detection efficiency than the comparison targets overall.","2169-3536","","10.1109/ACCESS.2022.3233875","Development of Industrial Technology for Electronic Components (Research on Constructing a Data Tree for the Federated Learning of Distributed Industrial Data) through the Ministry of Trade, Industry and Energy (MOTIE), South Korea(grant numbers:20018637); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10005088","Coverage-guided fuzzing;power schedule;reinforcement learning;seed scheduling;seed selection","Computer crashes;Fuzzing;Reinforcement learning;Scheduling;Computer security;Information security;Codes;Software testing;Computer bugs","program testing;reinforcement learning;scheduling","coverage guided fuzzing;crash detection performance;fuzz testing;general-purpose reinforced seed-scheduling method;heuristic seed-scheduling method;input seed;mutated test cases;program failure;seed scheduling method","","","","38","CCBYNCND","3 Jan 2023","","","IEEE","IEEE Journals"
"State-Dependent Parameter Tuning of the Apparent Tardiness Cost Dispatching Rule Using Deep Reinforcement Learning","B. Min; C. O. Kim","Department of Industrial Engineering, Yonsei University, Seoul, South Korea; Department of Industrial Engineering, Yonsei University, Seoul, South Korea","IEEE Access","1 Mar 2022","2022","10","","20187","20198","The apparent tardiness cost (ATC) is a dispatching rule that demonstrates excellent performance in minimizing the total weighted tardiness (TWT) in single-machine scheduling. The ATC rule’s performance is dependent on the lookahead parameter of an equation that calculates the job priority index. Existing studies recommend a fixed value or a value derived through a handcrafted function as an estimate of the lookahead parameter. However, such parameter estimation inevitably entails information loss from using summarized job data and generates an inferior schedule. This study proposes a reinforcement learning-based ATC dispatching rule that estimates the lookahead parameter directly from raw job data (processing time, weight, and slack time). The scheduling agent learns the relationship between raw job data and the continuous lookahead parameter while interacting with the scheduling environment using a deep deterministic policy gradient (DDPG) algorithm. We trained the DDPG model to minimize the TWT through a simulation in a single-machine scheduling problem with unequal job arrival times. Based on a preliminary experiment, we verified that the proposed dispatching rule, ATC-DDPG, successfully performed intelligent state-dependent parameter tuning. ATC-DDPG also displayed the best performance in the main experiment, which compared the performance with five existing dispatching rules.","2169-3536","","10.1109/ACCESS.2022.3152192","Korea Institute of Energy Technology Evaluation and Planning (KETEP) and the Ministry of Trade, Industry & Energy (MOTIE) of the Republic of Korea(grant numbers:20202020800290); National Research Foundation of Korea (NRF); Korean Government [Ministry of Science and ICT (MSIT)](grant numbers:NRF-2019R1A2B5B01070358); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9715142","Deep deterministic policy gradient;dispatching rule;dynamic scheduling;parameter tuning;reinforcement learning","Dispatching;Q-learning;Dynamic scheduling;Tuning;Heuristic algorithms;Single machine scheduling;Job shop scheduling","batch processing (industrial);dispatching;job shop scheduling;learning (artificial intelligence);scheduling;single machine scheduling","ATC rule;job priority index;parameter estimation;summarized job data;inferior schedule;reinforcement learning-based ATC dispatching rule;raw job data;scheduling agent;continuous lookahead parameter;scheduling environment;deep deterministic policy gradient algorithm;TWT;single-machine scheduling problem;unequal job arrival times;ATC-DDPG;intelligent state-dependent parameter tuning;existing dispatching rules;apparent tardiness cost dispatching rule;deep reinforcement learning;total weighted tardiness","","","","25","CCBY","16 Feb 2022","","","IEEE","IEEE Journals"
"Barrier Lyapunov Function-Based Safe Reinforcement Learning for Autonomous Vehicles With Optimized Backstepping","Y. Zhang; X. Liang; D. Li; S. S. Ge; B. Gao; H. Chen; T. H. Lee","Department of Electrical and Computer Engineering, National University of Singapore, Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore, Singapore; School of Cyber Science and Technology, Beihang University, Beijing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore, Singapore; Clean Energy Automotive Engineering Center, Tongji University, Shanghai, China; State Key Laboratory of Automotive Simulation and Control, Jilin University, Changchun, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore, Singapore","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","15","Guaranteed safety and performance under various circumstances remain technically critical and practically challenging for the wide deployment of autonomous vehicles. Safety-critical systems in general, require safe performance even during the reinforcement learning (RL) period. To address this issue, a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm is proposed here for the formulated nonlinear system in strict-feedback form. This approach appropriately arranges and incorporates the BLF items into the optimized backstepping control method to constrain the state-variables in the designed safety region during learning. Wherein, thus, the optimal virtual/actual control in every backstepping subsystem is decomposed with BLF items and also with an adaptive uncertain item to be learned, which achieves safe exploration during the learning process. Then, the principle of Bellman optimality of continuous-time Hamilton–Jacobi–Bellman equation in every backstepping subsystem is satisfied with independently approximated actor and critic under the framework of actor-critic through the designed iterative updating. Eventually, the overall system control is optimized with the proposed BLF-SRL method. It is furthermore noteworthy that the variance of the attained control performance under uncertainty is also reduced with the proposed method. The effectiveness of the proposed method is verified with two motion control problems for autonomous vehicles through appropriate comparison simulations.","2162-2388","","10.1109/TNNLS.2022.3186528","National Natural Science Foundation of China(grant numbers:61790564); China Automobile Industry Innovation and Development Joint Fund(grant numbers:U1864206); Jilin Provincial Science and Technology Department(grant numbers:20200301011RQ); International Technology Cooperation Program of Science and Technology Commission of Shanghai Municipality(grant numbers:21160710600); China Scholarship Council(grant numbers:202006170145); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9827972","Adaptive dynamic programming (ADP);autonomous vehicles;Barrier Lyapunov Function (BLF);safe reinforcement learning (SRL)","Safety;Backstepping;Uncertainty;Nonlinear systems;Autonomous vehicles;Lyapunov methods;Control systems","","","","","","","IEEE","12 Jul 2022","","","IEEE","IEEE Early Access Articles"
"Multi-Agent Partial Observable Safe Reinforcement Learning for Counter Uncrewed Aerial Systems","J. -E. Pierre; X. Sun; R. Fierro","Department of Electrical and Computer Engineering, The University of New Mexico, Albuquerque, NM, USA; Department of Electrical and Computer Engineering, The University of New Mexico, Albuquerque, NM, USA; Department of Electrical and Computer Engineering, The University of New Mexico, Albuquerque, NM, USA","IEEE Access","1 Aug 2023","2023","11","","78192","78206","The proliferation of small uncrewed aerial systems (UAS) poses many threats to airspace systems and critical infrastructures. In recent years, there has been a growing interest in using multi-agent reinforcement learning (MARL) to counter unwanted UAS systems. However, MARL is unable to generate safety actions to meet predefined constraints, thus hindering its use in real-life applications. In this paper, we formulate the Counter-UAS problem as a multi-agent partially observable Markov decision process (MAPOMDP), and we propose Multi-AGent partial observable deep reiNforcement lEarning for pursuer conTrol optimization (MAGNET) to train a group of UAS in terms of pursuers or agents, to pursue and intercept a faster UAS or evader, which tries to escape from capture while navigating through crowded airspace with several moving non-cooperating interacting entities (NCIEs). In MAGNET, we integrate the Control Barrier Function (CBF) based safety layer into proximal policy optimization (PPO) to provide safety guarantees during the training and testing process. In addition, we incorporate the DeepSet network into MAGNET to handle the time-varying dimension of an agent’s observations. We conduct extensive simulations and the results show that MAGNET is able to maintain a collision-free environment at the sacrifice of slight evader capture rate reduction as compared to the baseline implementations.","2169-3536","","10.1109/ACCESS.2023.3298601","Sandia National Laboratories, North Atlantic Treaty Organization (NATO), Air Force Research Laboratory (AFRL)(grant numbers:FA9453-18-2-0022); federal agency and industry partners as specified in the Resilient and Intelligent NextG Systems (RINGS) Program through the National Science Foundation(grant numbers:CNS 2148178); Sandia National Laboratories is a multi-mission laboratory managed and operated by National Technology and Engineering Solutions of Sandia LLC., a wholly owned subsidiary of Honeywell International, Inc., for the U.S. Department of Energy’s National Nuclear Security Administration(grant numbers:DE-NA0003525); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10193774","Multi-agent systems;deep reinforcement learning;counter uncrewed aerial systems;DeepSets;control barrier functions;machine learning","Reinforcement learning;Autonomous aerial vehicles;Safety;Optimization;Feature extraction;Collision avoidance;Task analysis;Multi-agent systems;Machine learning","autonomous aerial vehicles;deep learning (artificial intelligence);learning (artificial intelligence);Markov processes;multi-agent systems;optimisation;reinforcement learning","agent;airspace systems;Control Barrier Function based safety layer;Counter uncrewed aerial systems;Counter-UAS problem;critical infrastructures;crowded airspace;faster UAS;MARL;moving noncooperating interacting entities;MultiAGent partial observable deep reiNforcement lEarning;Multiagent partial observable safe reinforcement learning;multiagent partially observable Markov decision process;multiagent reinforcement learning;pursuer conTrol optimization;pursuers;safety actions;testing process;unwanted UAS systems","","","","38","CCBY","25 Jul 2023","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Real-Time Renewable Energy Bidding With Battery Control","J. Jeong; S. W. Kim; H. Kim","Department of Electronic Engineering, Sogang University, Seoul, South Korea; Department of Electrical Engineering, Chungnam National University, Daejeon, South Korea; Department of Electronic Engineering, Sogang University, Seoul, South Korea","IEEE Transactions on Energy Markets, Policy and Regulation","15 Jun 2023","2023","1","2","85","96","Recently, various renewable energy sources and large-scale batteries have been integrated into power grids, and renewable energy bidding and battery control become critical problems in the real-time energy market. However, bidding and control problems have been studied separately while these two problems simultaneously influence the total profit of renewable producers. In this paper, we propose a novel strategy where renewable energy bidding and battery control are collectively investigated. First, unlike the previous studies where bidding is simply the forecasted value, the proposed methods determine the bidding values considering the error compensability of the battery by switching the objective of forecasting from reducing errors to making errors compensable. After the error compensation, additional battery control is applied to utilize the energy arbitrage process considering the energy price. As there are energy price and renewable generation uncertainties, we propose a deep reinforcement learning based bidding combined with control, called DeepBid, for sequential decision making under uncertainty. Our extensive simulations with real solar and wind generation data show that the proposed DeepBid strategy substantially increases the total profit compared to existing bidding strategies by achieving as high revenues as the arbitrage strategy and as low deviation penalties as the error compensation strategy.","2771-9626","","10.1109/TEMPR.2023.3258409","Korea Institute of Energy Technology Evaluation and Planning; Ministry of Trade, Industry and Energy(grant numbers:20192010107290); National Research Foundation of Korea; Ministry of Science and ICT, South Korea(grant numbers:NRF-2021R1A2C1095435); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10075530","Battery system;bidding strategy;deep reinforcement learning;real-time energy market","Batteries;Renewable energy sources;Error compensation;Real-time systems;Forecasting;Optimization;Stochastic processes","decision making;deep learning (artificial intelligence);error compensation;power engineering computing;power generation economics;power grids;power markets;pricing;profitability;reinforcement learning;renewable energy sources;wind power plants","additional battery control;bidding strategies;control problems;deep reinforcement learning;energy arbitrage process;energy price;error compensability;error compensation strategy;large-scale batteries;real-time energy market;real-time renewable energy bidding;renewable energy sources;renewable generation uncertainties;renewable producers;total profit","","","","60","IEEE","17 Mar 2023","","","IEEE","IEEE Journals"
"Reinforcement Learning Heuristic A*","J. Ha; B. An; S. Kim","Center for Healthcare Robotics, Department of Artificial Intelligence Robot, Korea Institute of Science and Technology, Seoul, South Korea; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Center for Healthcare Robotics, Department of Artificial Intelligence Robot, Korea Institute of Science and Technology, Seoul, South Korea","IEEE Transactions on Industrial Informatics","8 Mar 2023","2023","19","3","2307","2316","In a graph search algorithm, a given environment is represented as a graph comprising a set of feasible system configurations and their neighboring connections. A path is generated by connecting the initial and goal configurations through graph exploration, whereby the path is often desired to be optimal or suboptimal. The computational performance of the optimal path generation depends on the avoidance of unnecessary explorations. Accordingly, heuristic functions have been widely adopted to guide the exploration efficiently by providing estimated costs to the goal configurations. The exploration is efficient when the heuristic functions estimate the optimal cost closely, which remains challenging because it requires a comprehensive understanding of the environment. However, this challenge presents the scope to improve the computational efficiency over the existing methods. Herein, we propose reinforcement learning heuristic A* (RLHA*), which adopts an artificial neural network as a learning heuristic function to closely estimate the optimal cost, while achieving a bounded suboptimal path. Instead of being trained by precomputed paths, the learning heuristic function keeps improving by using self-generated paths. Numerous simulations were performed to demonstrate the consistent and robust performance of RLHA* by comparing it with the existing methods.","1941-0050","","10.1109/TII.2022.3188359","Korea Medical Device Development Fund; Korea Government; Ministry of Science and ICT, South Korea; Ministry of Trade, Industry and Energy; Ministry of Health and Welfare; Ministry of Food and Drug Safety(grant numbers:RS-2020-KD000090); Korea Institute of Science and Technology Institutional Program(grant numbers:2E31572); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815131","Graph search;path planning;reinforcement learning","Costs;Heuristic algorithms;Path planning;Signal processing algorithms;Robots;Reinforcement learning;Planning","graph theory;metaheuristics;neural nets;reinforcement learning;search problems","artificial neural network;bounded suboptimal path;computational efficiency;cost estimation;graph exploration;graph search algorithm;learning heuristic function;optimal path generation;path planning;reinforcement learning heuristic A*;RLHA*;self-generated paths","","","","27","IEEE","4 Jul 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Secrecy Energy Efficiency Maximization in RIS-Assisted Networks","Y. Zhang; Y. Lu; R. Zhang; B. Ai; D. Niyato","School of Computer and Information Technology, Collaborative Innovation Center of Railway Traffic Safety, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Collaborative Innovation Center of Railway Traffic Safety, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Collaborative Innovation Center of Railway Traffic Safety, Beijing Jiaotong University, Beijing, China; State Key Laboratory of Rail Traffic Control and Safety, School of Electronics and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Vehicular Technology","18 Sep 2023","2023","72","9","12413","12418","This paper investigates the deep reinforcement learning (DRL) for maximization of the secrecy energy efficiency (SEE) in reconfigurable intelligent surface (RIS)-assisted networks. An SEE maximization problem is formulated under constraints of the rate requirement of each (legitimate) user, the power budget of the transmitter and the discrete phase shift coefficient of each reflecting element at the RIS by jointly optimizing the beamforming vectors for users and the artificial noise vectors for eavesdroppers as well as the phase shift matrix. The considered problem is first reformulated into a Markov decision process with the designed state space, action space and reward function, and then solved under a proximal policy optimization (PPO) framework. Numerical results are provided to evaluate the optimality, the generalization performance and the running time of proposed PPO-based algorithm.","1939-9359","","10.1109/TVT.2023.3269805","Fundamental Research Funds for the Central Universities(grant numbers:2021RC204); National Natural Science Foundation of China (NSFC)(grant numbers:62101025,62221001); China Postdoctoral Science Foundation(grant numbers:BX2021031,2021M690342); Beijing Nova Program(grant numbers:Z211100002121139); National Research Foundation, Singapore and Infocomm Media Development Authority; Future Communications Research Development Programme; DSO National Laboratories through AI Singapore Programme(grant numbers:AISG2-RP-2020-019); Energy Research Test-Bed and Industry Partnership Funding Initiative; DesCartes and the Campus for Research Excellence and Technological Enterprise (CREATE) programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10107766","DRL;PPO;RIS;SEE","Transmitters;Array signal processing;Optimization;Energy efficiency;Wireless networks;Safety;Reinforcement learning","array signal processing;concave programming;deep learning (artificial intelligence);energy conservation;Markov processes;reinforcement learning;telecommunication power management;telecommunication security;wireless channels","artificial noise vectors;beamforming vectors;deep reinforcement learning;discrete phase shift coefficient;generalization performance;Markov decision process;phase shift matrix;PPO-based algorithm;proximal policy optimization framework;RIS-assisted networks;secrecy energy efficiency in reconfigurable intelligent surface-assisted networks;secrecy energy efficiency maximization;SEE maximization problem","","","","17","IEEE","25 Apr 2023","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Active Network Management and Emergency Load-Shedding Control for Power Systems","H. Zhang; X. Sun; M. H. Lee; J. Moon","Department of Electrical Engineering, Hanyang University, Seoul, South Korea; Department of Electrical Engineering, Hanyang University, Seoul, South Korea; Research Institute of Electrical and Computer Engineering, Hanyang University, Seoul, South Korea; Department of Electrical Engineering, Hanyang University, Seoul, South Korea","IEEE Transactions on Smart Grid","","2023","PP","99","1","1","This paper presents two novel deep reinforcement learning (DRL) approaches aimed at solving complex power system control problems in a data-driven sense to maintain the stability of power systems. Specifically, we propose, respectively, SACPER (Soft Actor-Critic (SAC) with Prioritized Experience Replay (PER)) and Constrained Variational Policy Optimization (CVPO) DRL algorithms to address the sequential decision-making problem of active network management (ANM) in distributed power systems and optimizing emergency load shedding (ELS) control problems. First, we propose SACPER for the ANM problem, which prioritizes the training of samples with large errors and poor policy performance. Evaluation of SACPER in terms of stability improvement and convergence speed shows that the ANM problem is optimized and energy loss and operational constraint violations are minimized. Next, we introduce CVPO for the ELS control problem, which is formulated as the Safe Reinforcement Learning (SRL) framework to address safety constraint prioritization issues in power systems. We consider additional voltage variables in the network as strong constraints for SRL to achieve fast voltage recovery and minimize unnecessary energy loss, while ensuring good training performance and efficiency. To demonstrate the performances of SACPER, we apply it to ANM6-Easy environment. The CVPO algorithm is applied to IEEE 39-Bus and IEEE 300-Bus systems. The simulation results of SACPER and CVPO are validated through extensive comparisons with other state-of-the-art DRL approaches.","1949-3061","","10.1109/TSG.2023.3302846","National Research Foundation of Korea(grant numbers:NRF-2021R1A2C2094350, 2022R1I1A1A01073354); Development of GW class voltage sourced DC linkage technology for improved interconnectivity and carrying capacity of wind power in the Sinan and southwest regions(grant numbers:R22TA12); Institute of Information & communications Technology Planning and Evaluation(grant numbers:No.2020-0-01373); Korea Institute of Energy Technology Evaluation and Planning(KETEP) and the Ministry of Trade, Industry & Energy(MOTIE) of the Republic of Korea(grant numbers:RS-2023-00235742); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10210687","Deep reinforcement learning;active network management;emergency control;safe reinforcement learning;load shedding","Power system stability;Safety;Voltage control;Inference algorithms;Training;Power systems;Task analysis","","","","","","","IEEE","8 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Model-Based Reinforcement Learning for Closed-Loop Dynamic Control of Soft Robotic Manipulators","T. G. Thuruthel; E. Falotico; F. Renda; C. Laschi","BioRobotics Institute, Scuola Superiore Sant’Anna, Pisa, Italy; BioRobotics Institute, Scuola Superiore Sant’Anna, Pisa, Italy; Department of Mechanical Engineering and the Center for Autonomous Robotics Systems, Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates; BioRobotics Institute, Scuola Superiore Sant’Anna, Pisa, Italy","IEEE Transactions on Robotics","5 Feb 2019","2019","35","1","124","134","Dynamic control of soft robotic manipulators is an open problem yet to be well explored and analyzed. Most of the current applications of soft robotic manipulators utilize static or quasi-dynamic controllers based on kinematic models or linearity in the joint space. However, such approaches are not truly exploiting the rich dynamics of a soft-bodied system. In this paper, we present a model-based policy learning algorithm for closed-loop predictive control of a soft robotic manipulator. The forward dynamic model is represented using a recurrent neural network. The closed-loop policy is derived using trajectory optimization and supervised learning. The approach is verified first on a simulated piecewise constant strain model of a cable driven under-actuated soft manipulator. Furthermore, we experimentally demonstrate on a soft pneumatically actuated manipulator how closed-loop control policies can be derived that can accommodate variable frequency control and unmodeled external loads.","1941-0468","","10.1109/TRO.2018.2878318","European Unions Horizon 2020 Research and Innovation Program(grant numbers:785907 (HBP SGA2)); European Unions Horizon 2020 Research and Innovation Program(grant numbers:785907); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8531756","Dynamic control;machine learning;manipulation;reinforcement learning;soft robotics","Manipulator dynamics;Computational modeling;Soft robotics;Aerospace electronics;Trajectory optimization","closed loop systems;manipulator dynamics;manipulator kinematics;pneumatic actuators;predictive control;recurrent neural nets;supervised learning","model-based reinforcement learning;closed-loop dynamic control;soft robotic manipulator;quasidynamic controllers;kinematic models;soft-bodied system;model-based policy learning algorithm;closed-loop predictive control;soft pneumatically actuated manipulator;recurrent neural network;trajectory optimization;supervised learning;cable driven under-actuated soft manipulator;piecewise constant strain model","","178","","35","IEEE","11 Nov 2018","","","IEEE","IEEE Journals"
"Reinforcement Learning With Sequences of Motion Primitives for Robust Manipulation","F. Stulp; E. A. Theodorou; S. Schaal","Computational Learning and Motor Control Laboratory, University of Southern California, Los Angeles, CA, USA; Computational Learning and Motor Control Laboratory, USC, Seattle, USA; Computational Learning and Motor Control Laboratory, USC, USA","IEEE Transactions on Robotics","30 Nov 2012","2012","28","6","1360","1370","Physical contact events often allow a natural decomposition of manipulation tasks into action phases and subgoals. Within the motion primitive paradigm, each action phase corresponds to a motion primitive, and the subgoals correspond to the goal parameters of these primitives. Current state-of-the-art reinforcement learning algorithms are able to efficiently and robustly optimize the parameters of motion primitives in very high-dimensional problems. These algorithms often consider only shape parameters, which determine the trajectory between the start- and end-point of the movement. In manipulation, however, it is also crucial to optimize the goal parameters, which represent the subgoals between the motion primitives. We therefore extend the policy improvement with path integrals (PI2) algorithm to simultaneously optimize shape and goal parameters. Applying simultaneous shape and goal learning to sequences of motion primitives leads to the novel algorithm PI2 Seq. We use our methods to address a fundamental challenge in manipulation: improving the robustness of everyday pick-and-place tasks.","1941-0468","","10.1109/TRO.2012.2210294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6295672","Learning and adaptive systems;manipulation planning;reinforcement learning","Learning systems;Adaptive systems;Learning;Manipulators;Grasping","control engineering computing;learning (artificial intelligence);manipulators;motion control;trajectory control","reinforcement learning;robust manipulation;physical contact events;manipulation tasks;motion primitive sequences;movement trajectory;policy improvement;path integrals;PI2 algorithm;pick-and-place tasks","","133","1","29","IEEE","5 Sep 2012","","","IEEE","IEEE Journals"
"Barrier-Certified Adaptive Reinforcement Learning With Applications to Brushbot Navigation","M. Ohnishi; L. Wang; G. Notomista; M. Egerstedt","Georgia Robotics and Intelligent Systems Laboratory, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Mechanical Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Robotics","2 Oct 2019","2019","35","5","1186","1205","This paper presents a safe learning framework that employs an adaptive model learning algorithm together with barrier certificates for systems with possibly nonstationary agent dynamics. To extract the dynamic structure of the model, we use a sparse optimization technique. We use the learned model in combination with control barrier certificates that constrain policies (feedback controllers) in order to maintain safety, which refers to avoiding particular undesirable regions of the state space. Under certain conditions, recovery of safety in the sense of Lyapunov stability after violations of safety due to the nonstationarity is guaranteed. In addition, we reformulate an action-value function approximation to make any kernel-based nonlinear function estimation method applicable to our adaptive learning framework. Lastly, solutions to the barrier-certified policy optimization are guaranteed to be globally optimal, ensuring the greedy policy improvement under mild conditions. The resulting framework is validated via simulations of a quadrotor, which has previously been used under stationarity assumptions in the safe learnings literature, and is then tested on a real robot, the brushbot, whose dynamics is unknown, highly complex, and nonstationary.","1941-0468","","10.1109/TRO.2019.2920206","National Science Foundation(grant numbers:1531195); Scandinavia-Japan Sasakawa Foundation(grant numbers:GA17-JPN-0002); School of Electrical Engineering, Royal Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746143","Brushbot;control barrier certificate;kernel adaptive filter;safe learning;sparse optimization","Safety;Adaptation models;Kernel;Robots;Optimization;Reinforcement learning;Lyapunov methods","feedback;function approximation;learning (artificial intelligence);Lyapunov methods;nonlinear functions;optimisation;state-space methods","barrier-certified adaptive reinforcement;brushbot navigation;safe learning framework;adaptive model;possibly nonstationary agent dynamics;dynamic structure;sparse optimization technique;control barrier certificates;constrain policies;feedback controllers;particular undesirable regions;state space;Lyapunov stability;action-value function approximation;kernel-based nonlinear function estimation method;adaptive learning framework;barrier-certified policy optimization;greedy policy improvement;resulting framework;safe learnings literature","","40","1","64","IEEE","26 Jun 2019","","","IEEE","IEEE Journals"
"Real-World Reinforcement Learning via Multifidelity Simulators","M. Cutler; T. J. Walsh; J. P. How","Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA; Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA; Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE Transactions on Robotics","19 May 2017","2015","31","3","655","671","Reinforcement learning (RL) can be a tool for designing policies and controllers for robotic systems. However, the cost of real-world samples remains prohibitive as many RL algorithms require a large number of samples before learning useful policies. Simulators are one way to decrease the number of required real-world samples, but imperfect models make deciding when and how to trust samples from a simulator difficult. We present a framework for efficient RL in a scenario where multiple simulators of a target task are available, each with varying levels of fidelity. The framework is designed to limit the number of samples used in each successively higher-fidelity/cost simulator by allowing a learning agent to choose to run trajectories at the lowest level simulator that will still provide it with useful information. Theoretical proofs of the framework's sample complexity are given and empirical results are demonstrated on a remote-controlled car with multiple simulators. The approach enables RL algorithms to find near-optimal policies in a physical robot domain with fewer expensive real-world samples than previous transfer approaches or learning without simulators.","1941-0468","","10.1109/TRO.2015.2419431","National Science Foundation Graduate Research Fellowship(grant numbers:0645960); Office of Naval Research Science of Autonomy(grant numbers:N000140910625); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7106543","Animation and simulation;autonomous agents;learning and adaptive systems;reinforcement learning (RL);Animation and simulation;autonomous agents;learning and adaptive systems;reinforcement learning (RL)","Robots;Complexity theory;Data models;Learning (artificial intelligence);Mathematical model;Optimization;Accuracy","learning (artificial intelligence);telerobotics","reinforcement learning;multifidelity simulator;robotic system;RL algorithm;higher-fidelity-cost simulator;lowest level simulator;remote-controlled car","","27","2","45","IEEE","13 May 2015","","","IEEE","IEEE Journals"
"Data-Flow Graph Mapping Optimization for CGRA With Deep Reinforcement Learning","D. Liu; S. Yin; G. Luo; J. Shang; L. Liu; S. Wei; Y. Feng; S. Zhou","College of Computer Science, Chongqing University, Chongqing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Center for Energy-Efficient Computing and Applications, Peking University, Beijing, China; College of Computer Science, Chongqing University, Chongqing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","20 Nov 2019","2019","38","12","2271","2283","Coarse-grained reconfigurable architectures (CGRAs) have drawn increasing attention due to their flexibility and energy efficiency. Data flow graphs (DFGs) are often mapped onto CGRAs for acceleration. The problem of DFG mapping is challenging due to the diverse structures from DFGs and constrained hardware from CGRAs. Consequently, it is difficult to find a valid and high quality solution simultaneously. Inspired from the great progress in deep reinforcement learning (RL) for AI problems, we consider building methods that learn to map DFGs onto spatially programmed CGRAs directly from experiences. We propose RLMap, a solution that formulates DFG mapping on CGRA as an agent in RL, which unifies placement, routing and processing element insertion by interchange actions of the agent. Experimental results show that RLMap performs comparably to state-of-the-art heuristics in mapping quality, adapts to different architecture, and converges quickly.","1937-4151","","10.1109/TCAD.2018.2878183","National Natural Science Foundation of China(grant numbers:61804017); Northwestern Polytechnical University(grant numbers:2018CDXYJSJ0026); National Natural Science Foundation of China(grant numbers:61702059); National Basic Research Program of China (973 Program)(grant numbers:2017YFB1402400); Frontier and Application Foundation Research Program of CQ CSTC(grant numbers:cstc2017jcyjAX0340); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8509169","Coarse-grained reconfigurable architecture (CGRA);data flow graph (DFG);mapping;reinforcement learning (RL)","Flow graphs;Reconfigurable architectures;Reinforcement learning","","","","24","","33","IEEE","25 Oct 2018","","","IEEE","IEEE Journals"
"Toward Expedited Impedance Tuning of a Robotic Prosthesis for Personalized Gait Assistance by Reinforcement Learning Control","M. Li; Y. Wen; X. Gao; J. Si; H. Huang","NCSU/UNC Department of Biomedical Engineering, North Carolina State University, Raleigh, NC, USA; NCSU/UNC Department of Biomedical Engineering, North Carolina State University, Raleigh, NC, USA; Department of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; Department of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; NCSU/UNC Department of Biomedical Engineering, North Carolina State University, Raleigh, NC, USA","IEEE Transactions on Robotics","7 Feb 2022","2022","38","1","407","420","Personalizing medical devices such as lower limb wearable robots is challenging. While the initial feasibility of automating the process of knee prosthesis control parameter tuning has been demonstrated in a principled way, the next critical issue is to improve tuning efficiency and speed it up for the human user, in clinic settings, while maintaining human safety. We, therefore, propose a policy iteration with constraint embedded (PICE) method as an innovative solution to the problem under the framework of reinforcement learning. Central to PICE is the use of a projected Bellman equation with a constraint of assuring positive semidefiniteness of performance values during policy evaluation. Additionally, we developed both online and offline PICE implementations that provide additional flexibility for the designer to fully utilize measurement data, either from on-policy or off-policy, to further improve PICE tuning efficiency. Our human subject testing showed that the PICE provided effective policies with significantly reduced tuning time. For the first time, we also experimentally evaluated and demonstrated the robustness of the deployed policies by applying them to different tasks and users. Putting it together, our new way of problem solving has been effective as PICE has demonstrated its potential toward truly automating the process of control parameter tuning for robotic knee prosthesis users.","1941-0468","","10.1109/TRO.2021.3078317","National Science Foundation(grant numbers:1563454,1563921,1808752,1808898); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442858","Impedance control;knee prosthesis;policy iteration;rehabilitation robotics;reinforcement learning (RL)","Tuning;Impedance;Knee;Prosthetics;Legged locomotion;Robustness;Kinematics","biomechanics;gait analysis;iterative methods;learning (artificial intelligence);medical control systems;medical robotics;prosthetics","toward expedited impedance tuning;robotic prosthesis;personalized gait assistance;reinforcement learning control;medical devices;lower limb wearable robots;initial feasibility;knee prosthesis control parameter tuning;tuning efficiency;human user;clinic settings;human safety;policy iteration;projected Bellman equation;positive semidefiniteness;performance values;policy evaluation;additional flexibility;human subject testing;effective policies;reduced tuning time;deployed policies;robotic knee prosthesis users","","20","","56","IEEE","27 May 2021","","","IEEE","IEEE Journals"
"Bio-Inspired Rapid Escape and Tight Body Flip on an At-Scale Flapping Wing Hummingbird Robot Via Reinforcement Learning","Z. Tu; F. Fei; X. Deng","Institute of Unmanned System, Beihang University, Beijing, China; Amazon.com, Inc, Seattle, WA, USA; School of Mechanical Engineering, Purdue University, West Lafayette, IN, USA","IEEE Transactions on Robotics","1 Oct 2021","2021","37","5","1742","1751","Insects and hummingbirds are capable of acrobatic maneuvers such as rapid turns and tight 360$^\circ$ body flips. It is challenging for bio-inspired flapping wing micro aerial vehicles to achieve animal-like performance during such maneuvers due to their limitation in mechanism sophistication and flight control. Besides being significantly underactuated compared to their natural counterparts, flight dynamics is highly nonlinear with rapidly changing, unsteady aerodynamics which remains largely unknown during aggressive maneuvers. As a result, conventional model-based control methods are inadequate to address such maneuvers effectively due to the lack of control references and aerodynamic models. In addition, during acrobatic maneuvers such as body flips, conventional control methods with underlying stabilization mechanisms would temporarily contradict the maneuvering requirements when the vehicle undergoes a full body flip including turning upside-down. In this article, reinforcement learning (RL) has been used to complement model-based control to enable animal-like maneuverability. The learned control policy serves in two different ways to either aid or even completely takeover the conventional stabilization controller in certain cases. We experimentally demonstrate animal-like maneuverability on an at-scale, dual-motor actuated flapping wing hummingbird robot. Two test cases have been performed to demonstrate the effectiveness of such integrated control methods: 1) a rapid escape maneuver recorded from hummingbirds, 2) a tight 360$^\circ$ body flip inspired by houseflies. By leveraging RL, the hummingbird robot demonstrated a shorter completion time in escape maneuvers compared to the traditional control-based method. It also performed 360$^\circ$ body flip successfully within only one wingspan vertical displacement.","1941-0468","","10.1109/TRO.2021.3064882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387412","Aggressive maneuvers;biologically-inspired robots;flapping wing;reinforcement learning (RL)","Robots;Aerodynamics;Training;Task analysis;Vehicle dynamics;Torque;Trajectory","aerodynamics;aerospace components;aerospace computing;aerospace control;aerospace robotics;intelligent robots;learning (artificial intelligence);microrobots;mobile robots;robot dynamics;stability;vehicle dynamics","maneuvering requirements;reinforcement learning;animal-like maneuverability;learned control policy;stabilization controller;dual-motor actuated flapping wing hummingbird robot;integrated control methods;rapid escape maneuver;escape maneuvers;bio-inspired rapid escape;tight body flip;acrobatic maneuvers;rapid turns;body flips;bio-inspired flapping wing microaerial vehicles;animal-like performance;mechanism sophistication;flight control;flight dynamics;unsteady aerodynamics;aggressive maneuvers;control references;aerodynamic models;stabilization mechanisms;at-scale flapping wing hummingbird robot","","10","","27","IEEE","26 Mar 2021","","","IEEE","IEEE Journals"
"Multiagent Reinforcement Learning for Hyperparameter Optimization of Convolutional Neural Networks","A. Iranfar; M. Zapater; D. Atienza","Embedded Systems Laboratory, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Embedded Systems Laboratory, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Embedded Systems Laboratory, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","18 Mar 2022","2022","41","4","1034","1047","Nowadays, deep convolutional neural networks (DCNNs) play a significant role in many application domains, such as computer vision, medical imaging, and image processing. Nonetheless, designing a DCNN, able to defeat the state of the art, is a manual, challenging, and time-consuming task, due to the extremely large design space, as a consequence of a large number of layers and their corresponding hyperparameters. In this work, we address the challenge of performing hyperparameter optimization of DCNNs through a novel multiagent reinforcement learning (MARL)-based approach, eliminating the human effort. In particular, we adapt  $Q$ -learning and define learning agents per layer to split the design space into independent smaller design subspaces such that each agent fine tunes the hyperparameters of the assigned layer concerning a global reward. Moreover, we provide a novel formation of  $Q$ -tables along with a new update rule that facilitates agents’ communication. Our MARL-based approach is data driven and able to consider an arbitrary set of design objectives and constraints. We apply our MARL-based solution to different well-known DCNNs, including GoogLeNet, VGG, and U-Net, and various datasets for image classification and semantic segmentation. Our results have shown that compared to the original CNNs, the MARL-based approach can reduce the model size, training time, and inference time by up to, respectively,  $83\times $ , 52%, and 54% without any degradation in accuracy. Moreover, our approach is very competitive to state-of-the-art neural architecture search methods in terms of the designed CNN accuracy and its number of parameters while significantly reducing the optimization cost.","1937-4151","","10.1109/TCAD.2021.3077193","ERC Consolidator Grant COMPUSAPIEN(grant numbers:725657); H2020 DeepHealth Project(grant numbers:825111); Unrestricted Research Gift by the AI Hardware Infrastructure Unit of Facebook for ESL-EPFL; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9420739","Convolutional neural network (CNN);hyperparameter optimization;neural architecture search (NAS);reinforcement Learning (RL)","Optimization;Computer architecture;Training;Search problems;Kernel;Convolution;Reinforcement learning","convolutional neural nets;deep learning (artificial intelligence);image classification;image segmentation;multi-agent systems;optimisation;reinforcement learning","hyperparameter optimization;deep convolutional neural networks;DCNNs;application domains;computer vision;medical imaging;image processing;multiagent reinforcement learning-based approach;Q-learning;learning agents;independent smaller design subspaces;agent fine tunes;MARL-based approach;image classification;inference time;neural architecture search methods;semantic segmentation","","9","","45","IEEE","3 May 2021","","","IEEE","IEEE Journals"
"Pruning Deep Reinforcement Learning for Dual User Experience and Storage Lifetime Improvement on Mobile Devices","C. Wu; Y. Cui; C. Ji; T. -W. Kuo; C. J. Xue","Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Computer Science, City University of Hong Kong, Hong Kong; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Computer Science, City University of Hong Kong, Hong Kong","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","29 Oct 2020","2020","39","11","3993","4005","Background segment cleaning in log-structured file system has a significant impact on mobile devices. A low triggering frequency of the cleaning activity cannot reclaim enough free space for subsequent I/O, thus incurring foreground segment cleaning and impacting the user experience. In contrast, a high triggering frequency could generate excessive block migrations (BMs) and impair the storage lifetime. Prior works address this issue either by performance-biased solutions or incurring excessive memory overhead. In this article, a pruned reinforcement learning-based approach, MOBC, is proposed. Through learning the behaviors of I/O workloads and the statuses of logical address space, MOBC adaptively reduces the number of BMs and the number of triggered foreground segment cleanings. In order to integrate MOBC to resource-constraint mobile devices, a structured pruning method is proposed to reduce the time and space cost. The experimental results show that the pruned MOBC can reduce the worst case latency by 32.5%-68.6% at the 99.9th percentile, and improve the storage endurance by 24.3% over existing approaches, with significantly reduced overheads.","1937-4151","","10.1109/TCAD.2020.3012804","Research Grants Council of the Hong Kong Special Administrative Region, China(grant numbers:CityU 11204718); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211447","Log-structured file system (LFS);mobile device;multiobjective deep reinforcement learning (RL);neuron network pruning;segment cleaning;storage lifetime;user experience","Cleaning;Mobile handsets;Performance evaluation;Learning (artificial intelligence);Neural networks;Training;Machine learning","learning (artificial intelligence);mobile computing;storage management","mobile devices;structured pruning method;pruned MOBC;user experience;storage lifetime improvement;log-structured file system;triggering frequency;BMs;pruned reinforcement learning-based approach;address space;deep reinforcement learning","","8","","41","IEEE","2 Oct 2020","","","IEEE","IEEE Journals"
"Cooperative Communication Between Two Transiently Powered Sensor Nodes by Reinforcement Learning","Y. Wu; Z. Jia; F. Fang; J. Hu","Department of Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA, USA; Department of Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","22 Dec 2021","2022","41","1","76","90","Energy harvesting (EH)-powered sensor nodes can achieve theoretically unlimited lifetime by scavenging energy from ambient power sources, such as radio-frequency (RF) and kinetic energy. The nodes can collect and transmit data wirelessly with the harvested energy. However, the transmission between two sensor nodes is successful only when both nodes have enough energy at the same time. While the receiver can be actively listening, it may deplete the energy long before the sender has accumulated enough energy. Thus, given the scarce, unpredictable, and unevenly distributed energy among sensor nodes, it is challenging to ensure efficient data transmission between them. To address this challenge, we propose a sensor node architecture with multiple radios, each with different energy consumption on the sender and receiver. A node can be put into sleep when charged up and wakes up for communication when it infers that both nodes have enough energy based on its observations. What is more, two nodes can cooperatively and dynamically select different radios according to the stored energy and historical information to maximize the data throughput. To achieve cooperative communication adaptively, the communication procedure is modeled as a cooperative Markov game with partial observability on each node, and multiagent reinforcement learning (MARL) is employed to achieve the best results. Experimental results on hardware prototype and by simulation show that the proposed approaches achieve up to 89.1% of the optimal throughput and significantly outperform other online algorithms.","1937-4151","","10.1109/TCAD.2021.3054329","NSF(grant numbers:CNS-2007274); NSF(grant numbers:IIS-1850477); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9335978","Energy harvesting (EH);multiagent reinforcement learning (MARL);transiently powered data transmission","Receivers;Throughput;Monitoring;Games;Reinforcement learning;Markov processes;Backscatter","cooperative communication;energy harvesting;game theory;learning (artificial intelligence);Markov processes;multi-agent systems;telecommunication computing;wireless sensor networks","stored energy;multiagent reinforcement learning;transiently powered sensor nodes;energy harvesting-powered sensor nodes;ambient power sources;kinetic energy;harvested energy;sensor node architecture;energy consumption;cooperative communication;cooperative Markov game","","7","","45","IEEE","26 Jan 2021","","","IEEE","IEEE Journals"
"Automated Design of Analog Circuits Using Reinforcement Learning","K. Settaluri; Z. Liu; R. Khurana; A. Mirhaj; R. Jain; B. Nikolic","Department of Electrical and Computer Engineering, University of California at Berkeley, Berkeley, CA, USA; Department of Electrical and Computer Engineering, University of California at Berkeley, Berkeley, CA, USA; Qualcomm Inc., Bengaluru, India; Qualcomm Inc., San Diego, CA, USA; Qualcomm Inc., San Diego, CA, USA; Department of Electrical and Computer Engineering, University of California at Berkeley, Berkeley, CA, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","19 Aug 2022","2022","41","9","2794","2807","Analog and mixed-signal (AMS) blocks are often a crucial and time-consuming part of System-on-Chip (SoC) design, primarily due to a manual circuit and layout iterations. Existing automated solutions for selecting circuit parameters for a given target specification are often not efficient, accurate, or reliable. In order for an automated sizing tool to be practical, we posit that it must: 1) return valid results for a large range of target specifications; 2) understand where and why it is unable to meet certain specifications; 3) consider true layout parasitic simulations for complete end-to-end design; and 4) be automated, allowing most of the design effort to fall on the tool. In this article, we address these critical points by establishing an automated reinforcement learning framework, AutoCkt, by 1) successfully deploying it on a complex two-stage transimpedance amplifier and two-stage folded cascode with biasing in the 16-nm FinFet technology; 2) implementing a new combined distribution deployment algorithm to improve efficiency; 3) analyzing in-depth the efficacy of the trained agent; and 4) demonstrating the functionality of this tool when considering a topology that is highly sensitive to layout parasitics. Our algorithm not only successfully reaches unique, valid, and practical performances, but also does so in state-of-the-art run time, up to 38X more efficient than prior work. In addition, our tool averages just four parasitic simulations obtained by using the Berkeley Analog Generator, to achieve a target specification post-layout for the folded cascode. AutoCkt successfully generates LVS-passed designs with validation in process corner variation results.","1937-4151","","10.1109/TCAD.2021.3120547","DARPA CRAFT(grant numbers:HR0011-16-C-0052); NSF AI Institute for Advances in Optimization(grant numbers:2112533); ADEPT; Berkeley Wireless Research Center member companies; Qualcomm Innovation Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9576505","Analog sizing;Berkeley analog generator;design methodology;layout parasitics;reinforcement learning (RL)","Layout;Integrated circuit modeling;Measurement;Reinforcement learning;Tools;Analytical models;Trajectory","analogue integrated circuits;circuit CAD;circuit simulation;integrated circuit layout;learning (artificial intelligence);mixed analogue-digital integrated circuits;MOSFET circuits;operational amplifiers;system-on-chip","trained agent efficacy;complex two-stage transimpedance amplifier;automated sizing tool;circuit parameters;layout iterations;manual circuit;SoC design;system-on-chip design;analog and mixed-signal blocks;Analog circuits;automated design;process corner variation;LVS-passed designs;target specification post-layout;Berkeley Analog Generator;combined distribution deployment algorithm;FinFet technology;two-stage folded cascode amplifier;AutoCkt;automated reinforcement learning framework;layout parasitic simulations;size 16 nm","","6","","32","IEEE","15 Oct 2021","","","IEEE","IEEE Journals"
" $Q$ -Value Prediction for Reinforcement Learning Assisted Garbage Collection to Reduce Long Tail Latency in SSD","W. Kang; S. Yoo","Department of Computer Science and Engineering, Seoul National University, Seoul, South Korea; Department of Computer Science and Engineering, Seoul National University, Seoul, South Korea","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","22 Sep 2020","2020","39","10","2240","2253","Garbage collection (GC), an essential operation for flash storage systems, causes long tail latency which is one of the key problems in real-time and quality-critical systems. In this article, we take advantage of reinforcement learning (RL) to reduce long tail latency. Especially, we propose two novel techniques which are: 1) Q-table cache (QTC) and 2) Q-value prediction. The QTC allows us to utilize appropriate and frequently recurring key states at a small memory cost. We propose a neural network called Q-value prediction network (QP Net) that predicts the initial Q-value of a new state in the QTC. The integrated solution of QTC and QP Net enables us to benefit from both short-term (by QTC) and long-term (by QP Net) history of system behavior to reduce the long tail latency. The experimental results demonstrate that the proposed scheme offers significant (by 25%-37%) reductions in the long tail latency of storageintensive workloads compared with the state-of-the-art solution that adopts an RL-assisted GC scheduler.","1937-4151","","10.1109/TCAD.2019.2962781","National Research Foundation of Korea; Ministry of Science, ICT, and Future Planning (PE Class Heterogeneous High Performance Computer Development)(grant numbers:NRF-2016M3A7B4909604,NRF-2016M3C4A7952587); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946536","Garbage collection (GC);long tail latency;reinforcement learning (RL);solid-state drive (SSD)","Reinforcement learning;Memory management;Random access memory;Flash memories;Real-time systems;Neural networks;Three-dimensional displays","flash memories;learning (artificial intelligence);neural nets;scheduling;solid state drives","reinforcement learning;long tail latency;flash storage systems;quality-critical systems;QTC;Q-value prediction network;QP Net;RL-assisted GC scheduler;garbage collection;Q-table cache;SSD;solid-state drive;neural network","","5","","34","IEEE","31 Dec 2019","","","IEEE","IEEE Journals"
"ReLAccS: A Multilevel Approach to Accelerator Design for Reinforcement Learning on FPGA-Based Systems","A. R. Baranwal; S. Ullah; S. S. Sahoo; A. Kumar","Chair for Processor Design, TU Dresden, Dresden, Germany; Chair for Processor Design, TU Dresden, Dresden, Germany; Chair for Processor Design, TU Dresden, Dresden, Germany; Chair for Processor Design, TU Dresden, Dresden, Germany","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","19 Aug 2021","2021","40","9","1754","1767","Reinforcement learning (RL), specifically Q-learning, with human-like learning abilities to learn from experience without any a priori data, is being increasingly used in embedded systems in the field of control and navigation. However, finding the optimal policy in this approach can be highly compute-intensive, and a software-only implementation may not satisfy the application's timing constraints. To this end, we propose optimization methods at multiple levels of accelerator design for RL. Specifically, at the architecture-level, we exploit the instruction-level parallelism and the spatial parallelism in FPGAs to improve the throughput over state-of-the-art designs by up to 34%. Further, we propose lookup table-level optimizations to reduce the resource utilization and power dissipation of the accelerator. Finally, we propose algorithm-level approximation that can be used for acceleration of Q-learning problems with more states and for reducing the peak power dissipation. We report up to 10× reduction in power dissipation with marginal degradation in quality of results.","1937-4151","","10.1109/TCAD.2020.3028350","German Federal Ministry for Economic Affairs and Energy (BMWi); PRÄKLIMA Fassade(grant numbers:ZF4706601GM9); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211770","Cross-layer system design;embedded systems;field-programmable gate array (FPGA);high-level synthesis;reinforcement learning (RL)","Optimization;Memory management;Power dissipation;Decision making;Embedded systems;Field programmable gate arrays;Micromechanical devices","embedded systems;field programmable gate arrays;learning (artificial intelligence);optimisation;table lookup","embedded systems;navigation;optimal policy;application;optimization methods;accelerator design;RL;architecture-level;instruction-level parallelism;lookup table-level optimizations;power dissipation;algorithm-level approximation;Q-learning problems;multilevel approach;reinforcement learning;FPGA-based systems;a priori data","","5","","29","IEEE","2 Oct 2020","","","IEEE","IEEE Journals"
"Hardware-Level Thread Migration to Reduce On-Chip Data Movement Via Reinforcement Learning","Q. Fettes; A. Karanth; R. Bunescu; A. Louri; K. Shiflett","EECS Department, Ohio University, Athens, OH, USA; EECS Department, Ohio University, Athens, OH, USA; EECS Department, Ohio University, Athens, OH, USA; ECE Department, George Washington University, Washington, DC, USA; EECS Department, Ohio University, Athens, OH, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","29 Oct 2020","2020","39","11","3638","3649","As the number of processing cores and associated threads in chip multiprocessors (CMPs) continues to scale out, on-chip memory access latency dominates application execution time due to increased data movement. Although tiled CMP architectures with distributed shared caches provide a scalable design, increased physical distance between requesting and responding cores has led to both increased on-chip memory access latency and excess energy consumption. Near data processing is a promising approach that can migrate threads closer to data, however prior hand-engineered rules for fine-grained hardware-level thread migration are either too slow to react to changes in data access patterns, or unable to exploit the large variety of data access patterns. In this article, we propose to use reinforcement learning (RL) to learn relatively complex data access patterns to improve on hardware-level thread migration techniques. By utilizing the recent history of memory access locations as input, each thread learns to recognize the relationship between prior access patterns and future memory access locations. This leads to the unique ability of the proposed technique to make fewer, more effective migrations to intermediate cores that minimize the distance to multiple distinct memory access locations. By allowing a low-overhead RL agent to learn a policy from real interaction with parallel programming benchmarks in a parallel simulator, we show that a migration policy which recognizes more complex data access patterns can be learned. The proposed approach reduces on-chip data movement and energy consumption by an average of 41%, while reducing execution time by 43% when compared to a simple baseline with no thread migration; furthermore, energy consumption and execution time are reduced by an additional 10% when compared to a hand-engineered fine-grained migration policy.","1937-4151","","10.1109/TCAD.2020.3012650","NSF(grant numbers:CCF-1513606,CCF-1513923,CCF-1702980,CCF-1703013,CCF-1812495,CCF-1901192,CCF-1936794); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211404","Chip multiprocessors (CMPs);data movement;reinforcement learning (RL);thread migration","Message systems;Instruction sets;System-on-chip;Context;Learning (artificial intelligence);Pattern recognition;System recovery","cache storage;electronic engineering computing;learning (artificial intelligence);microprocessor chips;multiprocessing systems;multi-threading;parallel programming;power aware computing","excess energy consumption;reinforcement learning;multiple distinct memory access locations;on-chip data movement;hand-engineered fine-grained migration policy;chip multiprocessors;on-chip memory access latency;data access pattern learning;hardware-level thread migration;CMP;parallel programming;parallel simulator;distributed shared caches","","4","","41","IEEE","2 Oct 2020","","","IEEE","IEEE Journals"
"Analog Integrated Circuit Topology Synthesis With Deep Reinforcement Learning","Z. Zhao; L. Zhang","Department of Electrical and Computer Engineering, Faculty of Engineering and Applied Science, Memorial University of Newfoundland, St. John’s, NL, Canada; Department of Electrical and Computer Engineering, Faculty of Engineering and Applied Science, Memorial University of Newfoundland, St. John’s, NL, Canada","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","21 Nov 2022","2022","41","12","5138","5151","This article presents a novel deep-reinforcement-learning-based method for topology synthesis of analog-integrated circuits, especially operational amplifiers (OpAmps). It behaves like a human designer, who learns from trials, derives design knowledge and experience, and evolves gradually to finally figure out optimal manners to construct proper circuit topologies that meet design specifications. Essential design rules are defined and applied to set up the specialized environment for reinforcement learning in order to reasonably construct circuit topologies with building blocks as the basic components. Our proposed method can not only handle large-size circuit designs but also generate creative circuit topologies. The produced circuit topologies are verified by the simulation-in-loop sizing. In order to improve the evaluation efficiency, hash table and symbolic analysis techniques are utilized to significantly reduce the number of the produced topologies to be sized during the synthesis process. Compared with the state-of-the-art approaches, our proposed method significantly improves the synthesis efficiency by consuming only several hours on average to produce a trustworthy solution. Our experimental results demonstrate its sound efficiency, strong reliability, and wide applicability.","1937-4151","","10.1109/TCAD.2022.3153437","Natural Sciences and Engineering Research Council of Canada (NSERC); Canada Foundation for Innovation (CFI); Provincial Government of Newfoundland and Labrador; Memorial University of Newfoundland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9718525","Circuit topology synthesis;deep reinforcement learning (RL);parallel computing;symbolic analysis","Circuit topology;Integrated circuits;Integrated circuit modeling;Reinforcement learning;Parallel processing;Circuit synthesis","analogue integrated circuits;deep learning (artificial intelligence);electronic engineering computing;integrated circuit design;integrated circuit reliability;network topology;operational amplifiers;reinforcement learning","analog integrated circuit topology synthesis;analog-integrated circuits;deep-reinforcement-learning-based method;large-size circuit designs;OpAmps;operational amplifiers;reliability;simulation-in-loop sizing","","4","","34","IEEE","23 Feb 2022","","","IEEE","IEEE Journals"
"Optimizing Discharge Efficiency of Reconfigurable Battery With Deep Reinforcement Learning","S. Jeon; J. Kim; J. Ahn; H. Cha","Department of Computer Science, Yonsei University, Seoul, South Korea; Department of Computer Science, Yonsei University, Seoul, South Korea; Department of Computer Science, Yonsei University, Seoul, South Korea; Department of Computer Science, Yonsei University, Seoul, South Korea","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","29 Oct 2020","2020","39","11","3893","3905","Cell imbalance in a multicell battery occurs over time due to varying operating environments. This imbalance leads to overall inefficiency in battery discharging due to the relatively weak cells in the battery. Reconfiguring the cells in the battery is one option for addressing the problem, but relevant circuits may lead to severe safety issues. In this article, we aim to optimize the discharge efficiency of a multicell battery using safety-supplemented hardware. To this end, we first design a cell string-level reconfiguration scheme that is safe in hardware operations and also provides scalability due to the low switching complexity. Second, we propose a machine learning-based run-time switch control that considers various battery-related factors, such as the state of charge, state of health, temperature, and current distributions. Specifically, by exploiting the deep reinforcement learning (DRL) technique, we train the complex relationship among the battery factors and derive the best switch configuration in run-time. We implemented a hardware prototype, validated its functionalities, and evaluated the efficacy of the DRL-based control policy. The experimental results showed that the proposed scheme, along with the optimization method, improves the discharge efficiency of multicell batteries. In particular, the discharge efficiency gain is maximized when the cells constituting the battery are unevenly distributed in terms of cell health and exposed temperature.","1937-4151","","10.1109/TCAD.2020.3012230","Next-Generation Information Computing Development Program; Ministry of Science and ICT(grant numbers:NRF-2017M3C4A7083677); National Research Foundation of Korea(grant numbers:NRF-2019R1A2C2004619); Institute for Information and communications Technology Promotion; Korea Government [MSIT, Development of High-Assurance (≥EAL6) Secure Microkernel](grant numbers:2018-0-00532); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211558","Deep reinforcement learning (DRL);reconfigurable battery;switch control policy","Batteries;Discharges (electric);Switches;Hardware;Integrated circuit modeling;Safety","battery management systems;battery powered vehicles;learning (artificial intelligence);optimisation;power engineering computing;secondary cells","operating environments;battery discharging;relatively weak cells;severe safety issues;safety-supplemented hardware;cell imbalance;reconfigurable battery;cell health;discharge efficiency gain;multicell battery;battery factors;deep reinforcement learning technique;battery-related factors;machine learning-based run-time switch control;low switching complexity;hardware operations;cell string-level reconfiguration scheme","","3","","39","IEEE","2 Oct 2020","","","IEEE","IEEE Journals"
"RLPlace: Using Reinforcement Learning and Smart Perturbations to Optimize FPGA Placement","M. A. Elgammal; K. E. Murray; V. Betz","Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Cerebras Systems, Toronto, Canada; Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","18 Jul 2022","2022","41","8","2532","2545","Simulated annealing (SA) is one of the most common FPGA placement techniques, and is used both as a standalone algorithm and to improve an initial analytical placement. While SA-based placers can achieve high-quality results, they suffer from long runtimes. In this article, we introduce RLPlace, a novel SA-based FPGA placer that utilizes both reinforcement learning (RL) and targeted perturbations (directed moves). The proposed moves target both wirelength and timing optimization and explore the solution space more efficiently than traditional random moves while preventing oscillation in the Quality of Results (QoR). RL techniques are used to dynamically select the most effective move types as optimization progresses. The experimental results show that RLPlace outperforms the widely used VTR 8 placer across all runtime/quality tradeoff points, achieving better QoR placement solutions in less runtime. On average, across the Titan23 suite of large FPGA benchmarks, RLPlace can reduce CPU time by  $2.5\times $  with result quality comparable to VTR 8, or improve wirelength by 8% (at a high CPU time budget) −26% (at a low CPU time budget) versus VTR 8.0 given the same CPU time.","1937-4151","","10.1109/TCAD.2021.3109863","NSERC/Intel Industrial Research Chair in Programmable Silicon; Intel/VMware Crossroads 3-D FPGA Academic Research Centre; Huawei; Google; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9528855","Directed moves;FPGA;placement;reinforcement learning (RL)","Field programmable gate arrays;Runtime;Simulated annealing;Optimization;Reinforcement learning;Perturbation methods;Annealing","electronic engineering computing;field programmable gate arrays;integrated circuit layout;reinforcement learning;simulated annealing","timing optimization;traditional random moves;RL techniques;optimization progresses;RLPlace;VTR 8 placer;QoR placement solutions;FPGA benchmarks;result quality;high CPU time budget;low CPU time budget;reinforcement learning;smart perturbations;simulated annealing;common FPGA placement techniques;standalone algorithm;initial analytical placement;SA-based placers;high-quality results;long runtimes;FPGA placer;targeted perturbations;directed moves;wirelength optimization;quality of results;QoR;runtime-quality tradeoff points;Titan23 suite","","3","","41","IEEE","3 Sep 2021","","","IEEE","IEEE Journals"
"Deep Reinforcement-Learning-Guided Backup for Energy Harvesting Powered Systems","W. Sun; W. Fan; M. Zhao; W. Song; X. Cai; T. Liu; Z. Jia","School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; AInnovation Technology Ltd., Huanan Business Unit, Guangzhou, China; School of Computer Science and Technology, Shandong University, Qingdao, China","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","19 Jan 2022","2022","41","2","346","358","Energy harvesting technology has been widely developed as a promising alternative of battery to power embedded systems. However, energy harvesting powered embedded systems may have potential frequent power interruptions due to unstable energy supply. Nonvolatile processors (NVPs) are proposed to survive power failures by saving volatile data to nonvolatile memory (NVM) upon power failures and resuming them after power comes back. Traditionally, backup is triggered immediately when an energy warning occurs. However, it is also possible to more aggressively utilize the residual energy for program execution to improve forward progress. In this work, we propose a deep reinforcement-learning-guided backup strategy to improve forward progress in energy harvesting powered intermittent embedded systems. The experimental results show an average of 8.3%, 51.6%, and 325.3% improved forward progress compared with <inline-formula> <tex-math notation=""LaTeX"">$Q$ </tex-math></inline-formula>-learning, the related work ALD, and traditional instant backup, respectively.","1937-4151","","10.1109/TCAD.2021.3056328","NSFC–Shandong Joint Fund(grant numbers:U1806203); Major Scientific and Technological Innovation Project in Shandong Province(grant numbers:2019JZZY010449); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9344855","Double deep <italic xmlns:ali=""http://www.niso.org/schemas/ali/1.0/"" xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"">Q</italic>-network;energy harvesting system;forward progress;nonvolatile processor (NVP);reinforcement learning","Reinforcement learning;Energy harvesting;Capacitors;Nonvolatile memory;Program processors;Runtime;Embedded systems","deep learning (artificial intelligence);embedded systems;energy harvesting;power engineering computing;random-access storage;reinforcement learning","energy harvesting technology;power embedded systems;potential frequent power interruptions;unstable energy supply;power failures;energy warning;residual energy;deep reinforcement-learning-guided backup strategy;intermittent embedded systems;nonvolatile processors;NVP;nonvolatile memory;NVM;program execution","","2","","40","IEEE","2 Feb 2021","","","IEEE","IEEE Journals"
"Quality Optimization of Adaptive Applications via Deep Reinforcement Learning in Energy Harvesting Edge Devices","F. Chen; H. Yu; W. Jiang; Y. Ha","School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Computer Science, University of Nottingham Ningbo China, Ningbo, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology and the Shanghai Engineering Research Center of Energy Efficient and Custom AI IC, ShanghaiTech University, Shanghai, China","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","25 Oct 2022","2022","41","11","4873","4886","Applications with adaptability are widely available on the edge devices with energy harvesting capabilities. For their runtime quality optimization, however, current approaches cannot tackle the variations of quality modeling and harvested energy simultaneously. Therefore, in this article, we are the first to propose a deep reinforcement learning (DRL)-based dynamic voltage frequency scaling (DVFS) method that optimizes the application execution quality of energy harvesting edge devices to mitigate the variations. First, we propose a baseline DRL formulation that novelly migrates the objective of quality maximization into a reward function and constructs a DRL quality agent. Second, we devise a long short-term memory (LSTM)-based selector that performs DRL quality agent selection based on the energy harvesting history. Third, we further propose two optimization methods to alleviate the nonnegligible overhead of DRL computations: 1) an improved thinking-while-moving concurrent DRL scheme to compromise the “state drifting” issue during the DRL decision process and 2) a variable interstate duration decision scheme that compromises the DVFS overhead incurred in each action taken. The experiments take an adaptive stereo matching application as a case study. The results show that the proposed DRL-based DVFS method on average achieves 17.9% runtime reduction and 22.05% quality improvement compared to state-of-the-art solutions.","1937-4151","","10.1109/TCAD.2022.3142188","Natural Science Foundation of China(grant numbers:62074101,62150710549); Shanghai Science and Technology Commission Funding(grant numbers:19511131200,20ZR1435800); National Natural Science Foundation of China(grant numbers:72071116); Ningbo Science and Technology Bureau(grant numbers:2019B10026,2017D10034); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9677006","Adaptive application;deep reinforcement learning (DRL);energy harvesting;quality optimization;stereo matching","Adaptation models;Energy harvesting;Uncertainty;Optimization;Task analysis;Computational modeling;Runtime","energy harvesting;image matching;learning (artificial intelligence);multiprocessing systems;optimisation;power aware computing;stereo image processing","17.9% runtime reduction;22.05% quality improvement;adaptability;adaptive applications;adaptive stereo matching application;application execution quality;baseline DRL formulation;deep reinforcement learning-based dynamic voltage frequency;DRL computations;DRL decision process;DRL-based DVFS method;energy harvesting capabilities;energy harvesting edge devices;energy harvesting history;improved thinking-while-moving concurrent DRL scheme;optimization methods;performs DRL quality agent;quality maximization;quality modeling;reward function;runtime quality optimization;short-term memory-based selector;variable interstate duration decision scheme","","2","","44","IEEE","11 Jan 2022","","","IEEE","IEEE Journals"
"A Multiagent Reinforcement Learning-Assisted Cache Cleaning Scheme for DM-SMR","Z. Shen; Y. Yang; Y. Pan; Y. Zhang; Z. Jia; X. Cai; B. Li; Z. Shao","School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; Department of Computer and Information Science, Linköping University, Linköping, Sweden; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK, USA; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Kowloon Tong, Hong Kong","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","18 Jul 2023","2023","42","8","2500","2513","To support nonsequential writes, persistent cache (PC) is constructed in drive managed SMR (DM-SMR) drive. However, PC cleaning introduces drastic performance degradation and enlarges tail latencies. In this article, we propose to utilize reinforcement learning (RL) to mitigate the long-tail latency of PC cleaning. Our scheme uses the lightweight  $Q$ -learning method to monitor and learn the idle time of I/O workloads, based on which PC cleaning is intelligently guided, thus maximally exploit idle time between requests and hiding tail latency from normal requests. In addition, a multiagent RL scheme with clustering algorithm is adopted to further mitigate the tail latencies and adapt to variable workloads. We emulate a DM-SMR drive inside a Linux device driver to implement our proposed scheme. According to the experimental results, our scheme can effectively reduce the tail latency by 59.45% at the 99.9th percentile and the average latency by 48.75% compared with a typical shingled magnetic recording (SMR) design.","1937-4151","","10.1109/TCAD.2022.3222670","National Science Foundation for Young Scientists of China(grant numbers:61902218); National Natural Science Foundation of China(grant numbers:62272271); National Natural Science Foundation of Shandong Joint Fund(grant numbers:62272271); National Natural Science Foundation of Shandong Joint Fund(grant numbers:U1806203); Research Grants Council of the Hong Kong Special Administrative Region, China(grant numbers:15224918); Direct Grant for Research; Chinese University of Hong Kong; U.S. National Science Foundation(grant numbers:2208317,2204657); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9961956","Cleaning process;clustering algorithm;reinforcement learning (RL);shingled magnetic recording (SMR);tail latency","Cleaning;Q-learning;Degradation;Tail;System performance;Real-time systems;Linux","cache storage;device drivers;learning (artificial intelligence);Linux;magnetic recording;multi-agent systems;reinforcement learning","DM-SMR drive;drastic performance degradation;enlarges tail latencies;hiding tail;lightweightQ-learning method;long-tail latency;multiagent reinforcement learning-assisted cache cleaning scheme;multiagent RL scheme;PC cleaning;persistent cache;typical shingled magnetic recording design","","1","","38","IEEE","23 Nov 2022","","","IEEE","IEEE Journals"
"Asynchronous Reinforcement Learning Framework and Knowledge Transfer for Net-Order Exploration in Detailed Routing","Y. Lin; T. Qu; Z. Lu; Y. Su; Y. Wei","Computer Science Department, Peking University, Beijing, China; Institute of Microelectronics, Chinese Academy of Sciences, Beijing, China; Computer Science Department, Peking University, Beijing, China; Institute of Microelectronics, Chinese Academy of Sciences, Beijing, China; Institute of Microelectronics, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","19 Aug 2022","2022","41","9","3132","3142","The net orders in detailed routing are crucial to routing closure, especially in most modern routers following the sequential routing manner with the rip-up and reroute scheme. In advanced technology nodes, detailed routing has to deal with complicated design rules and large problem sizes, making its performance more sensitive to the order of nets to be routed. In the literature, the net orders are mostly determined by simple heuristic rules tuned for specific benchmarks. In this work, we propose an asynchronous reinforcement learning (RL) framework to automatically search for optimal ordering strategies and a transfer learning (TL) algorithm to improve performance. By asynchronous querying, the router, pretraining the RL agents, and finetuning with the TL algorithm, we can generate high-performance routing sequences to achieve a 26% reduction in the DRC violations and a 1.2% reduction in the total costs compared with the state-of-the-art detailed router.","1937-4151","","10.1109/TCAD.2021.3117505","National Natural Science Foundation of China(grant numbers:61874002,62034007); Beijing Municipal Natural Science Foundation(grant numbers:2017ZX02315001); National Key Research and Development Program of China(grant numbers:2019YFB2205005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9557780","Detailed routing;physical design;policy distillation;reinforcement learning (RL);transfer learning (TL)","Routing;Reinforcement learning;Transfer learning;Benchmark testing;Costs;Pins;Wires","reinforcement learning;telecommunication computing;telecommunication network routing","net orders;detailed routing;routing closure;sequential routing manner;complicated design rules;asynchronous reinforcement learning framework;optimal ordering strategies;transfer learning algorithm;high-performance routing sequences;knowledge transfer;net-order exploration","","1","","28","IEEE","4 Oct 2021","","","IEEE","IEEE Journals"
"DETERRENT: Detecting Trojans Using Reinforcement Learning","V. Gohil; S. Patnaik; H. Guo; D. Kalathil; J. Rajendran","Department of Electrical and Computer Engineering, Texas A&M University, College Station, USA; Department of Electrical and Computer Engineering, Texas A&M University, College Station, USA; Department of Electrical and Computer Engineering, Texas A&M University, College Station, USA; Department of Electrical and Computer Engineering, Texas A&M University, College Station, USA; Department of Electrical and Computer Engineering, Texas A&M University, College Station, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2023","PP","99","1","1","The globalized nature of the integrated circuits supply chain has given rise to several security problems. The insertion of malicious components, called hardware Trojans, is one such serious problem. Since Trojans are activated only under extremely rare trigger conditions and the search space is exponentially large, detecting them is arduous. Researchers have attempted to detect Trojans by querying the design-undertest using appropriate test patterns and monitoring its logical or side-channel response. However, techniques in both these categories lack either in terms of detection accuracy or scalability for larger designs. In this work, we investigate why existing techniques fall short and use our findings to propose a new reinforcement learning (RL) framework for detecting Trojans. We carefully design two RL agents (one for each category) that navigate the exponential search space of the test patterns and return minimal sets of patterns that are most likely to detect Trojans. We overcome challenges related to scalability and efficacy through appropriate solutions. Experimental results on a variety of benchmarks demonstrate the scalability and efficacy of our RL agents, which reduce the number of test patterns significantly (169.68× and 34.73× on average overall and 27.59× and 3.72× on average over large benchmarks) while maintaining or improving the Trojan-detection success rate compared to the state-of-the-art techniques.","1937-4151","","10.1109/TCAD.2023.3309731","Division of Graduate Education(grant numbers:2039610); Division of Computer and Network Systems(grant numbers:1822848); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10233936","Reinforcement Learning;Hardware Trojans;Logic Testing;Side-channel Analysis;Test Generation","Trojan horses;Benchmark testing;Test pattern generators;Hardware;Logic testing;Switches;Scalability","","","","","","","IEEE","29 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Exploring Rule-Free Layout Decomposition via Deep Reinforcement Learning","B. Jiang; X. Zang; M. D. F. Wong; E. F. Y. Young","Department of Computer Science and Engineering, The Chinese University of Hong Kong, SAR, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, SAR, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, SAR, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, SAR, Hong Kong","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","21 Aug 2023","2023","42","9","3067","3077","Multiple patterning lithography decomposition (MPLD) and mask optimization enable the ever-shrinking device feature sizes far below the lithography system limit. Conventional MPLD is solved by mathematical programming or graph-based approaches, where a set of predetermined rules is indispensable to identify the conflicts to be resolved. In this article, we explore rule-free layout decomposition following a simple but sweet principle, let the mask optimizer “teach” the layout decomposer how to generate suitable decompositions. Our flow includes a reinforcement-learning-based layout decomposer and a deep-learning-based mask optimizer. Without any handcrafted rules, our framework can perform competitively and even surpass the state-of-the-art rule-based methods with notable  $(7\times \sim 63\times)$  turn-around-time speedup.","1937-4151","","10.1109/TCAD.2022.3232992","Research Grants Council of the Hong Kong, SAR(grant numbers:CUHK 14209320); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002897","Design for manufacturability;double patterning;inverse lithography technique (ILT);reinforcement learning (RL)","Layout;Optimization;Lithography;Multiprotocol label switching;Law;Resists;Reinforcement learning","deep learning (artificial intelligence);electronic engineering computing;graph theory;lithography;masks;optimisation;reinforcement learning","conventional MPLD;deep reinforcement learning;graph based approaches;handcrafted rules;lithography system limit;mask optimizer teach;mathematical programming;multiple patterning lithography decomposition;reinforcement learning based layout decomposer;rule free layout decomposition;state-of-the-art rule-based methods","","","","28","IEEE","29 Dec 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Joint Reliability and Performance Optimization for Hybrid-Cache Computing Servers","D. Huang; A. Pahlevan; L. Costero; M. Zapater; D. Atienza","Embedded Systems Laboratory, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Embedded Systems Laboratory, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Embedded Systems Laboratory, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Embedded Systems Laboratory, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Embedded Systems Laboratory, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","21 Nov 2022","2022","41","12","5596","5609","Computing servers play a key role in the development and process of emerging compute-intensive applications in recent years. However, they need to operate efficiently from an energy perspective viewpoint, while maximizing the performance and lifetime of the hottest server components (i.e., cores and cache). Previous methods focused on either improving energy efficiency by adopting new hybrid-cache architectures including the resistive random-access memory (RRAM) and static random-access memory (SRAM) at the hardware level, or exploring tradeoffs between lifetime limitation and performance of multicore processors under stable workloads conditions. Therefore, no work has so far proposed a co-optimization method with hybrid-cache-based server architectures for real-life dynamic scenarios taking into account scalability, performance, lifetime reliability, and energy efficiency at the same time. In this article, we first formulate a reliability model for the hybrid-cache architecture to enable precise lifetime reliability management and energy efficiency optimization. We also include the performance and energy overheads of cache switching, and optimize the benefits of hybrid-cache usage for better energy efficiency and performance. Then, we propose a runtime  $q$ -learning-based reliability management and performance optimization approach for multicore microprocessors with the hybrid-cache architecture, jointly incorporated with a dynamic preemptive priority queue management method to improve the overall tasks’ performance by targeting to respect their end time limits. Experimental results show that our proposed method achieves up to 44% average performance (i.e., tasks execution time) improvement, while maintaining the whole system design lifetime longer than five years, when compared to the latest state-of-the-art energy efficiency optimization and reliability management methods for computing servers.","1937-4151","","10.1109/TCAD.2022.3158832","EC H2020 RECIPE FET-HPC Project(grant numbers:801137); ERC Consolidator Grant COMPUSAPIEN(grant numbers:725657); EC H2020 DeepHealth Project(grant numbers:GA 825111); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733030","Computing servers;hybrid cache;performance optimization;preemptive queue management;reinforcement learning (RL);reliability management","Reliability;Servers;Q-learning;Energy efficiency;Multicore processing;Nonvolatile memory;Reinforcement learning","cache storage;circuit reliability;electronic engineering computing;energy conservation;energy consumption;memory architecture;multiprocessing systems;power aware computing;queueing theory;reinforcement learning","cache switching;co-optimization;compute-intensive applications;dynamic preemptive priority queue management;energy efficiency optimization;energy overheads;hybrid-cache computing servers;hybrid-cache usage;hybrid-cache-based server architectures;lifetime limitation;lifetime reliability management;multicore microprocessors;performance optimization;reinforcement learning-based joint reliability;runtime q-learning-based reliability management;server components;system design lifetime;task execution time","","","","53","IEEE","11 Mar 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Approach for Efficient and Reliable Droplet Routing on MEDA Biochips","M. Elfar; Y. -C. Chang; H. H. -Y. Ku; T. -C. Liang; K. Chakrabarty; M. Pajic","Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","20 Mar 2023","2023","42","4","1212","1222","The micro-electrode-dot-array (MEDA) architecture provides precise droplet control and real-time sensing in digital microfluidic biochips. Previous work has shown that trapped charge under microelectrodes (MCs) leads to droplets being stuck and failures in fluidic operations. A recent approach utilizes real-time sensing of MC health status, and attempts to avoid degraded electrodes during droplet routing. However, the problem with this solution is that the computational complexity is unacceptable for MEDA biochips of realistic size. Consequently, in this work, we introduce a deep reinforcement learning (DRL)-based approach to bypass degraded electrodes and enhance the reliability of routing. The DRL model utilizes the information of health sensing in real time to proactively reduce the likelihood of charge trapping and avoid using degraded MCs. Simulation results show that our approach provides effective routing strategies for COVID-19 testing protocols. We also validate our DRL-based approach using fabricated prototype biochips. Experimental results show that the developed DRL model completed the routing tasks using a fewer number of clock cycles and shorter total execution time, compared with a baseline routing method. Moreover, our DRL-based approach provides reliable routing strategies even in the presence of degraded electrodes. Our experimental results show that the proposed DRL-based routing is robust to occurrences of electrode faults, as well as increases the lifetime and usability of microfluidic biochips compared to existing strategies.","1937-4151","","10.1109/TCAD.2022.3194808","National Science Foundation(grant numbers:ECCS-1914796); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9844153","Biomedical electronics;microelectrodes (MCs);microfluidics;real-time systems;reinforcement learning","Routing;Biochips;Biological system modeling;Sensors;Real-time systems;Degradation;Training","bioMEMS;computational complexity;deep learning (artificial intelligence);diseases;drops;epidemics;lab-on-a-chip;microelectrodes;microfluidics;reinforcement learning;telecommunication computing;telecommunication network routing","baseline routing method;deep reinforcement learning-based approach;degraded electrodes;developed DRL model;digital microfluidic biochips;DRL-based approach;DRL-based routing;droplet routing;droplets;effective routing strategies;electrode faults;fabricated prototype biochips;fluidic operations;health sensing;MC health status;MCs;MEDA biochips;microelectrode-dot-array architecture;precise droplet control;recent approach utilizes real-time sensing;reliable routing strategies;routing tasks;shorter total execution time;trapped charge","","","","40","IEEE","28 Jul 2022","","","IEEE","IEEE Journals"
"Flatfish: A Reinforcement Learning Approach for Application-Aware Address Mapping","X. Li; Z. Yuan; Y. Guan; G. Sun; T. Zhang; R. Wei; D. Niu","Center for Energy-Efficient Computing and Applications, Peking University, Beijing, China; Center for Energy-Efficient Computing and Applications, Peking University, Beijing, China; Computing Technology Laboratory, Alibaba DAMO Academy, Hangzhou, China; Center for Energy-Efficient Computing and Applications, Peking University, Beijing, China; Alibaba Group, T-Head Semiconductor Co., Ltd., Hangzhou, China; College of Physics and Information Engineering, Fuzhou University, Fuzhou, China; Computing Technology Laboratory, Alibaba DAMO Academy, Hangzhou, China","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","25 Oct 2022","2022","41","11","4758","4770","The DRAM performance has become a critical bottleneck of modern computing systems. Prior studies have proposed various optimization techniques on address mapping to bridge the gap between real performance and the peak performance. Nevertheless, these techniques have some common limitations. First, most of them focus on an indirect metric (e.g., bitwise flip ratio) and fail to address the effects of complicated organization hierarchy and timing constraints of DRAM. Second, these approaches do not leverage application-specific information and may not generate the proper address mapping schemes for modern applications. In this article, we propose Flatfish as a comprehensive solution to address these challenges. Flatfish is a self-adaptive memory controller that is able to generate address mapping schemes according to the memory access pattern. Different from prior approaches, Flatfish considers complicated memory hierarchy, including channel, rank, and bank group and addressed critical timing constraints. By mining the characteristics from the memory access traces, Flatfish integrates a reinforcement learning model to generate a binary invertible matrix (BIM) as the address mapping scheme. Flatfish can work in either offline mode or online mode to meet various requirements in different scenarios. The experimental results show that Flatfish can achieve  $1.91\times $  speedup in the offline mode on GPU, and  $1.63\times $  speedup in the online mode on CPU, over the commonly used Hynix address mapping scheme.","1937-4151","","10.1109/TCAD.2022.3146204","National Key Research and Development Program of China(grant numbers:2020AAA0105200); National Natural Science Foundation of China(grant numbers:61832020,62032001,92064006); Beijing Academy of Artificial Intelligence (BAAI); China National Postdoctoral Program for Innovative Talents(grant numbers:BX20200001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9691462","Memory address mapping;reinforcement learning (RL) for architecture","Random access memory;Timing;Optimization;Reinforcement learning;Computer architecture;Entropy;Memory management","cache storage;DRAM chips;learning (artificial intelligence);storage management","application-aware address;bitwise flip ratio;commonly used Hynix address mapping scheme;complicated organization hierarchy;critical bottleneck;critical timing constraints;DRAM performance;Flatfish;leverage application-specific information;memory access pattern;memory access traces;memory hierarchy;modern computing systems;optimization techniques;peak performance;prior approaches;proper address mapping schemes;reinforcement learning model;self-adaptive memory controller","","","","46","IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"CARTAD: Compiler-Assisted Reinforcement Learning for Thermal-Aware Task Scheduling and DVFS on Multicores","D. Liu; S. -G. Yang; Z. He; M. Zhao; W. Liu","School of Software, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","19 May 2022","2022","41","6","1813","1826","As the power density of modern CPUs is gradually increasing, thermal management has become one of the primary concerns for multicore systems, where task scheduling and dynamic voltage/frequency scaling (DVFS) play a pivotal role in effectively managing the system temperature. In this article, we propose CARTAD, a new reinforcement learning (RL)-based task scheduling and DVFS method for temperature minimization and latency guarantee on multicore systems. The novelty of CARTAD framework is that we exploit the machine learning technique to analyze the applications’ intermediate representations (IRs) generated by a compiler and identify an important feature which is critical for predicting the application’s performance. With the newly explored feature, we construct an RL-based scheduler with the more effective state representation and reward function such that the system temperature can be minimized while guaranteeing applications’ latency. We implement and evaluate CARTAD on real platforms in comparison with the state-of-the-art approaches. Experimental results show CARTAD can reduce the maximum temperature by up to 16 °C and the average temperature by up to 10 °C.","1937-4151","","10.1109/TCAD.2021.3095028","National Natural Science Foundation of China(grant numbers:61902341,61801418); Yunnan Applied Basic Research Projects(grant numbers:202101AT070182,2019FD-129); Open Foundation of Key Laboratory in Software Engineering of Yunnan Province(grant numbers:2020SE403,2020SE316); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9475514","Compiler;dynamic voltage/frequency scaling (DVFS);reinforcement learning (RL);task scheduling","Task analysis;Optimal scheduling;Data models;Multicore processing;Minimization;Hardware;Schedules","learning (artificial intelligence);multiprocessing systems;power aware computing;processor scheduling;program compilers;thermal management (packaging)","machine learning technique;applications;application;newly explored feature;RL-based scheduler;effective state representation;reward function;system temperature;maximum temperature;average temperature;compiler-assisted reinforcement learning;thermal-aware task scheduling;power density;modern CPUs;thermal management;multicore systems;reinforcement learning-based task scheduling;DVFS method;temperature minimization;CARTAD framework;temperature 16.0 degC;temperature 10.0 degC","","","","33","IEEE","6 Jul 2021","","","IEEE","IEEE Journals"
"Boosting the Convergence of Reinforcement Learning-based Auto-pruning Using Historical Data","J. Mu; M. Wang; F. Zhu; J. Yang; W. Lin; W. Zhang","Hong Kong University of Science and Technology (HKUST), China; Alibaba Group, China; Alibaba Group, China; Alibaba Group, China; Alibaba Group, China; Hong Kong University of Science and Technology (HKUST), China","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2023","PP","99","1","1","Recently, neural network compression schemes like channel pruning have been widely used to reduce the model size and computational complexity of deep neural networks (DNNs) for applications in power-constrained scenarios, such as embedded systems. Reinforcement learning (RL)-based auto-pruning has been further proposed to automate the DNN pruning process to avoid expensive hand-crafted work. However, the RL-based pruner involves a time-consuming training process, and pruning and evaluating each network comes at high computational expense. These problems have greatly restricted the real-world application of RL-based auto-pruning. Thus, we propose an efficient auto-pruning framework that solves this problem by taking advantage of the historical data from the previous auto-pruning process. In our framework, we first boost the convergence of the RL-pruner by transfer learning. Then, an augmented transfer learning scheme is proposed to further speed up the training process by improving the transferability. Finally, an assistant learning process is proposed to improve the sample efficiency of the RL agent. The experiments show that our framework can accelerate the auto-pruning process by 1.5x 2.5x for ResNet20, and 1.81x 2.375x for other neural networks, such as ResNet56, ResNet18, and MobileNet v1.","1937-4151","","10.1109/TCAD.2023.3319594","Alibaba; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10265172","Reinforcement Learning;Auto-pruning;DNN","Transfer learning;Training;Data models;Task analysis;Artificial neural networks;Convergence;Search problems","","","","","","","IEEE","27 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Reinforcement Learning of CPG-Regulated Locomotion Controller for a Soft Snake Robot","X. Liu; C. D. Onal; J. Fu","Robotics Engineering Department, Worcester Polytechnic Institute, Worcester, MA, USA; Robotics Engineering Department, Worcester Polytechnic Institute, Worcester, MA, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA","IEEE Transactions on Robotics","4 Oct 2023","2023","39","5","3382","3401","Intelligent control of soft robots is challenging due to the nonlinear and difficult-to-model dynamics. One promising model-free approach for soft robot control is reinforcement learning (RL). However, model-free RL methods tend to be computationally expensive and data-inefficient and may not yield natural and smooth locomotion patterns for soft robots. In this work, we develop a bioinspired design of a learning-based goal-tracking controller for a soft snake robot. The controller is composed of two modules: An RL module for learning goal-tracking behaviors given the unmodeled and stochastic dynamics of the robot, and a central pattern generator (CPG) with the Matsuoka oscillators for generating stable and diverse locomotion patterns. We theoretically investigate the maneuverability of Matsuoka CPG's oscillation bias, frequency, and amplitude for steering control, velocity control, and sim-to-real adaptation of the soft snake robot. Based on this analysis, we proposed a composition of RL and CPG modules such that the RL module regulates the tonic inputs to the CPG system given state feedback from the robot, and the output of the CPG module is then transformed into pressure inputs to pneumatic actuators of the soft snake robot. This design allows the RL agent to naturally learn to entrain the desired locomotion patterns determined by the CPG maneuverability. We validated the optimality and robustness of the control design in both simulation and real experiments, and performed extensive comparisons with state-of-art RL methods to demonstrate the benefit of our bioinspired control design.","1941-0468","","10.1109/TRO.2023.3286046","National Science Foundation(grant numbers:1728412); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10175020","Biomimetics;deep reinforcement learning (RL);learning and adaptive systems;neural oscillator;soft robot control","Oscillators;Robots;Snake robots;Actuators;Robot kinematics;Soft robotics;Frequency control","","","","","","42","IEEE","6 Jul 2023","","","IEEE","IEEE Journals"
"Reinforcement Learning Applied to Cognitive Space Communications","C. D. Schubert; R. Roché; J. C. Briones","University of Texas at Austin, Austin, TX; NASA Glenn Research Center, Cleveland, OH; NASA Glenn Research Center, Cleveland, OH","2019 IEEE Cognitive Communications for Aerospace Applications Workshop (CCAAW)","21 Nov 2019","2019","","","1","8","The future of space exploration depends on robust, reliable communication systems. As the number of such communication systems increase, automation is fast becoming a requirement to achieve this goal. A reinforcement learning solution can be employed as a possible automation method for such systems. The goal of this study is to build a reinforcement learning algorithm which optimizes data throughput of a single actor. A training environment was created to simulate a link within the NASA Space Communication and Navigation (SCaN) infrastructure, using state of the art simulation tools developed by the SCaN Center for Engineering, Networks, Integration, and Communications (SCENIC) laboratory at NASA Glenn Research Center to obtain the closest possible representation of the real operating environment. Reinforcement learning was then used to train an agent inside this environment to maximize data throughput. The simulation environment contained a single actor in low earth orbit capable of communicating with twenty-five ground stations that compose the Near-Earth Network (NEN). Initial experiments showed promising training results, so additional complexity was added by augmenting simulation data with link fading profiles obtained from real communication events with the International Space Station. A grid search was performed to find the optimal hyperparameters and model architecture for the agent. Using the results of the grid search, an agent was trained on the augmented training data. Testing shows that the agent performs well inside the training environment and can be used as a foundation for future studies with added complexity and eventually tested in the real space environment.","","978-1-7281-0048-7","10.1109/CCAAW.2019.8904912","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8904912","switching;machine learning;reinforcement learning;satellite communications;space links","Reinforcement learning;Training;Space vehicles;Orbits;Biological neural networks;Load modeling;NASA","","","","","","7","IEEE","21 Nov 2019","","","IEEE","IEEE Conferences"
"Optimizing Control of Waste Incineration Plants Using Reinforcement Learning and Digital Twins","M. Schlappa; J. Hegemann; S. Spinler","Institute for Logistics Management, WHU – Otto Beisheim School of Management, Vallendar, Germany; Department of Process Control and Instrumentation, Uniper Technologies GmbH, Gelsenkirchen, Germany; Institute for Logistics Management, WHU – Otto Beisheim School of Management, Vallendar, Germany","IEEE Transactions on Engineering Management","","2022","PP","99","1","12","More than 60 waste incineration plants (WIPs) are active across Germany. WIPs have various levels of automation, but they still rely on manual operations by human operators. Human operators tend to rely on a few operational levers and infrequent interventions to manage the complex combustion process. Consequently, the combustion process is managed rather inefficiently, and steam outputs and emission levels are not optimal. This article investigates how reinforcement learning (RL) can help enhance process automation and, thus, optimize the combustion process, e.g., by making more frequent and diverse interventions. An RL agent is trained via trial and error with a reward function that includes the optimization criteria. Since the actual equipment, i.e., the real WIP, cannot be used as the training environment, a digital twin is built using original plant data and a neural network. The RL agent is then trained in this offline environment with the deep Q-network algorithm. First, our work demonstrates that a digital twin of a WIP can be built in a data-driven way. Second, we show that the RL agent outperforms the human operator. Thus, the application of RL might benefit the plant operator in financial terms and the environment in terms of reduced emission levels.","1558-0040","","10.1109/TEM.2022.3201434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9899759","Combustion optimization;data-driven simulation;deep reinforcement learning;digital twin;optimal control;power plants;waste incineration nonlinear optimization","Combustion;Training;Digital twins;Process control;Power generation;Boilers;Reinforcement learning","","","","","","","IEEE","22 Sep 2022","","","IEEE","IEEE Early Access Articles"
"Which Channel to Ask My Question?: Personalized Customer Service Request Stream Routing Using Deep Reinforcement Learning","Z. Liu; C. Long; X. Lu; Z. Hu; J. Zhang; Y. Wang","School of Software, Shandong University, Jinan, China; Ant Financial Services Group, Z Space, Hangzhou, China; School of Science, RMIT University, Melbourne, VIC, Australia; Alibaba Group, Hangzhou, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Ant Financial Services Group, Z Space, Hangzhou, China","IEEE Access","14 Aug 2019","2019","7","","107744","107756","Customer services are critical to all companies, as they may directly connect to the brand reputation. Due to a great number of customers, e-commerce companies often employ multiple communication channels to answer customers' questions, for example, Chatbot and Hotline. On one hand, each channel has limited capacity to respond to customers' requests; on the other hand, customers have different preferences over these channels. The current production systems are mainly built based on business rules that merely consider the tradeoffs between the resources and customers' satisfaction. To achieve the optimal tradeoff between the resources and customers' satisfaction, we propose a new framework based on deep reinforcement learning that directly takes both resources and user model into account. In addition to the framework, we also propose a new deep-reinforcement-learning-based routing method-double dueling deep Q-learning with prioritized experience replay (PER-DoDDQN). We evaluate our proposed framework and method using both synthetic and a real customer service log data from a large financial technology company. We show that our proposed deep-reinforcement-learning-based framework is superior to the existing production system. Moreover, we also show that our proposed PER-DoDDQN is better than all other deep Q-learning variants in practice, which provides a more optimal routing plan. These observations suggest that our proposed method can seek the tradeoff, where both channel resources and customers' satisfaction are optimal.","2169-3536","","10.1109/ACCESS.2019.2932047","National Natural Science Foundation of China(grant numbers:61503217); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784156","Deep reinforcement learning;personalized customer service;time-series data processing","Customer services;Routing;Companies;Reinforcement learning;Channel capacity","customer satisfaction;customer services;electronic commerce;learning (artificial intelligence);marketing data processing;personal information systems;user modelling","e-commerce companies;multiple communication channels;user model;financial technology company;deep-reinforcement-learning-based framework;deep Q-learning variants;channel resources;personalized customer service request stream routing;brand reputation;customer questions;business rules;customer satisfaction;double dueling deep Q-learning with prioritized experience replay;PER-DoDDQN;customer service log data;optimal routing plan","","4","","44","CCBY","1 Aug 2019","","","IEEE","IEEE Journals"
"Demand Response Management for Industrial Facilities: A Deep Reinforcement Learning Approach","X. Huang; S. H. Hong; M. Yu; Y. Ding; J. Jiang","Department of Electronic Engineering, Hanyang University, Ansan, South Korea; School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China; Department of Electronic Engineering, Hanyang University, Ansan, South Korea; School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China; Department of Electronic Engineering, Hanyang University, Ansan, South Korea","IEEE Access","4 Jul 2019","2019","7","","82194","82205","As a major consumer of energy, the industrial sector must assume the responsibility for improving energy efficiency and reducing carbon emissions. However, most existing studies on industrial energy management are suffering from modeling complex industrial processes. To address this issue, a model-free demand response (DR) scheme for industrial facilities was developed. In practical terms, we first formulated the Markov decision process (MDP) for industrial DR, which presents the composition of the state, action, and reward function in detail. Then, we designed an actor-critic-based deep reinforcement learning algorithm to determine the optimal energy management policy, where both the actor (Policy) and the critic (Value function) are implemented by the deep neural network. We then confirmed the validity of our scheme by applying it to a real-world industry. Our algorithm identified an optimal energy consumption schedule, reducing energy costs without compromising production.","2169-3536","","10.1109/ACCESS.2019.2924030","National Research Foundation of Korea(grant numbers:NRF-2016K2A9A2A11938310); National Research Foundation of Korea(grant numbers:NRF-2018K1A3A1A61026320); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8742652","Artificial intelligence;deep reinforcement learning;demand response (DR);industrial facilities;actor-critic","Industrial facilities;Reinforcement learning;Electric vehicles;Energy consumption;Load management","air pollution control;cost reduction;demand side management;energy conservation;energy consumption;learning (artificial intelligence);Markov processes;neural nets;power engineering computing","demand response management;industrial facilities;deep reinforcement learning approach;industrial sector;energy efficiency;reducing carbon emissions;industrial energy management;complex industrial processes;Markov decision process;industrial DR;deep neural network;optimal energy consumption schedule","","37","","53","CCBY","20 Jun 2019","","","IEEE","IEEE Journals"
"A Reinforcement Learning Approach to Solve Service Restoration and Load Management Simultaneously for Distribution Networks","L. R. Ferreira; A. R. Aoki; G. Lambert-Torres","Electrical Engineering Department, Federal University of Parana, Curitiba, Brazil; Electrical Engineering Department, Federal University of Parana, Curitiba, Brazil; Research and Development Department, Gnarus Institute, Itajuba, Brazil","IEEE Access","16 Oct 2019","2019","7","","145978","145987","Energy and economy are increasing the relationship over the years, where the energy becomes a significant resource to keep a country developing, and it supports its economy. Then, more reliable the energy should become, especially the distribution network, to keep the entire process running. In this level of energy distribution, where residential consumers and medium and small industries are supplied, the number of interconnections of the network is enormous. However, for economic and environmental aspects, these complex systems, which are operating close to their capacity, needs to increase the automation, appearing the concept of smart grids and the Advanced Distribution Management System (ADMS) and its methods to control. Inside of the ADMS, there are a lot of essential techniques. Among them, there are two techniques which are the most relevant for this paper: the self-healing and load management. In an ADMS system, these two techniques are treated separately, but the best solution occurs when they are computed together. In this paper, it is proposed an approach that can address both problems at the same time or individually, i.e., in place to have a sequential method to solve step-by-step the issues in the networks. The proposed algorithm, through reinforcement learning technique, can handle both problems together. The proposed approach is tested in a real urban distribution network with some created scenarios to compare the results with outages and overloads. Some comparisons with other methods are carried out.","2169-3536","","10.1109/ACCESS.2019.2946282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8862818","Computational intelligence;load management;q-learning;power distribution;power operation;reinforcement learning;self-healing","Mathematical model;Reinforcement learning;Load management;Computational modeling;Indexes;Topology;Reliability","learning (artificial intelligence);load management;power distribution control;power distribution economics;power distribution planning;power distribution reliability;power engineering computing;power system restoration;smart power grids","complex systems;environmental aspects;economic aspects;residential consumers;energy distribution;developing country;significant resource;economy;distribution networks;service restoration;reinforcement learning approach;urban distribution network;reinforcement learning technique;ADMS system;load management;essential techniques;advanced distribution management system","","22","","28","CCBY","8 Oct 2019","","","IEEE","IEEE Journals"
"Cooperative Partial Task Offloading and Resource Allocation for IIoT Based on Decentralized Multi-Agent Deep Reinforcement Learning","F. Zhang; G. Han; L. Liu; Y. Zhang; Y. Peng; C. Li","College of Information Science and Engineering, Hohai University, Changzhou, China; College of Information Science and Engineering, Hohai University, Changzhou, China; College of Information Science and Engineering, Hohai University, Changzhou, China; Department of Aeronautical and Automotive Engineering, Loughborough University, UK; Research Institute of USV Engineering, Shanghai University, Shanghai, China; Zhejiang lab, Zhejiang, China","IEEE Internet of Things Journal","","2023","PP","99","1","1","Edge computing has become increasingly important to fulfill the diversified quality of service (QoS) or quality of experience (QoE) demands for industrial internet of things (IIoT) applications, such as machine condition monitoring, fault diagnosis, intelligent production scheduling, and production quality control. Due to the heterogeneity of IIoT systems, it is of urgent necessity to concentrate on the cloud-edge-end cooperative partial task offloading and resource allocation (CPTORA) problem for realizing workload balancing, efficient resource utilization, and better QoS/QoE of IIoT applications. However, the challenge lies in how to make real-time, accurate, decentralized task offloading and resource allocation decisions for dynamic and device-intensive IIoT. Therefore, this work examines the CPTORA problem for IIoT, aiming at minimizing its long-run overall delay and energy costs. To lower the problem complexity, this problem is decomposed into the task offloading subproblem and the resource allocation subproblem. Then, an improved soft actor-critic-based decentralized multi-agent deep reinforcement learning (MADRL) algorithm is proposed to address the task offloading subproblem, where each IIoT device can learn its globally optimal policy and make its decisions independently. This algorithm innovatively combines the divergence regularization, the distributional reinforcement learning, and the value function decomposition methods to improve convergence speed and accuracy of the existing MADRL methods. After receiving the task offloading decisions of every IIoT device, every edge server employs the Lagrange multiplier method and Karush-Kuhn-Tucker condition to solve its resource allocation subproblem. The experimental results show that the proposed algorithm decreases the overall delay and energy costs more effectively, compared to the other state-of-the-art MADRL approaches.","2327-4662","","10.1109/JIOT.2023.3306803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10225334","Improved soft actor-critic;cooperative task offloading;resource allocation;multi-agent deep reinforcement learning;IIoT","Industrial Internet of Things;Task analysis;Resource management;Heuristic algorithms;Delays;Costs;Quality of service","","","","","","","IEEE","21 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Intelligent Rack-Level Cooling Management in Data Centers With Active Ventilation Tiles: A Deep Reinforcement Learning Approach","J. Wan; J. Zhou; X. Gui","Inner Mongolia University of Technology, Hohhot, China; Inner Mongolia University of Technology, Hohhot, China; Massey University, Palmerston North, New Zealand","IEEE Intelligent Systems","23 Dec 2021","2021","36","6","42","52","The raised-floor architecture is widely adopted in the current data center industry. Due to the structural design and workload imbalance, many server racks suffer from a nonuniform inlet temperature in raised-floor data centers. This compromises not only the cooling performance, but also the computing capacity and system reliability. In this article, we employ the active ventilation tiles (AVTs), i.e., ordinary ventilation tiles with attached fans, to enhance the local cold air delivery and improve the cooling performance. We propose an AVT control algorithm adapted from the recently developed deep reinforcement learning (DRL) techniques to tackle the complex data center environment and thermodynamics. Different from previous studies where the algorithm required extensive pretraining before deployment, our algorithm is fully online. In order to improve the sample efficiency and accelerate the learning speed, we integrate the Dyna architecture to take advantage of the experienced system transitions. We also leverage the idea of shared reward and fingerprint to encourage the cooperation for multi-AVT control. The performance of the proposed solution is then evaluated by deploying a prototype implementation in a production data center. Experimental results reveal that our solution significantly improves the rack inlet temperature distribution.","1941-1294","","10.1109/MIS.2021.3049865","National Natural Science Foundation of China(grant numbers:61862048,61762070,61962045); Inner Mongolia Key Technological Development Program(grant numbers:2019ZD015); Key Scientific and Technological Research Program of Inner Mongolia Autonomous Region(grant numbers:2019GG273); Inner Mongolia Autonomous Region Special Program for Engineering Application of Scientific and Technological Researches(grant numbers:2020CG0073); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9319244","Data center cooling management;active ventilation tiles;deep reinforcement learning","Data centers;Servers;Temperature distribution;Reinforcement learning;Reliability;Prototypes;Ventilation","","","","7","","17","IEEE","11 Jan 2021","","","IEEE","IEEE Magazines"
"Privacy-Preserving Deep Reinforcement Learning in Vehicle Ad Hoc Networks","U. Ahmed; J. C. -W. Lin; G. Srivastava","Western Norway University of Applied Sciences, Norway; Western Norway University of Applied Sciences, Norway; Brandon University, Canada","IEEE Consumer Electronics Magazine","10 Oct 2022","2022","11","6","41","48","The increasing number of road vehicles results in more fatalities and accidents. Thus, the manufacturing industry is working on driver safety to secure and safe transportation in Vehicle Ad hoc networks. In addition, the mobile vehicles run in the geographical zone and communicate roadside units over the wireless medium with a certain radius. The Internet of Vehicles has become a new network type where vehicles communicate with the application over public networks. This results in an increase in data exploration and threats related to network security. We propose the deep reinforcement learning method to sensitize the private information for a given vehicle connect over Vehicle Ad hoc networks, maintaining a balance between security and privacy through any sanitization process. Furthermore, we provide a set of recommendations and potential applications for the Vehicle Ad hoc networks as use cases.","2162-2256","","10.1109/MCE.2021.3088408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9451559","","Privacy;Security;Wireless communication;Internet;Data privacy;Wireless sensor networks;Safety;Internet of Vehicles","data privacy;deep learning (artificial intelligence);reinforcement learning;telecommunication security;vehicular ad hoc networks","deep reinforcement learning method;Internet of Vehicles;mobile vehicles;network security;privacy-preserving deep reinforcement learning;road vehicles results;Vehicle Ad hoc networks;wireless medium","","6","","16","IEEE","10 Jun 2021","","","IEEE","IEEE Magazines"
"Applicability and Challenges of Deep Reinforcement Learning for Satellite Frequency Plan Design","J. J. G. Luis; E. Crawley; B. Cameron","Massasachusetts Institute of Technology, Cambridge, MA; Massasachusetts Institute of Technology, Cambridge, MA; Massasachusetts Institute of Technology, Cambridge, MA","2021 IEEE Aerospace Conference (50100)","7 Jun 2021","2021","","","1","11","The study and benchmarking of Deep Reinforcement Learning (DRL) models has become a trend in many industries, including aerospace engineering and communications. Recent studies in these fields propose these kinds of models to address certain complex real-time decision-making problems in which classic approaches do not meet time requirements or fail to obtain optimal solutions. While the good performance of DRL models has been proved for specific use cases or scenarios, most studies do not discuss the compromises and generalizability of such models during real operations. In this paper we explore the tradeoffs of different elements of DRL models and how they might impact the final performance. To that end, we choose the Frequency Plan Design (FPD) problem in the context of multibeam satellite constellations as our use case and propose a DRL model to address it. We identify six different core elements that have a major effect in its performance: the policy, the policy optimizer, the state, action, and reward representations, and the training environment. We analyze different alternatives for each of these elements and characterize their effect. We also use multiple environments to account for different scenarios in which we vary the dimensionality or make the environment non-stationary. Our findings show that DRL is a potential method to address the FPD problem in real operations, especially because of its speed in decision-making. However, no single DRL model is able to outperform the rest in all scenarios, and the best approach for each of the six core elements depends on the features of the operation environment. While we agree on the potential of DRL to solve future complex problems in the aerospace industry, we also reflect on the importance of designing appropriate models and training procedures, understanding the applicability of such models, and reporting the main performance tradeoffs.","1095-323X","978-1-7281-7436-5","10.1109/AERO50100.2021.9438291","SES; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9438291","","Training;Satellite constellations;Adaptation models;Satellites;Decision making;Reinforcement learning;Real-time systems","artificial satellites;decision making;deep learning (artificial intelligence);satellite communication;telecommunication computing","single DRL model;real-time decision-making problems;frequency plan design problem;deep reinforcement learning","","2","","30","IEEE","7 Jun 2021","","","IEEE","IEEE Conferences"
"An IoT-Based Intelligent Selection of Multidomain Feature for Smart Healthcare Using Reinforcement Learning in Schizophrenia","X. Li; H. Huang","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Internet of Things Journal","19 Oct 2023","2023","10","21","18517","18528","Currently, the health industry by the Internet of Things (IoT) is developing rapidly, and electroencephalogram (EEG) signal has become a bridge for human–machine communication. EEG signal feature selection is a key link in brain nerves research and has important practical application value. Therefore, we propose an IoT-based intelligent selection of multidomain feature for multimodal transformer EEG signal using reinforcement learning in Schizophrenia. First of all, the features of the multimodal EEG signal sequence by the IoT were extracted from time domain, frequency domain, and spatial domain; second, the average pooling layer was added to improve the design of the transformer model for deep feature extraction of EEG signal; finally, reinforcement learning was introduced to run operation with the extracted features as input and the improved transformer model as agent. At the same time, entropy and Pearson correlation coefficient calculation were introduced to select feature subsets, and feature intelligent selection of EEG signal in multiple domains was completed through interaction between the agent and the environment. Experiments were performed on DEAP, EEG Motor Movement/Imagery Data set, BCI2008 competition data set, and data set of a hospital. The results show that the feature extraction and feature visualization effect of the proposed algorithm is good, and the feature selection precision is as high as about 90%, and the average time is only about 13 s. These findings indicate the feasibility of the proposed feature intelligent selection algorithm in the study of EEG signal in schizophrenia, which provides a good basis for further study on the integration of IoT with the healthy industry.","2327-4662","","10.1109/JIOT.2023.3281509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10144441","Intelligent selection;Internet of Things (IoT);multidomain feature;multimodal electroencephalogram (EEG) signal;reinforcement learning;schizophrenia;transformer model","Feature extraction;Electroencephalography;Mental disorders;Reinforcement learning;Brain modeling;Transformers;Internet of Things","","","","","","37","IEEE","5 Jun 2023","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Energy-Aware Area Coverage for Reconfigurable hRombo Tiling Robot","A. V. Le; R. Parween; P. T. Kyaw; R. E. Mohan; T. H. Q. Minh; C. S. C. S. Borusu","ROAR lab, Engineering Product Development Pillar, Singapore University of Technology and Design, Singapore; ROAR lab, Engineering Product Development Pillar, Singapore University of Technology and Design, Singapore; ROAR lab, Engineering Product Development Pillar, Singapore University of Technology and Design, Singapore; ROAR lab, Engineering Product Development Pillar, Singapore University of Technology and Design, Singapore; Optoelectronics Research Group, Faculty of Electrical and Electronics Engineering, Ton Duc Thang University, Ho Chi Minh, Vietnam; ROAR lab, Engineering Product Development Pillar, Singapore University of Technology and Design, Singapore","IEEE Access","1 Dec 2020","2020","8","","209750","209761","Applying the automation in covering the areas entirely eases manual jobs in various domestic fields such as site investigation, search, rescue, security, cleaning, and maintenance. A self-reconfigurable robot with adjustable dimensions is a viable answer to improve the coverage percentage for predefined map areas. However, the shape-shifting of this robot class also adds to the complexity of locomotion components and the need for an optimal complete coverage strategy for this new type of robot. The typical complete coverage route, including the least times of shape-shifting, the shortest navigation route, and the minimum travel time, is presented in the article. By splitting the map into the sub-areas similar to the self-reconfigurable robot's available shapes, the robot can design the ideal tileset and optimal navigation strategies to cover the workspace. To this end, we propose a Complete Tileset Energy-Aware Coverage Path Planning (CTPP) framework for a tiling self-reconfigurable robot named hRombo with four rhombus-shaped modules. The robot can reconfigure its base structure into seven distinct forms by activating the servo motors to drive the three robot hinges connecting robot modules. The problem of optimal path planning assisting the proposed hRombo robot to clear optimally all predefined tiles within the arbitrary workspace is considered a classic Travel Salesman Problem (TSP), and this TSP is solved by the reinforcement learning (RL) approach. The RL's reward function and action space are based on robot kinematic and the required energies, including transformation, translation, and orientation actions, to move the robot inside the workspace. The CTPP for the hRombo robot is validated with conventional complete coverage methods in simulation and real workspace conditions. The results showed that the CTPP is suitable for producing Pareto plans that enable robots to navigate from source to target in different workspaces with the least consumed energy and time among considered methods.","2169-3536","","10.1109/ACCESS.2020.3038905","National Robotics Programme under its Robotics Enabling Capabilities and Technologies(grant numbers:192 25 00051); National Robotics Programme under its Robot Domain Specific(grant numbers:192 22 00058); Agency for Science, Technology, and Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9262896","Reconfigurable robot;tiling robotic;reinforcement learning;complete coverage planning;energy path planning","Robots;Shape;Navigation;Robot kinematics;Path planning;Wheels;Servomotors","learning (artificial intelligence);mobile robots;multi-robot systems;path planning;robot kinematics;travelling salesman problems","rhombus-shaped modules;base structure;reinforcement learning-based energy-aware area coverage;complete tileset energy-aware coverage path planning;optimal navigation strategies;minimum travel time;shortest navigation route;optimal complete coverage strategy;robot class;shape-shifting;predefined map areas;coverage percentage;reconfigurable hRombo tiling robot;conventional complete coverage methods;required energies;reinforcement learning approach;classic Travel Salesman Problem;clear optimally all predefined tiles;optimal path;robot modules;robot hinges","","22","","47","CCBY","18 Nov 2020","","","IEEE","IEEE Journals"
"Optimizing Time-Sensitive Software-Defined Wireless Networks With Reinforcement Learning","H. Joo; S. Lee; S. Lee; H. Kim","Department of Electrical Engineering, Korea University, Seoul, Republic of Korea; Department of Electrical Engineering, Korea University, Seoul, Republic of Korea; Department of Smart Convergence, Korea University, Seoul, Republic of Korea; Department of Electrical Engineering, Korea University, Seoul, Republic of Korea","IEEE Access","18 Nov 2022","2022","10","","119496","119505","Even though wireless networks are inevitable in mobile or infrastructure-less communication systems, such as vehicle-to-everything (V2X) infrastructure in automobile, precise formation control of unmanned vehicles (UVs), or other industries that employ ad hoc deployment of systems, operation and maintenance of network applications additionally impose time constraints on the wireless network. Such the requirement poses an immediate challenge to the time-sensitive aspects of devices, applications and network control, which has been addressed in the realm of time-sensitive networking (TSN). Meanwhile, software-defined networking (SDN) has successfully presented its efficiencies in ensuring quality of service for network traffic to accommodate many functions of network control and management. In this regard, we propose a traffic engineering solution based on reinforcement learning (RL) to implement TSN links with SDN over a wireless network, then optimize the quality of TSN links, and protect background traffic from excessive resource allocation for TSN-enabled but SDN-supported traffic. We implemented SDN-based TSN on a real testbed, consisting of real nodes as single board computers (SBCs) and an SDN controller, and applied RL-based network control solution to the network. The empirical results are promising in that the jitter of time-constrained traffic is improved by 24.6% and throughput of background traffic is increased by 6.5%, compared to the manual configuration mode.","2169-3536","","10.1109/ACCESS.2022.3222060","Unmanned Swarm CPS Research Laboratory Program of Defense Acquisition Program Administration and Agency for Defense Development(grant numbers:UD220005VD); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950062","Reinforcement learning;time-sensitive network;resource allocation;traffic control","Resource management;Wireless networks;Bandwidth;Jitter;Reinforcement learning;Packet loss;Throughput;Telecommunication traffic;Software defined networking","quality of service;radio links;reinforcement learning;resource allocation;software defined networking;telecommunication traffic","background traffic;network applications;network traffic;precise formation control;reinforcement learning;RL-based network control solution;SDN controller;SDN-based TSN;SDN-supported traffic;software-defined networking;time constraints;time-constrained traffic;time-sensitive networking;time-sensitive software-defined wireless networks;TSN links;vehicle-to-everything infrastructure;wireless network","","","","26","CCBYNCND","14 Nov 2022","","","IEEE","IEEE Journals"
"Hierarchical Reinforcement Learning With Universal Policies for Multistep Robotic Manipulation","X. Yang; Z. Ji; J. Wu; Y. -K. Lai; C. Wei; G. Liu; R. Setchi","Center for Artificial Intelligence, Robotics and Human-Machine Systems (IROHMS), School of Engineering, Cardiff University, Cardiff, U.K.; Center for Artificial Intelligence, Robotics and Human-Machine Systems (IROHMS), School of Engineering, Cardiff University, Cardiff, U.K.; School of Computer Science and Informatics, Cardiff University, Cardiff, U.K.; School of Computer Science and Informatics, Cardiff University, Cardiff, U.K.; Department of Robotics Engineering, Hohai University, Changzhou, China; School of Control Science and Engineering, Shandong University, Jinan, China; Center for Artificial Intelligence, Robotics and Human-Machine Systems (IROHMS), School of Engineering, Cardiff University, Cardiff, U.K.","IEEE Transactions on Neural Networks and Learning Systems","1 Sep 2022","2022","33","9","4727","4741","Multistep tasks, such as block stacking or parts (dis)assembly, are complex for autonomous robotic manipulation. A robotic system for such tasks would need to hierarchically combine motion control at a lower level and symbolic planning at a higher level. Recently, reinforcement learning (RL)-based methods have been shown to handle robotic motion control with better flexibility and generalizability. However, these methods have limited capability to handle such complex tasks involving planning and control with many intermediate steps over a long time horizon. First, current RL systems cannot achieve varied outcomes by planning over intermediate steps (e.g., stacking blocks in different orders). Second, the exploration efficiency of learning multistep tasks is low, especially when rewards are sparse. To address these limitations, we develop a unified hierarchical reinforcement learning framework, named Universal Option Framework (UOF), to enable the agent to learn varied outcomes in multistep tasks. To improve learning efficiency, we train both symbolic planning and kinematic control policies in parallel, aided by two proposed techniques: 1) an auto-adjusting exploration strategy (AAES) at the low level to stabilize the parallel training, and 2) abstract demonstrations at the high level to accelerate convergence. To evaluate its performance, we performed experiments on various multistep block-stacking tasks with blocks of different shapes and combinations and with different degrees of freedom for robot control. The results demonstrate that our method can accomplish multistep manipulation tasks more efficiently and stably, and with significantly less memory consumption.","2162-2388","","10.1109/TNNLS.2021.3059912","China Scholarship Council (CSC)(grant numbers:201908440400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9366328","Hierarchical reinforcement learning (HRL);multistep tasks;option framework (OF);planning and control;robotic manipulation;universal policy","Task analysis;Planning;Robots;Standards;Training;Reinforcement learning;Stacking","intelligent robots;learning (artificial intelligence);manipulators;mobile robots;motion control","multistep tasks;symbolic planning;kinematic control policies;multistep block-stacking tasks;robot control;manipulation tasks;Universal policies;multistep robotic manipulation;block stacking;autonomous robotic manipulation;robotic system;reinforcement learning-based methods;robotic motion control;intermediate steps;current RL systems;varied outcomes;unified hierarchical reinforcement learning framework;named Universal Option Framework","","11","","33","IEEE","1 Mar 2021","","","IEEE","IEEE Journals"
"Reinforcement Learning of Space Robotic Manipulation with Multiple Safety Constraints","L. Li; Y. Xie; Y. Wang; A. Chen","Beijing Institute of Control Engineering, Beijing, P. R. China; Beijing Institute of Control Engineering, Beijing, P. R. China; Beijing Institute of Control Engineering, Beijing, P. R. China; Beijing Institute of Control Engineering, Beijing, P. R. China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","7376","7382","To execute numerous space robotic manipulation tasks (for example on-orbit capture, refueling, assembly, etc.) reliably in presence of high environment uncertainty and multi-source obstacles, it is critical to incorporate safety design into trajectory optimization. Model-based planning approaches are commxonly used in the literature, yet suffer insufficient robustness to model uncertainty. In this paper, we study the sample-based, model-free learning of safe space robotic manipulation. We consider numerous practical on-orbit working conditions and propose a novel algorithm called Safe policy optimization with multiple constraints (SPOMC). It incorporates two constraints (namely energy constraint and geometric constraint) into the constrained policy optimization framework, maximizing expected total reward while maintaining a bounded expected total cost. Numerical simulation on rigid robotic arm manipulation at randomized base positions shows the effectiveness of SPOMC.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902690","Safety;Space robotic manipulation;Sample-based trajectory optimization;Reinforcement learning;Constrained Markov decision processes","Uncertainty;Process control;Reinforcement learning;Reliability engineering;Numerical simulation;Robustness;Safety","aerospace robotics;control engineering computing;manipulators;optimisation;position control;reinforcement learning","constrained policy optimization framework;energy constraint;environment uncertainty;geometric constraint;model-based planning;model-free learning;multiple safety constraints;multisource obstacles;numerical simulation;on-orbit working conditions;randomized base positions;reinforcement learning;rigid robotic arm manipulation;robustness;safe policy optimization;safety design;space robotic manipulation;SPOMC;trajectory optimization","","","","27","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Satellite Communications: From LEO to Deep Space Operations","P. V. R. Ferreira; R. Paffenroth; A. M. Wyglinski; T. M. Hackett; S. G. Bilen; R. C. Reinhart; D. J. Mortensen",Worcester Polytechnic Institute; Worcester Polytechnic Institute; Worcester Polytechnic Institute; The Pennsylvania State University; The Pennsylvania State University; NASA John H. Glenn Research Center; NASA John H. Glenn Research Center,"IEEE Communications Magazine","13 May 2019","2019","57","5","70","75","The National Aeronautics and Space Administration (NASA) is in the midst of defining and developing the future space and ground architecture for the coming decades to return science and exploration discovery data back to investigators on Earth. Optimizing the data return from these missions requires planning, design, standards, and operations coordinated from formulation and development throughout the mission. The use of automation enhanced by cognition and machine learning are potential methods for optimizing data return, reducing costs of operations, and helping manage the complexity of the automated systems. In this article, we discuss the potential role of machine learning in the linkto- link aspect of the communication systems. An experiment using NASA's Space Communication and Navigation Testbed onboard the International Space Station and the ground station located at NASA John H. Glenn Research Center demonstrates for the first time the benefits and challenges of applying machine learning to space links in the actual flight environment. The experiment used machine learning decisions to configure a space link from the ISS-based testbed to the ground station to achieve multiple objectives related to data throughput, bandwidth, and power. Aspects of the specific neural-network-based reinforcement learning algorithm formation and on-orbit testing are discussed.","1558-1896","","10.1109/MCOM.2019.1800796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8713802","","Space vehicles;Satellite communication;Machine learning;Space stations;Aerospace electronics;NASA;Satellite broadcasting;Reinforcement learning;Dtaa science;Low Earth Orbit","aerospace computing;learning (artificial intelligence);satellite communication;space communication links","reinforcement learning;satellite communications;ground architecture;data return;automated systems;communication systems;International Space Station;ground station;NASA John H. Glenn Research Center;space link;space communication;machine learning;deep space operations;link-to-link aspect;neural-network","","30","","15","IEEE","13 May 2019","","","IEEE","IEEE Magazines"
"Toward a Fully Automated Artificial Pancreas System Using a Bioinspired Reinforcement Learning Design: In Silico Validation","S. Lee; J. Kim; S. W. Park; S. -M. Jin; S. -M. Park","Department of Creative IT Engineering, Pohang University of Science and Technology (POSTECH), Pohang, South Korea; Department of Electrical Engineering, Pohang University of Science and Technology (POSTECH), Pohang, South Korea; Division of Endocrinology and Metabolism, Department of Medicine, Samsung Medical Center, Sungkyunkwan University School of Medicine, Seoul, South Korea; Division of Endocrinology and Metabolism, Department of Medicine, Samsung Medical Center, Sungkyunkwan University School of Medicine, Seoul, South Korea; Department of Creative IT Engineering, Pohang University of Science and Technology (POSTECH), Pohang, South Korea","IEEE Journal of Biomedical and Health Informatics","5 Feb 2021","2021","25","2","536","546","Objective: The automation of insulin treatment is the most challenge aspect of glucose management for type 1 diabetes owing to unexpected exogenous events (e.g., meal intake). In this article, we propose a novel reinforcement learning (RL) based artificial intelligence (AI) algorithm for a fully automated artificial pancreas (AP) system. Methods: A bioinspired RL designing method was developed for automated insulin infusion. This method has reward functions that imply the temporal homeostatic objective and discount factors that reflect an individual specific pharmacological characteristic. The proposed method was applied to a training method using an RL algorithm and was evaluated in virtual patients from the FDA approved UVA/Padova simulator with unannounced meal intakes. Results: For a single-meal experiment with preprandial fasting, the trained policy demonstrated fully automated regulation in both the basal and postprandial phases. In the in silico trial with a variation of insulin sensitivity and dawn phenomenon, the policy achieved a mean glucose of 124.72 mg/dL and percentage time in the normal range of 89.56%. The layer-wise relevance propagation provides interpretable information on AI-driven decision for robustness to sensor noise, automated postprandial regulation, and insulin stacking avoidance. Conclusion: The AP algorithm based on the bioinspired RL approach enables fully automated blood glucose control with unannounced meal intake. Significance: The proposed framework can be extended to other drug-based treatments for systems with significant uncertainties.","2168-2208","","10.1109/JBHI.2020.3002022","ICT Consilience Creative(grant numbers:IITP-2019-2011-1-00783); National Research Founcdation of Korea; Korea government(grant numbers:NRF-2017R1A5A1015596,2020R1A2C2005385); Pohang Iron & Steel Co. Ltd, Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9115809","Artificial pancreas;artificial intelligent;automated insulin treatment;diabetes;reinforcement learning","Glucose;Insulin;Regulation;Optimization;Pancreas;Artificial intelligence;Uncertainty","artificial organs;blood;diseases;learning (artificial intelligence);medical computing;patient treatment;sugar","fully automated artificial pancreas system;automated insulin infusion;single-meal experiment;insulin sensitivity;automated postprandial regulation;insulin stacking avoidance;drug-based treatments;bioinspired reinforcement learning design;insulin treatment;blood glucose management;type 1 diabetes;UVA-Padova simulator;artificial intelligence algorithm","Algorithms;Artificial Intelligence;Blood Glucose;Computer Simulation;Diabetes Mellitus, Type 1;Humans;Hypoglycemic Agents;Insulin;Insulin Infusion Systems;Pancreas, Artificial","30","","64","IEEE","12 Jun 2020","","","IEEE","IEEE Journals"
"Dot-to-Dot: Explainable Hierarchical Reinforcement Learning for Robotic Manipulation","B. Beyret; A. Shafti; A. A. Faisal","Dept. of Computing, Brain and Behaviour Lab; Dept. of Computing, Brain and Behaviour Lab; Dept. of Computing, Brain and Behaviour Lab","2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","28 Jan 2020","2019","","","5014","5019","Robotic systems are ever more capable of automation and fulfilment of complex tasks, particularly with reliance on recent advances in intelligent systems, deep learning and artificial intelligence in general. However, as robots and humans come closer together in their interactions, the matter of interpretability, or explainability of robot decision-making processes for the human grows in importance. A successful interaction and collaboration would only be possible through mutual understanding of underlying representations of the environment and the task at hand. This is currently a challenge in deep learning systems. We present a hierarchical deep reinforcement learning system, consisting of a low-level agent handling the large actions/states space of a robotic system efficiently, by following the directives of a high-level agent which is learning the high-level dynamics of the environment and task. This high-level agent forms a representation of the world and task at hand that is interpretable for a human operator. The method, which we call Dot-to-Dot, is tested on a MuJoCo-based model of the Fetch Robotics Manipulator, as well as a Shadow Hand, to test its performance. Results show efficient learning of complex actions/states spaces by the low-level agent, and an interpretable representation of the task and decision-making process learned by the high-level agent.","2153-0866","978-1-7281-4004-9","10.1109/IROS40897.2019.8968488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8968488","","","decision making;explanation;intelligent robots;learning (artificial intelligence);manipulators;multi-agent systems","Dot-to-Dot;explainable hierarchical reinforcement;robotic manipulation;intelligent systems;artificial intelligence;explainability;robot decision-making processes;deep reinforcement learning system;low-level agent;Fetch Robotics Manipulator;high-level agent;MuJoCo-based model;Shadow Hand;intelligent robotic agent","","19","","25","IEEE","28 Jan 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-based Task Offloading in Satellite-Terrestrial Edge Computing Networks","D. Zhu; H. Liu; T. Li; J. Sun; J. Liang; H. Zhang; L. Geng; Y. Liu","Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China","2021 IEEE Wireless Communications and Networking Conference (WCNC)","5 May 2021","2021","","","1","7","In remote regions (e.g., mountain and desert), cellular networks are usually sparsely deployed or unavailable. With the appearance of new applications (e.g., industrial automation and environment monitoring) in remote regions, resource-constrained terminals become unable to meet the latency requirements. Meanwhile, offloading tasks to urban terrestrial cloud (TC) via satellite link will lead to high delay. To tackle above issues, Satellite Edge Computing architecture is proposed, i.e., users can offload computing tasks to visible satellites for executing. However, existing works are usually limited to offload tasks in pure satellite networks, and make offloading decisions based on the predefined models of users. Besides, the runtime consumption of existing algorithms is rather high.In this paper, we study the task offloading problem in satellite-terrestrial edge computing networks, where tasks can be executed by satellite or urban TC. The proposed Deep Reinforcement learning-based Task Offloading (DRTO) algorithm can accelerate learning process by adjusting the number of candidate locations. In addition, offloading location and bandwidth allocation only depend on the current channel states. Simulation results show that DRTO achieves near-optimal offloading cost performance with much less runtime consumption, which is more suitable for satellite-terrestrial network with fast fading channel.","1558-2612","978-1-7281-9505-6","10.1109/WCNC49053.2021.9417127","Research and Development; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9417127","Satellite-terrestrial networks;Edge computing;Deep reinforcement learning;Computation offloading;Mixed-integer programming","Fading channels;Satellites;Runtime;Heuristic algorithms;Simulation;Reinforcement learning;Channel allocation","bandwidth allocation;cellular radio;cloud computing;fading channels;geophysics computing;learning (artificial intelligence);mobile computing;optimisation;satellite links","satellite edge computing architecture;deep reinforcement learning-based task offloading;satellite-terrestrial network;near-optimal offloading cost performance;task offloading problem;offloading decisions;pure satellite networks;visible satellites;satellite link;urban terrestrial cloud;cellular networks;remote regions;satellite-terrestrial edge computing networks","","19","","27","IEEE","5 May 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Scalable and Reliable Power Allocation in SDN-based Backscatter Heterogeneous Network","F. Jameel; W. U. Khan; M. A. Jamshed; H. Pervaiz; Q. Abbasi; R. Jäntti","Department of Communications and Networking, Aalto University, Espoo, Finland; School of Information Science and Engineering, Shandong University, Qingdao, People's Republic of China; Institute of Communication Systems (ICS), Home of 5G Innovation Centre (5GIC), University of Surrey, UK; School of Computing and Communications, Lancaster University, UK; School of Engineering, University of Glasgow, Glasgow, UK; Department of Communications and Networking, Aalto University, Espoo, Finland","IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)","10 Aug 2020","2020","","","1069","1074","Backscatter heterogeneous networks are expected to usher a new era of massive connectivity of low-powered devices. With the integration of software-defined networking (SDN), such networks hold the promise to be a key enabling technology for massive Internet-of-things (IoT) due to myriad applications in industrial automation, healthcare, and logistics management. However, there are many aspects of SDN-based backscatter heterogeneous networks that need further development before practical realization. One of the challenging aspects is the high level of interference due to the reuse of spectral resources for backscatter communications. To partly address this issue, this article provides a reinforcement learning-based solution for effective interference management when backscatter tags coexist with other legacy devices in a heterogeneous network. Specifically, using reinforcement learning, the agents are trained to minimize the interference for macro-cell (legacy users) and small-cell (backscatter tags). Novel reward functions for both macro- and small-cells have been designed that help in controlling the transmission power levels of users. The results show that the proposed framework not only improves the performance of macro-cell users but also fulfills the quality of service requirements of backscatter tags by optimizing the long-term rewards.","","978-1-7281-8695-5","10.1109/INFOCOMWKSHPS50562.2020.9162720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162720","Backscatter communications;Internet-of-things (IoT);Interference management;Reinforcement learning","Backscatter;Interference;Heterogeneous networks;Learning (artificial intelligence);Radio frequency;Optical transmitters;Signal to noise ratio","cellular radio;computer network reliability;Internet of Things;learning (artificial intelligence);next generation networks;quality of service;radiofrequency interference;software defined networking","backscatter tags;reliable power allocation;SDN-based backscatter heterogeneous network;backscatter heterogeneous networks;low-powered devices;software-defined networking;backscatter communications;reinforcement learning-based solution","","17","","26","IEEE","10 Aug 2020","","","IEEE","IEEE Conferences"
"Personalized and Automatic Model Repairing using Reinforcement Learning","A. Barriga; A. Rutle; R. Heldal","Department of Software Engineering, Western Norway University of Applied Sciences, Bergen, Norway; Department of Software Engineering, Western Norway University of Applied Sciences, Bergen, Norway; Department of Software Engineering, Western Norway University of Applied Sciences, Bergen, Norway","2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)","21 Nov 2019","2019","","","175","181","When performing modeling activities, the chances of breaking a model increase together with the size of development teams and number of changes in software specifications. Model repair research mostly proposes two different solutions to this issue: fully automatic, non-interactive model repairing tools or support systems where the repairing choice is left to the developer's criteria. In this paper, we propose the use of reinforcement learning algorithms to achieve the repair of broken models allowing both automation and personalization. We validate our proposal by repairing a large set of broken models randomly generated with a mutation tool.","","978-1-7281-5125-0","10.1109/MODELS-C.2019.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8904758","Model repair;Reinforcement learning;Personalization","","formal specification;learning (artificial intelligence)","broken models;reinforcement learning;software specifications;model repair research;noninteractive model repairing tools;automatic model repairing;personalized model repairing;mutation tool","","12","","25","IEEE","21 Nov 2019","","","IEEE","IEEE Conferences"
"Multi-Agent Collaborative Exploration through Graph-based Deep Reinforcement Learning","T. Luo; B. Subagdja; D. Wang; A. -H. Tan","School of Computer Science and Engineering, Nanyang Technological University; ST Engineering-NTU Corporate Laboratory, Nanyang Technological University; Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY), Nanyang Technological University; School of Computer Science and Engineering, Nanyang Technological University","2019 IEEE International Conference on Agents (ICA)","12 Dec 2019","2019","","","2","7","Autonomous exploration by a single or multiple agents in an unknown environment leads to various applications in automation, such as cleaning, search and rescue, etc. Traditional methods normally take frontier locations and segmented regions of the environment into account to efficiently allocate target locations to different agents to visit. They may employ ad hoc solutions to allocate the task to the agents, but the allocation may not be efficient. In the literature, few studies focused on enhancing the traditional methods by applying machine learning models for agent performance improvement. In this paper, we propose a graph-based deep reinforcement learning approach to effectively perform multi-agent exploration. Specifically, we first design a hierarchical map segmentation method to transform the environment exploration problem to the graph domain, wherein each node of the graph corresponds to a segmented region in the environment and each edge indicates the distance between two nodes. Subsequently, based on the graph structure, we apply a Graph Convolutional Network (GCN) to allocate the exploration target to each agent. Our experiments show that our proposed model significantly improves the efficiency of map explorations across varying sizes of collaborative agents over the traditional methods.","","978-1-7281-4026-1","10.1109/AGENTS.2019.8929168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8929168","Graph convolutional networks;Reinforcement learning;Multi-agent map exploration;Multi-robot system","Machine learning;Task analysis;Resource management;Subspace constraints;Robot kinematics;Clustering algorithms","graph theory;learning (artificial intelligence);multi-agent systems","graph corresponds;segmented region;graph structure;graph convolutional network;exploration target;map explorations;collaborative agents;multiagent collaborative exploration;autonomous exploration;single agents;multiple agents;unknown environment;frontier locations;target locations;ad hoc solutions;machine learning models;agent performance improvement;graph-based deep reinforcement learning approach;multiagent exploration;hierarchical map segmentation method;environment exploration problem;graph domain","","11","","20","IEEE","12 Dec 2019","","","IEEE","IEEE Conferences"
