"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"C-3PO: Cyclic-Three-Phase Optimization for Human-Robot Motion Retargeting based on Reinforcement Learning","T. Kim; J. -H. Lee","Dept. of Comp. SW and Eng., Korea University of Science and Technology, Daejeon, Rep. of Korea; Human-Robot Interaction Research Group, ETRI, Daejeon, Rep. of Korea","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","8425","8432","Motion retargeting between heterogeneous polymorphs with different sizes and kinematic configurations requires a comprehensive knowledge of (inverse) kinematics. Moreover, it is non-trivial to provide a kinematic independent general solution. In this study, we developed a cyclic three-phase optimization method based on deep reinforcement learning for human-robot motion retargeting. The motion retargeting learning is performed using refined data in a latent space by the cyclic and filtering paths of our method. In addition, the human- in-the-loop based three-phase approach provides a framework for the improvement of the motion retargeting policy by both quantitative and qualitative manners. Using the proposed C- 3PO method, we were successfully able to learn the motion retargeting skill between the human skeleton and motion of the multiple robots such as NAO, Pepper, Baxter and C-3PO.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196948","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196948","","Skeleton;Robot motion;Robot kinematics;Kinematics;Zirconium;Torso","human-robot interaction;learning (artificial intelligence);mobile robots;motion control;multi-robot systems","human-robot motion retargeting;kinematic configurations;kinematic independent general solution;three-phase optimization method;deep reinforcement learning;motion retargeting learning;motion retargeting policy;motion retargeting skill;human skeleton;cyclic-three-phase optimization;NAO robot;Pepper robot;Baxter robot;C-3PO robot","","4","","51","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Redundant Robot Control Using Multi Agent Reinforcement Learning","A. Perrusquía; W. Yu; X. Li","Departamento de Control Automatico, CINVESTAV-IPN (National Polytechnic Institute) Av.IPN 2508, Mexico City, Mexico; Departamento de Control Automatico, CINVESTAV-IPN (National Polytechnic Institute) Av.IPN 2508, Mexico City, Mexico; Departamento de Computacion, CINVESTAV-IPN (National Polytechnic Institute), Mexico City, Mexico","2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)","8 Oct 2020","2020","","","1650","1655","Robot control in task-space1 needs the inverse kinematics and Jacobian matrix. They are not available for redundant robots, because there are so many degrees-of-freedom (DOF). Intelligent learning methods, such as neural networks (NN) and reinforcement learning (RL) can learn them. However, NN needs big data and RL is not suitable for multilink robots as the redundant robots. In this paper, we propose a full cooperative multi-agent reinforcement learning (MARL) to solve the above problems. Each joint of the robot is regarded as one agent. Although the dimension of the learning space is very large, the full cooperative MARL uses the kinematic learning and avoids the function approximators in large learning space. The experimental results show that our MARL is much more better compared with the classic methods such as, Jacobian-based methods and neural networks.1Task-space (or Cartesian space) is defined by the position and orientation of the end effector of a robot. Joint-space is defined by angular displacements of each joint of a robot.","2161-8089","978-1-7281-6904-0","10.1109/CASE48305.2020.9216774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9216774","","Kinematics;Jacobian matrices;Task analysis;Aerospace electronics;Robot control;Learning (artificial intelligence)","attitude control;Big Data;end effectors;function approximation;Jacobian matrices;learning (artificial intelligence);multi-agent systems;neural nets;position control;redundant manipulators","intelligent learning methods;NN;RL;multiagent reinforcement learning;MARL;kinematic learning;Jacobian-based methods;Cartesian space;joint-space;redundant robot control;inverse kinematics;Jacobian matrix;degrees-of-freedom;neural networks;big data;end effectors;position control;orientation control","","4","","40","IEEE","8 Oct 2020","","","IEEE","IEEE Conferences"
"Local Policy Optimization for Trajectory-Centric Reinforcement Learning","P. Kolaric; D. K. Jha; A. U. Raghunathan; F. L. Lewis; M. Benosman; D. Romeres; D. Nikovski","UTA Research Institute, University of Texas at Arlington, Fort Worth, TX, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA; UTA Research Institute, University of Texas at Arlington, Fort Worth, TX, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","5094","5100","The goal of this paper is to present a method for simultaneous trajectory and local stabilizing policy optimization to generate local policies for trajectory-centric model-based reinforcement learning (MBRL). This is motivated by the fact that global policy optimization for non-linear systems could be a very challenging problem both algorithmically and numerically. However, a lot of robotic manipulation tasks are trajectory-centric, and thus do not require a global model or policy. Due to inaccuracies in the learned model estimates, an open-loop trajectory optimization process mostly results in very poor performance when used on the real system. Motivated by these problems, we try to formulate the problem of trajectory optimization and local policy synthesis as a single optimization problem. It is then solved simultaneously as an instance of nonlinear programming. We provide some results for analysis as well as achieved performance of the proposed technique under some simplifying assumptions.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197058","","Robustness;Trajectory optimization;Uncertainty;Learning (artificial intelligence);Robots","control engineering computing;learning (artificial intelligence);manipulators;nonlinear control systems;nonlinear programming;open loop systems","local policy optimization;trajectory-centric reinforcement learning;local stabilizing policy optimization;trajectory-centric model-based reinforcement learning;global policy optimization;nonlinear systems;robotic manipulation tasks;open-loop trajectory optimization;local policy synthesis;single optimization problem;nonlinear programming","","4","1","28","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Imitative Reinforcement Learning Fusing Vision and Pure Pursuit for Self-driving","M. Peng; Z. Gong; C. Sun; L. Chen; D. Cao","School of Data and Computer Science, Sun Yat-sen University, China; School of Data and Computer Science, Sun Yat-sen University, China; Waterloo CogDrive Lab, University of Waterloo, Canada; School of Data and Computer Science, Sun Yat-sen University, China; Waterloo CogDrive Lab, University of Waterloo, Canada","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","3298","3304","Autonomous urban driving navigation is still an open problem and has ample room for improvement in unknown complex environments and terrible weather conditions. In this paper, we propose a two-stage framework, called IPP-RL, to handle these problems. IPP means an Imitation learning method fusing visual information with the additional steering angle calculated by Pure-Pursuit (PP) method, and RL means using Reinforcement Learning for further training. In our IPP model, the visual information captured by camera can be compensated by the calculated steering angle, thus it could perform well under bad weather conditions. However, imitation learning performance is limited by the driving data severely. Thus we use a reinforcement learning method-Deep Deterministic Policy Gradient (DDPG)-in the second stage training, which shares the learned weights from pretrained IPP model. In this way, our IPP-RL can lower the dependency of imitation learning on demonstration data and solve the problem of low exploration efficiency caused by randomly initialized weights in reinforcement learning. Moreover, we design a more reasonable reward function and use the n-step return to update the critic-network in DDPG. Our experiments on CARLA driving benchmark demonstrate that our IPP-RL is robust to lousy weather conditions and shows remarkable generalization capability in unknown environments on navigation task.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197027","","Learning (artificial intelligence);Meteorology;Robustness;Task analysis;Navigation;Training;Autonomous vehicles","generalisation (artificial intelligence);intelligent robots;learning (artificial intelligence);road vehicles;sensor fusion;steering systems;traffic engineering computing","pretrained IPP model;CARLA driving benchmark;generalization capability;pure pursuit;autonomous urban driving navigation;two-stage framework;visual information;pure-pursuit method;steering angle;imitation learning performance;driving data;reinforcement learning method;deep deterministic policy gradient;IPP-RL framework","","4","","22","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Weakly Supervised Disentangled Representation for Goal-Conditioned Reinforcement Learning","Z. Qian; M. You; H. Zhou; B. He","College of Electronic and Information Engineering, Frontiers Science Center for Intelligent Autonomous Systems, Tongji University, Shanghai, China; College of Electronic and Information Engineering, Frontiers Science Center for Intelligent Autonomous Systems, Tongji University, Shanghai, China; College of Electronic and Information Engineering, Frontiers Science Center for Intelligent Autonomous Systems, Tongji University, Shanghai, China; College of Electronic and Information Engineering, Frontiers Science Center for Intelligent Autonomous Systems, Tongji University, Shanghai, China","IEEE Robotics and Automation Letters","24 Jan 2022","2022","7","2","2202","2209","Goal-conditioned reinforcement learning is a crucial yet challenging algorithm which enables agents to achieve multiple user-specified goals when learning a set of skills in a dynamic environment. However, it typically requires millions of the environmental interactions explored by agents, which is sample-inefficient. In the letter, we propose a skill learning framework DR-GRL that aims to improve the sample efficiency and policy generalization by combining the Disentangled Representation learning and Goal-conditioned visual Reinforcement Learning. In a weakly supervised manner, we propose a Spatial Transform AutoEncoder (STAE) to learn an interpretable and controllable representation in which different parts correspond to different object attributes (shape, color, position). Due to the high controllability of the representations, STAE can simply recombine and recode the representations to generate unseen goals for agents to practice themselves. The manifold structure of the learned representation maintains consistency with the physical position, which is beneficial for reward calculation. We empirically demonstrate that DR-GRL significantly outperforms the previous methods in sample efficiency and policy generalization. In addition, DR-GRL is also easy to expand to the real robot.","2377-3766","","10.1109/LRA.2022.3141148","National Natural Science Foundation of China(grant numbers:62073244); Shanghai Innovation Action Plan(grant numbers:20511100500,20511105802); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9677980","Deep learning for visual perception;reinforcement learning;representation learning","Visualization;Image color analysis;Transforms;Training;Task analysis;Shape;Reinforcement learning","image representation;multi-agent systems;neural nets;supervised learning","interpretable representation;controllable representation;DR-GRL;sample efficiency;policy generalization;user-specified goals;goal-conditioned visual reinforcement learning;agents;environmental interaction;skill learning framework;weakly supervised disentangled representation learning;spatial transform autoencoder;STAE;reward calculation","","4","","36","IEEE","11 Jan 2022","","","IEEE","IEEE Journals"
"Relationship Between the Order for Motor Skill Transfer and Motion Complexity in Reinforcement Learning","N. J. Cho; S. H. Lee; I. H. Suh; H. -S. Kim","Hanyang University, Seongdong-gu, Seoul, KR; Korea Institute of Industrial Technology, Cheonan, KR; Hanyang University, Seongdong-gu, Seoul, KR; Korea Institute of Industrial Technology, Cheonan, KR","IEEE Robotics and Automation Letters","9 Jan 2019","2019","4","2","293","300","We propose a method to generate an order for learning and transferring motor skills based on motion complexity, then evaluate the order to learn motor skills of a task and transfer them to another task as a form of reinforcement learning (RL). Here, motion complexity refers to the complexity calculated from multiple motion trajectories of a task. To do this, multiple human demonstrations are extracted and clustered to calculate motion complexity and identify the motor skills involved in a task. The motion trajectories of the task are then used to calculate the motion complexity considering temporal entropy and spatial entropy. Finally, both orders [Simple-to-Complex] and [Complex-to-Simple] are generated to learn and transfer motor skills based on the motion complexities of multiple tasks. To evaluate these orders, two tasks [Drawing] and [Fitting] are performed using an actual robotic arm. To verify the learning and transfer processes, we apply our method to three different figures as well as to pegs and holes of three different shapes and analyze the experimental results. In addition, we provide guidelines for using the [Simple-to-Complex] and [Complex-to-Simple] orders in RL.","2377-3766","","10.1109/LRA.2018.2889026","Technology Innovation Industrial Program; Ministry of Trade (MI, South Korea)(grant numbers:10048320&10073161); MSIT; Institute for Information and communications Technology Promotion(grant numbers:2018-0-00622); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8585125","Motor skill transfer;ordering;motion complexity;reinforcement learning;robot manipulation","Task analysis;Complexity theory;Entropy;Robots;Trajectory;Clustering algorithms","entropy;learning (artificial intelligence);manipulators;motion control","motor skill transfer;motion complexity;reinforcement learning;learning transferring motor skills;multiple motion trajectories;transfer processes;RL;actual robotic arm","","4","","18","IEEE","21 Dec 2018","","","IEEE","IEEE Journals"
"Batch Exploration With Examples for Scalable Robotic Reinforcement Learning","A. S. Chen; H. Nam; S. Nair; C. Finn","Computer Science Department, Stanford University, Boulder, CO, USA; Computer Science Department, Stanford University, Boulder, CO, USA; Computer Science Department, Stanford University, Boulder, CO, USA; Computer Science Department, Stanford University, Boulder, CO, USA","IEEE Robotics and Automation Letters","9 Apr 2021","2021","6","3","4401","4408","Learning from diverse offline datasets is a promising path towards learning general purpose robotic agents. However, a core challenge in this paradigm lies in collecting large amounts of meaningful data, while not depending on a human in the loop for data collection. One way to address this challenge is through task-agnostic exploration, where an agent attempts to explore without a task-specific reward function, and collect data that can be useful for any subsequent task. While these approaches have shown some promise in simple domains, they often struggle to explore the relevant regions of the state space in more challenging settings, such as vision-based robotic manipulation. This challenge stems from an objective that encourages exploring everything in a potentially vast state space. To mitigate this challenge, we propose to focus exploration on the important parts of the state space using weak human supervision. Concretely, we propose an exploration technique, Batch Exploration with Examples (BEE), that explores relevant regions of the state-space, guided by a modest number of human-provided images of important states. These human-provided images only need to be provided once at the beginning of data collection and can be acquired in a matter of minutes, allowing us to scalably collect diverse datasets, which can then be combined with any batch RL algorithm. We find that BEE is able to tackle challenging vision-based manipulation tasks both in simulation and on a real Franka Emika Panda robot, and observe that compared to task-agnostic and weakly-supervised exploration techniques, it (1) interacts more than twice as often with relevant objects, and (2) improves subsequent task performance when used in conjunction with offline RL.","2377-3766","","10.1109/LRA.2021.3068655","Schmidt Futures; ONR(grant numbers:N00014-20-1-2675); NSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9385945","Deep learning methods;reinforcement learning","Deep learning;Reinforcement learning;Data collection","","","","4","","51","IEEE","24 Mar 2021","","","IEEE","IEEE Journals"
"SoMoGym: A Toolkit for Developing and Evaluating Controllers and Reinforcement Learning Algorithms for Soft Robots","M. A. Graule; T. P. McCarthy; C. B. Teeple; J. Werfel; R. J. Wood","John A. Paulson School of Engineering and Applied Sciences, Harvard University, Allston, MA, USA; John A. Paulson School of Engineering and Applied Sciences, Harvard University, Allston, MA, USA; John A. Paulson School of Engineering and Applied Sciences, Harvard University, Allston, MA, USA; John A. Paulson School of Engineering and Applied Sciences, Harvard University, Allston, MA, USA; John A. Paulson School of Engineering and Applied Sciences, Harvard University, Allston, MA, USA","IEEE Robotics and Automation Letters","23 Feb 2022","2022","7","2","4071","4078","Soft robotsoffer a host of benefits over traditional rigid robots, including inherent compliance that lets them passively adapt to variable environments and operate safely around humans and fragile objects. However, that same compliance makes it hard to use model-based methods in planning tasks requiring high precision or complex actuation sequences. Reinforcement learning (RL) can potentially find effective control policies, but training RL using physical soft robots is often infeasible, and training using simulations has had a high barrier to adoption. To accelerate research in control and RL for soft robotic systems, we introduce SoMoGym (Soft Motion Gym), a software toolkit that facilitates training and evaluating controllers for continuum robots. SoMoGym provides a set of benchmark tasks in which soft robots interact with various objects and environments. It allows evaluation of performance on these tasks for controllers of interest, and enables the use of RL to generate new controllers. Custom environments and robots can likewise be added easily. We provide and evaluate baseline RL policies for each of the benchmark tasks. These results show that SoMoGym enables the use of RL for continuum robots, a class of robots not covered by existing benchmarks, giving them the capability to autonomously solve tasks that were previously unattainable.","2377-3766","","10.1109/LRA.2022.3149580","National Science Foundation(grant numbers:EFMA-1830901); Space Technology Research Institutes(grant numbers:80NSSC19K1076); NASA’s Space Technology Research Grants Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9707663","Soft robot applications;modeling;control;and learning for soft robots;reinforcement learning","Task analysis;Robots;Benchmark testing;Soft robotics;Manipulators;Actuators;Training","control engineering computing;learning (artificial intelligence);mobile robots;pneumatic actuators","SoMoGym;controller evaluation;reinforcement learning algorithms;traditional rigid robots;inherent compliance;variable environments;fragile objects;model-based methods;complex actuation sequences;effective control policies;training RL;physical soft robots;high barrier;soft robotic systems;Soft Motion Gym;software toolkit;continuum robots;benchmark tasks;baseline RL","","4","","36","IEEE","8 Feb 2022","","","IEEE","IEEE Journals"
"Improving the Robustness of Reinforcement Learning Policies With ${\mathcal {L}_{1}}$ Adaptive Control","Y. Cheng; P. Zhao; F. Wang; D. J. Block; N. Hovakimyan","Mechanical Science and Engineering Department, University of Illinois at Urbana-Champaign, IL, USA; Mechanical Science and Engineering Department, University of Illinois at Urbana-Champaign, IL, USA; Mechanical Science and Engineering Department, University of Illinois at Urbana-Champaign, IL, USA; Electrical and Computer Engineering Department, University of Illinois at Urbana-Champaign, IL, USA; Mechanical Science and Engineering Department, University of Illinois at Urbana-Champaign, IL, USA","IEEE Robotics and Automation Letters","23 May 2022","2022","7","3","6574","6581","A reinforcement learning (RL) control policy could fail in a new/perturbed environment that is different from the training environment, due to the presence of dynamic variations. For controlling systems with continuous state and action spaces, we propose an add-on approach to robustifying a pre-trained RL policy by augmenting it with an ${\mathcal {L}_{1}}$ adaptive controller (${\mathcal {L}_{1}}$AC). Leveraging the capability of an ${\mathcal {L}_{1}}$AC for fast estimation and active compensation of dynamic variations, the proposed approach can improve the robustness of an RL policy which is trained either in a simulator or in the real world without consideration of a broad class of dynamic variations. Numerical and real-world experiments empirically demonstrate the efficacy of the proposed approach in robustifying RL policies trained using both model-free and model-based methods.","2377-3766","","10.1109/LRA.2022.3169309","Air Force Office of Scientific Research; NSF(grant numbers:2133656,1830639); AI Institute Planning(grant numbers:2020289); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761728","Reinforcement learning;machine learning for robot control;robust/adaptive control;robot safety","Adaptation models;Training;Numerical models;Adaptive control;Uncertainty;Robustness;Predictive models","adaptive control;control system analysis computing;reinforcement learning","reinforcement learning control policy;training environment;dynamic variations;continuous state;L1 adaptive controller;robustifying RL policies;L1 AC;pretrained RL policy","","3","","35","CCBY","21 Apr 2022","","","IEEE","IEEE Journals"
"A New Sample-Efficient PAC Reinforcement Learning Algorithm","A. Zehfroosh; H. G. Tanner","Department of Mechanical Engineering, University of Delaware; Department of Mechanical Engineering, University of Delaware","2020 28th Mediterranean Conference on Control and Automation (MED)","1 Sep 2020","2020","","","788","793","This paper introduces a new hybrid PAC RL algorithm for MDPS, which intelligently maintains favorable features of its parents. The DDQ algorithm, integrates model-free and model-based learning approaches, preserving some advantages from both. A PAC analysis of the DDQ algorithm is presented and its sample complexity is explicitly bounded. Numerical results from a small-scale example motivated by work on human-robot interaction models corroborates the theoretical predictions on sample complexity.","2473-3504","978-1-7281-5742-9","10.1109/MED48518.2020.9182985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9182985","","Picture archiving and communication systems;Computational modeling;Complexity theory;Approximation algorithms;Brain modeling;Numerical models;Heuristic algorithms","computational complexity;human-robot interaction;learning (artificial intelligence);Markov processes","sample-efficient PAC reinforcement;learning algorithm;hybrid PAC RL algorithm;MDPS;favorable features;DDQ algorithm;model-free;learning approaches;PAC analysis;sample complexity;human-robot interaction models","","3","","23","IEEE","1 Sep 2020","","","IEEE","IEEE Conferences"
"Inferring Human-Robot Performance Objectives During Locomotion Using Inverse Reinforcement Learning and Inverse Optimal Control","W. Liu; J. Zhong; R. Wu; B. L. Fylstra; J. Si; H. H. Huang","UNC/NCSU Department of Biomedical Engineering, North Carolina State University, Raleigh, NC, USA; School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; UNC/NCSU Department of Biomedical Engineering, North Carolina State University, Raleigh, NC, USA; School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; UNC/NCSU Department of Biomedical Engineering, North Carolina State University, Raleigh, NC, USA","IEEE Robotics and Automation Letters","31 Jan 2022","2022","7","2","2549","2556","Quantitatively characterizing a locomotion performance objective for a human-robot system is an important consideration in the assistive wearable robot design towards human-robot symbiosis. This problem, however, has only been addressed sparsely in the literature. In this study, we propose a new inverse approach from observed human-robot walking behavior to infer a human-robot collective performance objective represented in a quadratic form. By an innovative design of human experiments and simulation study, respectively, we validated the effectiveness of two solution approaches to solving the inverse problem using inverse reinforcement learning (IRL) and inverse optimal control (IOC). The IRL-based experiments of human walking with robotic transfemoral prosthesis validated the realistic applicability of the proposed inverse approach, while the IOC-based analysis provided important human-robot system properties such as stability and robustness that are difficult to obtain from human experiments. This study introduces a new tool to the field of wearable lower limb robots. It is expected to be expandable to quantify joint human-robot locomotion performance objectives for personalizing wearable robot control in the future.","2377-3766","","10.1109/LRA.2022.3143579","National Science Foundation(grant numbers:1563454,1563921,1808752,1808898); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684732","Learning from demonstration;reinforcement learning;wearable robotics","Legged locomotion;Robots;Prosthetics;Knee;Cost function;Visualization;Wearable robots","gait analysis;human-robot interaction;inverse problems;learning (artificial intelligence);legged locomotion;man-machine systems;medical robotics;mobile robots;optimal control;patient rehabilitation;prosthetics;robots","inferring human-robot performance objectives;inverse reinforcement learning;inverse optimal control;locomotion performance objective;assistive wearable robot design;human-robot symbiosis;inverse approach;observed human-robot walking behavior;human-robot collective performance objective;innovative design;human experiments;inverse problem;IRL-based experiments;human walking;robotic transfemoral prosthesis;important human-robot system properties;wearable lower limb robots;joint human-robot locomotion performance objectives;wearable robot control","","3","","22","IEEE","18 Jan 2022","","","IEEE","IEEE Journals"
"Living Object Grasping Using Two-Stage Graph Reinforcement Learning","Z. Hu; Y. Zheng; J. Pan","Department of Biomedical Engineering, The City University of Hong Kong, Kowloon Tong, Hong Kong; Tencent Robotics X, Shenzhen, Guangdong, China; Department of Computer Science, The University of Hong Kong, Pokfulam, Hong Kong","IEEE Robotics and Automation Letters","9 Mar 2021","2021","6","2","1950","1957","Living objects are hard to grasp because they can actively dodge and struggle by writhing or deforming while or even prior to being contacted and modeling or predicting their responses to grasping is extremely difficult. This letter presents an algorithm based on reinforcement learning (RL) to attack this challenging problem. Considering the complexity of living object grasping, we divide the whole task into pre-grasp and in-hand stages and let the algorithm switch between the stages automatically. The pre-grasp stage is aimed at finding a good pose of a robot hand approaching a living object for performing a grasp. Dense reward functions are proposed for facilitating the learning of right hand actions based on the poses of both hand and object. Since an object held in hand may struggle to escape, the robot hand needs to adjust its configuration and respond correctly to the object's movement. Hence, the goal of the in-hand stage is to determine an appropriate adjustment of finger configuration in order for the robot hand to keep holding the object. At this stage, we treat the robot hand as a graph and use the graph convolutional network (GCN) to determine the hand action. We test our algorithm with both simulation and real experiments, which show its good performance in living object grasping. More results are available on our website: https://sites.google.com/view/graph-rl.","2377-3766","","10.1109/LRA.2021.3060636","HKSAR; General Research Fund(grant numbers:17204115,21203216); NSFC/RGC(grant numbers:HKU103/16); Innovation and Technology Fund(grant numbers:ITS/457/17FP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359355","Deep learning in grasping and manipulation;dexterous manipulation;grasping;in-hand manipulation;reinforcement learning","Robots;Grasping;Robot kinematics;Task analysis;Reinforcement learning;Robot sensing systems;Force","dexterous manipulators;graph theory;grippers;learning (artificial intelligence);robot programming","two-stage graph reinforcement learning;living object;in-hand stage;pre-grasp stage;robot hand;hand action;living object grasping","","3","","18","IEEE","19 Feb 2021","","","IEEE","IEEE Journals"
"TRC: Trust Region Conditional Value at Risk for Safe Reinforcement Learning","D. Kim; S. Oh","Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, South Korea","IEEE Robotics and Automation Letters","31 Jan 2022","2022","7","2","2621","2628","As safety is of paramount importance in robotics, reinforcement learning that reflects safety, called safe RL, has been studied extensively. In safe RL, we aim to find a policy which maximizes the desired return while satisfying the defined safety constraints. There are various types of constraints, among which constraints on conditional value at risk (CVaR) effectively lower the probability of failures caused by high costs since CVaR is a conditional expectation obtained above a certain percentile. In this paper, we propose a trust region-based safe RL method with CVaR constraints, called TRC. We first derive the upper bound on CVaR and then approximate the upper bound in a differentiable form in a trust region. Using this approximation, a subproblem to get policy gradients is formulated, and policies are trained by iteratively solving the subproblem. TRC is evaluated through safe navigation tasks in simulations with various robots and a sim-to-real environment with a Jackal robot from Clearpath. Compared to other safe RL methods, the performance is improved by 1.93 times while the constraints are satisfied in all experiments.","2377-3766","","10.1109/LRA.2022.3141829","Institute of Information & Communications Technology Planning & Evaluation(grant numbers:2019-0-01190); National Research Foundation(grant numbers:NRF-2017R1A2B2006136); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9677982","Collision avoidance;reinforcement learning;robot safety","Costs;Upper bound;Robots;Task analysis;Markov processes;Reinforcement learning;Random variables","control engineering computing;gradient methods;probability;reinforcement learning;robot programming;safety","safe reinforcement learning;robotics;CVaR constraints;TRC;policy gradients;safe navigation tasks;Jackal robot;trust region conditional value at risk;trust region-based safe RL","","3","","26","IEEE","11 Jan 2022","","","IEEE","IEEE Journals"
"Human-Feedback Shield Synthesis for Perceived Safety in Deep Reinforcement Learning","D. Marta; C. Pek; G. I. Melsión; J. Tumova; I. Leite","Division of Robotics, Perception and Learning, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Robotics, Perception and Learning, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Robotics, Perception and Learning, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Robotics, Perception and Learning, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Robotics, Perception and Learning, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden","IEEE Robotics and Automation Letters","26 Nov 2021","2022","7","1","406","413","Despite the successes of deep reinforcement learning (RL), it is still challenging to obtain safe policies. Formal verification approaches ensure safety at all times, but usually overly restrict the agent’s behaviors, since they assume adversarial behavior of the environment. Instead of assuming adversarial behavior, we suggest to focus on perceived safety instead, i.e., policies that avoid undesired behaviors while having a desired level of conservativeness. To obtain policies that are perceived as safe, we propose a shield synthesis framework with two distinct loops: (1) an inner loop that trains policies with a set of actions that is constrained by shields whose conservativeness is parameterized, and (2) an outer loop that presents example rollouts of the policy to humans and collects their feedback to update the parameters of the shields in the inner loop. We demonstrate our approach on a RL benchmark of Lunar landing and a scenario in which a mobile robot navigates around humans. For the latter, we conducted two user studies to obtain policies that were perceived as safe. Our results indicate that our framework converges to policies that are perceived as safe, is robust against noisy feedback, and can query feedback for multiple policies at the same time.","2377-3766","","10.1109/LRA.2021.3128237","Vinnova Competence Center for Trustworthy Edge Computing Systems and Applications; Kungliga Tekniska Högskolan; Wallenberg AI, Autonomous Systems, and Software Program; Knut och Alice Wallenbergs Stiftelse; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9616473","Safety in HRI;human factors and human-in-the-loop;reinforcement learning","Safety;Robots;Reinforcement learning;Task analysis;Noise measurement;Feedback loop;Training","feedback;formal verification;learning (artificial intelligence);mobile robots;path planning","trains policies;distinct loops;shield synthesis framework;adversarial behavior;agent;formal verification;safe policies;deep reinforcement learning;perceived safety;feedback shield synthesis;multiple policies;noisy feedback;RL benchmark;inner loop;shields;outer loop","","3","","26","IEEE","16 Nov 2021","","","IEEE","IEEE Journals"
"Constructive Policy: Reinforcement Learning Approach for Connected Multi-Agent Systems","S. J. A. Raza; M. Lin","Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA; Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA","2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)","19 Sep 2019","2019","","","257","262","Policy based reinforcement learning methods are widely used for multi-agent systems to learn optimal actions given any state; with partial or even no model representation. However multi-agent systems with complex structures (curse of dimensionality) or with high constraints (like bio-inspired (a) snake or serpentine robots) show limited performance in such environments due to sparse-reward nature of environment and no fully observable model representation. In this paper we present a constructive learning and planning scheme that reduces the complexity of high-diemensional agent model by decomposing it into identical, connected and scaled down multiagent structure and then apply learning framework in layers of local and global ranking. Our layered hierarchy method also decomposes the final goal into multiple sub-tasks and a global task (final goal) that is bias-induced function of local sub-tasks. Local layer deals with learning `reusable' local policy for a local agent to achieve a sub-task optimally; that local policy can also be reused by other identical local agents. Furthermore, global layer learns a policy to apply right combination of local policies that are parameterized over entire connected structure of local agents to achieve the global task by collaborative construction of local agents. After learning local policies and while learning global policy, the framework generates sub-tasks for each local agent, and accepts local agents' intrinsic rewards as positive bias towards maximum global reward based of optimal sub-tasks assignments. The advantage of proposed approach includes better exploration due to decomposition of dimensions, and reusability of learning paradigm over extended dimension spaces. We apply the constructive policy method to serpentine robot with hyper-redundant degrees of freedom (DOF), for achieving optimal control and we also outline connection to hierarchical apprenticeship learning methods which can be seen as layered learning framework for complex control tasks.","2161-8089","978-1-7281-0356-3","10.1109/COASE.2019.8843223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8843223","","Robots;Task analysis;Aerospace electronics;Multi-agent systems;Biological system modeling;Reinforcement learning;Computational modeling","learning (artificial intelligence);multi-agent systems","reinforcement learning approach;connected multiagent systems;policy based reinforcement learning methods;fully observable model representation;constructive learning;planning scheme;high-diemensional agent model;constructive policy method;hierarchical apprenticeship learning methods;layered learning framework;complex control tasks","","3","","43","IEEE","19 Sep 2019","","","IEEE","IEEE Conferences"
"Deep Imitative Reinforcement Learning for Temporal Logic Robot Motion Planning with Noisy Semantic Observations","Q. Gao; M. Pajic; M. M. Zavlanos","Duke University, Durham, NC, USA; Duke University, Durham, NC, USA; Duke University, Durham, NC, USA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","8490","8496","In this paper, we propose a Deep Imitative Q-learning (DIQL) method to synthesize control policies for mobile robots that need to satisfy Linear Temporal Logic (LTL) specifications using noisy semantic observations of their surroundings. The robot sensing error is modeled using probabilistic labels defined over the states of a Labeled Transition System (LTS) and the robot mobility is modeled using a Labeled Markov Decision Process (LMDP) with unknown transition probabilities. We use existing product-based model checkers (PMCs) as experts to guide the Q-learning algorithm to convergence. To the best of our knowledge, this is the first approach that models noise in semantic observations using probabilistic labeling functions and employs existing model checkers to provide suboptimal instructions to the Q-learning agent.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197297","","Robot sensing systems;Labeling;Uncertainty;Semantics;Probabilistic logic","Markov processes;mobile robots;motion control;neurocontrollers;path planning;probability;temporal logic","noisy semantic observations;mobile robots;linear temporal logic specifications;robot sensing error;probabilistic labels;labeled transition system;robot mobility;labeled Markov decision process;unknown transition probabilities;product-based model checkers;probabilistic labeling functions;Q-learning agent;deep imitative reinforcement learning;temporal logic robot motion planning;deep imitative Q-learning method;DIQL;control policies synthesis;LTL;LMDP;suboptimal instructions","","2","","41","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Dynamic Actor-Advisor Programming for Scalable Safe Reinforcement Learning","L. Zhu; Y. Cui; T. Matsubara","Graduate School of Science and Technology, Nara Institute of Science and Technology (NAIST), Nara, Japan; Graduate School of Science and Technology, Nara Institute of Science and Technology (NAIST), Nara, Japan; Graduate School of Science and Technology, Nara Institute of Science and Technology (NAIST), Nara, Japan","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","10681","10687","Real-world robots have complex strict constraints. Therefore, safe reinforcement learning algorithms that can simultaneously minimize the total cost and the risk of constraint violation are crucial. However, almost no algorithms exist that can scale to high-dimensional systems to the best of our knowledge. In this paper, we propose Dynamic Actor-Advisor Programming (DAAP), as an algorithm for sample-efficient and scalable safe reinforcement learning. DAAP employs two control policies, actor and advisor. They are updated to minimize total cost and risk of constraint violation intertwiningly and smoothly towards each other's direction by using the other as the baseline policy in the Kullback-Leibler divergence of Dynamic Policy Programming framework. We demonstrate the scalability and sample efficiency of DAAP through its application on simulated robot arm control tasks with performance comparisons to baselines.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197200","","Dynamic programming;Programming;Robots;Learning (artificial intelligence);Heuristic algorithms;Task analysis;Training","learning (artificial intelligence);mobile robots","scalable safe reinforcement learning;real-world robots;complex strict constraints;safe reinforcement learning algorithms;high-dimensional systems;DAAP;sample efficiency;dynamic actor-advisor programming;dynamic policy programming framework;constraint violation risk","","2","","23","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Aerobatic Tic-Toc Control of Planar Quadcopters via Reinforcement Learning","Z. Wang; R. Groß; S. Zhao","Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.; School of Engineering, Westlake University, Hangzhou, China","IEEE Robotics and Automation Letters","24 Jan 2022","2022","7","2","2140","2147","This letter studies aerobatic tic-toc control of quadcopters. Tic-toc control enables rotorcraft to fly almost in the vertical plane rather than the horizontal plane. It is one of the most challenging manoeuvrers to achieve autonomously. The problem has to our knowledge not yet been studied for quadcopters. Studying it could expand their flight envelope and improve their performance in extreme, aerobatic flight tasks. In this letter, we employ a deep deterministic gradient policy approach to train reinforcement learning (RL) controllers based on carefully designed rewards. The obtained RL controllers are shown to generate two flight modes, spin and tic-toc. We analyse the properties of these flight modes and screen out unfavourable RL controllers. The qualified RL controller is then enhanced by combining it with PID and LQR controllers which achieves better flight performance and enables the quadcopter to track a moving reference point and recover to hovering flight status. Physical simulations using Simscape are presented to verify the proposed approach.","2377-3766","","10.1109/LRA.2022.3142730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681212","Flight control;reinforcement learning;variable-pitch propeller quadcopter","Artificial neural networks;Force;Reinforcement learning;Propellers;Training;Autonomous aerial vehicles;Aerodynamics","aircraft control;autonomous aerial vehicles;control system synthesis;gradient methods;helicopters;learning (artificial intelligence);linear quadratic control;mobile robots;vehicle dynamics","aerobatic tic-toc control;planar quadcopters;quadcopter;vertical plane;horizontal plane;flight envelope;extreme flight tasks;aerobatic flight tasks;deep deterministic gradient policy approach;reinforcement learning controllers;RL controller;flight modes;unfavourable RL controllers;flight performance;hovering flight status","","1","","15","IEEE","13 Jan 2022","","","IEEE","IEEE Journals"
"Predicting optimal value functions by interpolating reward functions in scalarized multi-objective reinforcement learning","A. Kusari; J. P. How","Research and Advanced Engineering, Ford Motor Company, Dearborn, MI, USA; Department of Aeronautics and Astronautics, MIT, Cambridge, MA, USA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","7484","7490","A common approach for defining a reward function for multi-objective reinforcement learning (MORL) problems is the weighted sum of the multiple objectives. The weights are then treated as design parameters dependent on the expertise (and preference) of the person performing the learning, with the typical result that a new solution is required for any change in these settings. This paper investigates the relationship between the reward function and the optimal value function for MORL; specifically addressing the question of how to approximate the optimal value function well beyond the set of weights for which the optimization problem was actually solved, thereby avoiding the need to recompute for any particular choice. We prove that the value function transforms smoothly given a transformation of weights of the reward function (and thus a smooth interpolation in the policy space). A Gaussian process is used to obtain a smooth interpolation over the reward function weights of the optimal value function for three well-known examples: Gridworld, Objectworld and Pendulum. The results show that the interpolation can provide robust values for sample states and actions in both discrete and continuous domain problems. Significant advantages arise from utilizing this interpolation technique in the domain of autonomous vehicles: easy, instant adaptation of user preferences while driving and true randomization of obstacle vehicle behavior preferences during training.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197456","","Interpolation;Training;Learning (artificial intelligence);Gaussian processes;Mathematical model;Random variables;Optimization","Gaussian processes;interpolation;learning (artificial intelligence);optimisation","smooth interpolation;reward function weights;optimal value function;multiobjective reinforcement learning problems;Gaussian process;value function transforms;MORL problems","","1","","20","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Policy Gradient-based Integral Reinforcement Learning for Optimal Control Design of Nonaffine Morphing Aircraft Systems","H. Lee; S. -H. Kim; Y. Kim","Dept. of Aerospace Engineering, Seoul National University, Seoul, Republic of Korea; Dept. of Aerospace Engineering, Seoul National University, Seoul, Republic of Korea; Dept. of Aerospace Engineering, Seoul National University, Seoul, Republic of Korea","2020 28th Mediterranean Conference on Control and Automation (MED)","1 Sep 2020","2020","","","218","223","An online model-free optimal control design strategy is proposed for general continuous-time nonlinear nonaffine systems using policy gradient-based integral reinforcement learning. In the case of the nonaffine system such as a morphing wing aircraft considering the morphing parameters as control effectors, general nonlinear control design method cannot be applied and solving the Hamilton-Jacobi-Bellman equation analytically is difficult. The proposed online optimal control algorithm is constructed based on the actor-critic structure using Q-function and policy gradient scheme and the integral reinforcement learning approach is used to develop the actor-critic parameter estimation for the continuous-time system. The closed-loop stability analysis for the designed method is presented. Through the proposed method, the optimal controller can be designed for the general nonaffine system, which has an advantage in terms of a computational issue for the complex system. Note that the entire dynamic model is not required. Simulation results demonstrate the effectiveness of the proposed scheme.","2473-3504","978-1-7281-5742-9","10.1109/MED48518.2020.9183024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9183024","Aerospace control;Intelligent control systems;Nonlinear control","Optimal control;Mathematical model;Learning (artificial intelligence);Heuristic algorithms;Aircraft;Control design;Aerospace control","adaptive control;aerospace components;aircraft control;closed loop systems;continuous time systems;control system synthesis;gradient methods;learning (artificial intelligence);nonlinear control systems;optimal control;parameter estimation;stability","policy gradient-based integral reinforcement learning;optimal control design;nonaffine morphing aircraft systems;continuous-time nonlinear nonaffine systems;morphing wing aircraft;morphing parameters;control effectors;general nonlinear control design method;optimal control algorithm;policy gradient scheme;integral reinforcement learning approach;actor-critic parameter estimation;continuous-time system;optimal controller;general nonaffine system","","1","","18","IEEE","1 Sep 2020","","","IEEE","IEEE Conferences"
"Multi-Agent Active Search: A Reinforcement Learning Approach","C. Igoe; R. Ghods; J. Schneider","School of Computer Science, Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA; School of Computer Science, the Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; School of Computer Science, Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Robotics and Automation Letters","15 Dec 2021","2022","7","2","754","761","Multi-Agent Active Search (MAAS) is an active learning problem with the objective of locating sparse targets in an unknown environment by actively making data-collection decisions. Recently proposed algorithms, although well-motivated from a theoretical perspective, are limited in three key ways: they are either explicitly myopic (e.g. with respect to information gain) or introduce strong biases that fall short of fully non-myopic behaviour; they employ general-purpose coordination mechanisms to scale to multi-agent settings without optimising for any specific agent configuration; and they involve significant online computation to determine suitable sensing regions. In this letter, we introduce a Poisson Point Process formulation and cast MAAS as a Reinforcement Learning problem, learning policies in belief space of the associated POMDP. We demonstrate how such an approach can overcome each of the three issues of previous algorithms and is surprisingly robust to test-time miscommunication.","2377-3766","","10.1109/LRA.2021.3131697","Army Research Office; U.S. Army Futures Command(grant numbers:W911NF-20-D-0002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9632368","Aerial systems: perception and autonomy;reinforcement learning;multi-robot systems","Search problems;Sensors;Task analysis;Robot sensing systems;Training;Reinforcement learning;Optimization","control engineering computing;data acquisition;mobile robots;multi-agent systems;multi-robot systems;reinforcement learning;search problems;stochastic processes","multiagent active search;MAAS;active learning;data-collection decisions;agent configuration;reinforcement learning;sparse targets;information gain;fully nonmyopic behaviour;general-purpose coordination;online computation;sensing regions;Poisson point process formulation;learning policies;belief space;POMDP;test-time miscommunication;multirobot systems","","1","","21","CCBY","1 Dec 2021","","","IEEE","IEEE Journals"
"Toward Observation Based Least Restrictive Collision Avoidance Using Deep Meta Reinforcement Learning","S. Asayesh; M. Chen; M. Mehrandezh; K. Gupta","Faculty of Applied Science, Simon Fraser University, Burnaby, BC, Canada; Faculty of Applied Science, Simon Fraser University, Burnaby, BC, Canada; Faculty of Engineering and Applied Science, University of Regina, Regina, SK, Canada; Faculty of Applied Science, Simon Fraser University, Burnaby, BC, Canada","IEEE Robotics and Automation Letters","3 Aug 2021","2021","6","4","7445","7452","This letter presents the Observation-based Least-Restrictive Collision Avoidance Module (OLR-CAM) that can be added to any autonomous robot working in a shared environment and provide a high-level safety layer to the existing policy for each robot. The OLR-CAM takes raw sensory observations as input, evaluates the agents' safety against dynamic and static obstacles, and only intervenes the default policy when needed - in a least-restrictive fashion - to avoid a potential collision. In our approach, we meta-train the OLR-CAM policy within a “2D Navigation Meta World System”. Furthermore, to endow the policy with a notion of safety in multi-agent environments with obstacles, we propose a novel reward function based on a safety value function derived from the Hamilton-Jacobi reachability theory and a local cost map. The proposed reward function does not need any additional information about the environment's map. This facilitates the adoption of the algorithm in a new environment at the meta test stage. The proposed algorithm is fully meta-trained in simulation and tested on a real multi-agent system without any additional training conducted in the real setting. Our results show that the OLR-CAM success rate outperforms a well-known classical baseline approach by 10 percent on average and reduces the interruptions/changes to the preferred velocity by 15 percent.","2377-3766","","10.1109/LRA.2021.3098332","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9492812","Collision avoidance;reinforcement learning;optimization and optimal control","Collision avoidance;Safety;Heuristic algorithms;Task analysis;Navigation;Reinforcement learning;Multi-agent systems","collision avoidance;deep learning (artificial intelligence);mobile robots;multi-agent systems;reachability analysis","safety value function;Hamilton-Jacobi reachability theory;local cost map;reward function;meta test stage;multiagent system;OLR-CAM success rate;collision avoidance module;autonomous robot;shared environment;high-level safety layer;raw sensory observations;dynamic obstacles;static obstacles;default policy;least-restrictive fashion;OLR-CAM policy;2D Navigation Meta World System;multiagent environments;Observation-based Least-Restrictive Collision Avoidance Module;deep meta reinforcement learning;efficiency 10.0 percent;efficiency 15.0 percent","","1","","30","IEEE","21 Jul 2021","","","IEEE","IEEE Journals"
"Interactive Reinforcement Learning with Inaccurate Feedback","T. A. Kessler Faulkner; E. Schaertl Short; A. L. Thomaz","Department of Computer Science, The University of Texas at Austin, Austin, TX, USA; Department of Computer Science, Tufts University, Medford, MA, USA; Department of Electrical and Computer Engineering, The University of Texas of Austin, Austin, TX, USA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","7498","7504","Interactive Reinforcement Learning (RL) enables agents to learn from two sources: rewards taken from observations of the environment, and feedback or advice from a secondary critic source, such as human teachers or sensor feedback. The addition of information from a critic during the learning process allows the agents to learn more quickly than non-interactive RL. There are many methods that allow policy feedback or advice to be combined with RL. However, critics can often give imperfect information. In this work, we introduce a framework for characterizing Interactive RL methods with imperfect teachers and propose an algorithm, Revision Estimation from Partially Incorrect Resources (REPaIR), which can estimate corrections to imperfect feedback over time. We run experiments both in simulations and demonstrate performance on a physical robot, and find that when baseline algorithms do not have prior information on the exact quality of a feedback source, using REPaIR matches or improves the expected performance of these algorithms.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197219","","Maintenance engineering;Robot sensing systems;Estimation;Task analysis;Learning (artificial intelligence);Computer science","feedback;interactive systems;learning (artificial intelligence);mobile robots","interactive reinforcement learning;human teachers;sensor feedback;learning process;noninteractive RL;policy feedback;feedback source;interactive RL methods;revision estimation-from-partially incorrect resources;REPaIR;physical robot","","1","","22","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach in Assignment of Task Priorities in Kinematic Control of Redundant Robots","M. Karimi; M. Ahmadi","Department of Mechanical and Aerospace Engineering, Carleton University, Ottawa, ON, Canada; Department of Mechanical and Aerospace Engineering, Carleton University, Ottawa, ON, Canada","IEEE Robotics and Automation Letters","21 Dec 2021","2022","7","2","850","857","Based on the recent advances of Deep Reinforcement Learning (DRL) and promising results, in this paper, we propose a framework for strict priority assignment in the context of kinematic control of redundant robots. The presented method focuses on redundant robots performing multiple concurrent tasks with potentially conflicting requirements and learns how to re-assign task priorities to ensure critical tasks get executed through smooth transitions. A Deep Q-Network (DQN) reinforcement learning agent is trained to assign the proper strict priorities to a stack of predefined kinematic control tasks (e.g., position control, orientation control, obstacle avoidance control, etc.) in a varying environment. Furthermore, to address the discontinuities in the control law due to the changes in the task priorities, a smoothing algorithm is proposed to ensure continuous reference velocities to the robot’s joints. The proposed method is generic and extendable to a higher number of tasks and can be used when a reordering, swapping, addition, or deletion of tasks is required. The effectiveness of the proposed method is shown in simulation on a 5-DoF planar manipulator and a 7-DoF planar bipedal robot. The results show that the DRL agent is successful in assigning the correct hierarchy of tasks at each robot’s state based on the global goal of the robot.","2377-3766","","10.1109/LRA.2021.3133934","Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN-2015-04169); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645158","Machine learning for robot control;reinforcement learning;redundant robots;bipedal locomotion","Task analysis;Robots;Null space;Kinematics;Robot kinematics;Jacobian matrices;Training","collision avoidance;legged locomotion;motion control;redundant manipulators;reinforcement learning","redundant robots;strict priority assignment;multiple concurrent tasks;re-assign task priorities;smooth transitions;proper strict priorities;position control;orientation control;obstacle avoidance control;7-DoF planar bipedal robot;DRL agent;deep Q-network reinforcement learning agent;kinematic control tasks;5-DoF planar manipulator","","","","29","IEEE","10 Dec 2021","","","IEEE","IEEE Journals"
"Evaluating Adaptation Performance of Hierarchical Deep Reinforcement Learning","N. Van Stolen; S. Hyun Kim; H. T. Tran; G. Chowdhary","University of Illinois at Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, IL, USA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","11457","11463","Deep Reinforcement Learning has been used to exploit specific environments, but has difficulty transferring learned policies to new situations. This issue poses a problem for practical applications of Reinforcement Learning, as real-world scenarios may introduce unexpected differences that drastically reduce policy performance. We propose the use of differentiated sub-policies governed by a hierarchical controller to support adaptation in such scenarios. We also introduce a confidence- based training process for the hierarchical controller which improves training stability and convergence times. We evaluate these methods in a new Capture the Flag environment designed to explore adaptation in autonomous multi-agent settings.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197052","","Training;Adaptation models;Trajectory;Games;Learning (artificial intelligence);Robots;Switches","learning (artificial intelligence);multi-agent systems;neural nets","differentiated sub-policies;hierarchical controller;adaptation performance;hierarchical deep reinforcement learning;policy performance;confidence- based training process","","","","24","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Developing Cooperative Policies for Multi-Stage Reinforcement Learning Tasks","J. Erskine; C. Lehnert","Queensland University of Technology, Brisbane, QLD, Australia; Queensland University of Technology, Brisbane, QLD, Australia","IEEE Robotics and Automation Letters","25 May 2022","2022","7","3","6590","6597","Many hierarchical reinforcement learning algorithms utilise a series of independent skills as a basis to solve tasks at a higher level of reasoning. These algorithms don’t consider the value of using skills that are cooperative instead of independent. This paper proposes the Cooperative Consecutive Policies (CCP) method of enabling consecutive agents to cooperatively solve long time horizon multi-stage tasks. This method is achieved by modifying the policy of each agent to maximise both the current and next agent’s critic. Cooperatively maximising critics allows each agent to take actions that are beneficial for its task as well as subsequent tasks. Using this method in a multi-room maze domain and a peg in hole manipulation domain, the cooperative policies were able to outperform a set of naive policies, a single agent trained across the entire domain, as well as another sequential HRL algorithm.","2377-3766","","10.1109/LRA.2022.3174258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9772966","Reinforcement learning","Task analysis;Reinforcement learning;Training;Grasping;Navigation;Entropy;Cognition","multi-agent systems;optimisation;reinforcement learning","CCP;consecutive agents;cooperative consecutive policies;hole manipulation domain;long time horizon multistage tasks;multiroom maze domain;multistage reinforcement learning","","","","29","IEEE","11 May 2022","","","IEEE","IEEE Journals"
"Robotic Embodiment of Human-Like Motor Skills via Reinforcement Learning","L. Guzman; V. Morellas; N. Papanikolopoulos","Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN, USA; Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA","IEEE Robotics and Automation Letters","15 Feb 2022","2022","7","2","3711","3717","Current methods require robots to be reprogrammed for every new task, consuming many engineering resources. This work focuses on integrating real and simulated environments for our proposed “Internet of Skills,” which enables robots to learn advanced skills from a small set of expert demonstrations. By expanding on recent work in the areas of Learning from Demonstrations (LfD) and Reinforcement Learning (RL), we can train robot control policies that can not only effectively complete a given task but also do so with greater performance than the expert demonstrations used to train the policy. In this work, we create simulated environments to train RL algorithms for the task of inverse kinematics and obstacle avoidance. Many state-of-the-art RL algorithms are compared, and we provide a detailed analysis of the state space and parameters chosen. Lastly, we utilize a Vicon motion tracking system and train the robot agent to follow trajectories given by a human operator. Our results show that reinforcement learning algorithms such as proximal policy optimization can develop control policies that are capable of complex control tasks that integrate with the real world, an important first step towards developing a system that can autonomously learn new skills from human demonstrations.","2377-3766","","10.1109/LRA.2022.3147453","Minnesota Robotics Institute (MnRI), Honeywell; National Science Foundation(grant numbers:CNS-1439728,CNS-1531330,CNS-1939033); USDA/NIFA(grant numbers:2020-67021-30755); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9699095","Telerobotics and teleoperation;reinforcement learning;transfer learning;model learning for control;collision avoidance","Robots;Task analysis;Robot kinematics;Reinforcement learning;Training;Collision avoidance;Virtual environments","collision avoidance;mobile robots;motion control;multi-robot systems;reinforcement learning;robot kinematics","robotic embodiment;human-like motor Skills;engineering resources;robot control policies;RL algorithms;Vicon motion tracking system;human operator;reinforcement learning algorithms;proximal policy optimization;human demonstrations;learning from demonstrations;LfD;inverse kinematics;obstacle avoidance","","","","20","IEEE","1 Feb 2022","","","IEEE","IEEE Journals"
"Switching Decision of Air-Ground Amphibious Robot using Neural Network-based Reinforcement Learning","Z. Liu; Y. Liu","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","2019 IEEE 9th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","16 Apr 2020","2019","","","883","888","This paper studies the problem of autonomous decision-making motion mode switching based on external environmental information when the Air-Ground Amphibious Robot is in complex environment. In the process of robot autonomous decision-making, the limitation of motion time and energy consumption should be considered, and the motion state with short motion time and low energy consumption should be selected. In this paper, the methods of reinforcement learning based on neural network are put forward to solve the problem of intelligent switching decision of the Air-Ground Amphibious Robot, aiming at making the robot choose the appropriate mode to move in a certain state of environment. Reinforcement learning based on neural network can not only generate decision function in the process of learning, but also solve the reinforcement problem of continuous environment state space. When a robot performs a task, it is important to reduce the energy consumption of motion. Therefore, this paper takes the energy consumption and motion time as the basis of robot decision-making and the standard of motion evaluation. In this paper, we provide the simulation results and demonstrate the feasibility of our method, which can effectively realize the high-efficiency motion ability of the Air-Ground Amphibious Robot in the complex environment.","2379-7711","978-1-7281-0770-7","10.1109/CYBER46603.2019.9066633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066633","","Learning (artificial intelligence);Energy consumption;Robot kinematics;Decision making;Mobile robots;Switches","aerospace control;decision making;learning (artificial intelligence);mobile robots;motion control;multi-robot systems","intelligent switching decision;short motion time;motion state;robot autonomous decision-making;autonomous decision-making motion mode;neural network-based reinforcement learning;complex environment;air-ground amphibious robot;robot decision-making;energy consumption","","","","15","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"A multi-agent deep reinforcement learning framework for automated driving on highways","L. Bakker; S. Grammatico","Delft Center for Systems and Control, TU Delft, The Netherlands; Delft Center for Systems and Control, TU Delft, The Netherlands","2020 28th Mediterranean Conference on Control and Automation (MED)","1 Sep 2020","2020","","","770","775","We apply deep reinforcement learning to automated driving on highways. We propose a novel, simple framework with improved performance with respect to the state of the art. When implementing our algorithm on multilane highway scenarios, after the training phase, we observe via numerical simulations that the vehicles are able to avoid collisions and to reach their respective destination lanes with very high probability.","2473-3504","978-1-7281-5742-9","10.1109/MED48518.2020.9182882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9182882","","Road transportation;Training;Multi-agent systems;Machine learning;Control systems;Aerospace electronics;Learning (artificial intelligence)","learning (artificial intelligence);multi-agent systems;probability;traffic engineering computing","multiagent deep reinforcement learning framework;automated driving;multilane highway scenarios","","","","21","IEEE","1 Sep 2020","","","IEEE","IEEE Conferences"
"Wheel Loader Scooping Controller Using Deep Reinforcement Learning","O. Azulay; A. Shapiro","School of Mechanical Engineering, Tel Aviv University, Tel Aviv, Israel; Department of Mechanical Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel","IEEE Access","9 Feb 2021","2021","9","","24145","24154","This article presents a deep reinforcement learning-based controller for an unmanned ground vehicle with a custom-built scooping mechanism. The robot's aim is to autonomously perform earth scooping cycles with three degrees of freedom: lift, tilt and the robot's velocity. While the majority of previous studies on automated scooping processes are based on data recorded by expert operators, we present a method to autonomously control a wheel loader to perform the scooping cycle using deep reinforcement learning methods without any user-provided demonstrations. The controller's learning approach is based on the actorcritic, Deep Deterministic Policy Gradient algorithm which we use to map online sensor data as input to continuously update the actuator commands. The training of the scooping policy network is done solely in a simplified simulation environment using a virtual physics engine, which converges to an average of a 65% fill factor from the full bucket capacity and a 5 [sec] average cycle time. We illustrate the performance of the trained policy in simulations and in real-world experiments with 3 different inclination angles of the earth. An additional scooping experiment compared the performance of our controller to remote manual human control. Overall, the deep reinforcement learning-based controller exhibited good performance in terms of both achieved visually bucket fill with varying scooped earth weights of 4.1 - 7.2[kg], and a 5.1 - 7.1[sec] cycle time. The experimental results confirm the ability of our planner to fill bucket as required, indicating that our controller can be used for excavation purposes.","2169-3536","","10.1109/ACCESS.2021.3056625","Israeli Ministry of Defense through Project ROBIL(grant numbers:87726311); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9344588","Robotics in construction;machine learning;agricultural automation","Wheels;Trajectory;Robots;Reinforcement learning;Earth;Task analysis;Soil","excavators;learning (artificial intelligence);manipulator dynamics;mobile robots;remotely operated vehicles","scooping cycle;deep reinforcement learning methods;learning approach;deep deterministic policy gradient algorithm;scooping policy network;average cycle time;trained policy;remote manual human control;scooped earth weights;wheel loader scooping controller;unmanned ground vehicle;automated scooping processes;deep reinforcement learning-based controller","","23","","27","CCBY","2 Feb 2021","","","IEEE","IEEE Journals"
"Large-scale Cooperative Task Offloading and Resource Allocation in Heterogeneous MEC Systems via Multi-Agent Reinforcement Learning","Z. Gao; L. Yang; Y. Dai","School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of College of Software, Northeastern University, Shenyang, China","IEEE Internet of Things Journal","","2023","PP","99","1","1","In multi-access edge computing systems, existing task offloading methods have provided ultra-short latency services for heterogeneous tasks on mobile devices (MDs). Nevertheless, the complexity of MEC systems grows exponentially with the number of MDs or edge servers (ES), so learning a good offloading policy is a huge challenge when the number of MDs or ESs is large. Moreover, MDs are often unable to find optimal ESs for offloading since the restricted ESs infrastructures and the spatiotemporally imbalanced task offloading requirements. To solve these problems, we propose a Curriculum Spatio-Temporal Multi-Agent Actor-Critic (CSTMAAC)-based task offloading method. Each ES is regarded as an agent and the problem is formulated as a multi-objective optimization task. To adapt to the large-scale MEC systems, we first introduce an evolutionary curriculum learning by gradually raising the number of trained ES agents in a phased way. Second, to facilitate the coordination of the offloading policies among geographically distributed ESs, we design an attention-based centralized critic-network. Besides, a delayed access mechanism is introduced that uses information about future task processing competition to capture the impact of potential future task processing contention and help ES agents obtain a better offloading strategy. Finally, critic-network is expanded to multi-critics and a dynamic weight mechanism is designed to adaptively optimize multi-objectives and obtain a good balance for multiple objectives. Real-world datasets used in experiments demonstrate that CSTMAAC raises task completion rates and total utility by 13.01% 15.21% and 16.89% 18.32% compared with the existing algorithms.","2327-4662","","10.1109/JIOT.2023.3292387","Research, development and application of internet service platform for the whole industry chain collaboration in packaged food industry(grant numbers:2021YFF0901205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10173664","Multi-access edge computing (MEC);task offloading;resource allocation;reinforcement learning;curriculum learning","Task analysis;Resource management;Training;Optimization;Internet of Things;Collaboration;Heuristic algorithms","","","","","","","IEEE","5 Jul 2023","","","IEEE","IEEE Early Access Articles"
"Reinforcement-Learning-based Control of an Industrial Robotic Arm for Following a Randomly-Generated 2D-Trajectory","A. Jafari-Tabrizi; D. P. Gruber","Polymer Competence Center Leoben Gmbh, Leoben, Austria; Polymer Competence Center Leoben Gmbh, Leoben, Austria","2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS)","2 Sep 2021","2021","","","1","6","This paper introduces a reinforcement-learning-based method to control an industrial robotic arm. The goal is to improve the process of teaching the robotic arm to perform a full-scan of the surface of free-formed components during an automatic quality inspection. As today’s standard approach, the teaching of a robotic arm to follow a complicated trajectory is being done manually by experts. However, this approach is not always reliable and might not necessarily result in an optimal trajectory. As a consequence, a lot of time and effort by a human expert is required. In addition, when a new component with a modified design has to be inspected, the human expert must create a suitable inspection trajectory for this component and meanwhile the whole process of automatic inspection has to be stopped for a considerably long time. The latter forms the biggest motivation of the presented work. The main focus of this work is to examine whether the teaching task can be accelerated by employing reinforcement learning methodologies to use the available information about the shape of the component to control the robot’s tool center point (TCP). As an initial step, a simulation environment has been developed, in which the position and orientation of the robot’s TCP can be controlled. During the simulation episodes a randomly generated 2D trajectory appears on the panel in front of the robot, and while observing the points on the trajectory, the robot trained with Deep Deterministic Policy Gradient algorithm follows the trajectory, and its goal is to reach the end point, while minimizing the time and the deviation from the trajectory. The initial results from the simulation environment and the steps to be taken in the future are explained in the given paper.","","978-1-6654-3156-9","10.1109/COINS51742.2021.9524158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524158","Reinforcement Learning;Robotic Arms;Quality Inspection;Industry 4.0","Service robots;Shape;Education;Inspection;Tools;Manipulators;Trajectory","automatic optical inspection;deep learning (artificial intelligence);gradient methods;industrial robots;manipulators;trajectory control","reinforcement-learning-based control;industrial robotic arm;randomly-generated 2D-trajectory;free-formed components;automatic quality inspection;complicated trajectory;optimal trajectory;inspection trajectory;teaching task;robot tool center point;robot TCP;deep deterministic policy gradient algorithm","","","","22","IEEE","2 Sep 2021","","","IEEE","IEEE Conferences"
"Fast Adaptive Task Offloading and Resource Allocation in Large-Scale MEC Systems via Multi-Agent Graph Reinforcement Learning","Z. Gao; L. Yang; Y. Dai","School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of College of Software, Northeastern University, Shenyang, China","IEEE Internet of Things Journal","","2023","PP","99","1","1","In multi-access edge computing, when many mobile devices (MDs) offload their tasks to an edge server (ES), its resources might become constrained. These tasks may take a long time to complete or even be thrown away. Since the unknown information of both the ESs and other MDs, it is difficult for each MD to determine its offloading policy independently. Furthermore, most offloading methods have poor generalization to new environment since they focus on model architecture with a fixed quantity of MDs and ESs, preventing trained models from transferring to other environments. In the paper, we provide a full decentralized offloading scheme based on the Curriculum Attention-weighted Graph Recurrent Network-based Multi-Agent Actor-Critic (CAGR-MAAC). First, we build MEC as a shared MD agents-ESs graph and an AGR-based message network is designed to enable each MD aggregate the information of ESs and other MDs and solve the partial observability of MD agents for MEC system. Second, a learnable differentiable encoder network is introduced to construct MD agent’s local information encoding. Subsequently, the MD agent converts overall the information regarding the MEC system into a fixed-size embedding via an AGR Network to handle different quantity of MDs and ESs. Finally, we introduce curriculum learning to address the huge complexity of the MEC system and the training difficulties induced by the large amounts of MDs and ESs. Experiments demonstrate that compared with existing algorithms, CAGR-MAAC boosts task completion rates and decreases system costs by 13.01% 15.03% and 16.45% 18.56%, and can quickly adapt to the new environment.","2327-4662","","10.1109/JIOT.2023.3285950","Research, development and application of internet service platform for packaged food industry for the whole industry chain synergy(grant numbers:2021YFF0901205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10153416","Multi-access edge computing (MEC);task offloading;resources allocation;multi-agent reinforcement learning (MARL);recurrent graph neural networks (R-GNNs)","Task analysis;Resource management;Heuristic algorithms;Internet of Things;Complexity theory;Training;Observability","","","","","","","IEEE","14 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Real-Time Scheduling for Electric Vehicles Charging/Discharging Using Reinforcement Learning","N. Mhaisen; N. Fetais; A. Massoud","KINDI Computing Research Centre, College of Engineering, Qatar University, Doha, Qatar; KINDI Computing Research Centre, College of Engineering, Qatar University, Doha, Qatar; Department of Electrical Engineering, College of Engineering, Qatar University, Doha, Qatar","2020 IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT)","11 May 2020","2020","","","1","6","With the increase in Electric Vehicles (EVs) penetration, their charging needs form an additional burden on the grid. Thus, charging coordination is necessary for safe and efficient EV use. The scheduling of EVs is especially essential in Vehicle-to-Grid schemes, where EVs energy cost can be reduced by intelligently managing the charging and discharging according to real-time prices and owners' needs. Further, utilities can perform load shifting by price-based demand-response programs. However, the scheduling problem is challenging in the presence of multiple unknown variables, such as real-time prices, commuting behavior, and energy needs. Most of the scheduling techniques attempt to model the system uncertainties (e.g., forecast prices) and plan accordingly. In this paper, we propose the use of a model-free Reinforcement Learning (RL) technique that can deduce a charging/discharging strategy without requiring any model of the system. We evaluate the performance of the proposed system with real-world data and investigate the learned charging/discharging pattern to show the effectiveness of the proposed method.","","978-1-7281-4821-2","10.1109/ICIoT48696.2020.9089471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9089471","EV Scheduling;Reinforcement Learning;Model-free Control;Vehicle-to-grid;Smart grid","Mathematical model;Scheduling;Real-time systems;Load modeling;Learning (artificial intelligence);Vehicle-to-grid;Optimization","commutation;cost reduction;demand side management;electric vehicle charging;electric vehicles;electricity supply industry;learning (artificial intelligence);power engineering computing;power generation economics;power generation planning;power generation scheduling;pricing;vehicle-to-grid","real-time scheduling;reinforcement learning;vehicle-to-grid schemes;EVs energy cost;real-time prices;price-based demand-response programs;electric vehicles;electric utilities;commuting behavior;system planning;charging-discharging strategy","","11","","16","IEEE","11 May 2020","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Based Adaptive ROI Generation for Video Object Segmentation","U. A. Usmani; J. Watada; J. Jaafar; I. A. Aziz; A. Roy","Department of Computer and Information Science, Faculty of Science and IT, Universiti Teknologi PETRONAS (UTP), Seri Iskandar, Perak, Malaysia; Production and Systems, Graduate School of Information, Waseda University, Kitakyushu, Japan; Department of Computer and Information Science, Faculty of Science and IT, Universiti Teknologi PETRONAS (UTP), Seri Iskandar, Perak, Malaysia; Department of Computer and Information Science, Faculty of Science and IT, Universiti Teknologi PETRONAS (UTP), Seri Iskandar, Perak, Malaysia; Department of Computer Science, School of Information Technology, Monash University Malaysia, Subang Jaya, Selangor, Malaysia","IEEE Access","14 Dec 2021","2021","9","","161959","161977","Video object segmentation’s primary goal is to automatically extract the principal object(s) in the foreground from the background in videos. The primary focus of the current deep learning-based models is to learn the discriminative representations in the foreground over motion and appearance in small-term temporal segments. In the video segmentation process, it is difficult to handle various challenges such as deformation, scale variation, motion blur, and occlusion. Furthermore, relocating the segmentation target in the next frame is difficult if it is lost in the current frame during the segmentation process. This work aims at solving the zero-shot video object segmentation issue in a holistic fashion. We take advantage of the inherent correlations between the video frames by incorporating a global co-attention mechanism to overcome the limitations. We propose a novel reinforcement learning framework that provides competent and fast stages for gathering scene context and global correlations. The agent concurrently calculates and adds the responses of co-attention in the joint feature space. To capture the different aspects of the common feature space, the agent can generate multiple co-attention versions. Our framework is trained using pairs (or groups) of video frames, which adds to the training content, thus increasing the learning capacity. Our approach encodes the important information during the segmentation phase by a simultaneous process of various reference frames that are subsequently utilized to predict the persistent and conspicuous objects in the foreground. The proposed method has been validated using four commonly used video entity segmentation datasets: SegTrack V2, DAVIS 2016, CdNet 2014, and the Youtube-Object dataset. On the DAVIS 2016, the results reveal that the proposed results boost the state-of-the-art techniques on the F1 Measure by 4%, SegTrack V2 by a Jaccard Index of 12.03%, and Youtube Object by a Jaccard Index of 13.11%. Meanwhile, our algorithm improves the accuracy by 8%, F1 Measure by 12.25 %, and precision by 14% on the CdNet 2014, thus ranking higher than the current state-of-the-art methods.","2169-3536","","10.1109/ACCESS.2021.3132453","Yayasan UTP Prestigious Scholarship (YUTP) under Universiti Teknologi PETRONAS (UTP) with Cost Center(grant numbers:015LC0-281); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9634037","Model adaptation;object detection;object tracking;reinforcement learning;video object segmentation","Motion segmentation;Correlation;Feature extraction;Object segmentation;Computational modeling;Reinforcement learning;Training","adaptive signal processing;deep learning (artificial intelligence);feature extraction;image motion analysis;image representation;image segmentation;image sequences;object detection;object tracking;reinforcement learning;video signal processing","adaptive ROI generation;deep learning;video segmentation process;zero-shot video object segmentation;global co-attention mechanism;reinforcement learning;multiple co-attention versions;Youtube-Object dataset;principal object extraction;video entity segmentation datasets;SegTrack V2 dataset;DAVIS 2016 dataset;CdNet 2014 dataset;discriminative representation learning","","6","","94","CCBY","3 Dec 2021","","","IEEE","IEEE Journals"
"A Reinforcement Learning Hyper-heuristic for the optimisation of Flight Connections","Y. Pylyavskyy; A. Kheiri; L. Ahmed","Department of Management Science, Lancaster University, UK; Department of Management Science, Lancaster University, UK; School of Computer Science, Cardiff University, UK","2020 IEEE Congress on Evolutionary Computation (CEC)","3 Sep 2020","2020","","","1","8","Many combinatorial computational problems have been effectively solved by means of hyper-heuristics. In this study, we focus on a problem proposed by Kiwi.com and solve this problem by implementing a Reinforcement Learning (RL) hyperheuristic algorithm. Kiwi.com proposed a real-world NP-hard minimisation problem associated with air travelling services. The problem shares some characteristics with several TSP variants, such as time-dependence and time-windows that make the problem more complex in comparison to the classical TSP. In this work, we evaluate our proposed RL method on kiwi.com problem and compare its results statistically with common random-based hyper-heuristic approaches. The empirical results show that RL method achieves the best performance between the tested selection hyper-heuristics. Another significant achievement of RL is that better solutions were found compared to the best known solutions in several problem instances.","","978-1-7281-6929-3","10.1109/CEC48606.2020.9185803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9185803","Hyper-heuristics;Metaheuristics;TSP","Urban areas;Airports;Optimization;Heuristic algorithms;Learning (artificial intelligence);Search problems;Computer science","computational complexity;learning (artificial intelligence);optimisation;travel industry","reinforcement learning hyper-heuristic;optimisation;selection hyper-heuristics;RL method;classical TSP;time-windows;time-dependence;TSP variants;air travelling services;real-world NP-hard minimisation problem;combinatorial computational problems;flight connections","","5","","20","IEEE","3 Sep 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Neural Architecture Search for Audio Tagging","H. Liu; C. Zhang","Graduate School of Information, Production and Systems, Waseda University, Tokyo, Japan; School of Instrument Science and Engineering, Southeast University, Nanjing, China","2020 International Joint Conference on Neural Networks (IJCNN)","28 Sep 2020","2020","","","1","8","Audio tagging aims to assign tags for an audio chunk, and it has attracted increasing attention as its potential applications seem to be evident. Deep learning technologies have been successfully applied to domestic audio tagging task. However, the performance of deep models is heavily relied on the hyper-parameters selection such as the filter size in the convolutional layers. Recently, Neural Architecture Search (NAS) has been successfully applied to design deep model architectures for specified learning task. In this paper, we explore the neural architecture search method for domestic audio tagging. We propose to use the Convolutional Recurrent Neural Network (CRNN) with Attention and Location (ATT-LOC) as the audio tagging model. Then, we apply NAS to search for the optimal number of filters and the filter size. Finally, we employ a grid search over the mixup augmentation coefficient, the input size of the spectrogram and the value of batch size to further improve the classification results. As demonstrated in our experiments, the architecture found by automatic searching achieves an equal error rate of 0.095 on DCASE 2016 task 4 dataset, outperforming the CRNN baseline of 0.10. In addition, the architecture found by NAS achieves a faster convergence rate in training than the CRNN baseline.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9207530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207530","audio tagging;convolutional recurrent neural network;neural architecture search;reinforcement learning","Tagging;Task analysis;Recurrent neural networks;Training;Computer architecture;Reinforcement learning","acoustic signal processing;knowledge based systems;learning (artificial intelligence);recurrent neural nets","convolutional layers;NAS;deep model architectures;specified learning task;neural architecture search method;convolutional recurrent neural network;audio tagging model;filter size;grid search;input size;automatic searching;DCASE 2016 task 4 dataset;audio chunk;deep learning technologies;domestic audio tagging task;hyper-parameter selection","","2","","41","IEEE","28 Sep 2020","","","IEEE","IEEE Conferences"
"Automatic Itinerary Planning Using Triple-Agent Deep Reinforcement Learning","B. -H. Chen; J. Han; S. Chen; J. -L. Yin; Z. Chen","Department of Computer Science and Engineering, Yuan Ze University, Taoyuan, Taiwan; College of Computer Science and Big Data, Fuzhou University, Fuzhou, China; Department of Computer Science and Engineering, Yuan Ze University, Taoyuan, Taiwan; College of Computer Science and Big Data, Fuzhou University, Fuzhou, China; College of Computer Science and Big Data, Fuzhou University, Fuzhou, China","IEEE Transactions on Intelligent Transportation Systems","12 Oct 2022","2022","23","10","18864","18875","Automatic itinerary planning that provides an epic journey for each traveler is a fundamental yet inefficient task. Most existing planning methods apply heuristic guidelines for certain objective, and thereby favor popular preferred point of interests (POIs) with high probability, which ignore the intrinsic correlation between the POIs exploration, traveler’s preferences, and distinctive attractions. To tackle the itinerary planning problem, this paper explores the connections of these three objectives in probabilistic manner based on a Bayesian model and proposes a triple-agent deep reinforcement learning approach, which generates 4-way direction, 4-way distance, and 3-way selection strategy for iteratively determining next POI to visit in the itinerary. Experiments on five real-world cities demonstrate that our triple-agent deep reinforcement learning approach can provide better planning results in comparison with state-of-the-art multiobjective optimization methods.","1558-0016","","10.1109/TITS.2022.3169002","Ministry of Science and Technology, Taiwan(grant numbers:MOST 108-2221-E-155-034-MY3); Natural Science Foundation of Fujian Province, China(grant numbers:2018J01798); Youth Foundation of Fujian Province, China(grant numbers:2021J05129); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9766177","Automatic itinerary planning;deep reinforcement learning;multiobjective optimization","Planning;Search problems;Reinforcement learning;Urban areas;Space exploration;Probabilistic logic;Computer science","learning (artificial intelligence);planning;probability;search problems;travel industry","automatic itinerary planning;epic journey;existing planning methods;fundamental yet inefficient task;itinerary planning problem;planning results;POI;POIs exploration;popular preferred point;traveler;triple-agent deep reinforcement learning approach","","","","38","IEEE","2 May 2022","","","IEEE","IEEE Journals"
"Fairness analysis with cost impact for Brasilia's Flight Information Region using reinforcement learning approach","A. C. de Arruda; A. F. Leite; C. R. F. de Almeida; A. M. F. Crespo; L. Weigang","TI BB, Bank of Brazil, Brasilia, Brazil; University of Brasilia, Brasilia, Brazil; CIC, University of Brasilia, Brasilia, Brazil; First Integrated Center of Air Defense and AirSpace Control, CINDACTA I, Brasilia, Brazil; University of Brasilia, Brasilia, Brazil","13th International IEEE Conference on Intelligent Transportation Systems","9 Nov 2010","2010","","","539","544","To analyze fairness between passengers and airlines considering financial cost, the management of the adaptation for air traffic flow in a heuristic and dynamically manner is studied in this research. Multi-Agent theory with reinforcement learning approach is used as a basic methodology integrated with a system of Decision Support System Applied to Tactical Air Traffic Flow Management (SISCONFLUX). The objective to develop this model is to increase the safety preserve and reduce the air traffic congestions. Reward structure with evaluation functions of financial cost and delay's impact is proposed for related flights using real data fr om Brasilia's Flight Information Region (FIR-BS). With the developed model, the experimental results show that the time delay is 25% less than the results computed only by Graph Theory with the same data, and fairness considering financial cost factor can be used together with congestion scenario in the air traffic management without affecting safety and flow factors.","2153-0017","978-1-4244-7659-6","10.1109/ITSC.2010.5624988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5624988","","Delay;Aircraft;Atmospheric modeling;Markov processes;Multiagent systems;Airports;Safety","air traffic;decision support systems;delays;graph theory;learning (artificial intelligence);multi-agent systems;traffic engineering computing;travel industry","fairness analysis;Brasilia flight information region;reinforcement learning approach;airlines;multiagent theory;decision support system;tactical air traffic flow management;air traffic congestions;reward structure;financial cost evaluation function;delay impact;graph theory","","","","12","IEEE","9 Nov 2010","","","IEEE","IEEE Conferences"
"Discovering an Aid Policy to Minimize Student Evasion Using Offline Reinforcement Learning","L. M. De Lima; R. A. Krohling","Graduate Program in Computer Science PPGI, Federal University of Espirito Santo, UFES, Vitória, Brazil; Production Engineering Department, Graduate Program in Computer Science PPGI LABCIN, Federal University of Espirito Santo, UFES, Vitória, Brazil","2021 International Joint Conference on Neural Networks (IJCNN)","22 Sep 2021","2021","","","1","8","High dropout rates in tertiary education expose a lack of efficiency that causes frustration of expectations and financial waste. Predicting students at risk is not enough to avoid student dropout. Usually, an appropriate aid action must be discovered and applied in the proper time for each student. To tackle this sequential decision-making problem, we propose a decision support method to the selection of aid actions for students using offline reinforcement learning to support decision-makers effectively avoid student dropout. Additionally, a discretization of student's state space applying two different clustering methods is evaluated. Our experiments using logged data of real students shows, through off-policy evaluation, that the method should achieve roughly 1.0 to 1.5 times as much cumulative reward as the logged policy. So, it is feasible to help decision-makers apply appropriate aid actions and, possibly, reduce student dropout.","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9534159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9534159","","Clustering methods;Neural networks;Education;Decision making;Clustering algorithms;Reinforcement learning;Principal component analysis","","","","","","43","IEEE","22 Sep 2021","","","IEEE","IEEE Conferences"
"Reinforcement learning approach to locomotion adaptation in sloped environments","J. André; L. Costa; C. Santos","Department of Industrial Electronics, University of Minho; Department of Production and Systems, University of Minho; Department of Industrial Electronics, University of Minho","2014 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)","10 Jul 2014","2014","","","164","169","In this work, Path Integral Policy Improvement with Covariance Matrix Adaptation (PI2-CMA) is implemented and used to address biped locomotion optimization. Central Pattern Generators (CPGs) and Dynamic Movement Primitives (DMPs) are combined to easily produce complex joint trajectories for simulated DARwIn-OP humanoid robot. PI2-CMA seeks optimal DMPs' weights that maximize frontal velocity when facing different challenges. The simulation environments demand adaptation from the controller in order to successfully walk in different slopes. Elitism was introduced in PI2-CMA in order to improve the convergence property of the algorithm. Results show that these approaches enabled easy adaptation of DARwIn-OP to new situations. The results are very promising and demonstrate the flexibility at generating new trajectories for locomotion.","","978-1-4799-4254-1","10.1109/ICARSC.2014.6849780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6849780","","Legged locomotion;Trajectory;Joints;Convergence;Generators;Hip","covariance matrices;humanoid robots;learning (artificial intelligence);legged locomotion;motion control;trajectory control","reinforcement learning approach;locomotion adaptation;sloped environment;path integral policy improvement with covariance matrix adaptation;PI2-CMA;biped locomotion optimization;central pattern generators;CPG;dynamic movement primitives;DMP;DARwIn-OP humanoid robot;trajectory generation","","","","21","IEEE","10 Jul 2014","","","IEEE","IEEE Conferences"
"Dynamic provisioning of airport resources for inbound passenger flow using reinforcement learning","R. Suriyanarayanan; Y. S. Chati; A. Vasan","TCS Research, IIT Madras Research Park, Chennai, India; TCS Research, IIT Madras Research Park, Chennai, India; TCS Research, IIT Madras Research Park, Chennai, India","2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)","1 Nov 2022","2022","","","619","626","Correct provisioning of resources can help airports reduce cost of operations while ensuring quality of service seen by passengers. Specifically, dynamic provisioning of resources is needed to handle varying passenger loads that are subject to increased uncertainties due to the pandemic. Classical optimization methods could be computationally expensive; and rule-based heuristics that are well-tuned for normal operating conditions could be sub-optimal during periods of rapid change. We complement existing works with a Reinforcement Learning (RL)-based approach for dynamic provisioning of airport resources. Specifically, we consider the problem of jointly provisioning immigration and medical screening resources for inbound passenger flow. Our approach uses a state representation that encodes the supply and demand of the provisioned resources; and a reward engineered to capture the resource cost vs passenger delay trade-off. As baselines, we use two heuristics that statically and dynamically provision resources. We evaluate our approach on a simulated airport environment calibrated with real-world data. In scenarios that statistically significantly differ from the learning dataset, RL achieves up to ~20X decrease in passenger delay when compared to the heuristics for up to ~1.6X increase in resource cost.","","978-1-6654-6880-0","10.1109/ITSC55140.2022.9922383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9922383","","Costs;Supply and demand;Uncertainty;Pandemics;Optimization methods;Reinforcement learning;Quality of service","airports;optimisation;reinforcement learning;service industries","dynamic provisioning;airport resources;inbound passenger flow;reinforcement learning;varying passenger loads;optimization methods;immigration;medical screening resources;resource cost;passenger delay trade-off;simulated airport environment","","","","24","IEEE","1 Nov 2022","","","IEEE","IEEE Conferences"
"Tourism Route Recommendation Using Reinforcement Learning","M. I. Mubarak; Z. K. A. Baizal","School of Computing, Telkom University, Bandung, Indonesia; School of Computing, Telkom University, Bandung, Indonesia","2023 IEEE 8th International Conference for Convergence in Technology (I2CT)","23 May 2023","2023","","","1","7","Tourism is a rapidly growing sector that has a significant impact on increasing a country’s national income. Indonesia’s GDP is expected to grow in the coming years, and tourism is a major contributor to this growth. To address the high demand for tourism, we propose a personalized tourism route recommendation system that can assist tourists in planning their itineraries. This problem can be modeled as a Traveling Salesman Problem, which can be approached using Markov Decision Processes and reinforcement learning. In this paper, we proposed a method for generating N-days tourism routes in the Special Region of Yogyakarta that involves using Q-learning to recommend routes. We have included time constraints in our approach to fit the tour into a specific time frame and adhere to the operating hours of tourist attractions. Additionally, our method uses the Multi-Attribute Utility Theory to consider various attributes, such as rating, travel time, and cost, as cost functions to satisfy the user’s custom desired needs and preferences. The proposed method was compared to the Firefly algorithm in multiple experiments to assess its performance and determine its optimality. The experiment results showed that the proposed method is 42.89% more optimal for generating the tour than the Firefly algorithm.","","979-8-3503-3401-2","10.1109/I2CT57861.2023.10126347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10126347","Reinforcement learning;traveling salesman problem;route recommendation;itinerary planning","Q-learning;Costs;Economic indicators;Utility theory;Traveling salesman problems;Markov processes;Cost function","decision theory;Markov processes;recommender systems;reinforcement learning;travel industry;travelling salesman problems;utility theory","Indonesia;Markov decision processes;multiattribute utility theory;personalized tourism route recommendation system;Q-learning;reinforcement learning;specific time frame;time constraints;traveling salesman problem","","","","19","IEEE","23 May 2023","","","IEEE","IEEE Conferences"
"Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates","S. Gu; E. Holly; T. Lillicrap; S. Levine",MPI Tübingen; Google Brain; Google DeepMind; Google Brain,"2017 IEEE International Conference on Robotics and Automation (ICRA)","24 Jul 2017","2017","","","3389","3396","Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989385","","Robots;Training;Instruction sets;Learning (artificial intelligence);Neural networks;Safety;Heuristic algorithms","learning (artificial intelligence);manipulators","deep reinforcement learning;robotic manipulation;asynchronous off-policy updates;autonomous robots;hand-engineered policy representations;human-supplied demonstration;sample complexity;deep Q-functions;3D manipulation skills","","653","1","42","IEEE","24 Jul 2017","","","IEEE","IEEE Conferences"
"Target-driven visual navigation in indoor scenes using deep reinforcement learning","Y. Zhu; R. Mottaghi; E. Kolve; J. J. Lim; A. Gupta; L. Fei-Fei; A. Farhadi",Stanford University; Allen Institute for AI; Allen Institute for AI; Stanford University; Carnegie Mellon University; Stanford University; Allen Institute for AI,"2017 IEEE International Conference on Robotics and Automation (ICRA)","24 Jul 2017","2017","","","3357","3364","Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new goals, and (2) data inefficiency, i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows better generalization. To address the second issue, we propose the AI2-THOR framework, which provides an environment with high-quality 3D scenes and a physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989381","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989381","","Navigation;Training;Visualization;Learning (artificial intelligence);Three-dimensional displays;Physics;Robots","learning (artificial intelligence);path planning;robot vision","target-driven visual navigation;indoor scenes;deep reinforcement learning;actor-critic model;AI2-THOR framework;high-quality 3D scenes;physics engine;real robot scenario","","613","2","53","IEEE","24 Jul 2017","","","IEEE","IEEE Conferences"
"Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning","A. Nagabandi; G. Kahn; R. S. Fearing; S. Levine","University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","7559","7566","Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that neural network dynamics models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits that accomplish various complex locomotion tasks. We further propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5× on swimmer, cheetah, hopper, and ant agents. Videos can be found at https://sites.google.com/view/mbmf.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8463189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463189","","Task analysis;Predictive models;Neural networks;Data models;Heuristic algorithms;Machine learning;Complexity theory","computational complexity;learning (artificial intelligence);neural nets;predictive control","model-free learning;model-free fine-tuning;model-free deep reinforcement learning algorithms;model-based algorithms;model predictive control;model-based reinforcement learning algorithm;complex locomotion tasks;deep neural network dynamics models;model-free learner;model-based approaches;model-free methods;sample complexity;model-based deep reinforcement learning;robotic skills;MPC;plausible gaits;stable gaits","","273","","43","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Decentralized non-communicating multiagent collision avoidance with deep reinforcement learning","Y. F. Chen; M. Liu; M. Everett; J. P. How","Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, US; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA; Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA","2017 IEEE International Conference on Robotics and Automation (ICRA)","24 Jul 2017","2017","","","285","292","Finding feasible, collision-free paths for multiagent systems can be challenging, particularly in non-communicating scenarios where each agent's intent (e.g. goal) is unobservable to the others. In particular, finding time efficient paths often requires anticipating interaction with neighboring agents, the process of which can be computationally prohibitive. This work presents a decentralized multiagent collision avoidance algorithm based on a novel application of deep reinforcement learning, which effectively offloads the online computation (for predicting interaction patterns) to an offline learning procedure. Specifically, the proposed approach develops a value network that encodes the estimated time to the goal given an agent's joint configuration (positions and velocities) with its neighbors. Use of the value network not only admits efficient (i.e., real-time implementable) queries for finding a collision-free velocity vector, but also considers the uncertainty in the other agents' motion. Simulation results show more than 26% improvement in paths quality (i.e., time to reach the goal) when compared with optimal reciprocal collision avoidance (ORCA), a state-of-the-art collision avoidance strategy.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989037","","Collision avoidance;Learning (artificial intelligence);Kinematics;Decision making;Real-time systems;Navigation;Planning","collision avoidance;decentralised control;learning (artificial intelligence);mobile robots;multi-robot systems;robot kinematics","decentralized noncommunicating multiagent collision avoidance;deep reinforcement learning;collision-free paths;multiagent systems;online computation;interaction pattern prediction;offline learning procedure;agent joint configuration;agent positions;agent velocities;collision-free velocity vector;agent motion uncertainty;path quality improvement","","269","","26","IEEE","24 Jul 2017","","","IEEE","IEEE Conferences"
"Overcoming Exploration in Reinforcement Learning with Demonstrations","A. Nair; B. McGrew; M. Andrychowicz; W. Zaremba; P. Abbeel","University of California, Berkeley; OpenAI; OpenAI; OpenAI; University of California, Berkeley","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","6292","6299","Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8463162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463162","","Task analysis;Robots;Learning (artificial intelligence);Stacking;Training;Mathematical model;Games","control engineering computing;learning (artificial intelligence);manipulators","reinforcement learning;reward function;task horizon;RL methods;exploration problem;multistep robotics tasks;robot arm;deep deterministic policy gradients;hindsight experience replay;simulated robotics tasks","","236","","40","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Crowd-Robot Interaction: Crowd-Aware Robot Navigation With Attention-Based Deep Reinforcement Learning","C. Chen; Y. Liu; S. Kreiss; A. Alahi","VITA, Ecole Polytechnique Federal de Lausanne, EPFL, Switzerland; VITA, Ecole Polytechnique Federal de Lausanne, EPFL, Switzerland; VITA, Ecole Polytechnique Federal de Lausanne, EPFL, Switzerland; VITA, Ecole Polytechnique Federal de Lausanne, EPFL, Switzerland","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","6015","6022","Mobility in an effective and socially-compliant manner is an essential yet challenging task for robots operating in crowded spaces. Recent works have shown the power of deep reinforcement learning techniques to learn socially cooperative policies. However, their cooperation ability deteriorates as the crowd grows since they typically relax the problem as a one-way Human-Robot interaction problem. In this work, we want to go beyond first-order Human-Robot interaction and more explicitly model Crowd-Robot Interaction (CRI). We propose to (i) rethink pairwise interactions with a self-attention mechanism, and (ii) jointly model Human-Robot as well as Human-Human interactions in the deep reinforcement learning framework. Our model captures the Human-Human interactions occurring in dense crowds that indirectly affects the robot's anticipation capability. Our proposed attentive pooling mechanism learns the collective importance of neighboring humans with respect to their future states. Various experiments demonstrate that our model can anticipate human dynamics and navigate in crowds with time efficiency, outperforming state-of-the-art methods.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794134","","Robots;Navigation;Reinforcement learning;Planning;Task analysis;Human-robot interaction;Biological system modeling","human-robot interaction;learning (artificial intelligence);mobile robots;neural nets;pedestrians","crowded spaces;Human-Human interactions;deep reinforcement learning framework;dense crowds;human dynamics;crowd-aware robot navigation;attention-based deep reinforcement learning;robot operations;crowd-robot interaction","","214","","52","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning","P. Long; T. Fan; X. Liao; W. Liu; H. Zhang; J. Pan","Dorabot Inc., Shenzhen, China; Dorabot Inc., Shenzhen, China; Dorabot Inc., Shenzhen, China; Department of Computer Science, Fuzhou University, Fuzhou, China; Dorabot Inc., Shenzhen, China; Department of Mechanical and Biomedical Engineering, City University of Hong Kong, Hong Kong, China","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","6252","6259","Developing a safe and efficient collision avoidance policy for multiple robots is challenging in the decentralized scenarios where each robot generates its paths without observing other robots' states and intents. While other distributed multi-robot collision avoidance systems exist, they often require extracting agent-level features to plan a local collision-free action, which can be computationally prohibitive and not robust. More importantly, in practice the performance of these methods are much lower than their centralized counterparts. We present a decentralized sensor-level collision avoidance policy for multi-robot systems, which directly maps raw sensor measurements to an agent's steering commands in terms of movement velocity. As a first step toward reducing the performance gap between decentralized and centralized methods, we present a multi-scenario multi-stage training framework to learn an optimal policy. The policy is trained over a large number of robots on rich, complex environments simultaneously using a policy gradient based reinforcement learning algorithm. We validate the learned sensor-level collision avoidance policy in a variety of simulated scenarios with thorough performance evaluations and show that the final learned policy is able to find time efficient, collision-free paths for a large-scale robot system. We also demonstrate that the learned policy can be well generalized to new scenarios that do not appear in the entire training period, including navigating a heterogeneous group of robots and a large-scale scenario with 100 robots. Videos are available at https://sites.google.com/view/drlmaca.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8461113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461113","","Collision avoidance;Robot sensing systems;Robot kinematics;Navigation;Robustness;Training","collision avoidance;decentralised control;gradient methods;learning (artificial intelligence);mobile robots;multi-robot systems","multiscenario multistage training framework;optimal policy;policy gradient;reinforcement learning algorithm;learned sensor-level collision avoidance policy;final learned policy;collision-free paths;large-scale robot system;deep reinforcement learning;safe collision avoidance policy;efficient collision avoidance policy;optimally decentralized multirobot collision avoidance;agent-level feature extraction;decentralized methods;maps raw sensor measurements;multirobot systems;decentralized sensor-level collision avoidance policy;local collision-free action;distributed multirobot collision avoidance systems","","213","","27","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Information theoretic MPC for model-based reinforcement learning","G. Williams; N. Wagener; B. Goldfain; P. Drews; J. M. Rehg; B. Boots; E. A. Theodorou","The Institute for Robotics and Intelligent Machines, The Georgia Institute of Technology, Atlanta, GA, USA; The Institute for Robotics and Intelligent Machines, The Georgia Institute of Technology, Atlanta, GA, USA; The Institute for Robotics and Intelligent Machines, The Georgia Institute of Technology, Atlanta, GA, USA; The Institute for Robotics and Intelligent Machines, The Georgia Institute of Technology, Atlanta, GA, USA; The Institute for Robotics and Intelligent Machines, The Georgia Institute of Technology, Atlanta, GA, USA; The Institute for Robotics and Intelligent Machines, The Georgia Institute of Technology, Atlanta, GA, USA; The Institute for Robotics and Intelligent Machines, The Georgia Institute of Technology, Atlanta, GA, USA","2017 IEEE International Conference on Robotics and Automation (ICRA)","24 Jul 2017","2017","","","1714","1721","We introduce an information theoretic model predictive control (MPC) algorithm capable of handling complex cost criteria and general nonlinear dynamics. The generality of the approach makes it possible to use multi-layer neural networks as dynamics models, which we incorporate into our MPC algorithm in order to solve model-based reinforcement learning tasks. We test the algorithm in simulation on a cart-pole swing up and quadrotor navigation task, as well as on actual hardware in an aggressive driving task. Empirical results demonstrate that the algorithm is capable of achieving a high level of performance and does so only utilizing data collected from the system.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989202","","Robots;Heuristic algorithms;Trajectory;Learning (artificial intelligence);Cost function;Optimal control","control engineering computing;information theory;learning (artificial intelligence);mobile robots;multilayer perceptrons;nonlinear dynamical systems;optimal control;predictive control","information theoretic model predictive control;information theoretic MPC;model-based reinforcement learning;nonlinear dynamics;multilayer neural networks;dynamics models;cart-pole swing up;quadrotor navigation","","192","1","23","IEEE","24 Jul 2017","","","IEEE","IEEE Conferences"
"Navigating Occluded Intersections with Autonomous Vehicles Using Deep Reinforcement Learning","D. Isele; R. Rahimi; A. Cosgun; K. Subramanian; K. Fujimura",The University of Pennsylvania; The University of Virginia; Honda Research Institute; The Georgia Institute of Technology; Honda Research Institute,"2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","2034","2039","Providing an efficient strategy to navigate safely through unsignaled intersections is a difficult task that requires determining the intent of other drivers. We explore the effectiveness of Deep Reinforcement Learning to handle intersection problems. Using recent advances in Deep RL, we are able to learn policies that surpass the performance of a commonly-used heuristic approach in several metrics including task completion time and goal success rate and have limited ability to generalize. We then explore a system's ability to learn active sensing behaviors to enable navigating safely in the case of occlusions. Our analysis, provides insight into the intersection handling problem, the solutions learned by the network point out several shortcomings of current rule-based methods, and the failures of our current deep reinforcement learning system point to future research directions.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8461233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461233","","Autonomous vehicles;Automobiles;Machine learning;Safety;Navigation;Learning (artificial intelligence)","learning systems;mobile robots;navigation;path planning;road vehicles","autonomous vehicles;unsignaled intersections;Deep RL;intersection handling problem;deep reinforcement learning system;occluded intersections;active sensing behaviors","","183","4","30","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Self-Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation","G. Kahn; A. Villaflor; B. Ding; P. Abbeel; S. Levine","Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","5129","5136","Enabling robots to autonomously navigate complex environments is essential for real-world deployment. Prior methods approach this problem by having the robot maintain an internal map of the world, and then use a localization and planning method to navigate through the internal map. However, these approaches often include a variety of assumptions, are computationally intensive, and do not learn from failures. In contrast, learning-based methods improve as the robot acts in the environment, but are difficult to deploy in the real-world due to their high sample complexity. To address the need to learn complex policies with few samples, we propose a generalized computation graph that subsumes value-based model-free methods and model-based methods, with specific instantiations interpolating between model-free and model-based. We then instantiate this graph to form a navigation model that learns from raw images and is sample efficient. Our simulated car experiments explore the design decisions of our navigation model, and show our approach outperforms single-step and N-step double Q-learning. We also evaluate our approach on a real-world RC car and show it can learn to navigate through a complex indoor environment with a few hours of fully autonomous, self-supervised training. Videos of the experiments and code can be found at github.com/gkahn13/gcg.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460655","","Computational modeling;Navigation;Learning (artificial intelligence);Robots;Task analysis;Prediction algorithms;Planning","learning (artificial intelligence);mobile robots;path planning;robot vision","double Q-learning;self-supervised deep reinforcement learning;self-supervised training;model-based methods;value-based model-free methods;learning-based methods;planning method;internal map;robot navigation;generalized computation graph","","140","","32","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-Based Planning","A. Faust; K. Oslund; O. Ramirez; A. Francis; L. Tapia; M. Fiser; J. Davidson","Google Brain, Mountain View, CA; Google Brain, Mountain View, CA; Google Brain, Mountain View, CA; Google Brain, Mountain View, CA; Department of Computer Science, University of New Mexico, Albuquerque, NM; Google Brain, Mountain View, CA; Google Brain, Mountain View, CA","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","5113","5120","We present PRM-RL, a hierarchical method for long-range navigation task completion that combines sampling-based path planning with reinforcement learning (RL). The RL agents learn short-range, point-to-point navigation policies that capture robot dynamics and task constraints without knowledge of the large-scale topology. Next, the sampling-based planners provide roadmaps which connect robot configurations that can be successfully navigated by the RL agent. The same RL agents are used to control the robot under the direction of the planning, enabling long-range navigation. We use the Probabilistic Roadmaps (PRMs) for the sampling-based planner. The RL agents are constructed using feature-based and deep neural net policies in continuous state and action spaces. We evaluate PRM-RL, both in simulation and on-robot, on two navigation tasks with non-trivial robot dynamics: end-to-end differential drive indoor navigation in office environments, and aerial cargo delivery in urban environments with load displacement constraints. Our results show improvement in task completion over both RL agents on their own and traditional sampling-based planners. In the indoor navigation task, PRM-RL successfully completes up to 215 m long trajectories under noisy sensor conditions, and the aerial cargo delivery completes flights over 1000 m without violating the task constraints in an environment 63 million times larger than used in training.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8461096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461096","","Task analysis;Robot sensing systems;Indoor navigation;Aerospace electronics;Learning (artificial intelligence)","learning (artificial intelligence);mobile robots;navigation;neural nets;path planning;probability;robot dynamics;robot vision;sampling methods","sampling based planner;hierarchical method;sampling based path planning;large scale topology;probabilistic roadmaps;feature based deep neural net policies;continuous state;action spaces;simulation;office environments;aerial cargo delivery;urban environments;load displacement constraints;trajectories;noisy sensor conditions;flights;training;PRM RL;long range robotic navigation tasks;point to point navigation policies;end to end differential drive indoor navigation;nontrivial robot dynamics;robot configurations;task constraints;capture robot dynamics;RL agent;reinforcement learning","","133","","34","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Reinforcement learning of motor skills in high dimensions: A path integral approach","E. Theodorou; J. Buchli; S. Schaal","Computational Learning and Motor Control Laboratory in Computer Science, Biomedical Engineering, Neuroscience, University of Southern California, USA; Computational Learning and Motor Control Laboratory in Computer Science, Biomedical Engineering, Neuroscience, University of Southern California, USA; Computational Learning and Motor Control Laboratory in Computer Science, Biomedical Engineering, Neuroscience, University of Southern California, USA","2010 IEEE International Conference on Robotics and Automation","15 Jul 2010","2010","","","2397","2403","Reinforcement learning (RL) is one of the most general approaches to learning control. Its applicability to complex motor systems, however, has been largely impossible so far due to the computational difficulties that reinforcement learning encounters in high dimensional continuous state-action spaces. In this paper, we derive a novel approach to RL for parameterized control policies based on the framework of stochastic optimal control with path integrals. While solidly grounded in optimal control theory and estimation theory, the update equations for learning are surprisingly simple and have no danger of numerical instabilities as neither matrix inversions nor gradient learning rates are required. Empirical evaluations demonstrate significant performance improvements over gradient-based policy learning and scalability to high-dimensional control problems. Finally, a learning experiment on a robot dog illustrates the functionality of our algorithm in a real-world scenario. We believe that our new algorithm, Policy Improvement with Path Integrals (PI2), offers currently one of the most efficient, numerically robust, and easy to implement algorithms for RL in robotics.","1050-4729","978-1-4244-5038-1","10.1109/ROBOT.2010.5509336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5509336","","Optimal control;Function approximation;Stochastic processes;Integral equations;Scalability;Robots;Control systems;Learning systems;Inference algorithms;Stochastic resonance","control engineering computing;intelligent robots;learning (artificial intelligence);learning systems;optimal control;stochastic systems","reinforcement learning;motor skills;path integral approach;learning control;complex motor system;continuous state-action spaces;parameterized control policy;stochastic optimal control;path integrals;estimation theory;gradient-based policy learning;high-dimensional control problem;robot dog;policy improvement","","129","","26","IEEE","15 Jul 2010","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Vision-Based Robotic Grasping: A Simulated Comparative Evaluation of Off-Policy Methods","D. Quillen; E. Jang; O. Nachum; C. Finn; J. Ibarz; S. Levine","Google Brain; Google Brain; Google Brain; University of California, Berkeley; Google Brain; University of California, Berkeley","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","6284","6291","In this paper, we explore deep reinforcement learning algorithms for vision-based robotic grasping. Model-free deep reinforcement learning (RL) has been successfully applied to a range of challenging environments, but the proliferation of algorithms makes it difficult to discern which particular approach would be best suited for a rich, diverse task like grasping. To answer this question, we propose a simulated benchmark for robotic grasping that emphasizes off-policy learning and generalization to unseen objects. Off-policy learning enables utilization of grasping data over a wide variety of objects, and diversity is important to enable the method to generalize to new objects that were not seen during training. We evaluate the benchmark tasks against a variety of Q-function estimation methods, a method previously proposed for robotic grasping with deep neural network models, and a novel approach based on a combination of Monte Carlo return estimation and an off-policy correction. Our results indicate that several simple methods provide a surprisingly strong competitor to popular algorithms such as double Q-learning, and our analysis of stability sheds light on the relative tradeoffs between the algorithms1.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8461039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461039","","Grasping;Robots;Task analysis;Benchmark testing;Monte Carlo methods;Machine learning;Training","learning (artificial intelligence);Monte Carlo methods;neural nets;robot vision","deep neural network models;off-policy correction;vision-based robotic grasping;off-policy methods;deep reinforcement learning algorithms;off-policy learning;Monte Carlo methods","","114","","46","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"End-to-End Race Driving with Deep Reinforcement Learning","M. Jaritz; R. de Charette; M. Toromanoff; E. Perot; F. Nashashibi","Valeo Driving Assistance Research, Bobigny; Inria, RITS Team, Paris; Valeo Driving Assistance Research, Bobigny; Valeo Driving Assistance Research, Bobigny; Inria, RITS Team, Paris","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","2070","2075","We present research using the latest reinforcement learning algorithm for end-to-end driving without any mediated perception (object recognition, scene understanding). The newly proposed reward and learning strategies lead together to faster convergence and more robust driving using only RGB image from a forward facing camera. An Asynchronous Actor Critic (A3C) framework is used to learn the car control in a physically and graphically realistic rally game, with the agents evolving simultaneously on tracks with a variety of road structures (turns, hills), graphics (seasons, location) and physics (road adherence). A thorough evaluation is conducted and generalization is proven on unseen tracks and using legal speed limits. Open loop tests on real sequences of images show some domain adaption capability of our method.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460934","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460934","","Automobiles;Training;Brakes;Games;Roads;Physics;Computer architecture","automobiles;cameras;computer games;image colour analysis;learning (artificial intelligence);traffic engineering computing","deep reinforcement learning;mediated perception;object recognition;scene understanding;learning strategies;RGB image;forward facing camera;car control;road structures;end-to-end race driving;reinforcement learning algorithm;legal speed limits;asynchronous actor critic framework;rally game;temperature 3.0 C","","99","1","24","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Autonomous helicopter control using reinforcement learning policy search methods","J. A. Bagnell; J. G. Schneider","Carnegie Mellon's Robotics Institute, USA; NA","Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164)","9 Jul 2003","2001","2","","1615","1620 vol.2","Many control problems in the robotics field can be cast as partially observed Markovian decision problems (POMDPs), an optimal control formalism. Finding optimal solutions to such problems in general, however is known to be intractable. It has often been observed that in practice, simple structured controllers suffice for good sub-optimal control, and recent research in the artificial intelligence community has focused on policy search methods as techniques for finding sub-optimal controllers when such structured controllers do exist. Traditional model-based reinforcement learning algorithms make a certainty equivalence assumption on their learned models and calculate optimal policies for a maximum-likelihood Markovian model. We consider algorithms that evaluate and synthesize controllers under distributions of Markovian models. Previous work has demonstrated that algorithms that maximize mean reward with respect to model uncertainty leads to safer and more robust controllers. We consider briefly other performance criterion that emphasize robustness and exploration in the search for controllers, and note the relation with experiment design and active learning. To validate the power of the approach on a robotic application we demonstrate the presented learning control algorithm by flying an autonomous helicopter. We show that the controller learned is robust and delivers good performance in this real-world domain.","1050-4729","0-7803-6576-3","10.1109/ROBOT.2001.932842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=932842","","Helicopters;Search methods;Robots;Robust control;Learning systems;Optimal control;Uncertainty;Control system synthesis;Artificial intelligence;Computer crashes","aircraft control;helicopters;learning (artificial intelligence);suboptimal control;Markov processes;decision theory;computational complexity;remotely operated vehicles;control system synthesis;Bayes methods;search problems","autonomous helicopter control;reinforcement learning;policy search methods;partially observed Markovian decision problems;robustness;active learning","","99","1","12","IEEE","9 Jul 2003","","","IEEE","IEEE Conferences"
"Composable Deep Reinforcement Learning for Robotic Manipulation","T. Haarnoja; V. Pong; A. Zhou; M. Dalal; P. Abbeel; S. Levine","Berkeley Artificial Intelligence Research, Berkeley, UC; Berkeley Artificial Intelligence Research, Berkeley, UC; Berkeley Artificial Intelligence Research, Berkeley, UC; Berkeley Artificial Intelligence Research, Berkeley, UC; Open AI; Berkeley Artificial Intelligence Research, Berkeley, UC","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","6244","6251","Model-free deep reinforcement learning has been shown to exhibit good performance in domains ranging from video games to simulated robotic manipulation and locomotion. However, model-free methods are known to perform poorly when the interaction time with the environment is limited, as is the case for most real-world robotic tasks. In this paper, we study how maximum entropy policies trained using soft Q-learning can be applied to real-world robotic manipulation. The application of this method to real-world manipulation is facilitated by two important features of soft Q-learning. First, soft Q-learning can learn multimodal exploration strategies by learning policies represented by expressive energy-based models. Second, we show that policies learned with soft Q-learning can be composed to create new policies, and that the optimality of the resulting policy can be bounded in terms of the divergence between the composed policies. This compositionality provides an especially valuable tool for real-world manipulation, where constructing new policies by composing existing skills can provide a large gain in efficiency over training from scratch. Our experimental evaluation demonstrates that soft Q-learning is substantially more sample efficient than prior model-free deep reinforcement learning methods, and that compositionality can be performed for both simulated and real-world tasks.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460756","","Entropy;Robots;Learning (artificial intelligence);Neural networks;Machine learning;Task analysis;Training","control engineering computing;learning (artificial intelligence);manipulators","composable deep reinforcement;model-free deep reinforcement learning;simulated robotic manipulation;model-free methods;real-world robotic tasks;maximum entropy policies;soft Q-learning;real-world robotic manipulation","","87","1","38","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"A comparison of direct and model-based reinforcement learning","C. G. Atkeson; J. C. Santamaria","College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA","Proceedings of International Conference on Robotics and Automation","6 Aug 2002","1997","4","","3557","3564 vol.4","This paper compares direct reinforcement learning (no explicit model) and model-based reinforcement learning on a simple task: pendulum swing up. We find that in this task model-based approaches support reinforcement learning from smaller amounts of training data and efficient handling of changing goals.","","0-7803-3612-7","10.1109/ROBOT.1997.606886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=606886","","Learning;Training data;Robots;Control system synthesis;Educational institutions;Force control;Control systems;Computational modeling;Jacobian matrices;State-space methods","robots;nonlinear control systems;learning (artificial intelligence);model reference adaptive control systems","direct reinforcement learning;model-based reinforcement learning;pendulum swing-up;acrobot","","85","","38","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Multilateral surgical pattern cutting in 2D orthotropic gauze with deep reinforcement learning policies for tensioning","B. Thananjeyan; A. Garg; S. Krishnan; C. Chen; L. Miller; K. Goldberg","EECS, UC, Berkeley, CA, USA; CS, Stanford University, Stanford, CA, US; EECS, UC, Berkeley, CA, USA; EECS, UC, Berkeley, CA, USA; EECS, UC, Berkeley, CA, USA; EECS, UC, Berkeley, CA, USA","2017 IEEE International Conference on Robotics and Automation (ICRA)","24 Jul 2017","2017","","","2371","2378","In the Fundamentals of Laparoscopic Surgery (FLS) standard medical training regimen, the Pattern Cutting task requires residents to demonstrate proficiency by maneuvering two tools, surgical scissors and tissue gripper, to accurately cut a circular pattern on surgical gauze suspended at the corners. Accuracy of cutting depends on tensioning, wherein the gripper pinches a point on the gauze in R3 and pulls to induce and maintain tension in the material as cutting proceeds. An automated tensioning policy maps the current state of the gauze to output a direction of pulling as an action. The optimal tensioning policy depends on both the choice of pinch point and cutting trajectory. We explore the problem of learning a tensioning policy conditioned on specific cutting trajectories. Every timestep, we allow the gripper to react to the deformation of the gauze and progress of the cutting trajectory with a translation unit vector along an allowable set of directions. As deformation is difficult to analytically model and explicitly observe, we leverage deep reinforcement learning with direct policy search methods to learn tensioning policies using a finite-element simulator and then transfer them to a physical system. We compare the Deep RL tensioning policies with fixed and analytic (opposing the error vector with a fixed pinch point) policies on a set of 17 open and closed curved contours in simulation and 4 patterns in physical experiments with the da Vinci Research Kit (dVRK). Our simulation results suggest that learning to tension with Deep RL can significantly improve performance and robustness to noise and external forces.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989275","","Trajectory;Robots;Training;Learning (artificial intelligence);Minimally invasive surgery;Tools;Deformable models","biomedical equipment;finite element analysis;grippers;learning (artificial intelligence);medical robotics;surgery","multilateral surgical pattern cutting;2D orthotropic gauze;deep reinforcement learning policies;fundamentals-of-laparoscopic surgery standard medical training regimen;FLS standard medical training regimen;surgical scissors;tissue gripper;surgical gauze;automated optimal tensioning policy;pinch point;cutting trajectory;tensioning policy conditioned learning;grippers;gauze deformation;translation unit vector;analytical model;explicit model;direct policy search methods;finite-element simulator;physical system;deep RL tensioning policies;open curved contours;closed curved contours;da Vinci Research Kit;dVRK;performance improvement;noise robustness improvement;external force robustness improvement","","67","","33","IEEE","24 Jul 2017","","","IEEE","IEEE Conferences"
"Safe Reinforcement Learning With Model Uncertainty Estimates","B. Lütjens; M. Everett; J. P. How","Aerospace Controls Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Aerospace Controls Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Aerospace Controls Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","8662","8668","Many current autonomous systems are being designed with a strong reliance on black box predictions from deep neural networks (DNNs). However, DNNs tend to be overconfident in predictions on unseen data and can give unpredictable results for far-from-distribution test data. The importance of predictions that are robust to this distributional shift is evident for safety-critical applications, such as collision avoidance around pedestrians. Measures of model uncertainty can be used to identify unseen data, but the state-of-the-art extraction methods such as Bayesian neural networks are mostly intractable to compute. This paper uses MC-Dropout and Bootstrapping to give computationally tractable and parallelizable uncertainty estimates. The methods are embedded in a Safe Reinforcement Learning framework to form uncertainty-aware navigation around pedestrians. The result is a collision avoidance policy that knows what it does not know and cautiously avoids pedestrians that exhibit unseen behavior. The policy is demonstrated in simulation to be more robust to novel observations and take safer actions than an uncertainty-unaware baseline.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793611","","Uncertainty;Collision avoidance;Neural networks;Computational modeling;Training;Data models;Reinforcement learning","Bayes methods;collision avoidance;control engineering computing;learning (artificial intelligence);neural nets;safety","model uncertainty estimates;current autonomous systems;strong reliance;black box predictions;deep neural networks;DNNs;unpredictable results;far-from-distribution test data;distributional shift;safety-critical applications;pedestrians;state-of-the-art extraction methods;Bayesian neural networks;MC-Dropout;computationally tractable uncertainty estimates;parallelizable uncertainty estimates;uncertainty-aware navigation;collision avoidance policy;unseen behavior;uncertainty-unaware baseline;safe reinforcement learning framework","","65","","48","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Dexterous Manipulation with Deep Reinforcement Learning: Efficient, General, and Low-Cost","H. Zhu; A. Gupta; A. Rajeswaran; S. Levine; V. Kumar",UC Berkeley; UC Berkeley; University of Washington; UC Berkeley; Google Brain,"2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","3651","3657","Dexterous multi-fingered robotic hands can perform a wide range of manipulation skills, making them an appealing component for general-purpose robotic manipulators. However, such hands pose a major challenge for autonomous control, due to the high dimensionality of their configuration space and complex intermittent contact interactions. In this work, we propose deep reinforcement learning (deep RL) as a scalable solution for learning complex, contact rich behaviors with multi-fingered hands. Deep RL provides an end-to-end approach to directly map sensor readings to actions, without the need for task specific models or policy classes. We show that contact-rich manipulation behavior with multi-fingered hands can be learned by directly training with model-free deep RL algorithms in the real world, with minimal additional assumption and without the aid of simulation. We learn to perform a variety of tasks on two different low-cost hardware platforms entirely from scratch, and further study how the learning can be accelerated by using a small number of human demonstrations. Our experiments demonstrate that complex multi-fingered manipulation skills can be learned in the real world in about 4-7 hours for most tasks, and that demonstrations can decrease this to 2-3 hours, indicating that direct deep RL training in the real world is a viable and practical alternative to simulation and model-based control. https:// sites.google.com/view/deeprl-handmanipulation.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794102","","Task analysis;Valves;Acceleration;Reinforcement learning;Robot sensing systems;Hardware","control engineering computing;dexterous manipulators;learning (artificial intelligence);neural nets;robot programming","deep reinforcement learning;multifingered hands;contact-rich manipulation behavior;model-free deep RL algorithms;complex multifingered manipulation skills;direct deep RL training;model-based control;dexterous manipulation;dexterous multifingered robotic hands;general-purpose robotic manipulators;autonomous control;complex intermittent contact interactions","","60","","30","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Supervised Autonomous Exploration in Office Environments","D. Zhu; T. Li; D. Ho; C. Wang; M. Q. . -H. Meng","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, SAR, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, SAR, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, SAR, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, SAR, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, SAR, China","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","7548","7555","Exploration region selection is an essential decision making process in autonomous robot exploration task. While a majority of greedy methods are proposed to deal with this problem, few efforts are made to investigate the importance of predicting long-term planning. In this paper, we present an algorithm that utilizes deep reinforcement learning (DRL) to learn exploration knowledge over office blueprints, which enables the agent to predict a long-term visiting order for unexplored subregions. On the basis of this algorithm, we propose an exploration architecture that integrates a DRL model, a next-best-view (NBV) selection approach and a structural integrity measurement to further improve the exploration performance. At the end of this paper, we evaluate the proposed architecture against other methods on several new office maps, showing that the agent can efficiently explore uncertain regions with a shorter path and smarter behaviors.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8463213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463213","","Planning;Optimization;Prediction algorithms;Task analysis;Predictive models;Computer architecture;Uncertainty","decision making;learning (artificial intelligence);mobile robots;path planning","supervised autonomous exploration;office environments;exploration region selection;autonomous robot exploration task;greedy methods;long-term planning;deep reinforcement learning;exploration knowledge;office blueprints;DRL model;next-best-view selection approach;structural integrity measurement;office maps;decision making process","","52","","21","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Virtual vs. real: Trading off simulations and physical experiments in reinforcement learning with Bayesian optimization","A. Marco; F. Berkenkamp; P. Hennig; A. P. Schoellig; A. Krause; S. Schaal; S. Trimpe","Max Planck Institute for Intelligent Systems, Tübingen, Germany; Eidgenossische Technische Hochschule Zurich, Zurich, ZH, CH; Max Planck Institute for Intelligent Systems, Tübingen, Germany; Dynamic Systems Lab (DSL), University of Toronto Institute for Aerospace Studies (UTIAS), Canada; Max Planck ETH Center for Learning Systems, Tübingen, Switzerland; Max-Planck-Institut fur Intelligente Systeme, Stuttgart, Baden-WÃ¼rttemberg, DE; Max Planck Institute for Intelligent Systems, Tübingen, Germany","2017 IEEE International Conference on Robotics and Automation (ICRA)","24 Jul 2017","2017","","","1557","1563","In practice, the parameters of control policies are often tuned manually. This is time-consuming and frustrating. Reinforcement learning is a promising alternative that aims to automate this process, yet often requires too many experiments to be practical. In this paper, we propose a solution to this problem by exploiting prior knowledge from simulations, which are readily available for most robotic platforms. Specifically, we extend Entropy Search, a Bayesian optimization algorithm that maximizes information gain from each experiment, to the case of multiple information sources. The result is a principled way to automatically combine cheap, but inaccurate information from simulations with expensive and accurate physical experiments in a cost-effective manner. We apply the resulting method to a cart-pole system, which confirms that the algorithm can find good control policies with fewer experiments than standard Bayesian optimization on the physical system only.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989186","","Bayes methods;Robots;Learning (artificial intelligence);Entropy;Cost function;Kernel","Bayes methods;control engineering computing;entropy;learning (artificial intelligence);optimisation;robots;simulation","simulations;reinforcement learning;Bayesian optimization;control policies;robotic platforms;entropy search;information sources;cart-pole system","","52","","29","IEEE","24 Jul 2017","","","IEEE","IEEE Conferences"
"RTMBA: A Real-Time Model-Based Reinforcement Learning Architecture for robot control","T. Hester; M. Quinlan; P. Stone","Department of Computer Science, University of Texas, Austin, USA; Department of Computer Science, University of Texas, Austin, USA; Department of Computer Science, University of Texas, Austin, USA","2012 IEEE International Conference on Robotics and Automation","28 Jun 2012","2012","","","85","90","Reinforcement Learning (RL) is a paradigm for learning decision-making tasks that could enable robots to learn and adapt to their situation on-line. For an RL algorithm to be practical for robotic control tasks, it must learn in very few samples, while continually taking actions in real-time. Existing model-based RL methods learn in relatively few samples, but typically take too much time between each action for practical on-line learning. In this paper, we present a novel parallel architecture for model-based RL that runs in real-time by 1) taking advantage of sample-based approximate planning methods and 2) parallelizing the acting, model learning, and planning processes in a novel way such that the acting process is sufficiently fast for typical robot control cycles. We demonstrate that algorithms using this architecture perform nearly as well as methods using the typical sequential architecture when both are given unlimited time, and greatly out-perform these methods on tasks that require real-time actions such as controlling an autonomous vehicle.","1050-4729","978-1-4673-1405-3","10.1109/ICRA.2012.6225072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225072","","Planning;Real time systems;Computational modeling;Robots;Approximation algorithms;Multicore processing","approximation theory;control engineering computing;decision making;learning (artificial intelligence);parallel architectures;real-time systems;robots","RTMBA;real-time model-based reinforcement learning architecture;robot control;RL;decision-making learning;parallel architecture;sample based approximate planning methods;planning processes;autonomous vehicle","","49","7","17","IEEE","28 Jun 2012","","","IEEE","IEEE Conferences"
"Generalization through Simulation: Integrating Simulated and Real Data into Deep Reinforcement Learning for Vision-Based Autonomous Flight","K. Kang; S. Belkhale; G. Kahn; P. Abbeel; S. Levine","Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","6008","6014","Deep reinforcement learning provides a promising approach for vision-based control of real-world robots. However, the generalization of such models depends critically on the quantity and variety of data available for training. This data can be difficult to obtain for some types of robotic systems, such as fragile, small-scale quadrotors. Simulated rendering and physics can provide for much larger datasets, but such data is inherently of lower quality: many of the phenomena that make the real-world autonomous flight problem challenging, such as complex physics and air currents, are modeled poorly or not at all, and the systematic differences between simulation and the real world are typically impossible to eliminate. In this work, we investigate how data from both simulation and the real world can be combined in a hybrid deep reinforcement learning algorithm. Our method uses real-world data to learn about the dynamics of the system, and simulated data to learn a generalizable perception system that can enable the robot to avoid collisions using only a monocular camera. We demonstrate our approach on a real-world nano aerial vehicle collision avoidance task, showing that with only an hour of real-world data, the quadrotor can avoid collisions in new environments with various lighting conditions and geometry. Code, instructions for building the aerial vehicles, and videos of the experiments can be found at github.com/gkahn13/GtS.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793735","","Data models;Robots;Task analysis;Predictive models;Neural networks;Reinforcement learning;Collision avoidance","aircraft control;autonomous aerial vehicles;collision avoidance;data analysis;helicopters;learning (artificial intelligence);mobile robots;robot vision","vision-based autonomous flight;fragile scale quadrotors;small-scale quadrotors;complex physics;air currents;hybrid deep reinforcement learning algorithm;generalizable perception system;nanoaerial vehicle collision avoidance task;real data;simulated data","","47","","45","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Reinforcement learning-based inter- and intra-application thermal optimization for lifetime improvement of multicore systems","A. Das; R. A. Shafik; G. V. Merrett; B. M. Al-Hashimi; A. Kumar; B. Veeravalli","National University of Singapore, Singapore; University of Southampton, Southampton, UK; University of Southampton, Southampton, UK; University of Southampton, Southampton, UK; National University of Singapore, Singapore; National University of Singapore, Singapore","2014 51st ACM/EDAC/IEEE Design Automation Conference (DAC)","21 Aug 2014","2014","","","1","6","The thermal profile of multicore systems vary both within an application's execution (intra) and also when the system switches from one application to another (inter). In this paper, we propose an adaptive thermal management approach to improve the lifetime reliability of multicore systems by considering both inter- and intra-application thermal variations. Fundamental to this approach is a reinforcement learning algorithm, which learns the relationship between the mapping of threads to cores, the frequency of a core and its temperature (sampled from on-board thermal sensors). Action is provided by overriding the operating system's mapping decisions using affinity masks and dynamically changing CPU frequency using in-kernel governors. Lifetime improvement is achieved by controlling not only the peak and average temperatures but also thermal cycling, which is an emerging wear-out concern in modern systems. The proposed approach is validated experimentally using an Intel quad-core platform executing a diverse set of multimedia benchmarks. Results demonstrate that the proposed approach minimizes average temperature, peak temperature and thermal cycling, improving the mean-time-to-failure (MTTF) by an average of 2× for intra-application and 3× for inter-application scenarios when compared to existing thermal management techniques. Furthermore, the dynamic and static energy consumption are also reduced by an average 10% and 11% respectively.","0738-100X","978-1-4799-3017-3","10.1145/2593069.2593199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6881497","","Temperature sensors;Reliability;Linux;Temperature measurement;Legged locomotion","electronic engineering computing;integrated circuit reliability;learning (artificial intelligence);microprocessor chips;multiprocessing systems;thermal management (packaging)","reinforcement learning;interapplication thermal optimization;intraapplication thermal optimization;multicore system lifetime improvement;thermal profile;adaptive thermal management;lifetime reliability;affinity masks;CPU frequency;in-kernel governor;multimedia benchmark;average temperature minimization;peak temperature minimization;thermal cycling minimization;mean-time-to-failure improvement","","46","1","18","","21 Aug 2014","","","IEEE","IEEE Conferences"
"OptLayer - Practical Constrained Optimization for Deep Reinforcement Learning in the Real World","T. -H. Pham; G. De Magistris; R. Tachibana","IBM Research AI, Tokyo, Japan; IBM Research AI, Tokyo, Japan; IBM Research AI, Tokyo, Japan","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","6236","6243","While deep reinforcement learning techniques have recently produced considerable achievements on many decision-making problems, their use in robotics has largely been limited to simulated worlds or restricted motions, since unconstrained trial-and-error interactions in the real world can have undesirable consequences for the robot or its environment. To overcome such limitations, we propose a novel reinforcement learning architecture, OptLayer, that takes as inputs possibly unsafe actions predicted by a neural network and outputs the closest actions that satisfy chosen constraints. While learning control policies often requires carefully crafted rewards and penalties while exploring the range of possible actions, OptLayer ensures that only safe actions are actually executed and unsafe predictions are penalized during training. We demonstrate the effectiveness of our approach on robot reaching tasks, both simulated and in the real world.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460547","","Robot kinematics;Neural networks;Task analysis;Optimization;Learning (artificial intelligence);Training","control engineering computing;decision making;learning (artificial intelligence);manipulators;neural nets;optimisation","decision-making problems;reinforcement learning architecture;OptLayer;neural network;closest actions;safe actions;robot reaching tasks;practical constrained optimization;deep reinforcement learning techniques","","43","","20","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Efficient reinforcement learning for robots using informative simulated priors","M. Cutler; J. P. How","Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA; Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA","2015 IEEE International Conference on Robotics and Automation (ICRA)","2 Jul 2015","2015","","","2605","2612","Autonomous learning through interaction with the physical world is a promising approach to designing controllers and decision-making policies for robots. Unfortunately, learning on robots is often difficult due to the large number of samples needed for many learning algorithms. Simulators are one way to decrease the samples needed from the robot by incorporating prior knowledge of the dynamics into the learning algorithm. In this paper we present a novel method for transferring data from a simulator to a robot, using simulated data as a prior for real-world learning. A Bayesian nonparametric prior is learned from a potentially black-box simulator. The mean of this function is used as a prior for the Probabilistic Inference for Learning Control (PILCO) algorithm. The simulated prior improves the convergence rate and performance of PILCO by directing the policy search in areas of the state-space that have not yet been observed by the robot. Simulated and hardware results show the benefits of using the prior knowledge in the learning framework.","1050-4729","978-1-4799-6923-4","10.1109/ICRA.2015.7139550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7139550","","Heuristic algorithms;Robots;Data models;Gaussian processes;Hardware;Prediction algorithms;Mathematical model","Bayes methods;learning (artificial intelligence);learning systems;nonparametric statistics;robots","reinforcement learning algorithm;informative simulated priors;autonomous learning;decision-making policy;controller design;Bayesian nonparametric prior;black-box simulator;probabilistic inference for learning control algorithm;PILCO algorithm;convergence rate","","42","","24","IEEE","2 Jul 2015","","","IEEE","IEEE Conferences"
"Open Loop Position Control of Soft Continuum Arm Using Deep Reinforcement Learning","S. Satheeshbabu; N. K. Uppalapati; G. Chowdhary; G. Krishnan","Industrial and Systems Engineering Department, University of Illinois at Urbana-Champaign, 104 S Mathews Ave, Urbana; Industrial and Systems Engineering Department, University of Illinois at Urbana-Champaign, 104 S Mathews Ave, Urbana; Department of Agricultural and Biological Engineering and the Coordinated Science Lab (CSL), University of Illinois at Urbana-Champaign, Pennsylvania Urbana, Illinois; Industrial and Systems Engineering Department, University of Illinois at Urbana-Champaign, 104 S Mathews Ave, Urbana, IL","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","5133","5139","Soft robots undergo large nonlinear spatial deformations due to both inherent actuation and external loading. The physics underlying these deformations is complex, and often requires intricate analytical and numerical models. The complexity of these models may render traditional model-based control difficult and unsuitable. Model-free methods offer an alternative for analyzing the behavior of such complex systems without the need for elaborate modeling techniques. In this paper, we present a model-free approach for open loop position control of a soft spatial continuum arm, based on deep reinforcement learning. The continuum arm is pneumatically actuated and attains a spatial work-space by a combination of unidirectional bending and bidirectional torsional deformation. We use Deep-Q Learning with experience replay to train the system in simulation. The efficacy and robustness of the control policy obtained from the system is validated both in simulation and on the continuum arm prototype for varying external loading conditions.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793653","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793653","","Manipulators;Load modeling;Mathematical model;Numerical models;Strain","bending;control engineering computing;learning (artificial intelligence);manipulator dynamics;neural nets;numerical analysis;pneumatic actuators;position control;torsion","open loop position control;deep reinforcement learning;soft robots;nonlinear spatial deformations;inherent actuation;numerical models;soft spatial continuum arm;unidirectional bending deformation;bidirectional torsional deformation;Deep-Q Learning;continuum arm prototype;external loading conditions","","42","","34","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Deep reinforcement learning for tensegrity robot locomotion","M. Zhang; X. Geng; J. Bruce; K. Caluwaerts; M. Vespignani; V. SunSpiral; P. Abbeel; S. Levine","Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA; Department of Computer Engineering, University of California, Santa Cruz, CA; Autodesk, Inc., San Francisco, CA; Nasa Ames Research Center, Moffett Field, CA; Nasa Ames Research Center, Moffett Field, CA; OpenAI, San Francisco, CA; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA","2017 IEEE International Conference on Robotics and Automation (ICRA)","24 Jul 2017","2017","","","634","641","Tensegrity robots, composed of rigid rods connected by elastic cables, have a number of unique properties that make them appealing for use as planetary exploration rovers. However, control of tensegrity robots remains a difficult problem due to their unusual structures and complex dynamics. In this work, we show how locomotion gaits can be learned automatically using a novel extension of mirror descent guided policy search (MDGPS) applied to periodic locomotion movements, and we demonstrate the effectiveness of our approach on tensegrity robot locomotion. We evaluate our method with real-world and simulated experiments on the SUPERball tensegrity robot, showing that the learned policies generalize to changes in system parameters, unreliable sensor measurements, and variation in environmental conditions, including varied terrains and a range of different gravities. Our experiments demonstrate that our method not only learns fast, power-efficient feedback policies for rolling gaits, but that these policies can succeed with only the limited onboard sensing provided by SUPERball's accelerometers. We compare the learned feedback policies to learned open-loop policies and hand-engineered controllers, and demonstrate that the learned policy enables the first continuous, reliable locomotion gait for the real SUPERball robot. Our code and supplementary material is available from http://rll.berkeley.edu/drl_tensegrity.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989079","","Robot sensing systems;Mirrors;Neural networks;Training;Gravity;Robustness","learning (artificial intelligence);mobile robots;planetary rovers","deep reinforcement learning;tensegrity robot locomotion;rigid rods;elastic cables;planetary exploration rovers;complex dynamics;locomotion gaits;mirror descent guided policy search;MDGPS;periodic locomotion movements;SUPERball tensegrity robot;unreliable sensor measurements;power-efficient feedback policies;rolling gaits;onboard sensing;SUPERball accelerometers;open-loop policies;hand-engineered controllers;reliable locomotion gait;SUPERball robot","","41","1","38","IEEE","24 Jul 2017","","","IEEE","IEEE Conferences"
"Distributed reinforcement learning for power limited many-core system performance optimization","Z. Chen; D. Marculescu","Electrical and Computer Engineering, Carnegie Mellon University; Electrical and Computer Engineering, Carnegie Mellon University","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","23 Apr 2015","2015","","","1521","1526","As power density emerges as the main constraint for many-core systems, controlling power consumption under the Thermal Design Power (TDP) while maximizing the performance becomes increasingly critical. To dynamically save power, Dynamic Voltage Frequency Scaling (DVFS) techniques have proved to be effective and are widely available commercially. In this paper, we present an On-line Distributed Reinforcement Learning (OD-RL) based DVFS control algorithm for many-core system performance improvement under power constraints. At the finer grain, a per-core Reinforcement Learning (RL) method is used to learn the optimal control policy of the Voltage/Frequency (VF) levels in a system model-free manner. At the coarser grain, an efficient global power budget reallocation algorithm is used to maximize the overall performance. The experiments show that compared to the state-of-the-art algorithms: 1) OD-RL produces up to 98% less budget overshoot, 2) up to 44.3x better throughput per over-the-budget energy and up to 23% higher energy efficiency, and 3) two orders of magnitude speedup over state-of-the-art techniques for systems with hundreds of cores.","1558-1101","978-3-9815-3705-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092630","","Power demand;Algorithm design and analysis;Multicore processing;Learning (artificial intelligence);Throughput;Scalability;Complexity theory","electronic engineering computing;learning (artificial intelligence);multiprocessing systems;optimal control;performance evaluation;power aware computing;resource allocation","distributed reinforcement learning;power limited many-core system performance optimization;power consumption;thermal design power;TDP;dynamic voltage frequency scaling techniques;DVFS techniques;online distributed reinforcement learning based DVFS control algorithm;OD-RL based DVFS control algorithm;many-core system performance improvement;per-core reinforcement learning method;optimal control policy;global power budget reallocation algorithm;energy efficiency","","41","","25","","23 Apr 2015","","","IEEE","IEEE Conferences"
"A friction-model-based framework for Reinforcement Learning of robotic tasks in non-rigid environments","A. Colomé; A. Planells; C. Torras","Institut de Robòtica i Informàtica Industrial (UPC-CSIC), Barcelona, Spain; Institut de Robòtica i Informàtica Industrial (UPC-CSIC), Barcelona, Spain; Institut de Robòtica i Informàtica Industrial (UPC-CSIC), Barcelona, Spain","2015 IEEE International Conference on Robotics and Automation (ICRA)","2 Jul 2015","2015","","","5649","5654","Learning motion tasks in a real environment with deformable objects requires not only a Reinforcement Learning (RL) algorithm, but also a good motion characterization, a preferably compliant robot controller, and an agent giving feedback for the rewards/costs in the RL algorithm. In this paper, we unify all these parts in a simple but effective way to properly learn safety-critical robotic tasks such as wrapping a scarf around the neck (so far, of a mannequin).","1050-4729","978-1-4799-6923-4","10.1109/ICRA.2015.7139990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7139990","","Robots;Friction;Joints;Trajectory;Torque;Acceleration;Dynamics","friction;human-robot interaction;learning (artificial intelligence);motion control;robot dynamics","friction-model-based framework;reinforcement learning;robotic tasks;nonrigid environments;motion task learning;deformable objects;motion characterization;compliant robot controller;RL algorithm;safety-critical robotic tasks;compliant controller;inverse dynamic model;IDM;friction hystheresis;Barrett WAM;robot joints;friction-aware controller;dynamic movement primitives;DMP;visual-force feedback;RL algorithm","","38","","14","IEEE","2 Jul 2015","","","IEEE","IEEE Conferences"
"DRiLLS: Deep Reinforcement Learning for Logic Synthesis","A. Hosny; S. Hashemi; M. Shalan; S. Reda","Computer Science Dept, Brown University Providence, RI; School of Engineering, Brown University Providence, RI; Computer Science Dept, American University in Cairo, Cairo, Egypt; School of Engineering, Brown University Providence, RI","2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC)","26 Mar 2020","2020","","","581","586","Logic synthesis requires extensive tuning of the synthesis optimization flow where the quality of results (QoR) depends on the sequence of optimizations used. Efficient design space exploration is challenging due to the exponential number of possible optimization permutations. Therefore, automating the optimization process is necessary. In this work, we propose a novel reinforcement learning-based methodology that navigates the optimization space without human intervention. We demonstrate the training of an Advantage Actor Critic (A2C) agent that seeks to minimize area subject to a timing constraint. Using the proposed methodology, designs can be optimized autonomously with no-humans in-loop. Evaluation on the comprehensive EPFL benchmark suite shows that the agent outperforms existing exploration methodologies and improves QoRs by an average of 13%.","2153-697X","978-1-7281-4123-7","10.1109/ASP-DAC47756.2020.9045559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9045559","","Reinforcement learning;Benchmark testing;Space exploration;Delays;Circuit synthesis;Optimization;Tuning","learning (artificial intelligence);logic design","DRiLLS;deep reinforcement learning;logic synthesis;QoR;design space exploration;exponential number;optimization process;reinforcement learning-based methodology;advantage actor critic agent;EPFL benchmark suit;A2C agent;timing constraint;quality of results","","36","","20","IEEE","26 Mar 2020","","","IEEE","IEEE Conferences"
"Autonomous drifting using simulation-aided reinforcement learning","M. Cutler; J. P. How","Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA; Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA","2016 IEEE International Conference on Robotics and Automation (ICRA)","9 Jun 2016","2016","","","5442","5448","We introduce a framework that combines simple and complex continuous state-action simulators with a real-world robot to efficiently find good control policies, while minimizing the number of samples needed from the physical robot. The framework combines the strengths of various simulation levels by first finding optimal policies in a simple model, and then using that solution to initialize a gradient-based learner in a more complex simulation. The policy and transition dynamics from the complex simulation are in turn used to guide the learning in the physical world. A method is developed for transferring information gathered in the physical world back to the learning agent in the simulation. The new information is used to re-evaluate whether the original simulated policy is still optimal given the updated knowledge from the real-world. This reverse transfer is critical to minimizing samples from the physical world. The new framework is demonstrated on a robotic car learning to perform controlled drifting maneuvers. A video of the car's performance can be found at https: //youtu.be/opsmd5yuBF0.","","978-1-4673-8026-3","10.1109/ICRA.2016.7487756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487756","","Computational modeling;Mathematical model;Optimal control;Robots;Data models;Heuristic algorithms;Approximation algorithms","learning (artificial intelligence);robots","autonomous drifting;simulation-aided reinforcement learning;continuous state-action simulators;real-world robot;control policies;gradient-based learner;information transfer;robotic car learning;controlled drifting maneuvers","","36","","18","IEEE","9 Jun 2016","","","IEEE","IEEE Conferences"
"Risk Averse Robust Adversarial Reinforcement Learning","X. Pan; D. Seita; Y. Gao; J. Canny","University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","8522","8528","Deep reinforcement learning has recently made significant progress in solving computer games and robotic control tasks. A known problem, though, is that policies overfit to the training environment and may not avoid rare, catastrophic events such as automotive accidents. A classical technique for improving the robustness of reinforcement learning algorithms is to train on a set of randomized environments, but this approach only guards against common situations. Recently, robust adversarial reinforcement learning (RARL) was developed, which allows efficient applications of random and systematic perturbations by a trained adversary. A limitation of RARL is that only the expected control objective is optimized; there is no explicit modeling or optimization of risk. Thus the agents do not consider the probability of catastrophic events (i.e., those inducing abnormally large negative reward), except through their effect on the expected objective. In this paper we introduce risk-averse robust adversarial reinforcement learning (RARARL), using a risk-averse protagonist and a risk-seeking adversary. We test our approach on a self-driving vehicle controller. We use an ensemble of policy networks to model risk as the variance of value functions. We show through experiments that a risk-averse agent is better equipped to handle a risk-seeking adversary, and experiences substantially fewer crashes compared to agents trained without an adversary. Supplementary materials are available at https://sites.google.com/view/rararl.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794293","","Reinforcement learning;Training;Mathematical model;Robustness;Autonomous vehicles;Task analysis;Accidents","learning (artificial intelligence);optimisation;probability","risk averse robust adversarial reinforcement learning;deep reinforcement learning;computer games;robotic control;automotive accidents;optimization;probability;RARARL;self-driving vehicle controller","","34","","41","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Reinforcement learning with multi-fidelity simulators","M. Cutler; T. J. Walsh; J. P. How","Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA; Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA; Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA","2014 IEEE International Conference on Robotics and Automation (ICRA)","29 Sep 2014","2014","","","3888","3895","We present a framework for reinforcement learning (RL) in a scenario where multiple simulators are available with decreasing amounts of fidelity to the real-world learning scenario. Our framework is designed to limit the number of samples used in each successively higher-fidelity/cost simulator by allowing the agent to choose to run trajectories at the lowest level that will still provide it with information. The approach transfers state-action Q-values from lower-fidelity models as heuristics for the “Knows What It Knows” family of RL algorithms, which is applicable over a wide range of possible dynamics and reward representations. Theoretical proofs of the framework's sample complexity are given and empirical results are demonstrated on a remote controlled car with multiple simulators. The approach allows RL algorithms to find near-optimal policies for the real world with fewer expensive real-world samples than previous transfer approaches or learning without simulators.","1050-4729","978-1-4799-3685-4","10.1109/ICRA.2014.6907423","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6907423","","Heuristic algorithms;Complexity theory;Learning (artificial intelligence);Polynomials;Optimization;Data models;Silicon","automobiles;control engineering computing;learning (artificial intelligence);mobile robots;telerobotics;trajectory control","reinforcement learning;RL algorithms;multifidelity simulators;state-action Q-values;remote controlled car;robotic control algorithm;trajectory level","","32","1","18","IEEE","29 Sep 2014","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning of Navigation in a Complex and Crowded Environment with a Limited Field of View","J. Choi; K. Park; M. Kim; S. Seok","NAVER LABS, Gyeonggi-do, South Korea; NAVER LABS, Gyeonggi-do, South Korea; NAVER LABS, Gyeonggi-do, South Korea; NAVER LABS, Gyeonggi-do, South Korea","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","5993","6000","Mobile robots are required to navigate freely in a complex and crowded environment in order to provide services to humans. For this navigation ability, deep reinforcement learning (DRL)-based methods are gaining increasing attentions. However, existing DRL methods require a wide field of view (FOV), which imposes the usage of high-cost lidar devices. In this paper, we explore the possibility of replacing expensive lidar devices with affordable depth cameras which have a limited FOV. First, we analyze the effect of a limited field of view in the DRL agents. Second, we propose a LSTM agent with Local-Map Critic (LSTM-LMC), which is a novel DRL method to learn efficient navigation in a complex environment with a limited FOV. Lastly, we introduce the dynamics randomization technique to improve the robustness of the DRL agents in the real world. We found that our method with a limited FOV can outperform the methods having a wide FOV but limited memory. We provide the empirical evidence that our method learns to implicitly model the surrounding environment and dynamics of other agents. We also show that a robot with a single depth camera can navigate through a complex real-world environment using our method.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793979","","Navigation;Mobile robots;Reinforcement learning;Laser radar;Robot sensing systems;Cameras","cameras;learning (artificial intelligence);mobile robots;navigation;neural nets;optical radar;path planning;robot vision","deep reinforcement learning-based methods;DRL agents;LSTM agent;Local-Map Critic;LSTM-LMC;wide FOV;single depth camera;mobile robots;lidar devices;DRL method;depth cameras;dynamics randomization technique","","31","","36","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"A deep reinforcement learning framework for optimizing fuel economy of hybrid electric vehicles","P. Zhao; Y. Wang; N. Chang; Q. Zhu; X. Lin","Department of ECE, Northeastern University, Boston, MA, USA; Department of EECS, Syracuse University, Syracuse, NY, USA; School of EE, Korea Advanced Institute of Science and Engineering, Daejeon, Korea; Department of EECS, Northwestern University, Evanston, IL, USA; Department of ECE, Northeastern University, Boston, MA, USA","2018 23rd Asia and South Pacific Design Automation Conference (ASP-DAC)","22 Feb 2018","2018","","","196","202","Hybrid electric vehicles employ a hybrid propulsion system to combine the energy efficiency of electric motor and a long driving range of internal combustion engine, thereby achieving a higher fuel economy as well as convenience compared with conventional ICE vehicles. However, the relatively complicated powertrain structures of HEVs necessitate an effective power management policy to determine the power split between ICE and EM. In this work, we propose a deep reinforcement learning framework of the HEV power management with the aim of improving fuel economy. The DRL technique is comprised of an offline deep neural network construction phase and an online deep Q-learning phase. Unlike traditional reinforcement learning, DRL presents the capability of handling the high dimensional state and action space in the actual decision-making process, making it suitable for the HEV power management problem. Enabled by the DRL technique, the derived HEV power management policy is close to optimal, fully model-free, and independent of a prior knowledge of driving cycles. Simulation results based on actual vehicle setup over real-world and testing driving cycles demonstrate the effectiveness of the proposed framework on optimizing HEV fuel economy.","2153-697X","978-1-5090-0602-1","10.1109/ASPDAC.2018.8297305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8297305","","Hybrid electric vehicles;Ice;Power system management;Propulsion;Fuel economy;Batteries;Torque","electric motors;electric propulsion;energy management systems;fuel economy;hybrid electric vehicles;learning (artificial intelligence);neural nets;optimisation;power engineering computing","deep reinforcement learning framework;hybrid electric vehicles;hybrid propulsion system;energy efficiency;electric motor;long driving range;internal combustion engine;higher fuel economy;conventional ICE vehicles;effective power management policy;power split;DRL technique;offline deep neural network construction phase;online deep Q-learning phase;high dimensional state;HEV power management problem;actual vehicle setup;HEV power management policy;HEV fuel economy optimization","","26","","31","IEEE","22 Feb 2018","","","IEEE","IEEE Conferences"
"Rearrangement with Nonprehensile Manipulation Using Deep Reinforcement Learning","W. Yuan; J. A. Stork; D. Kragic; M. Y. Wang; K. Hang","Hong Kong University of Science and Technology, HKUST Robotics Institute; Robotics, Perception and Learning Lab, KTH Royal Institute of Technology, Sweden; Department of Electronic and Computer Engineering; Hong Kong University of Science and Technology, HKUST Robotics Institute; Hong Kong University of Science and Technology, HKUST Robotics Institute","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","270","277","Rearranging objects on a tabletop surface by means of nonprehensile manipulation is a task which requires skillful interaction with the physical world. Usually, this is achieved by precisely modeling physical properties of the objects, robot, and the environment for explicit planning. In contrast, as explicitly modeling the physical environment is not always feasible and involves various uncertainties, we learn a nonprehensile rearrangement strategy with deep reinforcement learning based on only visual feedback. For this, we model the task with rewards and train a deep Q-network. Our potential field-based heuristic exploration strategy reduces the amount of collisions which lead to suboptimal outcomes and we actively balance the training set to avoid bias towards poor examples. Our training process leads to quicker learning and better performance on the task as compared to uniform exploration and standard experience replay. We demonstrate empirical evidence from simulation that our method leads to a success rate of 85%, show that our system can cope with sudden changes of the environment, and compare our performance with human level performance.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8462863","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462863","","Planning;Task analysis;Robots;Tools;Cameras;Visualization;Training","learning (artificial intelligence);manipulators;mobile robots;path planning","potential field-based heuristic exploration strategy;deep Q-network;nonprehensile rearrangement strategy;physical environment;physical world;skillful interaction;tabletop surface;rearranging objects;deep reinforcement learning;nonprehensile manipulation;quicker learning;training process","","26","","34","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Neural networks and differential dynamic programming for reinforcement learning problems","A. Yamaguchi; C. G. Atkeson","The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States","2016 IEEE International Conference on Robotics and Automation (ICRA)","9 Jun 2016","2016","","","5434","5441","We explore a model-based approach to reinforcement learning where partially or totally unknown dynamics are learned and explicit planning is performed. We learn dynamics with neural networks, and plan behaviors with differential dynamic programming (DDP). In order to handle complicated dynamics, such as manipulating liquids (pouring), we consider temporally decomposed dynamics. We start from our recent work [1] where we used locally weighted regression (LWR) to model dynamics. The major contribution of this paper is making use of deep learning in the form of neural networks with stochastic DDP, and showing the advantages of neural networks over LWR. For this purpose, we extend neural networks for: (1) modeling prediction error and output noise, (2) computing an output probability distribution for a given input distribution, and (3) computing gradients of output expectation with respect to an input. Since neural networks have nonlinear activation functions, these extensions were not easy. We provide an analytic solution for these extensions using some simplifying assumptions. We verified this method in pouring simulation experiments. The learning performance with neural networks was better than that of LWR. The amount of spilled materials was reduced. We also present early results of robot experiments using a PR2. Accompanying video: https://youtu.be/aM3hE1J5W98.","","978-1-4673-8026-3","10.1109/ICRA.2016.7487755","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487755","","Neural networks;Robots;Computational modeling;Stochastic processes;Dynamic programming;Learning (artificial intelligence);Predictive models","dynamic programming;learning (artificial intelligence);neural nets;regression analysis;statistical distributions;transfer functions","nonlinear activation function;output probability distribution;LWR;locally weighted regression;reinforcement learning;DDP;differential dynamic programming;neural network","","24","","25","IEEE","9 Jun 2016","","","IEEE","IEEE Conferences"
"Decentralized Reinforcement Learning Control of a Robotic Manipulator","L. Busoniu; B. De Schutter; R. Babuska","Delft Center for Systems and Control, Delft University of Technology, Delft, The Netherlands; Delft Center for Systems and Control, Delft University of Technology, Delft, The Netherlands; Delft Center for Systems and Control, Delft University of Technology, Delft, The Netherlands","2006 9th International Conference on Control, Automation, Robotics and Vision","16 Jul 2007","2006","","","1","6","Multi-agent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, etc. Learning approaches to multi-agent control, many of them based on reinforcement learning (RL), are investigated in complex domains such as teams of mobile robots. However, the application of decentralized RL to low-level control tasks is not as intensively studied. In this paper, we investigate centralized and decentralized RL, emphasizing the challenges and potential advantages of the latter. These are then illustrated on an example: learning to control a two-link rigid manipulator. Some open issues and future research directions in decentralized RL are outlined","","1-4244-0341-3","10.1109/ICARCV.2006.345351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4150192","multi-agent learning;decentralized control;rein-forcement learning","Learning;Robot control;Manipulators;Telecommunication control;Distributed control;Mobile robots;Control systems;Multiagent systems;Process control;Resource management","decentralised control;learning systems;manipulators","multiagent learning;decentralized reinforcement learning control;robotic manipulator;multiagent systems;centralized reinforcement learning;two-link rigid manipulator","","24","","22","IEEE","16 Jul 2007","","","IEEE","IEEE Conferences"
"Algorithm for Autonomous Power-Increase Operation Using Deep Reinforcement Learning and a Rule-Based System","D. Lee; A. M. Arigi; J. Kim","Department of Nuclear Engineering, Chosun University, Gwangju, South Korea; Department of Nuclear Engineering, Chosun University, Gwangju, South Korea; Department of Nuclear Engineering, Chosun University, Gwangju, South Korea","IEEE Access","13 Nov 2020","2020","8","","196727","196746","The power start-up operation of a nuclear power plant (NPP) increases the reactor power to the full-power condition for electricity generation. Compared to full-power operation, the power-increase operation requires significantly more decision-making and therefore increases the potential for human errors. While previous studies have investigated the use of artificial intelligence (AI) techniques for NPP control, none of them have addressed making the relatively complicated power-increase operation fully autonomous. This study focused on developing an algorithm for converting all the currently manual activities in the NPP power-increase process to autonomous operations. An asynchronous advantage actor-critic, which is a type of deep reinforcement learning method, and a long short-term memory network were applied to the operator tasks for which establishing clear rules or logic was challenging, while a rule-based system was developed for those actions, which could be described by simple logic (such as if-then logic). The proposed autonomous power-increase control algorithm was trained and validated using a compact nuclear simulator (CNS). The simulation results were used to evaluate the algorithm’s ability to control the parameters within allowable limits, and the proposed power-increase control algorithm was proven capable of identifying an acceptable operation path for increasing the reactor power from 2% to 100% at a specified rate of power increase. In addition, the pattern of operation that resulted from the autonomous control simulation was found to be identical to that of the established operation strategy. These results demonstrate the potential feasibility of fully autonomous control of the NPP power-increase operation.","2169-3536","","10.1109/ACCESS.2020.3034218","Basic Science Research Program through the National Research Foundation of Korea (NRF), funded by the Ministry of Science, ICT & Future Planning(grant numbers:N01190021-06); Korean Government, Ministry of Science and ICT(grant numbers:NRF-2018M2B2B1065651); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240916","Nuclear power plant;autonomous operation;power-increase operation;reinforcement learning;asynchronous advantage actor-critic","Inductors;Task analysis;Reinforcement learning;Neural networks;Automation;Control systems","control engineering computing;decision making;learning (artificial intelligence);nuclear engineering computing;nuclear power stations;power engineering computing;power station control","rule-based system;power start-up operation;nuclear power plant;reactor power;full-power condition;full-power operation;decision-making;artificial intelligence techniques;NPP control;relatively complicated power-increase operation;deep reinforcement learning method;operator tasks;clear rules;autonomous power-increase control algorithm;compact nuclear simulator;fully autonomous control;NPP power-increase operation;electricity generation;long short-term memory network","","24","","61","CCBY","27 Oct 2020","","","IEEE","IEEE Journals"
"Bayesian reinforcement learning in continuous POMDPs with application to robot navigation","S. Ross; B. Chaib-draa; J. Pineau","School of Computer Science, McGill University, Montreal, Canada; Department of Computer Science, Laval University, QUE, Canada; School of Computer Science, McGill University, Montreal, Canada","2008 IEEE International Conference on Robotics and Automation","13 Jun 2008","2008","","","2845","2851","We consider the problem of optimal control in continuous and partially observable environments when the parameters of the model are not known exactly. Partially observable Markov decision processes (POMDPs) provide a rich mathematical model to handle such environments but require a known model to be solved by most approaches. This is a limitation in practice as the exact model parameters are often difficult to specify exactly. We adopt a Bayesian approach where a posterior distribution over the model parameters is maintained and updated through experience with the environment. We propose a particle filter algorithm to maintain the posterior distribution and an online planning algorithm, based on trajectory sampling, to plan the best action to perform under the current posterior. The resulting approach selects control actions which optimally trade-off between 1) exploring the environment to learn the model, 2) identifying the system's state, and 3) exploiting its knowledge in order to maximize long-term rewards. Our preliminary results on a simulated robot navigation problem show that our approach is able to learn good models of the sensors and actuators, and performs as well as if it had the true model.","1050-4729","978-1-4244-1646-2","10.1109/ROBOT.2008.4543641","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4543641","","Bayesian methods;Learning;Navigation;Optimal control;Mathematical model;Particle filters;Trajectory;Sampling methods;Robot sensing systems;Motion planning","belief networks;learning (artificial intelligence);Markov processes;optimal control;particle filtering (numerical methods);path planning;position control;robots","Bayesian reinforcement learning;continuous POMDP;robot navigation;optimal control;observable Markov decision processes;particle filter algorithm;online planning algorithm;trajectory sampling","","23","","28","IEEE","13 Jun 2008","","","IEEE","IEEE Conferences"
"End-to-end Decentralized Multi-robot Navigation in Unknown Complex Environments via Deep Reinforcement Learning","J. Lin; X. Yang; P. Zheng; H. Cheng","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, Guangdong Province, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, Guangdong Province, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, Guangdong Province, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, Guangdong Province, China","2019 IEEE International Conference on Mechatronics and Automation (ICMA)","29 Aug 2019","2019","","","2493","2500","In this paper, a novel deep reinforcement learning (DRL)-based method is proposed to navigate the robot team through unknown complex environments, where the geometric centroid of the robot team aims to reach the goal position while avoiding collisions and maintaining connectivity. Decentralized robot-level policies are derived using a mechanism of centralized learning and decentralized executing. The proposed method can derive end-to-end policies, which map raw lidar measurements into velocity control commands of robots without the necessity of constructing obstacle maps. Simulation and indoor real-world unmanned ground vehicles (UGVs) experimental results verify the effectiveness of the proposed method.","2152-744X","978-1-7281-1699-0","10.1109/ICMA.2019.8816208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816208","Multi-robot system;Navigation;Deep reinforcement learning","Navigation;Collision avoidance;Task analysis;Velocity control;Robot sensing systems;Multi-robot systems","collision avoidance;decentralised control;learning (artificial intelligence);mobile robots;multi-robot systems;optical radar;remotely operated vehicles;velocity control","decentralized robot-level policies;centralized learning;end-to-end policies;end-to-end decentralized multirobot navigation;unknown complex environments;deep reinforcement learning-based method;robot team;geometric centroid;connectivity;raw lidar measurements;velocity control commands;obstacle maps;indoor real-world unmanned ground vehicles","","22","","25","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Quasi-online reinforcement learning for robots","B. Bakker; V. Zhumatiy; G. Gruener; J. Schmidhuber","Informatics Institute, University of Amsterdam, Netherlands; IDSIA, Manno Lugano, Switzerland; CSEM, Alpnach, Switzerland; IDSIA, Manno Lugano, Switzerland","Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.","26 Jun 2006","2006","","","2997","3002","This paper describes quasi-online reinforcement learning: while a robot is exploring its environment, in the background a probabilistic model of the environment is built on the fly as new experiences arrive; the policy is trained concurrently based on this model using an anytime algorithm. Prioritized sweeping, directed exploration, and transformed reward functions provide additional speed-ups. The robot quickly learns goal-directed policies from scratch, requiring few interactions with the environment and making efficient use of available computation time. From an outside perspective it learns the behavior online and in real time. We describe comparisons with standard methods and show the individual utility of each of the proposed techniques","1050-4729","0-7803-9505-0","10.1109/ROBOT.2006.1642157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1642157","","Learning;Robot sensing systems;Acceleration;Informatics;Signal processing;Functional programming;Robot programming;Concurrent computing;Availability","learning (artificial intelligence);mobile robots;path planning;probability","quasi-online reinforcement learning;robots;probabilistic model;prioritized sweeping;directed exploration;transformed reward functions","","21","","12","IEEE","26 Jun 2006","","","IEEE","IEEE Conferences"
"Hybrid Zero Dynamics Inspired Feedback Control Policy Design for 3D Bipedal Locomotion using Reinforcement Learning","G. A. Castillo; B. Weng; W. Zhang; A. Hereid","SUSTech Institute of Robotics, Southern University of Science and Technology, Shenzhen, China; SUSTech Institute of Robotics, Southern University of Science and Technology, Shenzhen, China; SUSTech Institute of Robotics, Southern University of Science and Technology, China; Mechanical and Aerospace Engineering, Ohio State University, Columbus, OH, USA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","8746","8752","This paper presents a novel model-free reinforcement learning (RL) framework to design feedback control policies for 3D bipedal walking. Existing RL algorithms are often trained in an end-to-end manner or rely on prior knowledge of some reference joint trajectories. Different from these studies, we propose a novel policy structure that appropriately incorporates physical insights gained from the hybrid nature of the walking dynamics and the well-established hybrid zero dynamics approach for 3D bipedal walking. As a result, the overall RL framework has several key advantages, including lightweight network structure, short training time, and less dependence on prior knowledge. We demonstrate the effectiveness of the proposed method on Cassie, a challenging 3D bipedal robot. The proposed solution produces stable limit walking cycles that can track various walking speed in different directions. Surprisingly, without specifically trained with disturbances to achieve robustness, it also performs robustly against various adversarial forces applied to the torso towards both the forward and the backward directions.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197175","","Legged locomotion;Three-dimensional displays;Trajectory;Robustness;Torso;Robot kinematics","feedback;humanoid robots;learning systems;legged locomotion;robot dynamics","hybrid zero dynamics inspired feedback control policy design;3D bipedal locomotion;model-free reinforcement learning framework;feedback control policies;3D bipedal walking;RL algorithms;reference joint trajectories;policy structure;hybrid nature;walking dynamics;RL framework;lightweight network structure;short training time;3D bipedal robot;stable limit walking cycles;walking speed","","21","","34","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Autonomous Driving with Latent State Inference and Spatial-Temporal Relationships","X. Ma; J. Li; M. J. Kochenderfer; D. Isele; K. Fujimura","Honda Research Institute, US; Honda Research Institute, US; Stanford University; Honda Research Institute, US; Honda Research Institute, US","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","6064","6071","Deep reinforcement learning (DRL) provides a promising way for learning navigation in complex autonomous driving scenarios. However, identifying the subtle cues that can indicate drastically different outcomes remains an open problem with designing autonomous systems that operate in human environments. In this work, we show that explicitly inferring the latent state and encoding spatial-temporal relationships in a reinforcement learning framework can help address this difficulty. We encode prior knowledge on the latent states of other drivers through a framework that combines the reinforcement learner with a supervised learner. In addition, we model the influence passing between different vehicles through graph neural networks (GNNs). The proposed framework significantly improves performance in the context of navigating T-intersections compared with state-of-the-art baseline approaches.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9562006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562006","","Training;Space vehicles;Couplings;Navigation;Supervised learning;Reinforcement learning;Graph neural networks","deep learning (artificial intelligence);graph theory;mobile robots;reinforcement learning","latent state inference;deep reinforcement learning;navigation;complex autonomous driving scenarios;subtle cues;open problem;autonomous systems;human environments;encoding spatial-temporal relationships;reinforcement learning framework;encode prior knowledge;reinforcement learner;navigating T-intersections;state-of-the-art baseline approaches;DRL","","21","","42","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Autonomous Navigation of an Ultrasound Probe Towards Standard Scan Planes with Deep Reinforcement Learning","K. Li; J. Wang; Y. Xu; H. Qin; D. Liu; L. Liu; M. Q. . -H. Meng","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China; School of Biomedical Engineering, Shenzhen University, Shenzhen, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China; Sonoscape Medical Corp, Shenzhen, China; Department of Pain, Peking University Shenzhen Hospital, Shenzhen, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Electronic and Electrical Engineering of the Southern University of Science and Technology in Shenzhen, China","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","8302","8308","Autonomous ultrasound (US) acquisition is an important yet challenging task, as it involves interpretation of the highly complex and variable images and their spatial relationships. In this work, we propose a deep reinforcement learning framework to autonomously control the 6-D pose of a virtual US probe based on real-time image feedback to navigate towards the standard scan planes under the restrictions in real-world US scans. Furthermore, we propose a confidence-based approach to encode the optimization of image quality in the learning process. We validate our method in a simulation environment built with real-world data collected in the US imaging of the spine. Experimental results demonstrate that our method can perform reproducible US probe navigation towards the standard scan plane with an accuracy of 4.91mm/4.65° in the intra-patient setting, and accomplish the task in the intra- and inter-patient settings with a success rate of 92% and 46%, respectively. The results also show that the introduction of image quality optimization in our method can effectively improve the navigation performance.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561295","Autonomous Ultrasound Acquisition;Deep Reinforcement Learning;Image Quality Optimization","Image quality;Ultrasonic imaging;Navigation;Magnetic resonance imaging;Reinforcement learning;Real-time systems;Probes","biomedical ultrasonics;learning (artificial intelligence);medical image processing;navigation;object tracking;pneumodynamics","autonomous navigation;ultrasound probe;standard scan plane;highly complex images;variable images;deep reinforcement learning framework;virtual US probe;real-time image feedback;confidence-based approach;US imaging;reproducible US probe navigation;image quality optimization;navigation performance;autonomous ultrasound acquisition","","21","","30","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"DeepWalk: Omnidirectional Bipedal Gait by Deep Reinforcement Learning","D. Rodriguez; S. Behnke","Autonomous Intelligent Systems (AIS) Group, Computer Science Institute VI, University of Bonn, Germany; Autonomous Intelligent Systems (AIS) Group, Computer Science Institute VI, University of Bonn, Germany","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","3033","3039","Bipedal walking is one of the most difficult but exciting challenges in robotics. The difficulties arise from the complexity of high-dimensional dynamics, sensing and actuation limitations combined with real-time and computational constraints. Deep Reinforcement Learning (DRL) holds the promise to address these issues by fully exploiting the robot dynamics with minimal craftsmanship. In this paper, we propose a novel DRL approach that enables an agent to learn omnidirectional locomotion for humanoid (bipedal) robots. Notably, the locomotion behaviors are accomplished by a single control policy (a single neural network). We achieve this by introducing a new curriculum learning method that gradually increases the task difficulty by scheduling target velocities. In addition, our method does not require reference motions which facilities its application to robots with different kinematics, and reduces the overall complexity. Finally, different strategies for sim-to-real transfer are presented which allow us to transfer the learned policy to a real humanoid robot.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561717","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561717","","Legged locomotion;Three-dimensional displays;Neural networks;Humanoid robots;Reinforcement learning;Robot sensing systems;Real-time systems","control engineering computing;deep learning (artificial intelligence);gait analysis;humanoid robots;legged locomotion;robot dynamics;robot kinematics","computational constraints;robot dynamics;minimal craftsmanship;DRL approach;humanoid robot;single control policy;single neural network;curriculum learning method;task difficulty;omnidirectional bipedal gait;bipedal walking;high-dimensional dynamics;DeepWalk;deep reinforcement learning;omnidirectional locomotion learning;target velocity scheduling;robot kinematics;sim-to-real transfer","","20","","22","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Modular reinforcement learning for self-adaptive energy efficiency optimization in multicore system","Z. Wang; Z. Tian; J. Xu; R. K. V. Maeda; H. Li; P. Yang; Z. Wang; L. H. K. Duong; Z. Wang; X. Chen",The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology,"2017 22nd Asia and South Pacific Design Automation Conference (ASP-DAC)","20 Feb 2017","2017","","","684","689","Energy-efficiency is becoming increasingly important to modern computing systems with multi-/many-core architectures. Dynamic Voltage and Frequency Scaling (DVFS), as an effective low-power technique, has been widely applied to improve energy-efficiency in commercial multi-core systems. However, due to the large number of cores and growing complexity of emerging applications, it is difficult to efficiently find a globally optimized voltage/frequency assignment at runtime. In order to improve the energy-efficiency for the overall multicore system, we propose an online DVFS control strategy based on core-level Modular Reinforcement Learning (MRL) to adaptively select appropriate operating frequencies for each individual core. Instead of focusing solely on the local core conditions, MRL is able to make comprehensive decisions by considering the running-states of multiple cores without incurring exponential memory cost which is necessary in traditional Monolithic Reinforcement Learning (RL). Experimental results on various realistic applications and different system scales show that the proposed approach improves up to 28% energy-efficiency compared to the recent individual-RL approach.","2153-697X","978-1-5090-1558-0","10.1109/ASPDAC.2017.7858403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858403","","Learning (artificial intelligence);Mediation","learning (artificial intelligence);multiprocessing systems;optimisation;power aware computing","modular reinforcement learning;self-adaptive energy efficiency optimization;multicore system;dynamic voltage and frequency scaling;online DVFS control strategy;MRL;many-core architectures","","20","","22","IEEE","20 Feb 2017","","","IEEE","IEEE Conferences"
"Inverse reinforcement learning of behavioral models for online-adapting navigation strategies","M. Herman; V. Fischer; T. Gindele; W. Burgard","Robert Bosch GmbH, Corporate Sector Research and Advance Engineering, Stuttgart, Germany; Robert Bosch GmbH, Corporate Sector Research and Advance Engineering, Stuttgart, Germany; Robert Bosch GmbH, Corporate Sector Research and Advance Engineering, Stuttgart, Germany; Department of Computer Science, University of Freiburg, Freiburg, Germany","2015 IEEE International Conference on Robotics and Automation (ICRA)","2 Jul 2015","2015","","","3215","3222","To increase the acceptance of autonomous systems in populated environments, it is indispensable to teach them social behavior. We would expect a social robot, which plans its motions among humans, to consider both the social acceptability of its behavior as well as task constraints, such as time limits. These requirements are often contradictory and therefore resulting in a trade-off. For example, a robot has to decide whether it is more important to quickly achieve its goal or to comply with social conventions, such as the proximity to humans, i.e., the robot has to react adaptively to task-specific priorities. In this paper, we present a method for priority-adaptive navigation of mobile autonomous systems, which optimizes the social acceptability of the behavior while meeting task constraints. We learn acceptability-dependent behavioral models from human demonstrations by using maximum entropy (MaxEnt) inverse reinforcement learning (IRL). These models are generative and describe the learned stochastic behavior. We choose the optimum behavioral model by maximizing the social acceptability under constraints on expected time-limits and reliabilities. This approach is evaluated in the context of driving behaviors based on the highway scenario of Levine et al. [1].","1050-4729","978-1-4799-6923-4","10.1109/ICRA.2015.7139642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7139642","","Trajectory;Robots;Navigation;Computational modeling;Stochastic processes;Optimization;Adaptation models","learning (artificial intelligence);mobile robots;path planning","behavioral models;online-adapting navigation strategy;social robot;social behavior;task constraints;mobile autonomous systems;priority-adaptive navigation;acceptability-dependent behavioral models;MaxEnt inverse reinforcement learning;maximum entropy;driving behaviors context","","20","","29","IEEE","2 Jul 2015","","","IEEE","IEEE Conferences"
"Demonstration-Guided Deep Reinforcement Learning of Control Policies for Dexterous Human-Robot Interaction","S. Christen; S. Stevšić; O. Hilliges","AIT Lab, Department of Computer Science, ETH, Zurich, Zurich, Switzerland; AIT Lab, Department of Computer Science, ETH, Zurich, Zurich, Switzerland; AIT Lab, Department of Computer Science, ETH, Zurich, Zurich, Switzerland","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","2161","2167","In this paper, we propose a method for training control policies for human-robot interactions such as handshakes or hand claps via Deep Reinforcement Learning. The policy controls a humanoid Shadow Dexterous Hand, attached to a robot arm. We propose a parameterizable multi-objective reward function that allows learning of a variety of interactions without changing the reward structure. The parameters of the reward function are estimated directly from motion capture data of human-human interactions in order to produce policies that are perceived as being natural and human-like by observers. We evaluate our method on three significantly different hand interactions: handshake, hand clap and finger touch. We provide detailed analysis of the proposed reward function and the resulting policies and conduct a large-scale user study, indicating that our policy produces natural looking motions.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794065","","Training;Humanoid robots;Task analysis;Robot kinematics;Reinforcement learning;Convergence","dexterous manipulators;humanoid robots;human-robot interaction;learning (artificial intelligence);motion control;neural nets","handshake;hand clap;finger touch;training control policies;human-robot interactions;hand claps;humanoid Shadow Dexterous Hand;robot arm;multiobjective reward function;reward structure;motion capture data;human-human interactions;hand interactions;dexterous human-robot interaction;demonstration-guided deep reinforcement learning","","19","","26","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Throughput Maximization for Ambient Backscatter Communication: A Reinforcement Learning Approach","X. Wen; S. Bi; X. Lin; L. Yuan; J. Wang","College of Information Engineering, Shenzhen University Shenzhen, Guangdong, China; College of Information Engineering, Shenzhen University Shenzhen, Guangdong, China; College of Information Engineering, Shenzhen University Shenzhen, Guangdong, China; College of Information Engineering, Shenzhen University Shenzhen, Guangdong, China; College of Information Engineering, Shenzhen University Shenzhen, Guangdong, China","2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)","6 Jun 2019","2019","","","997","1003","Ambient backscatter (AB) communication is an emerging wireless communication technology that enables wireless devices (WDs) to communicate without requiring active radio transmission. In an AB communication system, a WD switches between communication and energy harvesting modes. The harvested energy is used to power the devices operations, e.g., circuit power consumption and sensing operation. In this paper, we focus on maximizing the throughput performance of AB communication system by adaptively selecting the operating mode under fading channel environment. We model the problem as an infinite-horizon Markov Decision Process (MDP) and accordingly obtain the optimal mode switching policy by the value iteration algorithm given the channel distributions. Meanwhile, when the knowledge of channel distribution is absent, a Q-learning (QL) method is applied to explore a suboptimal strategy through device repeated interaction with the environment. Finally, our simulations show that the proposed QL method can achieve closeto-optimal throughput performance and significantly outperforms the other than representative benchmark methods.","","978-1-5386-6243-4","10.1109/ITNEC.2019.8729322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8729322","Ambient backscatter communication;Markov decision process;reinforcement learning;Q-learning","Receivers;Backscatter;Radio frequency;Throughput;Batteries;Energy harvesting;Wireless communication","backscatter;decision theory;energy harvesting;fading channels;iterative methods;learning (artificial intelligence);Markov processes;optimisation;radiocommunication;telecommunication computing;telecommunication power management","throughput maximization;ambient backscatter communication;reinforcement learning approach;active radio transmission;AB communication system;energy harvesting modes;circuit power consumption;fading channel environment;infinite-horizon Markov Decision Process;optimal mode switching policy;channel distribution;Q-learning method;wireless communication technology","","19","","15","IEEE","6 Jun 2019","","","IEEE","IEEE Conferences"
"A hierarchical reinforcement learning based control architecture for semi-autonomous rescue robots in cluttered environments","B. Doroodgar; G. Nejat","Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical, Industrial Engineering University of Toronto, Toronto, ONT, Canada; Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical, Industrial Engineering University of Toronto, Toronto, ONT, Canada","2010 IEEE International Conference on Automation Science and Engineering","21 Oct 2010","2010","","","948","953","Teleoperated rescue robots designed to explore disaster scenes and find victims face serious limitations due to the cluttered nature of the environments as well as the rescue operators becoming stressed and disoriented in these scenes. An alternative to using teleoperated control is to develop fully autonomous controllers for rescue robots. However, these robots are also not capable of traversing these complex unpredictable environments. In order to address the limitations of both teleoperation and fully autonomous robotic control for urban search and rescue (USAR) environments, semi-autonomous controllers can be developed to allow task sharing and cooperation between a human operator and a robot. In this paper, a unique Hierarchical Reinforcement Learning (HRL) based semi-autonomous control architecture is proposed. The architecture provides the robot with the ability to learn and make decisions regarding which rescue tasks, exploration or victim identification, should be carried out at a given time and whether an autonomous robot or a human controlled robot can perform these tasks more quickly and efficiently without compromising the safety of the victims, rescue workers and the rescue robot. Preliminary experiments presented here evaluate the performance of the proposed HRL control approach for a rescue robot in an unknown cluttered USAR environment.","2161-8089","978-1-4244-5449-5","10.1109/COASE.2010.5584599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5584599","","Robot sensing systems;Navigation;Humans;Three dimensional displays;Robot kinematics","control engineering computing;disasters;learning (artificial intelligence);mobile robots;performance evaluation;telerobotics","hierarchical reinforcement learning;semi-autonomous rescue robots;cluttered environments;teleoperated rescue robots;disaster scenes;rescue operators;teleoperated control;fully autonomous controllers;teleoperation;fully autonomous robotic control;urban search and rescue environment;USAR environments;semi-autonomous controllers;task sharing;task cooperation;human operator;HRL based semi-autonomous control architecture;rescue tasks;exploration identifiction;victim identification;human controlled robot;rescue workers;performance evaluation;HRL control approach","","19","1","19","IEEE","21 Oct 2010","","","IEEE","IEEE Conferences"
"High-performance, energy-efficient, fault-tolerant network-on-chip design using reinforcement learning","K. Wang; A. Louri; A. Karanth; R. Bunescu","Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA; Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA; School of Electrical Engineering and Computer Science, Ohio University, Athens, Ohio, USA; School of Electrical Engineering and Computer Science, Ohio University, Athens, Ohio, USA","2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)","16 May 2019","2019","","","1166","1171","Network-on-Chips (NoCs) are becoming the standard communication fabric for multi-core and system on a chip (SoC) architectures. As technology continues to scale, transistors and wires on the chip are becoming increasingly vulnerable to various fault mechanisms, especially timing errors, resulting in exacerbation of energy efficiency and performance for NoCs. Typical techniques for handling timing errors are reactive in nature, responding to the faults after their occurrence. They rely on error detection/correction techniques which have resulted in excessive power consumption and degraded performance, since the error detection/correction hardware is constantly enabled. On the other hand, indiscriminately disabling error handling hardware can induce more errors and intrusive retransmission traffic. Therefore, the challenge is to balance the trade-offs among error rate, packet retransmission, performance, and energy. In this paper, we propose a proactive fault-tolerant mechanism to optimize energy efficiency and performance with reinforcement learning (RL). First, we propose a new proactive error handling technique comprised of a dynamic scheme for enabling per-router error detection/correction hardware and an effective retransmission mechanism. Second, we propose the use of RL to train the dynamic control policy with the goals of providing increased fault-tolerance, reduced power consumption and improved performance as compared to conventional techniques. Our evaluation indicates that, on average, end-to-end packet latency is lowered by 55%, energy efficiency is improved by 64%, and retransmission caused by faults is reduced by 48% over the reactive error correction techniques.","1558-1101","978-3-9819263-2-3","10.23919/DATE.2019.8714869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8714869","","Fault tolerance;Fault tolerant systems;Error correction codes;Hardware;Decoding;Reinforcement learning;Timing","electronic engineering computing;energy conservation;error correction;error detection;fault tolerance;integrated circuit design;learning (artificial intelligence);network-on-chip","fault-tolerant network-on-chip design;reinforcement learning;standard communication fabric;energy efficiency;intrusive retransmission traffic;proactive fault-tolerant mechanism;reactive error correction techniques;NoC;power consumption;indiscriminate disabling error handling hardware;per-router error detection-correction hardware techniques;proactive error rate;transistors;wires;RL;packet retransmission mechanism;dynamic control policy;end-to-end packet latency;efficiency 64 percent","","18","","18","","16 May 2019","","","IEEE","IEEE Conferences"
"SimGAN: Hybrid Simulator Identification for Domain Adaptation via Adversarial Reinforcement Learning","Y. Jiang; T. Zhang; D. Ho; Y. Bai; C. K. Liu; S. Levine; J. Tan","Robotics at Google, Mountain View, CA, USA; Robotics at Google, Mountain View, CA, USA; Everyday Robots, X The Moonshot Factory, Mountain View, CA, USA; Everyday Robots, X The Moonshot Factory, Mountain View, CA, USA; Computer Science Department, Stanford University, Stanford, CA, USA; Robotics at Google, Mountain View, CA, USA; Robotics at Google, Mountain View, CA, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","2884","2890","As learning-based approaches progress towards automating robot controllers design, transferring learned policies to new domains with different dynamics (e.g. sim-to-real transfer) still demands manual effort. This paper introduces SimGAN, a framework to tackle domain adaptation by identifying a hybrid physics simulator to match the simulated trajectories to the ones from the target domain, using a learned discriminative loss to address the limitations associated with manual loss design. Our hybrid simulator combines neural networks and traditional physics simulation to balance expressiveness and generalizability, and alleviates the need for a carefully selected parameter set in System ID. Once the hybrid simulator is identified via adversarial reinforcement learning, it can be used to refine policies for the target domain, without the need to interleave data collection and policy refinement. We show that our approach outperforms multiple strong baselines on six robotic locomotion tasks for domain adaptation.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561731","","Conferences;Neural networks;Reinforcement learning;Manuals;Data collection;Data models;Trajectory","control system synthesis;mobile robots;motion control;neural nets;reinforcement learning","domain adaptation;hybrid physics;simulated trajectories;target domain;learned discriminative loss;manual loss design;neural networks;traditional physics simulation;adversarial reinforcement learning;data collection;policy refinement;SimGAN;hybrid simulator identification;learning-based approaches progress;robot controllers design;sim-to-real transfer;manual effort;hybrid simulator","","17","","46","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Robot Navigation in Constrained Pedestrian Environments using Reinforcement Learning","C. Pérez-D’Arpino; C. Liu; P. Goebel; R. Martín-Martín; S. Savarese","Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","1140","1146","Navigating fluently around pedestrians is a necessary capability for mobile robots deployed in human environments, such as buildings and homes. While research on social navigation has focused mainly on the scalability with the number of pedestrians in open spaces, typical indoor environments present the additional challenge of constrained spaces such as corridors and doorways that limit maneuverability and influence patterns of pedestrian interaction. We present an approach based on reinforcement learning (RL) to learn policies capable of dynamic adaptation to the presence of moving pedestrians while navigating between desired locations in constrained environments. The policy network receives guidance from a motion planner that provides waypoints to follow a globally planned trajectory, whereas RL handles the local interactions. We explore a compositional principle for multi-layout training and find that policies trained in a small set of geometrically simple layouts successfully generalize to more complex unseen layouts that exhibit composition of the structural elements available during training. Going beyond walls-world like domains, we show transfer of the learned policy to unseen 3D reconstructions of two real environments. These results support the applicability of the compositional principle to navigation in real-world buildings and indicate promising usage of multi-agent simulation within reconstructed environments for tasks that involve interaction. https://ai.stanford.edu/∼cdarpino/socialnavconstrained/","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560893","","Training;Solid modeling;Three-dimensional displays;Navigation;Scalability;Layout;Buildings","control engineering computing;human-robot interaction;mobile robots;multi-agent systems;multi-robot systems;navigation;path planning;pedestrians;reinforcement learning;robot vision","reinforcement learning;RL;multilayout training;real-world buildings;robot navigation;constrained pedestrian environments;mobile robots;social navigation;open spaces;pedestrian interaction;multi-agent simulation;3D reconstruction","","17","","37","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Intent-Aware Multi-Agent Reinforcement Learning","S. Qi; S. -C. Zhu","University of California, Los Angeles; University of California, Los Angeles","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","7533","7540","This paper proposes an intent-aware multi-agent planning framework as well as a learning algorithm. Under this framework, an agent plans in the goal space to maximize the expected utility. The planning process takes the belief of other agents' intents into consideration. Instead of formulating the learning problem as a partially observable Markov decision process (POMDP), we propose a simple but effective linear function approximation of the utility function. It is based on the observation that for humans, other people's intents will pose an influence on our utility for a goal. The proposed framework has several major advantages: i) it is computationally feasible and guaranteed to converge. ii) It can easily integrate existing intent prediction and low-level planning algorithms. iii) It does not suffer from sparse feedbacks in the action space. We experiment our algorithm in a real-world problem that is non-episodic, and the number of agents and goals can vary over time. Our algorithm is trained in a scene in which aerial robots and humans interact, and tested in a novel scene with a different environment. Experimental results show that our algorithm achieves the best performance and human-like behaviors emerge during the dynamic process.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8463211","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463211","","Planning;Prediction algorithms;Automata;Vehicles;History;Computational modeling","aerospace robotics;control engineering computing;decision theory;function approximation;learning (artificial intelligence);Markov processes;multi-agent systems;planning (artificial intelligence);robot dynamics","low-level planning algorithms;intent-aware multiagent reinforcement learning;learning algorithm;planning process;partially observable Markov decision process;linear function approximation;intent-aware multiagent planning;aerial robots;human interaction;dynamic process;POMDP","","17","2","42","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
